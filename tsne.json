[{"key": "a2016ensemble", "year": "2016", "title": "An Ensemble Diversity Approach To Supervised Binary Hashing", "abstract": "<p>Binary hashing is a well-known approach for fast approximate nearest-neighbor search in information retrieval. Much work has focused on affinity-based objective functions involving the hash functions or binary codes. These objective functions encode neighborhood information between data points and are often inspired by manifold learning algorithms. They ensure that the hash functions differ from each other through constraints or penalty terms that encourage codes to be orthogonal or dissimilar across bits but this couples the binary variables and complicates the already difficult optimization. We propose a much simpler approach we train each hash function (or bit) independently from each other but introduce diversity among them using techniques from classifier ensembles. Surprisingly we find that not only is this faster and trivially parallelizable but it also improves over the more complex coupled objective function and achieves state-of-the-art precision and recall in experiments with image retrieval.</p>\n", "tags": ["Image Retrieval", "NEURIPS", "Supervised"], "tsne_embedding": [-19.49884605407715, 11.405407905578613]}, {"key": "aamand2018non", "year": "2018", "title": "Non-empty Bins With Simple Tabulation Hashing", "abstract": "<p>We consider the hashing of a set X(subseteq) U with X=m using a simple tabulation hash function hU(to) n=0(dots)n-1 and analyse the number of non-empty bins that is the size of h(X). We show that the expected size of h(X) matches that with fully random hashing to within low-order terms. We also provide concentration bounds. The number of non-empty bins is a fundamental measure in the balls and bins paradigm and it is critical in applications such as Bloom filters and Filter hashing. For example normally Bloom filters are proportioned for a desired low false-positive probability assuming fully random hashing (see (url)en.wikipedia.org/wiki/Bloom_filter). Our results imply that if we implement the hashing with simple tabulation we obtain the same low false-positive probability for any possible input.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-25.01112174987793, -2.8415279388427734]}, {"key": "aghazadeh2016near", "year": "2016", "title": "Near-isometric Binary Hashing For Large-scale Datasets", "abstract": "<p>We develop a scalable algorithm to learn binary hash codes for indexing large-scale datasets. Near-isometric binary hashing (NIBH) is a data-dependent hashing scheme that quantizes the output of a learned low-dimensional embedding to obtain a binary hash code. In contrast to conventional hashing schemes which typically rely on an (ell_2)-norm (i.e. average distortion) minimization NIBH is based on a (ell_infty)-norm (i.e. worst-case distortion) minimization that provides several benefits including superior distance ranking and near-neighbor preservation performance. We develop a practical and efficient algorithm for NIBH based on column generation that scales well to large datasets. A range of experimental evaluations demonstrate the superiority of NIBH over ten state-of-the-art binary hashing schemes.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-29.391094207763672, -3.2971842288970947]}, {"key": "ahn2022do", "year": "2022", "title": "Do As I Can Not As I Say Grounding Language In Robotic Affordances", "abstract": "<p>Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level temporally extended instructions expressed in natural language. However a significant weakness of language models is that they lack real-world experience which makes it difficult to leverage them for decision making within a given embodiment. For example asking a language model to describe how to clean a spill might result in a reasonable narrative but it may not be applicable to a particular agent such as a robot that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language models hands and eyes while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks where we show the need for real-world grounding and that this approach is capable of completing long-horizon abstract natural language instructions on a mobile manipulator. The projects website and the video can be found at https://say-can.github.io/.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [29.81667709350586, -12.986305236816406]}, {"key": "alnuhait2023facechat", "year": "2023", "title": "Facechat An Emotion-aware Face-to-face Dialogue Framework", "abstract": "<p>While current dialogue systems like ChatGPT have made significant advancements in text-based interactions they often overlook the potential of other modalities in enhancing the overall user experience. We present FaceChat a web-based dialogue framework that enables emotionally-sensitive and face-to-face conversations. By seamlessly integrating cutting-edge technologies in natural language processing computer vision and speech processing FaceChat delivers a highly immersive and engaging user experience. FaceChat framework has a wide range of potential applications including counseling emotional support and personalized customer service. The system is designed to be simple and flexible as a platform for future researchers to advance the field of multimodal dialogue systems. The code is publicly available at https://github.com/qywu/FaceChat.</p>\n", "tags": ["ARXIV", "Cross Modal", "Has Code"], "tsne_embedding": [36.05073928833008, -8.269015312194824]}, {"key": "amara2021nearest", "year": "2021", "title": "Nearest Neighbor Search With Compact Codes A Decoder Perspective", "abstract": "<p>Modern approaches for fast retrieval of similar vectors on billion-scaled datasets rely on compressed-domain approaches such as binary sketches or product quantization. These methods minimize a certain loss typically the mean squared error or other objective functions tailored to the retrieval problem. In this paper we re-interpret popular methods such as binary hashing or product quantizers as auto-encoders and point out that they implicitly make suboptimal assumptions on the form of the decoder. We design backward-compatible decoders that improve the reconstruction of the vectors from the same codes which translates to a better performance in nearest neighbor search. Our method significantly improves over binary hashing methods or product quantization on popular benchmarks.</p>\n", "tags": ["ARXIV", "Quantisation"], "tsne_embedding": [-24.39492416381836, 12.214271545410156]}, {"key": "an2024agla", "year": "2024", "title": "AGLA Mitigating Object Hallucinations In Large Vision-language Models With Assembly Of Global And Local Attention", "abstract": "<p>Despite their great success across various multimodal tasks Large Vision-Language Models (LVLMs) are facing a prevalent problem with object hallucinations where the generated textual responses are inconsistent with ground-truth objects in the given image. This paper investigates various LVLMs and pinpoints attention deficiency toward discriminative local image features as one root cause of object hallucinations. Specifically LVLMs predominantly attend to prompt-independent global image features while failing to capture prompt-relevant local features consequently undermining the visual grounding capacity of LVLMs and leading to hallucinations. To this end we propose Assembly of Global and Local Attention (AGLA) a training-free and plug-and-play approach that mitigates object hallucinations by exploring an ensemble of global features for response generation and local features for visual discrimination simultaneously. Our approach exhibits an image-prompt matching scheme that captures prompt-relevant local features from images leading to an augmented view of the input image where prompt-relevant content is reserved while irrelevant distractions are masked. With the augmented view a calibrated decoding distribution can be derived by integrating generative global features from the original image and discriminative local features from the augmented image. Extensive experiments show that AGLA consistently mitigates object hallucinations and enhances general perception capability for LVLMs across various discriminative and generative benchmarks. Our code will be released at https://github.com/Lackel/AGLA.</p>\n", "tags": ["ARXIV", "Cross Modal", "Has Code", "Independent"], "tsne_embedding": [46.3387451171875, 8.648580551147461]}, {"key": "anagnostidis2023dynamic", "year": "2023", "title": "Dynamic Context Pruning For Efficient And Interpretable Autoregressive Transformers", "abstract": "<p>Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost most of LLMs still adopt attention layers between all pairs of tokens in the sequence thus incurring a quadratic cost. In this study we present a novel approach that dynamically prunes contextual information while preserving the models expressiveness resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so our approach not only addresses performance concerns but also enhances interpretability providing valuable insight into the models decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process and the pruning strength can be specified by a sparsity parameter. Notably our empirical findings demonstrate that we can effectively prune up to 8037; of the context without significant performance degradation on downstream tasks offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to (2times) increase in inference throughput and even greater memory savings.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [20.698240280151367, -11.097787857055664]}, {"key": "andoni2006near", "year": "2006", "title": "Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions", "abstract": "<p>We present an algorithm for the c-approximate nearest neighbor problem in a d-dimensional Euclidean space, achieving query time of O(dn 1c2/+o(1)) and space O(dn + n1+1c2/+o(1)). This almost matches the lower bound for hashing-based algorithm recently obtained in (R. Motwani et al., 2006). We also obtain a space-efficient version of the algorithm, which uses dn+n logO(1) n space, with a query time of dnO(1/c2). Finally, we discuss practical variants of the algorithms that utilize fast bounded-distance decoders for the Leech lattice</p>\n", "tags": ["FOCS"], "tsne_embedding": [-34.088600158691406, 1.8535070419311523]}, {"key": "andoni2013beyond", "year": "2013", "title": "Beyond Locality-sensitive Hashing", "abstract": "<p>We present a new data structure for the c-approximate near neighbor problem (ANN) in the Euclidean space. For n points in R^d our algorithm achieves O(n^rho + d log n) query time and O(n^1 + rho + d log n) space where rho &lt;= 7/(8c^2) + O(1 / c^3) + o(1). This is the first improvement over the result by Andoni and Indyk (FOCS 2006) and the first data structure that bypasses a locality-sensitive hashing lower bound proved by ODonnell Wu and Zhou (ICS 2011). By a standard reduction we obtain a data structure for the Hamming space and ell_1 norm with rho &lt;= 7/(8c) + O(1/c^3/2) + o(1) which is the first improvement over the result of Indyk and Motwani (STOC 1998).</p>\n", "tags": ["ARXIV", "FOCS"], "tsne_embedding": [-33.362892150878906, 1.0933574438095093]}, {"key": "andoni2015optimal", "year": "2015", "title": "Optimal Data-dependent Hashing For Approximate Near Neighbors", "abstract": "<p>We show an optimal data-dependent hashing scheme for the approximate near neighbor problem. For an (n)-point data set in a (d)-dimensional space our data structure achieves query time (O(d n^rho+o(1))) and space O(n^1+rho+o(1) + dn)( where )rho=tfrac12c^2-1 for the Euclidean space and approximation (c1). For the Hamming space we obtain an exponent of (rho=2c-1). Our result completes the direction set forth in AINR14 who gave a proof-of-concept that data-dependent hashing can outperform classical Locality Sensitive Hashing (LSH). In contrast to AINR14 the new bound is not only optimal but in fact improves over the best (optimal) LSH data structures IM98AI06 for all approximation factors (c1). From the technical perspective we proceed by decomposing an arbitrary dataset into several subsets that are in a certain sense pseudo-random.</p>\n", "tags": ["ARXIV", "Independent", "LSH"], "tsne_embedding": [-32.50957107543945, -0.07888240367174149]}, {"key": "andoni2015practical", "year": "2015", "title": "Practical And Optimal LSH For Angular Distance", "abstract": "<p>We show the existence of a Locality-Sensitive Hashing (LSH) family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent. Unlike earlier algorithms with this property (e.g. Spherical LSH (Andoni-Indyk-Nguyen-Razenshteyn 2014) (Andoni-Razenshteyn 2015)) our algorithm is also practical improving upon the well-studied hyperplane LSH (Charikar 2002) in practice. We also introduce a multiprobe version of this algorithm and conduct an experimental evaluation on real and synthetic data sets.We complement the above positive results with a fine-grained lower bound for the quality of any LSH family for angular distance. Our lower bound implies that the above LSH family exhibits a trade-off between evaluation time and quality that is close to optimal for a natural class of LSH functions.</p>\n", "tags": ["Independent", "LSH", "NEURIPS"], "tsne_embedding": [-31.389509201049805, 4.299788475036621]}, {"key": "andoni2015tight", "year": "2015", "title": "Tight Lower Bounds For Data-dependent Locality-sensitive Hashing", "abstract": "<p>We prove a tight lower bound for the exponent (rho) for data-dependent Locality-Sensitive Hashing schemes recently used to design efficient solutions for the c-approximate nearest neighbor search. In particular our lower bound matches the bound of (rho)(le) (frac1)2c-1+o(1) for the (ell)_1 space obtained via the recent algorithm from Andoni-Razenshteyn STOC15. In recent years it emerged that data-dependent hashing is strictly superior to the classical Locality-Sensitive Hashing when the hash function is data-independent. In the latter setting the best exponent has been already known for the (ell)_1 space the tight bound is (rho)=1/c with the upper bound from Indyk-Motwani STOC98 and the matching lower bound from ODonnell-Wu-Zhou ITCS11. We prove that even if the hashing is data-dependent it must hold that (rho)(ge) (frac1)2c-1-o(1). To prove the result we need to formalize the exact notion of data-dependent hashing that also captures the complexity of the hash functions (in addition to their collision properties). Without restricting such complexity we would allow for obviously infeasible solutions such as the Voronoi diagram of a dataset. To preclude such solutions we require our hash functions to be succinct. This condition is satisfied by all the known algorithmic results.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-32.67116928100586, -1.232169508934021]}, {"key": "andoni2021from", "year": "2021", "title": "From Average Embeddings To Nearest Neighbor Search", "abstract": "<p>In this note we show that one can use average embeddings introduced recently in Naor20 arXiv1905.01280 to obtain efficient algorithms for approximate nearest neighbor search. In particular a metric (X) embeds into (ell_2) on average with distortion (D) if for any distribution (mu) on (X) the embedding is (D) Lipschitz and the (square of) distance does not decrease on average (wrt (mu)). In particular existence of such an embedding (assuming it is efficient) implies a (O(D^3)) approximate nearest neighbor search under (X). This can be seen as a strengthening of the classic (bi-Lipschitz) embedding approach to nearest neighbor search and is another application of data-dependent hashing paradigm.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-31.701568603515625, 3.5657033920288086]}, {"key": "andoni2021learning", "year": "2021", "title": "Learning to Hash Robustly, with Guarantees", "abstract": "<p>The indexing algorithms for the high-dimensional nearest neighbor search (NNS) with the best worst-case guarantees are based on the randomized Locality Sensitive Hashing (LSH), and its derivatives. In practice, many heuristic approaches exist to \u201clearn\u201d the best indexing method in order to speed-up NNS, crucially adapting to the structure of the given dataset. Oftentimes, these heuristics outperform the LSH-based algorithms on real datasets, but, almost always, come at the cost of losing the guarantees of either correctness or robust performance on adversarial queries, or apply to datasets with an assumed extra structure/model. In this paper, we design an NNS algorithm for the Hamming space that has worst-case guarantees essentially matching that of theoretical algorithms, while optimizing the hashing to the structure of the dataset (think instance-optimal algorithms) for performance on the minimum-performing query. We evaluate the algorithm\u2019s ability to optimize for a given dataset both theoretically and practically. On the theoretical side, we exhibit a natural setting (dataset model) where our algorithm is much better than the standard theoretical one. On the practical side, we run experiments that show that our algorithm has a 1.8x and 2.1x better recall on the worst-performing queries to the MNIST and ImageNet datasets.</p>\n", "tags": ["ARXIV", "Has Code", "Image Retrieval"], "tsne_embedding": [-23.563371658325195, -4.572701454162598]}, {"key": "andrecut2021additive", "year": "2021", "title": "Additive Feature Hashing", "abstract": "<p>The hashing trick is a machine learning technique used to encode categorical features into a numerical vector representation of pre-defined fixed length. It works by using the categorical hash values as vector indices and updating the vector values at those indices. Here we discuss a different approach based on additive-hashing and the almost orthogonal property of high-dimensional random vectors. That is we show that additive feature hashing can be performed directly by adding the hash values and converting them into high-dimensional numerical vectors. We show that the performance of additive feature hashing is similar to the hashing trick and we illustrate the results numerically using synthetic language recognition and SMS spam detection data.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-13.986624717712402, -13.874543190002441]}, {"key": "apple2021halftimehash", "year": "2021", "title": "Halftimehash Modern Hashing Without 64-bit Multipliers Or Finite Fields", "abstract": "<p>HalftimeHash is a new algorithm for hashing long strings. The goals are few collisions (different inputs that produce identical output hash values) and high performance. Compared to the fastest universal hash functions on long strings (clhash and UMASH) HalftimeHash decreases collision probability while also increasing performance by over 5037; exceeding 16 bytes per cycle. In addition HalftimeHash does not use any widening 64-bit multiplications or any finite field arithmetic that could limit its portability.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-26.54755210876465, -15.783411026000977]}, {"key": "appleton2015multi", "year": "2015", "title": "Multi-probe Consistent Hashing", "abstract": "<p>We describe a consistent hashing algorithm which performs multiple lookups per key in a hash table of nodes. It requires no additional storage beyond the hash table and achieves a peak-to-average load ratio of 1 + epsilon with just 1 + 1/epsilon lookups per key.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-24.323461532592773, -19.912006378173828]}, {"key": "argerich2016feature", "year": "2016", "title": "Hash2vec Feature Hashing For Word Embeddings", "abstract": "<p>In this paper we propose the application of feature hashing to create word embeddings for natural language processing. Feature hashing has been used successfully to create document vectors in related tasks like document classification. In this work we show that feature hashing can be applied to obtain word embeddings in linear time with the size of the data. The results show that this algorithm that does not need training is able to capture the semantic meaning of words. We compare the results against GloVe showing that they are similar. As far as we know this is the first application of feature hashing to the word embeddings problem and the results indicate this is a scalable technique with practical results for NLP applications.</p>\n", "tags": ["Supervised"], "tsne_embedding": [17.204753875732422, 5.303130149841309]}, {"key": "argerich2017generic", "year": "2017", "title": "Generic LSH Families For The Angular Distance Based On Johnson-lindenstrauss Projections And Feature Hashing LSH", "abstract": "<p>In this paper we propose the creation of generic LSH families for the angular distance based on Johnson-Lindenstrauss projections. We show that feature hashing is a valid J-L projection and propose two new LSH families based on feature hashing. These new LSH families are tested on both synthetic and real datasets with very good results and a considerable performance improvement over other LSH families. While the theoretical analysis is done for the angular distance these families can also be used in practice for the euclidean distance with excellent results 2. Our tests using real datasets show that the proposed LSH functions work well for the euclidean distance.</p>\n", "tags": ["ARXIV", "Independent", "LSH"], "tsne_embedding": [-31.70305633544922, 6.039196014404297]}, {"key": "arora2022ask", "year": "2022", "title": "Ask Me Anything A Simple Strategy For Prompting Language Models", "abstract": "<p>Large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt that demonstrates how to perform the task and no additional training. Prompting is a brittle process wherein small modifications to the prompt can cause large variations in the model predictions and therefore significant effort is dedicated towards designing a painstakingly perfect prompt for a task. To mitigate the high degree of effort involved in prompt-design we instead ask whether producing multiple effective yet imperfect prompts and aggregating them can lead to a high quality prompting strategy. Our observations motivate our proposed prompting method ASK ME ANYTHING (AMA). We first develop an understanding of the effective prompt formats finding that question-answering (QA) prompts which encourage open-ended generation (Who went to the park) tend to outperform those that restrict the model outputs (John went to the park. Output True or False.). Our approach recursively uses the LLM itself to transform task inputs to the effective QA format. We apply the collected prompts to obtain several noisy votes for the inputs true label. We find that the prompts can have very different accuracies and complex dependencies and thus propose to use weak supervision a procedure for combining the noisy predictions to produce the final predictions for the inputs. We evaluate AMA across open-source model families (e.g. EleutherAI BLOOM OPT and T0) and model sizes (125M-175B parameters) demonstrating an average performance lift of 10.237; over the few-shot baseline. This simple strategy enables the open-source GPT-J-6B model to match and exceed the performance of few-shot GPT3-175B on 15 of 20 popular benchmarks. Averaged across these tasks the GPT-J-6B model outperforms few-shot GPT3-175B. We release our code here https://github.com/HazyResearch/ama_prompting</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [42.831058502197266, -4.992943286895752]}, {"key": "arponen2019shrewd", "year": "2019", "title": "SHREWD Semantic Hierarchy-based Relational Embeddings For Weakly-supervised Deep Hashing", "abstract": "<p>Using class labels to represent class similarity is a typical approach to training deep hashing systems for retrieval; samples from the same or different classes take binary 1 or 0 similarity values. This similarity does not model the full rich knowledge of semantic relations that may be present between data points. In this work we build upon the idea of using semantic hierarchies to form distance metrics between all available sample labels; for example cat to dog has a smaller distance than cat to guitar. We combine this type of semantic distance into a loss function to promote similar distances between the deep neural network embeddings. We also introduce an empirical Kullback-Leibler divergence loss term to promote binarization and uniformity of the embeddings. We test the resulting SHREWD method and demonstrate improvements in hierarchical retrieval scores using compact binary hash codes instead of real valued ones and show that in a weakly supervised hashing setting we are able to learn competitively without explicitly relying on class labels but instead on similarities between labels.</p>\n", "tags": ["ARXIV", "Supervised", "Weakly Supervised"], "tsne_embedding": [-12.060799598693848, -7.955385208129883]}, {"key": "arponen2020learning", "year": "2020", "title": "Learning To Hash With Semantic Similarity Metrics And Empirical KL Divergence", "abstract": "<p>Learning to hash is an efficient paradigm for exact and approximate nearest neighbor search from massive databases. Binary hash codes are typically extracted from an image by rounding output features from a CNN which is trained on a supervised binary similar/ dissimilar task. Drawbacks of this approach are (i) resulting codes do not necessarily capture semantic similarity of the input data (ii) rounding results in information loss manifesting as decreased retrieval performance and (iii) Using only class-wise similarity as a target can lead to trivial solutions simply encoding classifier outputs rather than learning more intricate relations which is not detected by most performance metrics. We overcome (i) via a novel loss function encouraging the relative hash code distances of learned features to match those derived from their targets. We address (ii) via a differentiable estimate of the KL divergence between network outputs and a binary target distribution resulting in minimal information loss when the features are rounded to binary. Finally we resolve (iii) by focusing on a hierarchical precision metric. Efficiency of the methods is demonstrated with semantic image retrieval on the CIFAR-100 ImageNet and Conceptual Captions datasets using similarities inferred from the WordNet label hierarchy or sentence embeddings.</p>\n", "tags": ["ARXIV", "CNN", "Image Retrieval", "Supervised"], "tsne_embedding": [-8.343609809875488, 5.052391052246094]}, {"key": "arthur2010reverse", "year": "2010", "title": "Reverse Nearest Neighbors Search In High Dimensions Using Locality-sensitive Hashing", "abstract": "<p>We investigate the problem of finding reverse nearest neighbors efficiently. Although provably good solutions exist for this problem in low or fixed dimensions to this date the methods proposed in high dimensions are mostly heuristic. We introduce a method that is both provably correct and efficient in all dimensions based on a reduction of the problem to one instance of (e)-nearest neighbor search plus a controlled number of instances of (em) exhaustive r-(pleb) a variant of (em) Point Location among Equal Balls where all the r-balls centered at the data points that contain the query point are sought for not just one. The former problem has been extensively studied and elegantly solved in high dimensions using Locality-Sensitive Hashing (LSH) techniques. By contrast the latter problem has a complexity that is still not fully understood. We revisit the analysis of the LSH scheme for exhaustive r-(pleb) using a somewhat refined notion of locality-sensitive family of hash function which brings out a meaningful output-sensitive term in the complexity of the problem. Our analysis combined with a non-isometric lifting of the data enables us to answer exhaustive r-(pleb) queries (and down the road reverse nearest neighbors queries) efficiently. Along the way we obtain a simple algorithm for answering exact nearest neighbor queries whose complexity is parametrized by some (em) condition number measuring the inherent difficulty of a given instance of the problem.</p>\n", "tags": ["ARXIV", "Independent", "LSH"], "tsne_embedding": [-31.169200897216797, -1.9254474639892578]}, {"key": "askell2021general", "year": "2021", "title": "A General Language Assistant As A Laboratory For Alignment", "abstract": "<p>Given the broad capabilities of large language models it should be possible to work towards a general-purpose text-based assistant that is aligned with human values meaning that it is helpful honest and harmless. As an initial foray in this direction we study simple baseline techniques and evaluations such as prompting. We find that the benefits from modest interventions increase with model size generalize to a variety of alignment evaluations and do not compromise the performance of large models. Next we investigate scaling trends for several training objectives relevant to alignment comparing imitation learning binary discrimination and ranked preference modeling. We find that ranked preference modeling performs much better than imitation learning and often scales more favorably with model size. In contrast binary discrimination typically performs and scales very similarly to imitation learning. Finally we study a preference model pre-training stage of training with the goal of improving sample efficiency when finetuning on human preferences.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [43.098304748535156, -0.42383041977882385]}, {"key": "aum\u00fcller2012explicit", "year": "2012", "title": "Explicit And Efficient Hash Families Suffice For Cuckoo Hashing With A Stash", "abstract": "<p>It is shown that for cuckoo hashing with a stash as proposed by Kirsch Mitzenmacher and Wieder (2008) families of very simple hash functions can be used maintaining the favorable performance guarantees with stash size s the probability of a rehash is O(1/n^s+1) and the evaluation time is O(s). Instead of the full randomness needed for the analysis of Kirsch et al. and of Kutzelnigg (2010) (resp. (Theta)((log) n)-wise independence for standard cuckoo hashing) the new approach even works with 2-wise independent hash families as building blocks. Both construction and analysis build upon the work of Dietzfelbinger and Woelfel (2003). The analysis which can also be applied to the fully random case utilizes a graph counting argument and is much simpler than previous proofs. As a byproduct an algorithm for simulating uniform hashing is obtained. While it requires about twice as much space as the most space efficient solutions it is attractive because of its simple and direct structure.</p>\n", "tags": ["ARXIV", "Graph", "Independent"], "tsne_embedding": [-26.28608512878418, -9.203682899475098]}, {"key": "auvolat2015clustering", "year": "2015", "title": "Clustering Is Efficient For Approximate Maximum Inner Product Search", "abstract": "<p>Efficient Maximum Inner Product Search (MIPS) is an important task that has a wide applicability in recommendation systems and classification with a large number of classes. Solutions based on locality-sensitive hashing (LSH) as well as tree-based solutions have been investigated in the recent literature to perform approximate MIPS in sublinear time. In this paper we compare these to another extremely simple approach for solving approximate MIPS based on variants of the k-means clustering algorithm. Specifically we propose to train a spherical k-means after having reduced the MIPS problem to a Maximum Cosine Similarity Search (MCSS). Experiments on two standard recommendation system benchmarks as well as on large vocabulary word embeddings show that this simple approach yields much higher speedups for the same retrieval precision than current state-of-the-art hashing-based and tree-based methods. This simple method also yields more robust retrievals when the query is corrupted by noise.</p>\n", "tags": ["ARXIV", "LSH", "Unsupervised"], "tsne_embedding": [-20.848411560058594, 1.82965087890625]}, {"key": "azarafrooz2018fuzzy", "year": "2018", "title": "Fuzzy Hashing As Perturbation-consistent Adversarial Kernel Embedding", "abstract": "<p>Measuring the similarity of two files is an important task in malware analysis with fuzzy hash functions being a popular approach. Traditional fuzzy hash functions are data agnostic they do not learn from a particular dataset how to determine similarity; their behavior is fixed across all datasets. In this paper we demonstrate that fuzzy hash functions can be learned in a novel minimax training framework and that these learned fuzzy hash functions outperform traditional fuzzy hash functions at the file similarity task for Portable Executable files. In our approach hash digests can be extracted from the kernel embeddings of two kernel networks trained in a minimax framework where the roles of players during training (i.e adversary versus generator) alternate along with the input data. We refer to this new minimax architecture as perturbation-consistent. The similarity score for a pair of files is the utility of the minimax game in equilibrium. Our experiments show that learned fuzzy hash functions generalize well capable of determining that two files are similar even when one of those files was generated using insertion and deletion operations.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-11.079578399658203, -9.994159698486328]}, {"key": "b2018fully", "year": "2018", "title": "Fully Understanding The Hashing Trick", "abstract": "<p>Feature hashing also known as em the hashing trick introduced by Weinberger et al. (2009) is one of the key techniques used in scaling-up machine learning algorithms. Loosely speaking feature hashing uses a random sparse projection matrix (A ^n to ^m) (where (m ll n)) in order to reduce the dimension of the data from (n) to (m) while approximately preserving the Euclidean norm. Every column of (A) contains exactly one non-zero entry equals to either (-1) or (1). Weinberger et al. showed tail bounds on (Ax_2^2). Specifically they showed that for every (varepsilon delta) if (x_infty / x_2) is sufficiently small and (m) is sufficiently large then beginequationPr ; ;Ax_2^2 - x_2^2; &lt; varepsilon x_2^2 ; ge 1 - delta ;.endequation These bounds were later extended by Dasgupta et al. (2010) and most recently refined by Dahlgaard et al. (2017) however the true nature of the performance of this key technique and specifically the correct tradeoff between the pivotal parameters (x_infty / x_2 m varepsilon delta) remained an open question. We settle this question by giving tight asymptotic bounds on the exact tradeoff between the central parameters thus providing a complete understanding of the performance of feature hashing. We complement the asymptotic bound with empirical data which shows that the constants hiding in the asymptotic notation are in fact very close to (1) thus further illustrating the tightness of the presented bounds in practice.</p>\n", "tags": ["Independent", "NEURIPS"], "tsne_embedding": [-34.62861633300781, -4.359066486358643]}, {"key": "b2020language", "year": "2020", "title": "Language Models Are Few-shot Learners", "abstract": "<p>Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic few-shot performance sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically we train GPT-3 an autoregressive language model with 175 billion parameters 10x more than any previous non-sparse language model and test its performance in the few-shot setting. For all tasks GPT-3 is applied without any gradient updates or fine-tuning with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets including translation question-answering and cloze tasks as well as several tasks that require on-the-fly reasoning or domain adaptation such as unscrambling words using a novel word in a sentence or performing 3-digit arithmetic. At the same time we also identify some datasets where GPT-3s few-shot learning still struggles as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [29.146638870239258, -0.8869196772575378]}, {"key": "baek2023knowledge", "year": "2023", "title": "Knowledge-augmented Large Language Models For Personalized Contextual Query Suggestion", "abstract": "<p>Large Language Models (LLMs) excel at tackling various natural language tasks. However due to the significant costs involved in re-training or fine-tuning them they remain largely static and difficult to personalize. Nevertheless a variety of applications could benefit from generations that are tailored to users preferences goals and knowledge. Among them is web search where knowing what a user is trying to accomplish what they care about and what they know can lead to improved search experiences. In this work we propose a novel and general approach that augments an LLM with relevant context from users interaction histories with a search engine in order to personalize its outputs. Specifically we construct an entity-centric knowledge store for each user based on their search and browsing activities on the web which is then leveraged to provide contextually relevant LLM prompt augmentations. This knowledge store is light-weight since it only produces user-specific aggregate projections of interests and knowledge onto public knowledge graphs and leverages existing search log infrastructure thereby mitigating the privacy compliance and scalability concerns associated with building deep user profiles for personalization. We validate our approach on the task of contextual query suggestion which requires understanding not only the users current search context but also what they historically know and care about. Through a number of experiments based on human evaluation we show that our approach is significantly better than several other LLM-powered baselines generating query suggestions that are contextually more relevant personalized and useful.</p>\n", "tags": ["ARXIV", "Graph", "Independent"], "tsne_embedding": [40.205223083496094, -14.47070026397705]}, {"key": "bai2020bai", "year": "2020", "title": "Targeted Attack for Deep Hashing based Retrieval", "abstract": "<p>The deep hashing based retrieval method is widely adopted in large-scale image and video retrieval. However, there is little investigation on its security. In this paper, we propose a novel method, dubbed deep hashing targeted attack (DHTA), to study the targeted attack on such retrieval. Specifically, we first formulate the targeted attack as a point-to-set optimization, which minimizes the average distance between the hash code of an adversarial example and those of a set of objects with the target label. Then we design a novel component-voting scheme to obtain an anchor code as the representative of the set of hash codes of objects with the target label, whose optimality guarantee is also theoretically derived. To balance the performance and perceptibility, we propose to minimize the Hamming distance between the hash code of the adversarial example and the anchor code under the \u2113\u221e restriction on the perturbation. Extensive experiments verify that DHTA is effective in attacking both deep hashing based image retrieval and video retrieval.</p>\n", "tags": ["Deep Learning", "Image Retrieval", "Video Retrieval"], "tsne_embedding": [-11.465636253356934, -26.730934143066406]}, {"key": "bai2020targeted", "year": "2020", "title": "Targeted Attack For Deep Hashing Based Retrieval", "abstract": "<p>The deep hashing based retrieval method is widely adopted in large-scale image and video retrieval. However there is little investigation on its security. In this paper we propose a novel method dubbed deep hashing targeted attack (DHTA) to study the targeted attack on such retrieval. Specifically we first formulate the targeted attack as a point-to-set optimization which minimizes the average distance between the hash code of an adversarial example and those of a set of objects with the target label. Then we design a novel component-voting scheme to obtain an anchor code as the representative of the set of hash codes of objects with the target label whose optimality guarantee is also theoretically derived. To balance the performance and perceptibility we propose to minimize the Hamming distance between the hash code of the adversarial example and the anchor code under the (ell)^(infty) restriction on the perturbation. Extensive experiments verify that DHTA is effective in attacking both deep hashing based image retrieval and video retrieval.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Independent", "Video Retrieval"], "tsne_embedding": [-11.482488632202148, -26.726381301879883]}, {"key": "bang2023multitask", "year": "2023", "title": "A Multitask Multilingual Multimodal Evaluation Of Chatgpt On Reasoning Hallucination And Interactivity", "abstract": "<p>This paper proposes a framework for quantitatively evaluating interactive LLMs such as ChatGPT using publicly available data sets. We carry out an extensive technical evaluation of ChatGPT using 23 data sets covering 8 different common NLP application tasks. We evaluate the multitask multilingual and multi-modal aspects of ChatGPT based on these data sets and a newly designed multimodal dataset. We find that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts via an intermediate code generation step. Moreover we find that ChatGPT is 63.4137; accurate on average in 10 different reasoning categories under logical reasoning non-textual reasoning and commonsense reasoning hence making it an unreliable reasoner. It is for example better at deductive than inductive reasoning. ChatGPT suffers from hallucination problems like other LLMs and it generates more extrinsic hallucinations from its parametric memory as it does not have access to an external knowledge base. Finally the interactive feature of ChatGPT enables human collaboration with the underlying LLM to improve its performance i.e 837; ROUGE-1 on summarization and 237; ChrF++ on machine translation in a multi-turn prompt engineering fashion. We also release codebase for evaluation set extraction.</p>\n", "tags": ["ARXIV", "Cross Modal", "Dataset"], "tsne_embedding": [32.550994873046875, 0.07373954355716705]}, {"key": "baranchuk2023dedrift", "year": "2023", "title": "Dedrift Robust Similarity Search Under Content Drift", "abstract": "<p>The statistical distribution of content uploaded and searched on media sharing sites changes over time due to seasonal sociological and technical factors. We investigate the impact of this content drift for large-scale similarity search tools based on nearest neighbor search in embedding space. Unless a costly index reconstruction is performed frequently content drift degrades the search accuracy and efficiency. The degradation is especially severe since in general both the query and database distributions change. We introduce and analyze real-world image and video datasets for which temporal information is available over a long time period. Based on the learnings we devise DeDrift a method that updates embedding quantizers to continuously adapt large-scale indexing structures on-the-fly. DeDrift almost eliminates the accuracy degradation due to the query and database content drift while being up to 100x faster than a full index reconstruction.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-5.738768100738525, -20.039884567260742]}, {"key": "barz2017automatic", "year": "2017", "title": "Automatic Query Image Disambiguation For Content-based Image Retrieval", "abstract": "<p>Query images presented to content-based image retrieval systems often have various different interpretations making it difficult to identify the search objective pursued by the user. We propose a technique for overcoming this ambiguity while keeping the amount of required user interaction at a minimum. To achieve this the neighborhood of the query image is divided into coherent clusters from which the user may choose the relevant ones. A novel feedback integration technique is then employed to re-rank the entire database with regard to both the user feedback and the original query. We evaluate our approach on the publicly available MIRFLICKR-25K dataset where it leads to a relative improvement of average precision by 2337; over the baseline retrieval which does not distinguish between different image senses.</p>\n", "tags": ["ARXIV", "Image Retrieval"], "tsne_embedding": [10.806451797485352, 17.872533798217773]}, {"key": "bawa2005forest", "year": "2005", "title": "LSH Forest: Self-Tuning Indexes for Similarity Search", "abstract": "<p>We consider the problem of indexing high-dimensional data for answering (approximate) similarity-search queries. Similarity indexes prove to be important in a wide variety of settings: Web search\nengines desire fast, parallel, main-memory-based indexes for similarity search on text data; database systems desire disk-based similarity indexes for high-dimensional data, including text and images;\npeer-to-peer systems desire distributed similarity indexes with low\ncommunication cost. We propose an indexing scheme called LSH\nForest which is applicable in all the above contexts. Our index uses the well-known technique of locality-sensitive hashing (LSH),\nbut improves upon previous designs by (a) eliminating the different data-dependent parameters for which LSH must be constantly hand-tuned, and (b) improving on LSH\u2019s performance guarantees for skewed data distributions while retaining the same storage\nand query overhead. We show how to construct this index in main\nmemory, on disk, in parallel systems, and in peer-to-peer systems.\nWe evaluate the design with experiments on multiple text corpora\nand demonstrate both the self-tuning nature and the superior performance of LSH Forest.</p>\n", "tags": ["LSH", "WWW"], "tsne_embedding": [-19.429677963256836, -2.468202829360962]}, {"key": "bengio2009group", "year": "2009", "title": "Group Sparse Coding", "abstract": "<p>Bag-of-words document representations are often used in text image and video processing. While it is relatively easy to determine a suitable word dictionary for text documents there is no simple mapping from raw images or videos to dictionary terms. The classical approach builds a dictionary using vector quantization over a large set of useful visual descriptors extracted from a training set and uses a nearest-neighbor algorithm to count the number of occurrences of each dictionary word in documents to be encoded. More robust approaches have been proposed recently that represent each visual descriptor as a sparse weighted combination of dictionary words. While favoring a sparse representation at the level of visual descriptors those methods however do not ensure that images have sparse representation. In this work we use mixed-norm regularization to achieve sparsity at the image level as well as a small overall dictionary. This approach can also be used to encourage using the same dictionary words for all the images in a class providing a discriminative signal in the construction of image representations. Experimental results on a benchmark image classification dataset show that when compact image or dictionary representations are needed for computational efficiency the proposed approach yields better mean average precision in classification.</p>\n", "tags": ["NEURIPS", "Quantisation", "Supervised"], "tsne_embedding": [-6.396385192871094, 4.947319984436035]}, {"key": "benkish2023mitigating", "year": "2023", "title": "Mitigating Open-vocabulary Caption Hallucinations", "abstract": "<p>While recent years have seen rapid progress in image-conditioned text generation image captioning still suffers from the fundamental issue of hallucinations namely the generation of spurious details that cannot be inferred from the given image. Existing methods largely use closed-vocabulary object lists to mitigate or evaluate hallucinations in image captioning ignoring the long-tailed nature of hallucinations that occur in practice. To this end we propose a framework for addressing hallucinations in image captioning in the open-vocabulary setting. Our framework includes a new benchmark OpenCHAIR that leverages generative foundation models to evaluate open-vocabulary object hallucinations for image captioning surpassing the popular and similarly-sized CHAIR benchmark in both diversity and accuracy. Furthermore to mitigate open-vocabulary hallucinations without using a closed object list we propose MOCHa an approach harnessing advancements in reinforcement learning. Our multi-objective reward function explicitly targets the trade-off between fidelity and adequacy in generations without requiring any strong supervision. MOCHa improves a large variety of image captioning models as captured by our OpenCHAIR benchmark and other existing metrics. We will release our code and models.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [49.24836349487305, 4.233551979064941]}, {"key": "bercea2023locally", "year": "2023", "title": "Locally Uniform Hashing", "abstract": "<p>Hashing is a common technique used in data processing with a strong impact on the time and resources spent on computation. Hashing also affects the applicability of theoretical results that often assume access to (unrealistic) uniform/fully-random hash functions. In this paper we are concerned with designing hash functions that are practical and come with strong theoretical guarantees on their performance. To this end we present tornado tabulation hashing which is simple fast and exhibits a certain full local randomness property that provably makes diverse algorithms perform almost as if (abstract) fully-random hashing was used. For example this includes classic linear probing the widely used HyperLogLog algorithm of Flajolet Fusy Gandouet Meunier AOFA 97 for counting distinct elements and the one-permutation hashing of Li Owen and Zhang NIPS 12 for large-scale machine learning. We also provide a very efficient solution for the classical problem of obtaining fully-random hashing on a fixed (but unknown to the hash function) set of n keys using O(n) space. As a consequence we get more efficient implementations of the splitting trick of Dietzfelbinger and Rink ICALP09 and the succinct space uniform hashing of Pagh and Pagh SICOMP08. Tornado tabulation hashing is based on a simple method to systematically break dependencies in tabulation-based hashing techniques.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-18.680646896362305, -12.59813117980957]}, {"key": "berriche2024leveraging", "year": "2024", "title": "Leveraging High-resolution Features For Improved Deep Hashing-based Image Retrieval", "abstract": "<p>Deep hashing techniques have emerged as the predominant approach for efficient image retrieval. Traditionally these methods utilize pre-trained convolutional neural networks (CNNs) such as AlexNet and VGG-16 as feature extractors. However the increasing complexity of datasets poses challenges for these backbone architectures in capturing meaningful features essential for effective image retrieval. In this study we explore the efficacy of employing high-resolution features learned through state-of-the-art techniques for image retrieval tasks. Specifically we propose a novel methodology that utilizes High-Resolution Networks (HRNets) as the backbone for the deep hashing task termed High-Resolution Hashing Network (HHNet). Our approach demonstrates superior performance compared to existing methods across all tested benchmark datasets including CIFAR-10 NUS-WIDE MS COCO and ImageNet. This performance improvement is more pronounced for complex datasets which highlights the need to learn high-resolution features for intricate image retrieval tasks. Furthermore we conduct a comprehensive analysis of different HRNet configurations and provide insights into the optimal architecture for the deep hashing task</p>\n", "tags": ["ARXIV", "Image Retrieval", "Supervised"], "tsne_embedding": [8.57382583618164, 10.565507888793945]}, {"key": "bessa2023weighted", "year": "2023", "title": "Weighted Minwise Hashing Beats Linear Sketching For Inner Product Estimation", "abstract": "<p>We present a new approach for computing compact sketches that can be used to approximate the inner product between pairs of high-dimensional vectors. Based on the Weighted MinHash algorithm our approach admits strong accuracy guarantees that improve on the guarantees of popular linear sketching approaches for inner product estimation such as CountSketch and Johnson-Lindenstrauss projection. Specifically while our method admits guarantees that exactly match linear sketching for dense vectors it yields significantly lower error for sparse vectors with limited overlap between non-zero entries. Such vectors arise in many applications involving sparse data. They are also important in increasingly popular dataset search applications where inner product sketches are used to estimate data covariance conditional means and other quantities involving columns in unjoined tables. We complement our theoretical results by showing that our approach empirically outperforms existing linear sketches and unweighted hashing-based sketches for sparse vectors.</p>\n", "tags": ["Independent", "SIGMOD"], "tsne_embedding": [-25.546142578125, 5.808840751647949]}, {"key": "bhatia2022exploiting", "year": "2022", "title": "Exploiting And Defending Against The Approximate Linearity Of Apples Neuralhash", "abstract": "<p>Perceptual hashes map images with identical semantic content to the same n-bit hash value while mapping semantically-different images to different hashes. These algorithms carry important applications in cybersecurity such as copyright infringement detection content fingerprinting and surveillance. Apples NeuralHash is one such system that aims to detect the presence of illegal content on users devices without compromising consumer privacy. We make the surprising discovery that NeuralHash is approximately linear which inspires the development of novel black-box attacks that can (i) evade detection of illegal images (ii) generate near-collisions and (iii) leak information about hashed images all without access to model parameters. These vulnerabilities pose serious threats to NeuralHashs security goals; to address them we propose a simple fix using classical cryptographic standards.</p>\n", "tags": ["ARXIV", "Graph"], "tsne_embedding": [-2.155416250228882, -21.251699447631836]}, {"key": "bhunia2018texture", "year": "2018", "title": "Texture Synthesis Guided Deep Hashing For Texture Image Retrieval", "abstract": "<p>With the large-scale explosion of images and videos over the internet efficient hashing methods have been developed to facilitate memory and time efficient retrieval of similar images. However none of the existing works uses hashing to address texture image retrieval mostly because of the lack of sufficiently large texture image databases. Our work addresses this problem by developing a novel deep learning architecture that generates binary hash codes for input texture images. For this we first pre-train a Texture Synthesis Network (TSN) which takes a texture patch as input and outputs an enlarged view of the texture by injecting newer texture content. Thus it signifies that the TSN encodes the learnt texture specific information in its intermediate layers. In the next stage a second network gathers the multi-scale feature representations from the TSNs intermediate layers using channel-wise attention combines them in a progressive manner to a dense continuous representation which is finally converted into a binary hash code with the help of individual and pairwise label information. The new enlarged texture patches also help in data augmentation to alleviate the problem of insufficient texture data and are used to train the second stage of the network. Experiments on three public texture image retrieval datasets indicate the superiority of our texture synthesis guided hashing approach over current state-of-the-art methods.</p>\n", "tags": ["ARXIV", "Deep Learning", "Image Retrieval", "Independent"], "tsne_embedding": [2.3015923500061035, 7.20860481262207]}, {"key": "biderman2023emergent", "year": "2023", "title": "Emergent And Predictable Memorization In Large Language Models", "abstract": "<p>Memorization or the tendency of large language models (LLMs) to output entire sequences from their training data verbatim is a key concern for safely deploying language models. In particular it is vital to minimize a models memorization of sensitive datapoints such as those containing personal identifiable information (PII). The prevalence of such undesirable memorization can pose issues for model trainers and may even require discarding an otherwise functional model. We therefore seek to predict which sequences will be memorized before a large models full train-time by extrapolating the memorization behavior of lower-compute trial runs. We measure memorization of the Pythia model suite and plot scaling laws for forecasting memorization allowing us to provide equi-compute recommendations to maximize the reliability (recall) of such predictions. We additionally provide further novel discoveries on the distribution of memorization scores across models and data. We release all code and data necessary to reproduce the results in this paper at https://github.com/EleutherAI/pythia</p>\n", "tags": ["ARXIV", "Has Code", "Supervised"], "tsne_embedding": [31.118425369262695, -7.302151679992676]}, {"key": "biderman2023pythia", "year": "2023", "title": "Pythia A Suite For Analyzing Large Language Models Across Training And Scaling", "abstract": "<p>How do large language models (LLMs) develop and evolve over the course of training How do these patterns change as models scale To answer these questions we introduce textitPythia a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models alongside tools to download and reconstruct their exact training dataloaders for further study. We intend textitPythia to facilitate research in many areas and we present several case studies including novel results in memorization term frequency effects on few-shot performance and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models analysis code training code and training data can be found at urlhttps://github.com/EleutherAI/pythia}.</p>\n", "tags": ["ARXIV", "Case Study", "Has Code", "Supervised"], "tsne_embedding": [30.72756004333496, -7.644876480102539]}, {"key": "bigann", "year": "2009", "title": "Datasets for approximate nearest neighbor search", "abstract": "<p>BIGANN consists of SIFT descriptors applied to images from extracted from a large image dataset.</p>\n", "tags": ["Dataset"], "tsne_embedding": [-18.290159225463867, 24.799150466918945]}, {"key": "biswas2021state", "year": "2021", "title": "State Of The Art Image Hashing", "abstract": "<p>Perceptual image hashing methods are often applied in various objectives such as image retrieval finding duplicate or near-duplicate images and finding similar images from large-scale image content. The main challenge in image hashing techniques is robust feature extraction which generates the same or similar hashes in images that are visually identical. In this article we present a short review of the state-of-the-art traditional perceptual hashing and deep learning-based perceptual hashing methods identifying the best approaches.</p>\n", "tags": ["ARXIV", "Deep Learning", "Image Retrieval", "Survey Paper"], "tsne_embedding": [-16.887468338012695, 14.907502174377441]}, {"key": "black2022gpt", "year": "2022", "title": "Gpt-neox-20b An Open-source Autoregressive Language Model", "abstract": "<p>We introduce GPT-NeoX-20B a 20 billion parameter autoregressive language model trained on the Pile whose weights will be made freely and openly available to the public through a permissive license. It is to the best of our knowledge the largest dense autoregressive model that has publicly available weights at the time of submission. In this work we describe models architecture and training and evaluate its performance on a range of language-understanding mathematics and knowledge-based tasks. We find that GPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more in performance when evaluated five-shot than similarly sized GPT-3 and FairSeq models. We open-source the training and evaluation code as well as the model weights at https://github.com/EleutherAI/gpt-neox.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [21.238725662231445, 5.449699401855469]}, {"key": "bondugula2015shoe", "year": "2015", "title": "SHOE Supervised Hashing With Output Embeddings", "abstract": "<p>We present a supervised binary encoding scheme for image retrieval that learns projections by taking into account similarity between classes obtained from output embeddings. Our motivation is that binary hash codes learned in this way improve both the visual quality of retrieval results and existing supervised hashing schemes. We employ a sequential greedy optimization that learns relationship aware projections by minimizing the difference between inner products of binary codes and output embedding vectors. We develop a joint optimization framework to learn projections which improve the accuracy of supervised hashing over the current state of the art with respect to standard and sibling evaluation metrics. We further boost performance by applying the supervised dimensionality reduction technique on kernelized input CNN features. Experiments are performed on three datasets CUB-2011 SUN-Attribute and ImageNet ILSVRC 2010. As a by-product of our method we show that using a simple k-nn pooling classifier with our discriminative codes improves over the complex classification models on fine grained datasets like CUB and offer an impressive compression ratio of 1024 on CNN features.</p>\n", "tags": ["ARXIV", "CNN", "Image Retrieval", "Supervised"], "tsne_embedding": [-12.643067359924316, 18.67691421508789]}, {"key": "botelho2007perfect", "year": "2007", "title": "Perfect Hashing For Data Management Applications", "abstract": "<p>Perfect hash functions can potentially be used to compress data in connection with a variety of data management tasks. Though there has been considerable work on how to construct good perfect hash functions there is a gap between theory and practice among all previous methods on minimal perfect hashing. On one side there are good theoretical results without experimentally proven practicality for large key sets. On the other side there are the theoretically analyzed time and space usage algorithms that assume that truly random hash functions are available for free which is an unrealistic assumption. In this paper we attempt to bridge this gap between theory and practice using a number of techniques from the literature to obtain a novel scheme that is theoretically well-understood and at the same time achieves an order-of-magnitude increase in performance compared to previous practical methods. This improvement comes from a combination of a novel theoretically optimal perfect hashing scheme that greatly simplifies previous methods and the fact that our algorithm is designed to make good use of the memory hierarchy. We demonstrate the scalability of our algorithm by considering a set of over one billion URLs from the World Wide Web of average length 64 for which we construct a minimal perfect hash function on a commodity PC in a little more than 1 hour. Our scheme produces minimal perfect hash functions using slightly more than 3 bits per key. For perfect hash functions in the range 0\u20262n-1 the space usage drops to just over 2 bits per key (i.e. one bit more than optimal for representing the key). This is significantly below of what has been achieved previously for very large values of n.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-30.736774444580078, -11.736746788024902]}, {"key": "boytsov2013learning", "year": "2013", "title": "Learning To Prune In Metric And Non-metric Spaces", "abstract": "<p>Our focus is on approximate nearest neighbor retrieval in metric and non-metric spaces. We employ a VP-tree and explore two simple yet effective learning-to prune approaches density estimation through sampling and stretching of the triangle inequality. Both methods are evaluated using data sets with metric (Euclidean) and non-metric (KL-divergence and Itakura-Saito) distance functions. Conditions on spaces where the VP-tree is applicable are discussed. The VP-tree with a learned pruner is compared against the recently proposed state-of-the-art approaches the bbtree the multi-probe locality sensitive hashing (LSH) and permutation methods. Our method was competitive against state-of-the-art methods and in most cases was more efficient for the same rank approximation quality.</p>\n", "tags": ["Independent", "LSH", "NEURIPS"], "tsne_embedding": [-23.673913955688477, 7.472860813140869]}, {"key": "boytsov2019accurate", "year": "2019", "title": "Accurate And Fast Retrieval For Complex Non-metric Data Via Neighborhood Graphs", "abstract": "<p>We demonstrate that a graph-based search algorithm-relying on the construction of an approximate neighborhood graph-can directly work with challenging non-metric and/or non-symmetric distances without resorting to metric-space mapping and/or distance symmetrization which in turn lead to substantial performance degradation. Although the straightforward metrization and symmetrization is usually ineffective we find that constructing an index using a modified e.g. symmetrized distance can improve performance. This observation paves a way to a new line of research of designing index-specific graph-construction distance functions.</p>\n", "tags": ["ARXIV", "Graph"], "tsne_embedding": [-3.5774590969085693, 28.751863479614258]}, {"key": "andoni2006near", "year": "1998", "title": "Min-Wise Independent Permutations", "abstract": "<p>We define and study the notion of min-wise independent families of permutations. Our research was motivated by the fact that such a family (under some relaxations) is essential to the algorithm used in practice by the AltaVista web index software to detect and filter near-duplicate documents. However, in the course of our investigation we have discovered interesting and challenging theoretical questions related to this concept we present the solutions to some of them and we list the rest as open problems.</p>\n", "tags": ["Independent"], "tsne_embedding": [-3.6092753410339355, -17.40520477294922]}, {"key": "bronstein2011kernel", "year": "2011", "title": "Kernel Diff-hash", "abstract": "<p>This paper presents a kernel formulation of the recently introduced diff-hash algorithm for the construction of similarity-sensitive hash functions. Our kernel diff-hash algorithm that shows superior performance on the problem of image feature descriptor matching.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-17.47088050842285, 16.31672477722168]}, {"key": "bronstein2011multimodal", "year": "2011", "title": "Multimodal Diff-hash", "abstract": "<p>Many applications require comparing multimodal data with different structure and dimensionality that cannot be compared directly. Recently there has been increasing interest in methods for learning and efficiently representing such multimodal similarity. In this paper we present a simple algorithm for multimodal similarity-preserving hashing trying to map multimodal data into the Hamming space while preserving the intra- and inter-modal similarities. We show that our method significantly outperforms the state-of-the-art method in the field.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [-17.22629737854004, -1.6169973611831665]}, {"key": "brooks2017multi", "year": "2017", "title": "Multi-level Spherical Locality Sensitive Hashing For Approximate Near Neighbors", "abstract": "<p>This paper introduces Multi-Level Spherical LSH parameter-free a multi-level data-dependant Locality Sensitive Hashing data structure for solving the Approximate Near Neighbors Problem (ANN). This data structure uses a modified version of a multi-probe adaptive querying algorithm with the potential of achieving a (O(n^p + t)) query run time for all inputs n where t &lt;= n.</p>\n", "tags": ["ARXIV", "Independent", "LSH"], "tsne_embedding": [-32.769569396972656, 2.775667667388916]}, {"key": "brunet2023icl", "year": "2023", "title": "ICL Markup Structuring In-context Learning Using Soft-token Tags", "abstract": "<p>Large pretrained language models (LLMs) can be rapidly adapted to a wide variety of tasks via a text-to-text approach where the instruction and input are fed to the model in natural language. Combined with in-context learning (ICL) this paradigm is impressively flexible and powerful. However it also burdens users with an overwhelming number of choices many of them arbitrary. Inspired by markup languages like HTML we contribute a method of using soft-token tags to compose prompt templates. This approach reduces arbitrary decisions and streamlines the application of ICL. Our method is a form of meta-learning for ICL; it learns these tags in advance during a parameter-efficient fine-tuning warm-up process. The tags can subsequently be used in templates for ICL on new unseen tasks without any additional fine-tuning. Our experiments with this approach yield promising initial results improving LLM performance on important enterprise applications such as few-shot and open-world intent detection as well as text classification in news and legal domains.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [28.502599716186523, -9.317644119262695]}, {"key": "bubeck2023sparks", "year": "2023", "title": "Sparks Of Artificial General Intelligence Early Experiments With GPT-4", "abstract": "<p>Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks challenging our understanding of learning and cognition. The latest model developed by OpenAI GPT-4 was trained using an unprecedented scale of compute and data. In this paper we report on our investigation of an early version of GPT-4 when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Googles PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that beyond its mastery of language GPT-4 can solve novel and difficult tasks that span mathematics coding vision medicine law psychology and more without needing any special prompting. Moreover in all of these tasks GPT-4s performance is strikingly close to human-level performance and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4s capabilities we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4 we put special emphasis on discovering its limitations and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [33.025447845458984, -13.169778823852539]}, {"key": "bulatov2023scaling", "year": "2023", "title": "Scaling Transformer To 1M Tokens And Beyond With RMT", "abstract": "<p>A major limitation for the broader scope of problems solvable by transformers is the quadratic scaling of computational complexity with input size. In this study we investigate the recurrent memory augmentation of pre-trained transformer models to extend input context length while linearly scaling compute. Our approach demonstrates the capability to store information in memory for sequences of up to an unprecedented two million tokens while maintaining high retrieval accuracy. Experiments with language modeling tasks show perplexity improvement as the number of processed input segments increases. These results underscore the effectiveness of our method which has significant potential to enhance long-term dependency handling in natural language understanding and generation tasks as well as enable large-scale context processing for memory-intensive applications.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [17.70496368408203, -13.064133644104004]}, {"key": "cai2016revisit", "year": "2016", "title": "A Revisit Of Hashing Algorithms For Approximate Nearest Neighbor Search", "abstract": "<p>Approximate Nearest Neighbor Search (ANNS) is a fundamental problem in many areas of machine learning and data mining. During the past decade numerous hashing algorithms are proposed to solve this problem. Every proposed algorithm claims outperform other state-of-the-art hashing methods. However the evaluation of these hashing papers was not thorough enough and those claims should be re-examined. The ultimate goal of an ANNS method is returning the most accurate answers (nearest neighbors) in the shortest time. If implemented correctly almost all the hashing methods will have their performance improved as the code length increases. However many existing hashing papers only report the performance with the code length shorter than 128. In this paper we carefully revisit the problem of search with a hash index and analyze the pros and cons of two popular hash index search procedures. Then we proposed a very simple but effective two level index structures and make a thorough comparison of eleven popular hashing algorithms. Surprisingly the random-projection-based Locality Sensitive Hashing (LSH) is the best performed algorithm which is in contradiction to the claims in all the other ten hashing papers. Despite the extreme simplicity of random-projection-based LSH our results show that the capability of this algorithm has been far underestimated. For the sake of reproducibility all the codes used in the paper are released on GitHub which can be used as a testing platform for a fair comparison between various hashing algorithms.</p>\n", "tags": ["ARXIV", "Independent", "LSH"], "tsne_embedding": [-31.2960147857666, -10.980913162231445]}, {"key": "cai2017revisit", "year": "2017", "title": "A Revisit On Deep Hashings For Large-scale Content Based Image Retrieval", "abstract": "<p>There is a growing trend in studying deep hashing methods for content-based image retrieval (CBIR) where hash functions and binary codes are learnt using deep convolutional neural networks and then the binary codes can be used to do approximate nearest neighbor (ANN) search. All the existing deep hashing papers report their methods superior performance over the traditional hashing methods according to their experimental results. However there are serious flaws in the evaluations of existing deep hashing papers (1) The datasets they used are too small and simple to simulate the real CBIR situation. (2) They did not correctly include the search time in their evaluation criteria while the search time is crucial in real CBIR systems. (3) The performance of some unsupervised hashing algorithms (e.g. LSH) can easily be boosted if one uses multiple hash tables which is an important factor should be considered in the evaluation while most of the deep hashing papers failed to do so. We re-evaluate several state-of-the-art deep hashing methods with a carefully designed experimental setting. Empirical results reveal that the performance of these deep hashing methods are inferior to multi-table IsoH a very simple unsupervised hashing method. Thus the conclusions in all the deep hashing papers should be carefully re-examined.</p>\n", "tags": ["ARXIV", "Image Retrieval", "LSH", "Unsupervised"], "tsne_embedding": [-8.195541381835938, -6.579252243041992]}, {"key": "cakir2015adaptive", "year": "2015", "title": "Adaptive Hashing for Fast Similarity Search", "abstract": "<p>With the staggering growth in image and video datasets,\nalgorithms that provide fast similarity search and compact\nstorage are crucial. Hashing methods that map the\ndata into Hamming space have shown promise; however,\nmany of these methods employ a batch-learning strategy\nin which the computational cost and memory requirements\nmay become intractable and infeasible with larger and\nlarger datasets. To overcome these challenges, we propose\nan online learning algorithm based on stochastic gradient\ndescent in which the hash functions are updated iteratively\nwith streaming data. In experiments with three image retrieval\nbenchmarks, our online algorithm attains retrieval\naccuracy that is comparable to competing state-of-the-art\nbatch-learning solutions, while our formulation is orders\nof magnitude faster and being online it is adaptable to the\nvariations of the data. Moreover, our formulation yields improved\nretrieval performance over a recently reported online\nhashing technique, Online Kernel Hashing.</p>\n", "tags": ["Has Code", "ICCV", "Streaming Data"], "tsne_embedding": [-6.782936096191406, -20.22683334350586]}, {"key": "cakir2015online", "year": "2015", "title": "Online Supervised Hashing For Ever-growing Datasets", "abstract": "<p>Supervised hashing methods are widely-used for nearest neighbor search in computer vision applications. Most state-of-the-art supervised hashing approaches employ batch-learners. Unfortunately batch-learning strategies can be inefficient when confronted with large training datasets. Moreover with batch-learners it is unclear how to adapt the hash functions as a dataset continues to grow and diversify over time. Yet in many practical scenarios the dataset grows and diversifies; thus both the hash functions and the indexing must swiftly accommodate these changes. To address these issues we propose an online hashing method that is amenable to changes and expansions of the datasets. Since it is an online algorithm our approach offers linear complexity with the dataset size. Our solution is supervised in that we incorporate available label information to preserve the semantic neighborhood. Such an adaptive hashing method is attractive; but it requires recomputing the hash table as the hash functions are updated. If the frequency of update is high then recomputing the hash table entries may cause inefficiencies in the system especially for large indexes. Thus we also propose a framework to reduce hash table updates. We compare our method to state-of-the-art solutions on two benchmarks and demonstrate significant improvements over previous work.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [3.81577205657959, -11.59060287475586]}, {"key": "cakir2017mihash", "year": "2017", "title": "Mihash Online Hashing With Mutual Information", "abstract": "<p>Learning-based hashing methods are widely used for nearest neighbor retrieval and recently online hashing methods have demonstrated good performance-complexity trade-offs by learning hash functions from streaming data. In this paper we first address a key challenge for online hashing the binary codes for indexed data must be recomputed to keep pace with updates to the hash functions. We propose an efficient quality measure for hash functions based on an information-theoretic quantity mutual information and use it successfully as a criterion to eliminate unnecessary hash table updates. Next we also show how to optimize the mutual information objective using stochastic gradient descent. We thus develop a novel hashing method MIHash that can be used in both online and batch settings. Experiments on image retrieval benchmarks (including a 2.5M image dataset) confirm the effectiveness of our formulation both in reducing hash table recomputations and in learning high-quality hash functions.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Independent", "Streaming Data"], "tsne_embedding": [-8.041773796081543, -19.135589599609375]}, {"key": "cakir2017online", "year": "2017", "title": "MIHash: Online Hashing with Mutual Information", "abstract": "<p>Learning-based hashing methods are widely used for\nnearest neighbor retrieval, and recently, online hashing\nmethods have demonstrated good performance-complexity\ntrade-offs by learning hash functions from streaming data.\nIn this paper, we first address a key challenge for online\nhashing: the binary codes for indexed data must be recomputed\nto keep pace with updates to the hash functions.\nWe propose an efficient quality measure for hash functions,\nbased on an information-theoretic quantity, mutual information,\nand use it successfully as a criterion to eliminate\nunnecessary hash table updates. Next, we also show how to\noptimize the mutual information objective using stochastic\ngradient descent. We thus develop a novel hashing method,\nMIHash, that can be used in both online and batch settings.\nExperiments on image retrieval benchmarks (including a\n2.5M image dataset) confirm the effectiveness of our formulation,\nboth in reducing hash table recomputations and\nin learning high-quality hash functions.</p>\n", "tags": ["Has Code", "ICCV", "Streaming Data"], "tsne_embedding": [-8.033082962036133, -19.132287979125977]}, {"key": "cakir2018hashing", "year": "2018", "title": "Hashing With Binary Matrix Pursuit", "abstract": "<p>We propose theoretical and empirical improvements for two-stage hashing methods. We first provide a theoretical analysis on the quality of the binary codes and show that under mild assumptions a residual learning scheme can construct binary codes that fit any neighborhood structure with arbitrary accuracy. Secondly we show that with high-capacity hash functions such as CNNs binary code inference can be greatly simplified for many standard neighborhood definitions yielding smaller optimization problems and more robust codes. Incorporating our findings we propose a novel two-stage hashing method that significantly outperforms previous hashing studies on widely used image retrieval benchmarks.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Independent"], "tsne_embedding": [-2.6170084476470947, 17.50565528869629]}, {"key": "cakir2019hashing", "year": "2019", "title": "Hashing with Mutual Information", "abstract": "<p>Binary vector embeddings enable fast nearest neighbor retrieval in large databases of high-dimensional objects, and play an important role in many practical applications, such as image and video retrieval. We study the problem of learning binary vector embeddings under a supervised setting, also known as hashing. We propose a novel supervised hashing method based on optimizing an information-theoretic quantity: mutual information. We show that optimizing mutual information can reduce ambiguity in the induced neighborhood structure in the learned Hamming space, which is essential in obtaining high retrieval performance. To this end, we optimize mutual information in deep neural networks with minibatch stochastic gradient descent, with a formulation that maximally and efficiently utilizes available supervision. Experiments on four image retrieval benchmarks, including ImageNet, confirm the effectiveness of our method in learning high-quality binary embeddings for nearest neighbor retrieval.</p>\n", "tags": ["Has Code", "Image Retrieval", "Supervised", "TPAMI"], "tsne_embedding": [-13.544037818908691, 16.744260787963867]}, {"key": "cao2016correlation", "year": "2016", "title": "Correlation Hashing Network For Efficient Cross-modal Retrieval", "abstract": "<p>Hashing is widely applied to approximate nearest neighbor search for large-scale multimodal retrieval with storage and computation efficiency. Cross-modal hashing improves the quality of hash coding by exploiting semantic correlations across different modalities. Existing cross-modal hashing methods first transform data into low-dimensional feature vectors and then generate binary codes by another separate quantization step. However suboptimal hash codes may be generated since the quantization error is not explicitly minimized and the feature representation is not jointly optimized with the binary codes. This paper presents a Correlation Hashing Network (CHN) approach to cross-modal hashing which jointly learns good data representation tailored to hash coding and formally controls the quantization error. The proposed CHN is a hybrid deep architecture that constitutes a convolutional neural network for learning good image representations a multilayer perception for learning good text representations two hashing layers for generating compact binary codes and a structured max-margin loss that integrates all things together to enable learning similarity-preserving and high-quality hash codes. Extensive empirical study shows that CHN yields state of the art cross-modal retrieval performance on standard benchmarks.</p>\n", "tags": ["ARXIV", "Cross Modal", "Quantisation", "Supervised"], "tsne_embedding": [-10.475845336914062, 7.218616485595703]}, {"key": "cao2016deep", "year": "2016", "title": "Deep Visual-Semantic Hashing for Cross-Modal Retrieval", "abstract": "<p>Due to the storage and retrieval efficiency, hashing has been\nwidely applied to approximate nearest neighbor search for\nlarge-scale multimedia retrieval. Cross-modal hashing, which\nenables efficient retrieval of images in response to text queries\nor vice versa, has received increasing attention recently. Most\nexisting work on cross-modal hashing does not capture the\nspatial dependency of images and temporal dynamics of text\nsentences for learning powerful feature representations and\ncross-modal embeddings that mitigate the heterogeneity of\ndifferent modalities. This paper presents a new Deep Visual Semantic Hashing (DVSH) model that generates compact\nhash codes of images and sentences in an end-to-end deep\nlearning architecture, which capture the intrinsic cross-modal\ncorrespondences between visual data and natural language.\nDVSH is a hybrid deep architecture that constitutes a visual semantic fusion network for learning joint embedding space\nof images and text sentences, and two modality-specific hashing networks for learning hash functions to generate compact\nbinary codes. Our architecture effectively unifies joint multimodal embedding and cross-modal hashing, which is based\non a novel combination of Convolutional Neural Networks\nover images, Recurrent Neural Networks over sentences, and\na structured max-margin objective that integrates all things\ntogether to enable learning of similarity-preserving and highquality hash codes. Extensive empirical evidence shows that\nour DVSH approach yields state of the art results in crossmodal retrieval experiments on image-sentences datasets,\ni.e. standard IAPR TC-12 and large-scale Microsoft COCO.</p>\n", "tags": ["Cross Modal", "Deep Learning", "KDD"], "tsne_embedding": [-11.059606552124023, 7.2361860275268555]}, {"key": "cao2016transitive", "year": "2016", "title": "Transitive Hashing Network For Heterogeneous Multimedia Retrieval", "abstract": "<p>Hashing has been widely applied to large-scale multimedia retrieval due to the storage and retrieval efficiency. Cross-modal hashing enables efficient retrieval from database of one modality in response to a query of another modality. Existing work on cross-modal hashing assumes heterogeneous relationship across modalities for hash function learning. In this paper we relax the strong assumption by only requiring such heterogeneous relationship in an auxiliary dataset different from the query/database domain. We craft a hybrid deep architecture to simultaneously learn the cross-modal correlation from the auxiliary dataset and align the dataset distributions between the auxiliary dataset and the query/database domain which generates transitive hash codes for heterogeneous multimedia retrieval. Extensive experiments exhibit that the proposed approach yields state of the art multimedia retrieval performance on public datasets i.e. NUS-WIDE ImageNet-YahooQA.</p>\n", "tags": ["ARXIV", "Cross Modal", "Independent"], "tsne_embedding": [-15.854731559753418, -4.2204084396362305]}, {"key": "cao2017collective", "year": "2017", "title": "Collective Deep Quantization for Efficient Cross-Modal Retrieval", "abstract": "<p>Cross-modal similarity retrieval is a problem about designing a retrieval system that supports querying across\ncontent modalities, e.g., using an image to retrieve for\ntexts. This paper presents a compact coding solution for\nefficient cross-modal retrieval, with a focus on the quantization approach which has already shown the superior\nperformance over the hashing solutions in single-modal\nsimilarity retrieval. We propose a collective deep quantization (CDQ) approach, which is the first attempt to\nintroduce quantization in end-to-end deep architecture\nfor cross-modal retrieval. The major contribution lies in\njointly learning deep representations and the quantizers\nfor both modalities using carefully-crafted hybrid networks and well-specified loss functions. In addition, our\napproach simultaneously learns the common quantizer\ncodebook for both modalities through which the crossmodal correlation can be substantially enhanced. CDQ\nenables efficient and effective cross-modal retrieval using inner product distance computed based on the common codebook with fast distance table lookup. Extensive experiments show that CDQ yields state of the art\ncross-modal retrieval results on standard benchmarks.</p>\n", "tags": ["AAAI", "Cross Modal", "Deep Learning", "Quantisation"], "tsne_embedding": [-12.586088180541992, 5.120393753051758]}, {"key": "cao2017correlation", "year": "2017", "title": "Correlation Autoencoder Hashing for Supervised Cross-Modal Search", "abstract": "<p>Hashing is widely applied to approximate nearest neighbor search for large-scale multimodal retrieval with storage and computation efficiency. Cross-modal hashing improves the quality of hash coding by exploiting semantic correlations across different modalities. Existing cross-modal hashing methods first transform data into low-dimensional feature vectors, and then generate binary codes by another separate quantization step. However, suboptimal hash codes may be generated since the quantization error is not explicitly minimized and the feature representation is not jointly optimized with the binary codes.\nThis paper presents a Correlation Hashing Network (CHN) approach to cross-modal hashing, which jointly learns good data representation tailored to hash coding and formally controls the quantization error. The proposed CHN is a hybrid deep architecture that constitutes a convolutional neural network for learning good image representations, a multilayer perception for learning good text representations, two hashing layers for generating compact binary codes, and a structured max-margin loss that integrates all things together to enable learning similarity-preserving and high-quality hash codes. Extensive empirical study shows that CHN yields state of the art cross-modal retrieval performance on standard benchmarks.</p>\n", "tags": ["BMVC", "Cross Modal", "Deep Learning", "Supervised"], "tsne_embedding": [-10.546701431274414, 6.930015563964844]}, {"key": "cao2017hashnet", "year": "2017", "title": "HashNet: Deep Learning to Hash by Continuation", "abstract": "<p>Learning to hash has been widely applied to approximate nearest neighbor search for large-scale multimedia retrieval, due to its computation efficiency and retrieval quality. Deep learning to hash, which improves retrieval quality\nby end-to-end representation learning and hash encoding,\nhas received increasing attention recently. Subject to the illposed gradient difficulty in the optimization with sign activations, existing deep learning to hash methods need to first\nlearn continuous representations and then generate binary\nhash codes in a separated binarization step, which suffer\nfrom substantial loss of retrieval quality.  This work presents\nHashNet, a novel deep architecture for deep learning to\nhash by continuation method with convergence guarantees,\nwhich learns exactly binary hash codes from imbalanced\nsimilarity data. The key idea is to attack the ill-posed gradient problem in optimizing deep networks with non-smooth\nbinary activations by continuation method, in which we begin from learning an easier network with smoothed activation function and let it evolve during the training, until it\neventually goes back to being the original, difficult to optimize, deep network with the sign activation function. Comprehensive empirical evidence shows that HashNet can generate exactly binary hash codes and yield state-of-the-art\nmultimedia retrieval performance on standard benchmarks.</p>\n", "tags": ["CVPR", "Deep Learning", "Has Code"], "tsne_embedding": [-2.9068241119384766, -3.9190735816955566]}, {"key": "cao2017transfer", "year": "2017", "title": "Transfer Adversarial Hashing For Hamming Space Retrieval", "abstract": "<p>Hashing is widely applied to large-scale image retrieval due to the storage and retrieval efficiency. Existing work on deep hashing assumes that the database in the target domain is identically distributed with the training set in the source domain. This paper relaxes this assumption to a transfer retrieval setting which allows the database and the training set to come from different but relevant domains. However the transfer retrieval setting will introduce two technical difficulties first the hash model trained on the source domain cannot work well on the target domain due to the large distribution gap; second the domain gap makes it difficult to concentrate the database points to be within a small Hamming ball. As a consequence transfer retrieval performance within Hamming Radius 2 degrades significantly in existing hashing methods. This paper presents Transfer Adversarial Hashing (TAH) a new hybrid deep architecture that incorporates a pairwise t-distribution cross-entropy loss to learn concentrated hash codes and an adversarial network to align the data distributions between the source and target domains. TAH can generate compact transfer hash codes for efficient image retrieval on both source and target domains. Comprehensive experiments validate that TAH yields state of the art Hamming space retrieval performance on standard datasets.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Independent"], "tsne_embedding": [-13.873002052307129, 6.719642639160156]}, {"key": "cao2018cauchy", "year": "2018", "title": "Deep Cauchy Hashing for Hamming Space Retrieval", "abstract": "<p>Due to its computation efficiency and retrieval quality,\nhashing has been widely applied to approximate nearest\nneighbor search for large-scale image retrieval, while deep\nhashing further improves the retrieval quality by end-toend representation learning and hash coding. With compact\nhash codes, Hamming space retrieval enables the most efficient constant-time search that returns data points within a\ngiven Hamming radius to each query, by hash table lookups\ninstead of linear scan. However, subject to the weak capability of concentrating relevant images to be within a small\nHamming ball due to mis-specified loss functions, existing deep hashing methods may underperform for Hamming\nspace retrieval.  This work presents Deep Cauchy Hashing\n(DCH), a novel deep hashing model that generates compact\nand concentrated binary hash codes to enable efficient and\neffective Hamming space retrieval. The main idea is to design a pairwise cross-entropy loss based on Cauchy distribution, which penalizes significantly on similar image pairs\nwith Hamming distance larger than the given Hamming radius threshold. Comprehensive experiments demonstrate\nthat DCH can generate highly concentrated hash codes and\nyield state-of-the-art Hamming space retrieval performance\non three datasets, NUS-WIDE, CIFAR-10, and MS-COCO.</p>\n", "tags": ["CVPR", "Deep Learning", "Has Code", "Image Retrieval"], "tsne_embedding": [-13.11340045928955, 7.619623184204102]}, {"key": "cao2018deep", "year": "2018", "title": "Deep Priority Hashing", "abstract": "<p>Deep hashing enables image retrieval by end-to-end learning of deep representations and hash codes from training data with pairwise similarity information. Subject to the distribution skewness underlying the similarity information most existing deep hashing methods may underperform for imbalanced data due to misspecified loss functions. This paper presents Deep Priority Hashing (DPH) an end-to-end architecture that generates compact and balanced hash codes in a Bayesian learning framework. The main idea is to reshape the standard cross-entropy loss for similarity-preserving learning such that it down-weighs the loss associated to highly-confident pairs. This idea leads to a novel priority cross-entropy loss which prioritizes the training on uncertain pairs over confident pairs. Also we propose another priority quantization loss which prioritizes hard-to-quantize examples for generation of nearly lossless hash codes. Extensive experiments demonstrate that DPH can generate high-quality hash codes and yield state-of-the-art image retrieval results on three datasets ImageNet NUS-WIDE and MS-COCO.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Quantisation", "Supervised"], "tsne_embedding": [-6.844951152801514, 9.448456764221191]}, {"key": "cao2018hashgan", "year": "2018", "title": "HashGAN: Deep Learning to Hash with Pair Conditional Wasserstein GAN", "abstract": "<p>Deep learning to hash improves image retrieval performance by end-to-end representation learning and hash coding from training data with pairwise similarity information.\nSubject to the scarcity of similarity information that is often\nexpensive to collect for many application domains, existing\ndeep learning to hash methods may overfit the training data\nand result in substantial loss of retrieval quality. This paper\npresents HashGAN, a novel architecture for deep learning\nto hash, which learns compact binary hash codes from both\nreal images and diverse images synthesized by generative\nmodels. The main idea is to augment the training data with\nnearly real images synthesized from a new Pair Conditional\nWasserstein GAN (PC-WGAN) conditioned on the pairwise\nsimilarity information. Extensive experiments demonstrate\nthat HashGAN can generate high-quality binary hash codes\nand yield state-of-the-art image retrieval performance on\nthree benchmarks, NUS-WIDE, CIFAR-10, and MS-COCO.</p>\n", "tags": ["CVPR", "Deep Learning", "GAN", "Image Retrieval"], "tsne_embedding": [-6.7898359298706055, 9.615543365478516]}, {"key": "cao2020learning", "year": "2020", "title": "Learning to Hash with a Dimension Analysis-based Quantizer for Image Retrieval", "abstract": "<p>The last few years have witnessed the rise of the big data era in which approximate nearest neighbor search is a fundamental problem in many applications, such as large-scale image retrieval. Recently, many research results have demonstrated that hashing can achieve promising performance due to its appealing storage and search efficiency. Since complex optimization problems for loss functions are difficult to solve, most hashing methods decompose the hash code learning problem into two steps: projection and quantization. In the quantization step, binary codes are widely used because ranking them by the Hamming distance is very efficient. However, the massive information loss produced by the quantization step should be reduced in applications where high search accuracy is required, such as in image retrieval. Since many two-step hashing methods produce uneven projected dimensions in the projection step, in this paper, we propose a novel dimension analysis-based quantization (DAQ) on two-step hashing methods for image retrieval. We first perform an importance analysis of the projected dimensions and select a subset of them that are more informative than others, and then we divide the selected projected dimensions into several regions with our quantizer. Every region is quantized with its corresponding codebook. Finally, the similarity between two hash codes is estimated by the Manhattan distance between their corresponding codebooks, which is also efficient. We conduct experiments on three public benchmarks containing up to one million descriptors and show that the proposed DAQ method consistently leads to significant accuracy improvements over state-of-the-art quantization methods.</p>\n", "tags": ["Image Retrieval", "Quantisation"], "tsne_embedding": [-15.960294723510742, 9.953458786010742]}, {"key": "carraro2024enhancing", "year": "2024", "title": "Enhancing Recommendation Diversity By Re-ranking With Large Language Models", "abstract": "<p>It has long been recognized that it is not enough for a Recommender System (RS) to provide recommendations based only on their relevance to users. Among many other criteria the set of recommendations may need to be diverse. Diversity is one way of handling recommendation uncertainty and ensuring that recommendations offer users a meaningful choice. The literature reports many ways of measuring diversity and improving the diversity of a set of recommendations most notably by re-ranking and selecting from a larger set of candidate recommendations. Driven by promising insights from the literature on how to incorporate versatile Large Language Models (LLMs) into the RS pipeline in this paper we show how LLMs can be used for diversity re-ranking. We begin with an informal study that verifies that LLMs can be used for re-ranking tasks and do have some understanding of the concept of item diversity. Then we design a more rigorous methodology where LLMs are prompted to generate a diverse ranking from a candidate ranking using various prompt templates with different re-ranking instructions in a zero-shot fashion. We conduct comprehensive experiments testing state-of-the-art LLMs from the GPT and Llama families. We compare their re-ranking capabilities with random re-ranking and various traditional re-ranking methods from the literature. We open-source the code of our experiments for reproducibility. Our findings suggest that the trade-offs (in terms of performance and costs among others) of LLM-based re-rankers are superior to those of random re-rankers but as yet inferior to the ones of traditional re-rankers. However the LLM approach is promising. LLMs exhibit improved performance on many natural language processing and recommendation tasks and lower inference costs. Given these trends we can expect LLM-based re-ranking to become more competitive soon.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [42.898345947265625, -8.779458045959473]}, {"key": "carreira2015hashing", "year": "2015", "title": "Hashing with Binary Autoencoders", "abstract": "<p>An attractive approach for fast search in image\ndatabases is binary hashing, where each high-dimensional,\nreal-valued image is mapped onto a low-dimensional, binary\nvector and the search is done in this binary space.\nFinding the optimal hash function is difficult because it involves\nbinary constraints, and most approaches approximate\nthe optimization by relaxing the constraints and then\nbinarizing the result. Here, we focus on the binary autoencoder\nmodel, which seeks to reconstruct an image from the\nbinary code produced by the hash function. We show that\nthe optimization can be simplified with the method of auxiliary\ncoordinates. This reformulates the optimization as\nalternating two easier steps: one that learns the encoder\nand decoder separately, and one that optimizes the code for\neach image. Image retrieval experiments show the resulting\nhash function outperforms or is competitive with state-ofthe-art\nmethods for binary hashing.</p>\n", "tags": ["CVPR", "Deep Learning", "Has Code"], "tsne_embedding": [-18.573549270629883, 13.613184928894043]}, {"key": "carreiraperpi\u00f1\u00e1n2015hashing", "year": "2015", "title": "Hashing With Binary Autoencoders", "abstract": "<p>An attractive approach for fast search in image databases is binary hashing where each high-dimensional real-valued image is mapped onto a low-dimensional binary vector and the search is done in this binary space. Finding the optimal hash function is difficult because it involves binary constraints and most approaches approximate the optimization by relaxing the constraints and then binarizing the result. Here we focus on the binary autoencoder model which seeks to reconstruct an image from the binary code produced by the hash function. We show that the optimization can be simplified with the method of auxiliary coordinates. This reformulates the optimization as alternating two easier steps one that learns the encoder and decoder separately and one that optimizes the code for each image. Image retrieval experiments using precision/recall and a measure of code utilization show the resulting hash function outperforms or is competitive with state-of-the-art methods for binary hashing.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Unsupervised"], "tsne_embedding": [-18.57624053955078, 13.634110450744629]}, {"key": "carreiraperpi\u00f1\u00e1n2016ensemble", "year": "2016", "title": "An Ensemble Diversity Approach To Supervised Binary Hashing", "abstract": "<p>Binary hashing is a well-known approach for fast approximate nearest-neighbor search in information retrieval. Much work has focused on affinity-based objective functions involving the hash functions or binary codes. These objective functions encode neighborhood information between data points and are often inspired by manifold learning algorithms. They ensure that the hash functions differ from each other through constraints or penalty terms that encourage codes to be orthogonal or dissimilar across bits but this couples the binary variables and complicates the already difficult optimization. We propose a much simpler approach we train each hash function (or bit) independently from each other but introduce diversity among them using techniques from classifier ensembles. Surprisingly we find that not only is this faster and trivially parallelizable but it also improves over the more complex coupled objective function and achieves state-of-the-art precision and recall in experiments with image retrieval.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Supervised"], "tsne_embedding": [-19.498741149902344, 11.405513763427734]}, {"key": "cayton2007learning", "year": "2007", "title": "A Learning Framework For Nearest Neighbor Search", "abstract": "<p>Can we leverage learning techniques to build a fast nearest-neighbor (NN) retrieval data structure We present a general learning framework for the NN problem in which sample queries are used to learn the parameters of a data structure that minimize the retrieval time and/or the miss rate. We explore the potential of this novel framework through two popular NN data structures KD-trees and the rectilinear structures employed by locality sensitive hashing. We derive a generalization theory for these data structure classes and present simple learning algorithms for both. Experimental results reveal that learning often improves on the already strong performance of these data structures.</p>\n", "tags": ["NEURIPS"], "tsne_embedding": [11.092041969299316, 0.41201508045196533]}, {"key": "ceccarello2018fresh", "year": "2018", "title": "FRESH Frechet Similarity With Hashing", "abstract": "<p>This paper studies the (r)-range search problem for curves under the continuous Frechet distance given a dataset (S) of (n) polygonal curves and a threshold (r0) construct a data structure that for any query curve (q) efficiently returns all entries in (S) with distance at most (r) from (q). We propose FRESH an approximate and randomized approach for (r)-range search that leverages on a locality sensitive hashing scheme for detecting candidate near neighbors of the query curve and on a subsequent pruning step based on a cascade of curve simplifications. We experimentally compare fresh to exact and deterministic solutions and we show that high performance can be reached by suitably relaxing precision and recall.</p>\n", "tags": ["Independent"], "tsne_embedding": [-25.604326248168945, 1.7913212776184082]}, {"key": "cerra2012fast", "year": "2012", "title": "A Fast Compression-based Similarity Measure With Applications To Content-based Image Retrieval", "abstract": "<p>Compression-based similarity measures are effectively employed in applications on diverse data types with a basically parameter-free approach. Nevertheless there are problems in applying these techniques to medium-to-large datasets which have been seldom addressed. This paper proposes a similarity measure based on compression with dictionaries the Fast Compression Distance (FCD) which reduces the complexity of these methods without degradations in performance. On its basis a content-based color image retrieval system is defined which can be compared to state-of-the-art methods based on invariant color features. Through the FCD a better understanding of compression-based techniques is achieved by performing experiments on datasets which are larger than the ones analyzed so far in literature.</p>\n", "tags": ["Image Retrieval"], "tsne_embedding": [-24.054790496826172, 19.17247772216797]}, {"key": "chadha2016voronoi", "year": "2016", "title": "Voronoi-based Compact Image Descriptors Efficient Region-of-interest Retrieval With VLAD And Deep-learning-based Descriptors", "abstract": "<p>We investigate the problem of image retrieval based on visual queries when the latter comprise arbitrary regions-of-interest (ROI) rather than entire images. Our proposal is a compact image descriptor that combines the state-of-the-art in content-based descriptor extraction with a multi-level Voronoi-based spatial partitioning of each dataset image. The proposed multi-level Voronoi-based encoding uses a spatial hierarchical K-means over interest-point locations and computes a content-based descriptor over each cell. In order to reduce the matching complexity with minimal or no sacrifice in retrieval performance (i) we utilize the tree structure of the spatial hierarchical K-means to perform a top-to-bottom pruning for local similarity maxima; (ii) we propose a new image similarity score that combines relevant information from all partition levels into a single measure for similarity; (iii) we combine our proposal with a novel and efficient approach for optimal bit allocation within quantized descriptor representations. By deriving both a Voronoi-based VLAD descriptor (termed as Fast-VVLAD) and a Voronoi-based deep convolutional neural network (CNN) descriptor (termed as Fast-VDCNN) we demonstrate that our Voronoi-based framework is agnostic to the descriptor basis and can easily be slotted into existing frameworks. Via a range of ROI queries in two standard datasets it is shown that the Voronoi-based descriptors achieve comparable or higher mean Average Precision against conventional grid-based spatial search while offering more than two-fold reduction in complexity. Finally beyond ROI queries we show that Voronoi partitioning improves the geometric invariance of compact CNN descriptors thereby resulting in competitive performance to the current state-of-the-art on whole image retrieval.</p>\n", "tags": ["ARXIV", "CNN", "Deep Learning", "Image Retrieval", "Supervised"], "tsne_embedding": [-10.246370315551758, 15.768138885498047]}, {"key": "chaidaroon2017variational", "year": "2017", "title": "Variational Deep Semantic Hashing for Text Documents", "abstract": "<p>As the amount of textual data has been rapidly increasing over\nthe past decade, efficient similarity search methods have become\na crucial component of large-scale information retrieval systems.\nA popular strategy is to represent original data samples by compact binary codes through hashing. A spectrum of machine learning methods have been utilized, but they often lack expressiveness\nand flexibility in modeling to learn effective representations. The\nrecent advances of deep learning in a wide range of applications\nhas demonstrated its capability to learn robust and powerful feature representations for complex data. Especially, deep generative\nmodels naturally combine the expressiveness of probabilistic generative models with the high capacity of deep neural networks,\nwhich is very suitable for text modeling. However, little work has\nleveraged the recent progress in deep learning for text hashing. In this paper, we propose a series of novel deep document generative models for text hashing. The first proposed model is unsupervised while the second one is supervised by utilizing document labels/tags for hashing. The third model further considers document-specific factors that affect the generation of words. The probabilistic generative formulation of the proposed models provides a principled framework for model extension, uncertainty estimation, simulation, and interpretability. Based on variational inference and reparameterization, the proposed models can be interpreted as encoder-decoder deep neural networks and thus they are capable of learning complex nonlinear distributed representations of the original documents. We conduct a comprehensive set of experiments on four public testbeds. The experimental results have demonstrated the effectiveness of the proposed supervised learning models for text hashing.</p>\n", "tags": ["Deep Learning", "SIGIR", "Supervised", "Text Retrieval"], "tsne_embedding": [11.658304214477539, -4.968383312225342]}, {"key": "chaidaroon2019deep", "year": "2019", "title": "Deep Semantic Text Hashing with Weak Supervision", "abstract": "<p>With an ever increasing amount of data available on the web, fast similarity search has become the critical component for large-scale information retrieval systems. One solution is semantic hashing which designs binary codes to accelerate similarity search. Recently, deep learning has been successfully applied to the semantic hashing problem and produces high-quality compact binary codes compared to traditional methods. However, most state-of-the-art semantic hashing approaches require large amounts of hand-labeled training data which are often expensive and time consuming to collect. The cost of getting labeled data is the key bottleneck in deploying these hashing methods. Motivated by the recent success in machine learning that makes use of weak supervision, we employ unsupervised ranking methods such as BM25 to extract weak signals from training data. We further introduce two deep generative semantic hashing models to leverage weak signals for text hashing. The experimental results on four public datasets show that our models can generate high-quality binary codes without using hand-labeled training data and significantly outperform the competitive unsupervised semantic hashing baselines.</p>\n", "tags": ["Deep Learning", "SIGIR", "Supervised", "Weakly Supervised"], "tsne_embedding": [3.4755191802978516, -6.580582618713379]}, {"key": "chakrabarti2020efficient", "year": "2020", "title": "Efficient Image Retrieval Using Multi Neural Hash Codes And Bloom Filters", "abstract": "<p>This paper aims to deliver an efficient and modified approach for image retrieval using multiple neural hash codes and limiting the number of queries using bloom filters by identifying false positives beforehand. Traditional approaches involving neural networks for image retrieval tasks tend to use higher layers for feature extraction. But it has been seen that the activations of lower layers have proven to be more effective in a number of scenarios. In our approach we have leveraged the use of local deep convolutional neural networks which combines the powers of both the features of lower and higher layers for creating feature maps which are then compressed using PCA and fed to a bloom filter after binary sequencing using a modified multi k-means approach. The feature maps obtained are further used in the image retrieval process in a hierarchical coarse-to-fine manner by first comparing the images in the higher layers for semantically similar images and then gradually moving towards the lower layers searching for structural similarities. While searching the neural hashes for the query image are again calculated and queried in the bloom filter which tells us whether the query image is absent in the set or maybe present. If the bloom filter doesnt necessarily rule out the query then it goes into the image retrieval process. This approach can be particularly helpful in cases where the image store is distributed since the approach supports parallel querying.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Independent"], "tsne_embedding": [-10.593658447265625, 17.0444393157959]}, {"key": "chakrabarty2022help", "year": "2022", "title": "Help Me Write A Poem Instruction Tuning As A Vehicle For Collaborative Poetry Writing", "abstract": "<p>Recent work in training large language models (LLMs) to follow natural language instructions has opened up exciting opportunities for natural language interface design. Building on the prior success of LLMs in the realm of computer-assisted creativity we aim to study if LLMs can improve the quality of user-generated content through collaboration. We present CoPoet a collaborative poetry writing system. In contrast to auto-completing a users text CoPoet is controlled by user instructions that specify the attributes of the desired text such as Write a sentence about love or Write a sentence ending in fly. The core component of our system is a language model fine-tuned on a diverse collection of instructions for poetry writing. Our model is not only competitive with publicly available LLMs trained on instructions (InstructGPT) but is also capable of satisfying unseen compositional instructions. A study with 15 qualified crowdworkers shows that users successfully write poems with CoPoet on diverse topics ranging from Monarchy to Climate change. Further the collaboratively written poems are preferred by third-party evaluators over those written without the system.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [29.991436004638672, -9.76521110534668]}, {"key": "charikar2018hashing", "year": "2018", "title": "Hashing-based-estimators For Kernel Density In High Dimensions", "abstract": "<p>Given a set of points P(subset) (mathbbR)^d and a kernel k the Kernel Density Estimate at a point x(in)(mathbbR)^d is defined as (mathrmKDE)_P(x)=(frac1)P(sum)_y(in) P k(xy). We study the problem of designing a data structure that given a data set P and a kernel function returns approximations to the kernel density of a query point in sublinear time. We introduce a class of unbiased estimators for kernel density implemented through locality-sensitive hashing and give general theorems bounding the variance of such estimators. These estimators give rise to efficient data structures for estimating the kernel density in high dimensions for a variety of commonly used kernels. Our work is the first to provide data-structures with theoretical guarantees that improve upon simple random sampling in high dimensions.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-36.71418762207031, -2.6789262294769287]}, {"key": "charikar2018multi", "year": "2018", "title": "Multi-resolution Hashing For Fast Pairwise Summations", "abstract": "<p>A basic computational primitive in the analysis of massive datasets is summing simple functions over a large number of objects. Modern applications pose an additional challenge in that such functions often depend on a parameter vector y (query) that is unknown a priori. Given a set of points X(subset) (mathbbR)^d and a pairwise function w(mathbbR)^d(times) (mathbbR)^d(to) 01 we study the problem of designing a data-structure that enables sublinear-time approximation of the summation Z_w(y)=(frac1)X(sum)_x(in) Xw(xy) for any query y(in) (mathbbR)^d. By combining ideas from Harmonic Analysis (partitions of unity and approximation theory) with Hashing-Based-Estimators Charikar Siminelakis FOCS17 we provide a general framework for designing such data structures through hashing that reaches far beyond what previous techniques allowed. A key design principle is a collection of T(geq) 1 hashing schemes with collision probabilities p_1(ldots) p_T such that (sup)_t(in) Tp_t(xy) = (Theta)((sqrt)w(xy)). This leads to a data-structure that approximates Z_w(y) using a sub-linear number of samples from each hash family. Using this new framework along with Distance Sensitive Hashing Aumuller Christiani Pagh Silvestri PODS18 we show that such a collection can be constructed and evaluated efficiently for any log-convex function w(xy)=e^(phi)((langle) xy(rangle)) of the inner product on the unit sphere xy(in) (mathcalS)^d-1. Our method leads to data structures with sub-linear query time that significantly improve upon random sampling and can be used for Kernel Density or Partition Function Estimation. We provide extensions of our result from the sphere to (mathbbR)^d and from scalar functions to vector functions.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-36.53953552246094, -2.8575644493103027]}, {"key": "chee2023quip", "year": "2023", "title": "Quip 2-bit Quantization Of Large Language Models With Guarantees", "abstract": "<p>This work studies post-training parameter quantization in large language models (LLMs). We introduce quantization with incoherence processing (QuIP) a new method based on the insight that quantization benefits from () weight and Hessian matrices i.e. from the weights being even in magnitude and the directions in which it is important to round them accurately being unaligned with the coordinate axes. QuIP consists of two steps (1) an adaptive rounding procedure minimizing a quadratic proxy objective; (2) efficient pre- and post-processing that ensures weight and Hessian incoherence via multiplication by random orthogonal matrices. We complement QuIP with the first theoretical analysis for an LLM-scale quantization algorithm and show that our theory also applies to an existing method OPTQ. Empirically we find that our incoherence preprocessing improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight. Our code can be found at https://github.com/Cornell-RelaxML/QuIP.</p>\n", "tags": ["ARXIV", "Has Code", "Independent", "Quantisation"], "tsne_embedding": [-0.7653855681419373, 12.96207332611084]}, {"key": "chen2015compressing", "year": "2015", "title": "Compressing Neural Networks With The Hashing Trick", "abstract": "<p>As deep nets are increasingly used in applications suited for mobile devices a fundamental dilemma becomes apparent the trend in deep learning is to grow models to absorb ever-increasing data set sizes; however mobile devices are designed with very little memory and cannot store such large models. We present a novel network architecture HashedNets that exploits inherent redundancy in neural networks to achieve drastic reductions in model sizes. HashedNets uses a low-cost hash function to randomly group connection weights into hash buckets and all connections within the same hash bucket share a single parameter value. These parameters are tuned to adjust to the HashedNets weight sharing architecture with standard backprop during training. Our hashing procedure introduces no additional memory overhead and we demonstrate on several benchmark data sets that HashedNets shrink the storage requirements of neural networks substantially while mostly preserving generalization performance.</p>\n", "tags": ["ARXIV", "Deep Learning", "Supervised"], "tsne_embedding": [7.6165771484375, -10.093836784362793]}, {"key": "chen2016revisiting", "year": "2016", "title": "Revisiting Winner Take All (WTA) Hashing For Sparse Datasets", "abstract": "<p>WTA (Winner Take All) hashing has been successfully applied in many large scale vision applications. This hashing scheme was tailored to take advantage of the comparative reasoning (or order based information) which showed significant accuracy improvements. In this paper we identify a subtle issue with WTA which grows with the sparsity of the datasets. This issue limits the discriminative power of WTA. We then propose a solution for this problem based on the idea of Densification which provably fixes the issue. Our experiments show that Densified WTA Hashing outperforms Vanilla WTA both in image classification and retrieval tasks consistently and significantly.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [-18.549272537231445, -10.839576721191406]}, {"key": "chen2018deep", "year": "2018", "title": "Deep Hashing via Discrepancy Minimization", "abstract": "<p>This paper presents a discrepancy minimizing model to\naddress the discrete optimization problem in hashing learning. The discrete optimization introduced by binary constraint is an NP-hard mixed integer programming problem.\nIt is usually addressed by relaxing the binary variables into\ncontinuous variables to adapt to the gradient based learning of hashing functions, especially the training of deep\nneural networks. To deal with the objective discrepancy\ncaused by relaxation, we transform the original binary optimization into differentiable optimization problem over hash\nfunctions through series expansion. This transformation decouples the binary constraint and the similarity preserving\nhashing function optimization. The transformed objective\nis optimized in a tractable alternating optimization framework with gradual discrepancy minimization. Extensive experimental results on three benchmark datasets validate the\nefficacy of the proposed discrepancy minimizing hashing.</p>\n", "tags": ["CVPR", "Deep Learning"], "tsne_embedding": [-1.7150934934616089, -2.408841848373413]}, {"key": "chen2019deep", "year": "2019", "title": "Deep Supervised Hashing With Anchor Graph", "abstract": "<p>Recently, a series of deep supervised hashing methods were proposed for binary code learning. However, due to the high computation cost and the limited hardware\u2019s memory, these methods will first select a subset from the training set, and then form a mini-batch data to update the network in each iteration. Therefore, the remaining labeled data cannot be fully utilized and the model cannot directly obtain the binary codes of the entire training set for retrieval. To address these problems, this paper proposes an interesting regularized deep model to seamlessly integrate the advantages of deep hashing and efficient binary code learning by using the anchor graph. As such, the deep features and label matrix can be jointly used to optimize the binary codes, and the network can obtain more discriminative feedback from the linear combinations of the learned bits. Moreover, we also reveal the algorithm mechanism and its computation essence. Experiments on three large-scale datasets indicate that the proposed method achieves better retrieval performance with less training time compared to previous deep hashing methods.</p>\n", "tags": ["Deep Learning", "ICCV", "Supervised"], "tsne_embedding": [3.6339590549468994, -8.503144264221191]}, {"key": "chen2019hadamard", "year": "2019", "title": "Hadamard Codebook Based Deep Hashing", "abstract": "<p>As an approximate nearest neighbor search technique hashing has been widely applied in large-scale image retrieval due to its excellent efficiency. Most supervised deep hashing methods have similar loss designs with embedding learning while quantizing the continuous high-dim feature into compact binary space. We argue that the existing deep hashing schemes are defective in two issues that seriously affect the performance i.e. bit independence and bit balance. The former refers to hash codes of different classes should be independent of each other while the latter means each bit should have a balanced distribution of +1s and -1s. In this paper we propose a novel supervised deep hashing method termed Hadamard Codebook based Deep Hashing (HCDH) which solves the above two problems in a unified formulation. Specifically we utilize an off-the-shelf algorithm to generate a binary Hadamard codebook to satisfy the requirement of bit independence and bit balance which subsequently serves as the desired outputs of the hash functions learning. We also introduce a projection matrix to solve the inconsistency between the order of Hadamard matrix and the number of classes. Besides the proposed HCDH further exploits the supervised labels by constructing a classifier on top of the outputs of hash functions. Extensive experiments demonstrate that HCDH can yield discriminative and balanced binary codes which well outperforms many state-of-the-arts on three widely-used benchmarks.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Supervised"], "tsne_embedding": [-7.528495788574219, 2.1053884029388428]}, {"key": "chen2019locality", "year": "2019", "title": "Locality-sensitive Hashing For F-divergences Mutual Information Loss And Beyond", "abstract": "<p>Computing approximate nearest neighbors in high dimensional spaces is a central problem in large-scale data mining with a wide range of applications in machine learning and data science. A popular and effective technique in computing nearest neighbors approximately is the locality-sensitive hashing (LSH) scheme. In this paper we aim to develop LSH schemes for distance functions that measure the distance between two probability distributions particularly for f-divergences as well as a generalization to capture mutual information loss. First we provide a general framework to design LHS schemes for f-divergence distance functions and develop LSH schemes for the generalized Jensen-Shannon divergence and triangular discrimination in this framework. We show a two-sided approximation result for approximation of the generalized Jensen-Shannon divergence by the Hellinger distance which may be of independent interest. Next we show a general method of reducing the problem of designing an LSH scheme for a Krein kernel (which can be expressed as the difference of two positive definite kernels) to the problem of maximum inner product search. We exemplify this method by applying it to the mutual information loss due to its several important applications such as model compression.</p>\n", "tags": ["Independent", "LSH", "NEURIPS"], "tsne_embedding": [-32.803043365478516, 4.654629707336426]}, {"key": "chen2019twostep", "year": "2019", "title": "A Two-step Cross-modal Hashing by Exploiting Label Correlations and Preserving Similarity in Both Steps", "abstract": "<p>In this paper, we present a novel Two-stEp Cross-modal Hashing method, TECH for short, for cross-modal retrieval tasks. As a two-step method, it first learns hash codes based on semantic labels, while preserving the similarity in the original space and exploiting the label correlations in the label space. In the light of this, it is able to make better use of label information and generate better binary codes. In addition, different from other two-step methods that mainly focus on the hash codes learning, TECH adopts a new hash function learning strategy in the second step, which also preserves the similarity in the original space. Moreover, with the help of well designed objective function and optimization scheme, it is able to generate hash codes discretely and scalable for large scale data. To the best of our knowledge, it is the first cross-modal hashing method exploiting label correlations, and also the first two-step hashing model preserving the similarity while leaning hash function. Extensive experiments demonstrate that the proposed approach outperforms some state-of-the-art cross-modal hashing methods.</p>\n", "tags": ["Cross Modal", "Deep Learning", "Image Retrieval", "MM"], "tsne_embedding": [-0.38796886801719666, 6.645822525024414]}, {"key": "chen2020enhanced", "year": "2020", "title": "Enhanced Discrete Multi-modal Hashing: More Constraints yet Less Time to Learn", "abstract": "<p>Due to the exponential growth of multimedia data, multi-modal hashing as a promising technique to make cross-view retrieval scalable is attracting more and more attention. However, most of the existing multi-modal hashing methods either divide the learning process unnaturally into two separate stages or treat the discrete optimization problem simplistically as a continuous one, which leads to suboptimal results. Recently, a few discrete multi-modal hashing methods that try to address such issues have emerged, but they still ignore several important discrete constraints (such as the balance and decorrelation of hash bits). In this paper, we overcome those limitations by proposing a novel method named \u201cEnhanced Discrete Multi-modal Hashing (EDMH)\u201d which learns binary codes and hashing functions simultaneously from the pairwise similarity matrix of data, under the aforementioned discrete constraints. Although the model of EDMH looks a lot more complex than the other models for multi-modal hashing, we are actually able to develop a fast iterative learning algorithm for it, since the subproblems of its optimization all have closed-form solutions after introducing two auxiliary variables. Our experimental results on three real-world datasets have demonstrated that EDMH not only performs much better than state-of-the-art competitors but also runs much faster than them.</p>\n", "tags": ["Has Code"], "tsne_embedding": [-3.1826648712158203, -9.792351722717285]}, {"key": "chen2020making", "year": "2020", "title": "Making Online Sketching Hashing Even Faster", "abstract": "<p>Data-dependent hashing methods have demonstrated good performance in various machine learning applications to learn a low-dimensional representation from the original data. However they still suffer from several obstacles First most of existing hashing methods are trained in a batch mode yielding inefficiency for training streaming data. Second the computational cost and the memory consumption increase extraordinarily in the big data setting which perplexes the training procedure. Third the lack of labeled data hinders the improvement of the model performance. To address these difficulties we utilize online sketching hashing (OSH) and present a FasteR Online Sketching Hashing (FROSH) algorithm to sketch the data in a more compact form via an independent transformation. We provide theoretical justification to guarantee that our proposed FROSH consumes less time and achieves a comparable sketching precision under the same memory cost of OSH. We also extend FROSH to its distributed implementation namely DFROSH to further reduce the training time cost of FROSH while deriving the theoretical bound of the sketching precision. Finally we conduct extensive experiments on both synthetic and real datasets to demonstrate the attractive merits of FROSH and DFROSH.</p>\n", "tags": ["Streaming Data", "Supervised", "TKDE"], "tsne_embedding": [5.972038745880127, -12.608814239501953]}, {"key": "chen2020strongly", "year": "2020", "title": "Strongly Constrained Discrete Hashing", "abstract": "<p>Learning to hash is a fundamental technique widely used in large-scale image retrieval. Most existing methods for learning to hash address the involved discrete optimization problem by the continuous relaxation of the binary constraint, which usually leads to large quantization errors and consequently suboptimal binary codes. A few discrete hashing methods have emerged recently. However, they either completely ignore some useful constraints (specifically the balance and decorrelation of hash bits) or just turn those constraints into regularizers that would make the optimization easier but less accurate. In this paper, we propose a novel supervised hashing method named Strongly Constrained Discrete Hashing (SCDH) which overcomes such limitations. It can learn the binary codes for all examples in the training set, and meanwhile obtain a hash function for unseen samples with the above mentioned constraints preserved. Although the model of SCDH is fairly sophisticated, we are able to find closed-form solutions to all of its optimization subproblems and thus design an efficient algorithm that converges quickly. In addition, we extend SCDH to a kernelized version SCDH K . Our experiments on three large benchmark datasets have demonstrated that not only can SCDH and SCDH K achieve substantially higher MAP scores than state-of-the-art baselines, but they train much faster than those that are also supervised as well.</p>\n", "tags": ["Has Code", "Supervised", "TIP"], "tsne_embedding": [-0.5379031300544739, -4.514501094818115]}, {"key": "chen2021dvhn", "year": "2021", "title": "DVHN A Deep Hashing Framework For Large-scale Vehicle Re-identification", "abstract": "<p>In this paper we make the very first attempt to investigate the integration of deep hash learning with vehicle re-identification. We propose a deep hash-based vehicle re-identification framework dubbed DVHN which substantially reduces memory usage and promotes retrieval efficiency while reserving nearest neighbor search accuracy. Concretely~DVHN directly learns discrete compact binary hash codes for each image by jointly optimizing the feature learning network and the hash code generating module. Specifically we directly constrain the output from the convolutional neural network to be discrete binary codes and ensure the learned binary codes are optimal for classification. To optimize the deep discrete hashing framework we further propose an alternating minimization method for learning binary similarity-preserved hashing codes. Extensive experiments on two widely-studied vehicle re-identification datasets- (textbfVehicleID) and (textbfVeRi)-~have demonstrated the superiority of our method against the state-of-the-art deep hash methods. (textbfDVHN) of 2048 bits can achieve 13.9437; and 10.2137; accuracy improvement in terms of (textbfmAP) and (textbf)Rank@1 for (textbf)VehicleID (800) dataset. For (textbfVeRi) we achieve 35.4537; and 32.7237; performance gains for (textbf)Rank@1 and (textbfmAP) respectively.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [-9.349261283874512, -8.612015724182129]}, {"key": "chen2021evaluating", "year": "2021", "title": "Evaluating Large Language Models Trained On Code", "abstract": "<p>We introduce Codex a GPT language model fine-tuned on publicly available code from GitHub and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings our model solves 28.837; of the problems while GPT-3 solves 037; and GPT-J solves 11.437;. Furthermore we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method we solve 70.237; of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally we discuss the potential broader impacts of deploying powerful code generation technologies covering safety security and economics.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [25.716426849365234, -13.563810348510742]}, {"key": "chen2021long", "year": "2021", "title": "Long-Tail Hashing", "abstract": "<p>Hashing, which represents data items as compact binary codes, has\nbeen becoming a more and more popular technique, e.g., for large-scale image retrieval, owing to its super fast search speed as well\nas its extremely economical memory consumption. However, existing hashing methods all try to learn binary codes from artificially\nbalanced datasets which are not commonly available in real-world\nscenarios. In this paper, we propose Long-Tail Hashing Network\n(LTHNet), a novel two-stage deep hashing approach that addresses\nthe problem of learning to hash for more realistic datasets where\nthe data labels roughly exhibit a long-tail distribution. Specifically,\nthe first stage is to learn relaxed embeddings of the given dataset\nwith its long-tail characteristic taken into account via an end-to-end deep neural network; the second stage is to binarize those\nobtained embeddings. A critical part of LTHNet is its extended dynamic meta-embedding module which can adaptively realize visual\nknowledge transfer between head and tail classes, and thus enrich\nimage representations for hashing. Our experiments have shown\nthat LTHNet achieves dramatic performance improvements over all\nstate-of-the-art competitors on long-tail datasets, with no or little\nsacrifice on balanced datasets. Further analyses reveal that while to\nour surprise directly manipulating class weights in the loss function\nhas little effect, the extended dynamic meta-embedding module, the\nusage of cross-entropy loss instead of square loss, and the relatively\nsmall batch-size for training all contribute to LTHNet\u2019s success.</p>\n", "tags": ["Has Code", "Image Retrieval", "SIGIR"], "tsne_embedding": [2.6296305656433105, 0.8004043698310852]}, {"key": "chen2021transhash", "year": "2021", "title": "Transhash Transformer-based Hamming Hashing For Efficient Image Retrieval", "abstract": "<p>Deep hamming hashing has gained growing popularity in approximate nearest neighbour search for large-scale image retrieval. Until now the deep hashing for the image retrieval community has been dominated by convolutional neural network architectures e.g. textttResnetcitehe2016deep. In this paper inspired by the recent advancements of vision transformers we present textbfTranshash a pure transformer-based framework for deep hashing learning. Concretely our framework is composed of two major modules (1) Based on textitVision Transformer (ViT) we design a siamese vision transformer backbone for image feature extraction. To learn fine-grained features we innovate a dual-stream feature learning on top of the transformer to learn discriminative global and local features. (2) Besides we adopt a Bayesian learning scheme with a dynamically constructed similarity matrix to learn compact binary hash codes. The entire framework is jointly trained in an end-to-end manner.~To the best of our knowledge this is the first work to tackle deep hashing learning problems without convolutional neural networks (textitCNNs). We perform comprehensive experiments on three widely-studied datasets textbfCIFAR-10 textbfNUSWIDE and textbfIMAGENET. The experiments have evidenced our superiority against the existing state-of-the-art deep hashing methods. Specifically we achieve 8.237; 2.637; 12.737; performance gains in terms of average textitmAP for different hash bit lengths on three public datasets respectively.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Supervised"], "tsne_embedding": [-9.303479194641113, 7.861136436462402]}, {"key": "chen2022large", "year": "2022", "title": "Large Language Models Are Few(1)-shot Table Reasoners", "abstract": "<p>Recent literature has shown that large language models (LLMs) are generally excellent few-shot reasoners to solve text reasoning tasks. However the capability of LLMs on table reasoning tasks is yet to be explored. In this paper we aim at understanding how well LLMs can perform table-related tasks with few-shot in-context learning. Specifically we evaluated LLMs on popular table QA and fact verification datasets like WikiTableQuestion FetaQA TabFact and FEVEROUS and found that LLMs are competent at complex reasoning over table structures though these models are not pre-trained on any table corpus. When combined with chain of thoughts prompting LLMs can achieve very strong performance with only a 1-shot demonstration even on par with some SoTA models. We show that LLMs are even more competent at generating comprehensive long-form answers on FetaQA than tuned T5-large. We further manually studied the reasoning chains elicited from LLMs and found that these reasoning chains are highly consistent with the underlying semantic form. We believe that LLMs can serve as a simple yet generic baseline for future research. The code and data are released in https://github.com/wenhuchen/TableCoT.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [31.47972297668457, 1.5927671194076538]}, {"key": "chen2022program", "year": "2022", "title": "Program Of Thoughts Prompting Disentangling Computation From Reasoning For Numerical Reasoning Tasks", "abstract": "<p>Recently there has been significant progress in teaching language models to perform step-by-step reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting (CoT) is by far the state-of-art method for these tasks. CoT uses language models to perform both reasoning and computation in the multi-step thought process. To disentangle computation from reasoning we propose Program of Thoughts (PoT) which uses language models (mainly Codex) to express the reasoning process as a program. The computation is relegated to an external computer which executes the generated programs to derive the answer. We evaluate PoT on five math word problem datasets (GSM AQuA SVAMP TabMWP MultiArith) and three financial-QA datasets (FinQA ConvFinQA TATQA) for both few-shot and zero-shot setups. Under both few-shot and zero-shot settings PoT can show an average performance gain over CoT by around 1237; across all the evaluated datasets. By combining PoT with self-consistency decoding we can achieve SoTA performance on all math problem datasets and near-SoTA performance on financial datasets. All of our data and code are released in Github https://github.com/wenhuchen/Program-of-Thoughts</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [29.239784240722656, 6.728939056396484]}, {"key": "chen2023beyond", "year": "2023", "title": "Beyond Factuality A Comprehensive Evaluation Of Large Language Models As Knowledge Generators", "abstract": "<p>Large language models (LLMs) outperform information retrieval techniques for downstream knowledge-intensive tasks when being prompted to generate world knowledge. However community concerns abound regarding the factuality and potential implications of using this uncensored knowledge. In light of this we introduce CONNER a COmpreheNsive kNowledge Evaluation fRamework designed to systematically and automatically evaluate generated knowledge from six important perspectives \u2013 Factuality Relevance Coherence Informativeness Helpfulness and Validity. We conduct an extensive empirical analysis of the generated knowledge from three different types of LLMs on two widely studied knowledge-intensive tasks i.e. open-domain question answering and knowledge-grounded dialogue. Surprisingly our study reveals that the factuality of generated knowledge even if lower does not significantly hinder downstream tasks. Instead the relevance and coherence of the outputs are more important than small factual mistakes. Further we show how to use CONNER to improve knowledge-intensive tasks by designing two strategies Prompt Engineering and Knowledge Selection. Our evaluation code and LLM-generated knowledge with human annotations will be released to facilitate future research.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [38.457176208496094, -6.033347129821777]}, {"key": "chen2023bipartite", "year": "2023", "title": "Bipartite Graph Convolutional Hashing For Effective And Efficient Top-n Search In Hamming Space", "abstract": "<p>Searching on bipartite graphs is basal and versatile to many real-world Web applications e.g. online recommendation database retrieval and query-document searching. Given a query node the conventional approaches rely on the similarity matching with the vectorized node embeddings in the continuous Euclidean space. To efficiently manage intensive similarity computation developing hashing techniques for graph structured data has recently become an emerging research direction. Despite the retrieval efficiency in Hamming space prior work is however confronted with catastrophic performance decay. In this work we investigate the problem of hashing with Graph Convolutional Network on bipartite graphs for effective Top-N search. We propose an end-to-end Bipartite Graph Convolutional Hashing approach namely BGCH which consists of three novel and effective modules (1) adaptive graph convolutional hashing (2) latent feature dispersion and (3) Fourier serialized gradient estimation. Specifically the former two modules achieve the substantial retention of the structural information against the inevitable information loss in hash encoding; the last module develops Fourier Series decomposition to the hashing function in the frequency domain mainly for more accurate gradient estimation. The extensive experiments on six real-world datasets not only show the performance superiority over the competing hashing-based counterparts but also demonstrate the effectiveness of all proposed model components contained therein.</p>\n", "tags": ["ARXIV", "Graph"], "tsne_embedding": [-0.14371232688426971, 26.13926124572754]}, {"key": "chen2023empowering", "year": "2023", "title": "Empowering Private Tutoring By Chaining Large Language Models", "abstract": "<p>Artificial intelligence has been applied in various aspects of online education to facilitate teaching and learning. However few approaches has been made toward a complete AI-powered tutoring system. In this work we explore the development of a full-fledged intelligent tutoring system powered by state-of-the-art large language models (LLMs) covering automatic course planning and adjusting tailored instruction and flexible quiz evaluation. To make the system robust to prolonged interaction and cater to individualized education the system is decomposed into three inter-connected core processes-interaction reflection and reaction. Each process is implemented by chaining LLM-powered tools along with dynamically updated memory modules. Tools are LLMs prompted to execute one specific task at a time while memories are data storage that gets updated during education process. Statistical results from learning logs demonstrate the effectiveness and mechanism of each tool usage. Subjective feedback from human users reveal the usability of each function and comparison with ablation systems further testify the benefits of the designed processes in long-term interaction.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [26.08685302734375, -8.995105743408203]}, {"key": "chen2023how", "year": "2023", "title": "How Robust Is GPT-3.5 To Predecessors A Comprehensive Study On Language Understanding Tasks", "abstract": "<p>The GPT-3.5 models have demonstrated impressive performance in various Natural Language Processing (NLP) tasks showcasing their strong understanding and reasoning capabilities. However their robustness and abilities to handle various complexities of the open world have yet to be explored which is especially crucial in assessing the stability of models and is a key aspect of trustworthy AI. In this study we perform a comprehensive experimental analysis of GPT-3.5 exploring its robustness using 21 datasets (about 116K test samples) with 66 text transformations from TextFlint that cover 9 popular Natural Language Understanding (NLU) tasks. Our findings indicate that while GPT-3.5 outperforms existing fine-tuned models on some tasks it still encounters significant robustness degradation such as its average performance dropping by up to 35.7437; and 43.5937; in natural language inference and sentiment analysis tasks respectively. We also show that GPT-3.5 faces some specific robustness challenges including robustness instability prompt sensitivity and number sensitivity. These insights are valuable for understanding its limitations and guiding future research in addressing these challenges to enhance GPT-3.5s overall performance and generalization abilities.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [34.35124969482422, 5.900253772735596]}, {"key": "chen2023mitigating", "year": "2023", "title": "Mitigating Hallucination In Visual Language Models With Visual Supervision", "abstract": "<p>Large vision-language models (LVLMs) suffer from hallucination a lot generating responses that apparently contradict to the image content occasionally. The key problem lies in its weak ability to comprehend detailed content in a multi-modal context which can be mainly attributed to two factors in training data and loss function. The vision instruction dataset primarily focuses on global description and the auto-regressive loss function favors text modeling rather than image understanding. In this paper we bring more detailed vision annotations and more discriminative vision models to facilitate the training of LVLMs so that they can generate more precise responses without encounter hallucination. On one hand we generate image-text pairs with detailed relationship annotations in panoptic scene graph dataset (PSG). These conversations pay more attention on detailed facts in the image encouraging the model to answer questions based on multi-modal contexts. On the other hand we integrate SAM and mask prediction loss as auxiliary supervision forcing the LVLMs to have the capacity to identify context-related objects so that they can generate more accurate responses mitigating hallucination. Moreover to provide a deeper evaluation on the hallucination in LVLMs we propose a new benchmark RAH-Bench. It divides vision hallucination into three different types that contradicts the image with wrong categories attributes or relations and introduces False Positive Rate as detailed sub-metric for each type. In this benchmark our approach demonstrates an +8.437; enhancement compared to original LLaVA and achieves widespread performance improvements across other models.</p>\n", "tags": ["ARXIV", "Cross Modal", "Graph", "Supervised"], "tsne_embedding": [48.31991195678711, 7.448172092437744]}, {"key": "chen2023mixture", "year": "2023", "title": "Mixture Of Soft Prompts For Controllable Data Generation", "abstract": "<p>Large language models (LLMs) effectively generate fluent text when the target output follows natural language patterns. However structured prediction tasks confine the output format to a limited ontology causing even very large models to struggle since they were never trained with such restrictions in mind. The difficulty of using LLMs for direct prediction is exacerbated in few-shot learning scenarios which commonly arise due to domain shift and resource limitations. We flip the problem on its head by leveraging the LLM as a tool for data augmentation rather than direct prediction. Our proposed Mixture of Soft Prompts (MSP) serves as a parameter-efficient procedure for generating data in a controlled manner. Denoising mechanisms are further applied to improve the quality of synthesized data. Automatic metrics show our method is capable of producing diverse and natural text while preserving label semantics. Moreover MSP achieves state-of-the-art results on three benchmarks when compared against strong baselines. Our method offers an alternate data-centric approach for applying LLMs to complex prediction tasks.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [33.625362396240234, -1.5365501642227173]}, {"key": "chen2023pali", "year": "2023", "title": "Pali-3 Vision Language Models Smaller Faster Stronger", "abstract": "<p>This paper presents PaLI-3 a smaller faster and stronger vision language model (VLM) that compares favorably to similar models that are 10x larger. As part of arriving at this strong performance we compare Vision Transformer (ViT) models pretrained using classification objectives to contrastively (SigLIP) pretrained ones. We find that while slightly underperforming on standard image classification benchmarks SigLIP-based PaLI shows superior performance across various multimodal benchmarks especially on localization and visually-situated text understanding. We scale the SigLIP image encoder up to 2 billion parameters and achieves a new state-of-the-art on multilingual cross-modal retrieval. We hope that PaLI-3 at only 5B parameters rekindles research on fundamental pieces of complex VLMs and could fuel a new generation of scaled-up models.</p>\n", "tags": ["ARXIV", "Cross Modal", "Supervised"], "tsne_embedding": [-21.44867515563965, 19.34033966064453]}, {"key": "chen2023shikra", "year": "2023", "title": "Shikra Unleashing Multimodal Llms Referential Dialogue Magic", "abstract": "<p>In human conversations individuals can indicate relevant regions within a scene while addressing others. In turn the other person can then respond by referring to specific regions if necessary. This natural referential ability in dialogue remains absent in current Multimodal Large Language Models (MLLMs). To fill this gap this paper proposes an MLLM called Shikra which can handle spatial coordinate inputs and outputs in natural language. Its architecture consists of a vision encoder an alignment layer and a LLM. It is designed to be straightforward and simple without the need for extra vocabularies position encoder pre-/post-detection modules or external plug-in models. All inputs and outputs are in natural language form. Referential dialogue is a superset of various vision-language (VL) tasks. Shikra can naturally handle location-related tasks like REC and PointQA as well as conventional VL tasks such as Image Captioning and VQA. Experimental results showcase Shikras promising performance. Furthermore it enables numerous exciting applications like providing mentioned objects coordinates in chains of thoughts and comparing user-pointed regions similarities. Our code model and dataset are accessed at https://github.com/shikras/shikra.</p>\n", "tags": ["ARXIV", "Cross Modal", "Has Code"], "tsne_embedding": [38.73825454711914, 8.08163070678711]}, {"key": "chen2023supervised", "year": "2023", "title": "Supervised Auto-encoding Twin-bottleneck Hashing", "abstract": "<p>Deep hashing has shown to be a complexity-efficient solution for the Approximate Nearest Neighbor search problem in high dimensional space. Many methods usually build the loss function from pairwise or triplet data points to capture the local similarity structure. Other existing methods construct the similarity graph and consider all points simultaneously. Auto-encoding Twin-bottleneck Hashing is one such method that dynamically builds the graph. Specifically each input data is encoded into a binary code and a continuous variable or the so-called twin bottlenecks. The similarity graph is then computed from these binary codes which get updated consistently during the training. In this work we generalize the original model into a supervised deep hashing network by incorporating the label information. In addition we examine the differences of codes structure between these two networks and consider the class imbalance problem especially in multi-labeled datasets. Experiments on three datasets yield statistically significant improvement against the original model. Results are also comparable and competitive to other supervised methods.</p>\n", "tags": ["ARXIV", "Graph", "Supervised"], "tsne_embedding": [-2.337773323059082, 20.187124252319336]}, {"key": "chen2023survey", "year": "2023", "title": "A Survey On Multi-behavior Sequential Recommendation", "abstract": "<p>Recommender systems is set up to address the issue of information overload in traditional information retrieval systems which is focused on recommending information that is of most interest to users from massive information. Generally there is a sequential nature and heterogeneity to the behavior of a person interacting with a system leading to the proposal of multi-behavior sequential recommendation (MBSR). MBSR is a relatively new and worthy direction for in-depth research which can achieve state-of-the-art recommendation through suitable modeling and some related works have been proposed. This survey aims to shed light on the MBSR problem. Firstly we introduce MBSR in detail including its problem definition application scenarios and challenges faced. Secondly we detail the classification of MBSR including neighborhood-based methods matrix factorization-based methods and deep learning-based methods where we further classify the deep learning-based methods into different learning architectures based on RNN GNN Transformer and generic architectures as well as architectures that integrate hybrid techniques. In each method we present related works based on the data perspective and the modeling perspective as well as analyze the strengths weaknesses and features of these works. Finally we discuss some promising future research directions to address the challenges and improve the current status of MBSR.</p>\n", "tags": ["ARXIV", "Deep Learning", "Supervised", "Survey Paper"], "tsne_embedding": [38.091407775878906, -11.99434757232666]}, {"key": "chen2023towards", "year": "2023", "title": "Towards End-to-end Embodied Decision Making Via Multi-modal Large Language Model Explorations With Gpt4-vision And Beyond", "abstract": "<p>In this study we explore the potential of Multimodal Large Language Models (MLLMs) in improving embodied decision-making processes for agents. While Large Language Models (LLMs) have been widely used due to their advanced reasoning skills and vast world knowledge MLLMs like GPT4-Vision offer enhanced visual understanding and reasoning capabilities. We investigate whether state-of-the-art MLLMs can handle embodied decision-making in an end-to-end manner and whether collaborations between LLMs and MLLMs can enhance decision-making. To address these questions we introduce a new benchmark called PCA-EVAL which evaluates embodied decision-making from the perspectives of Perception Cognition and Action. Additionally we propose HOLMES a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision-making. We compare end-to-end embodied decision-making and HOLMES on our benchmark and find that the GPT4-Vision model demonstrates strong end-to-end embodied decision-making abilities outperforming GPT4-HOLMES in terms of average decision accuracy (+337;). However this performance is exclusive to the latest GPT4-Vision model surpassing the open-source state-of-the-art MLLM by 2637;. Our results indicate that powerful MLLMs like GPT4-Vision hold promise for decision-making in embodied agents offering new avenues for MLLM research. Code and data are open at https://github.com/pkunlp-icler/PCA-EVAL/.</p>\n", "tags": ["ARXIV", "Cross Modal", "Has Code", "Unsupervised"], "tsne_embedding": [35.0798454284668, -9.806585311889648]}, {"key": "chen2023when", "year": "2023", "title": "When Large Language Models Meet Personalization Perspectives Of Challenges And Opportunities", "abstract": "<p>The advent of large language models marks a revolutionary breakthrough in artificial intelligence. With the unprecedented scale of training and model parameters the capability of large language models has been dramatically improved leading to human-like performances in understanding language synthesizing and common-sense reasoning etc. Such a major leap-forward in general AI capacity will change the pattern of how personalization is conducted. For one thing it will reform the way of interaction between humans and personalization systems. Instead of being a passive medium of information filtering large language models present the foundation for active user engagement. On top of such a new foundation user requests can be proactively explored and users required information can be delivered in a natural and explainable way. For another thing it will also considerably expand the scope of personalization making it grow from the sole function of collecting personalized information to the compound function of providing personalized services. By leveraging large language models as general-purpose interface the personalization systems may compile user requests into plans calls the functions of external tools to execute the plans and integrate the tools outputs to complete the end-to-end personalization tasks. Today large language models are still being developed whereas the application in personalization is largely unexplored. Therefore we consider it to be the right time to review the challenges in personalization and the opportunities to address them with LLMs. In particular we dedicate this perspective paper to the discussion of the following aspects the development and challenges for the existing personalization system the newly emerged capabilities of large language models and the potential ways of making use of large language models for personalization.</p>\n", "tags": ["ARXIV", "Survey Paper"], "tsne_embedding": [38.7962646484375, -13.843700408935547]}, {"key": "chen2024halc", "year": "2024", "title": "HALC Object Hallucination Reduction Via Adaptive Focal-contrast Decoding", "abstract": "<p>While large vision-language models (LVLMs) have demonstrated impressive capabilities in interpreting multi-modal contexts they invariably suffer from object hallucinations (OH). We introduce HALC a novel decoding algorithm designed to mitigate OH in LVLMs. HALC leverages distinct fine-grained optimal visual information in vision-language tasks and operates on both local and global contexts simultaneously. Specifically HALC integrates a robust auto-focal grounding mechanism (locally) to correct hallucinated tokens on the fly and a specialized beam search algorithm (globally) to significantly reduce OH while preserving text generation quality. Additionally HALC can be integrated into any LVLMs as a plug-and-play module without extra training. Extensive experimental studies demonstrate the effectiveness of HALC in reducing OH outperforming state-of-the-arts across four benchmarks.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [46.22936248779297, 9.096686363220215]}, {"key": "chen2024supervised", "year": "2024", "title": "Supervised Consensus Anchor Graph Hashing for Cross Modal Retrieval", "abstract": "<p>The target of cross-modal hashing is to embed heterogeneous multimedia data into a common low-dimensional Hamming space, which plays a pivotal part in multimedia retrieval due to the emergence of big multimodal data. Recently, matrix factorization has achieved great success in cross-modal hashing. However, how to effectively use label information and local geometric structure is still a challenging problem for these approaches. To address this issue, we propose a cross-modal hashing method based on collective matrix factorization, which considers both the label consistency across different modalities and the local geometric consistency in each modality. These two elements are formulated as a graph Laplacian term in the objective function, leading to a substantial improvement on the discriminative power of latent semantic features obtained by collective matrix factorization. Moreover, the proposed method learns unified hash codes for different modalities of an instance to facilitate cross-modal search, and the objective function is solved using an iterative strategy. The experimental results on two benchmark data sets show the effectiveness of the proposed method and its superiority over state-of-the-art cross-modal hashing methods.</p>\n", "tags": ["Cross Modal", "Supervised", "TIP"], "tsne_embedding": [-0.1642005890607834, 23.17418670654297]}, {"key": "chen2024towards", "year": "2024", "title": "Towards Effective Top-n Hamming Search Via Bipartite Graph Contrastive Hashing", "abstract": "<p>Searching on bipartite graphs serves as a fundamental task for various real-world applications such as recommendation systems database retrieval and document querying. Conventional approaches rely on similarity matching in continuous Euclidean space of vectorized node embeddings. To handle intensive similarity computation efficiently hashing techniques for graph-structured data have emerged as a prominent research direction. However despite the retrieval efficiency in Hamming space previous studies have encountered catastrophic performance decay. To address this challenge we investigate the problem of hashing with Graph Convolutional Network for effective Top-N search. Our findings indicate the learning effectiveness of incorporating hashing techniques within the exploration of bipartite graph reception fields as opposed to simply treating hashing as post-processing to output embeddings. To further enhance the model performance we advance upon these findings and propose Bipartite Graph Contrastive Hashing (BGCH+). BGCH+ introduces a novel dual augmentation approach to both intermediate information and hash code outputs in the latent feature spaces thereby producing more expressive and robust hash codes within a dual self-supervised learning paradigm. Comprehensive empirical analyses on six real-world benchmarks validate the effectiveness of our dual feature contrastive learning in boosting the performance of BGCH+ compared to existing approaches.</p>\n", "tags": ["ARXIV", "Graph", "Supervised"], "tsne_embedding": [-0.054461631923913956, 25.909862518310547]}, {"key": "chen2024unified", "year": "2024", "title": "Unified Hallucination Detection For Multimodal Large Language Models", "abstract": "<p>Despite significant strides in multimodal tasks Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has therefore become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks an inadequate range of hallucination categories addressed and a lack of detailed granularity. In response to these challenges our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark MHaluBench meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally we unveil a novel unified multimodal hallucination detection framework UNIHD which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD through meticulous evaluation and comprehensive analysis. We also provide strategic insights on the application of specific tools for addressing various categories of hallucinations.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [52.017208099365234, 7.708942413330078]}, {"key": "cheng2021robust", "year": "2021", "title": "Robust Unsupervised Cross-modal Hashing for Multimedia Retrieval", "abstract": "<p>With the quick development of social websites, there are more opportunities to have different media types (such as text, image, video, etc.) describing the same topic from large-scale heterogeneous data sources. To efficiently identify the inter-media correlations for multimedia retrieval, unsupervised cross-modal hashing (UCMH) has gained increased interest due to the significant reduction in computation and storage. However, most UCMH methods assume that the data from different modalities are well paired. As a result, existing UCMH methods may not achieve satisfactory performance when partially paired data are given only. In this article, we propose a new-type of UCMH method called robust unsupervised cross-modal hashing (RUCMH). The major contribution lies in jointly learning modal-specific hash function, exploring the correlations among modalities with partial or even without any pairwise correspondence, and preserving the information of original features as much as possible. The learning process can be modeled via a joint minimization problem, and the corresponding optimization algorithm is presented. A series of experiments is conducted on four real-world datasets (Wiki, MIRFlickr, NUS-WIDE, and MS-COCO). The results demonstrate that RUCMH can significantly outperform the state-of-the-art unsupervised cross-modal hashing methods, especially for the partially paired case, which validates the effectiveness of RUCMH.</p>\n", "tags": ["Cross Modal", "Supervised"], "tsne_embedding": [-9.33873462677002, -4.673579216003418]}, {"key": "cheng2022binding", "year": "2022", "title": "Binding Language Models In Symbolic Languages", "abstract": "<p>Though end-to-end neural approaches have recently been dominating NLP tasks in both performance and ease-of-use they lack interpretability and robustness. We propose Binder a training-free neural-symbolic framework that maps the task input to a program which (1) allows binding a unified API of language model (LM) functionalities to a programming language (e.g. SQL Python) to extend its grammar coverage and thus tackle more diverse questions (2) adopts an LM as both the program parser and the underlying model called by the API during execution and (3) requires only a few in-context exemplar annotations. Specifically we employ GPT-3 Codex as the LM. In the parsing stage with only a few in-context exemplars Codex is able to identify the part of the task input that cannot be answerable by the original programming language correctly generate API calls to prompt Codex to solve the unanswerable part and identify where to place the API calls while being compatible with the original grammar. In the execution stage Codex can perform versatile functionalities (e.g. commonsense QA information extraction) given proper prompts in the API calls. Binder achieves state-of-the-art results on WikiTableQuestions and TabFact datasets with explicit output programs that benefit human debugging. Note that previous best systems are all finetuned on tens of thousands of task-specific samples while Binder only uses dozens of annotations as in-context exemplars without any training. Our code is available at https://github.com/HKUNLP/Binder .</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [27.759912490844727, -0.15750116109848022]}, {"key": "choromanski2015efficient", "year": "2015", "title": "Efficient Data Hashing With Structured Binary Embeddings", "abstract": "<p>We present here new mechanisms for hashing data via binary embeddings. Contrary to most of the techniques presented before the embedding matrix of our mechanism is highly structured. That enables us to perform hashing more efficiently and use less memory. What is crucial and nonintuitive is the fact that imposing structured mechanism does not affect the quality of the produced hash. To the best of our knowledge we are the first to give strong theoretical guarantees of the proposed binary hashing method by proving the efficiency of the mechanism for several classes of structured projection matrices. As a corollary we obtain binary hashing mechanisms with strong concentration results for circulant and Topelitz matrices. Our approach is however much more general.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-21.72877311706543, -17.433597564697266]}, {"key": "chowdhery2022palm", "year": "2022", "title": "Palm Scaling Language Modeling With Pathways", "abstract": "<p>Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning we trained a 540-billion parameter densely activated Transformer language model which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks PaLM 540B achieves breakthrough performance outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity and study the extent of training data memorization with respect to model scale. Finally we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [20.135866165161133, 0.37489572167396545]}, {"key": "christiani2017fast", "year": "2017", "title": "Fast Locality-sensitive Hashing Frameworks For Approximate Near Neighbor Search", "abstract": "<p>The Indyk-Motwani Locality-Sensitive Hashing (LSH) framework (STOC 1998) is a general technique for constructing a data structure to answer approximate near neighbor queries by using a distribution (mathcalH) over locality-sensitive hash functions that partition space. For a collection of n points after preprocessing the query time is dominated by O(n^(rho) (log) n) evaluations of hash functions from (mathcalH) and O(n^(rho)) hash table lookups and distance computations where (rho) (in) (01) is determined by the locality-sensitivity properties of (mathcalH). It follows from a recent result by Dahlgaard et al. (FOCS 2017) that the number of locality-sensitive hash functions can be reduced to O((log)^2 n) leaving the query time to be dominated by O(n^(rho)) distance computations and O(n^(rho) (log) n) additional word-RAM operations. We state this result as a general framework and provide a simpler analysis showing that the number of lookups and distance computations closely match the Indyk-Motwani framework making it a viable replacement in practice. Using ideas from another locality-sensitive hashing framework by Andoni and Indyk (SODA 2006) we are able to reduce the number of additional word-RAM operations to O(n^(rho)).</p>\n", "tags": ["ARXIV", "FOCS", "Independent", "LSH"], "tsne_embedding": [-35.95623016357422, 0.6976226568222046]}, {"key": "christiani2019algorithms", "year": "2019", "title": "Algorithms For Similarity Search And Pseudorandomness", "abstract": "<p>We study the problem of approximate near neighbor (ANN) search and show the following results - An improved framework for solving the ANN problem using locality-sensitive hashing reducing the number of evaluations of locality-sensitive hash functions and the word-RAM complexity compared to the standard framework. - A framework for solving the ANN problem with space-time tradeoffs as well as tight upper and lower bounds for the space-time tradeoff of framework solutions to the ANN problem under cosine similarity. - A novel approach to solving the ANN problem on sets along with a matching lower bound improving the state of the art. - A self-tuning version of the algorithm is shown through experiments to outperform existing similarity join algorithms. - Tight lower bounds for asymmetric locality-sensitive hashing which has applications to the approximate furthest neighbor problem orthogonal vector search and annulus queries. - A proof of the optimality of a well-known Boolean locality-sensitive hashing scheme. We study the problem of efficient algorithms for producing high-quality pseudorandom numbers and obtain the following results - A deterministic algorithm for generating pseudorandom numbers of arbitrarily high quality in constant time using near-optimal space. - A randomized construction of a family of hash functions that outputs pseudorandom numbers of arbitrarily high quality with space usage and running time nearly matching known cell-probe lower bounds.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-30.547502517700195, -0.6326086521148682]}, {"key": "christiani2020dartminhash", "year": "2020", "title": "Dartminhash Fast Sketching For Weighted Sets", "abstract": "<p>Weighted minwise hashing is a standard dimensionality reduction technique with applications to similarity search and large-scale kernel machines. We introduce a simple algorithm that takes a weighted set x in mathbbR_geq 0^d( and computes )k( independent minhashes in expected time )O(k log k + Vert x Vert_0log( Vert x Vert_1 + 1/Vert x Vert_1)) improving upon the state-of-the-art BagMinHash algorithm (KDD 18) and representing the fastest weighted minhash algorithm for sparse data. Our experiments show running times that scale better with (k) and (Vert x Vert_0) compared to ICWS (ICDM 10) and BagMinhash obtaining (10)x speedups in common use cases. Our approach also gives rise to a technique for computing fully independent locality-sensitive hash values for ((L K))-parameterized approximate near neighbor search under weighted Jaccard similarity in optimal expected time (O(LK + Vert x Vert_0)) improving on prior work even in the case of unweighted sets.</p>\n", "tags": ["ARXIV", "Independent", "KDD"], "tsne_embedding": [-27.712547302246094, -2.330665349960327]}, {"key": "chu2023fine", "year": "2023", "title": "Fine-tune Language Models To Approximate Unbiased In-context Learning", "abstract": "<p>In-context learning (ICL) is an astonishing emergent ability of large language models (LLMs). By presenting a prompt that includes multiple input-output pairs as examples and introducing a new query input models can generate the corresponding output. However the performance of models heavily relies on the quality of the input prompt when implementing in-context learning. Biased or imbalanced input prompts can significantly degrade the performance of language models. To address this issue we introduce a reweighted algorithm called RICL (Reweighted In-context Learning). This algorithm fine-tunes language models using an unbiased validation set to determine the optimal weight for each input-output example to approximate unbiased in-context learning. Furthermore we also introduce a low-cost reweighted algorithm a linear optimal weight approximation algorithm called LARICL (Linear Approximation of Reweighted In-context Learning). This algorithm requires minimal training cost while providing effective results. We prove the convergence of our algorithm and validate its performance through experiments conducted on a numerical dataset. The experimental findings reveal a substantial improvement in comparison to benchmarks including the performance of casual prompt-based in-context learning and the performance of a classic fine-tuning method.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [12.853513717651367, 1.7875986099243164]}, {"key": "chu2023leveraging", "year": "2023", "title": "Leveraging Large Language Models For Pre-trained Recommender Systems", "abstract": "<p>Recent advancements in recommendation systems have shifted towards more comprehensive and personalized recommendations by utilizing large language models (LLM). However effectively integrating LLMs commonsense knowledge and reasoning abilities into recommendation systems remains a challenging problem. In this paper we propose RecSysLLM a novel pre-trained recommendation model based on LLMs. RecSysLLM retains LLM reasoning and knowledge while integrating recommendation domain knowledge through unique designs of data training and inference. This allows RecSysLLM to leverage LLMs capabilities for recommendation tasks in an efficient unified framework. We demonstrate the effectiveness of RecSysLLM on benchmarks and real-world scenarios. RecSysLLM provides a promising approach to developing unified recommendation systems by fully exploiting the power of pre-trained language models.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [39.46671676635742, -9.3809175491333]}, {"key": "cifar2009learning", "year": "2009", "title": "Learning Multiple Layers of Features from Tiny Images", "abstract": "<p>Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It\nis, in principle, an excellent dataset for unsupervised training of deep generative models, but previous\nresearchers who have tried this have found it difficult to learn a good set of\nfilters from the images.\nWe show how to train a multi-layer generative model that learns to extract meaningful features which\nresemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute\nthe work among multiple machines connected on a network, we show how training such a model can be\ndone in reasonable time.\nA second problematic aspect of the tiny images dataset is that there are no reliable class labels\nwhich makes it hard to use for object recognition experiments. We created two sets of reliable labels.\nThe CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of\neach of 100 non-overlapping classes. Using these labels, we show that object recognition is significantly\nimproved by pre-training a layer of features on a large set of unlabeled tiny images.</p>\n", "tags": ["Dataset", "Supervised"], "tsne_embedding": [1.7023876905441284, 8.684854507446289]}, {"key": "cis\u0142ak2017lightweight", "year": "2017", "title": "Lightweight Fingerprints For Fast Approximate Keyword Matching Using Bitwise Operations", "abstract": "<p>We aim to speed up approximate keyword matching by storing a lightweight fixed-size block of data for each string called a fingerprint. These work in a similar way to hash values; however they can be also used for matching with errors. They store information regarding symbol occurrences using individual bits and they can be compared against each other with a constant number of bitwise operations. In this way certain strings can be deduced to be at least within the distance k from each other (using Hamming or Levenshtein distance) without performing an explicit verification. We show experimentally that for a preprocessed collection of strings fingerprints can provide substantial speedups for k = 1 namely over 2.5 times for the Hamming distance and over 10 times for the Levenshtein distance. Tests were conducted on synthetic and real-world English and URL data.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-13.659621238708496, -11.991461753845215]}, {"key": "clavi\u00e92023large", "year": "2023", "title": "Large Language Models In The Workplace A Case Study On Prompt Engineering For Job Type Classification", "abstract": "<p>This case study investigates the task of job classification in a real-world setting where the goal is to determine whether an English-language job posting is appropriate for a graduate or entry-level position. We explore multiple approaches to text classification including supervised approaches such as traditional models like Support Vector Machines (SVMs) and state-of-the-art deep learning methods such as DeBERTa. We compare them with Large Language Models (LLMs) used in both few-shot and zero-shot classification settings. To accomplish this task we employ prompt engineering a technique that involves designing prompts to guide the LLMs towards the desired output. Specifically we evaluate the performance of two commercially available state-of-the-art GPT-3.5-based language models text-davinci-003 and gpt-3.5-turbo. We also conduct a detailed analysis of the impact of different aspects of prompt engineering on the models performance. Our results show that with a well-designed prompt a zero-shot gpt-3.5-turbo classifier outperforms all other models achieving a 637; increase in Precision@9537; Recall compared to the best supervised approach. Furthermore we observe that the wording of the prompt is a critical factor in eliciting the appropriate reasoning in the model and that seemingly minor aspects of the prompt significantly affect the models performance.</p>\n", "tags": ["ARXIV", "Case Study", "Deep Learning", "Supervised"], "tsne_embedding": [36.99965286254883, -3.060690402984619]}, {"key": "coco2014new", "year": "2014", "title": "Microsoft COCO: Common Objects in Context", "abstract": "<p>We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old.\nWith a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.</p>\n", "tags": ["Dataset"], "tsne_embedding": [9.229833602905273, 20.183446884155273]}, {"key": "collobert2011natural", "year": "2011", "title": "Natural Language Processing (almost) From Scratch", "abstract": "<p>We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging chunking named entity recognition and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [26.292667388916016, -6.6229071617126465]}, {"key": "coluzzi2024binomialhash", "year": "2024", "title": "Binomialhash A Constant Time Minimal Memory Consistent Hash Algorithm", "abstract": "<p>Consistent hashing is employed in distributed systems and networking applications to evenly and effectively distribute data across a cluster of nodes. This paper introduces BinomialHash a consistent hashing algorithm that operates in constant time and requires minimal memory. We provide a detailed explanation of the algorithm offer a pseudo-code implementation and formally establish its strong theoretical guarantees.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-23.21233367919922, -19.876779556274414]}, {"key": "conjeti2016deep", "year": "2016", "title": "Deep Residual Hashing", "abstract": "<p>Hashing aims at generating highly compact similarity preserving code words which are well suited for large-scale image retrieval tasks. Most existing hashing methods first encode the images as a vector of hand-crafted features followed by a separate binarization step to generate hash codes. This two-stage process may produce sub-optimal encoding. In this paper for the first time we propose a deep architecture for supervised hashing through residual learning termed Deep Residual Hashing (DRH) for an end-to-end simultaneous representation learning and hash coding. The DRH model constitutes four key elements (1) a sub-network with multiple stacked residual blocks; (2) hashing layer for binarization; (3) supervised retrieval loss function based on neighbourhood component analysis for similarity preserving embedding; and (4) hashing related losses and regularisation to control the quantization error and improve the quality of hash coding. We present results of extensive experiments on a large public chest x-ray image database with co-morbidities and discuss the outcome showing substantial improvements over the latest state-of-the art methods.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Quantisation", "Supervised"], "tsne_embedding": [-10.424663543701172, 9.93124771118164]}, {"key": "conjeti2017learning", "year": "2017", "title": "Learning Robust Hash Codes For Multiple Instance Image Retrieval", "abstract": "<p>In this paper for the first time we introduce a multiple instance (MI) deep hashing technique for learning discriminative hash codes with weak bag-level supervision suited for large-scale retrieval. We learn such hash codes by aggregating deeply learnt hierarchical representations across bag members through a dedicated MI pool layer. For better trainability and retrieval quality we propose a two-pronged approach that includes robust optimization and training with an auxiliary single instance hashing arm which is down-regulated gradually. We pose retrieval for tumor assessment as an MI problem because tumors often coexist with benign masses and could exhibit complementary signatures when scanned from different anatomical views. Experimental validations on benchmark mammography and histology datasets demonstrate improved retrieval performance over the state-of-the-art methods.</p>\n", "tags": ["ARXIV", "Graph", "Image Retrieval", "Independent"], "tsne_embedding": [-10.429676055908203, 11.239346504211426]}, {"key": "creswell2022selection", "year": "2022", "title": "Selection-inference Exploiting Large Language Models For Interpretable Logical Reasoning", "abstract": "<p>Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules and alternates between selection and inference to generate a series of interpretable casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting with no fine-tuning yields a performance improvement of over 10037; compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace which has important implications for the safety and trustworthiness of the system.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [33.39323806762695, 5.353844165802002]}, {"key": "cui2020exchnet", "year": "2020", "title": "ExchNet: A Unified Hashing Network for Large-Scale Fine-Grained Image Retrieval", "abstract": "<p>Retrieving content relevant images from a large-scale fine grained dataset could suffer from intolerably slow query speed and highly\nredundant storage cost, due to high-dimensional real-valued embeddings\nwhich aim to distinguish subtle visual differences of fine-grained objects.\nIn this paper, we study the novel fine-grained hashing topic to generate compact binary codes for fine-grained images, leveraging the search\nand storage efficiency of hash learning to alleviate the aforementioned\nproblems. Specifically, we propose a unified end-to-end trainable network,\ntermed as ExchNet. Based on attention mechanisms and proposed attention constraints, it can firstly obtain both local and global features\nto represent object parts and whole fine-grained objects, respectively.\nFurthermore, to ensure the discriminative ability and semantic meaning\u2019s\nconsistency of these part-level features across images, we design a local\nfeature alignment approach by performing a feature exchanging operation. Later, an alternative learning algorithm is employed to optimize\nthe whole ExchNet and then generate the final binary hash codes. Validated by extensive experiments, our proposal consistently outperforms\nstate-of-the-art generic hashing methods on five fine-grained datasets,\nwhich shows our effectiveness. Moreover, compared with other approximate nearest neighbor methods, ExchNet achieves the best speed-up and\nstorage reduction, revealing its efficiency and practicality.</p>\n", "tags": ["Deep Learning", "ECCV", "Image Retrieval"], "tsne_embedding": [-8.885751724243164, 14.798894882202148]}, {"key": "curtin2016fast", "year": "2016", "title": "Fast Approximate Furthest Neighbors With Data-dependent Hashing", "abstract": "<p>We present a novel hashing strategy for approximate furthest neighbor search that selects projection bases using the data distribution. This strategy leads to an algorithm which we call DrusillaHash that is able to outperform existing approximate furthest neighbor strategies. Our strategy is motivated by an empirical study of the behavior of the furthest neighbor search problem which lends intuition for where our algorithm is most useful. We also present a variant of the algorithm that gives an absolute approximation guarantee; to our knowledge this is the first such approximate furthest neighbor hashing approach to give such a guarantee. Performance studies indicate that DrusillaHash can achieve comparable levels of approximation to other algorithms while giving up to an order of magnitude speedup. An implementation is available in the mlpack machine learning library (found at http://www.mlpack.org).</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-30.098041534423828, 3.781510591506958]}, {"key": "d2023susceptibility", "year": "2023", "title": "Susceptibility To Influence Of Large Language Models", "abstract": "<p>Two studies tested the hypothesis that a Large Language Model (LLM) can be used to model psychological change following exposure to influential input. The first study tested a generic mode of influence - the Illusory Truth Effect (ITE) - where earlier exposure to a statement (through for example rating its interest) boosts a later truthfulness test rating. Data was collected from 1000 human participants using an online experiment and 1000 simulated participants using engineered prompts and LLM completion. 64 ratings per participant were collected using all exposure-test combinations of the attributes truth interest sentiment and importance. The results for human participants reconfirmed the ITE and demonstrated an absence of effect for attributes other than truth and when the same attribute is used for exposure and test. The same pattern of effects was found for LLM-simulated participants. The second study concerns a specific mode of influence - populist framing of news to increase its persuasion and political mobilization. Data from LLM-simulated participants was collected and compared to previously published data from a 15-country experiment on 7286 human participants. Several effects previously demonstrated from the human study were replicated by the simulated study including effects that surprised the authors of the human study by contradicting their theoretical expectations (anti-immigrant framing of news decreases its persuasion and mobilization); but some significant relationships found in human data (modulation of the effectiveness of populist framing according to relative deprivation of the participant) were not present in the LLM data. Together the two studies support the view that LLMs have potential to act as models of the effect of influence.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [45.29677200317383, 1.5016882419586182]}, {"key": "dadaneh2020pairwise", "year": "2020", "title": "Pairwise Supervised Hashing With Bernoulli Variational Auto-encoder And Self-control Gradient Estimator", "abstract": "<p>Semantic hashing has become a crucial component of fast similarity search in many large-scale information retrieval systems in particular for text data. Variational auto-encoders (VAEs) with binary latent variables as hashing codes provide state-of-the-art performance in terms of precision for document retrieval. We propose a pairwise loss function with discrete latent VAE to reward within-class similarity and between-class dissimilarity for supervised hashing. Instead of solving the optimization relying on existing biased gradient estimators an unbiased low-variance gradient estimator is adopted to optimize the hashing function by evaluating the non-differentiable loss function over two correlated sets of binary hashing codes to control the variance of gradient estimates. This new semantic hashing framework achieves superior performance compared to the state-of-the-arts as demonstrated by our comprehensive experiments.</p>\n", "tags": ["Supervised", "WWW"], "tsne_embedding": [-10.394932746887207, 1.4202483892440796]}, {"key": "dahlgaard2014hashing", "year": "2014", "title": "Hashing For Statistics Over K-partitions", "abstract": "<p>In this paper we analyze a hash function for k-partitioning a set into bins obtaining strong concentration bounds for standard algorithms combining statistics from each bin. This generic method was originally introduced by Flajolet and Martin~FOCS83 in order to save a factor (Omega)(k) of time per element over k independent samples when estimating the number of distinct elements in a data stream. It was also used in the widely used HyperLogLog algorithm of Flajolet et al.~AOFA97 and in large-scale machine learning by Li et al.~NIPS12 for minwise estimation of set similarity. The main issue of k-partition is that the contents of different bins may be highly correlated when using popular hash functions. This means that methods of analyzing the marginal distribution for a single bin do not apply. Here we show that a tabulation based hash function mixed tabulation does yield strong concentration bounds on the most popular applications of k-partitioning similar to those we would get using a truly random hash function. The analysis is very involved and implies several new results of independent interest for both simple and double tabulation e.g. a simple and efficient construction for invertible bloom filters and uniform hashing on a given set.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-24.415416717529297, -3.274343729019165]}, {"key": "dahlgaard2017practical", "year": "2017", "title": "Practical Hash Functions For Similarity Estimation And Dimensionality Reduction", "abstract": "<p>Hashing is a basic tool for dimensionality reduction employed in several aspects of machine learning. However the perfomance analysis is often carried out under the abstract assumption that a truly random unit cost hash function is used without concern for which concrete hash function is employed. The concrete hash function may work fine on sufficiently random input. The question is if it can be trusted in the real world when faced with more structured input. In this paper we focus on two prominent applications of hashing namely similarity estimation with the one permutation hashing (OPH) scheme of Li et al. NIPS12 and feature hashing (FH) of Weinberger et al. ICML09 both of which have found numerous applications i.e. in approximate near-neighbour search with LSH and large-scale classification with SVM. We consider the recent mixed tabulation hash function of Dahlgaard et al. FOCS15 which was proved theoretically to perform like a truly random hash function in many applications including the above OPH. Here we first show improved concentration bounds for FH with truly random hashing and then argue that mixed tabulation performs similar when the input vectors are sparse. Our main contribution however is an experimental comparison of different hashing schemes when used inside FH OPH and LSH. We find that mixed tabulation hashing is almost as fast as the classic multiply-mod-prime scheme ax+b mod p. Mutiply-mod-prime is guaranteed to work well on sufficiently random data but we demonstrate that in the above applications it can lead to bias and poor concentration on both real-world and synthetic data. We also compare with the very popular MurmurHash3 which has no proven guarantees. Mixed tabulation and MurmurHash3 both perform similar to truly random hashing in our experiments. However mixed tabulation was 4037; faster than MurmurHash3 and it has the proven guarantee of good performance on all possible input making it more reliable.</p>\n", "tags": ["LSH", "NEURIPS", "Unsupervised"], "tsne_embedding": [-31.749980926513672, -10.020112991333008]}, {"key": "dai2017stochastic", "year": "2017", "title": "Stochastic Generative Hashing", "abstract": "<p>Learning-based binary hashing has become a powerful paradigm for fast search and retrieval in massive databases. However due to the requirement of discrete outputs for the hash functions learning such functions is known to be very challenging. In addition the objective functions adopted by existing hashing techniques are mostly chosen heuristically. In this paper we propose a novel generative approach to learn hash functions through Minimum Description Length principle such that the learned hash codes maximally compress the dataset and can also be used to regenerate the inputs. We also develop an efficient learning algorithm based on the stochastic distributional gradient which avoids the notorious difficulty caused by binary output constraints to jointly optimize the parameters of the hash function and the associated generative model. Extensive experiments on a variety of large-scale datasets show that the proposed method achieves better retrieval results than the existing state-of-the-art methods.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [0.31784284114837646, -6.983553886413574]}, {"key": "dai2023instructblip", "year": "2023", "title": "Instructblip Towards General-purpose Vision-language Models With Instruction Tuning", "abstract": "<p>Large-scale pre-training and instruction tuning have been successful at creating general-purpose language models with broad competence. However building general-purpose vision-language models is challenging due to the rich input distributions and task diversity resulting from the additional visual input. Although vision-language pretraining has been widely studied vision-language instruction tuning remains under-explored. In this paper we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pretrained BLIP-2 models. We gather 26 publicly available datasets covering a wide variety of tasks and capabilities and transform them into instruction tuning format. Additionally we introduce an instruction-aware Query Transformer which extracts informative features tailored to the given instruction. Trained on 13 held-in datasets InstructBLIP attains state-of-the-art zero-shot performance across all 13 held-out datasets substantially outperforming BLIP-2 and larger Flamingo models. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g. 90.737; accuracy on ScienceQA questions with image contexts). Furthermore we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models. All InstructBLIP models are open-sourced at https://github.com/salesforce/LAVIS/tree/main/projects/instructblip.</p>\n", "tags": ["ARXIV", "Cross Modal", "Has Code", "Survey Paper"], "tsne_embedding": [28.75633430480957, -1.226235032081604]}, {"key": "dai2023uncovering", "year": "2023", "title": "Uncovering Chatgpts Capabilities In Recommender Systems", "abstract": "<p>The debut of ChatGPT has recently attracted the attention of the natural language processing (NLP) community and beyond. Existing studies have demonstrated that ChatGPT shows significant improvement in a range of downstream NLP tasks but the capabilities and limitations of ChatGPT in terms of recommendations remain unclear. In this study we aim to conduct an empirical analysis of ChatGPTs recommendation ability from an Information Retrieval (IR) perspective including point-wise pair-wise and list-wise ranking. To achieve this goal we re-formulate the above three recommendation policies into a domain-specific prompt format. Through extensive experiments on four datasets from different domains we demonstrate that ChatGPT outperforms other large language models across all three ranking policies. Based on the analysis of unit cost improvements we identify that ChatGPT with list-wise ranking achieves the best trade-off between cost and performance compared to point-wise and pair-wise ranking. Moreover ChatGPT shows the potential for mitigating the cold start problem and explainable recommendation. To facilitate further explorations in this area the full code and detailed original results are open-sourced at https://github.com/rainym00d/LLM4RS.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [36.24415969848633, 1.107648491859436]}, {"key": "dao2024transformers", "year": "2024", "title": "Transformers Are Ssms Generalized Models And Efficient Algorithms Through Structured State Space Duality", "abstract": "<p>While Transformers have been the main architecture behind deep learnings success in language modeling state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related and develop a rich framework of theoretical connections between SSMs and variants of attention connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mambas selective SSM that is 2-8X faster while continuing to be competitive with Transformers on language modeling.</p>\n", "tags": ["ARXIV", "Deep Learning"], "tsne_embedding": [18.280303955078125, -13.623461723327637]}, {"key": "datar2004locality", "year": "2004", "title": "Locality-sensitive hashing scheme based on p-stable distributions", "abstract": "<p>We present a novel Locality-Sensitive Hashing scheme for the Approximate Nearest Neighbor Problem under lp norm, based on p-stable distributions.Our scheme improves the running time of the earlier algorithm for the case of the lp norm. It also yields the first known provably efficient approximate NN algorithm for the case p&lt;1. We also show that the algorithm finds the exact near neigbhor in O(log n) time for data satisfying certain \u201cbounded growth\u201d condition.Unlike earlier schemes, our LSH scheme works directly on points in the Euclidean space without embeddings. Consequently, the resulting query time bound is free of large factors and is simple and easy to implement. Our experiments (on synthetic data sets) show that the our data structure is up to 40 times faster than kd-tree.</p>\n", "tags": ["LSH"], "tsne_embedding": [-30.627164840698242, -4.963099479675293]}, {"key": "deepseekai2024deepseek", "year": "2024", "title": "Deepseek-v2 A Strong Economical And Efficient Mixture-of-experts Language Model", "abstract": "<p>We present DeepSeek-V2 a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters of which 21B are activated for each token and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B DeepSeek-V2 achieves significantly stronger performance and meanwhile saves 42.537; of training costs reduces the KV cache by 93.337; and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that even with only 21B activated parameters DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [15.324438095092773, -3.1391921043395996]}, {"key": "defranca2014iterative", "year": "2014", "title": "Iterative Universal Hash Function Generator For Minhashing", "abstract": "<p>Minhashing is a technique used to estimate the Jaccard Index between two sets by exploiting the probability of collision in a random permutation. In order to speed up the computation a random permutation can be approximated by using an universal hash function such as the h_ab function proposed by Carter and Wegman. A better estimate of the Jaccard Index can be achieved by using many of these hash functions created at random. In this paper a new iterative procedure to generate a set of h_ab functions is devised that eliminates the need for a list of random values and avoid the multiplication operation during the calculation. The properties of the generated hash functions remains that of an universal hash function family. This is possible due to the random nature of features occurrence on sparse datasets. Results show that the uniformity of hashing the features is maintaned while obtaining a speed up of up to 1.38 compared to the traditional approach.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-26.09819221496582, -5.493260860443115]}, {"key": "deherve2014perceptual", "year": "2014", "title": "A Perceptual Hash Function To Store And Retrieve Large Scale DNA Sequences", "abstract": "<p>This paper proposes a novel approach for storing and retrieving massive DNA sequences.. The method is based on a perceptual hash function commonly used to determine the similarity between digital images that we adapted for DNA sequences. Perceptual hash function presented here is based on a Discrete Cosine Transform Sign Only (DCT-SO). Each nucleotide is encoded as a fixed gray level intensity pixel and the hash is calculated from its significant frequency characteristics. This results to a drastic data reduction between the sequence and the perceptual hash. Unlike cryptographic hash functions perceptual hashes are not affected by avalanche effect and thus can be compared. The similarity distance between two hashes is estimated with the Hamming Distance which is used to retrieve DNA sequences. Experiments that we conducted show that our approach is relevant for storing massive DNA sequences and retrieving them.</p>\n", "tags": ["ARXIV", "Graph", "Independent"], "tsne_embedding": [-15.379371643066406, -16.919281005859375]}, {"key": "deng2019triplet", "year": "2019", "title": "Triplet-based Deep Hashing Network For Cross-modal Retrieval", "abstract": "<p>Given the benefits of its low storage requirements and high retrieval efficiency hashing has recently received increasing attention. In particularcross-modal hashing has been widely and successfully used in multimedia similarity search applications. However almost all existing methods employing cross-modal hashing cannot obtain powerful hash codes due to their ignoring the relative similarity between heterogeneous data that contains richer semantic information leading to unsatisfactory retrieval performance. In this paper we propose a triplet-based deep hashing (TDH) network for cross-modal retrieval. First we utilize the triplet labels which describes the relative relationships among three instances as supervision in order to capture more general semantic correlations between cross-modal instances. We then establish a loss function from the inter-modal view and the intra-modal view to boost the discriminative abilities of the hash codes. Finally graph regularization is introduced into our proposed TDH method to preserve the original semantic similarity between hash codes in Hamming space. Experimental results show that our proposed method outperforms several state-of-the-art approaches on two popular cross-modal datasets.</p>\n", "tags": ["ARXIV", "Cross Modal", "Graph", "Independent"], "tsne_embedding": [0.36192312836647034, 22.868576049804688]}, {"key": "deng2020twostream", "year": "2020", "title": "Two-Stream Deep Hashing With Class-Specific Centers for Supervised Image Search", "abstract": "<p>Hashing has been widely used for large-scale approximate nearest neighbor search due to its storage and search efficiency. Recent supervised hashing research has shown that deep learning-based methods can significantly outperform nondeep methods. Most existing supervised deep hashing methods exploit supervisory signals to generate similar and dissimilar image pairs for training. However, natural images can have large intraclass and small interclass variations, which may degrade the accuracy of hash codes. To address this problem, we propose a novel two-stream ConvNet architecture, which learns hash codes with class-specific representation centers. Our basic idea is that if we can learn a unified binary representation for each class as a center and encourage hash codes of images to be close to the corresponding centers, the intraclass variation will be greatly reduced. Accordingly, we design a neural network that leverages label information and outputs a unified binary representation for each class. Moreover, we also design an image network to learn hash codes from images and force these hash codes to be close to the corresponding class-specific centers. These two neural networks are then seamlessly incorporated to create a unified, end-to-end trainable framework. Extensive experiments on three popular benchmarks corroborate that our proposed method outperforms current state-of-the-art methods.</p>\n", "tags": ["Deep Learning", "Image Retrieval", "Supervised", "TNNLS"], "tsne_embedding": [-2.462186813354492, 8.17599105834961]}, {"key": "dettmers2023qlora", "year": "2023", "title": "Qlora Efficient Finetuning Of Quantized Llms", "abstract": "<p>We present QLoRA an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family which we name Guanaco outperforms all previous openly released models on the Vicuna benchmark reaching 99.337; of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance (a) 4-bit NormalFloat (NF4) a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1000 models providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets multiple model types (LLaMA T5) and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code including CUDA kernels for 4-bit training.</p>\n", "tags": ["ARXIV", "Quantisation"], "tsne_embedding": [8.523423194885254, -13.278656005859375]}, {"key": "devlin2018bert", "year": "2018", "title": "BERT Pre-training Of Deep Bidirectional Transformers For Language Understanding", "abstract": "<p>We introduce a new language representation model called BERT which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks such as question answering and language inference without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks including pushing the GLUE score to 80.537; (7.737; point absolute improvement) MultiNLI accuracy to 86.737; (4.637; absolute improvement) SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).</p>\n", "tags": ["ARXIV"], "tsne_embedding": [27.512346267700195, -8.1561918258667]}, {"key": "dey2010gb", "year": "2010", "title": "Gb-hash Hash Functions Using Groebner Basis", "abstract": "<p>In this paper we present an improved version of HF-hash viz. GB-hash Hash Functions Using Groebner Basis. In case of HF-hash the compression function consists of 32 polynomials with 64 variables which were taken from the first 32 polynomials of hidden field equations challenge-1 by forcing last 16 variables as 0. In GB-hash we have designed the compression function in such way that these 32 polynomials with 64 variables form a minimal Groebner basis of the ideal generated by them with respect to graded lexicographical (grlex) ordering as well as with respect to graded reverse lexicographical (grevlex) ordering. In this paper we will prove that GB-hash is more secure than HF-hash as well as more secure than SHA-256. We have also compared the efficiency of our GB-hash with SHA-256 and HF-hash.</p>\n", "tags": ["ARXIV", "Graph", "Independent"], "tsne_embedding": [-18.13640785217285, 2.5505261421203613]}, {"key": "dhingra2023mind", "year": "2023", "title": "Mind Meets Machine Unravelling Gpt-4s Cognitive Psychology", "abstract": "<p>Cognitive psychology delves on understanding perception attention memory language problem-solving decision-making and reasoning. Large language models (LLMs) are emerging as potent tools increasingly capable of performing human-level tasks. The recent development in the form of GPT-4 and its demonstrated success in tasks complex to humans exam and complex problems has led to an increased confidence in the LLMs to become perfect instruments of intelligence. Although GPT-4 report has shown performance on some cognitive psychology tasks a comprehensive assessment of GPT-4 via the existing well-established datasets is required. In this study we focus on the evaluation of GPT-4s performance on a set of cognitive psychology datasets such as CommonsenseQA SuperGLUE MATH and HANS. In doing so we understand how GPT-4 processes and integrates cognitive psychology with contextual information providing insight into the underlying cognitive processes that enable its ability to generate the responses. We show that GPT-4 exhibits a high level of accuracy in cognitive psychology tasks relative to the prior state-of-the-art models. Our results strengthen the already available assessments and confidence on GPT-4s cognitive psychology abilities. It has significant potential to revolutionize the field of AI by enabling machines to bridge the gap between human and machine reasoning.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [34.496944427490234, -10.443145751953125]}, {"key": "diao2023active", "year": "2023", "title": "Active Prompting With Chain-of-thought For Large Language Models", "abstract": "<p>The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs ability to produce high-quality answers. In particular an effective approach for complex question-and-answer tasks is example-based prompting with chain-of-thought (CoT) reasoning which significantly improves the performance of LLMs. However current CoT methods rely on a fixed set of human-annotated exemplars which are not necessarily the most effective examples for different tasks. This paper proposes a new method Active-Prompt to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose we propose a solution to the key problem of determining which questions are the most important and helpful ones to annotate from a pool of task-specific queries. By borrowing ideas from the related problem of uncertainty-based active learning we introduce several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation. Experimental results demonstrate the superiority of our proposed method achieving state-of-the-art on eight complex reasoning tasks. Further analyses of different uncertainty metrics pool sizes zero-shot learning and accuracy-uncertainty relationship demonstrate the effectiveness of our method. Our code will be available at https://github.com/shizhediao/active-prompt.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [38.29901123046875, -4.2731804847717285]}, {"key": "dias2022pattern", "year": "2022", "title": "Pattern Spotting And Image Retrieval In Historical Documents Using Deep Hashing", "abstract": "<p>This paper presents a deep learning approach for image retrieval and pattern spotting in digital collections of historical documents. First a region proposal algorithm detects object candidates in the document page images. Next deep learning models are used for feature extraction considering two distinct variants which provide either real-valued or binary code representations. Finally candidate images are ranked by computing the feature similarity with a given input query. A robust experimental protocol evaluates the proposed approach considering each representation scheme (real-valued and binary code) on the DocExplore image database. The experimental results show that the proposed deep models compare favorably to the state-of-the-art image retrieval approaches for images of historical documents outperforming other deep models by 2.56 percentage points using the same techniques for pattern spotting. Besides the proposed approach also reduces the search time by up to 200x and the storage cost up to 6000x when compared to related works based on real-valued representations.</p>\n", "tags": ["ARXIV", "Deep Learning", "Image Retrieval"], "tsne_embedding": [-15.632292747497559, 20.231481552124023]}, {"key": "ding2014collective", "year": "2014", "title": "Collective Matrix Factorization Hashing for Multimodal data", "abstract": "<p>Nearest neighbor search methods based on hashing have\nattracted considerable attention for effective and efficient\nlarge-scale similarity search in computer vision and information\nretrieval community. In this paper, we study the\nproblems of learning hash functions in the context of multimodal\ndata for cross-view similarity search. We put forward\na novel hashing method, which is referred to Collective\nMatrix Factorization Hashing (CMFH). CMFH learns unified\nhash codes by collective matrix factorization with latent\nfactor model from different modalities of one instance,\nwhich can not only supports cross-view search but also increases\nthe search accuracy by merging multiple view information\nsources. We also prove that CMFH, a similaritypreserving\nhashing learning method, has upper and lower\nboundaries. Extensive experiments verify that CMFH significantly\noutperforms several state-of-the-art methods on\nthree different datasets.</p>\n", "tags": ["CVPR", "Cross Modal"], "tsne_embedding": [-16.949604034423828, -0.9886215925216675]}, {"key": "ding2015knn", "year": "2015", "title": "kNN Hashing with Factorized Neighborhood Representation", "abstract": "<p>Hashing is very effective for many tasks in reducing the\nprocessing time and in compressing massive databases. Although lots of approaches have been developed to learn\ndata-dependent hash functions in recent years, how to learn\nhash functions to yield good performance with acceptable\ncomputational and memory cost is still a challenging problem. Based on the observation that retrieval precision is\nhighly related to the kNN classification accuracy, this paper\nproposes a novel kNN-based supervised hashing method,\nwhich learns hash functions by directly maximizing the kNN\naccuracy of the Hamming-embedded training data. To make\nit scalable well to large problem, we propose a factorized\nneighborhood representation to parsimoniously model the\nneighborhood relationships inherent in training data. Considering that real-world data are often linearly inseparable,\nwe further kernelize this basic model to improve its performance. As a result, the proposed method is able to learn\naccurate hashing functions with tolerable computation and\nstorage cost. Experiments on four benchmarks demonstrate\nthat our method outperforms the state-of-the-arts.</p>\n", "tags": ["ICCV", "Supervised"], "tsne_embedding": [2.927060604095459, -8.154351234436035]}, {"key": "ding2018mean", "year": "2018", "title": "Mean Local Group Average Precision (mlgap) A New Performance Metric For Hashing-based Retrieval", "abstract": "<p>The research on hashing techniques for visual data is gaining increased attention in recent years due to the need for compact representations supporting efficient search/retrieval in large-scale databases such as online images. Among many possibilities Mean Average Precision(mAP) has emerged as the dominant performance metric for hashing-based retrieval. One glaring shortcoming of mAP is its inability in balancing retrieval accuracy and utilization of hash codes pushing a system to attain higher mAP will inevitably lead to poorer utilization of the hash codes. Poor utilization of the hash codes hinders good retrieval because of increased collision of samples in the hash space. This means that a model giving a higher mAP values does not necessarily do a better job in retrieval. In this paper we introduce a new metric named Mean Local Group Average Precision (mLGAP) for better evaluation of the performance of hashing-based retrieval. The new metric provides a retrieval performance measure that also reconciles the utilization of hash codes leading to a more practically meaningful performance metric than conventional ones like mAP. To this end we start by mathematical analysis of the deficiencies of mAP for hashing-based retrieval. We then propose mLGAP and show why it is more appropriate for hashing-based retrieval. Experiments on image retrieval are used to demonstrate the effectiveness of the proposed metric.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Independent"], "tsne_embedding": [-16.154529571533203, 8.72173023223877]}, {"key": "ding2019bilinear", "year": "2019", "title": "Bilinear Supervised Hashing Based On 2D Image Features", "abstract": "<p>Hashing has been recognized as an efficient representation learning method to effectively handle big data due to its low computational complexity and memory cost. Most of the existing hashing methods focus on learning the low-dimensional vectorized binary features based on the high-dimensional raw vectorized features. However studies on how to obtain preferable binary codes from the original 2D image features for retrieval is very limited. This paper proposes a bilinear supervised discrete hashing (BSDH) method based on 2D image features which utilizes bilinear projections to binarize the image matrix features such that the intrinsic characteristics in the 2D image space are preserved in the learned binary codes. Meanwhile the bilinear projection approximation and vectorization binary codes regression are seamlessly integrated together to formulate the final robust learning framework. Furthermore a discrete optimization strategy is developed to alternatively update each variable for obtaining the high-quality binary codes. In addition two 2D image features traditional SURF-based FVLAD feature and CNN-based AlexConv5 feature are designed for further improving the performance of the proposed BSDH method. Results of extensive experiments conducted on four benchmark datasets show that the proposed BSDH method almost outperforms all competing hashing methods with different input features by different evaluation protocols.</p>\n", "tags": ["ARXIV", "CNN", "Supervised"], "tsne_embedding": [2.079970121383667, 1.6025733947753906]}, {"key": "ding2023can", "year": "2023", "title": "Gpt4image Can Large Pre-trained Models Help Vision Models On Perception Tasks", "abstract": "<p>The recent upsurge in pre-trained large models (e.g. GPT-4) has swept across the entire deep learning community. Such powerful large language models (LLMs) demonstrate advanced generative ability and multimodal understanding capability which quickly achieve new state-of-the-art performances on a variety of benchmarks. The pre-trained LLM usually plays the role as a universal AI model that can conduct various tasks including context reasoning article analysis and image content comprehension. However considering the prohibitively high memory and computational cost for implementing such a large model the conventional models (such as CNN and ViT) are still essential for many visual perception tasks. In this paper we propose to enhance the representation ability of ordinary vision models for perception tasks (e.g. image classification) by taking advantage of large pre-trained models. We present a new learning paradigm in which the knowledge extracted from large pre-trained models are utilized to help models like CNN and ViT learn enhanced representations and achieve better performance. Firstly we curate a high quality description set by prompting a multimodal LLM to generate descriptive text for all training images. Furthermore we feed these detailed descriptions into a pre-trained encoder to extract text embeddings with rich semantic information that encodes the content of images. During training text embeddings will serve as extra supervising signals and be aligned with image representations learned by vision models. The alignment process helps vision models learn better and achieve higher accuracy with the assistance of pre-trained LLMs. We conduct extensive experiments to verify that the proposed algorithm consistently improves the performance for various vision models with heterogeneous architectures.</p>\n", "tags": ["ARXIV", "CNN", "Cross Modal", "Deep Learning", "Supervised"], "tsne_embedding": [32.178714752197266, -11.709651947021484]}, {"key": "do2015discrete", "year": "2015", "title": "Discrete Hashing With Deep Neural Network", "abstract": "<p>This paper addresses the problem of learning binary hash codes for large scale image search by proposing a novel hashing method based on deep neural network. The advantage of our deep model over previous deep model used in hashing is that our model contains necessary criteria for producing good codes such as similarity preserving balance and independence. Another advantage of our method is that instead of relaxing the binary constraint of codes during the learning process as most previous works in this paper by introducing the auxiliary variable we reformulate the optimization into two sub-optimization steps allowing us to efficiently solve binary constraints without any relaxation. The proposed method is also extended to the supervised hashing by leveraging the label information such that the learned binary codes preserve the pairwise label of inputs. The experimental results on three benchmark datasets show the proposed methods outperform state-of-the-art hashing methods.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [-1.0313163995742798, 7.904422283172607]}, {"key": "do2016embedding", "year": "2016", "title": "Embedding Based On Function Approximation For Large Scale Image Search", "abstract": "<p>The objective of this paper is to design an embedding method that maps local features describing an image (e.g. SIFT) to a higher dimensional representation useful for the image retrieval problem. First motivated by the relationship between the linear approximation of a nonlinear function in high dimensional space and the stateof-the-art feature representation used in image retrieval i.e. VLAD we propose a new approach for the approximation. The embedded vectors resulted by the function approximation process are then aggregated to form a single representation for image retrieval. Second in order to make the proposed embedding method applicable to large scale problem we further derive its fast version in which the embedded vectors can be efficiently computed i.e. in the closed-form. We compare the proposed embedding methods with the state of the art in the context of image search under various settings when the images are represented by medium length vectors short vectors or binary vectors. The experimental results show that the proposed embedding methods outperform existing the state of the art on the standard public image retrieval benchmarks.</p>\n", "tags": ["ARXIV", "Image Retrieval"], "tsne_embedding": [-9.617533683776855, 21.103580474853516]}, {"key": "do2016learning", "year": "2016", "title": "Learning To Hash With Binary Deep Neural Network", "abstract": "<p>This work proposes deep network models and learning algorithms for unsupervised and supervised binary hashing. Our novel network design constrains one hidden layer to directly output the binary codes. This addresses a challenging issue in some previous works optimizing non-smooth objective functions due to binarization. Moreover we incorporate independence and balance properties in the direct and strict forms in the learning. Furthermore we include similarity preserving property in our objective function. Our resulting optimization with these binary independence and balance constraints is difficult to solve. We propose to attack it with alternating optimization and careful relaxation. Experimental results on three benchmark datasets show that our proposed methods compare favorably with the state of the art.</p>\n", "tags": ["ARXIV", "Unsupervised"], "tsne_embedding": [-2.258410930633545, -14.876835823059082]}, {"key": "do2017compact", "year": "2017", "title": "Compact Hash Code Learning With Binary Deep Neural Network", "abstract": "<p>Learning compact binary codes for image retrieval problem using deep neural networks has recently attracted increasing attention. However training deep hashing networks is challenging due to the binary constraints on the hash codes. In this paper we propose deep network models and learning algorithms for learning binary hash codes given image representations under both unsupervised and supervised manners. The novelty of our network design is that we constrain one hidden layer to directly output the binary codes. This design has overcome a challenging problem in some previous works optimizing non-smooth objective functions because of binarization. In addition we propose to incorporate independence and balance properties in the direct and strict forms into the learning schemes. We also include a similarity preserving property in our objective functions. The resulting optimizations involving these binary independence and balance constraints are difficult to solve. To tackle this difficulty we propose to learn the networks with alternating optimization and careful relaxation. Furthermore by leveraging the powerful capacity of convolutional neural networks we propose an end-to-end architecture that jointly learns to extract visual features and produce binary hash codes. Experimental results for the benchmark datasets show that the proposed methods compare favorably or outperform the state of the art.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Unsupervised"], "tsne_embedding": [0.09703924506902695, 1.111426591873169]}, {"key": "do2017simultaneous", "year": "2017", "title": "Simultaneous Feature Aggregating And Hashing For Large-scale Image Search", "abstract": "<p>In most state-of-the-art hashing-based visual search systems local image descriptors of an image are first aggregated as a single feature vector. This feature vector is then subjected to a hashing function that produces a binary hash code. In previous work the aggregating and the hashing processes are designed independently. In this paper we propose a novel framework where feature aggregating and hashing are designed simultaneously and optimized jointly. Specifically our joint optimization produces aggregated representations that can be better reconstructed by some binary codes. This leads to more discriminative binary hash codes and improved retrieval accuracy. In addition we also propose a fast version of the recently-proposed Binary Autoencoder to be used in our proposed framework. We perform extensive retrieval experiments on several benchmark datasets with both SIFT and convolutional features. Our results suggest that the proposed framework achieves significant improvements over the state of the art.</p>\n", "tags": ["ARXIV", "Unsupervised"], "tsne_embedding": [-14.543601036071777, 13.166170120239258]}, {"key": "do2018binary", "year": "2018", "title": "Binary Constrained Deep Hashing Network For Image Retrieval Without Manual Annotation", "abstract": "<p>Learning compact binary codes for image retrieval task using deep neural networks has attracted increasing attention recently. However training deep hashing networks for the task is challenging due to the binary constraints on the hash codes the similarity preserving property and the requirement for a vast amount of labelled images. To the best of our knowledge none of the existing methods has tackled all of these challenges completely in a unified framework. In this work we propose a novel end-to-end deep learning approach for the task in which the network is trained to produce binary codes directly from image pixels without the need of manual annotation. In particular to deal with the non-smoothness of binary constraints we propose a novel pairwise constrained loss function which simultaneously encodes the distances between pairs of hash codes and the binary quantization error. In order to train the network with the proposed loss function we propose an efficient parameter learning algorithm. In addition to provide similar / dissimilar training images to train the network we exploit 3D models reconstructed from unlabelled images for automatic generation of enormous training image pairs. The extensive experiments on image retrieval benchmark datasets demonstrate the improvements of the proposed method over the state-of-the-art compact representation methods on the image retrieval problem.</p>\n", "tags": ["ARXIV", "Deep Learning", "Image Retrieval", "Quantisation", "Supervised"], "tsne_embedding": [-0.11535818874835968, 1.171536922454834]}, {"key": "do2018from", "year": "2018", "title": "From Selective Deep Convolutional Features To Compact Binary Representations For Image Retrieval", "abstract": "<p>In the large-scale image retrieval task the two most important requirements are the discriminability of image representations and the efficiency in computation and storage of representations. Regarding the former requirement Convolutional Neural Network (CNN) is proven to be a very powerful tool to extract highly discriminative local descriptors for effective image search. Additionally in order to further improve the discriminative power of the descriptors recent works adopt fine-tuned strategies. In this paper taking a different approach we propose a novel computationally efficient and competitive framework. Specifically we firstly propose various strategies to compute masks namely SIFT-mask SUM-mask and MAX-mask to select a representative subset of local convolutional features and eliminate redundant features. Our in-depth analyses demonstrate that proposed masking schemes are effective to address the burstiness drawback and improve retrieval accuracy. Secondly we propose to employ recent embedding and aggregating methods which can significantly boost the feature discriminability. Regarding the computation and storage efficiency we include a hashing module to produce very compact binary image representations. Extensive experiments on six image retrieval benchmarks demonstrate that our proposed framework achieves the state-of-the-art retrieval performances.</p>\n", "tags": ["ARXIV", "CNN", "Image Retrieval", "Supervised"], "tsne_embedding": [-8.851749420166016, 16.322284698486328]}, {"key": "do2019simultaneous", "year": "2019", "title": "Simultaneous Feature Aggregating And Hashing For Compact Binary Code Learning", "abstract": "<p>Representing images by compact hash codes is an attractive approach for large-scale content-based image retrieval. In most state-of-the-art hashing-based image retrieval systems for each image local descriptors are first aggregated as a global representation vector. This global vector is then subjected to a hashing function to generate a binary hash code. In previous works the aggregating and the hashing processes are designed independently. Hence these frameworks may generate suboptimal hash codes. In this paper we first propose a novel unsupervised hashing framework in which feature aggregating and hashing are designed simultaneously and optimized jointly. Specifically our joint optimization generates aggregated representations that can be better reconstructed by some binary codes. This leads to more discriminative binary hash codes and improved retrieval accuracy. In addition the proposed method is flexible. It can be extended for supervised hashing. When the data label is available the framework can be adapted to learn binary codes which minimize the reconstruction loss w.r.t. label vectors. Furthermore we also propose a fast version of the state-of-the-art hashing method Binary Autoencoder to be used in our proposed frameworks. Extensive experiments on benchmark datasets under various settings show that the proposed methods outperform state-of-the-art unsupervised and supervised hashing methods.</p>\n", "tags": ["ARXIV", "Deep Learning", "Image Retrieval", "Unsupervised"], "tsne_embedding": [-14.391508102416992, 12.82272720336914]}, {"key": "doan2020image", "year": "2020", "title": "Image Hashing By Minimizing Discrete Component-wise Wasserstein Distance", "abstract": "<p>Image hashing is one of the fundamental problems that demand both efficient and effective solutions for various practical scenarios. Adversarial autoencoders are shown to be able to implicitly learn a robust locality-preserving hash function that generates balanced and high-quality hash codes. However the existing adversarial hashing methods are inefficient to be employed for large-scale image retrieval applications. Specifically they require an exponential number of samples to be able to generate optimal hash codes and a significantly high computational cost to train. In this paper we show that the high sample-complexity requirement often results in sub-optimal retrieval performance of the adversarial hashing methods. To address this challenge we propose a new adversarial-autoencoder hashing approach that has a much lower sample requirement and computational cost. Specifically by exploiting the desired properties of the hash function in the low-dimensional discrete space our method efficiently estimates a better variant of Wasserstein distance by averaging a set of easy-to-compute one-dimensional Wasserstein distances. The resulting hashing approach has an order-of-magnitude better sample complexity thus better generalization property compared to the other adversarial hashing methods. In addition the computational cost is significantly reduced using our approach. We conduct experiments on several real-world datasets and show that the proposed method outperforms the competing hashing methods achieving up to 1037; improvement over the current state-of-the-art image hashing methods. The code accompanying this paper is available on Github (https://github.com/khoadoan/adversarial-hashing).</p>\n", "tags": ["ARXIV", "Has Code", "Image Retrieval", "Unsupervised"], "tsne_embedding": [-4.502078056335449, 0.6622400283813477]}, {"key": "doan2022asymmetric", "year": "2022", "title": "Asymmetric Hashing For Fast Ranking Via Neural Network Measures", "abstract": "<p>Fast item ranking is an important task in recommender systems. In previous works graph-based Approximate Nearest Neighbor (ANN) approaches have demonstrated good performance on item ranking tasks with generic searching/matching measures (including complex measures such as neural network measures). However since these ANN approaches must go through the neural measures several times during ranking the computation is not practical if the neural measure is a large network. On the other hand fast item ranking using existing hashing-based approaches such as Locality Sensitive Hashing (LSH) only works with a limited set of measures. Previous learning-to-hash approaches are also not suitable to solve the fast item ranking problem since they can take a significant amount of time and computation to train the hash functions. Hashing approaches however are attractive because they provide a principle and efficient way to retrieve candidate items. In this paper we propose a simple and effective learning-to-hash approach for the fast item ranking problem that can be used for any type of measure including neural network measures. Specifically we solve this problem with an asymmetric hashing framework based on discrete inner product fitting. We learn a pair of related hash functions that map heterogeneous objects (e.g. users and items) into a common discrete space where the inner product of their binary codes reveals their true similarity defined via the original searching measure. The fast ranking problem is reduced to an ANN search via this asymmetric hashing scheme. Then we propose a sampling strategy to efficiently select relevant and contrastive samples to train the hashing model. We empirically validate the proposed method against the existing state-of-the-art fast item ranking methods in several combinations of non-linear searching functions and prominent datasets.</p>\n", "tags": ["ARXIV", "Cross Modal", "Graph", "LSH", "Supervised"], "tsne_embedding": [6.955049514770508, -4.048369407653809]}, {"key": "doan2022coophash", "year": "2022", "title": "Coophash Cooperative Learning Of Multipurpose Descriptor And Contrastive Pair Generator Via Variational MCMC Teaching For Supervised Image Hashing", "abstract": "<p>Leveraging supervised information can lead to superior retrieval performance in the image hashing domain but the performance degrades significantly without enough labeled data. One effective solution to boost performance is to employ generative models such as Generative Adversarial Networks (GANs) to generate synthetic data in an image hashing model. However GAN-based methods are difficult to train which prevents the hashing approaches from jointly training the generative models and the hash functions. This limitation results in sub-optimal retrieval performance. To overcome this limitation we propose a novel framework the generative cooperative hashing network which is based on energy-based cooperative learning. This framework jointly learns a powerful generative representation of the data and a robust hash function via two components a top-down contrastive pair generator that synthesizes contrastive images and a bottom-up multipurpose descriptor that simultaneously represents the images from multiple perspectives including probability density hash code latent code and category. The two components are jointly learned via a novel likelihood-based cooperative learning scheme. We conduct experiments on several real-world datasets and show that the proposed method outperforms the competing hashing supervised methods achieving up to 1037; relative improvement over the current state-of-the-art supervised hashing methods and exhibits a significantly better performance in out-of-distribution retrieval.</p>\n", "tags": ["ARXIV", "GAN", "Supervised"], "tsne_embedding": [3.096991539001465, 4.140296459197998]}, {"key": "doan2022one", "year": "2022", "title": "One Loss For Quantization Deep Hashing With Discrete Wasserstein Distributional Matching", "abstract": "<p>Image hashing is a principled approximate nearest neighbor approach to find similar items to a query in a large collection of images. Hashing aims to learn a binary-output function that maps an image to a binary vector. For optimal retrieval performance producing balanced hash codes with low-quantization error to bridge the gap between the learning stages continuous relaxation and the inference stages discrete quantization is important. However in the existing deep supervised hashing methods coding balance and low-quantization error are difficult to achieve and involve several losses. We argue that this is because the existing quantization approaches in these methods are heuristically constructed and not effective to achieve these objectives. This paper considers an alternative approach to learning the quantization constraints. The task of learning balanced codes with low quantization error is re-formulated as matching the learned distribution of the continuous codes to a pre-defined discrete uniform distribution. This is equivalent to minimizing the distance between two distributions. We then propose a computationally efficient distributional distance by leveraging the discrete property of the hash functions. This distributional distance is a valid distance and enjoys lower time and sample complexities. The proposed single-loss quantization objective can be integrated into any existing supervised hashing method to improve code balance and quantization error. Experiments confirm that the proposed approach substantially improves the performance of several representative hashing~methods.</p>\n", "tags": ["ARXIV", "Quantisation", "Supervised"], "tsne_embedding": [-8.72949504852295, 4.17427396774292]}, {"key": "dohan2022language", "year": "2022", "title": "Language Model Cascades", "abstract": "<p>Prompted models have demonstrated impressive few-shot learning abilities. Repeated interactions at test-time with a single model or the composition of multiple models together further expands capabilities. These compositions are probabilistic models and may be expressed in the language of graphical models with random variables whose values are complex data types such as strings. Cases with control flow and dynamic structure require techniques from probabilistic programming which allow implementing disparate model structures and inference strategies in a unified language. We formalize several existing techniques from this perspective including scratchpads / chain of thought verifiers STaR selection-inference and tool use. We refer to the resulting programs as language model cascades.</p>\n", "tags": ["ARXIV", "Graph", "Independent"], "tsne_embedding": [21.745540618896484, -16.106521606445312]}, {"key": "dolhansky2020adversarial", "year": "2020", "title": "Adversarial Collision Attacks On Image Hashing Functions", "abstract": "<p>Hashing images with a perceptual algorithm is a common approach to solving duplicate image detection problems. However perceptual image hashing algorithms are differentiable and are thus vulnerable to gradient-based adversarial attacks. We demonstrate that not only is it possible to modify an image to produce an unrelated hash but an exact image hash collision between a source and target image can be produced via minuscule adversarial perturbations. In a white box setting these collisions can be replicated across nearly every image pair and hash type (including both deep and non-learned hashes). Furthermore by attacking points other than the output of a hashing function an attacker can avoid having to know the details of a particular algorithm resulting in collisions that transfer across different hash sizes or model architectures. Using these techniques an adversary can poison the image lookup table of a duplicate image detection service resulting in undefined or unwanted behavior. Finally we offer several potential mitigations to gradient-based image hash attacks.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-16.741336822509766, 13.408863067626953]}, {"key": "domnita2020genetic", "year": "2020", "title": "A Genetic Algorithm For Obtaining Memory Constrained Near-perfect Hashing", "abstract": "<p>The problem of fast items retrieval from a fixed collection is often encountered in most computer science areas from operating system components to databases and user interfaces. We present an approach based on hash tables that focuses on both minimizing the number of comparisons performed during the search and minimizing the total collection size. The standard open-addressing double-hashing approach is improved with a non-linear transformation that can be parametrized in order to ensure a uniform distribution of the data in the hash table. The optimal parameter is determined using a genetic algorithm. The paper results show that near-perfect hashing is faster than binary search yet uses less memory than perfect hashing being a good choice for memory-constrained applications where search time is also critical.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-23.3681583404541, -12.578505516052246]}, {"key": "dong2017video", "year": "2017", "title": "Video Retrieval Based On Deep Convolutional Neural Network", "abstract": "<p>Recently with the enormous growth of online videos fast video retrieval research has received increasing attention. As an extension of image hashing techniques traditional video hashing methods mainly depend on hand-crafted features and transform the real-valued features into binary hash codes. As videos provide far more diverse and complex visual information than images extracting features from videos is much more challenging than that from images. Therefore high-level semantic features to represent videos are needed rather than low-level hand-crafted methods. In this paper a deep convolutional neural network is proposed to extract high-level semantic features and a binary hash function is then integrated into this framework to achieve an end-to-end optimization. Particularly our approach also combines triplet loss function which preserves the relative similarity and difference of videos and classification loss function as the optimization objective. Experiments have been performed on two public datasets and the results demonstrate the superiority of our proposed method compared with other state-of-the-art video retrieval methods.</p>\n", "tags": ["ARXIV", "Supervised", "Video Retrieval"], "tsne_embedding": [-6.804123878479004, -21.239912033081055]}, {"key": "dong2019document", "year": "2019", "title": "Document Hashing With Mixture-prior Generative Models", "abstract": "<p>Hashing is promising for large-scale information retrieval tasks thanks to the efficiency of distance evaluation between binary codes. Generative hashing is often used to generate hashing codes in an unsupervised way. However existing generative hashing methods only considered the use of simple priors like Gaussian and Bernoulli priors which limits these methods to further improve their performance. In this paper two mixture-prior generative models are proposed under the objective to produce high-quality hashing codes for documents. Specifically a Gaussian mixture prior is first imposed onto the variational auto-encoder (VAE) followed by a separate step to cast the continuous latent representation of VAE into binary code. To avoid the performance loss caused by the separate casting a model using a Bernoulli mixture prior is further developed in which an end-to-end training is admitted by resorting to the straight-through (ST) discrete gradient estimator. Experimental results on several benchmark datasets demonstrate that the proposed methods especially the one using Bernoulli mixture priors consistently outperform existing ones by a substantial margin.</p>\n", "tags": ["ARXIV", "Unsupervised"], "tsne_embedding": [-10.125831604003906, 2.662292957305908]}, {"key": "dong2020learning", "year": "2020", "title": "Learning Space Partitions for Nearest Neighbor Search", "abstract": "<p>Space partitions of underlie a vast and important\nclass of fast nearest neighbor search (NNS) algorithms. Inspired by recent theoretical work on NNS for general metric spaces (Andoni et al. 2018b,c), we develop a new framework for building space partitions reducing the problem to balanced graph partitioning followed by supervised classification.\nWe instantiate this general approach with the KaHIP graph partitioner (Sanders and Schulz 2013) and neural networks, respectively, to obtain a new partitioning procedure called Neural Locality-Sensitive Hashing (Neural LSH). On several standard benchmarks for NNS (Aumuller et al. 2017), our experiments show that the partitions obtained by Neural LSH consistently outperform partitions found by quantization-based and tree-based methods as well as classic, data-oblivious LSH.</p>\n\n", "tags": ["Deep Learning", "Has Code", "Supervised"], "tsne_embedding": [-5.9459404945373535, 28.297639846801758]}, {"key": "dong2022survey", "year": "2022", "title": "A Survey On In-context Learning", "abstract": "<p>With the increasing capabilities of large language models (LLMs) in-context learning (ICL) has emerged as a new paradigm for natural language processing (NLP) where LLMs make predictions based on contexts augmented with a few examples. It has been a significant trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then we organize and discuss advanced techniques including training strategies prompt designing strategies and related analysis. Additionally we explore various ICL application scenarios such as data engineering and knowledge updating. Finally we address the challenges of ICL and suggest potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.</p>\n", "tags": ["ARXIV", "Survey Paper"], "tsne_embedding": [36.987815856933594, -13.120340347290039]}, {"key": "douze2016polysemous", "year": "2016", "title": "Polysemous Codes", "abstract": "<p>This paper considers the problem of approximate nearest neighbor search in the compressed domain. We introduce polysemous codes which offer both the distance estimation quality of product quantization and the efficient comparison of binary codes with Hamming distance. Their design is inspired by algorithms introduced in the 90s to construct channel-optimized vector quantizers. At search time this dual interpretation accelerates the search. Most of the indexed vectors are filtered out with Hamming distance letting only a fraction of the vectors to be ranked with an asymmetric distance estimator. The method is complementary with a coarse partitioning of the feature space such as the inverted multi-index. This is shown by our experiments performed on several public benchmarks such as the BIGANN dataset comprising one billion vectors for which we report state-of-the-art results for query times below 0.3millisecond per core. Last but not least our approach allows the approximate computation of the k-NN graph associated with the Yahoo Flickr Creative Commons 100M described by CNN image descriptors in less than 8 hours on a single machine.</p>\n", "tags": ["ARXIV", "CNN", "Graph", "Quantisation"], "tsne_embedding": [-21.22149085998535, 4.796317100524902]}, {"key": "driess2023palm", "year": "2023", "title": "Palm-e An Embodied Multimodal Language Model", "abstract": "<p>Large language models excel at a wide range of complex tasks. However enabling general inference in the real world e.g. for robotics problems raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual continuous state estimation and textual input encodings. We train these encodings end-to-end in conjunction with a pre-trained large language model for multiple embodied tasks including sequential robotic manipulation planning visual question answering and captioning. Our evaluations show that PaLM-E a single large embodied multimodal model can address a variety of embodied reasoning tasks from a variety of observation modalities on multiple embodiments and further exhibits positive transfer the model benefits from diverse joint training across internet-scale language vision and visual-language domains. Our largest model PaLM-E-562B with 562B parameters in addition to being trained on robotics tasks is a visual-language generalist with state-of-the-art performance on OK-VQA and retains generalist language capabilities with increasing scale.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [30.021114349365234, -12.870845794677734]}, {"key": "drori2022from", "year": "2022", "title": "From Human Days To Machine Seconds Automatically Answering And Generating Machine Learning Final Exams", "abstract": "<p>A final exam in machine learning at a top institution such as MIT Harvard or Cornell typically takes faculty days to write and students hours to solve. We demonstrate that large language models pass machine learning finals at a human level on finals available online after the models were trained and automatically generate new human-quality final exam questions in seconds. Previous work has developed program synthesis and few-shot learning methods to solve university-level problem set questions in mathematics and STEM courses. In this work we develop and compare methods that solve final exams which differ from problem sets in several ways the questions are longer have multiple parts are more complicated and span a broader set of topics. We curate a dataset and benchmark of questions from machine learning final exams available online and code for answering these questions and generating new questions. We show how to generate new questions from other questions and course notes. For reproducibility and future research on this final exam benchmark we use automatic checkers for multiple-choice numeric and questions with expression answers. We perform ablation studies comparing zero-shot learning with few-shot learning and chain-of-thought prompting using GPT-3 OPT Codex and ChatGPT across machine learning topics and find that few-shot learning methods perform best. We highlight the transformative potential of language models to streamline the writing and solution of large-scale assessments significantly reducing the workload from human days to mere machine seconds. Our results suggest that rather than banning large language models such as ChatGPT in class instructors should teach students to harness them by asking students meta-questions about correctness completeness and originality of the responses generated encouraging critical thinking in academic studies.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [26.58657455444336, 3.4671857357025146]}, {"key": "drozdov2022compositional", "year": "2022", "title": "Compositional Semantic Parsing With Large Language Models", "abstract": "<p>Humans can reason compositionally when presented with new tasks. Previous research shows that appropriate prompting techniques enable large language models (LLMs) to solve artificial compositional generalization tasks such as SCAN. In this work we identify additional challenges in more realistic semantic parsing tasks with larger vocabulary and refine these prompting techniques to address them. Our best method is based on least-to-most prompting it decomposes the problem using prompting-based syntactic parsing then uses this decomposition to select appropriate exemplars and to sequentially generate the semantic parse. This method allows us to set a new state of the art for CFQ while requiring only 137; of the training data used by traditional approaches. Due to the general nature of our approach we expect similar efforts will lead to new results in other tasks and domains especially for knowledge-intensive applications.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [28.130151748657227, -5.77891206741333]}, {"key": "du2021glam", "year": "2021", "title": "Glam Efficient Scaling Of Language Models With Mixture-of-experts", "abstract": "<p>Scaling language models with more data compute and parameters has driven significant progress in natural language processing. For example thanks to scaling GPT-3 was able to achieve strong results on in-context learning tasks. However training these large dense models requires significant amounts of computing resources. In this paper we propose and develop a family of language models named GLaM (Generalist Language Model) which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 trillion parameters which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference while still achieving better overall zero-shot and one-shot performance across 29 NLP tasks.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [18.263484954833984, -3.4193806648254395]}, {"key": "du2024large", "year": "2024", "title": "Large Language Model With Graph Convolution For Recommendation", "abstract": "<p>In recent years efforts have been made to use text information for better user profiling and item characterization in recommendations. However text information can sometimes be of low quality hindering its effectiveness for real-world applications. With knowledge and reasoning capabilities capsuled in Large Language Models (LLMs) utilizing LLMs emerges as a promising way for description improvement. However existing ways of prompting LLMs with raw texts ignore structured knowledge of user-item interactions which may lead to hallucination problems like inconsistent description generation. To this end we propose a Graph-aware Convolutional LLM method to elicit LLMs to capture high-order relations in the user-item graph. To adapt text-based LLMs with structured graphs We use the LLM as an aggregator in graph processing allowing it to understand graph-based information step by step. Specifically the LLM is required for description enhancement by exploring multi-hop neighbors layer by layer thereby propagating information progressively in the graph. To enable LLMs to capture large-scale graph information we break down the description task into smaller parts which drastically reduces the context length of the token input with each step. Extensive experiments on three real-world datasets show that our method consistently outperforms state-of-the-art methods.</p>\n", "tags": ["ARXIV", "Graph"], "tsne_embedding": [1.5122969150543213, 26.342802047729492]}, {"key": "dubey2021vision", "year": "2021", "title": "Vision Transformer Hashing For Image Retrieval", "abstract": "<p>Deep learning has shown a tremendous growth in hashing techniques for image retrieval. Recently Transformer has emerged as a new architecture by utilizing self-attention without convolution. Transformer is also extended to Vision Transformer (ViT) for the visual recognition with a promising performance on ImageNet. In this paper we propose a Vision Transformer based Hashing (VTS) for image retrieval. We utilize the pre-trained ViT on ImageNet as the backbone network and add the hashing head. The proposed VTS model is fine tuned for hashing under six different image retrieval frameworks including Deep Supervised Hashing (DSH) HashNet GreedyHash Improved Deep Hashing Network (IDHN) Deep Polarized Network (DPN) and Central Similarity Quantization (CSQ) with their objective functions. We perform the extensive experiments on CIFAR10 ImageNet NUS-Wide and COCO datasets. The proposed VTS based image retrieval outperforms the recent state-of-the-art hashing techniques with a great margin. We also find the proposed VTS model as the backbone network is better than the existing networks such as AlexNet and ResNet. The code is released at urlhttps://github.com/shivram1987/VisionTransformerHashing}.</p>\n", "tags": ["ARXIV", "Deep Learning", "Has Code", "Image Retrieval", "Quantisation", "Supervised"], "tsne_embedding": [6.315980434417725, 8.377443313598633]}, {"key": "dubey2024llama", "year": "2024", "title": "The Llama 3 Herd Of Models", "abstract": "<p>Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models called Llama 3. It is a herd of language models that natively support multilinguality coding reasoning and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3 including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image video and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image video and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [35.73291778564453, 0.10037324577569962]}, {"key": "dubey2024transformer", "year": "2024", "title": "Transformer-based Clipped Contrastive Quantization Learning For Unsupervised Image Retrieval", "abstract": "<p>Unsupervised image retrieval aims to learn the important visual characteristics without any given level to retrieve the similar images for a given query image. The Convolutional Neural Network (CNN)-based approaches have been extensively exploited with self-supervised contrastive learning for image hashing. However the existing approaches suffer due to lack of effective utilization of global features by CNNs and biased-ness created by false negative pairs in the contrastive learning. In this paper we propose a TransClippedCLR model by encoding the global context of an image using Transformer having local context through patch based processing by generating the hash codes through product quantization and by avoiding the potential false negative pairs through clipped contrastive learning. The proposed model is tested with superior performance for unsupervised image retrieval on benchmark datasets including CIFAR10 NUS-Wide and Flickr25K as compared to the recent state-of-the-art deep models. The results using the proposed clipped contrastive learning are greatly improved on all datasets as compared to same backbone network with vanilla contrastive learning.</p>\n", "tags": ["ARXIV", "CNN", "Image Retrieval", "Quantisation", "Unsupervised"], "tsne_embedding": [1.6850712299346924, 13.096338272094727]}, {"key": "duda2016distortion", "year": "2016", "title": "Distortion-resistant Hashing For Rapid Search Of Similar DNA Subsequence", "abstract": "<p>One of the basic tasks in bioinformatics is localizing a short subsequence S read while sequencing in a long reference sequence R like the human geneome. A natural rapid approach would be finding a hash value for S and compare it with a prepared database of hash values for each of length S subsequences of R. The problem with such approach is that it would only spot a perfect match while in reality there are lots of small changes substitutions deletions and insertions. This issue could be repaired if having a hash function designed to tolerate some small distortion accordingly to an alignment metric (like Needleman-Wunch) designed to make that two similar sequences should most likely give the same hash value. This paper discusses construction of Distortion-Resistant Hashing (DRH) to generate such fingerprints for rapid search of similar subsequences. The proposed approach is based on the rate distortion theory in a nearly uniform subset of length S sequences the hash value represents the closest sequence to S. This gives some control of the distance of collisions sequences having the same hash value.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-15.376858711242676, -16.897396087646484]}, {"key": "eghbali2016fast", "year": "2016", "title": "Fast Cosine Similarity Search In Binary Space With Angular Multi-index Hashing", "abstract": "<p>Given a large dataset of binary codes and a binary query point we address how to efficiently find K codes in the dataset that yield the largest cosine similarities to the query. The straightforward answer to this problem is to compare the query with all items in the dataset but this is practical only for small datasets. One potential solution to enhance the search time and achieve sublinear cost is to use a hash table populated with binary codes of the dataset and then look up the nearby buckets to the query to retrieve the nearest neighbors. However if codes are compared in terms of cosine similarity rather than the Hamming distance then the main issue is that the order of buckets to probe is not evident. To examine this issue we first elaborate on the connection between the Hamming distance and the cosine similarity. Doing this allows us to systematically find the probing sequence in the hash table. However solving the nearest neighbor search with a single table is only practical for short binary codes. To address this issue we propose the angular multi-index hashing search algorithm which relies on building multiple hash tables on binary code substrings. The proposed search algorithm solves the exact angular K nearest neighbor problem in a time that is often orders of magnitude faster than the linear scan baseline and even approximation methods.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-28.498741149902344, 11.893498420715332]}, {"key": "eghbali2019deep", "year": "2019", "title": "Deep Spherical Quantization For Image Search", "abstract": "<p>Hashing methods which encode high-dimensional images with compact discrete codes have been widely applied to enhance large-scale image retrieval. In this paper we put forward Deep Spherical Quantization (DSQ) a novel method to make deep convolutional neural networks generate supervised and compact binary codes for efficient image search. Our approach simultaneously learns a mapping that transforms the input images into a low-dimensional discriminative space and quantizes the transformed data points using multi-codebook quantization. To eliminate the negative effect of norm variance on codebook learning we force the network to L_2 normalize the extracted features and then quantize the resulting vectors using a new supervised quantization technique specifically designed for points lying on a unit hypersphere. Furthermore we introduce an easy-to-implement extension of our quantization technique that enforces sparsity on the codebooks. Extensive experiments demonstrate that DSQ and its sparse variant can generate semantically separable compact binary codes outperforming many state-of-the-art image retrieval methods on three benchmarks.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Quantisation", "Supervised"], "tsne_embedding": [-20.48116111755371, 15.77348804473877]}, {"key": "engels2021practical", "year": "2021", "title": "Practical Near Neighbor Search Via Group Testing", "abstract": "<p>We present a new algorithm for the approximate near neighbor problem that combines classical ideas from group testing with locality-sensitive hashing (LSH). We reduce the near neighbor search problem to a group testing problem by designating neighbors as positives non-neighbors as negatives and approximate membership queries as group tests. We instantiate this framework using distance-sensitive Bloom Filters to Identify Near-Neighbor Groups (FLINNG). We prove that FLINNG has sub-linear query time and show that our algorithm comes with a variety of practical advantages. For example FLINNG can be constructed in a single pass through the data consists entirely of efficient integer operations and does not require any distance computations. We conduct large-scale experiments on high-dimensional search tasks such as genome search URL similarity search and embedding search over the massive YFCC100M dataset. In our comparison with leading algorithms such as HNSW and FAISS we find that FLINNG can provide up to a 10x query speedup with substantially smaller indexing time and memory.</p>\n", "tags": ["Independent", "LSH", "NEURIPS"], "tsne_embedding": [-30.6419677734375, -4.670123100280762]}, {"key": "ercoli2016compact", "year": "2016", "title": "Compact Hash Codes For Efficient Visual Descriptors Retrieval In Large Scale Databases", "abstract": "<p>In this paper we present an efficient method for visual descriptors retrieval based on compact hash codes computed using a multiple k-means assignment. The method has been applied to the problem of approximate nearest neighbor (ANN) search of local and global visual content descriptors and it has been tested on different datasets three large scale public datasets of up to one billion descriptors (BIGANN) and supported by recent progress in convolutional neural networks (CNNs) also on the CIFAR-10 and MNIST datasets. Experimental results show that despite its simplicity the proposed method obtains a very high performance that makes it superior to more complex state-of-the-art methods.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-28.802438735961914, 14.268762588500977]}, {"key": "erik2023consistency", "year": "2023", "title": "Consistency Analysis Of Chatgpt", "abstract": "<p>ChatGPT has gained a huge popularity since its introduction. Its positive aspects have been reported through many media platforms and some analyses even showed that ChatGPT achieved a decent grade in professional exams adding extra support to the claim that AI can now assist and even replace humans in industrial fields. Others however doubt its reliability and trustworthiness. This paper investigates the trustworthiness of ChatGPT and GPT-4 regarding logically consistent behaviour focusing specifically on semantic consistency and the properties of negation symmetric and transitive consistency. Our findings suggest that while both models appear to show an enhanced language understanding and reasoning ability they still frequently fall short of generating logically consistent predictions. We also ascertain via experiments that prompt designing few-shot learning and employing larger large language models (LLMs) are unlikely to be the ultimate solution to resolve the inconsistency issue of LLMs.</p>\n", "tags": ["AAAI"], "tsne_embedding": [40.014854431152344, 0.6126331090927124]}, {"key": "ermon2013embed", "year": "2013", "title": "Embed And Project Discrete Sampling With Universal Hashing", "abstract": "<p>We consider the problem of sampling from a probability distribution defined over a high-dimensional discrete set specified for instance by a graphical model. We propose a sampling algorithm called PAWS based on embedding the set into a higher-dimensional space which is then randomly projected using universal hash functions to a lower-dimensional subspace and explored using combinatorial search methods. Our scheme can leverage fast combinatorial optimization tools as a blackbox and unlike MCMC methods samples produced are guaranteed to be within an (arbitrarily small) constant factor of the true probability distribution. We demonstrate that by using state-of-the-art combinatorial search tools PAWS can efficiently sample from Ising grids with strong interactions and from software verification instances while MCMC and variational methods fail in both cases.</p>\n", "tags": ["Graph", "Independent", "NEURIPS"], "tsne_embedding": [-18.034818649291992, -18.955768585205078]}, {"key": "ertl2017superminhash", "year": "2017", "title": "Superminhash - A New Minwise Hashing Algorithm For Jaccard Similarity Estimation", "abstract": "<p>This paper presents a new algorithm for calculating hash signatures of sets which can be directly used for Jaccard similarity estimation. The new approach is an improvement over the MinHash algorithm because it has a better runtime behavior and the resulting signatures allow a more precise estimation of the Jaccard index.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-16.160022735595703, -13.61664867401123]}, {"key": "ertl2018bagminhash", "year": "2018", "title": "Bagminhash - Minwise Hashing Algorithm For Weighted Sets", "abstract": "<p>Minwise hashing has become a standard tool to calculate signatures which allow direct estimation of Jaccard similarities. While very efficient algorithms already exist for the unweighted case the calculation of signatures for weighted sets is still a time consuming task. BagMinHash is a new algorithm that can be orders of magnitude faster than current state of the art without any particular restrictions or assumptions on weights or data dimensionality. Applied to the special case of unweighted sets it represents the first efficient algorithm producing independent signature components. A series of tests finally verifies the new algorithm and also reveals limitations of other approaches published in the recent past.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-21.888559341430664, -16.666563034057617]}, {"key": "ertl2019probminhash", "year": "2019", "title": "Probminhash -- A Class Of Locality-sensitive Hash Algorithms For The (probability) Jaccard Similarity", "abstract": "<p>The probability Jaccard similarity was recently proposed as a natural generalization of the Jaccard similarity to measure the proximity of sets whose elements are associated with relative frequencies or probabilities. In combination with a hash algorithm that maps those weighted sets to compact signatures which allow fast estimation of pairwise similarities it constitutes a valuable method for big data applications such as near-duplicate detection nearest neighbor search or clustering. This paper introduces a class of one-pass locality-sensitive hash algorithms that are orders of magnitude faster than the original approach. The performance gain is achieved by calculating signature components not independently but collectively. Four different algorithms are proposed based on this idea. Two of them are statistically equivalent to the original approach and can be used as drop-in replacements. The other two may even improve the estimation error by introducing statistical dependence between signature components. Moreover the presented techniques can be specialized for the conventional Jaccard similarity resulting in highly efficient algorithms that outperform traditional minwise hashing and that are able to compete with the state of the art.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-24.448408126831055, 0.18315152823925018]}, {"key": "facebooksimsearchnet", "year": "2021", "title": "Facebook SimSearchNet++", "abstract": "<p>Facebook SimSearchNet++ is a new dataset released by Facebook for this competition. It consists of features used for image copy detection for integrity purposes. The features are generated by Facebook SimSearchNet++ model.</p>\n", "tags": ["Dataset"], "tsne_embedding": [13.03104305267334, 21.922809600830078]}, {"key": "fan2013supervised", "year": "2013", "title": "Supervised binary hash code learning with jensen shannon divergence", "abstract": "<p>This paper proposes to learn binary hash codes within\na statistical learning framework, in which an upper bound\nof the probability of Bayes decision errors is derived for\ndifferent forms of hash functions and a rigorous proof of\nthe convergence of the upper bound is presented. Consequently, minimizing such an upper bound leads to consistent\nperformance improvements of existing hash code learning\nalgorithms, regardless of whether original algorithms are\nunsupervised or supervised. This paper also illustrates a\nfast hash coding method that exploits simple binary tests to\nachieve orders of magnitude improvement in coding speed\nas compared to projection based methods.</p>\n", "tags": ["ICCV", "Supervised"], "tsne_embedding": [-9.821825981140137, -12.741087913513184]}, {"key": "fan2020deep", "year": "2020", "title": "Deep Polarized Network for Supervised Learning of Accurate Binary Hashing Codes", "abstract": "<p>This paper proposes a novel deep polarized network (DPN) for learning to hash, in which each channel in the network outputs is pushed far away\nfrom zero by employing a differentiable bit-wise hinge-like loss which is dubbed as polarization loss. Reformulated within a generic Hamming Distance Metric Learning framework [Norouzi et al.,\n2012], the proposed polarization loss bypasses the requirement to prepare pairwise labels for (dis-)similar items and, yet, the proposed loss strictly bounds from above the pairwise Hamming Distance based losses. The intrinsic connection between pairwise and pointwise label information, as\ndisclosed in this paper, brings about the following methodological improvements: (a) we may directly employ the proposed differentiable polarization loss with no large deviations incurred from\nthe target Hamming distance based loss; and (b) the subtask of assigning binary codes becomes extremely simple \u2014 even random codes assigned to each class suffice to result in state-of-the-art performances, as demonstrated in CIFAR10, NUS-WIDE and ImageNet100 datasets.</p>\n", "tags": ["Deep Learning", "Has Code", "IJCAI", "Supervised"], "tsne_embedding": [10.498514175415039, 5.156722545623779]}, {"key": "fang2021deep", "year": "2021", "title": "Deep Triplet Hashing Network For Case-based Medical Image Retrieval", "abstract": "<p>Deep hashing methods have been shown to be the most efficient approximate nearest neighbor search techniques for large-scale image retrieval. However existing deep hashing methods have a poor small-sample ranking performance for case-based medical image retrieval. The top-ranked images in the returned query results may be as a different class than the query image. This ranking problem is caused by classification regions of interest (ROI) and small-sample information loss in the hashing space. To address the ranking problem we propose an end-to-end framework called Attention-based Triplet Hashing (ATH) network to learn low-dimensional hash codes that preserve the classification ROI and small-sample information. We embed a spatial-attention module into the network structure of our ATH to focus on ROI information. The spatial-attention module aggregates the spatial information of feature maps by utilizing max-pooling element-wise maximum and element-wise mean operations jointly along the channel axis. The triplet cross-entropy loss can help to map the classification information of images and similarity between images into the hash codes. Extensive experiments on two case-based medical datasets demonstrate that our proposed ATH can further improve the retrieval performance compared to the state-of-the-art deep hashing methods and boost the ranking performance for small samples. Compared to the other loss methods the triplet cross-entropy loss can enhance the classification performance and hash code-discriminability</p>\n", "tags": ["ARXIV", "Image Retrieval", "Supervised"], "tsne_embedding": [-10.475520133972168, 13.53048324584961]}, {"key": "fang2023is", "year": "2023", "title": "Is Chatgpt A Highly Fluent Grammatical Error Correction System A Comprehensive Evaluation", "abstract": "<p>ChatGPT a large-scale language model based on the advanced GPT-3.5 architecture has shown remarkable potential in various Natural Language Processing (NLP) tasks. However there is currently a dearth of comprehensive study exploring its potential in the area of Grammatical Error Correction (GEC). To showcase its capabilities in GEC we design zero-shot chain-of-thought (CoT) and few-shot CoT settings using in-context learning for ChatGPT. Our evaluation involves assessing ChatGPTs performance on five official test sets in three different languages along with three document-level GEC test sets in English. Our experimental results and human evaluations demonstrate that ChatGPT has excellent error detection capabilities and can freely correct errors to make the corrected sentences very fluent possibly due to its over-correction tendencies and not adhering to the principle of minimal edits. Additionally its performance in non-English and low-resource settings highlights its potential in multilingual GEC tasks. However further analysis of various types of errors at the document-level has shown that ChatGPT cannot effectively correct agreement coreference tense errors across sentences and cross-sentence boundary errors.</p>\n", "tags": ["ARXIV", "Survey Paper"], "tsne_embedding": [34.32851791381836, 5.044759750366211]}, {"key": "fedorov2016large", "year": "2016", "title": "Large Scale Near-duplicate Image Retrieval Using Triples Of Adjacent Ranked Features (TARF) With Embedded Geometric Information", "abstract": "<p>Most approaches to large-scale image retrieval are based on the construction of the inverted index of local image descriptors or visual words. A search in such an index usually results in a large number of candidates. This list of candidates is then re-ranked with the help of a geometric verification using a RANSAC algorithm for example. In this paper we propose a feature representation which is built as a combination of three local descriptors. It allows one to significantly decrease the number of false matches and to shorten the list of candidates after the initial search in the inverted index. This combination of local descriptors is both reproducible and highly discriminative and thus can be efficiently used for large-scale near-duplicate image retrieval.</p>\n", "tags": ["ARXIV", "Image Retrieval"], "tsne_embedding": [-16.71050453186035, 23.580896377563477]}, {"key": "fei2023reasoning", "year": "2023", "title": "Reasoning Implicit Sentiment With Chain-of-thought Prompting", "abstract": "<p>While sentiment analysis systems try to determine the sentiment polarities of given targets based on the key opinion expressions in input texts in implicit sentiment analysis (ISA) the opinion cues come in an implicit and obscure manner. Thus detecting implicit sentiment requires the common-sense and multi-hop reasoning ability to infer the latent intent of opinion. Inspired by the recent chain-of-thought (CoT) idea in this work we introduce a Three-hop Reasoning (THOR) CoT framework to mimic the human-like reasoning process for ISA. We design a three-step prompting principle for THOR to step-by-step induce the implicit aspect opinion and finally the sentiment polarity. Our THOR+Flan-T5 (11B) pushes the state-of-the-art (SoTA) by over 637; F1 on supervised setup. More strikingly THOR+GPT3 (175B) boosts the SoTA by over 5037; F1 on zero-shot setting. Our code is open at https://github.com/scofield7419/THOR-ISA.</p>\n", "tags": ["ARXIV", "Has Code", "Supervised"], "tsne_embedding": [32.84977340698242, -4.223041534423828]}, {"key": "feng2014learning", "year": "2014", "title": "Learning To Rank Binary Codes", "abstract": "<p>Binary codes have been widely used in vision problems as a compact feature representation to achieve both space and time advantages. Various methods have been proposed to learn data-dependent hash functions which map a feature vector to a binary code. However considerable data information is inevitably lost during the binarization step which also causes ambiguity in measuring sample similarity using Hamming distance. Besides the learned hash functions cannot be changed after training which makes them incapable of adapting to new data outside the training data set. To address both issues in this paper we propose a flexible bitwise weight learning framework based on the binary codes obtained by state-of-the-art hashing methods and incorporate the learned weights into the weighted Hamming distance computation. We then formulate the proposed framework as a ranking problem and leverage the Ranking SVM model to offline tackle the weight learning. The framework is further extended to an online mode which updates the weights at each time new data comes thereby making it scalable to large and dynamic data sets. Extensive experimental results demonstrate significant performance gains of using binary codes with bitwise weighting in image retrieval tasks. It is appealing that the online weight learning leads to comparable accuracy with its offline counterpart which thus makes our approach practical for realistic applications.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Supervised"], "tsne_embedding": [-1.3587548732757568, 12.647178649902344]}, {"key": "feng2016deep", "year": "2016", "title": "Deep Image Set Hashing", "abstract": "<p>In applications involving matching of image sets the information from multiple images must be effectively exploited to represent each set. State-of-the-art methods use probabilistic distribution or subspace to model a set and use specific distance measure to compare two sets. These methods are slow to compute and not compact to use in a large scale scenario. Learning-based hashing is often used in large scale image retrieval as they provide a compact representation of each sample and the Hamming distance can be used to efficiently compare two samples. However most hashing methods encode each image separately and discard knowledge that multiple images in the same set represent the same object or person. We investigate the set hashing problem by combining both set representation and hashing in a single deep neural network. An image set is first passed to a CNN module to extract image features then these features are aggregated using two types of set feature to capture both set specific and database-wide distribution information. The computed set feature is then fed into a multilayer perceptron to learn a compact binary embedding. Triplet loss is used to train the network by forming set similarity relations using class labels. We extensively evaluate our approach on datasets used for image matching and show highly competitive performance compared to state-of-the-art methods.</p>\n", "tags": ["ARXIV", "CNN", "Image Retrieval", "Supervised"], "tsne_embedding": [-0.10245054215192795, 16.65443992614746]}, {"key": "feng2023layoutgpt", "year": "2023", "title": "Layoutgpt Compositional Visual Planning And Generation With Large Language Models", "abstract": "<p>Attaining a high degree of user controllability in visual generation often requires intricate fine-grained inputs like layouts. However such inputs impose a substantial burden on users when compared to simple text inputs. To address the issue we study how Large Language Models (LLMs) can serve as visual planners by generating layouts from text conditions and thus collaborate with visual generative models. We propose LayoutGPT a method to compose in-context visual demonstrations in style sheet language to enhance the visual planning skills of LLMs. LayoutGPT can generate plausible layouts in multiple domains ranging from 2D images to 3D indoor scenes. LayoutGPT also shows superior performance in converting challenging language concepts like numerical and spatial relations to layout arrangements for faithful text-to-image generation. When combined with a downstream image generation model LayoutGPT outperforms text-to-image models/systems by 20-4037; and achieves comparable performance as human users in designing visual layouts for numerical and spatial correctness. Lastly LayoutGPT achieves comparable performance to supervised methods in 3D indoor scene synthesis demonstrating its effectiveness and potential in multiple visual domains.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [39.70705032348633, 9.419059753417969]}, {"key": "feng2023towards", "year": "2023", "title": "Towards Efficient Deep Hashing Retrieval Condensing Your Data Via Feature-embedding Matching", "abstract": "<p>The expenses involved in training state-of-the-art deep hashing retrieval models have witnessed an increase due to the adoption of more sophisticated models and large-scale datasets. Dataset Distillation (DD) or Dataset Condensation(DC) focuses on generating smaller synthetic dataset that retains the original information. Nevertheless existing DD methods face challenges in maintaining a trade-off between accuracy and efficiency. And the state-of-the-art dataset distillation methods can not expand to all deep hashing retrieval methods. In this paper we propose an efficient condensation framework that addresses these limitations by matching the feature-embedding between synthetic set and real set. Furthermore we enhance the diversity of features by incorporating the strategies of early-stage augmented models and multi-formation. Extensive experiments provide compelling evidence of the remarkable superiority of our approach both in terms of performance and efficiency compared to state-of-the-art baseline methods.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [14.029281616210938, -0.5703440308570862]}, {"key": "fernandez2022active", "year": "2022", "title": "Active Image Indexing", "abstract": "<p>Image copy detection and retrieval from large databases leverage two components. First a neural network maps an image to a vector representation that is relatively robust to various transformations of the image. Second an efficient but approximate similarity search algorithm trades scalability (size and speed) against quality of the search thereby introducing a source of error. This paper improves the robustness of image copy detection with active indexing that optimizes the interplay of these two components. We reduce the quantization loss of a given image representation by making imperceptible changes to the image before its release. The loss is back-propagated through the deep neural network back to the image under perceptual constraints. These modifications make the image more retrievable. Our experiments show that the retrieval and copy detection of activated images is significantly improved. For instance activation improves by +4037; the Recall1@1 on various image transformations and for several popular indexing structures based on product quantization and locality sensitivity hashing.</p>\n", "tags": ["ARXIV", "Quantisation", "Supervised"], "tsne_embedding": [-9.09886646270752, 18.3081111907959]}, {"key": "feyza2023generating", "year": "2023", "title": "RL4F Generating Natural Language Feedback With Reinforcement Learning For Repairing Model Outputs", "abstract": "<p>Despite their unprecedented success even the largest language models make mistakes. Similar to how humans learn and improve using feedback previous work proposed providing language models with natural language feedback to guide them in repairing their outputs. Because human-generated critiques are expensive to obtain researchers have devised learned critique generators in lieu of human critics while assuming one can train downstream models to utilize generated feedback. However this approach does not apply to black-box or limited access models such as ChatGPT as they cannot be fine-tuned. Moreover in the era of large general-purpose language agents fine-tuning is neither computationally nor spatially efficient as it results in multiple copies of the network. In this work we introduce RL4F (Reinforcement Learning for Feedback) a multi-agent collaborative framework where the critique generator is trained to maximize end-task performance of GPT-3 a fixed model more than 200 times its size. RL4F produces critiques that help GPT-3 revise its outputs. We study three datasets for action planning summarization and alphabetization and show relative improvements up to 1037; in multiple text similarity metrics over other learned retrieval-augmented or prompting-based critique generators.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [41.11896514892578, -1.4666155576705933]}, {"key": "frei2023bounds", "year": "2023", "title": "Bounds For C-ideal Hashing", "abstract": "<p>In this paper we analyze hashing from a worst-case perspective. To this end we study a new property of hash families that is strongly related to d-perfect hashing namely c-ideality. On the one hand this notion generalizes the definition of perfect hashing which has been studied extensively; on the other hand it provides a direct link to the notion of c-approximativity. We focus on the usually neglected case where the average load (alpha) is at least 1 and prove upper and lower parametrized bounds on the minimal size of c-ideal hash families. As an aside we show how c-ideality helps to analyze the advice complexity of hashing. The concept of advice introduced a decade ago lets us measure the information content of an online problem. We prove hashings advice complexity to be linear in the hash table size.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-4.842350959777832, -16.351682662963867]}, {"key": "frome2006image", "year": "2006", "title": "Image Retrieval And Classification Using Local Distance Functions", "abstract": "<p>In this paper we introduce and experiment with a framework for learning local perceptual distance functions for visual recognition. We learn a distance function for each training image as a combination of elementary distances between patch-based visual features. We apply these combined local distance functions to the tasks of image retrieval and classification of novel images. On the Caltech 101 object recognition benchmark we achieve 60.337; mean recognition across classes using 15 training images per class which is better than the best published performance by Zhang et al.</p>\n", "tags": ["Image Retrieval", "NEURIPS", "Supervised"], "tsne_embedding": [6.292189598083496, 24.13794708251953]}, {"key": "fu2016auto", "year": "2016", "title": "Auto-jacobin Auto-encoder Jacobian Binary Hashing", "abstract": "<p>Binary codes can be used to speed up nearest neighbor search tasks in large scale data sets as they are efficient for both storage and retrieval. In this paper we propose a robust auto-encoder model that preserves the geometric relationships of high-dimensional data sets in Hamming space. This is done by considering a noise-removing function in a region surrounding the manifold where the training data points lie. This function is defined with the property that it projects the data points near the manifold into the manifold wisely and we approximate this function by its first order approximation. Experimental results show that the proposed method achieves better than state-of-the-art results on three large scale high dimensional data sets.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [-24.271947860717773, 11.91210651397705]}, {"key": "fu2018neurons", "year": "2018", "title": "Neurons Merging Layer Towards Progressive Redundancy Reduction For Deep Supervised Hashing", "abstract": "<p>Deep supervised hashing has become an active topic in information retrieval. It generates hashing bits by the output neurons of a deep hashing network. During binary discretization there often exists much redundancy between hashing bits that degenerates retrieval performance in terms of both storage and accuracy. This paper proposes a simple yet effective Neurons Merging Layer (NMLayer) for deep supervised hashing. A graph is constructed to represent the redundancy relationship between hashing bits that is used to guide the learning of a hashing network. Specifically it is dynamically learned by a novel mechanism defined in our active and frozen phases. According to the learned relationship the NMLayer merges the redundant neurons together to balance the importance of each output neuron. Moreover multiple NMLayers are progressively trained for a deep hashing network to learn a more compact hashing code from a long redundant code. Extensive experiments on four datasets demonstrate that our proposed method outperforms state-of-the-art hashing methods.</p>\n", "tags": ["ARXIV", "Graph", "Supervised"], "tsne_embedding": [4.663325786590576, -9.084178924560547]}, {"key": "fu2023improving", "year": "2023", "title": "Improving Language Model Negotiation With Self-play And In-context Learning From AI Feedback", "abstract": "<p>We study whether multiple large language models (LLMs) can autonomously improve each other in a negotiation game by playing reflecting and criticizing. We are interested in this question because if LLMs were able to improve each other it would imply the possibility of creating strong AI agents with minimal human intervention. We ask two LLMs to negotiate with each other playing the roles of a buyer and a seller respectively. They aim to reach a deal with the buyer targeting a lower price and the seller a higher one. A third language model playing the critic provides feedback to a player to improve the players negotiation strategies. We let the two agents play multiple rounds using previous negotiation history and AI feedback as in-context demonstrations to improve the models negotiation strategy iteratively. We use different LLMs (GPT and Claude) for different roles and use the deal price as the evaluation metric. Our experiments reveal multiple intriguing findings (1) Only a subset of the language models we consider can self-play and improve the deal price from AI feedback weaker models either do not understand the games rules or cannot incorporate AI feedback for further improvement. (2) Models abilities to learn from the feedback differ when playing different roles. For example it is harder for Claude-instant to improve as the buyer than as the seller. (3) When unrolling the game to multiple rounds stronger agents can consistently improve their performance by meaningfully using previous experiences and iterative AI feedback yet have a higher risk of breaking the deal. We hope our work provides insightful initial explorations of having models autonomously improve each other with game playing and AI feedback.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [43.606101989746094, -2.1140522956848145]}, {"key": "fu2023mme", "year": "2023", "title": "MME A Comprehensive Evaluation Benchmark For Multimodal Large Language Models", "abstract": "<p>Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks showing amazing emergent abilities in recent studies such as writing poems based on an image. However it is difficult for these case studies to fully reflect the performance of MLLM lacking a comprehensive evaluation. In this paper we fill in this blank presenting the first comprehensive MLLM Evaluation benchmark MME. It measures both perception and cognition abilities on a total of 14 subtasks. In order to avoid data leakage that may arise from direct use of public datasets for evaluation the annotations of instruction-answer pairs are all manually designed. The concise instruction design allows us to fairly compare MLLMs instead of struggling in prompt engineering. Besides with such an instruction we can also easily carry out quantitative statistics. A total of 30 advanced MLLMs are comprehensively evaluated on our MME which not only suggests that existing MLLMs still have a large room for improvement but also reveals the potential directions for the subsequent model optimization. The data application manner and online leaderboards are released at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation.</p>\n", "tags": ["ARXIV", "Case Study", "Cross Modal", "Has Code"], "tsne_embedding": [31.10626792907715, -4.173842430114746]}, {"key": "fu2024video", "year": "2024", "title": "Video-mme The First-ever Comprehensive Evaluation Benchmark Of Multi-modal Llms In Video Analysis", "abstract": "<p>In the quest for artificial general intelligence Multi-modal Large Language Models (MLLMs) have emerged as a focal point in recent advancements. However the predominant focus remains on developing their capabilities in static image understanding. The potential of MLLMs in processing sequential visual data is still insufficiently explored highlighting the absence of a comprehensive high-quality assessment of their performance. In this paper we introduce Video-MME the first-ever full-spectrum Multi-Modal Evaluation benchmark of MLLMs in Video analysis. Our work distinguishes from existing benchmarks through four key features 1) Diversity in video types spanning 6 primary visual domains with 30 subfields to ensure broad scenario generalizability; 2) Duration in temporal dimension encompassing both short- medium- and long-term videos ranging from 11 seconds to 1 hour for robust contextual dynamics; 3) Breadth in data modalities integrating multi-modal inputs besides video frames including subtitles and audios to unveil the all-round capabilities of MLLMs; 4) Quality in annotations utilizing rigorous manual labeling by expert annotators to facilitate precise and reliable model assessment. 900 videos with a total of 254 hours are manually selected and annotated by repeatedly viewing all the video content resulting in 2700 question-answer pairs. With Video-MME we extensively evaluate various state-of-the-art MLLMs including GPT-4 series and Gemini 1.5 Pro as well as open-source image models like InternVL-Chat-V1.5 and video models like LLaVA-NeXT-Video. Our experiments reveal that Gemini 1.5 Pro is the best-performing commercial model significantly outperforming the open-source models. Our dataset along with these findings underscores the need for further improvements in handling longer sequences and multi-modal data. Project Page https://video-mme.github.io</p>\n", "tags": ["ARXIV"], "tsne_embedding": [44.4128532409668, 7.71199369430542]}, {"key": "fu2024vita", "year": "2024", "title": "VITA Towards Open-source Interactive Omni Multimodal LLM", "abstract": "<p>The remarkable multimodal capabilities and interactive experience of GPT-4o underscore their necessity in practical applications yet open-source models rarely excel in both areas. In this paper we introduce VITA the first-ever open-source Multimodal Large Language Model (MLLM) adept at simultaneous processing and analysis of Video Image Text and Audio modalities and meanwhile has an advanced multimodal interactive experience. Starting from Mixtral 8x7B as a language foundation we expand its Chinese vocabulary followed by bilingual instruction tuning. We further endow the language model with visual and audio capabilities through two-stage multi-task learning of multimodal alignment and instruction tuning. VITA demonstrates robust foundational capabilities of multilingual vision and audio understanding as evidenced by its strong performance across a range of both unimodal and multimodal benchmarks. Beyond foundational capabilities we have made considerable progress in enhancing the natural multimodal human-computer interaction experience. VITA is the first step for the open-source community to explore the seamless integration of multimodal understanding and interaction. While there is still lots of work to be done on VITA to get close to close-source counterparts we hope that its role as a pioneer can serve as a cornerstone for subsequent research. Project Page https://vita-home.github.io.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [34.61013412475586, -14.031678199768066]}, {"key": "gajic2019bag", "year": "2019", "title": "Bag Of Negatives For Siamese Architectures", "abstract": "<p>Training a Siamese architecture for re-identification with a large number of identities is a challenging task due to the difficulty of finding relevant negative samples efficiently. In this work we present Bag of Negatives (BoN) a method for accelerated and improved training of Siamese networks that scales well on datasets with a very large number of identities. BoN is an efficient and loss-independent method able to select a bag of high quality negatives based on a novel online hashing strategy.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [4.093923568725586, -7.6062798500061035]}, {"key": "gao2020making", "year": "2020", "title": "Making Pre-trained Language Models Better Few-shot Learners", "abstract": "<p>The recent GPT-3 model (Brown et al. 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings we study few-shot learning in a more practical scenario where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF\u2013better few-shot fine-tuning of language models\u2013a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting achieving up to 3037; absolute improvement and 1137; on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise and hence constitutes a strong task-agnostic method for few-shot learning.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [24.360103607177734, -1.2319979667663574]}, {"key": "gao2021backdoor", "year": "2021", "title": "Backdoor Attack On Hash-based Image Retrieval Via Clean-label Data Poisoning", "abstract": "<p>A backdoored deep hashing model is expected to behave normally on original query images and return the images with the target label when a specific trigger pattern presents. To this end we propose the confusing perturbations-induced backdoor attack (CIBA). It injects a small number of poisoned images with the correct label into the training data which makes the attack hard to be detected. To craft the poisoned images we first propose the confusing perturbations to disturb the hashing code learning. As such the hashing model can learn more about the trigger. The confusing perturbations are imperceptible and generated by optimizing the intra-class dispersion and inter-class shift in the Hamming space. We then employ the targeted adversarial patch as the backdoor trigger to improve the attack performance. We have conducted extensive experiments to verify the effectiveness of our proposed CIBA. Our code is available at https://github.com/KuofengGao/CIBA.\u201d</p>\n", "tags": ["ARXIV", "Has Code", "Image Retrieval", "Supervised"], "tsne_embedding": [-5.6623616218566895, 10.939373016357422]}, {"key": "gao2022pal", "year": "2022", "title": "PAL Program-aided Language Models", "abstract": "<p>Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks when provided with a few examples at test time (few-shot prompting). Much of this success can be attributed to prompting methods such as chain-of-thought which employ LLMs for both understanding the problem description by decomposing it into steps as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition LLMs often make logical and arithmetic mistakes in the solution part even when the problem is decomposed correctly. In this paper we present Program-Aided Language models (PAL) a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps but offloads the solution step to a runtime such as a Python interpreter. With PAL decomposing the natural language problem into runnable steps remains the only learning task for the LLM while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical symbolic and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems surpassing PaLM-540B which uses chain-of-thought by absolute 1537; top-1. Our code and data are publicly available at http://reasonwithpal.com/ .</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [28.42439842224121, 1.7700276374816895]}, {"key": "gao2023chat", "year": "2023", "title": "Chat-rec Towards Interactive And Explainable Llms-augmented Recommender System", "abstract": "<p>Large language models (LLMs) have demonstrated their significant potential to be applied for addressing various application tasks. However traditional recommender systems continue to face great challenges such as poor interactivity and explainability which actually also hinder their broad deployment in real-world systems. To address these limitations this paper proposes a novel paradigm called Chat-Rec (ChatGPT Augmented Recommender System) that innovatively augments LLMs for building conversational recommender systems by converting user profiles and historical interactions into prompts. Chat-Rec is demonstrated to be effective in learning user preferences and establishing connections between users and products through in-context learning which also makes the recommendation process more interactive and explainable. Whats more within the Chat-Rec framework users preferences can transfer to different products for cross-domain recommendations and prompt-based injection of information into LLMs can also handle the cold-start scenarios with new items. In our experiments Chat-Rec effectively improve the results of top-k recommendations and performs better in zero-shot rating prediction task. Chat-Rec offers a novel approach to improving recommender systems and presents new practical scenarios for the implementation of AIGC (AI generated content) in recommender system studies.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [43.100528717041016, -12.743122100830078]}, {"key": "gao2023retrieval", "year": "2023", "title": "Retrieval-augmented Generation For Large Language Models A Survey", "abstract": "<p>Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination outdated knowledge and non-transparent untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation particularly for knowledge-intensive tasks and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs intrinsic knowledge with the vast dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms encompassing the Naive RAG the Advanced RAG and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks which includes the retrieval the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components providing a profound understanding of the advancements in RAG systems. Furthermore this paper introduces up-to-date evaluation framework and benchmark. At the end this article delineates the challenges currently faced and points out prospective avenues for research and development.</p>\n", "tags": ["ARXIV", "Survey Paper"], "tsne_embedding": [14.573266983032227, -0.7361945509910583]}, {"key": "gao2024cantor", "year": "2024", "title": "Cantor Inspiring Multimodal Chain-of-thought Of MLLM", "abstract": "<p>With the advent of large language models(LLMs) enhanced by the chain-of-thought(CoT) methodology visual reasoning problem is usually decomposed into manageable sub-tasks and tackled sequentially with various external tools. However such a paradigm faces the challenge of the potential determining hallucinations in decision-making due to insufficient visual information and the limitation of low-level perception tools that fail to provide abstract summaries necessary for comprehensive reasoning. We argue that converging visual context acquisition and logical reasoning is pivotal for tackling visual reasoning tasks. This paper delves into the realm of multimodal CoT to solve intricate visual reasoning tasks with multimodal large language models(MLLMs) and their cognitive capability. To this end we propose an innovative multimodal CoT framework termed Cantor characterized by a perception-decision architecture. Cantor first acts as a decision generator and integrates visual inputs to analyze the image and problem ensuring a closer alignment with the actual context. Furthermore Cantor leverages the advanced cognitive functions of MLLMs to perform as multifaceted experts for deriving higher-level information enhancing the CoT generation process. Our extensive experiments demonstrate the efficacy of the proposed framework showing significant improvements in multimodal CoT performance across two complex visual reasoning datasets without necessitating fine-tuning or ground-truth rationales. Project Page https://ggg0919.github.io/cantor/ .</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [40.6153678894043, 4.374916076660156]}, {"key": "garg2019nearly", "year": "2019", "title": "Nearly-unsupervised Hashcode Representations For Relation Extraction", "abstract": "<p>Recently kernelized locality sensitive hashcodes have been successfully employed as representations of natural language text especially showing high relevance to biomedical relation extraction tasks. In this paper we propose to optimize the hashcode representations in a nearly unsupervised manner in which we only use data points but not their class labels for learning. The optimized hashcode representations are then fed to a supervised classifier following the prior work. This nearly unsupervised approach allows fine-grained optimization of each hash function which is particularly suitable for building hashcode representations generalizing from a training set to a test set. We empirically evaluate the proposed approach for biomedical relation extraction tasks obtaining significant accuracy improvements w.r.t. state-of-the-art supervised and semi-supervised approaches.</p>\n", "tags": ["ARXIV", "Unsupervised"], "tsne_embedding": [11.944957733154297, 8.708930015563965]}, {"key": "gaskill2019bitwise", "year": "2019", "title": "The Bitwise Hashing Trick For Personalized Search", "abstract": "<p>Many real world problems require fast and efficient lexical comparison of large numbers of short text strings. Search personalization is one such domain. We introduce the use of feature bit vectors using the hashing trick for improving relevance in personalized search and other personalization applications. We present results of several lexical hashing and comparison methods. These methods are applied to a users historical behavior and are used to predict future behavior. Using a single bit per dimension instead of floating point results in an order of magnitude decrease in data structure size while preserving or even improving quality. We use real data to simulate a search personalization task. A simple method for combining bit vectors demonstrates an order of magnitude improvement in compute time on the task with only a small decrease in accuracy.</p>\n", "tags": [], "tsne_embedding": [-22.699180603027344, -9.600743293762207]}, {"key": "gattupalli2018weakly", "year": "2018", "title": "Weakly Supervised Deep Image Hashing Through Tag Embeddings", "abstract": "<p>Many approaches to semantic image hashing have been formulated as supervised learning problems that utilize images and label information to learn the binary hash codes. However large-scale labeled image data is expensive to obtain thus imposing a restriction on the usage of such algorithms. On the other hand unlabelled image data is abundant due to the existence of many Web image repositories. Such Web images may often come with images tags that contain useful information although raw tags in general do not readily lead to semantic labels. Motivated by this scenario we formulate the problem of semantic image hashing as a weakly-supervised learning problem. We utilize the information contained in the user-generated tags associated with the images to learn the hash codes. More specifically we extract the word2vec semantic embeddings of the tags and use the information contained in them for constraining the learning. Accordingly we name our model Weakly Supervised Deep Hashing using Tag Embeddings (WDHT). WDHT is tested for the task of semantic image retrieval and is compared against several state-of-art models. Results show that our approach sets a new state-of-art in the area of weekly supervised image hashing.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Supervised", "Weakly Supervised"], "tsne_embedding": [0.47105130553245544, 4.7028656005859375]}, {"key": "gattupalli2019weakly", "year": "2019", "title": "Weakly Supervised Deep Image Hashing through Tag Embeddings", "abstract": "<p>Many approaches to semantic image hashing have been formulated as supervised learning problems that utilize images and label information to learn the binary hash codes. However, large-scale labeled image data is expensive to obtain, thus imposing a restriction on the usage of such algorithms. On the other hand, unlabelled image data is abundant due to the existence of many Web image repositories. Such Web images may often come with images tags that contain useful information, although raw tags, in general, do not readily lead to semantic labels.\nMotivated by this scenario, we formulate the problem of semantic image hashing as a weakly-supervised learning problem. We utilize the information contained in the user-generated tags associated with the images to learn the hash codes. More specifically, we extract the word2vec semantic embeddings of the tags and use the information contained in them for constraining the learning.\nAccordingly, we name our model Weakly Supervised Deep Hashing using Tag Embeddings (WDHT). WDHT is tested for the task of semantic image retrieval and is compared against several state-of-art models. Results show that our approach sets a new state-of-art in the area of weekly supervised image hashing.</p>\n", "tags": ["CVPR", "Deep Learning", "Has Code", "Image Retrieval", "Supervised", "Weakly Supervised"], "tsne_embedding": [0.4979439675807953, 4.67312479019165]}, {"key": "ge2014graph", "year": "2014", "title": "Graph Cuts for Supervised Binary Coding", "abstract": "<p>Learning short binary codes is challenged by the inherent discrete\nnature of the problem. The graph cuts algorithm is a well-studied\ndiscrete label assignment solution in computer vision, but has not yet\nbeen applied to solve the binary coding problems. This is partially because\nit was unclear how to use it to learn the encoding (hashing) functions\nfor out-of-sample generalization. In this paper, we formulate supervised\nbinary coding as a single optimization problem that involves both\nthe encoding functions and the binary label assignment. Then we apply\nthe graph cuts algorithm to address the discrete optimization problem\ninvolved, with no continuous relaxation. This method, named as Graph\nCuts Coding (GCC), shows competitive results in various datasets.</p>\n", "tags": ["ECCV", "Image Retrieval", "Supervised"], "tsne_embedding": [-2.7170448303222656, 27.436426162719727]}, {"key": "ge2023chain", "year": "2023", "title": "Chain Of Thought Prompt Tuning In Vision Language Models", "abstract": "<p>Language-Image Pre-training has demonstrated promising results on zero-shot and few-shot downstream tasks by prompting visual models with natural language prompts. However most recent studies only use a single prompt for tuning neglecting the inherent step-to-step cognitive reasoning process that humans conduct in complex task settings for example when processing images from unfamiliar domains. Chain of Thought is a simple and effective approximation to human reasoning process and has been proven useful for natural language processing (NLP) tasks. Based on this cognitive intuition we believe that conducting effective reasoning is also an important problem in visual tasks and a chain of thought could be a solution to this problem. In this work we propose a novel chain of thought prompt tuning for vision-language modeling. Extensive experiments show that our method not only generalizes better in image classification tasks has greater transferability beyond a single dataset and has stronger domain generalization performance but also performs much better in imagetext retrieval and visual question answering which require more reasoning capabilities. We are the first to successfully adapt chain-of-thought prompting that combines visual and textual embeddings. We will release our codes</p>\n", "tags": ["ARXIV", "Cross Modal", "Supervised", "Text Retrieval"], "tsne_embedding": [38.65297317504883, 3.6833648681640625]}, {"key": "ge2023planting", "year": "2023", "title": "Planting A SEED Of Vision In Large Language Model", "abstract": "<p>We present SEED an elaborate image tokenizer that empowers Large Language Models (LLMs) with the emergent ability to SEE and Draw at the same time. Research on image tokenizers has previously reached an impasse as frameworks employing quantized visual tokens have lost prominence due to subpar performance and convergence in multimodal comprehension (compared to BLIP-2 etc.) or generation (compared to Stable Diffusion etc.). Despite the limitations we remain confident in its natural capacity to unify visual and textual representations facilitating scalable multimodal training with LLMs original recipe. In this study we identify two crucial principles for the architecture and training of SEED that effectively ease subsequent alignment with LLMs. (1) Image tokens should be independent of 2D physical patch positions and instead be produced with a 1D causal dependency exhibiting intrinsic interdependence that aligns with the left-to-right autoregressive prediction mechanism in LLMs. (2) Image tokens should capture high-level semantics consistent with the degree of semantic abstraction in words and be optimized for both discriminativeness and reconstruction during the tokenizer training phase. As a result the off-the-shelf LLM is able to perform both image-to-text and text-to-image generation by incorporating our SEED through efficient LoRA tuning. Comprehensive multimodal pretraining and instruction tuning which may yield improved results are reserved for future investigation. This version of SEED was trained in 5.7 days using only 64 V100 GPUs and 5M publicly available image-text pairs. Our preliminary study emphasizes the great potential of discrete visual tokens in versatile multimodal LLMs and the importance of proper image tokenizers in broader research.</p>\n", "tags": ["ARXIV", "Cross Modal", "Independent"], "tsne_embedding": [12.818460464477539, -15.10746955871582]}, {"key": "geng2018regularizing", "year": "2018", "title": "Regularizing Deep Hashing Networks Using GAN Generated Fake Images", "abstract": "<p>Recently deep-networks-based hashing (deep hashing) has become a leading approach for large-scale image retrieval. It aims to learn a compact bitwise representation for images via deep networks so that similar images are mapped to nearby hash codes. Since a deep network model usually has a large number of parameters it may probably be too complicated for the training data we have leading to model over-fitting. To address this issue in this paper we propose a simple two-stage pipeline to learn deep hashing models by regularizing the deep hashing networks using fake images. The first stage is to generate fake images from the original training set without extra data via a generative adversarial network (GAN). In the second stage we propose a deep architec- ture to learn hash functions in which we use a maximum-entropy based loss to incorporate the newly created fake images by the GAN. We show that this loss acts as a strong regularizer of the deep architecture by penalizing low-entropy output hash codes. This loss can also be interpreted as a model ensemble by simultaneously training many network models with massive weight sharing but over different training sets. Empirical evaluation results on several benchmark datasets show that the proposed method has superior performance gains over state-of-the-art hashing methods.</p>\n", "tags": ["ARXIV", "GAN", "Image Retrieval", "Supervised"], "tsne_embedding": [-1.6282131671905518, 2.9132745265960693]}, {"key": "ghosh2024vdgd", "year": "2024", "title": "VDGD Mitigating LVLM Hallucinations In Cognitive Prompts By Bridging The Visual Perception Gap", "abstract": "<p>Recent interest in Large Vision-Language Models (LVLMs) for practical applications is moderated by the significant challenge of hallucination or the inconsistency between the factual information and the generated text. In this paper we first perform an in-depth analysis of hallucinations and discover several novel insights about how and when LVLMs hallucinate. From our analysis we show that (1) The communitys efforts have been primarily targeted towards reducing hallucinations related to visual recognition (VR) prompts (e.g. prompts that only require describing the image) thereby ignoring hallucinations for cognitive prompts (e.g. prompts that require additional skills like reasoning on contents of the image). (2) LVLMs lack visual perception i.e. they can see but not necessarily understand or perceive the input image. We analyze responses to cognitive prompts and show that LVLMs hallucinate due to a perception gap although LVLMs accurately recognize visual elements in the input image and possess sufficient cognitive skills they struggle to respond accurately and hallucinate. To overcome this shortcoming we propose Visual Description Grounded Decoding (VDGD) a simple robust and training-free method for alleviating hallucinations. Specifically we first describe the image and add it as a prefix to the instruction. Next during auto-regressive decoding we sample from the plausible candidates according to their KL-Divergence (KLD) to the description where lower KLD is given higher preference. Experimental results on several benchmarks and LVLMs show that VDGD improves significantly over other baselines in reducing hallucinations. We also propose VaLLu a benchmark for the comprehensive evaluation of the cognitive capabilities of LVLMs.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [49.15998077392578, 7.928905010223389]}, {"key": "gilardi2023chatgpt", "year": "2023", "title": "Chatgpt Outperforms Crowd-workers For Text-annotation Tasks", "abstract": "<p>Many NLP applications require manual data annotations for a variety of tasks notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity the tasks may be conducted by crowd-workers on platforms such as MTurk as well as trained annotators such as research assistants. Using a sample of 2382 tweets we demonstrate that ChatGPT outperforms crowd-workers for several annotation tasks including relevance stance topics and frames detection. Specifically the zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of five tasks while ChatGPTs intercoder agreement exceeds that of both crowd-workers and trained annotators for all tasks. Moreover the per-annotation cost of ChatGPT is less than 0.003 \u2013 about twenty times cheaper than MTurk. These results show the potential of large language models to drastically increase the efficiency of text classification.</p>\n", "tags": ["ARXIV", "Unsupervised"], "tsne_embedding": [17.668663024902344, -1.6608185768127441]}, {"key": "gilreath2004hash", "year": "2004", "title": "Hash Sort A Linear Time Complexity Multiple-dimensional Sort Algorithm", "abstract": "<p>Sorting and hashing are two completely different concepts in computer science and appear mutually exclusive to one another. Hashing is a search method using the data as a key to map to the location within memory and is used for rapid storage and retrieval. Sorting is a process of organizing data from a random permutation into an ordered arrangement and is a common activity performed frequently in a variety of applications. Almost all conventional sorting algorithms work by comparison and in doing so have a linearithmic greatest lower bound on the algorithmic time complexity. Any improvement in the theoretical time complexity of a sorting algorithm can result in overall larger gains in implementation performance.. A gain in algorithmic performance leads to much larger gains in speed for the application that uses the sort algorithm. Such a sort algorithm needs to use an alternative method for ordering the data than comparison to exceed the linearithmic time complexity boundary on algorithmic performance. The hash sort is a general purpose non-comparison based sorting algorithm by hashing which has some interesting features not found in conventional sorting algorithms. The hash sort asymptotically outperforms the fastest traditional sorting algorithm the quick sort. The hash sort algorithm has a linear time complexity factor \u2013 even in the worst case. The hash sort opens an area for further work and investigation into alternative means of sorting.</p>\n", "tags": ["Independent"], "tsne_embedding": [-23.16680908203125, -12.801058769226074]}, {"key": "gionis1999similarity", "year": "1999", "title": "Similarity Search in High Dimensions via Hashing", "abstract": "<p>The nearest- or near-neighbor query problems arise in a large variety of database applications, usually in the context of similarity searching. Of late, there has been increasing interest in building search/index structures for performing similarity search over high-dimensional data, e.g., image databases, document collections, time-series databases, and genome databases. Unfortunately,\nall known techniques for solving this problem fall prey to the curse of dimensionality. That is, the data structures scale poorly with data dimensionality;\nin fact, if the number of dimensions exceeds 10 to 20, searching in k-d trees and related structures involves the inspection of a large fraction of the database, thereby doing no better than brute-force linear search. It has been suggested that since the selection of features and the choice of a distance metric in typical applications is rather heuristic, determining an approximate nearest neighbor should suffice for most practical purposes. In this paper, we examine a novel scheme for approximate similarity search based on hashing. The basic idea is to hash the points from the database so as to ensure that the probability of collision is much higher for objects that are close to each other than for those that are far apart. We provide experimental evidence that our\nmethod gives significant improvement in running time over other methods for searching in highdimensional spaces based on hierarchical tree decomposition.\nExperimental results also indicate that our scheme scales well even for a relatively large number of dimensions (more than 50).</p>\n", "tags": [], "tsne_embedding": [-21.03254508972168, -3.0137581825256348]}, {"key": "glaese2022improving", "year": "2022", "title": "Improving Alignment Of Dialogue Agents Via Targeted Human Judgements", "abstract": "<p>We present Sparrow an information-seeking dialogue agent trained to be more helpful correct and harmless compared to prompted language model baselines. We use reinforcement learning from human feedback to train our models with two new additions to help human raters judge agent behaviour. First to make our agent more helpful and harmless we break down the requirements for good dialogue into natural language rules the agent should follow and ask raters about each rule separately. We demonstrate that this breakdown enables us to collect more targeted human judgements of agent behaviour and allows for more efficient rule-conditional reward models. Second our agent provides evidence from sources supporting factual claims when collecting preference judgements over model statements. For factual questions evidence provided by Sparrow supports the sampled response 7837; of the time. Sparrow is preferred more often than baselines while being more resilient to adversarial probing by humans violating our rules only 837; of the time when probed. Finally we conduct extensive analyses showing that though our model learns to follow our rules it can exhibit distributional biases.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [42.91337203979492, -1.4957977533340454]}, {"key": "gominski2019challenging", "year": "2019", "title": "Challenging Deep Image Descriptors For Retrieval In Heterogeneous Iconographic Collections", "abstract": "<p>This article proposes to study the behavior of recent and efficient state-of-the-art deep-learning based image descriptors for content-based image retrieval facing a panel of complex variations appearing in heterogeneous image datasets in particular in cultural collections that may involve multi-source multi-date and multi-view Permission to make digital</p>\n", "tags": ["ARXIV", "Cross Modal", "Deep Learning", "Graph", "Image Retrieval"], "tsne_embedding": [7.350956439971924, 18.03713607788086]}, {"key": "goncalves2023geometric", "year": "2023", "title": "Geometric Covering Using Random Fields", "abstract": "<p>A set of vectors S (subseteq) (mathbbR)^d is (k_1(varepsilon))-clusterable if there are k_1 balls of radius (varepsilon) that cover S. A set of vectors S (subseteq) (mathbbR)^d is (k_2(delta))-far from being clusterable if there are at least k_2 vectors in S with all pairwise distances at least (delta). We propose a probabilistic algorithm to distinguish between these two cases. Our algorithm reaches a decision by only looking at the extreme values of a scalar valued hash function defined by a random field on S; hence it is especially suitable in distributed and online settings. An important feature of our method is that the algorithm is oblivious to the number of vectors in the online setting for example the algorithm stores only a constant number of scalars which is independent of the stream length. We introduce random field hash functions which are a key ingredient in our paradigm. Random field hash functions generalize locality-sensitive hashing (LSH). In addition to the LSH requirement that nearby vectors are hashed to similar values our hash function also guarantees that the hash values are (nearly) independent random variables for distant vectors. We formulate necessary conditions for the kernels which define the random fields applied to our problem as well as a measure of kernel optimality for which we provide a bound. Then we propose a method to construct kernels which approximate the optimal one.</p>\n", "tags": ["ARXIV", "Independent", "LSH"], "tsne_embedding": [-33.58024597167969, -7.549487113952637]}, {"key": "gonen2022demystifying", "year": "2022", "title": "Demystifying Prompts In Language Models Via Perplexity Estimation", "abstract": "<p>Language models can be prompted to perform a wide variety of zero- and few-shot learning problems. However performance varies significantly with the choice of prompt and we do not yet understand why this happens or how to pick the best prompts. In this work we analyze the factors that contribute to this variance and establish a new empirical hypothesis the performance of a prompt is coupled with the extent to which the model is familiar with the language it contains. Over a wide range of tasks we show that the lower the perplexity of the prompt is the better the prompt is able to perform the task. As a result we devise a method for creating prompts (1) automatically extend a small seed set of manually written prompts by paraphrasing using GPT3 and backtranslation and (2) choose the lowest perplexity prompts to get significant gains in performance.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [24.595731735229492, -1.9854869842529297]}, {"key": "gong2011using", "year": "2011", "title": "Iterative Quantization: A Procrustean Approach to Learning Binary Codes", "abstract": "<p>This paper addresses the problem of learning similarity preserving binary codes for efficient retrieval in large-scale image collections. We propose a simple and efficient alternating minimization scheme for finding a rotation of zerocentered data so as to minimize the quantization error of\nmapping this data to the vertices of a zero-centered binary\nhypercube. This method, dubbed iterative quantization\n(ITQ), has connections to multi-class spectral clustering\nand to the orthogonal Procrustes problem, and it can be\nused both with unsupervised data embeddings such as PCA\nand supervised embeddings such as canonical correlation\nanalysis (CCA). Our experiments show that the resulting\nbinary coding schemes decisively outperform several other\nstate-of-the-art methods.</p>\n", "tags": ["CVPR", "Has Code", "Image Retrieval", "Quantisation", "Supervised"], "tsne_embedding": [-21.797183990478516, 10.896400451660156]}, {"key": "gong2012angular", "year": "2012", "title": "Angular Quantization-based Binary Codes For Fast Similarity Search", "abstract": "<p>This paper focuses on the problem of learning binary embeddings for efficient retrieval of high-dimensional non-negative data. Such data typically arises in a large number of vision and text applications where counts or frequencies are used as features. Also cosine distance is commonly used as a measure of dissimilarity between such vectors. In this work we introduce a novel spherical quantization scheme to generate binary embedding of such data and analyze its properties. The number of quantization landmarks in this scheme grows exponentially with data dimensionality resulting in low-distortion quantization. We propose a very efficient method for computing the binary embedding using such large number of landmarks. Further a linear transformation is learned to minimize the quantization error by adapting the method to the input data resulting in improved embedding. Experiments on image and text retrieval applications show superior performance of the proposed method over other existing state-of-the-art methods.</p>\n", "tags": ["NEURIPS", "Quantisation", "Text Retrieval"], "tsne_embedding": [-22.03290367126465, 16.418601989746094]}, {"key": "gong2013bilinear", "year": "2013", "title": "Learning Binary Codes for High-Dimensional Data Using Bilinear Projections", "abstract": "<p>Recent advances in visual recognition indicate that to\nachieve good retrieval and classification accuracy on largescale\ndatasets like ImageNet, extremely high-dimensional\nvisual descriptors, e.g., Fisher Vectors, are needed. We\npresent a novel method for converting such descriptors to\ncompact similarity-preserving binary codes that exploits\ntheir natural matrix structure to reduce their dimensionality\nusing compact bilinear projections instead of a single\nlarge projection matrix. This method achieves comparable\nretrieval and classification accuracy to the original descriptors\nand to the state-of-the-art Product Quantization\napproach while having orders of magnitude faster code generation\ntime and smaller memory footprint.</p>\n", "tags": [], "tsne_embedding": [-21.017759323120117, 18.344160079956055]}, {"key": "gong2022unsupervised", "year": "2022", "title": "Vit2hash Unsupervised Information-preserving Hashing", "abstract": "<p>Unsupervised image hashing which maps images into binary codes without supervision is a compressor with a high compression rate. Hence how to preserving meaningful information of the original data is a critical problem. Inspired by the large-scale vision pre-training model known as ViT which has shown significant progress for learning visual representations in this paper we propose a simple information-preserving compressor to finetune the ViT model for the target unsupervised hashing task. Specifically from pixels to continuous features we first propose a feature-preserving module using the corrupted image as input to reconstruct the original feature from the pre-trained ViT model and the complete image so that the feature extractor can focus on preserving the meaningful information of original data. Secondly from continuous features to hash codes we propose a hashing-preserving module which aims to keep the semantic information from the pre-trained ViT model by using the proposed Kullback-Leibler divergence loss. Besides the quantization loss and the similarity loss are added to minimize the quantization error. Our method is very simple and achieves a significantly higher degree of MAP on three benchmark image datasets.</p>\n", "tags": ["ARXIV", "Quantisation", "Unsupervised"], "tsne_embedding": [-5.108421325683594, 17.63566017150879]}, {"key": "gordon2008optimal", "year": "2008", "title": "Optimal Hash Functions For Approximate Closest Pairs On The N-cube", "abstract": "<p>One way to find closest pairs in large datasets is to use hash functions. In recent years locality-sensitive hash functions for various metrics have been given projecting an n-cube onto k bits is simple hash function that performs well. In this paper we investigate alternatives to projection. For various parameters hash functions given by complete decoding algorithms for codes work better and asymptotically random codes perform better than projection.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-29.009811401367188, 13.37504768371582]}, {"key": "gottlieb2014near", "year": "2014", "title": "Near-optimal Sample Compression For Nearest Neighbors", "abstract": "<p>We present the first sample compression algorithm for nearest neighbors with non-trivial performance guarantees. We complement these guarantees by demonstrating almost matching hardness lower bounds which show that our bound is nearly optimal. Our result yields new insight into margin-based nearest neighbor classification in metric spaces and allows us to significantly sharpen and simplify existing bounds. Some encouraging empirical results are also presented.</p>\n", "tags": ["NEURIPS", "Supervised"], "tsne_embedding": [-30.669252395629883, 3.746151924133301]}, {"key": "gou2023critic", "year": "2023", "title": "CRITIC Large Language Models Can Self-correct With Tool-interactive Critiquing", "abstract": "<p>Recent developments in large language models (LLMs) have been impressive. However these models sometimes show inconsistencies and problematic behavior such as hallucinating facts generating flawed code or creating offensive and toxic content. Unlike these models humans typically utilize external tools to cross-check and refine their initial content like using a search engine for fact-checking or a code interpreter for debugging. Inspired by this observation we introduce a framework called CRITIC that allows LLMs which are essentially black boxes to validate and progressively amend their own outputs in a manner similar to human interaction with tools. More specifically starting with an initial output CRITIC interacts with appropriate tools to evaluate certain aspects of the text and then revises the output based on the feedback obtained during this validation process. Comprehensive evaluations involving free-form question answering mathematical program synthesis and toxicity reduction demonstrate that CRITIC consistently enhances the performance of LLMs. Meanwhile our research highlights the crucial importance of external feedback in promoting the ongoing self-improvement of LLMs.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [36.46925354003906, -4.056860446929932]}, {"key": "goyal2022news", "year": "2022", "title": "News Summarization And Evaluation In The Era Of GPT-3", "abstract": "<p>The recent success of prompting large language models like GPT-3 has led to a paradigm shift in NLP research. In this paper we study its impact on text summarization focusing on the classic benchmark domain of news summarization. First we investigate how GPT-3 compares against fine-tuned models trained on large summarization datasets. We show that not only do humans overwhelmingly prefer GPT-3 summaries prompted using only a task description but these also do not suffer from common dataset-specific issues such as poor factuality. Next we study what this means for evaluation particularly the role of gold standard test sets. Our experiments show that both reference-based and reference-free automatic metrics cannot reliably evaluate GPT-3 summaries. Finally we evaluate models on a setting beyond generic summarization specifically keyword-based summarization and show how dominant fine-tuning approaches compare to prompting. To support further research we release (a) a corpus of 10K generated summaries from fine-tuned and prompt-based models across 4 standard summarization benchmarks (b) 1K human preference judgments comparing different systems for generic- and keyword-based summarization.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [30.715078353881836, -0.35375431180000305]}, {"key": "grauman2013learning", "year": "2013", "title": "Learning Binary Hash Codes for Large-Scale Image Search", "abstract": "<p>Algorithms to rapidly search massive image or video collections are critical for many vision applications, including visual search, content-based retrieval, and non-parametric models for object recognition. Recent work shows that learned binary projections are a powerful way to index large collections according to their content. The basic idea is to formulate the projections so as to approximately preserve a given similarity function of interest. Having done so, one can then search the data efficiently using hash tables, or by exploring the Hamming ball volume around a novel query. Both enable sub-linear time retrieval with respect to the database size. Further, depending on the design of the projections, in some cases it is possible to bound the number of database examples that must be searched in order to achieve a given level of accuracy.</p>\n\n<p>This chapter overviews data structures for fast search with binary codes, and then describes several supervised and unsupervised strategies for generating the codes. In particular, we review supervised methods that integrate metric learning, boosting, and neural networks into the hash key construction, and unsupervised methods based on spectral analysis or kernelized random projections that compute affinity-preserving binary codes.Whether learning from explicit semantic supervision or exploiting the structure among unlabeled data, these methods make scalable retrieval possible for a variety of robust visual similarity measures.We focus on defining the algorithms, and illustrate the main points with results using millions of images.</p>\n", "tags": ["Image Retrieval", "Supervised", "Survey Paper"], "tsne_embedding": [-12.475972175598145, 12.157386779785156]}, {"key": "green2019hashgraph", "year": "2019", "title": "Hashgraph -- Scalable Hash Tables Using A Sparse Graph Data Structure", "abstract": "<p>Hash tables are ubiquitous and used in a wide range of applications for efficient probing of large and unsorted data. If designed properly hash-tables can enable efficients look ups in a constant number of operations or commonly referred to as O(1) operations. As data sizes continue to grow and data becomes less structured (as is common for big-data applications) the need for efficient and scalable hash table also grows. In this paper we introduce HashGraph a new scalable approach for building hash tables that uses concepts taken from sparse graph representations\u2013hence the name HashGraph. We show two different variants of HashGraph a simple algorithm that outlines the method to create the hash-table and an advanced method that creates the hash table in a more efficient manner (with an improved memory access pattern). HashGraph shows a new way to deal with hash-collisions that does not use open-addressing or chaining yet has all the benefits of both these approaches. HashGraph currently works for static inputs though recent progress with dynamic graph data structures suggest that HashGraph might be extended to dynamic inputs as well. We show that HashGraph can deal with a large number of hash-values per entry without loss of performance as most open-addressing and chaining approaches have. Further we show that HashGraph is indifferent to the load-factor. Lastly we show a new probing algorithm for the second phase of value lookups. Given the above HashGraph is extremely fast and outperforms several state of the art hash-table implementations. The implementation of HashGraph in this paper is for NVIDIA GPUs though HashGraph is not architecture dependent. Using a NVIDIA GV100 GPU HashGraph is anywhere from 2X-8X faster than cuDPP WarpDrive and cuDF. HashGraph is able to build a hash-table at a rate of 2.5 billion keys per second and can probe at nearly the same rate.</p>\n", "tags": ["ARXIV", "Graph"], "tsne_embedding": [-30.162832260131836, -12.250090599060059]}, {"key": "grossi2011fast", "year": "2011", "title": "Fast Compressed Tries Through Path Decompositions", "abstract": "<p>Tries are popular data structures for storing a set of strings where common prefixes are represented by common root-to-node paths. Over fifty years of usage have produced many variants and implementations to overcome some of their limitations. We explore new succinct representations of path-decomposed tries and experimentally evaluate the corresponding reduction in space usage and memory latency comparing with the state of the art. We study two cases of applications (1) a compressed dictionary for (compressed) strings and (2) a monotone minimal perfect hash for strings that preserves their lexicographic order. For (1) we obtain data structures that outperform other state-of-the-art compressed dictionaries in space efficiency while obtaining predictable query times that are competitive with data structures preferred by the practitioners. In (2) our tries perform several times faster than other trie-based monotone perfect hash functions while occupying nearly the same space.</p>\n", "tags": ["ARXIV", "Graph", "Independent"], "tsne_embedding": [-21.78024673461914, -13.135191917419434]}, {"key": "gu2015cross", "year": "2015", "title": "Cross-modality Hashing With Partial Correspondence", "abstract": "<p>Learning a hashing function for cross-media search is very desirable due to its low storage cost and fast query speed. However the data crawled from Internet cannot always guarantee good correspondence among different modalities which affects the learning for hashing function. In this paper we focus on cross-modal hashing with partially corresponded data. The data without full correspondence are made in use to enhance the hashing performance. The experiments on Wiki and NUS-WIDE datasets demonstrates that the proposed method outperforms some state-of-the-art hashing approaches with fewer correspondence information.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [-5.01358699798584, -11.690191268920898]}, {"key": "gu2022accelerating", "year": "2022", "title": "Accelerating Code Search With Deep Hashing And Code Classification", "abstract": "<p>Code search is to search reusable code snippets from source code corpus based on natural languages queries. Deep learning-based methods of code search have shown promising results. However previous methods focus on retrieval accuracy but lacked attention to the efficiency of the retrieval process. We propose a novel method CoSHC to accelerate code search with deep hashing and code classification aiming to perform an efficient code search without sacrificing too much accuracy. To evaluate the effectiveness of CoSHC we apply our method to five code search models. Extensive experimental results indicate that compared with previous code search baselines CoSHC can save more than 9037; of retrieval time meanwhile preserving at least 9937; of retrieval accuracy.</p>\n", "tags": ["ARXIV", "Deep Learning", "Supervised"], "tsne_embedding": [11.24754810333252, -1.522302508354187]}, {"key": "gu2023mamba", "year": "2023", "title": "Mamba Linear-time Sequence Modeling With Selective State Spaces", "abstract": "<p>Foundation models now powering most of the exciting applications in deep learning are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention gated convolution and recurrent models and structured state space models (SSMs) have been developed to address Transformers computational inefficiency on long sequences but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning and make several improvements. First simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second even though this change prevents the use of efficient convolutions we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5(times) higher throughput than Transformers) and linear scaling in sequence length and its performance improves on real data up to million-length sequences. As a general sequence model backbone Mamba achieves state-of-the-art performance across several modalities such as language audio and genomics. On language modeling our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size both in pretraining and downstream evaluation.</p>\n", "tags": ["ARXIV", "Deep Learning", "Supervised"], "tsne_embedding": [14.254257202148438, -11.148470878601074]}, {"key": "gu2023pre", "year": "2023", "title": "Pre-training To Learn In Context", "abstract": "<p>In-context learning where pre-trained language models learn to perform tasks from task examples and instructions in their contexts has attracted much attention in the NLP community. However the ability of in-context learning is not fully exploited because language models are not explicitly trained to learn in context. To this end we propose PICL (Pre-training for In-Context Learning) a framework to enhance the language models in-context learning ability by pre-training the model on a large collection of intrinsic tasks in the general plain-text corpus using the simple language modeling objective. PICL encourages the model to infer and perform tasks by conditioning on the contexts while maintaining task generalization of pre-trained models. We evaluate the in-context learning performance of the model trained with PICL on seven widely-used text classification datasets and the Super-NaturalInstrctions benchmark which contains 100+ NLP tasks formulated to text generation. Our experiments show that PICL is more effective and task-generalizable than a range of baselines outperforming larger language models with nearly 4x parameters. The code is publicly available at https://github.com/thu-coai/PICL.</p>\n", "tags": ["ARXIV", "Has Code", "Supervised"], "tsne_embedding": [29.374284744262695, -6.520416259765625]}, {"key": "guha2012image", "year": "2012", "title": "Image Similarity Using Sparse Representation And Compression Distance", "abstract": "<p>A new line of research uses compression methods to measure the similarity between signals. Two signals are considered similar if one can be compressed significantly when the information of the other is known. The existing compression-based similarity methods although successful in the discrete one dimensional domain do not work well in the context of images. This paper proposes a sparse representation-based approach to encode the information content of an image using information from the other image and uses the compactness (sparsity) of the representation as a measure of its compressibility (how much can the image be compressed) with respect to the other image. The more sparse the representation of an image the better it can be compressed and the more it is similar to the other image. The efficacy of the proposed measure is demonstrated through the high accuracies achieved in image clustering retrieval and classification.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [-24.103923797607422, 19.185617446899414]}, {"key": "gui2019fast", "year": "2019", "title": "Fast Supervised Discrete Hashing", "abstract": "<p>Learning-based hashing algorithms are hot topics because they can greatly increase the scale at which existing methods operate. In this paper we propose a new learning-based hashing method called fast supervised discrete hashing (FSDH) based on supervised discrete hashing (SDH). Regressing the training examples (or hash code) to the corresponding class labels is widely used in ordinary least squares regression. Rather than adopting this method FSDH uses a very simple yet effective regression of the class labels of training examples to the corresponding hash code to accelerate the algorithm. To the best of our knowledge this strategy has not previously been used for hashing. Traditional SDH decomposes the optimization into three sub-problems with the most critical sub-problem - discrete optimization for binary hash codes - solved using iterative discrete cyclic coordinate descent (DCC) which is time-consuming. However FSDH has a closed-form solution and only requires a single rather than iterative hash code-solving step which is highly efficient. Furthermore FSDH is usually faster than SDH for solving the projection matrix for least squares regression making FSDH generally faster than SDH. For example our results show that FSDH is about 12-times faster than SDH when the number of hashing bits is 128 on the CIFAR-10 data base and FSDH is about 151-times faster than FastHash when the number of hashing bits is 64 on the MNIST data-base. Our experimental results show that FSDH is not only fast but also outperforms other comparative methods.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [-1.0215972661972046, -8.517343521118164]}, {"key": "gui2019supervised", "year": "2019", "title": "Supervised Discrete Hashing With Relaxation", "abstract": "<p>Data-dependent hashing has recently attracted attention due to being able to support efficient retrieval and storage of high-dimensional data such as documents images and videos. In this paper we propose a novel learning-based hashing method called Supervised Discrete Hashing with Relaxation (SDHR) based on Supervised Discrete Hashing (SDH). SDH uses ordinary least squares regression and traditional zero-one matrix encoding of class label information as the regression target (code words) thus fixing the regression target. In SDHR the regression target is instead optimized. The optimized regression target matrix satisfies a large margin constraint for correct classification of each example. Compared with SDH which uses the traditional zero-one matrix SDHR utilizes the learned regression target matrix and therefore more accurately measures the classification error of the regression model and is more flexible. As expected SDHR generally outperforms SDH. Experimental results on two large-scale image datasets (CIFAR-10 and MNIST) and a large-scale and challenging face dataset (FRGC) demonstrate the effectiveness and efficiency of SDHR.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [-4.55279541015625, 9.703596115112305]}, {"key": "gunjal2023detecting", "year": "2023", "title": "Detecting And Preventing Hallucinations In Large Vision Language Models", "abstract": "<p>Instruction tuned Large Vision Language Models (LVLMs) have significantly advanced in generalizing across a diverse set of multi-modal tasks especially for Visual Question Answering (VQA). However generating detailed responses that are visually grounded is still a challenging task for these models. We find that even the current state-of-the-art LVLMs (InstructBLIP) still contain a staggering 30 percent of the hallucinatory text in the form of non-existent objects unfaithful descriptions and inaccurate relationships. To address this we introduce M-HalDetect a (M)ultimodal (Hal)lucination (Detect)ion Dataset that can be used to train and benchmark models for hallucination detection and prevention. M-HalDetect consists of 16k fine-grained annotations on VQA examples making it the first comprehensive multi-modal hallucination detection dataset for detailed image descriptions. Unlike previous work that only consider object hallucination we additionally annotate both entity descriptions and relationships that are unfaithful. To demonstrate the potential of this dataset for hallucination prevention we optimize InstructBLIP through our novel Fine-grained Direct Preference Optimization (FDPO). We also train fine-grained multi-modal reward models from InstructBLIP and evaluate their effectiveness with best-of-n rejection sampling. We perform human evaluation on both FDPO and rejection sampling and find that they reduce hallucination rates in InstructBLIP by 4137; and 5537; respectively. We also find that our reward model generalizes to other multi-modal models reducing hallucinations in LLaVA and mPLUG-OWL by 1537; and 5737; respectively and has strong correlation with human evaluated accuracy scores.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [48.00926971435547, 8.286876678466797]}, {"key": "guo2014tight", "year": "2014", "title": "On Tight Bounds For Binary Frameproof Codes", "abstract": "<p>In this paper we study w-frameproof codes which are equivalent to 1w-separating hash families. Our main results concern binary codes which are defined over an alphabet of two symbols. For all w (geq) 3 and for w+1 (leq) N (leq) 3w we show that an SHF(N; n2 1w ) exists only if n (leq) N and an SHF(N; N2 1w ) must be a permutation matrix of degree N.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-37.60163879394531, 3.9602720737457275]}, {"key": "guo2015cnn", "year": "2015", "title": "CNN Based Hashing For Image Retrieval", "abstract": "<p>Along with data on the web increasing dramatically hashing is becoming more and more popular as a method of approximate nearest neighbor search. Previous supervised hashing methods utilized similarity/dissimilarity matrix to get semantic information. But the matrix is not easy to construct for a new dataset. Rather than to reconstruct the matrix we proposed a straightforward CNN-based hashing method i.e. binarilizing the activations of a fully connected layer with threshold 0 and taking the binary result as hash codes. This method achieved the best performance on CIFAR-10 and was comparable with the state-of-the-art on MNIST. And our experiments on CIFAR-10 suggested that the signs of activations may carry more information than the relative values of activations between samples and that the co-adaption between feature extractor and hash functions is important for hashing.</p>\n", "tags": ["ARXIV", "CNN", "Image Retrieval", "Supervised"], "tsne_embedding": [5.779426574707031, 14.421566009521484]}, {"key": "guo2015tight", "year": "2015", "title": "A Tight Bound On The Size Of Certain Separating Hash Families", "abstract": "<p>In this paper we present a new lower bound on the size of separating hash families of type w_1^q-1w_2 where w_1 &lt; w_2. Our result extends the paper by Guo et al. on binary frameproof codes. This bound compares well against known general bounds and is especially useful when trying to bound the size of strong separating hash families. We also show that our new bound is tight by constructing hash families that meet the new bound with equality.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-36.83980941772461, 4.150317668914795]}, {"key": "guo2019deep", "year": "2019", "title": "Deep Hashing For Signed Social Network Embedding", "abstract": "<p>Network embedding is a promising way of network representation facilitating many signed social network processing and analysis tasks such as link prediction and node classification. Recently feature hashing has been adopted in several existing embedding algorithms to improve the efficiency which has obtained a great success. However the existing feature hashing based embedding algorithms only consider the positive links in signed social networks. Intuitively negative links can also help improve the performance. Thus in this paper we propose a novel deep hashing method for signed social network embedding by considering simultaneously positive and negative links. Extensive experiments show that the proposed method performs better than several state-of-the-art baselines through link prediction task over two real-world signed social networks.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [6.830901622772217, 14.984766960144043]}, {"key": "guo2020deep", "year": "2020", "title": "Deep Kernel Supervised Hashing For Node Classification In Structural Networks", "abstract": "<p>Node classification in structural networks has been proven to be useful in many real world applications. With the development of network embedding the performance of node classification has been greatly improved. However nearly all the existing network embedding based methods are hard to capture the actual category features of a node because of the linearly inseparable problem in low-dimensional space; meanwhile they cannot incorporate simultaneously network structure information and node label information into network embedding. To address the above problems in this paper we propose a novel Deep Kernel Supervised Hashing (DKSH) method to learn the hashing representations of nodes for node classification. Specifically a deep multiple kernel learning is first proposed to map nodes into suitable Hilbert space to deal with linearly inseparable problem. Then instead of only considering structural similarity between two nodes a novel similarity matrix is designed to merge both network structure information and node label information. Supervised by the similarity matrix the learned hashing representations of nodes simultaneously preserve the two kinds of information well from the learned Hilbert space. Extensive experiments show that the proposed method significantly outperforms the state-of-the-art baselines over three real world benchmark datasets.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [8.11190414428711, -0.8429914116859436]}, {"key": "guo2022from", "year": "2022", "title": "From Images To Textual Prompts Zero-shot VQA With Frozen Large Language Models", "abstract": "<p>Large language models (LLMs) have demonstrated excellent zero-shot generalization to new language tasks. However effective utilization of LLMs for zero-shot visual question-answering (VQA) remains challenging primarily due to the modality disconnection and task disconnection between LLM and VQA task. End-to-end training on vision and language data may bridge the disconnections but is inflexible and computationally expensive. To address this issue we propose emphImg2Prompt a plug-and-play module that provides the prompts that can bridge the aforementioned modality and task disconnections so that LLMs can perform zero-shot VQA tasks without end-to-end training. In order to provide such prompts we further employ LLM-agnostic models to provide prompts that can describe image content and self-constructed question-answer pairs which can effectively guide LLM to perform zero-shot VQA tasks. Img2Prompt offers the following benefits 1) It can flexibly work with various LLMs to perform VQA. 2)~Without the needing of end-to-end training it significantly reduces the cost of deploying LLM for zero-shot VQA tasks. 3) It achieves comparable or better performance than methods relying on end-to-end training. For example we outperform Flamingo citeDeepmindFlamingo2022 by 5.637; on VQAv2. On the challenging A-OKVQA dataset our method even outperforms few-shot methods by as much as 2037;.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [26.19239616394043, 7.693190574645996]}, {"key": "guo2023how", "year": "2023", "title": "How Close Is Chatgpt To Human Experts Comparison Corpus Evaluation And Detection", "abstract": "<p>The introduction of ChatGPT has garnered widespread attention in both academic and industrial communities. ChatGPT is able to respond effectively to a wide range of human questions providing fluent and comprehensive answers that significantly surpass previous public chatbots in terms of security and usefulness. On one hand people are curious about how ChatGPT is able to achieve such strength and how far it is from human experts. On the other hand people are starting to worry about the potential negative impacts that large language models (LLMs) like ChatGPT could have on society such as fake news plagiarism and social security issues. In this work we collected tens of thousands of comparison responses from both human experts and ChatGPT with questions ranging from open-domain financial medical legal and psychological areas. We call the collected dataset the Human ChatGPT Comparison Corpus (HC3). Based on the HC3 dataset we study the characteristics of ChatGPTs responses the differences and gaps from human experts and future directions for LLMs. We conducted comprehensive human evaluations and linguistic analyses of ChatGPT-generated content compared with that of humans where many interesting results are revealed. After that we conduct extensive experiments on how to effectively detect whether a certain text is generated by ChatGPT or humans. We build three different detection systems explore several key factors that influence their effectiveness and evaluate them in different scenarios. The dataset code and models are all publicly available at https://github.com/Hello-SimpleAI/chatgpt-comparison-detection.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [47.0115852355957, -5.210923194885254]}, {"key": "gupta2010hashing", "year": "2010", "title": "Hashing Image Patches For Zooming", "abstract": "<p>In this paper we present a Bayesian image zooming/super-resolution algorithm based on a patch based representation. We work on a patch based model with overlap and employ a Locally Linear Embedding (LLE) based approach as our data fidelity term in the Bayesian inference. The image prior imposes continuity constraints across the overlapping patches. We apply an error back-projection technique with an approximate cross bilateral filter. The problem of nearest neighbor search is handled by a variant of the locality sensitive hashing (LSH) scheme. The novelty of our work lies in the speed up achieved by the hashing scheme and the robustness and inherent modularity and parallel structure achieved by the LLE setup. The ill-posedness of the image reconstruction problem is handled by the introduction of regularization priors which encode the knowledge present in vast collections of natural images. We present comparative results for both run-time as well as visual image quality based measurements.</p>\n", "tags": ["ARXIV", "Independent", "LSH"], "tsne_embedding": [-9.917133331298828, 20.876989364624023]}, {"key": "hamann2019hamming", "year": "2019", "title": "Hamming Sentence Embeddings For Information Retrieval", "abstract": "<p>In retrieval applications binary hashes are known to offer significant improvements in terms of both memory and speed. We investigate the compression of sentence embeddings using a neural encoder-decoder architecture which is trained by minimizing reconstruction error. Instead of employing the original real-valued embeddings we use latent representations in Hamming space produced by the encoder for similarity calculations. In quantitative experiments on several benchmarks for semantic similarity tasks we show that our compressed hamming embeddings yield a comparable performance to uncompressed embeddings (Sent2Vec InferSent Glove-BoW) at compression ratios of up to 2561. We further demonstrate that our model strongly decorrelates input features and that the compressor generalizes well when pre-trained on Wikipedia sentences. We publish the source code on Github and all experimental results.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [15.271554946899414, 7.578728199005127]}, {"key": "han2024hashing", "year": "2024", "title": "Hashing Based Contrastive Learning For Virtual Screening", "abstract": "<p>Virtual screening (VS) is a critical step in computer-aided drug discovery aiming to identify molecules that bind to a specific target receptor like protein. Traditional VS methods such as docking are often too time-consuming for screening large-scale molecular databases. Recent advances in deep learning have demonstrated that learning vector representations for both proteins and molecules using contrastive learning can outperform traditional docking methods. However given that target databases often contain billions of molecules real-valued vector representations adopted by existing methods can still incur significant memory and time costs in VS. To address this problem in this paper we propose a hashing-based contrastive learning method called DrugHash for VS. DrugHash treats VS as a retrieval task that uses efficient binary hash codes for retrieval. In particular DrugHash designs a simple yet effective hashing strategy to enable end-to-end learning of binary hash codes for both protein and molecule modalities which can dramatically reduce the memory and time costs with higher accuracy compared with existing methods. Experimental results show that DrugHash can outperform existing methods to achieve state-of-the-art accuracy with a memory saving of 32(times) and a speed improvement of 3.5(times).</p>\n", "tags": ["ARXIV", "Deep Learning", "Self Supervised"], "tsne_embedding": [12.061064720153809, -19.252445220947266]}, {"key": "han2024instinctive", "year": "2024", "title": "The Instinctive Bias Spurious Images Lead To Hallucination In Mllms", "abstract": "<p>Large language models (LLMs) have recently experienced remarkable progress where the advent of multi-modal large language models (MLLMs) has endowed LLMs with visual capabilities leading to impressive performances in various multi-modal tasks. However those powerful MLLMs such as GPT-4V still fail spectacularly when presented with certain image and text inputs. In this paper we identify a typical class of inputs that baffles MLLMs which consist of images that are highly relevant but inconsistent with answers causing MLLMs to suffer from hallucination. To quantify the effect we propose CorrelationQA the first benchmark that assesses the hallucination level given spurious images. This benchmark contains 7308 text-image pairs across 13 categories. Based on the proposed CorrelationQA we conduct a thorough analysis on 9 mainstream MLLMs illustrating that they universally suffer from this instinctive bias to varying degrees. We hope that our curated benchmark and evaluation results aid in better assessments of the MLLMs robustness in the presence of misleading images. The resource is available in https://github.com/MasaiahHan/CorrelationQA.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [50.92482376098633, 9.046446800231934]}, {"key": "hansen2019unsupervised", "year": "2019", "title": "Unsupervised Neural Generative Semantic Hashing", "abstract": "<p>Fast similarity search is a key component in large-scale information retrieval, where semantic hashing has become a popular strategy for representing documents as binary hash codes. Recent advances in this area have been obtained through neural network based models: generative models trained by learning to reconstruct the original documents. We present a novel unsupervised generative semantic hashing approach, \\textit{Ranking based Semantic Hashing} (RBSH) that consists of both a variational and a ranking based component. Similarly to variational autoencoders, the variational component is trained to reconstruct the original document conditioned on its generated hash code, and as in prior work, it only considers documents individually. The ranking component solves this limitation by incorporating inter-document similarity into the hash code generation, modelling document ranking through a hinge loss. To circumvent the need for labelled data to compute the hinge loss, we use a weak labeller and thus keep the approach fully unsupervised.\nExtensive experimental evaluation on four publicly available datasets against traditional baselines and recent state-of-the-art methods for semantic hashing shows that RBSH significantly outperforms all other methods across all evaluated hash code lengths. In fact, RBSH hash codes are able to perform similarly to state-of-the-art hash codes while using 2-4x fewer bits.</p>\n", "tags": ["Deep Learning", "Has Code", "SIGIR", "Supervised"], "tsne_embedding": [-6.325540542602539, -3.0673415660858154]}, {"key": "hansen2020content", "year": "2020", "title": "Content-aware Neural Hashing for Cold-start Recommendation", "abstract": "<p>Content-aware recommendation approaches are essential for providing meaningful recommendations for new (i.e., cold-start) items in a recommender system. We present a content-aware neural hashing-based collaborative filtering approach (NeuHash-CF), which generates binary hash codes for users and items, such that the highly efficient Hamming distance can be used for estimating user-item relevance. NeuHash-CF is modelled as an autoencoder architecture, consisting of two joint hashing components for generating user and item hash codes. Inspired from semantic hashing, the item hashing component generates a hash code directly from an item\u2019s content information (i.e., it generates cold-start and seen item hash codes in the same manner). This contrasts existing state-of-the-art models, which treat the two item cases separately. The user hash codes are generated directly based on user id, through learning a user embedding matrix. We show experimentally that NeuHash-CF significantly outperforms state-of-the-art baselines by up to 12% NDCG and 13% MRR in cold-start recommendation settings, and up to 4% in both NDCG and MRR in standard settings where all items are present while training. Our approach uses 2-4x shorter hash codes, while obtaining the same or better performance compared to the state of the art, thus consequently also enabling a notable storage reduction.</p>\n", "tags": ["Deep Learning", "SIGIR"], "tsne_embedding": [-11.894206047058105, -5.745147228240967]}, {"key": "hansen2020unsupervised", "year": "2020", "title": "Unsupervised Semantic Hashing With Pairwise Reconstruction", "abstract": "<p>Semantic Hashing is a popular family of methods for efficient similarity search in large-scale datasets. In Semantic Hashing documents are encoded as short binary vectors (i.e. hash codes) such that semantic similarity can be efficiently computed using the Hamming distance. Recent state-of-the-art approaches have utilized weak supervision to train better performing hashing models. Inspired by this we present Semantic Hashing with Pairwise Reconstruction (PairRec) which is a discrete variational autoencoder based hashing model. PairRec first encodes weakly supervised training pairs (a query document and a semantically similar document) into two hash codes and then learns to reconstruct the same query document from both of these hash codes (i.e. pairwise reconstruction). This pairwise reconstruction enables our model to encode local neighbourhood structures within the hash code directly through the decoder. We experimentally compare PairRec to traditional and state-of-the-art approaches and obtain significant performance improvements in the task of document similarity search.</p>\n", "tags": ["ARXIV", "Unsupervised", "Weakly Supervised"], "tsne_embedding": [-13.068885803222656, -7.139044761657715]}, {"key": "hansen2021representation", "year": "2021", "title": "Representation Learning For Efficient And Effective Similarity Search And Recommendation", "abstract": "<p>How data is represented and operationalized is critical for building computational solutions that are both effective and efficient. A common approach is to represent data objects as binary vectors denoted (textit)hash codes which require little storage and enable efficient similarity search through direct indexing into a hash table or through similarity computations in an appropriate space. Due to the limited expressibility of hash codes compared to real-valued representations a core open challenge is how to generate hash codes that well capture semantic content or latent properties using a small number of bits while ensuring that the hash codes are distributed in a way that does not reduce their search efficiency. State of the art methods use representation learning for generating such hash codes focusing on neural autoencoder architectures where semantics are encoded into the hash codes by learning to reconstruct the original inputs of the hash codes. This thesis addresses the above challenge and makes a number of contributions to representation learning that (i) improve effectiveness of hash codes through more expressive representations and a more effective similarity measure than the current state of the art namely the Hamming distance and (ii) improve efficiency of hash codes by learning representations that are especially suited to the choice of search method. The contributions are empirically validated on several tasks related to similarity search and recommendation.</p>\n", "tags": ["ARXIV", "Unsupervised"], "tsne_embedding": [0.8930085897445679, 0.10812656581401825]}, {"key": "hansen2021unsupervised", "year": "2021", "title": "Unsupervised Multi-index Semantic Hashing", "abstract": "<p>Semantic hashing represents documents as compact binary vectors (hash codes) and allows both efficient and effective similarity search in large-scale information retrieval. The state of the art has primarily focused on learning hash codes that improve similarity search effectiveness while assuming a brute-force linear scan strategy for searching over all the hash codes even though much faster alternatives exist. One such alternative is multi-index hashing an approach that constructs a smaller candidate set to search over which depending on the distribution of the hash codes can lead to sub-linear search time. In this work we propose Multi-Index Semantic Hashing (MISH) an unsupervised hashing model that learns hash codes that are both effective and highly efficient by being optimized for multi-index hashing. We derive novel training objectives which enable to learn hash codes that reduce the candidate sets produced by multi-index hashing while being end-to-end trainable. In fact our proposed training objectives are model agnostic i.e. not tied to how the hash codes are generated specifically in MISH and are straight-forward to include in existing and future semantic hashing models. We experimentally compare MISH to state-of-the-art semantic hashing baselines in the task of document similarity search. We find that even though multi-index hashing also improves the efficiency of the baselines compared to a linear scan they are still upwards of 3337; slower than MISH while MISH is still able to obtain state-of-the-art effectiveness.</p>\n", "tags": ["ARXIV", "Unsupervised"], "tsne_embedding": [-2.291736364364624, -4.149066925048828]}, {"key": "hao2022language", "year": "2022", "title": "Language Models Are General-purpose Interfaces", "abstract": "<p>Foundation models have received much attention due to their effectiveness across a broad range of downstream applications. Though there is a big convergence in terms of architecture most pretrained models are typically still developed for specific tasks or modalities. In this work we propose to use language models as a general-purpose interface to various foundation models. A collection of pretrained encoders perceive diverse modalities (such as vision and language) and they dock with a language model that plays the role of a universal task layer. We propose a semi-causal language modeling objective to jointly pretrain the interface and the modular encoders. We subsume the advantages and capabilities from both causal and non-causal modeling thereby combining the best of two worlds. Specifically the proposed method not only inherits the capabilities of in-context learning and open-ended generation from causal language modeling but also is conducive to finetuning because of the bidirectional encoders. More importantly our approach seamlessly unlocks the combinations of the above capabilities e.g. enabling in-context learning or instruction following with finetuned encoders. Experimental results across various language-only and vision-language benchmarks show that our model outperforms or is competitive with specialized models on finetuning zero-shot generalization and few-shot learning.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [28.675851821899414, -11.701838493347168]}, {"key": "hao2022structured", "year": "2022", "title": "Structured Prompting Scaling In-context Learning To 1000 Examples", "abstract": "<p>Large language models have exhibited intriguing in-context learning capability achieving promising zero- and few-shot performance without updating the parameters. However conventional in-context learning is usually restricted by length constraints rendering it ineffective to absorb supervision from a large number of examples. In order to go beyond few shots we introduce structured prompting that breaks the length limit and scales in-context learning to thousands of examples. Specifically demonstration examples are separately encoded with well-designed position embeddings and then they are jointly attended by the test example using a rescaled attention mechanism. So we can scale the number of exemplars with linear complexity instead of quadratic complexity with respect to length. Experimental results on a diverse set of tasks show that our approach improves end-task performance and reduces evaluation variance over conventional in-context learning as the number of demonstration examples increases. Code has been released at https://aka.ms/structured-prompting.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [24.209197998046875, -3.3097105026245117]}, {"key": "hao2023reasoning", "year": "2023", "title": "Reasoning With Language Model Is Planning With World Model", "abstract": "<p>Large language models (LLMs) have shown remarkable reasoning capabilities especially when prompted to generate intermediate reasoning steps (e.g. Chain-of-Thought CoT). However LLMs can still struggle with problems that are easy for humans such as generating action plans for executing tasks in a given environment or performing complex math logical and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal () to predict the world () (e.g. environment status intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains which involves exploring alternative reasoning paths anticipating future states and rewards and iteratively refining existing reasoning steps. To overcome the limitations we propose a new LLM reasoning framework ()easoning vi() ()lanning (). RAP repurposes the LLM as both a world model and a reasoning agent and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards and obtains a high-reward reasoning path efficiently with a proper balance between exploration () exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation math reasoning and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 3337; relative improvement in a plan generation setting.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [39.250484466552734, -3.930723190307617]}, {"key": "he2012difficulty", "year": "2012", "title": "On The Difficulty Of Nearest Neighbor Search", "abstract": "<p>Fast approximate nearest neighbor (NN) search in large databases is becoming popular. Several powerful learning-based formulations have been proposed recently. However not much attention has been paid to a more fundamental question how difficult is (approximate) nearest neighbor search in a given data set And which data properties affect the difficulty of nearest neighbor search and how This paper introduces the first concrete measure called Relative Contrast that can be used to evaluate the influence of several crucial data characteristics such as dimensionality sparsity and database size simultaneously in arbitrary normed metric spaces. Moreover we present a theoretical analysis to prove how the difficulty measure (relative contrast) determines/affects the complexity of Local Sensitive Hashing a popular approximate NN search method. Relative contrast also provides an explanation for a family of heuristic hashing algorithms with good practical performance based on PCA. Finally we show that most of the previous works in measuring NN search meaningfulness/difficulty can be derived as special asymptotic cases for dense vectors of the proposed measure.</p>\n", "tags": ["ARXIV", "Unsupervised"], "tsne_embedding": [-20.91233253479004, -7.467895984649658]}, {"key": "he2017hashing", "year": "2017", "title": "Hashing As Tie-aware Learning To Rank", "abstract": "<p>Hashing or learning binary embeddings of data is frequently used in nearest neighbor retrieval. In this paper we develop learning to rank formulations for hashing aimed at directly optimizing ranking-based evaluation metrics such as Average Precision (AP) and Normalized Discounted Cumulative Gain (NDCG). We first observe that the integer-valued Hamming distance often leads to tied rankings and propose to use tie-aware versions of AP and NDCG to evaluate hashing for retrieval. Then to optimize tie-aware ranking metrics we derive their continuous relaxations and perform gradient-based optimization with deep neural networks. Our results establish the new state-of-the-art for image retrieval by Hamming ranking in common benchmarks.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Supervised"], "tsne_embedding": [-14.295194625854492, 17.332115173339844]}, {"key": "he2018hashing", "year": "2018", "title": "Hashing as Tie-Aware Learning to Rank", "abstract": "<p>Hashing, or learning binary embeddings of data, is frequently used in nearest neighbor retrieval. In this paper, we develop learning to rank formulations for hashing, aimed at directly optimizing ranking-based evaluation metrics such as Average Precision (AP) and Normalized Discounted Cumulative Gain (NDCG). We first observe that the integer-valued Hamming distance often leads to tied rankings, and propose to use tie-aware versions of AP and NDCG to evaluate hashing for retrieval. Then, to optimize tie-aware ranking metrics, we derive their continuous relaxations, and perform gradient-based optimization with deep neural networks. Our results establish the new state-of-the-art for image retrieval by Hamming ranking in common benchmarks.</p>\n", "tags": ["CVPR", "Has Code"], "tsne_embedding": [-14.23481273651123, 17.288715362548828]}, {"key": "he2019knearest", "year": "2019", "title": "K-Nearest Neighbors Hashing", "abstract": "<p>Hashing based approximate nearest neighbor search embeds high dimensional data to compact binary codes, which\nenables efficient similarity search and storage. However,\nthe non-isometry sign(\u00b7) function makes it hard to project\nthe nearest neighbors in continuous data space into the\nclosest codewords in discrete Hamming space. In this work,\nwe revisit the sign(\u00b7) function from the perspective of space partitioning.\nIn specific, we bridge the gap between\nk-nearest neighbors and binary hashing codes with Shannon entropy. We further propose a novel K-Nearest Neighbors Hashing (KNNH) method to learn binary representations from KNN within the subspaces generated by sign(\u00b7).\nTheoretical and experimental results show that the KNN relation is of central importance to neighbor preserving embeddings, and the proposed method outperforms the state-of-the-arts on benchmark datasets.</p>\n", "tags": ["CVPR"], "tsne_embedding": [-29.150257110595703, 12.462050437927246]}, {"key": "he2023exploring", "year": "2023", "title": "Exploring Human-like Translation Strategy With Large Language Models", "abstract": "<p>Large language models (LLMs) have demonstrated impressive capabilities in general scenarios exhibiting a level of aptitude that approaches in some aspects even surpasses human-level intelligence. Among their numerous skills the translation abilities of LLMs have received considerable attention. Compared to typical machine translation that focuses solely on source-to-target mapping LLM-based translation can potentially mimic the human translation process which might take preparatory steps to ensure high-quality translation. This work explores this possibility by proposing the MAPS framework which stands for Multi-Aspect Prompting and Selection. Specifically we enable LLMs first to analyze the given source sentence and induce three aspects of translation-related knowledge keywords topics and relevant demonstrations to guide the final translation process. Moreover we employ a selection mechanism based on quality estimation to filter out noisy and unhelpful knowledge. Both automatic (3 LLMs x 11 directions x 2 automatic metrics) and human evaluation (preference study and MQM) demonstrate the effectiveness of MAPS. Further analysis shows that by mimicking the human translation process MAPS reduces various translation errors such as hallucination ambiguity mistranslation awkward style untranslated text and omission. Source code is available at https://github.com/zwhe99/MAPS-mt.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [35.193294525146484, -2.1204700469970703]}, {"key": "he2023large", "year": "2023", "title": "Large Language Models As Zero-shot Conversational Recommenders", "abstract": "<p>In this paper we present empirical studies on conversational recommendation tasks using representative large language models in a zero-shot setting with three primary contributions. (1) Data To gain insights into model behavior in in-the-wild conversational recommendation scenarios we construct a new dataset of recommendation-related conversations by scraping a popular discussion website. This is the largest public real-world conversational recommendation dataset to date. (2) Evaluation On the new dataset and two existing conversational recommendation datasets we observe that even without fine-tuning large language models can outperform existing fine-tuned conversational recommendation models. (3) Analysis We propose various probing tasks to investigate the mechanisms behind the remarkable performance of large language models in conversational recommendation. We analyze both the large language models behaviors and the characteristics of the datasets providing a holistic understanding of the models effectiveness limitations and suggesting directions for the design of future conversational recommenders</p>\n", "tags": ["ARXIV"], "tsne_embedding": [44.942596435546875, -6.822251319885254]}, {"key": "he2024bit", "year": "2024", "title": "Bit-mask Robust Contrastive Knowledge Distillation For Unsupervised Semantic Hashing", "abstract": "<p>Unsupervised semantic hashing has emerged as an indispensable technique for fast image search which aims to convert images into binary hash codes without relying on labels. Recent advancements in the field demonstrate that employing large-scale backbones (e.g. ViT) in unsupervised semantic hashing models can yield substantial improvements. However the inference delay has become increasingly difficult to overlook. Knowledge distillation provides a means for practical model compression to alleviate this delay. Nevertheless the prevailing knowledge distillation approaches are not explicitly designed for semantic hashing. They ignore the unique search paradigm of semantic hashing the inherent necessities of the distillation process and the property of hash codes. In this paper we propose an innovative Bit-mask Robust Contrastive knowledge Distillation (BRCD) method specifically devised for the distillation of semantic hashing models. To ensure the effectiveness of two kinds of search paradigms in the context of semantic hashing BRCD first aligns the semantic spaces between the teacher and student models through a contrastive knowledge distillation objective. Additionally to eliminate noisy augmentations and ensure robust optimization a cluster-based method within the knowledge distillation process is introduced. Furthermore through a bit-level analysis we uncover the presence of redundancy bits resulting from the bit independence property. To mitigate these effects we introduce a bit mask mechanism in our knowledge distillation objective. Finally extensive experiments not only showcase the noteworthy performance of our BRCD method in comparison to other knowledge distillation methods but also substantiate the generality of our methods across diverse semantic hashing models and backbones. The code for BRCD is available at https://github.com/hly1998/BRCD.\u201d</p>\n", "tags": ["ARXIV", "Has Code", "Unsupervised"], "tsne_embedding": [3.972064256668091, -0.46288707852363586]}, {"key": "he2024hybridhash", "year": "2024", "title": "Hybridhash Hybrid Convolutional And Self-attention Deep Hashing For Image Retrieval", "abstract": "<p>Deep image hashing aims to map input images into simple binary hash codes via deep neural networks and thus enable effective large-scale image retrieval. Recently hybrid networks that combine convolution and Transformer have achieved superior performance on various computer tasks and have attracted extensive attention from researchers. Nevertheless the potential benefits of such hybrid networks in image retrieval still need to be verified. To this end we propose a hybrid convolutional and self-attention deep hashing method known as HybridHash. Specifically we propose a backbone network with stage-wise architecture in which the block aggregation function is introduced to achieve the effect of local self-attention and reduce the computational complexity. The interaction module has been elaborately designed to promote the communication of information between image blocks and to enhance the visual representations. We have conducted comprehensive experiments on three widely used datasets CIFAR-10 NUS-WIDE and IMAGENET. The experimental results demonstrate that the method proposed in this paper has superior performance with respect to state-of-the-art deep hashing methods. Source code is available https://github.com/shuaichaochao/HybridHash.</p>\n", "tags": ["ARXIV", "Has Code", "Image Retrieval", "Supervised"], "tsne_embedding": [-4.034662246704102, 12.145328521728516]}, {"key": "heddes2022hyperdimensional", "year": "2022", "title": "Hyperdimensional Hashing A Robust And Efficient Dynamic Hash Table", "abstract": "<p>Most cloud services and distributed applications rely on hashing algorithms that allow dynamic scaling of a robust and efficient hash table. Examples include AWS Google Cloud and BitTorrent. Consistent and rendezvous hashing are algorithms that minimize key remapping as the hash table resizes. While memory errors in large-scale cloud deployments are common neither algorithm offers both efficiency and robustness. Hyperdimensional Computing is an emerging computational model that has inherent efficiency robustness and is well suited for vector or hardware acceleration. We propose Hyperdimensional (HD) hashing and show that it has the efficiency to be deployed in large systems. Moreover a realistic level of memory errors causes more than 2037; mismatches for consistent hashing while HD hashing remains unaffected.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-25.847644805908203, -17.617263793945312]}, {"key": "hegeman2024compact", "year": "2024", "title": "Compact Parallel Hash Tables On The GPU", "abstract": "<p>On the GPU hash table operation speed is determined in large part by cache line efficiency and state-of-the-art hashing schemes thus divide tables into cache line-sized buckets. This raises the question whether performance can be further improved by increasing the number of entries that fit in such buckets. Known compact hashing techniques have not yet been adapted to the massively parallel setting nor have they been evaluated on the GPU. We consider a compact version of bucketed cuckoo hashing and a version of compact iceberg hashing suitable for the GPU. We discuss the tables from a theoretical perspective and provide an open source implementation of both schemes in CUDA for comparative benchmarking. In terms of performance the state-of-the-art cuckoo hashing benefits from compactness on lookups and insertions (most experiments show at least 10-2037; increase in throughput) and the iceberg table benefits significantly to the point of being comparable to compact cuckoo hashing\u2013while supporting performant dynamic operation.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-24.46562385559082, -14.650165557861328]}, {"key": "helbling2020directed", "year": "2020", "title": "Directed Graph Hashing", "abstract": "<p>This paper presents several algorithms for hashing directed graphs. The algorithms given are capable of hashing entire graphs as well as assigning hash values to specific nodes in a given graph. The notion of node symmetry is made precise via computation of vertex orbits and the graph automorphism group and nodes that are symmetrically identical are assigned equal hashes. We also present a novel Merkle-style hashing algorithm that seeks to fulfill the recursive principle that a hash of a node should depend only on the hash of its neighbors. This algorithm works even in the presence of cycles which would not be possible with a naive approach. Structurally hashing trees has seen widespread use in blockchain source code version control and web applications. Despite the popularity of tree hashing directed graph hashing remains unstudied in the literature. Our algorithms open new possibilities to hashing both directed graphs and more complex data structures that can be reduced to directed graphs such as hypergraphs.</p>\n", "tags": ["ARXIV", "Graph"], "tsne_embedding": [-0.5354118943214417, 29.055919647216797]}, {"key": "hemati2020non", "year": "2020", "title": "A Non-alternating Graph Hashing Algorithm For Large Scale Image Search", "abstract": "<p>In the era of big data methods for improving memory and computational efficiency have become crucial for successful deployment of technologies. Hashing is one of the most effective approaches to deal with computational limitations that come with big data. One natural way for formulating this problem is spectral hashing that directly incorporates affinity to learn binary codes. However due to binary constraints the optimization becomes intractable. To mitigate this challenge different relaxation approaches have been proposed to reduce the computational load of obtaining binary codes and still attain a good solution. The problem with all existing relaxation methods is resorting to one or more additional auxiliary variables to attain high quality binary codes while relaxing the problem. The existence of auxiliary variables leads to coordinate descent approach which increases the computational complexity. We argue that introducing these variables is unnecessary. To this end we propose a novel relaxed formulation for spectral hashing that adds no additional variables to the problem. Furthermore instead of solving the problem in original space where number of variables is equal to the data points we solve the problem in a much smaller space and retrieve the binary codes from this solution. This trick reduces both the memory and computational complexity at the same time. We apply two optimization techniques namely projected gradient and optimization on manifold to obtain the solution. Using comprehensive experiments on four public datasets we show that the proposed efficient spectral hashing (ESH) algorithm achieves highly competitive retrieval performance compared with state of the art at low complexity.</p>\n", "tags": ["ARXIV", "Graph"], "tsne_embedding": [-0.7642189264297485, -11.777573585510254]}, {"key": "hemati2021beyond", "year": "2021", "title": "Beyond Neighbourhood-preserving Transformations For Quantization-based Unsupervised Hashing", "abstract": "<p>An effective unsupervised hashing algorithm leads to compact binary codes preserving the neighborhood structure of data as much as possible. One of the most established schemes for unsupervised hashing is to reduce the dimensionality of data and then find a rigid (neighbourhood-preserving) transformation that reduces the quantization error. Although employing rigid transformations is effective we may not reduce quantization loss to the ultimate limits. As well reducing dimensionality and quantization loss in two separate steps seems to be sub-optimal. Motivated by these shortcomings we propose to employ both rigid and non-rigid transformations to reduce quantization error and dimensionality simultaneously. We relax the orthogonality constraint on the projection in a PCA-formulation and regularize this by a quantization term. We show that both the non-rigid projection matrix and rotation matrix contribute towards minimizing quantization loss but in different ways. A scalable nested coordinate descent approach is proposed to optimize this mixed-integer optimization problem. We evaluate the proposed method on five public benchmark datasets providing almost half a million images. Comparative results indicate that the proposed method mostly outperforms state-of-art linear methods and competes with end-to-end deep solutions.</p>\n", "tags": ["ARXIV", "Quantisation", "Unsupervised"], "tsne_embedding": [-33.607303619384766, 11.311074256896973]}, {"key": "hemati2021pattern", "year": "2021", "title": "Beyond Neighbourhood-Preserving Transformations for Quantization-Based Unsupervised Hashing", "abstract": "<p>An effective unsupervised hashing algorithm leads to compact binary codes preserving the neighborhood structure of data as much as possible. One of the most established schemes for unsupervised hashing is to reduce the dimensionality of data and then find a rigid (neighbourhood-preserving) transformation that reduces the quantization error. Although employing rigid transformations is effective, we may not reduce quantization loss to the ultimate limits. As well, reducing dimensionality and quantization loss in two separate steps seems to be sub-optimal. Motivated by these shortcomings, we propose to employ both rigid and non-rigid transformations to reduce quantization error and dimensionality simultaneously. We relax the orthogonality constraint on the projection in a PCA-formulation and regularize this by a quantization term. We show that both the non-rigid projection matrix and rotation matrix contribute towards minimizing quantization loss but in different ways. A scalable nested coordinate descent approach is proposed to optimize this mixed-integer optimization problem. We evaluate the proposed method on five public benchmark datasets providing almost half a million images. Comparative results indicate that the proposed method mostly outperforms state-of-art linear methods and competes with end-to-end deep solutions.</p>\n", "tags": ["Pattern Recognition Letters", "Quantisation", "Supervised"], "tsne_embedding": [-33.614864349365234, 11.282483100891113]}, {"key": "hemati2022graph", "year": "2022", "title": "A non-alternating graph hashing algorithm for large scale image search", "abstract": "<p>In the era of big data, methods for improving memory and computational efficiency have become crucial for successful deployment of technologies. Hashing is one of the most effective approaches to deal with computational limitations that come with big data. One natural way for formulating this problem is spectral hashing that directly incorporates affinity to learn binary codes. However, due to binary constraints, the optimization becomes intractable. To mitigate this challenge, different relaxation approaches have been proposed to reduce the computational load of obtaining binary codes and still attain a good solution. The problem with all existing relaxation methods is resorting to one or more additional auxiliary variables to attain high quality binary codes while relaxing the problem. The existence of auxiliary variables leads to coordinate descent approach which increases the computational complexity. We argue that introducing these variables is unnecessary. To this end, we propose a novel relaxed formulation for spectral hashing that adds no additional variables to the problem. Furthermore, instead of solving the problem in original space where number of variables is equal to the data points, we solve the problem in a much smaller space and retrieve the binary codes from this solution. This trick reduces both the memory and computational complexity at the same time. We apply two optimization techniques, namely projected gradient and optimization on manifold, to obtain the solution. Using comprehensive experiments on four public datasets, we show that the proposed efficient spectral hashing (ESH) algorithm achieves highly competitive retrieval performance compared with state of the art at low complexity.</p>\n", "tags": [], "tsne_embedding": [-0.7608926296234131, -11.75119686126709]}, {"key": "heo2012spherical", "year": "2012", "title": "Spherical Hashing", "abstract": "<p>Many binary code encoding schemes based on hashing\nhave been actively studied recently, since they can provide\nefficient similarity search, especially nearest neighbor\nsearch, and compact data representations suitable for handling\nlarge scale image databases in many computer vision\nproblems. Existing hashing techniques encode highdimensional\ndata points by using hyperplane-based hashing\nfunctions. In this paper we propose a novel hyperspherebased\nhashing function, spherical hashing, to map more\nspatially coherent data points into a binary code compared\nto hyperplane-based hashing functions. Furthermore, we\npropose a new binary code distance function, spherical\nHamming distance, that is tailored to our hyperspherebased\nbinary coding scheme, and design an efficient iterative\noptimization process to achieve balanced partitioning\nof data points for each hash function and independence between\nhashing functions. Our extensive experiments show\nthat our spherical hashing technique significantly outperforms\nsix state-of-the-art hashing techniques based on hyperplanes\nacross various image benchmarks of sizes ranging\nfrom one to 75 million of GIST descriptors. The performance\ngains are consistent and large, up to 100% improvements.\nThe excellent results confirm the unique merits of\nthe proposed idea in using hyperspheres to encode proximity\nregions in high-dimensional spaces. Finally, our method\nis intuitive and easy to implement.</p>\n", "tags": ["CVPR", "Has Code", "Image Retrieval"], "tsne_embedding": [-23.796316146850586, 14.752148628234863]}, {"key": "hill2016learning", "year": "2016", "title": "Learning Distributed Representations Of Sentences From Unlabelled Data", "abstract": "<p>Unsupervised methods for learning distributed representations of words are ubiquitous in todays NLP research but far less is known about the best ways to learn distributed phrase or sentence representations from unlabelled data. This paper is a systematic comparison of models that learn such representations. We find that the optimal approach depends critically on the intended application. Deeper more complex models are preferable for representations to be used in supervised systems but shallow log-linear models work best for building representation spaces that can be decoded with simple spatial distance metrics. We also propose two new unsupervised representation-learning objectives designed to optimise the trade-off between training time domain portability and performance.</p>\n", "tags": ["ARXIV", "Unsupervised"], "tsne_embedding": [23.747631072998047, -3.635183811187744]}, {"key": "himakunthala2023lets", "year": "2023", "title": "Lets Think Frame By Frame With VIP A Video Infilling And Prediction Dataset For Evaluating Video Chain-of-thought", "abstract": "<p>Despite exciting recent results showing vision-language systems capacity to reason about images using natural language their capacity for video reasoning remains under-explored. We motivate framing video reasoning as the sequential understanding of a small number of keyframes thereby leveraging the power and robustness of vision-language while alleviating the computational complexities of processing videos. To evaluate this novel application we introduce VIP an inference-time challenge dataset designed to explore models reasoning capabilities through video chain-of-thought. Inspired by visually descriptive scene plays we propose two formats for keyframe description unstructured dense captions and structured scene descriptions that identify the focus action mood objects and setting (FAMOuS) of the keyframe. To evaluate video reasoning we propose two tasks Video Infilling and Video Prediction which test abilities to generate multiple intermediate keyframes and predict future keyframes respectively. We benchmark GPT-4 GPT-3 and VICUNA on VIP demonstrate the performance gap in these complex video reasoning tasks and encourage future work to prioritize language models for efficient and generalized video reasoning.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [41.001712799072266, 5.377396106719971]}, {"key": "ho2022large", "year": "2022", "title": "Large Language Models Are Reasoning Teachers", "abstract": "<p>Recent works have shown that chain-of-thought (CoT) prompting can elicit language models to solve complex reasoning tasks step-by-step. However prompt-based CoT methods are dependent on very large models such as GPT-3 175B which are prohibitive to deploy at scale. In this paper we use these large models as reasoning teachers to enable complex reasoning in smaller models and reduce model size requirements by several orders of magnitude. We propose Fine-tune-CoT a method that generates reasoning samples from very large teacher models to fine-tune smaller models. We evaluate our method on a wide range of public models and complex tasks. We find that Fine-tune-CoT enables substantial reasoning capability in small models far outperforming prompt-based baselines and even the teacher model in many tasks. Additionally we extend our method by leveraging the teacher models ability to generate multiple distinct rationales for each original sample. Enriching the fine-tuning data with such diverse reasoning results in a substantial performance boost across datasets even for very small models. We conduct ablations and sample studies to understand the emergence of reasoning capabilities of student models. Our code implementation and data are available at https://github.com/itsnamgyu/reasoning-teacher.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [29.599510192871094, 10.841629981994629]}, {"key": "hoang2017enhance", "year": "2017", "title": "Enhance Feature Discrimination For Unsupervised Hashing", "abstract": "<p>We introduce a novel approach to improve unsupervised hashing. Specifically we propose a very efficient embedding method Gaussian Mixture Model embedding (Gemb). The proposed method using Gaussian Mixture Model embeds feature vector into a low-dimensional vector and simultaneously enhances the discriminative property of features before passing them into hashing. Our experiment shows that the proposed method boosts the hashing performance of many state-of-the-art e.g. Binary Autoencoder (BA) 1 Iterative Quantization (ITQ) 2 in standard evaluation metrics for the three main benchmark datasets.</p>\n", "tags": ["ARXIV", "Quantisation", "Unsupervised"], "tsne_embedding": [-7.298048496246338, 19.918838500976562]}, {"key": "hoang2018simultaneous", "year": "2018", "title": "Simultaneous Compression And Quantization A Joint Approach For Efficient Unsupervised Hashing", "abstract": "<p>For unsupervised data-dependent hashing the two most important requirements are to preserve similarity in the low-dimensional feature space and to minimize the binary quantization loss. A well-established hashing approach is Iterative Quantization (ITQ) which addresses these two requirements in separate steps. In this paper we revisit the ITQ approach and propose novel formulations and algorithms to the problem. Specifically we propose a novel approach named Simultaneous Compression and Quantization (SCQ) to jointly learn to compress (reduce dimensionality) and binarize input data in a single formulation under strict orthogonal constraint. With this approach we introduce a loss function and its relaxed version termed Orthonormal Encoder (OnE) and Orthogonal Encoder (OgE) respectively which involve challenging binary and orthogonal constraints. We propose to attack the optimization using novel algorithms based on recent advances in cyclic coordinate descent approach. Comprehensive experiments on unsupervised image retrieval demonstrate that our proposed methods consistently outperform other state-of-the-art hashing methods. Notably our proposed methods outperform recent deep neural networks and GAN based hashing in accuracy while being very computationally-efficient.</p>\n", "tags": ["ARXIV", "GAN", "Image Retrieval", "Quantisation", "Unsupervised"], "tsne_embedding": [-9.056069374084473, 3.525742292404175]}, {"key": "hoang2020unsupervised", "year": "2020", "title": "Unsupervised Deep Cross-modality Spectral Hashing", "abstract": "<p>This paper presents a novel framework namely Deep Cross-modality Spectral Hashing (DCSH) to tackle the unsupervised learning problem of binary hash codes for efficient cross-modal retrieval. The framework is a two-step hashing approach which decouples the optimization into (1) binary optimization and (2) hashing function learning. In the first step we propose a novel spectral embedding-based algorithm to simultaneously learn single-modality and binary cross-modality representations. While the former is capable of well preserving the local structure of each modality the latter reveals the hidden patterns from all modalities. In the second step to learn mapping functions from informative data inputs (images and word embeddings) to binary codes obtained from the first step we leverage the powerful CNN for images and propose a CNN-based deep architecture to learn text modality. Quantitative evaluations on three standard benchmark datasets demonstrate that the proposed DCSH method consistently outperforms other state-of-the-art methods.</p>\n", "tags": ["ARXIV", "CNN", "Cross Modal", "Unsupervised"], "tsne_embedding": [1.3753458261489868, 14.607379913330078]}, {"key": "hoang2021multi", "year": "2021", "title": "Multi-modal Mutual Information Maximization A Novel Approach For Unsupervised Deep Cross-modal Hashing", "abstract": "<p>In this paper we adopt the maximizing mutual information (MI) approach to tackle the problem of unsupervised learning of binary hash codes for efficient cross-modal retrieval. We proposed a novel method dubbed Cross-Modal Info-Max Hashing (CMIMH). First to learn informative representations that can preserve both intra- and inter-modal similarities we leverage the recent advances in estimating variational lower-bound of MI to maximize the MI between the binary representations and input features and between binary representations of different modalities. By jointly maximizing these MIs under the assumption that the binary representations are modelled by multivariate Bernoulli distributions we can learn binary representations which can preserve both intra- and inter-modal similarities effectively in a mini-batch manner with gradient descent. Furthermore we find out that trying to minimize the modality gap by learning similar binary representations for the same instance from different modalities could result in less informative representations. Hence balancing between reducing the modality gap and losing modality-private information is important for the cross-modal retrieval tasks. Quantitative evaluations on standard benchmark datasets demonstrate that the proposed method consistently outperforms other state-of-the-art cross-modal retrieval methods.</p>\n", "tags": ["ARXIV", "Cross Modal", "Unsupervised"], "tsne_embedding": [-10.011756896972656, 4.090625286102295]}, {"key": "hoe2021loss", "year": "2021", "title": "One Loss for All: Deep Hashing with a Single Cosine Similarity based Learning Objective", "abstract": "<p>A deep hashing model typically has two main learning objectives: to make the learned binary hash codes discriminative and to minimize a quantization error. With further constraints such as bit balance and code orthogonality, it is not uncommon for existing models to employ a large number (&gt;4) of losses. This leads to difficulties in model training and subsequently impedes their effectiveness. In this work, we propose a novel deep hashing model with only a single learning objective. Specifically, we show that maximizing the cosine similarity between the continuous codes and their corresponding binary orthogonal codes can ensure both hash code discriminativeness and quantization error minimization. Further, with this learning objective, code balancing can be achieved by simply using a Batch Normalization (BN) layer and multi-label classification is also straightforward with label smoothing. The result is an one-loss deep hashing model that removes all the hassles of tuning the weights of various losses. Importantly, extensive experiments show that our model is highly effective, outperforming the state-of-the-art multi-loss hashing models on three large-scale instance retrieval benchmarks, often by significant margins.</p>\n", "tags": ["Deep Learning", "Has Code", "NEURIPS"], "tsne_embedding": [2.718893527984619, -2.8921549320220947]}, {"key": "hoe2021one", "year": "2021", "title": "One Loss For All Deep Hashing With A Single Cosine Similarity Based Learning Objective", "abstract": "<p>A deep hashing model typically has two main learning objectives to make the learned binary hash codes discriminative and to minimize a quantization error. With further constraints such as bit balance and code orthogonality it is not uncommon for existing models to employ a large number (4) of losses. This leads to difficulties in model training and subsequently impedes their effectiveness. In this work we propose a novel deep hashing model with only a single learning objective. Specifically we show that maximizing the cosine similarity between the continuous codes and their corresponding binary orthogonal codes can ensure both hash code discriminativeness and quantization error minimization. Further with this learning objective code balancing can be achieved by simply using a Batch Normalization (BN) layer and multi-label classification is also straightforward with label smoothing. The result is an one-loss deep hashing model that removes all the hassles of tuning the weights of various losses. Importantly extensive experiments show that our model is highly effective outperforming the state-of-the-art multi-loss hashing models on three large-scale instance retrieval benchmarks often by significant margins. Code is available at https://github.com/kamwoh/orthohash</p>\n", "tags": ["ARXIV", "Has Code", "Quantisation", "Supervised"], "tsne_embedding": [2.7833755016326904, -2.9474027156829834]}, {"key": "hoffmann2022training", "year": "2022", "title": "Training Compute-optimal Large Language Models", "abstract": "<p>We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens we find that for compute-optimal training the model size and the number of training tokens should be scaled equally for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model Chinchilla that uses the same compute budget as Gopher but with 70B parameters and 4(times) more more data. Chinchilla uniformly and significantly outperforms Gopher (280B) GPT-3 (175B) Jurassic-1 (178B) and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference greatly facilitating downstream usage. As a highlight Chinchilla reaches a state-of-the-art average accuracy of 67.537; on the MMLU benchmark greater than a 737; improvement over Gopher.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [17.28936004638672, -4.425854682922363]}, {"key": "holden2023identifying", "year": "2023", "title": "Identifying Reducible K-tuples Of Vectors With Subspace-proximity Sensitive Hashing/filtering", "abstract": "<p>We introduce and analyse a family of hash and predicate functions that are more likely to produce collisions for small reducible configurations of vectors. These may offer practical improvements to lattice sieving for short vectors. In particular in one asymptotic regime the family exhibits significantly different convergent behaviour than existing hash functions and predicates.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-25.798555374145508, -4.869690418243408]}, {"key": "hou2023large", "year": "2023", "title": "Large Language Models Are Zero-shot Rankers For Recommender Systems", "abstract": "<p>Recently large language models (LLMs) (e.g. GPT-4) have demonstrated impressive general-purpose task-solving abilities including the potential to approach recommendation tasks. Along this line of research this work aims to investigate the capacity of LLMs that act as the ranking model for recommender systems. We first formalize the recommendation problem as a conditional ranking task considering sequential interaction histories as conditions and the items retrieved by other candidate generation models as candidates. To solve the ranking task by LLMs we carefully design the prompting template and conduct extensive experiments on two widely-used datasets. We show that LLMs have promising zero-shot ranking abilities but (1) struggle to perceive the order of historical interactions and (2) can be biased by popularity or item positions in the prompts. We demonstrate that these issues can be alleviated using specially designed prompting and bootstrapping strategies. Equipped with these insights zero-shot LLMs can even challenge conventional recommendation models when ranking candidates are retrieved by multiple candidate generators. The code and processed datasets are available at https://github.com/RUCAIBox/LLMRank.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [43.50126266479492, -7.87485933303833]}, {"key": "houen2023sparse", "year": "2023", "title": "A Sparse Johnson-lindenstrauss Transform Using Fast Hashing", "abstract": "<p>The emphSparse Johnson-Lindenstrauss Transform of Kane and Nelson (SODA 2012) provides a linear dimensionality-reducing map A in mathbbR^m times u( in )ell_2( that preserves distances up to distortion of )1 + varepsilon with probability (1 - delta) where (m = O(varepsilon^-2 log 1/delta)) and each column of (A) has (O(varepsilon m)) non-zero entries. The previous analyses of the Sparse Johnson-Lindenstrauss Transform all assumed access to a (Omega(log 1/delta))-wise independent hash function. The main contribution of this paper is a more general analysis of the Sparse Johnson-Lindenstrauss Transform with less assumptions on the hash function. We also show that the emphMixed Tabulation hash function of Dahlgaard Knudsen Rotenberg and Thorup (FOCS 2015) satisfies the conditions of our analysis thus giving us the first analysis of a Sparse Johnson-Lindenstrauss Transform that works with a practical hash function.</p>\n", "tags": ["ARXIV", "FOCS", "Independent"], "tsne_embedding": [-35.772464752197266, 1.8036171197891235]}, {"key": "hsieh2016fast", "year": "2016", "title": "Fast Binary Embedding Via Circulant Downsampled Matrix -- A Data-independent Approach", "abstract": "<p>Binary embedding of high-dimensional data aims to produce low-dimensional binary codes while preserving discriminative power. State-of-the-art methods often suffer from high computation and storage costs. We present a simple and fast embedding scheme by first downsampling N-dimensional data into M-dimensional data and then multiplying the data with an MxM circulant matrix. Our method requires O(N +M log M) computation and O(N) storage costs. We prove if data have sparsity our scheme can achieve similarity-preserving well. Experiments further demonstrate that though our method is cost-effective and fast it still achieves comparable performance in image applications.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-19.60984230041504, 12.795621871948242]}, {"key": "hu2017supervised", "year": "2017", "title": "Supervised Hashing Based On Energy Minimization", "abstract": "<p>Recently supervised hashing methods have attracted much attention since they can optimize retrieval speed and storage cost while preserving semantic information. Because hashing codes learning is NP-hard many methods resort to some form of relaxation technique. But the performance of these methods can easily deteriorate due to the relaxation. Luckily many supervised hashing formulations can be viewed as energy functions hence solving hashing codes is equivalent to learning marginals in the corresponding conditional random field (CRF). By minimizing the KL divergence between a fully factorized distribution and the Gibbs distribution of this CRF a set of consistency equations can be obtained but updating them in parallel may not yield a local optimum since the variational lower bound is not guaranteed to increase. In this paper we use a linear approximation of the sigmoid function to convert these consistency equations to linear systems which have a closed-form solution. By applying this novel technique to two classical hashing formulations KSH and SPLH we obtain two new methods called EM (energy minimizing based)-KSH and EM-SPLH. Experimental results on three datasets show the superiority of our methods.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [-1.164322018623352, -10.354519844055176]}, {"key": "hu2018deep", "year": "2018", "title": "Deep LDA Hashing", "abstract": "<p>The conventional supervised hashing methods based on classification do not entirely meet the requirements of hashing technique but Linear Discriminant Analysis (LDA) does. In this paper we propose to perform a revised LDA objective over deep networks to learn efficient hashing codes in a truly end-to-end fashion. However the complicated eigenvalue decomposition within each mini-batch in every epoch has to be faced with when simply optimizing the deep network w.r.t. the LDA objective. In this work the revised LDA objective is transformed into a simple least square problem which naturally overcomes the intractable problems and can be easily solved by the off-the-shelf optimizer. Such deep extension can also overcome the weakness of LDA Hashing in the limited linear projection and feature learning. Amounts of experiments are conducted on three benchmark datasets. The proposed Deep LDA Hashing shows nearly 70 points improvement over the conventional one on the CIFAR-10 dataset. It also beats several state-of-the-art methods on various metrics.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [-3.9647278785705566, -8.669548988342285]}, {"key": "hu2018from", "year": "2018", "title": "From Hashing To Cnns Training Binaryweight Networks Via Hashing", "abstract": "<p>Deep convolutional neural networks (CNNs) have shown appealing performance on various computer vision tasks in recent years. This motivates people to deploy CNNs to realworld applications. However most of state-of-art CNNs require large memory and computational resources which hinders the deployment on mobile devices. Recent studies show that low-bit weight representation can reduce much storage and memory demand and also can achieve efficient network inference. To achieve this goal we propose a novel approach named BWNH to train Binary Weight Networks via Hashing. In this paper we first reveal the strong connection between inner-product preserving hashing and binary weight networks and show that training binary weight networks can be intrinsically regarded as a hashing problem. Based on this perspective we propose an alternating optimization method to learn the hash codes instead of directly learning binary weights. Extensive experiments on CIFAR10 CIFAR100 and ImageNet demonstrate that our proposed BWNH outperforms current state-of-art by a large margin.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [3.4939863681793213, 13.209805488586426]}, {"key": "hu2019separated", "year": "2019", "title": "Separated Variational Hashing Networks for Cross-Modal Retrieval", "abstract": "<p>Cross-modal hashing, due to its low storage cost and high query speed, has been successfully used for similarity search in multimedia retrieval applications. It projects high-dimensional data into a shared isomorphic Hamming space with similar binary codes for semantically-similar data. In some applications, all modalities may not be obtained or trained simultaneously for some reasons, such as privacy, secret, storage limitation, and computational resource limitation. However, most existing cross-modal hashing methods need all modalities to jointly learn the common Hamming space, thus hindering them from handling these problems. In this paper, we propose a novel approach called Separated Variational Hashing Networks (SVHNs) to overcome the above challenge. Firstly, it adopts a label network (LabNet) to exploit available and nonspecific label annotations to learn a latent common Hamming space by projecting each semantic label into a common binary representation. Then, each modality-specific network can separately map the samples of the corresponding modality into their binary semantic codes learned by LabNet. We achieve it by conducting variational inference to match the aggregated posterior of the hashing code of LabNet with an arbitrary prior distribution. The effectiveness and efficiency of our SVHNs are verified by extensive experiments carried out on four widely-used multimedia databases, in comparison with 11 state-of-the-art approaches.</p>\n", "tags": ["Cross Modal", "MM"], "tsne_embedding": [-9.312518119812012, 0.4036503732204437]}, {"key": "hu2020creating", "year": "2020", "title": "Creating Something from Nothing: Unsupervised Knowledge Distillation for Cross-Modal Hashing", "abstract": "<p>In recent years, cross-modal hashing (CMH) has attracted increasing attentions, mainly because its potential\nability of mapping contents from different modalities, especially in vision and language, into the same space, so that\nit becomes efficient in cross-modal data retrieval. There are\ntwo main frameworks for CMH, differing from each other in\nwhether semantic supervision is required. Compared to the\nunsupervised methods, the supervised methods often enjoy\nmore accurate results, but require much heavier labors in\ndata annotation. In this paper, we propose a novel approach\nthat enables guiding a supervised method using outputs produced by an unsupervised method. Specifically, we make\nuse of teacher-student optimization for propagating knowledge. Experiments are performed on two popular CMH\nbenchmarks, i.e., the MIRFlickr and NUS-WIDE datasets.\nOur approach outperforms all existing unsupervised methods by a large margin</p>\n", "tags": ["CVPR", "Cross Modal", "Deep Learning", "Supervised"], "tsne_embedding": [5.720418930053711, -0.8885469436645508]}, {"key": "hu2023chatdb", "year": "2023", "title": "Chatdb Augmenting Llms With Databases As Their Symbolic Memory", "abstract": "<p>Large language models (LLMs) with memory are computationally universal. However mainstream LLMs are not taking full advantage of memory and the designs are heavily influenced by biological brains. Due to their approximate nature and proneness to the accumulation of errors conventional neural memory mechanisms cannot support LLMs to simulate complex reasoning. In this paper we seek inspiration from modern computer architectures to augment LLMs with symbolic memory for complex multi-hop reasoning. Such a symbolic memory framework is instantiated as an LLM and a set of SQL databases where the LLM generates SQL instructions to manipulate the SQL databases. We validate the effectiveness of the proposed memory framework on a synthetic dataset requiring complex reasoning. The project website is available at https://chatdatabase.github.io/ .</p>\n", "tags": ["ARXIV"], "tsne_embedding": [25.357376098632812, -17.888975143432617]}, {"key": "hu2023ciem", "year": "2023", "title": "CIEM Contrastive Instruction Evaluation Method For Better Instruction Tuning", "abstract": "<p>Nowadays the research on Large Vision-Language Models (LVLMs) has been significantly promoted thanks to the success of Large Language Models (LLM). Nevertheless these Vision-Language Models (VLMs) are suffering from the drawback of hallucination \u2013 due to insufficient understanding of vision and language modalities VLMs may generate incorrect perception information when doing downstream applications for example captioning a non-existent entity. To address the hallucination phenomenon on the one hand we introduce a Contrastive Instruction Evaluation Method (CIEM) which is an automatic pipeline that leverages an annotated image-text dataset coupled with an LLM to generate factual/contrastive question-answer pairs for the evaluation of the hallucination of VLMs. On the other hand based on CIEM we further propose a new instruction tuning method called CIT (the abbreviation of Contrastive Instruction Tuning) to alleviate the hallucination of VLMs by automatically producing high-quality factual/contrastive question-answer pairs and corresponding justifications for model tuning. Through extensive experiments on CIEM and CIT we pinpoint the hallucination issues commonly present in existing VLMs the disability of the current instruction-tuning dataset to handle the hallucination phenomenon and the superiority of CIT-tuned VLMs over both CIEM and public datasets.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [48.72145462036133, 6.8664655685424805]}, {"key": "huang2015hash", "year": "2015", "title": "Hash Function Learning Via Codewords", "abstract": "<p>In this paper we introduce a novel hash learning framework that has two main distinguishing features when compared to past approaches. First it utilizes codewords in the Hamming space as ancillary means to accomplish its hash learning task. These codewords which are inferred from the data attempt to capture similarity aspects of the datas hash codes. Secondly and more importantly the same framework is capable of addressing supervised unsupervised and even semi-supervised hash learning tasks in a natural manner. A series of comparative experiments focused on content-based image retrieval highlights its performance advantages.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Unsupervised"], "tsne_embedding": [-3.8796467781066895, -15.213887214660645]}, {"key": "huang2016local", "year": "2016", "title": "Local Similarity-aware Deep Feature Embedding", "abstract": "<p>Existing deep embedding methods in vision tasks are capable of learning a compact Euclidean space from images where Euclidean distances correspond to a similarity metric. To make learning more effective and efficient hard sample mining is usually employed with samples identified through computing the Euclidean feature distance. However the global Euclidean distance cannot faithfully characterize the true feature similarity in a complex visual feature space where the intraclass distance in a high-density region may be larger than the interclass distance in low-density regions. In this paper we introduce a Position-Dependent Deep Metric (PDDM) unit which is capable of learning a similarity metric adaptive to local feature structure. The metric can be used to select genuinely hard samples in a local neighborhood to guide the deep embedding learning in an online and robust manner. The new layer is appealing in that it is pluggable to any convolutional networks and is trained end-to-end. Our local similarity-aware feature embedding not only demonstrates faster convergence and boosted performance on two complex image retrieval datasets its large margin nature also leads to superior generalization results under the large and open set scenarios of transfer learning and zero-shot learning on ImageNet 2010 and ImageNet-10K datasets.</p>\n", "tags": ["Image Retrieval", "NEURIPS"], "tsne_embedding": [-8.883467674255371, 24.008405685424805]}, {"key": "huang2017online", "year": "2017", "title": "Online Hashing", "abstract": "<p>Although hash function learning algorithms have achieved great success in recent years most existing hash models are off-line which are not suitable for processing sequential or online data. To address this problem this work proposes an online hash model to accommodate data coming in stream for online learning. Specifically a new loss function is proposed to measure the similarity loss between a pair of data samples in hamming space. Then a structured hash model is derived and optimized in a passive-aggressive way. Theoretical analysis on the upper bound of the cumulative loss for the proposed online hash model is provided. Furthermore we extend our online hashing from a single-model to a multi-model online hashing that trains multiple models so as to retain diverse online hashing models in order to avoid biased update. The competitive efficiency and effectiveness of the proposed online hash models are verified through extensive experiments on several large-scale datasets as compared to related hashing methods.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-8.024551391601562, -15.347572326660156]}, {"key": "huang2017unsupervised", "year": "2017", "title": "Unsupervised Triplet Hashing For Fast Image Retrieval", "abstract": "<p>Hashing has played a pivotal role in large-scale image retrieval. With the development of Convolutional Neural Network (CNN) hashing learning has shown great promise. But existing methods are mostly tuned for classification which are not optimized for retrieval tasks especially for instance-level retrieval. In this study we propose a novel hashing method for large-scale image retrieval. Considering the difficulty in obtaining labeled datasets for image retrieval task in large scale we propose a novel CNN-based unsupervised hashing method namely Unsupervised Triplet Hashing (UTH). The unsupervised hashing network is designed under the following three principles 1) more discriminative representations for image retrieval; 2) minimum quantization loss between the original real-valued feature descriptors and the learned hash codes; 3) maximum information entropy for the learned hash codes. Extensive experiments on CIFAR-10 MNIST and In-shop datasets have shown that UTH outperforms several state-of-the-art unsupervised hashing methods in terms of retrieval accuracy.</p>\n", "tags": ["ARXIV", "CNN", "Image Retrieval", "Quantisation", "Unsupervised"], "tsne_embedding": [5.3538947105407715, 11.930282592773438]}, {"key": "huang2019accelerate", "year": "2019", "title": "Accelerate Learning of Deep Hashing With Gradient Attention", "abstract": "<p>Recent years have witnessed the success of learning to hash in fast large-scale image retrieval. As deep learning has shown its superior performance on many computer vision applications, recent designs of learning-based hashing models have been moving from shallow ones to deep architectures. However, based on our analysis, we find that gradient descent based algorithms used in deep hashing models would potentially cause hash codes of a pair of training instances to be updated towards the directions of each other simultaneously during optimization. In the worst case, the paired hash codes switch their directions after update, and consequently, their corresponding distance in the Hamming space remain unchanged. This makes the overall learning process highly inefficient. To address this issue, we propose a new deep hashing model integrated with a novel gradient attention mechanism. Extensive experimental results on three benchmark datasets show that our proposed algorithm is able to accelerate the learning process and obtain competitive retrieval performance compared with state-of-the-art deep hashing models.</p>\n", "tags": ["Deep Learning", "ICCV"], "tsne_embedding": [5.954835414886475, 0.6139129996299744]}, {"key": "huang2019learning", "year": "2019", "title": "Learning Hash Function Through Codewords", "abstract": "<p>In this paper we propose a novel hash learning approach that has the following main distinguishing features when compared to past frameworks. First the codewords are utilized in the Hamming space as ancillary techniques to accomplish its hash learning task. These codewords which are inferred from the data attempt to capture grouping aspects of the datas hash codes. Furthermore the proposed framework is capable of addressing supervised unsupervised and even semi-supervised hash learning scenarios. Additionally the framework adopts a regularization term over the codewords which automatically chooses the codewords for the problem. To efficiently solve the problem one Block Coordinate Descent algorithm is showcased in the paper. We also show that one step of the algorithms can be casted into several Support Vector Machine problems which enables our algorithms to utilize efficient software package. For the regularization term a closed form solution of the proximal operator is provided in the paper. A series of comparative experiments focused on content-based image retrieval highlights its performance advantages.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Unsupervised"], "tsne_embedding": [-3.648111581802368, -15.431795120239258]}, {"key": "huang2022language", "year": "2022", "title": "Language Models As Zero-shot Planners Extracting Actionable Knowledge For Embodied Agents", "abstract": "<p>Can world knowledge learned by large language models (LLMs) be used to act in interactive environments In this paper we investigate the possibility of grounding high-level tasks expressed in natural language (e.g. make breakfast) to a chosen set of actionable steps (e.g. open fridge). While prior work focused on learning from explicit step-by-step examples of how to act we surprisingly find that if pre-trained LMs are large enough and prompted appropriately they can effectively decompose high-level tasks into mid-level plans without any further training. However the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models. Website at https://huangwl18.github.io/language-planner</p>\n", "tags": ["ARXIV"], "tsne_embedding": [31.405364990234375, -9.100679397583008]}, {"key": "huang2022large", "year": "2022", "title": "Large Language Models Can Self-improve", "abstract": "<p>Large Language Models (LLMs) have achieved excellent performances in various tasks. However fine-tuning an LLM requires extensive supervision. Human on the other hand may improve their reasoning abilities by self-thinking without external inputs. In this work we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate high-confidence rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.437;-82.137; on GSM8K 78.237;-83.037; on DROP 90.037;-94.437; on OpenBookQA and 63.437;-67.937; on ANLI-A3) and achieves state-of-the-art-level performance without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [32.77350616455078, 1.131964087486267]}, {"key": "huang2022towards", "year": "2022", "title": "Towards Reasoning In Large Language Models A Survey", "abstract": "<p>Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving decision making and critical thinking. In recent years large language models (LLMs) have made significant progress in natural language processing and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs including techniques for improving and eliciting reasoning in these models methods and benchmarks for evaluating reasoning abilities findings and implications of previous research in this field and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.</p>\n", "tags": ["ARXIV", "Survey Paper"], "tsne_embedding": [35.933189392089844, -11.204178810119629]}, {"key": "huang2023audiogpt", "year": "2023", "title": "Audiogpt Understanding And Generating Speech Music Sound And Talking Head", "abstract": "<p>Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks challenging our understanding of learning and cognition. Despite the recent success current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work we propose a multi-modal AI system named AudioGPT which complements LLMs (i.e. ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models we outline the principles and processes and test AudioGPT in terms of consistency capability and robustness. Experimental results demonstrate the capabilities of AudioGPT in solving AI tasks with speech music sound and talking head understanding and generation in multi-round dialogues which empower humans to create rich and diverse audio content with unprecedented ease. Our system is publicly available at urlhttps://github.com/AIGC-Audio/AudioGPT}.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [37.978153228759766, -1.852746844291687]}, {"key": "huang2023language", "year": "2023", "title": "Language Is Not All You Need Aligning Perception With Language Models", "abstract": "<p>A big convergence of language multimodal perception action and world modeling is a key step toward artificial general intelligence. In this work we introduce Kosmos-1 a Multimodal Large Language Model (MLLM) that can perceive general modalities learn in context (i.e. few-shot) and follow instructions (i.e. zero-shot). Specifically we train Kosmos-1 from scratch on web-scale multimodal corpora including arbitrarily interleaved text and images image-caption pairs and text data. We evaluate various settings including zero-shot few-shot and multimodal chain-of-thought prompting on a wide range of tasks without any gradient updates or finetuning. Experimental results show that Kosmos-1 achieves impressive performance on (i) language understanding generation and even OCR-free NLP (directly fed with document images) (ii) perception-language tasks including multimodal dialogue image captioning visual question answering and (iii) vision tasks such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer i.e. transfer knowledge from language to multimodal and from multimodal to language. In addition we introduce a dataset of Raven IQ test which diagnoses the nonverbal reasoning capability of MLLMs.</p>\n", "tags": ["ARXIV", "Cross Modal", "Supervised"], "tsne_embedding": [30.511825561523438, -2.662238836288452]}, {"key": "huang2023opera", "year": "2023", "title": "OPERA Alleviating Hallucination In Multi-modal Large Language Models Via Over-trust Penalty And Retrospection-allocation", "abstract": "<p>Hallucination posed as a pervasive challenge of multi-modal large language models (MLLMs) has significantly impeded their real-world usage that demands precise judgment. Existing methods mitigate this issue with either training with specific designed data or inferencing with external knowledge from other sources incurring inevitable additional costs. In this paper we present OPERA a novel MLLM decoding method grounded in an Over-trust Penalty and a Retrospection-Allocation strategy serving as a nearly free lunch to alleviate the hallucination issue without additional data knowledge or training. Our approach begins with an interesting observation that most hallucinations are closely tied to the knowledge aggregation patterns manifested in the self-attention matrix i.e. MLLMs tend to generate new tokens by focusing on a few summary tokens but not all the previous tokens. Such partial over-trust inclination results in the neglecting of image tokens and describes the image content with hallucination. Based on the observation OPERA introduces a penalty term on the model logits during the beam-search decoding to mitigate the over-trust issue along with a rollback strategy that retrospects the presence of summary tokens in the previously generated tokens and re-allocate the token selection if necessary. With extensive experiments OPERA shows significant hallucination-mitigating performance on different MLLMs and metrics proving its effectiveness and generality. Our code is available at https://github.com/shikiw/OPERA.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [51.279483795166016, 4.578526496887207]}, {"key": "huang2023recommender", "year": "2023", "title": "Recommender AI Agent Integrating Large Language Models For Interactive Recommendations", "abstract": "<p>Recommender models excel at providing domain-specific item recommendations by leveraging extensive user behavior data. Despite their ability to act as lightweight domain experts they struggle to perform versatile tasks such as providing explanations and engaging in conversations. On the other hand large language models (LLMs) represent a significant step towards artificial general intelligence showcasing remarkable capabilities in instruction comprehension commonsense reasoning and human interaction. However LLMs lack the knowledge of domain-specific item catalogs and behavioral patterns particularly in areas that diverge from general world knowledge such as online e-commerce. Finetuning LLMs for each domain is neither economic nor efficient. In this paper we bridge the gap between recommender models and LLMs combining their respective strengths to create a versatile and interactive recommender system. We introduce an efficient framework called textbfInteRecAgent which employs LLMs as the brain and recommender models as tools. We first outline a minimal set of essential tools required to transform LLMs into InteRecAgent. We then propose an efficient workflow within InteRecAgent for task execution incorporating key components such as memory components dynamic demonstration-augmented task planning and reflection. InteRecAgent enables traditional recommender systems such as those ID-based matrix factorization models to become interactive systems with a natural language interface through the integration of LLMs. Experimental results on several public datasets show that InteRecAgent achieves satisfying performance as a conversational recommender system outperforming general-purpose LLMs. The source code of InteRecAgent is released at https://aka.ms/recagent.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [41.66487503051758, -5.9173736572265625]}, {"key": "huang2024large", "year": "2024", "title": "Large Language Model Interaction Simulator For Cold-start Item Recommendation", "abstract": "<p>Recommending cold items is a long-standing challenge for collaborative filtering models because these cold items lack historical user interactions to model their collaborative features. The gap between the content of cold items and their behavior patterns makes it difficult to generate accurate behavioral embeddings for cold items. Existing cold-start models use mapping functions to generate fake behavioral embeddings based on the content feature of cold items. However these generated embeddings have significant differences from the real behavioral embeddings leading to a negative impact on cold recommendation performance. To address this challenge we propose an LLM Interaction Simulator (LLM-InS) to model users behavior patterns based on the content aspect. This simulator allows recommender systems to simulate vivid interactions for each cold item and transform them from cold to warm items directly. Specifically we outline the designing and training process of a tailored LLM-simulator that can simulate the behavioral patterns of users and items. Additionally we introduce an efficient filtering-and-refining approach to take full advantage of the simulation power of the LLMs. Finally we propose an updating method to update the embeddings of the items. we unified trains for both cold and warm items within a recommender model based on the simulated and real interactions. Extensive experiments using real behavioral embeddings demonstrate that our proposed model LLM-InS outperforms nine state-of-the-art cold-start methods and three LLM models in cold-start item recommendations.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [43.665061950683594, -13.065866470336914]}, {"key": "hyv\u00f6nen2015fast", "year": "2015", "title": "Fast K-nn Search", "abstract": "<p>Efficient index structures for fast approximate nearest neighbor queries are required in many applications such as recommendation systems. In high-dimensional spaces many conventional methods suffer from excessive usage of memory and slow response times. We propose a method where multiple random projection trees are combined by a novel voting scheme. The key idea is to exploit the redundancy in a large number of candidate sets obtained by independently generated random projections in order to reduce the number of expensive exact distance evaluations. The method is straightforward to implement using sparse projections which leads to a reduced memory footprint and fast index construction. Furthermore it enables grouping of the required computations into big matrix multiplications which leads to additional savings due to cache effects and low-level parallelization. We demonstrate by extensive experiments on a wide variety of data sets that the method is faster than existing partitioning tree or hashing based approaches making it the fastest available technique on high accuracy levels.</p>\n", "tags": ["ICML", "Independent"], "tsne_embedding": [-24.421274185180664, -12.27741813659668]}, {"key": "imagenet2009using", "year": "2009", "title": "ImageNet: A large-scale hierarchical image database", "abstract": "<p>The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called \u201cImageNet\u201d, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.</p>\n", "tags": ["Dataset"], "tsne_embedding": [8.392289161682129, 11.563010215759277]}, {"key": "ioffe1998learning", "year": "1998", "title": "Learning To Find Pictures Of People", "abstract": "<p>Finding articulated objects like people in pictures present.s a par- ticularly difficult object. recognition problem. We show how t.o find people by finding putative body segments and then construct.- ing assemblies of those segments that are consist.ent with the con- straints on the appearance of a person that result from kinematic properties. Since a reasonable model of a person requires at. least nine segments it is not possible to present every group to a classi- fier. Instead the search can be pruned by using projected versions of a classifier that accepts groups corresponding to people. We describe an efficient projection algorithm for one popular classi- fier and demonstrate that our approach can be used to determine whether images of real scenes contain people.</p>\n", "tags": ["Independent", "NEURIPS"], "tsne_embedding": [10.694120407104492, 19.187267303466797]}, {"key": "irie2014locality", "year": "2014", "title": "Locally Linear Hashing for Extracting Non-Linear Manifolds", "abstract": "<p>Previous efforts in hashing intend to preserve data variance\nor pairwise affinity, but neither is adequate in capturing\nthe manifold structures hidden in most visual data. In\nthis paper, we tackle this problem by reconstructing the locally\nlinear structures of manifolds in the binary Hamming\nspace, which can be learned by locality-sensitive sparse\ncoding. We cast the problem as a joint minimization of\nreconstruction error and quantization loss, and show that,\ndespite its NP-hardness, a local optimum can be obtained\nefficiently via alternative optimization. Our method distinguishes\nitself from existing methods in its remarkable ability\nto extract the nearest neighbors of the query from the\nsame manifold, instead of from the ambient space. On extensive\nexperiments on various image benchmarks, our results\nimprove previous state-of-the-art by 28-74% typically,\nand 627% on the Yale face data.</p>\n", "tags": ["CVPR", "Image Retrieval"], "tsne_embedding": [-19.206295013427734, 10.569018363952637]}, {"key": "iscen2017fast", "year": "2017", "title": "Fast Spectral Ranking For Similarity Search", "abstract": "<p>Despite the success of deep learning on representing images for particular object retrieval recent studies show that the learned representations still lie on manifolds in a high dimensional space. This makes the Euclidean nearest neighbor search biased for this task. Exploring the manifolds online remains expensive even if a nearest neighbor graph has been computed offline. This work introduces an explicit embedding reducing manifold search to Euclidean search followed by dot product similarity search. This is equivalent to linear graph filtering of a sparse signal in the frequency domain. To speed up online search we compute an approximate Fourier basis of the graph offline. We improve the state of art on particular object retrieval datasets including the challenging Instre dataset containing small objects. At a scale of 10^5 images the offline cost is only a few hours while query time is comparable to standard similarity search.</p>\n", "tags": ["ARXIV", "Deep Learning", "Graph"], "tsne_embedding": [-5.871825218200684, 26.484209060668945]}, {"key": "ishikawa2015pairwise", "year": "2015", "title": "Pairwise Rotation Hashing For High-dimensional Features", "abstract": "<p>Binary Hashing is widely used for effective approximate nearest neighbors search. Even though various binary hashing methods have been proposed very few methods are feasible for extremely high-dimensional features often used in visual tasks today. We propose a novel highly sparse linear hashing method based on pairwise rotations. The encoding cost of the proposed algorithm is ((n log n)) for n-dimensional features whereas that of the existing state-of-the-art method is typically ((n^2)). The proposed method is also remarkably faster in the learning phase. Along with the efficiency the retrieval accuracy is comparable to or slightly outperforming the state-of-the-art. Pairwise rotations used in our method are formulated from an analytical study of the trade-off relationship between quantization error and entropy of binary codes. Although these hashing criteria are widely used in previous researches its analytical behavior is rarely studied. All building blocks of our algorithm are based on the analytical solution and it thus provides a fairly simple and efficient procedure.</p>\n", "tags": ["ARXIV", "Quantisation"], "tsne_embedding": [-27.734743118286133, -8.16103744506836]}, {"key": "iyer2022opt", "year": "2022", "title": "OPT-IML Scaling Language Model Instruction Meta Learning Through The Lens Of Generalization", "abstract": "<p>Recent work has shown that fine-tuning large pre-trained language models on a collection of tasks described via instructions a.k.a. instruction-tuning improves their zero and few-shot generalization to unseen tasks. However there is a limited understanding of the performance trade-offs of different decisions made during the instruction-tuning process. These decisions include the scale and diversity of the instruction-tuning benchmark different task sampling strategies fine-tuning with and without demonstrations training using specialized datasets for reasoning and dialogue and finally the fine-tuning objectives themselves. In this paper we characterize the effect of instruction-tuning decisions on downstream task performance when scaling both model and benchmark sizes. To this end we create OPT-IML Bench a large benchmark for Instruction Meta-Learning (IML) of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks and prepare an evaluation framework to measure three types of model generalizations to tasks from fully held-out categories to held-out tasks from seen categories and to held-out instances from seen tasks. Through the lens of this framework we first present insights about instruction-tuning decisions as applied to OPT-30B and further exploit these insights to train OPT-IML 30B and 175B which are instruction-tuned versions of OPT. OPT-IML demonstrates all three generalization abilities at both scales on four different evaluation benchmarks with diverse tasks and input formats \u2013 PromptSource FLAN Super-NaturalInstructions and UnifiedSKG. Not only does it significantly outperform OPT on all benchmarks but is also highly competitive with existing models fine-tuned on each specific benchmark. We release OPT-IML at both scales together with the OPT-IML Bench evaluation framework.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [22.308141708374023, -12.1752290725708]}, {"key": "j2021lora", "year": "2021", "title": "Lora Low-rank Adaptation Of Large Language Models", "abstract": "<p>An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models full fine-tuning which retrains all model parameters becomes less feasible. Using GPT-3 175B as an example \u2013 deploying independent instances of fine-tuned models each with 175B parameters is prohibitively expensive. We propose Low-Rank Adaptation or LoRA which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam LoRA can reduce the number of trainable parameters by 10000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa DeBERTa GPT-2 and GPT-3 despite having fewer trainable parameters a higher training throughput and unlike adapters no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa DeBERTa and GPT-2 at https://github.com/microsoft/LoRA.</p>\n", "tags": ["ARXIV", "Has Code", "Independent"], "tsne_embedding": [17.92978858947754, -4.5170674324035645]}, {"key": "j2023could", "year": "2023", "title": "Could A Large Language Model Be Conscious", "abstract": "<p>There has recently been widespread discussion of whether large language models might be sentient. Should we take this idea seriously I will break down the strongest reasons for and against. Given mainstream assumptions in the science of consciousness there are significant obstacles to consciousness in current models for example their lack of recurrent processing a global workspace and unified agency. At the same time it is quite possible that these obstacles will be overcome in the next decade or so. I conclude that while it is somewhat unlikely that current large language models are conscious we should take seriously the possibility that successors to large language models may be conscious in the not-too-distant future.</p>\n", "tags": [], "tsne_embedding": [45.933921813964844, -0.385913223028183]}, {"key": "jafari2019efficient", "year": "2019", "title": "Efficient Bitmap-based Indexing And Retrieval Of Similarity Search Image Queries", "abstract": "<p>Finding similar images is a necessary operation in many multimedia applications. Images are often represented and stored as a set of high-dimensional features which are extracted using localized feature extraction algorithms. Locality Sensitive Hashing is one of the most popular approximate processing techniques for finding similar points in high-dimensional spaces. Locality Sensitive Hashing (LSH) and its variants are designed to find similar points but they are not designed to find objects (such as images which are made up of a collection of points) efficiently. In this paper we propose an index structure Bitmap-Image LSH (bImageLSH) for efficient processing of high-dimensional images. Using a real dataset we experimentally show the performance benefit of our novel design while keeping the accuracy of the image results high.</p>\n", "tags": ["Independent", "LSH"], "tsne_embedding": [-15.745389938354492, 15.376566886901855]}, {"key": "jagadeesan2019understanding", "year": "2019", "title": "Understanding Sparse JL For Feature Hashing", "abstract": "<p>Feature hashing and other random projection schemes are commonly used to reduce the dimensionality of feature vectors. The goal is to efficiently project a high-dimensional feature vector living in R^n into a much lower-dimensional space R^m while approximately preserving Euclidean norm. These schemes can be constructed using sparse random projections for example using a sparse Johnson-Lindenstrauss (JL) transform. A line of work introduced by Weinberger et. al (ICML 09) analyzes the accuracy of sparse JL with sparsity 1 on feature vectors with small linfinity-to-l2 norm ratio. Recently Freksen Kamma and Larsen (NeurIPS 18) closed this line of work by proving a tight tradeoff between linfinity-to-l2 norm ratio and accuracy for sparse JL with sparsity 1. In this paper we demonstrate the benefits of using sparsity s greater than 1 in sparse JL on feature vectors. Our main result is a tight tradeoff between linfinity-to-l2 norm ratio and accuracy for a general sparsity s that significantly generalizes the result of Freksen et. al. Our result theoretically demonstrates that sparse JL with s 1 can have significantly better norm-preservation properties on feature vectors than sparse JL with s = 1; we also empirically demonstrate this finding.</p>\n", "tags": ["ICML", "Independent", "NEURIPS"], "tsne_embedding": [-34.9335823059082, 10.321836471557617]}, {"key": "jain2008online", "year": "2008", "title": "Online Metric Learning And Fast Similarity Search", "abstract": "<p>Metric learning algorithms can provide useful distance functions for a variety of domains and recent work has shown good accuracy for problems where the learner can access all distance constraints at once. However in many real applications constraints are only available incrementally thus necessitating methods that can perform online updates to the learned metric. Existing online algorithms offer bounds on worst-case performance but typically do not perform well in practice as compared to their offline counterparts. We present a new online metric learning algorithm that updates a learned Mahalanobis metric based on LogDet regularization and gradient descent. We prove theoretical worst-case performance bounds and empirically compare the proposed method against existing online metric learning algorithms. To further boost the practicality of our approach we develop an online locality-sensitive hashing scheme which leads to efficient updates for approximate similarity search data structures. We demonstrate our algorithm on multiple datasets and show that it outperforms relevant baselines.</p>\n", "tags": ["NEURIPS"], "tsne_embedding": [-5.274332523345947, 28.5656795501709]}, {"key": "jain2009fast", "year": "2009", "title": "Fast Similarity Search for Learned Metrics", "abstract": "<p>We propose a method to efficiently index into a large database of examples according to a learned metric.\nGiven a collection of examples, we learn a Mahalanobis distance using an information-theoretic metric\nlearning technique that adapts prior knowledge about pairwise distances to incorporate similarity and dissimilarity\nconstraints. To enable sub-linear time similarity search under the learned metric, we show how\nto encode a learned Mahalanobis parameterization into randomized locality-sensitive hash functions. We\nfurther formulate an indirect solution that enables metric learning and hashing for sparse input vector spaces\nwhose high dimensionality make it infeasible to learn an explicit weighting over the feature dimensions.\nWe demonstrate the approach applied to systems and image datasets, and show that our learned metrics\nimprove accuracy relative to commonly-used metric baselines, while our hashing construction permits effi-\ncient indexing with a learned distance and very large databases.</p>\n", "tags": ["Image Retrieval", "TPAMI"], "tsne_embedding": [-15.470417976379395, 18.691356658935547]}, {"key": "jain2010hashing", "year": "2010", "title": "Hashing Hyperplane Queries To Near Points With Applications To Large-scale Active Learning", "abstract": "<p>We consider the problem of retrieving the database points nearest to a given em hyperplane query without exhaustively scanning the database. We propose two hashing-based solutions. Our first approach maps the data to two-bit binary keys that are locality-sensitive for the angle between the hyperplane normal and a database point. Our second approach embeds the data into a vector space where the Euclidean norm reflects the desired distance between the original points and hyperplane query. Both use hashing to retrieve near points in sub-linear time. Our first methods preprocessing stage is more efficient while the second has stronger accuracy guarantees. We apply both to pool-based active learning taking the current hyperplane classifier as a query our algorithm identifies those points (approximately) satisfying the well-known minimal distance-to-hyperplane selection criterion. We empirically demonstrate our methods tradeoffs and show that they make it practical to perform active selection with millions of unlabeled points.</p>\n", "tags": ["NEURIPS"], "tsne_embedding": [-16.407047271728516, -10.129563331604004]}, {"key": "jain2017learning", "year": "2017", "title": "Learning A Complete Image Indexing Pipeline", "abstract": "<p>To work at scale a complete image indexing system comprises two components An inverted file index to restrict the actual search to only a subset that should contain most of the items relevant to the query; An approximate distance computation mechanism to rapidly scan these lists. While supervised deep learning has recently enabled improvements to the latter the former continues to be based on unsupervised clustering in the literature. In this work we propose a first system that learns both components within a unifying neural framework of structured binary encoding.</p>\n", "tags": ["ARXIV", "Deep Learning", "Unsupervised"], "tsne_embedding": [-15.458703994750977, 23.12849235534668]}, {"key": "jain2017subic", "year": "2017", "title": "SUBIC A Supervised Structured Binary Code For Image Search", "abstract": "<p>For large-scale visual search highly compressed yet meaningful representations of images are essential. Structured vector quantizers based on product quantization and its variants are usually employed to achieve such compression while minimizing the loss of accuracy. Yet unlike binary hashing schemes these unsupervised methods have not yet benefited from the supervision end-to-end learning and novel architectures ushered in by the deep learning revolution. We hence propose herein a novel method to make deep convolutional neural networks produce supervised compact structured binary codes for visual search. Our method makes use of a novel block-softmax non-linearity and of batch-based entropy losses that together induce structure in the learned encodings. We show that our method outperforms state-of-the-art compact representations based on deep hashing or structured quantization in single and cross-domain category retrieval instance retrieval and classification. We make our code and models publicly available online.</p>\n", "tags": ["ARXIV", "Cross Modal", "Deep Learning", "Quantisation", "Supervised"], "tsne_embedding": [0.2433827817440033, 9.10827350616455]}, {"key": "jain2023vcoder", "year": "2023", "title": "Vcoder Versatile Vision Encoders For Multimodal Large Language Models", "abstract": "<p>Humans possess the remarkable skill of Visual Perception the ability to see and understand the seen helping them make sense of the visual world and in turn reason. Multimodal Large Language Models (MLLM) have recently achieved impressive performance on vision-language tasks ranging from visual question-answering and image captioning to visual reasoning and image generation. However when prompted to identify or count (perceive) the entities in a given image existing MLLM systems fail. Working towards developing an accurate MLLM system for perception and reasoning we propose using Versatile vision enCoders (VCoder) as perception eyes for Multimodal LLMs. We feed the VCoder with perception modalities such as segmentation or depth maps improving the MLLMs perception abilities. Secondly we leverage the images from COCO and outputs from off-the-shelf vision perception models to create our COCO Segmentation Text (COST) dataset for training and evaluating MLLMs on the object perception task. Thirdly we introduce metrics to assess the object perception abilities in MLLMs on our COST dataset. Lastly we provide extensive experimental evidence proving the VCoders improved object-level perception skills over existing Multimodal LLMs including GPT-4V. We open-source our dataset code and models to promote research. We open-source our code at https://github.com/SHI-Labs/VCoder</p>\n", "tags": ["ARXIV", "Cross Modal", "Has Code"], "tsne_embedding": [41.589569091796875, 6.468178749084473]}, {"key": "jaiswal2023emergence", "year": "2023", "title": "The Emergence Of Essential Sparsity In Large Pre-trained Models The Weights That Matter", "abstract": "<p>Large pre-trained transformers are show-stealer in modern-day deep learning and it becomes crucial to comprehend the parsimonious patterns that exist within them as they grow in scale. With exploding parameter counts Lottery Ticket Hypothesis (LTH) and its variants have lost their pragmatism in sparsifying them due to high computation and memory bottleneck of repetitive train-prune-retrain routine of iterative magnitude pruning (IMP) which worsens with increasing model size. This paper comprehensively studies induced sparse patterns across multiple large pre-trained vision and language transformers. We propose the existence of \u2013 essential sparsity defined with a sharp dropping point beyond which the performance declines much faster w.r.t the rise of sparsity level when we directly remove weights with the smallest magnitudes in one-shot without re-training. We also find essential sparsity to hold valid for NM sparsity patterns as well as on modern-scale large language models (Vicuna-7B). We also present an intriguing emerging phenomenon of abrupt sparsification during the pre-training of BERT i.e. BERT suddenly becomes heavily sparse in pre-training after certain iterations. Moreover our observations also indicate a counter-intuitive finding that BERT trained with a larger amount of pre-training data tends to have a better ability to condense knowledge in comparatively relatively fewer parameters. Lastly we investigate the effect of the pre-training loss on essential sparsity and discover that self-supervised learning (SSL) objectives trigger stronger emergent sparsification properties than supervised learning (SL). Our codes are available at urlhttps://github.com/VITA-Group/essential_sparsity}.</p>\n", "tags": ["ARXIV", "Deep Learning", "Has Code", "Supervised"], "tsne_embedding": [14.727444648742676, -10.94243049621582]}, {"key": "james2019deephashing", "year": "2019", "title": "Deephashing Using Tripletloss", "abstract": "<p>Hashing is one of the most efficient techniques for approximate nearest neighbour search for large scale image retrieval. Most of the techniques are based on hand-engineered features and do not give optimal results all the time. Deep Convolutional Neural Networks have proven to generate very effective representation of images that are used for various computer vision tasks and inspired by this there have been several Deep Hashing models like Wang et al. (2016) have been proposed. These models train on the triplet loss function which can be used to train models with superior representation capabilities. Taking the latest advancements in training using the triplet loss I propose new techniques that help the Deep Hash-ing models train more faster and efficiently. Experiment result1show that using the more efficient techniques for training on the triplet loss we have obtained a 537;percent improvement in our model compared to the original work of Wang et al.(2016). Using a larger model and more training data we can drastically improve the performance using the techniques we propose</p>\n", "tags": ["ARXIV", "Image Retrieval", "Supervised"], "tsne_embedding": [8.912349700927734, 7.612663269042969]}, {"key": "jang2020generalized", "year": "2020", "title": "Generalized Product Quantization Network For Semi-supervised Image Retrieval", "abstract": "<p>Image retrieval methods that employ hashing or vector quantization have achieved great success by taking advantage of deep learning. However these approaches do not meet expectations unless expensive label information is sufficient. To resolve this issue we propose the first quantization-based semi-supervised image retrieval scheme Generalized Product Quantization (GPQ) network. We design a novel metric learning strategy that preserves semantic similarity between labeled data and employ entropy regularization term to fully exploit inherent potentials of unlabeled data. Our solution increases the generalization capacity of the quantization network which allows overcoming previous limitations in the retrieval community. Extensive experimental results demonstrate that GPQ yields state-of-the-art performance on large-scale real image benchmark datasets.</p>\n", "tags": ["ARXIV", "Deep Learning", "Image Retrieval", "Quantisation", "Supervised"], "tsne_embedding": [8.127518653869629, 9.523938179016113]}, {"key": "jang2021deep", "year": "2021", "title": "Deep Hash Distillation For Image Retrieval", "abstract": "<p>In hash-based image retrieval systems degraded or transformed inputs usually generate different codes from the original deteriorating the retrieval accuracy. To mitigate this issue data augmentation can be applied during training. However even if augmented samples of an image are similar in real feature space the quantization can scatter them far away in Hamming space. This results in representation discrepancies that can impede training and degrade performance. In this work we propose a novel self-distilled hashing scheme to minimize the discrepancy while exploiting the potential of augmented data. By transferring the hash knowledge of the weakly-transformed samples to the strong ones we make the hash code insensitive to various transformations. We also introduce hash proxy-based similarity learning and binary cross entropy-based quantization loss to provide fine quality hash codes. Ultimately we construct a deep hashing framework that not only improves the existing deep hashing approaches but also achieves the state-of-the-art retrieval results. Extensive experiments are conducted and confirm the effectiveness of our work.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Independent", "Quantisation"], "tsne_embedding": [-6.184168338775635, 10.448127746582031]}, {"key": "jang2021self", "year": "2021", "title": "Self-supervised Product Quantization For Deep Unsupervised Image Retrieval", "abstract": "<p>Supervised deep learning-based hash and vector quantization are enabling fast and large-scale image retrieval systems. By fully exploiting label annotations they are achieving outstanding retrieval performances compared to the conventional methods. However it is painstaking to assign labels precisely for a vast amount of training data and also the annotation process is error-prone. To tackle these issues we propose the first deep unsupervised image retrieval method dubbed Self-supervised Product Quantization (SPQ) network which is label-free and trained in a self-supervised manner. We design a Cross Quantized Contrastive learning strategy that jointly learns codewords and deep visual descriptors by comparing individually transformed images (views). Our method analyzes the image contents to extract descriptive features allowing us to understand image representations for accurate retrieval. By conducting extensive experiments on benchmarks we demonstrate that the proposed method yields state-of-the-art results even without supervised pretraining.</p>\n", "tags": ["ARXIV", "Deep Learning", "Image Retrieval", "Quantisation", "Unsupervised"], "tsne_embedding": [0.04116830974817276, 10.632020950317383]}, {"key": "jang2021similarity", "year": "2021", "title": "Similarity Guided Deep Face Image Retrieval", "abstract": "<p>Face image retrieval which searches for images of the same identity from the query input face image is drawing more attention as the size of the image database increases rapidly. In order to conduct fast and accurate retrieval a compact hash code-based methods have been proposed and recently deep face image hashing methods with supervised classification training have shown outstanding performance. However classification-based scheme has a disadvantage in that it cannot reveal complex similarities between face images into the hash code learning. In this paper we attempt to improve the face image retrieval quality by proposing a Similarity Guided Hashing (SGH) method which gently considers self and pairwise-similarity simultaneously. SGH employs various data augmentations designed to explore elaborate similarities between face images solving both intra and inter identity-wise difficulties. Extensive experimental results on the protocols with existing benchmarks and an additionally proposed large scale higher resolution face image dataset demonstrate that our SGH delivers state-of-the-art retrieval performance.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Supervised"], "tsne_embedding": [-9.05276107788086, 12.00843620300293]}, {"key": "jang2024distilling", "year": "2024", "title": "Distilling Vision-language Pretraining For Efficient Cross-modal Retrieval", "abstract": "<p>Learning to hash is a practical solution for efficient retrieval offering fast search speed and low storage cost. It is widely applied in various applications such as image-text cross-modal search. In this paper we explore the potential of enhancing the performance of learning to hash with the proliferation of powerful large pre-trained models such as Vision-Language Pre-training (VLP) models. We introduce a novel method named Distillation for Cross-Modal Quantization (DCMQ) which leverages the rich semantic knowledge of VLP models to improve hash representation learning. Specifically we use the VLP as a teacher to distill knowledge into a student hashing model equipped with codebooks. This process involves the replacement of supervised labels which are composed of multi-hot vectors and lack semantics with the rich semantics of VLP. In the end we apply a transformation termed Normalization with Paired Consistency (NPC) to achieve a discriminative target for distillation. Further we introduce a new quantization method Product Quantization with Gumbel (PQG) that promotes balanced codebook learning thereby improving the retrieval performance. Extensive benchmark testing demonstrates that DCMQ consistently outperforms existing supervised cross-modal hashing approaches showcasing its significant potential.</p>\n", "tags": ["ARXIV", "Cross Modal", "Quantisation", "Supervised"], "tsne_embedding": [4.103841781616211, -0.6249064803123474]}, {"key": "jeong2018efficient", "year": "2018", "title": "Efficient End-to-end Learning For Quantizable Representations", "abstract": "<p>Embedding representation learning via neural networks is at the core foundation of modern similarity based search. While much effort has been put in developing algorithms for learning binary hamming code representations for search efficiency this still requires a linear scan of the entire dataset per each query and trades off the search accuracy through binarization. To this end we consider the problem of directly learning a quantizable embedding representation and the sparse binary hash code end-to-end which can be used to construct an efficient hash table not only providing significant search reduction in the number of data but also achieving the state of the art search accuracy outperforming previous state of the art deep metric learning methods. We also show that finding the optimal sparse binary hash code in a mini-batch can be computed exactly in polynomial time by solving a minimum cost flow problem. Our results on Cifar-100 and on ImageNet datasets show the state of the art search accuracy in precision@k and NMI metrics while providing up to 98X and 478X search speedup respectively over exhaustive linear search. The source code is available at https://github.com/maestrojeong/Deep-Hash-Table-ICML18\u201d</p>\n", "tags": ["ARXIV", "Has Code", "Supervised"], "tsne_embedding": [0.35811638832092285, -5.55945348739624]}, {"key": "jeong2019end", "year": "2019", "title": "End-to-end Efficient Representation Learning Via Cascading Combinatorial Optimization", "abstract": "<p>We develop hierarchically quantized efficient embedding representations for similarity-based search and show that this representation provides not only the state of the art performance on the search accuracy but also provides several orders of speed up during inference. The idea is to hierarchically quantize the representation so that the quantization granularity is greatly increased while maintaining the accuracy and keeping the computational complexity low. We also show that the problem of finding the optimal sparse compound hash code respecting the hierarchical structure can be optimized in polynomial time via minimum cost flow in an equivalent flow network. This allows us to train the method end-to-end in a mini-batch stochastic gradient descent setting. Our experiments on Cifar100 and ImageNet datasets show the state of the art search accuracy while providing several orders of magnitude search speedup respectively over exhaustive linear search over the dataset.</p>\n", "tags": ["ARXIV", "Independent", "Quantisation"], "tsne_embedding": [-22.035043716430664, 5.806942462921143]}, {"key": "jhuo2017set", "year": "2017", "title": "Set-to-set Hashing With Applications In Visual Recognition", "abstract": "<p>Visual data such as an image or a sequence of video frames is often naturally represented as a point set. In this paper we consider the fundamental problem of finding a nearest set from a collection of sets to a query set. This problem has obvious applications in large-scale visual retrieval and recognition and also in applied fields beyond computer vision. One challenge stands out in solving the problem\u2014set representation and measure of similarity. Particularly the query set and the sets in dataset collection can have varying cardinalities. The training collection is large enough such that linear scan is impractical. We propose a simple representation scheme that encodes both statistical and structural information of the sets. The derived representations are integrated in a kernel framework for flexible similarity measurement. For the query set process we adopt a learning-to-hash pipeline that turns the kernel representations into hash bits based on simple learners using multiple kernel learning. Experiments on two visual retrieval datasets show unambiguously that our set-to-set hashing framework outperforms prior methods that do not take the set-to-set search setting.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-6.260605812072754, -7.075173854827881]}, {"key": "ji2011image", "year": "2011", "title": "Image Retrieval Method Using Top-surf Descriptor", "abstract": "<p>This report presents the results and details of a content-based image retrieval project using the Top-surf descriptor. The experimental results are preliminary however it shows the capability of deducing objects from parts of the objects or from the objects that are similar. This paper uses a dataset consisting of 1200 images of which 800 images are equally divided into 8 categories namely airplane beach motorbike forest elephants horses bus and building while the other 400 images are randomly picked from the Internet. The best results achieved are from building category.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Independent"], "tsne_embedding": [4.3624091148376465, 19.075448989868164]}, {"key": "ji2012super", "year": "2012", "title": "Super-bit Locality-sensitive Hashing", "abstract": "<p>Sign-random-projection locality-sensitive hashing (SRP-LSH) is a probabilistic dimension reduction method which provides an unbiased estimate of angular similarity yet suffers from the large variance of its estimation. In this work we propose the Super-Bit locality-sensitive hashing (SBLSH). It is easy to implement which orthogonalizes the random projection vectors in batches and it is theoretically guaranteed that SBLSH also provides an unbiased estimate of angular similarity yet with a smaller variance when the angle to estimate is within ((0pi/2). The extensive experiments on real data well validate that given the same length of binary code SBLSH may achieve significant mean squared error reduction in estimating pairwise angular similarity. Moreover SBLSH shows the superiority over SRP-LSH in approximate nearest neighbor (ANN) retrieval experiments.</p>\n", "tags": ["Independent", "LSH", "NEURIPS"], "tsne_embedding": [-31.50992774963379, 9.807817459106445]}, {"key": "ji2018attribute", "year": "2018", "title": "Attribute-guided Network For Cross-modal Zero-shot Hashing", "abstract": "<p>Zero-Shot Hashing aims at learning a hashing model that is trained only by instances from seen categories but can generate well to those of unseen categories. Typically it is achieved by utilizing a semantic embedding space to transfer knowledge from seen domain to unseen domain. Existing efforts mainly focus on single-modal retrieval task especially Image-Based Image Retrieval (IBIR). However as a highlighted research topic in the field of hashing cross-modal retrieval is more common in real world applications. To address the Cross-Modal Zero-Shot Hashing (CMZSH) retrieval task we propose a novel Attribute-Guided Network (AgNet) which can perform not only IBIR but also Text-Based Image Retrieval (TBIR). In particular AgNet aligns different modal data into a semantically rich attribute space which bridges the gap caused by modality heterogeneity and zero-shot setting. We also design an effective strategy that exploits the attribute to guide the generation of hash codes for image and text within the same network. Extensive experimental results on three benchmark datasets (AwA SUN and ImageNet) demonstrate the superiority of AgNet on both cross-modal and single-modal zero-shot image retrieval tasks.</p>\n", "tags": ["ARXIV", "Cross Modal", "Image Retrieval", "Independent"], "tsne_embedding": [-5.810679912567139, 6.782501220703125]}, {"key": "jia2022fast", "year": "2022", "title": "Fast Online Hashing With Multi-label Projection", "abstract": "<p>Hashing has been widely researched to solve the large-scale approximate nearest neighbor search problem owing to its time and storage superiority. In recent years a number of online hashing methods have emerged which can update the hash functions to adapt to the new stream data and realize dynamic retrieval. However existing online hashing methods are required to update the whole database with the latest hash functions when a query arrives which leads to low retrieval efficiency with the continuous increase of the stream data. On the other hand these methods ignore the supervision relationship among the examples especially in the multi-label case. In this paper we propose a novel Fast Online Hashing (FOH) method which only updates the binary codes of a small part of the database. To be specific we first build a query pool in which the nearest neighbors of each central point are recorded. When a new query arrives only the binary codes of the corresponding potential neighbors are updated. In addition we create a similarity matrix which takes the multi-label supervision information into account and bring in the multi-label projection loss to further preserve the similarity among the multi-label data. The experimental results on two common benchmarks show that the proposed FOH can achieve dramatic superiority on query time up to 6.28 seconds less than state-of-the-art baselines with competitive retrieval accuracy.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-10.929327964782715, -18.74225616455078]}, {"key": "jia2023fast", "year": "2023", "title": "Fast Online Hashing with Multi-Label Projection", "abstract": "<p>Hashing has been widely researched to solve the large-scale approximate nearest neighbor search problem owing to its time and storage superiority. In recent years, a number of online hashing methods have emerged, which can update the hash functions to adapt to the new stream data and realize dynamic retrieval. However, existing online hashing methods are required to update the whole database with the latest hash functions when a query arrives, which leads to low retrieval efficiency with the continuous increase of the stream data. On the other hand, these methods ignore the supervision relationship among the examples, especially in the multi-label case. In this paper, we propose a novel Fast Online Hashing (FOH) method which only updates the binary codes of a small part of the database. To be specific, we first build a query pool in which the nearest neighbors of each central point are recorded. When a new query arrives, only the binary codes of the corresponding potential neighbors are updated. In addition, we create a similarity matrix which takes the multi-label supervision information into account and bring in the multi-label projection loss to further preserve the similarity among the multi-label data. The experimental results on two common benchmarks show that the proposed FOH can achieve dramatic superiority on query time up to 6.28 seconds less than state-of-the-art baselines with competitive retrieval accuracy.</p>\n", "tags": ["AAAI"], "tsne_embedding": [-10.9196138381958, -18.752521514892578]}, {"key": "jiang2014revisiting", "year": "2014", "title": "Revisiting Kernelized Locality-sensitive Hashing For Improved Large-scale Image Retrieval", "abstract": "<p>We present a simple but powerful reinterpretation of kernelized locality-sensitive hashing (KLSH) a general and popular method developed in the vision community for performing approximate nearest-neighbor searches in an arbitrary reproducing kernel Hilbert space (RKHS). Our new perspective is based on viewing the steps of the KLSH algorithm in an appropriately projected space and has several key theoretical and practical benefits. First it eliminates the problematic conceptual difficulties that are present in the existing motivation of KLSH. Second it yields the first formal retrieval performance bounds for KLSH. Third our analysis reveals two techniques for boosting the empirical performance of KLSH. We evaluate these extensions on several large-scale benchmark image retrieval data sets and show that our analysis leads to improved recall performance of at least 1237; and sometimes much higher over the standard KLSH method.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Independent"], "tsne_embedding": [-17.644973754882812, 18.23873519897461]}, {"key": "jiang2015scalable", "year": "2015", "title": "Scalable Graph Hashing with Feature Transformation", "abstract": "<p>Hashing has been widely used for approximate nearest\nneighbor (ANN) search in big data applications\nbecause of its low storage cost and fast retrieval\nspeed. The goal of hashing is to map the data\npoints from the original space into a binary-code\nspace where the similarity (neighborhood structure)\nin the original space is preserved. By directly\nexploiting the similarity to guide the hashing\ncode learning procedure, graph hashing has attracted\nmuch attention. However, most existing graph\nhashing methods cannot achieve satisfactory performance\nin real applications due to the high complexity\nfor graph modeling. In this paper, we propose\na novel method, called scalable graph hashing\nwith feature transformation (SGH), for large-scale\ngraph hashing. Through feature transformation, we\ncan effectively approximate the whole graph without\nexplicitly computing the similarity graph matrix,\nbased on which a sequential learning method\nis proposed to learn the hash functions in a bit-wise\nmanner. Experiments on two datasets with one million\ndata points show that our SGH method can\noutperform the state-of-the-art methods in terms of\nboth accuracy and scalability.</p>\n", "tags": ["Has Code", "IJCAI"], "tsne_embedding": [-0.3347880244255066, 27.777673721313477]}, {"key": "jiang2016deep", "year": "2016", "title": "Deep Cross-modal Hashing", "abstract": "<p>Due to its low storage cost and fast query speed cross-modal hashing (CMH) has been widely used for similarity search in multimedia retrieval applications. However almost all existing CMH methods are based on hand-crafted features which might not be optimally compatible with the hash-code learning procedure. As a result existing CMH methods with handcrafted features may not achieve satisfactory performance. In this paper we propose a novel cross-modal hashing method called deep crossmodal hashing (DCMH) by integrating feature learning and hash-code learning into the same framework. DCMH is an end-to-end learning framework with deep neural networks one for each modality to perform feature learning from scratch. Experiments on two real datasets with text-image modalities show that DCMH can outperform other baselines to achieve the state-of-the-art performance in cross-modal retrieval applications.</p>\n", "tags": ["ARXIV", "Cross Modal", "Supervised"], "tsne_embedding": [9.21169662475586, -6.256801605224609]}, {"key": "jiang2017asymmetric", "year": "2017", "title": "Asymmetric Deep Supervised Hashing", "abstract": "<p>Hashing has been widely used for large-scale approximate nearest neighbor search because of its storage and search efficiency. Recent work has found that deep supervised hashing can significantly outperform non-deep supervised hashing in many applications. However most existing deep supervised hashing methods adopt a symmetric strategy to learn one deep hash function for both query points and database (retrieval) points. The training of these symmetric deep supervised hashing methods is typically time-consuming which makes them hard to effectively utilize the supervised information for cases with large-scale database. In this paper we propose a novel deep supervised hashing method called asymmetric deep supervised hashing (ADSH) for large-scale nearest neighbor search. ADSH treats the query points and database points in an asymmetric way. More specifically ADSH learns a deep hash function only for query points while the hash codes for database points are directly learned. The training of ADSH is much more efficient than that of traditional symmetric deep supervised hashing methods. Experiments show that ADSH can achieve state-of-the-art performance in real applications.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [1.774277687072754, -8.063252449035645]}, {"key": "jiang2017deep", "year": "2017", "title": "Deep Cross-Modal Hashing", "abstract": "<p>Due to its low storage cost and fast query speed, crossmodal hashing (CMH) has been widely used for similarity\nsearch in multimedia retrieval applications. However, most\nexisting CMH methods are based on hand-crafted features\nwhich might not be optimally compatible with the hash-code\nlearning procedure. As a result, existing CMH methods\nwith hand-crafted features may not achieve satisfactory\nperformance. In this paper, we propose a novel CMH\nmethod, called deep cross-modal hashing (DCMH), by\nintegrating feature learning and hash-code learning into\nthe same framework. DCMH is an end-to-end learning\nframework with deep neural networks, one for each modality, to perform feature learning from scratch. Experiments\non three real datasets with image-text modalities show\nthat DCMH can outperform other baselines to achieve\nthe state-of-the-art performance in cross-modal retrieval\napplications.</p>\n", "tags": ["CVPR", "Cross Modal", "Deep Learning", "Has Code", "Image Retrieval"], "tsne_embedding": [9.227862358093262, -6.242549419403076]}, {"key": "jiang2017discrete", "year": "2017", "title": "Discrete Latent Factor Model For Cross-modal Hashing", "abstract": "<p>Due to its storage and retrieval efficiency cross-modal hashing~(CMH) has been widely used for cross-modal similarity search in multimedia applications. According to the training strategy existing CMH methods can be mainly divided into two categories relaxation-based continuous methods and discrete methods. In general the training of relaxation-based continuous methods is faster than discrete methods but the accuracy of relaxation-based continuous methods is not satisfactory. On the contrary the accuracy of discrete methods is typically better than relaxation-based continuous methods but the training of discrete methods is time-consuming. In this paper we propose a novel CMH method called discrete latent factor model based cross-modal hashing~(DLFH) for cross modal similarity search. DLFH is a discrete method which can directly learn the binary hash codes for CMH. At the same time the training of DLFH is efficient. Experiments on real datasets show that DLFH can achieve significantly better accuracy than existing methods and the training time of DLFH is comparable to that of relaxation-based continuous methods which are much faster than existing discrete methods.</p>\n", "tags": ["ARXIV", "Cross Modal", "Independent"], "tsne_embedding": [-1.5018856525421143, -9.834680557250977]}, {"key": "jiang2019evaluation", "year": "2019", "title": "On The Evaluation Metric For Hashing", "abstract": "<p>Due to its low storage cost and fast query speed hashing has been widely used for large-scale approximate nearest neighbor (ANN) search. Bucket search also called hash lookup can achieve fast query speed with a sub-linear time cost based on the inverted index table constructed from hash codes. Many metrics have been adopted to evaluate hashing algorithms. However all existing metrics are improper to evaluate the hash codes for bucket search. On one hand all existing metrics ignore the retrieval time cost which is an important factor reflecting the performance of search. On the other hand some of them such as mean average precision (MAP) suffer from the uncertainty problem as the ranked list is based on integer-valued Hamming distance and are insensitive to Hamming radius as these metrics only depend on relative Hamming distance. Other metrics such as precision at Hamming radius R fail to evaluate global performance as these metrics only depend on one specific Hamming radius. In this paper we first point out the problems of existing metrics which have been ignored by the hashing community and then propose a novel evaluation metric called radius aware mean average precision (RAMAP) to evaluate hash codes for bucket search. Furthermore two coding strategies are also proposed to qualitatively show the problems of existing metrics. Experiments demonstrate that our proposed RAMAP can provide more proper evaluation than existing metrics.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-22.2130184173584, -4.1054253578186035]}, {"key": "jiang2019graph", "year": "2019", "title": "Graph-based Multi-view Binary Learning For Image Clustering", "abstract": "<p>Hashing techniques also known as binary code learning have recently gained increasing attention in large-scale data analysis and storage. Generally most existing hash clustering methods are single-view ones which lack complete structure or complementary information from multiple views. For cluster tasks abundant prior researches mainly focus on learning discrete hash code while few works take original data structure into consideration. To address these problems we propose a novel binary code algorithm for clustering which adopts graph embedding to preserve the original data structure called (Graph-based Multi-view Binary Learning) GMBL in this paper. GMBL mainly focuses on encoding the information of multiple views into a compact binary code which explores complementary information from multiple views. In particular in order to maintain the graph-based structure of the original data we adopt a Laplacian matrix to preserve the local linear relationship of the data and map it to the Hamming space. Considering different views have distinctive contributions to the final clustering results GMBL adopts a strategy of automatically assign weights for each view to better guide the clustering. Finally An alternating iterative optimization method is adopted to optimize discrete binary codes directly instead of relaxing the binary constraint in two steps. Experiments on five public datasets demonstrate the superiority of our proposed method compared with previous approaches in terms of clustering performance.</p>\n", "tags": ["ARXIV", "Cross Modal", "Graph", "Unsupervised"], "tsne_embedding": [-1.8245325088500977, 27.84871482849121]}, {"key": "jiang2023hallucination", "year": "2023", "title": "Hallucination Augmented Contrastive Learning For Multimodal Large Language Model", "abstract": "<p>Multi-modal large language models (MLLMs) have been shown to efficiently integrate natural language with visual information to handle multi-modal tasks. However MLLMs still face a fundamental limitation of hallucinations where they tend to generate erroneous or fabricated information. In this paper we address hallucinations in MLLMs from a novel perspective of representation learning. We first analyzed the representation distribution of textual and visual tokens in MLLM revealing two important findings 1) there is a significant gap between textual and visual representations indicating unsatisfactory cross-modal representation alignment; 2) representations of texts that contain and do not contain hallucinations are entangled making it challenging to distinguish them. These two observations inspire us with a simple yet effective method to mitigate hallucinations. Specifically we introduce contrastive learning into MLLMs and use text with hallucination as hard negative examples naturally bringing representations of non-hallucinative text and visual samples closer while pushing way representations of non-hallucinating and hallucinative text. We evaluate our method quantitatively and qualitatively showing its effectiveness in reducing hallucination occurrences and improving performance across multiple benchmarks. On the MMhal-Bench benchmark our method obtains a 34.6637; /29.537; improvement over the baseline MiniGPT-4/LLaVA. Our code is available on https://github.com/X-PLUG/mPLUG-HalOwl/tree/main/hacl.</p>\n", "tags": ["ARXIV", "Cross Modal", "Has Code", "Self Supervised"], "tsne_embedding": [48.704811096191406, 6.400805473327637]}, {"key": "jiao2023is", "year": "2023", "title": "Is Chatgpt A Good Translator Yes With GPT-4 As The Engine", "abstract": "<p>This report provides a preliminary evaluation of ChatGPT for machine translation including translation prompt multilingual translation and translation robustness. We adopt the prompts advised by ChatGPT to trigger its translation ability and find that the candidate prompts generally work well with minor performance differences. By evaluating on a number of benchmark test sets we find that ChatGPT performs competitively with commercial translation products (e.g. Google Translate) on high-resource European languages but lags behind significantly on low-resource or distant languages. As for the translation robustness ChatGPT does not perform as well as the commercial systems on biomedical abstracts or Reddit comments but exhibits good results on spoken language. Further we explore an interesting strategy named () for distant languages which asks ChatGPT to translate the source sentence into a high-resource pivot language before into the target language improving the translation performance noticeably. With the launch of the GPT-4 engine the translation performance of ChatGPT is significantly boosted becoming comparable to commercial translation products even for distant languages. Human analysis on Google Translate and ChatGPT suggests that ChatGPT with GPT-3.5 tends to generate more hallucinations and mis-translation errors while that with GPT-4 makes the least errors. In other words ChatGPT has already become a good translator. Please refer to our Github project for more details https://github.com/wxjiao/Is-ChatGPT-A-Good-Translator</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [35.63603591918945, -1.2202173471450806]}, {"key": "jie2013novel", "year": "2013", "title": "A Novel Block-dct And PCA Based Image Perceptual Hashing Algorithm", "abstract": "<p>Image perceptual hashing finds applications in content indexing large-scale image database management certification and authentication and digital watermarking. We propose a Block-DCT and PCA based image perceptual hash in this article and explore the algorithm in the application of tamper detection. The main idea of the algorithm is to integrate color histogram and DCT coefficients of image blocks as perceptual feature then to compress perceptual features as inter-feature with PCA and to threshold to create a robust hash. The robustness and discrimination properties of the proposed algorithm are evaluated in detail. Our algorithms first construct a secondary image derived from input image by pseudo-randomly extracting features that approximately capture semi-global geometric characteristics. From the secondary image (which does not perceptually resemble the input) we further extract the final features which can be used as a hash value (and can be further suitably quantized). In this paper we use spectral matrix invariants as embodied by Singular Value Decomposition. Surprisingly formation of the secondary image turns out be quite important since it not only introduces further robustness but also enhances the security properties. Indeed our experiments reveal that our hashing algorithms extract most of the geometric information from the images and hence are robust to severe perturbations (e.g. up to 37;50 cropping by area with 20 degree rotations) on images while avoiding misclassification. Experimental results show that the proposed image perceptual hash algorithm can effectively address the tamper detection problem with advantageous robustness and discrimination.</p>\n", "tags": ["ARXIV", "Unsupervised"], "tsne_embedding": [-9.650311470031738, 5.4540324211120605]}, {"key": "jin2013complementary", "year": "2013", "title": "Complementary Projection Hashing", "abstract": "<p>Recently, hashing techniques have been widely applied\nto solve the approximate nearest neighbors search problem\nin many vision applications. Generally, these hashing\napproaches generate 2^c buckets, where c is the length\nof the hash code. A good hashing method should satisfy\nthe following two requirements: 1) mapping the nearby\ndata points into the same bucket or nearby (measured by\nthe Hamming distance) buckets. 2) all the data points are\nevenly distributed among all the buckets. In this paper,\nwe propose a novel algorithm named Complementary Projection\nHashing (CPH) to find the optimal hashing functions\nwhich explicitly considers the above two requirements.\nSpecifically, CPH aims at sequentially finding a series of hyperplanes\n(hashing functions) which cross the sparse region\nof the data. At the same time, the data points are evenly distributed\nin the hypercubes generated by these hyperplanes.\nThe experiments comparing with the state-of-the-art hashing\nmethods demonstrate the effectiveness of the proposed\nmethod.</p>\n", "tags": ["Has Code", "ICCV"], "tsne_embedding": [-21.947681427001953, 3.400972604751587]}, {"key": "jin2018deep", "year": "2018", "title": "Deep Ordinal Hashing With Spatial Attention", "abstract": "<p>Hashing has attracted increasing research attentions in recent years due to its high efficiency of computation and storage in image retrieval. Recent works have demonstrated the superiority of simultaneous feature representations and hash functions learning with deep neural networks. However most existing deep hashing methods directly learn the hash functions by encoding the global semantic information while ignoring the local spatial information of images. The loss of local spatial structure makes the performance bottleneck of hash functions therefore limiting its application for accurate similarity retrieval. In this work we propose a novel Deep Ordinal Hashing (DOH) method which learns ordinal representations by leveraging the ranking structure of feature space from both local and global views. In particular to effectively build the ranking structure we propose to learn the rank correlation space by exploiting the local spatial information from Fully Convolutional Network (FCN) and the global semantic information from the Convolutional Neural Network (CNN) simultaneously. More specifically an effective spatial attention model is designed to capture the local spatial information by selectively learning well-specified locations closely related to target objects. In such hashing frameworkthe local spatial and global semantic nature of images are captured in an end-to-end ranking-to-hashing manner. Experimental results conducted on three widely-used datasets demonstrate that the proposed DOH method significantly outperforms the state-of-the-art hashing methods.</p>\n", "tags": ["ARXIV", "CNN", "Image Retrieval", "Supervised"], "tsne_embedding": [-3.4031028747558594, 14.864944458007812]}, {"key": "jin2018unsupervised", "year": "2018", "title": "Unsupervised Semantic Deep Hashing", "abstract": "<p>In recent years deep hashing methods have been proved to be efficient since it employs convolutional neural network to learn features and hashing codes simultaneously. However these methods are mostly supervised. In real-world application it is a time-consuming and overloaded task for annotating a large number of images. In this paper we propose a novel unsupervised deep hashing method for large-scale image retrieval. Our method namely unsupervised semantic deep hashing (textbfUSDH) uses semantic information preserved in the CNN feature layer to guide the training of network. We enforce four criteria on hashing codes learning based on VGG-19 model 1) preserving relevant information of feature space in hashing space; 2) minimizing quantization loss between binary-like codes and hashing codes; 3) improving the usage of each bit in hashing codes by using maximum information entropy and 4) invariant to image rotation. Extensive experiments on CIFAR-10 NUSWIDE have demonstrated that textbfUSDH outperforms several state-of-the-art unsupervised hashing methods for image retrieval. We also conduct experiments on Oxford 17 datasets for fine-grained classification to verify its efficiency for other computer vision tasks.</p>\n", "tags": ["ARXIV", "CNN", "Image Retrieval", "Quantisation", "Unsupervised"], "tsne_embedding": [3.366109848022461, 12.31507396697998]}, {"key": "jin2019deep", "year": "2019", "title": "Deep Saliency Hashing for Fine-grained Retrieval", "abstract": "<p>In recent years, hashing methods have been proved to be\neffective and efficient for the large-scale Web media search.\nHowever, the existing general hashing methods have limited discriminative power for describing fine-grained objects that share similar overall appearance but have subtle\ndifference. To solve this problem, we for the first time introduce the attention mechanism to the learning of fine-grained\nhashing codes. Specifically, we propose a novel deep hashing model, named deep saliency hashing (DSaH), which\nautomatically mines salient regions and learns semanticpreserving hashing codes simultaneously. DSaH is a twostep end-to-end model consisting of an attention network\nand a hashing network. Our loss function contains three\nbasic components, including the semantic loss, the saliency\nloss, and the quantization loss. As the core of DSaH, the\nsaliency loss guides the attention network to mine discriminative regions from pairs of images. We conduct extensive experiments on both fine-grained and general retrieval\ndatasets for performance evaluation. Experimental results\non fine grained dataset, including Oxford Flowers-17, Stanford Dogs-120 and CUB Bird demonstrate that our DSaH\nperforms the best for fine-grained retrieval task and beats\nstrongest competitor (DTQ) by approximately 10% on both\nStanford Dogs-120 and CUB Bird. DSaH is also comparable to several state-of-the-art hashing methods on general\ndatasets, including CIFAR-10 and NUS-WIDE.</p>\n", "tags": ["ARXIV", "Deep Learning", "Image Retrieval"], "tsne_embedding": [3.9266178607940674, 7.728466510772705]}, {"key": "jin2020semi", "year": "2020", "title": "SSAH: Semi-supervised Adversarial Deep Hashing with Self-paced Hard Sample Generation", "abstract": "<p>Deep hashing methods have been proved to be effective and efficient for large-scale Web media search. The success of these data-driven methods largely depends on collecting sufficient labeled data, which is usually a crucial limitation in practical cases. The current solutions to this issue utilize Generative Adversarial Network (GAN) to augment data in semi-supervised learning. However, existing GAN-based methods treat image generations and hashing learning as two isolated processes, leading to generation ineffectiveness. Besides, most works fail to exploit the semantic information in unlabeled data. In this paper, we propose a novel Semi-supervised Self-pace Adversarial Hashing method, named SSAH to solve the above problems in a unified framework. The SSAH method consists of an adversarial network (A-Net) and a hashing network (H-Net). To improve the quality of generative images, first, the A-Net learns hard samples with multi-scale occlusions and multi-angle rotated deformations which compete against the learning of accurate hashing codes. Second, we design a novel self-paced hard generation policy to gradually increase the hashing difficulty of generated samples. To make use of the semantic information in unlabeled ones, we propose a semi-supervised consistent loss. The experimental results show that our method can significantly improve state-of-the-art models on both the widely-used hashing datasets and fine-grained datasets.</p>\n", "tags": ["AAAI", "Deep Learning", "Semi Supervised"], "tsne_embedding": [-2.5168163776397705, 2.5066921710968018]}, {"key": "jin2021unsupervised", "year": "2021", "title": "Unsupervised Discrete Hashing with Affinity Similarity", "abstract": "<p>In recent years, supervised hashing has been validated to greatly boost the performance of image retrieval. However, the label-hungry property requires massive label collection, making it intractable in practical scenarios. To liberate the model training procedure from laborious manual annotations, some unsupervised methods are proposed. However, the following two factors make unsupervised algorithms inferior to their supervised counterparts: (1) Without manually-defined labels, it is difficult to capture the semantic information across data, which is of crucial importance to guide robust binary code learning. (2) The widely adopted relaxation on binary constraints results in quantization error accumulation in the optimization procedure. To address the above-mentioned problems, in this paper, we propose a novel Unsupervised Discrete Hashing method (UDH). Specifically, to capture the semantic information, we propose a balanced graph-based semantic loss which explores the affinity priors in the original feature space. Then, we propose a novel self-supervised loss, termed orthogonal consistent loss, which can leverage semantic loss of instance and impose independence of codes. Moreover, by integrating the discrete optimization into the proposed unsupervised framework, the binary constraints are consistently preserved, alleviating the influence of quantization errors. Extensive experiments demonstrate that UDH outperforms state-of-the-art unsupervised methods for image retrieval.</p>\n", "tags": ["Supervised", "TIP"], "tsne_embedding": [-0.09280333667993546, 2.5453875064849854]}, {"key": "jin2023tab", "year": "2023", "title": "Tab-cot Zero-shot Tabular Chain Of Thought", "abstract": "<p>The chain-of-though (CoT) prompting methods were successful in various natural language processing (NLP) tasks thanks to their ability to unveil the underlying complex reasoning processes. Such reasoning processes typically exhibit implicitly structured steps. Recent efforts also started investigating methods to encourage more explicitly structured reasoning procedures to be captured. In this work we propose Tab-CoT a novel tabular-format CoT prompting method which allows the complex reasoning process to be explicitly modelled in a highly structured manner. Despite its simplicity we show that our approach is capable of performing reasoning across multiple dimensions (i.e. both rows and columns). We demonstrate our approachs strong zero-shot and few-shot capabilities through extensive experiments on a range of reasoning tasks.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [36.83021545410156, 3.9174435138702393]}, {"key": "jin2023unified", "year": "2023", "title": "Unified Language-vision Pretraining In LLM With Dynamic Discrete Visual Tokenization", "abstract": "<p>Recently the remarkable advance of the Large Language Model (LLM) has inspired researchers to transfer its extraordinary reasoning capability to both vision and language data. However the prevailing approaches primarily regard the visual input as a prompt and focus exclusively on optimizing the text generation process conditioned upon vision content by a frozen LLM. Such an inequitable treatment of vision and language heavily constrains the models potential. In this paper we break through this limitation by representing both vision and language in a unified form. Specifically we introduce a well-designed visual tokenizer to translate the non-linguistic image into a sequence of discrete tokens like a foreign language that LLM can read. The resulting visual tokens encompass high-level semantics worthy of a word and also support dynamic sequence length varying from the image. Coped with this tokenizer the presented foundation model called LaVIT can handle both image and text indiscriminately under the same generative learning paradigm. This unification empowers LaVIT to serve as an impressive generalist interface to understand and generate multi-modal content simultaneously. Extensive experiments further showcase that it outperforms the existing models by a large margin on massive vision-language tasks. Our code and models are available at https://github.com/jy0205/LaVIT.</p>\n", "tags": ["ARXIV", "Cross Modal", "Has Code"], "tsne_embedding": [32.554935455322266, -2.2823901176452637]}, {"key": "jing2023faithscore", "year": "2023", "title": "FAITHSCORE Evaluating Hallucinations In Large Vision-language Models", "abstract": "<p>We introduce FAITHSCORE (Faithfulness to Atomic Image Facts Score) a reference-free and fine-grained evaluation metric that measures the faithfulness of the generated free-form answers from large vision-language models (LVLMs). The FAITHSCORE evaluation first identifies sub-sentences containing descriptive statements that need to be verified then extracts a comprehensive list of atomic facts from these sub-sentences and finally conducts consistency verification between fine-grained atomic facts and the input image. Meta-evaluation demonstrates that our metric highly correlates with human judgments of faithfulness. We collect two benchmark datasets (i.e. LLaVA-1k and MSCOCO-Cap) for evaluating LVLMs instruction-following hallucinations. We measure hallucinations in state-of-the-art LVLMs with FAITHSCORE on the datasets. Results reveal that current systems are prone to generate hallucinated content unfaithful to the image which leaves room for future improvements. Further we find that current LVLMs despite doing well on color and counting still struggle with long answers relations and multiple objects.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [48.56284713745117, 9.415979385375977]}, {"key": "joon2022self", "year": "2022", "title": "Self-generated In-context Learning Leveraging Auto-regressive Language Models As A Demonstration Generator", "abstract": "<p>Large-scale pre-trained language models (PLMs) are well-known for being capable of solving a task simply by conditioning a few input-label pairs dubbed demonstrations on a prompt without being explicitly tuned for the desired downstream task. Such a process (i.e. in-context learning) however naturally leads to high reliance on the demonstrations which are usually selected from external datasets. In this paper we propose self-generated in-context learning (SG-ICL) which generates demonstrations for in-context learning from PLM itself to minimize the reliance on the external demonstration. We conduct experiments on four different text classification tasks and show SG-ICL significantly outperforms zero-shot learning and is generally worth approximately 0.6 gold training samples. Moreover our generated demonstrations show more consistent performance with low variance compared to randomly selected demonstrations from the training dataset.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [24.06349754333496, -3.2403929233551025]}, {"key": "junussov2019note", "year": "2019", "title": "Note On Distance Matrix Hashing", "abstract": "<p>Hashing algorithm of dynamical set of distances is described. Proposed hashing function is residual. Data structure which implementation accelerates computations is presented</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-24.119232177734375, 3.0897634029388428]}, {"key": "j\u00e9gou2011anti", "year": "2011", "title": "Anti-sparse Coding For Approximate Nearest Neighbor Search", "abstract": "<p>This paper proposes a binarization scheme for vectors of high dimension based on the recent concept of anti-sparse coding and shows its excellent performance for approximate nearest neighbor search. Unlike other binarization schemes this framework allows up to a scaling factor the explicit reconstruction from the binary representation of the original vector. The paper also shows that random projections which are used in Locality Sensitive Hashing algorithms are significantly outperformed by regular frames for both synthetic and real data if the number of bits exceeds the vector dimensionality i.e. when high precision is required.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-26.118717193603516, 8.20002555847168]}, {"key": "k2022can", "year": "2022", "title": "Can Language Models Learn From Explanations In Context", "abstract": "<p>Language Models (LMs) can perform new tasks by adapting to a few in-context examples. For humans explanations that connect examples to task principles can improve learning. We therefore investigate whether explanations of few-shot examples can help LMs. We annotate questions from 40 challenging tasks with answer explanations and various matched control explanations. We evaluate how different types of explanations instructions and controls affect zero- and few-shot performance. We analyze these results using statistical multilevel modeling techniques that account for the nested dependencies among conditions tasks prompts and models. We find that explanations can improve performance \u2013 even without tuning. Furthermore explanations hand-tuned for performance on a small validation set offer substantially larger benefits and building a prompt by selecting examples and explanations together substantially improves performance over selecting examples alone. Finally even untuned explanations outperform carefully matched controls suggesting that the benefits are due to the link between an example and its explanation rather than lower-level features. However only large models benefit. In summary explanations can support the in-context learning of large LMs on challenging tasks.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [38.55101776123047, 0.3890189230442047]}, {"key": "kaga2019pdh", "year": "2019", "title": "PDH Probabilistic Deep Hashing Based On MAP Estimation Of Hamming Distance", "abstract": "<p>With the growth of image on the web research on hashing which enables high-speed image retrieval has been actively studied. In recent years various hashing methods based on deep neural networks have been proposed and achieved higher precision than the other hashing methods. In these methods multiple losses for hash codes and the parameters of neural networks are defined. They generate hash codes that minimize the weighted sum of the losses. Therefore an expert has to tune the weights for the losses heuristically and the probabilistic optimality of the loss function cannot be explained. In order to generate explainable hash codes without weight tuning we theoretically derive a single loss function with no hyperparameters for the hash code from the probability distribution of the images. By generating hash codes that minimize this loss function highly accurate image retrieval with probabilistic optimality is performed. We evaluate the performance of hashing using MNIST CIFAR-10 SVHN and show that the proposed method outperforms the state-of-the-art hashing methods.</p>\n", "tags": ["Image Retrieval", "Supervised"], "tsne_embedding": [-12.332656860351562, -15.673829078674316]}, {"key": "kalantidis2016loh", "year": "2016", "title": "LOH And Behold Web-scale Visual Search Recommendation And Clustering Using Locally Optimized Hashing", "abstract": "<p>We propose a novel hashing-based matching scheme called Locally Optimized Hashing (LOH) based on a state-of-the-art quantization algorithm that can be used for efficient large-scale search recommendation clustering and deduplication. We show that matching with LOH only requires set intersections and summations to compute and so is easily implemented in generic distributed computing systems. We further show application of LOH to a) large-scale search tasks where performance is on par with other state-of-the-art hashing approaches; b) large-scale recommendation where queries consisting of thousands of images can be used to generate accurate recommendations from collections of hundreds of millions of images; and c) efficient clustering with a graph-based algorithm that can be scaled to massive collections in a distributed environment or can be used for deduplication for small collections like search results performing better than traditional hashing approaches while only requiring a few milliseconds to run. In this paper we experiment on datasets of up to 100 million images but in practice our system can scale to larger collections and can be used for other types of data that have a vector representation in a Euclidean space.</p>\n", "tags": ["ARXIV", "Graph", "Quantisation", "Unsupervised"], "tsne_embedding": [-22.721940994262695, 5.406558036804199]}, {"key": "kanda2019b", "year": "2019", "title": "b-bit Sketch Trie Scalable Similarity Search On Integer Sketches", "abstract": "<p>Recently randomly mapping vectorial data to strings of discrete symbols (i.e. sketches) for fast and space-efficient similarity searches has become popular. Such random mapping is called similarity-preserving hashing and approximates a similarity metric by using the Hamming distance. Although many efficient similarity searches have been proposed most of them are designed for binary sketches. Similarity searches on integer sketches are in their infancy. In this paper we present a novel space-efficient trie named b-bit sketch trie on integer sketches for scalable similarity searches by leveraging the idea behind succinct data structures (i.e. space-efficient data structures while supporting various data operations in the compressed format) and a favorable property of integer sketches as fixed-length strings. Our experimental results obtained using real-world datasets show that a trie-based index is built from integer sketches and efficiently performs similarity searches on the index by pruning useless portions of the search space which greatly improves the search time and space-efficiency of the similarity search. The experimental results show that our similarity search is at most one order of magnitude faster than state-of-the-art similarity searches. Besides our method needs only 10 GiB of memory on a billion-scale database while state-of-the-art similarity searches need 29 GiB of memory.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-22.336172103881836, -0.20374412834644318]}, {"key": "kanda2019sketch", "year": "2019", "title": "(b)-bit Sketch Trie Scalable Similarity Search On Integer Sketches", "abstract": "<p>Recently randomly mapping vectorial data to strings of discrete symbols (i.e. sketches) for fast and space-efficient similarity searches has become popular. Such random mapping is called similarity-preserving hashing and approximates a similarity metric by using the Hamming distance. Although many efficient similarity searches have been proposed most of them are designed for binary sketches. Similarity searches on integer sketches are in their infancy. In this paper we present a novel space-efficient trie named (b)-bit sketch trie on integer sketches for scalable similarity searches by leveraging the idea behind succinct data structures (i.e. space-efficient data structures while supporting various data operations in the compressed format) and a favorable property of integer sketches as fixed-length strings. Our experimental results obtained using real-world datasets show that a trie-based index is built from integer sketches and efficiently performs similarity searches on the index by pruning useless portions of the search space which greatly improves the search time and space-efficiency of the similarity search. The experimental results show that our similarity search is at most one order of magnitude faster than state-of-the-art similarity searches. Besides our method needs only 10 GiB of memory on a billion-scale database while state-of-the-art similarity searches need 29 GiB of memory.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-22.359909057617188, -0.19860443472862244]}, {"key": "kanda2020dynamic", "year": "2020", "title": "Dynamic Similarity Search On Integer Sketches", "abstract": "<p>Similarity-preserving hashing is a core technique for fast similarity searches and it randomly maps data points in a metric space to strings of discrete symbols (i.e. sketches) in the Hamming space. While traditional hashing techniques produce binary sketches recent ones produce integer sketches for preserving various similarity measures. However most similarity search methods are designed for binary sketches and inefficient for integer sketches. Moreover most methods are either inapplicable or inefficient for dynamic datasets although modern real-world datasets are updated over time. We propose dynamic filter trie (DyFT) a dynamic similarity search method for both binary and integer sketches. An extensive experimental analysis using large real-world datasets shows that DyFT performs superiorly with respect to scalability time performance and memory efficiency. For example on a huge dataset of 216 million data points DyFT performs a similarity search 6000 times faster than a state-of-the-art method while reducing to one-thirteenth in memory.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-22.5582275390625, 0.16922497749328613]}, {"key": "kang2016columnsample", "year": "2016", "title": "Column Sampling Based Discrete Supervised Hashing", "abstract": "<p>By leveraging semantic (label) information, supervised hashing has demonstrated better accuracy than unsupervised hashing in many real applications. Because the hashing-code learning problem is essentially a discrete optimization problem which is hard to solve, most existing supervised hashing methods try to solve a relaxed continuous optimization problem by dropping the discrete constraints.\nHowever, these methods typically suffer from poor performance due to the errors caused by the relaxation. Some other methods try to directly solve the discrete optimization problem. However, they are typically time-consuming and unscalable. In this paper, we propose a novel method, called column sampling based discrete supervised hashing (COSDISH), to directly learn the discrete hashing code from semantic information.\nCOSDISH is an iterative method, in each iteration of which several columns are sampled from the semantic similarity matrix and then the hashing code is decomposed into two parts which can be alternately optimized in a discrete way. Theoretical analysis shows that the learning (optimization) algorithm of COSDISH has a constant-approximation bound in each step of the alternating optimization procedure. Empirical results on datasets with semantic labels illustrate that COSDISH can outperform the state-of-the-art methods in real applications like image retrieval.</p>\n", "tags": ["AAAI", "Supervised"], "tsne_embedding": [-0.5940015912055969, -3.5162320137023926]}, {"key": "kang2019maximum", "year": "2019", "title": "Maximum-Margin Hamming Hashing", "abstract": "<p>Deep hashing enables computation and memory efficient\nimage search through end-to-end learning of feature representations and binary codes. While linear scan over binary\nhash codes is more efficient than over the high-dimensional\nrepresentations, its linear-time complexity is still unacceptable for very large databases. Hamming space retrieval enables constant-time search through hash lookups, where for\neach query, there is a Hamming ball centered at the query\nand the data points within the ball are returned as relevant.\nSince inside the Hamming ball implies retrievable while\noutside irretrievable, it is crucial to explicitly characterize\nthe Hamming ball. The main idea of this work is to directly\nembody the Hamming radius into the loss functions, leading\nto Maximum-Margin Hamming Hashing (MMHH), a new\nmodel specifically optimized for Hamming space retrieval.\nWe introduce a max-margin t-distribution loss, where the\nt-distribution concentrates more similar data points to be\nwithin the Hamming ball, and the margin characterizes the\nHamming radius such that less penalization is applied to\nsimilar data points within the Hamming ball. The loss function also introduces robustness to data noise, where the similarity supervision may be inaccurate in practical problems.\nThe model is trained end-to-end using a new semi-batch optimization algorithm tailored to extremely imbalanced data.\nOur method yields state-of-the-art results on four datasets\nand shows superior performance on noisy data.</p>\n", "tags": ["ICCV"], "tsne_embedding": [-12.926876068115234, 7.341095924377441]}, {"key": "kang2023do", "year": "2023", "title": "Do Llms Understand User Preferences Evaluating Llms On User Rating Prediction", "abstract": "<p>Large Language Models (LLMs) have demonstrated exceptional capabilities in generalizing to new tasks in a zero-shot or few-shot manner. However the extent to which LLMs can comprehend user preferences based on their previous behavior remains an emerging and still unclear research question. Traditionally Collaborative Filtering (CF) has been the most effective method for these tasks predominantly relying on the extensive volume of rating data. In contrast LLMs typically demand considerably less data while maintaining an exhaustive world knowledge about each item such as movies or products. In this paper we conduct a thorough examination of both CF and LLMs within the classic task of user rating prediction which involves predicting a users rating for a candidate item based on their past ratings. We investigate various LLMs in different sizes ranging from 250M to 540B parameters and evaluate their performance in zero-shot few-shot and fine-tuning scenarios. We conduct comprehensive analysis to compare between LLMs and strong CF methods and find that zero-shot LLMs lag behind traditional recommender models that have the access to user interaction data indicating the importance of user interaction data. However through fine-tuning LLMs achieve comparable or even better performance with only a small fraction of the training data demonstrating their potential through data efficiency.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [44.699649810791016, -8.698136329650879]}, {"key": "kaplan2020locality", "year": "2020", "title": "Locality Sensitive Hashing For Set-queries Motivated By Group Recommendations", "abstract": "<p>Locality Sensitive Hashing (LSH) is an effective method to index a set of points such that we can efficiently find the nearest neighbors of a query point. We extend this method to our novel Set-query LSH (SLSH) such that it can find the nearest neighbors of a set of points given as a query. Let ( s(xy) ) be the similarity between two points ( x ) and ( y ). We define a similarity between a set ( Q) and a point ( x ) by aggregating the similarities ( s(px) ) for all ( pin Q ). For example we can take ( s(px) ) to be the angular similarity between ( p ) and ( x ) (i.e. 1-angle (xp)/pi) and aggregate by arithmetic or geometric averaging or taking the lowest similarity. We develop locality sensitive hash families and data structures for a large set of such arithmetic and geometric averaging similarities and analyze their collision probabilities. We also establish an analogous framework and hash families for distance functions. Specifically we give a structure for the euclidean distance aggregated by either averaging or taking the maximum. We leverage SLSH to solve a geometric extension of the approximate near neighbors problem. In this version we consider a metric for which the unit ball is an ellipsoid and its orientation is specified with the query. An important application that motivates our work is group recommendation systems. Such a system embeds movies and users in the same feature space and the task of recommending a movie for a group to watch together translates to a set-query ( Q ) using an appropriate similarity.</p>\n", "tags": ["ARXIV", "Independent", "LSH"], "tsne_embedding": [-35.390682220458984, 7.181558132171631]}, {"key": "kaplan2020scaling", "year": "2020", "title": "Scaling Laws For Neural Language Models", "abstract": "<p>We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size dataset size and the amount of compute used for training with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [19.413679122924805, -3.0383410453796387]}, {"key": "kapralov2024adversarial", "year": "2024", "title": "On The Adversarial Robustness Of Locality-sensitive Hashing In Hamming Space", "abstract": "<p>Locality-sensitive hashing~IndykMotwani98 is a classical data structure for approximate nearest neighbor search. It allows after a close to linear time preprocessing of the input dataset to find an approximately nearest neighbor of any fixed query in sublinear time in the dataset size. The resulting data structure is randomized and succeeds with high probability for every fixed query. In many modern applications of nearest neighbor search the queries are chosen adaptively. In this paper we study the robustness of the locality-sensitive hashing to adaptive queries in Hamming space. We present a simple adversary that can under mild assumptions on the initial point set provably find a query to the approximate near neighbor search data structure that the data structure fails on. Crucially our adaptive algorithm finds the hard query exponentially faster than random sampling.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-27.206796646118164, -3.95890736579895]}, {"key": "karaman2019unsupervised", "year": "2019", "title": "Unsupervised Rank-preserving Hashing For Large-scale Image Retrieval", "abstract": "<p>We propose an unsupervised hashing method which aims to produce binary codes that preserve the ranking induced by a real-valued representation. Such compact hash codes enable the complete elimination of real-valued feature storage and allow for significant reduction of the computation complexity and storage cost of large-scale image retrieval applications. Specifically we learn a neural network-based model which transforms the input representation into a binary representation. We formalize the training objective of the network in an intuitive and effective way considering each training sample as a query and aiming to obtain the same retrieval results using the produced hash codes as those obtained with the original features. This training formulation directly optimizes the hashing model for the target usage of the hash codes it produces. We further explore the addition of a decoder trained to obtain an approximated reconstruction of the original features. At test time we retrieved the most promising database samples with an efficient graph-based search procedure using only our hash codes and perform re-ranking using the reconstructed features thus without needing to access the original features at all. Experiments conducted on multiple publicly available large-scale datasets show that our method consistently outperforms all compared state-of-the-art unsupervised hashing methods and that the reconstruction procedure can effectively boost the search accuracy with a minimal constant additional cost.</p>\n", "tags": ["ARXIV", "Graph", "Image Retrieval", "Unsupervised"], "tsne_embedding": [0.9501253366470337, -1.867885947227478]}, {"key": "karjalainen2024fast", "year": "2024", "title": "Fast Redescription Mining Using Locality-sensitive Hashing", "abstract": "<p>Redescription mining is a data analysis technique that has found applications in diverse fields. The most used redescription mining approaches involve two phases finding matching pairs among data attributes and extending the pairs. This process is relatively efficient when the number of attributes remains limited and when the attributes are Boolean but becomes almost intractable when the data consist of many numerical attributes. In this paper we present new algorithms that perform the matching and extension orders of magnitude faster than the existing approaches. Our algorithms are based on locality-sensitive hashing with a tailored approach to handle the discretisation of numerical attributes as used in redescription mining.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-23.191965103149414, 1.565115213394165]}, {"key": "katz2022inferring", "year": "2022", "title": "Inferring Implicit Relations In Complex Questions With Language Models", "abstract": "<p>A prominent challenge for modern language understanding systems is the ability to answer implicit reasoning questions where the required reasoning steps for answering the question are not mentioned in the text explicitly. In this work we investigate why current models struggle with implicit reasoning question answering (QA) tasks by decoupling inference of reasoning steps from their execution. We define a new task of implicit relation inference and construct a benchmark IMPLICITRELATIONS where given a question a model should output a list of concept-relation pairs where the relations describe the implicit reasoning steps required for answering the question. Using IMPLICITRELATIONS we evaluate models from the GPT-3 family and find that while these models struggle on the implicit reasoning QA task they often succeed at inferring implicit relations. This suggests that the challenge in implicit reasoning questions does not stem from the need to plan a reasoning strategy alone but to do it while also retrieving and reasoning over relevant information.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [31.458759307861328, 4.1531853675842285]}, {"key": "kehl2016hashmod", "year": "2016", "title": "Hashmod A Hashing Method For Scalable 3D Object Detection", "abstract": "<p>We present a scalable method for detecting objects and estimating their 3D poses in RGB-D data. To this end we rely on an efficient representation of object views and employ hashing techniques to match these views against the input frame in a scalable way. While a similar approach already exists for 2D detection we show how to extend it to estimate the 3D pose of the detected objects. In particular we explore different hashing strategies and identify the one which is more suitable to our problem. We show empirically that the complexity of our method is sublinear with the number of objects and we enable detection and pose estimation of many 3D objects with high accuracy while outperforming the state-of-the-art in terms of runtime.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-7.978851318359375, 13.87458610534668]}, {"key": "kelly2019lock", "year": "2019", "title": "Lock-free Hopscotch Hashing", "abstract": "<p>In this paper we present a lock-free version of Hopscotch Hashing. Hopscotch Hashing is an open addressing algorithm originally proposed by Herlihy Shavit and Tzafrir which is known for fast performance and excellent cache locality. The algorithm allows users of the table to skip or jump over irrelevant entries allowing quick search insertion and removal of entries. Unlike traditional linear probing Hopscotch Hashing is capable of operating under a high load factor as probe counts remain small. Our lock-free version improves on both speed cache locality and progress guarantees of the original being a chimera of two concurrent hash tables. We compare our data structure to various other lock-free and blocking hashing algorithms and show that its performance is in many cases superior to existing strategies. The proposed lock-free version overcomes some of the drawbacks associated with the original blocking version leading to a substantial boost in scalability while maintaining attractive features like physical deletion or probe-chain compression.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-25.432371139526367, -13.49195671081543]}, {"key": "kennedy2016fast", "year": "2016", "title": "Fast Cross-polytope Locality-sensitive Hashing", "abstract": "<p>We provide a variant of cross-polytope locality sensitive hashing with respect to angular distance which is provably optimal in asymptotic sensitivity and enjoys (mathcalO)(d (ln) d ) hash computation time. Building on a recent result (by Andoni Indyk Laarhoven Razenshteyn Schmidt 2015) we show that optimal asymptotic sensitivity for cross-polytope LSH is retained even when the dense Gaussian matrix is replaced by a fast Johnson-Lindenstrauss transform followed by discrete pseudo-rotation reducing the hash computation time from (mathcalO)(d^2) to (mathcalO)(d (ln) d ). Moreover our scheme achieves the optimal rate of convergence for sensitivity. By incorporating a low-randomness Johnson-Lindenstrauss transform our scheme can be modified to require only (mathcalO)((ln)^9(d)) random bits</p>\n", "tags": ["ARXIV", "Independent", "LSH"], "tsne_embedding": [-34.91881561279297, 2.6760778427124023]}, {"key": "kim2019nearest", "year": "2019", "title": "Nearest Neighbor Search-based Bitwise Source Separation Using Discriminant Winner-take-all Hashing", "abstract": "<p>We propose an iteration-free source separation algorithm based on Winner-Take-All (WTA) hash codes which is a faster yet accurate alternative to a complex machine learning model for single-channel source separation in a resource-constrained environment. We first generate random permutations with WTA hashing to encode the shape of the multidimensional audio spectrum to a reduced bitstring representation. A nearest neighbor search on the hash codes of an incoming noisy spectrum as the query string results in the closest matches among the hashed mixture spectra. Using the indices of the matching frames we obtain the corresponding ideal binary mask vectors for denoising. Since both the training data and the search operation are bitwise the procedure can be done efficiently in hardware implementations. Experimental results show that the WTA hash codes are discriminant and provide an affordable dictionary search mechanism that leads to a competent performance compared to a comprehensive model and oracle masking.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [-18.718570709228516, -10.489322662353516]}, {"key": "kim2021what", "year": "2021", "title": "What Changes Can Large-scale Language Models Bring Intensive Study On Hyperclova Billions-scale Korean Generative Pretrained Transformers", "abstract": "<p>GPT-3 shows remarkable in-context learning ability of large-scale language models (LMs) trained on hundreds of billion scale data. Here we address some remaining issues less reported by the GPT-3 paper such as a non-English LM the performances of different sized models and the effect of recently introduced prompt optimization on in-context learning. To achieve this we introduce HyperCLOVA a Korean variant of 82B GPT-3 trained on a Korean-centric corpus of 560B tokens. Enhanced by our Korean-specific tokenization HyperCLOVA with our training configuration shows state-of-the-art in-context zero-shot and few-shot learning performances on various downstream tasks in Korean. Also we show the performance benefits of prompt-based learning and demonstrate how it can be integrated into the prompt engineering pipeline. Then we discuss the possibility of materializing the No Code AI paradigm by providing AI prototyping capabilities to non-experts of ML by introducing HyperCLOVA studio an interactive prompt engineering interface. Lastly we demonstrate the potential of our methods with three successful in-house applications.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [24.200637817382812, -8.692237854003906]}, {"key": "kim2023cot", "year": "2023", "title": "The Cot Collection Improving Zero-shot And Few-shot Learning Of Language Models Via Chain-of-thought Fine-tuning", "abstract": "<p>Language models (LMs) with less than 100B parameters are known to perform poorly on chain-of-thought (CoT) reasoning in contrast to large LMs when solving unseen tasks. In this work we aim to equip smaller LMs with the step-by-step reasoning capability by instruction tuning with CoT rationales. In order to achieve this goal we first introduce a new instruction-tuning dataset called the CoT Collection which augments the existing Flan Collection (including only 9 CoT tasks) with additional 1.84 million rationales across 1060 tasks. We show that CoT fine-tuning Flan-T5 (3B amp; 11B) with CoT Collection enables smaller LMs to have better CoT capabilities on unseen tasks. On the BIG-Bench-Hard (BBH) benchmark we report an average improvement of +4.3437; (Flan-T5 3B) and +2.6037; (Flan-T5 11B) in terms of zero-shot task accuracy. Furthermore we show that instruction tuning with CoT Collection allows LMs to possess stronger few-shot learning capabilities on 4 domain-specific tasks resulting in an improvement of +2.2437; (Flan-T5 3B) and +2.3737; (Flan-T5 11B) even outperforming ChatGPT utilizing demonstrations until the max length by a +13.9837; margin. Our code the CoT Collection data and model checkpoints are publicly available.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [23.8282470703125, 1.473099946975708]}, {"key": "kim2024code", "year": "2024", "title": "CODE Contrasting Self-generated Description To Combat Hallucination In Large Multi-modal Models", "abstract": "<p>Large Multi-modal Models (LMMs) have recently demonstrated remarkable abilities in visual context understanding and coherent response generation. However alongside these advancements the issue of hallucinations has emerged as a significant challenge producing erroneous responses that are unrelated to the visual contents. In this paper we introduce a novel contrastive-based decoding method COuntering DEscription Contrastive Decoding (CODE) which leverages self-generated descriptions as contrasting references during the decoding phase of LMMs to address hallucination issues. CODE utilizes the comprehensive descriptions from model itself as visual counterpart to correct and improve response alignment with actual visual content. By dynamically adjusting the information flow and distribution of next-token predictions in the LMMs vocabulary CODE enhances the coherence and informativeness of generated responses. Extensive experiments demonstrate that our method significantly reduces hallucinations and improves cross-modal consistency across various benchmarks and cutting-edge LMMs. Our method provides a simple yet effective decoding strategy that can be integrated to existing LMM frameworks without additional training.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [49.51298522949219, 5.7151594161987305]}, {"key": "kim2024what", "year": "2024", "title": "What If... Thinking Counterfactual Keywords Helps To Mitigate Hallucination In Large Multi-modal Models", "abstract": "<p>This paper presents a way of enhancing the reliability of Large Multi-modal Models (LMMs) in addressing hallucination where the models generate cross-modal inconsistent responses. Without additional training we propose Counterfactual Inception a novel method that implants counterfactual thinking into LMMs using self-generated counterfactual keywords. Our method is grounded in the concept of counterfactual thinking a cognitive process where human considers alternative realities enabling more extensive context exploration. Bridging the human cognition mechanism into LMMs we aim for the models to engage with and generate responses that span a wider contextual scene understanding mitigating hallucinatory outputs. We further introduce Plausibility Verification Process (PVP) a simple yet robust keyword constraint that effectively filters out sub-optimal keywords to enable the consistent triggering of counterfactual thinking in the model responses. Comprehensive analyses across various LMMs including both open-source and proprietary models corroborate that counterfactual thinking significantly reduces hallucination and helps to broaden contextual understanding based on true visual clues.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [50.4286994934082, 5.447082996368408]}, {"key": "klassen2011independence", "year": "2011", "title": "Independence Of Tabulation-based Hash Classes", "abstract": "<p>A tabulation-based hash function maps a key into d derived characters indexing random values in tables that are then combined with bitwise xor operations to give the hash. Thorup and Zhang (2004) presented d-wise independent tabulation-based hash classes that use linear maps over finite fields to map a key considered as a vector (ab) to derived characters. We show that a variant where the derived characters are a+bi for i=0\u2026 q-1 (using integer arithmetic) yielding (2d-1)-wise independence. Our analysis is based on an algebraic property that characterizes k-wise independence of tabulation-based hashing schemes and combines this characterization with a geometric argument. We also prove a non-trivial lower bound on the number of derived characters necessary for k-wise independence with our and related hash classes.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-36.27952194213867, 3.2021656036376953]}, {"key": "klein2017end", "year": "2017", "title": "End-to-end Supervised Product Quantization For Image Search And Retrieval", "abstract": "<p>Product Quantization a dictionary based hashing method is one of the leading unsupervised hashing techniques. While it ignores the labels it harnesses the features to construct look up tables that can approximate the feature space. In recent years several works have achieved state of the art results on hashing benchmarks by learning binary representations in a supervised manner. This work presents Deep Product Quantization (DPQ) a technique that leads to more accurate retrieval and classification than the latest state of the art methods while having similar computational complexity and memory footprint as the Product Quantization method. To our knowledge this is the first work to introduce a dictionary-based representation that is inspired by Product Quantization and which is learned end-to-end and thus benefits from the supervised signal. DPQ explicitly learns soft and hard representations to enable an efficient and accurate asymmetric search by using a straight-through estimator. Our method obtains state of the art results on an extensive array of retrieval and classification experiments.</p>\n", "tags": ["ARXIV", "Quantisation", "Supervised"], "tsne_embedding": [1.3884223699569702, -4.358623504638672]}, {"key": "knudsen2015quicksort", "year": "2015", "title": "Quicksort Largest Bucket And Min-wise Hashing With Limited Independence", "abstract": "<p>Randomized algorithms and data structures are often analyzed under the assumption of access to a perfect source of randomness. The most fundamental metric used to measure how random a hash function or a random number generator is is its independence a sequence of random variables is said to be k-independent if every variable is uniform and every size k subset is independent. In this paper we consider three classic algorithms under limited independence. We provide new bounds for randomized quicksort min-wise hashing and largest bucket size under limited independence. Our results can be summarized as follows. -Randomized quicksort. When pivot elements are computed using a 5-independent hash function Karloff and Raghavan J.ACM93 showed O ( n (log) n) expected worst-case running time for a special version of quicksort. We improve upon this showing that the same running time is achieved with only 4-independence. -Min-wise hashing. For a set A consider the probability of a particular element being mapped to the smallest hash value. It is known that 5-independence implies the optimal probability O (1 /n). Broder et al. STOC98 showed that 2-independence implies it is O(1 / (sqrt)A). We show a matching lower bound as well as new tight bounds for 3- and 4-independent hash functions. -Largest bucket. We consider the case where n balls are distributed to n buckets using a k-independent hash function and analyze the largest bucket size. Alon et. al STOC97 showed that there exists a 2-independent hash function implying a bucket of size (Omega) ( n^1/2). We generalize the bound providing a k-independent family of functions that imply size (Omega) ( n^1/k).</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-33.48805236816406, -7.768794536590576]}, {"key": "kojima2022large", "year": "2022", "title": "Large Language Models Are Zero-shot Reasoners", "abstract": "<p>Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably chain of thought (CoT) prompting a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples achieved the state-of-the-art performances in arithmetics and symbolic reasoning difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs ability for few-shot learning we show that LLMs are decent zero-shot reasoners by simply adding Lets think step by step before each answer. Experimental results demonstrate that our Zero-shot-CoT using the same single prompt template significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith GSM8K AQUA-RAT SVAMP) symbolic reasoning (Last Letter Coin Flip) and other logical reasoning tasks (Date Understanding Tracking Shuffled Objects) without any hand-crafted few-shot examples e.g. increasing the accuracy on MultiArith from 17.737; to 78.737; and GSM8K from 10.437; to 40.737; with large InstructGPT model (text-davinci-002) as well as similar magnitudes of improvements with another off-the-shelf large model 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs suggesting high-level multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [28.95890998840332, 1.8183562755584717]}, {"key": "komorowski2017evaluation", "year": "2017", "title": "Evaluation Of Hashing Methods Performance On Binary Feature Descriptors", "abstract": "<p>In this paper we evaluate performance of data-dependent hashing methods on binary data. The goal is to find a hashing method that can effectively produce lower dimensional binary representation of 512-bit FREAK descriptors. A representative sample of recent unsupervised semi-supervised and supervised hashing methods was experimentally evaluated on large datasets of labelled binary FREAK feature descriptors.</p>\n", "tags": ["ARXIV", "Unsupervised"], "tsne_embedding": [-12.164249420166016, -22.239713668823242]}, {"key": "komorowski2017random", "year": "2017", "title": "Random Binary Trees For Approximate Nearest Neighbour Search In Binary Space", "abstract": "<p>Approximate nearest neighbour (ANN) search is one of the most important problems in computer science fields such as data mining or computer vision. In this paper we focus on ANN for high-dimensional binary vectors and we propose a simple yet powerful search method that uses Random Binary Search Trees (RBST). We apply our method to a dataset of 1.25M binary local feature descriptors obtained from a real-life image-based localisation system provided by Google as a part of Project Tango. An extensive evaluation of our method against the state-of-the-art variations of Locality Sensitive Hashing (LSH) namely Uniform LSH and Multi-probe LSH shows the superiority of our method in terms of retrieval precision with performance boost of over 2037;</p>\n", "tags": ["ARXIV", "Independent", "LSH"], "tsne_embedding": [-29.695945739746094, 1.6557077169418335]}, {"key": "kong2012ausing", "year": "2012", "title": "Manhattan Hashing for Large-Scale Image Retrieval", "abstract": "<p>Hashing is used to learn binary-code representation for data with\nexpectation of preserving the neighborhood structure in the original\nfeature space. Due to its fast query speed and reduced storage\ncost, hashing has been widely used for efficient nearest neighbor\nsearch in a large variety of applications like text and image retrieval.\nMost existing hashing methods adopt Hamming distance to\nmeasure the similarity (neighborhood) between points in the hashcode\nspace. However, one problem with Hamming distance is that\nit may destroy the neighborhood structure in the original feature\nspace, which violates the essential goal of hashing. In this paper,\nManhattan hashing (MH), which is based on Manhattan distance, is\nproposed to solve the problem of Hamming distance based hashing.\nThe basic idea of MH is to encode each projected dimension with\nmultiple bits of natural binary code (NBC), based on which the\nManhattan distance between points in the hashcode space is calculated\nfor nearest neighbor search. MH can effectively preserve the\nneighborhood structure in the data to achieve the goal of hashing.\nTo the best of our knowledge, this is the first work to adopt Manhattan\ndistance with NBC for hashing. Experiments on several largescale\nimage data sets containing up to one million points show that\nour MH method can significantly outperform other state-of-the-art\nmethods.</p>\n", "tags": ["Image Retrieval", "Quantisation", "SIGIR"], "tsne_embedding": [-31.90385627746582, 11.560529708862305]}, {"key": "kong2012busing", "year": "2012", "title": "Double-Bit Quantisation for Hashing", "abstract": "<p>Hashing, which tries to learn similarity-preserving binary\ncodes for data representation, has been widely\nused for efficient nearest neighbor search in massive\ndatabases due to its fast query speed and low storage\ncost. Because it is NP hard to directly compute the best\nbinary codes for a given data set, mainstream hashing\nmethods typically adopt a two-stage strategy. In the\nfirst stage, several projected dimensions of real values\nare generated. Then in the second stage, the real values\nwill be quantized into binary codes by thresholding.\nCurrently, most existing methods use one single bit to\nquantize each projected dimension. One problem with\nthis single-bit quantization (SBQ) is that the threshold\ntypically lies in the region of the highest point density\nand consequently a lot of neighboring points close to\nthe threshold will be hashed to totally different bits,\nwhich is unexpected according to the principle of hashing.\nIn this paper, we propose a novel quantization strategy,\ncalled double-bit quantization (DBQ), to solve the\nproblem of SBQ. The basic idea of DBQ is to quantize\neach projected dimension into double bits with adaptively\nlearned thresholds. Extensive experiments on two\nreal data sets show that our DBQ strategy can signifi-\ncantly outperform traditional SBQ strategy for hashing.</p>\n", "tags": [], "tsne_embedding": [-21.659894943237305, -5.888819694519043]}, {"key": "kong2012cusing", "year": "2012", "title": "Isotropic Hashing", "abstract": "<p>Most existing hashing methods adopt some projection functions to project the original data into several dimensions of real values, and then each of these projected dimensions is quantized into one bit (zero or one) by thresholding. Typically, the variances of different projected dimensions are different for existing projection functions such as principal component analysis (PCA). Using the same number of bits for different projected dimensions is unreasonable because larger-variance dimensions will carry more information. Although this viewpoint has been widely accepted by many researchers, it is still not verified by either theory or experiment because no methods have been proposed to find a projection with equal variances for different dimensions. In this paper, we propose a novel method, called isotropic hashing (IsoHash), to learn projection functions which can produce projected dimensions with isotropic variances (equal variances). Experimental results on real data sets show that IsoHash can outperform its counterpart with different variances for different dimensions, which verifies the viewpoint that projections with isotropic variances will be better than those with anisotropic variances.</p>\n", "tags": [], "tsne_embedding": [-37.484588623046875, -9.6079740524292]}, {"key": "kong2012isotropic", "year": "2012", "title": "Isotropic Hashing", "abstract": "<p>Most existing hashing methods adopt some projection functions to project the original data into several dimensions of real values and then each of these projected dimensions is quantized into one bit (zero or one) by thresholding. Typically the variances of different projected dimensions are different for existing projection functions such as principal component analysis (PCA). Using the same number of bits for different projected dimensions is unreasonable because larger-variance dimensions will carry more information. Although this viewpoint has been widely accepted by many researchers it is still not verified by either theory or experiment because no methods have been proposed to find a projection with equal variances for different dimensions. In this paper we propose a novel method called isotropic hashing (IsoHash) to learn projection functions which can produce projected dimensions with isotropic variances (equal variances). Experimental results on real data sets show that IsoHash can outperform its counterpart with different variances for different dimensions which verifies the viewpoint that projections with isotropic variances will be better than those with anisotropic variances.</p>\n", "tags": ["NEURIPS", "Unsupervised"], "tsne_embedding": [-37.484928131103516, -9.608067512512207]}, {"key": "konoshima2012hyperplane", "year": "2012", "title": "Hyperplane Arrangements And Locality-sensitive Hashing With Lift", "abstract": "<p>Locality-sensitive hashing converts high-dimensional feature vectors such as image and speech into bit arrays and allows high-speed similarity calculation with the Hamming distance. There is a hashing scheme that maps feature vectors to bit arrays depending on the signs of the inner products between feature vectors and the normal vectors of hyperplanes placed in the feature space. This hashing can be seen as a discretization of the feature space by hyperplanes. If labels for data are given one can determine the hyperplanes by using learning algorithms. However many proposed learning methods do not consider the hyperplanes offsets. Not doing so decreases the number of partitioned regions and the correlation between Hamming distances and Euclidean distances becomes small. In this paper we propose a lift map that converts learning algorithms without the offsets to the ones that take into account the offsets. With this method the learning methods without the offsets give the discretizations of spaces as if it takes into account the offsets. For the proposed method we input several high-dimensional feature data sets and studied the relationship between the statistical characteristics of data the number of hyperplanes and the effect of the proposed method.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-29.153671264648438, 7.446586608886719]}, {"key": "konoshima2012locality", "year": "2012", "title": "Locality-sensitive Hashing With Margin Based Feature Selection", "abstract": "<p>We propose a learning method with feature selection for Locality-Sensitive Hashing. Locality-Sensitive Hashing converts feature vectors into bit arrays. These bit arrays can be used to perform similarity searches and personal authentication. The proposed method uses bit arrays longer than those used in the end for similarity and other searches and by learning selects the bits that will be used. We demonstrated this method can effectively perform optimization for cases such as fingerprint images with a large number of labels and extremely few data that share the same labels as well as verifying that it is also effective for natural images handwritten digits and speech features.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-13.559525489807129, -11.32705307006836]}, {"key": "korfhage2023elastichash", "year": "2023", "title": "Elastichash Semantic Image Similarity Search By Deep Hashing With Elasticsearch", "abstract": "<p>We present ElasticHash a novel approach for high-quality efficient and large-scale semantic image similarity search. It is based on a deep hashing model to learn hash codes for fine-grained image similarity search in natural images and a two-stage method for efficiently searching binary hash codes using Elasticsearch (ES). In the first stage a coarse search based on short hash codes is performed using multi-index hashing and ES terms lookup of neighboring hash codes. In the second stage the list of results is re-ranked by computing the Hamming distance on long hash codes. We evaluate the retrieval performance of textitElasticHash for more than 120000 query images on about 6.9 million database images of the OpenImages data set. The results show that our approach achieves high-quality retrieval results and low search latencies.</p>\n", "tags": ["AAAI", "Independent"], "tsne_embedding": [-17.632619857788086, 21.21856689453125]}, {"key": "koutaki2016fast", "year": "2016", "title": "Fast Supervised Discrete Hashing And Its Analysis", "abstract": "<p>In this paper we propose a learning-based supervised discrete hashing method. Binary hashing is widely used for large-scale image retrieval as well as video and document searches because the compact representation of binary code is essential for data storage and reasonable for query searches using bit-operations. The recently proposed Supervised Discrete Hashing (SDH) efficiently solves mixed-integer programming problems by alternating optimization and the Discrete Cyclic Coordinate descent (DCC) method. We show that the SDH model can be simplified without performance degradation based on some preliminary experiments; we call the approximate model for this the Fast SDH (FSDH) model. We analyze the FSDH model and provide a mathematically exact solution for it. In contrast to SDH our model does not require an alternating optimization algorithm and does not depend on initial values. FSDH is also easier to implement than Iterative Quantization (ITQ). Experimental results involving a large-scale database showed that FSDH outperforms conventional SDH in terms of precision recall and computation time.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Quantisation", "Supervised"], "tsne_embedding": [-1.2321516275405884, -5.546276092529297]}, {"key": "krishna2019video", "year": "2019", "title": "Video Segment Copy Detection Using Memory Constrained Hierarchical Batch-normalized LSTM Autoencoder", "abstract": "<p>In this report we introduce a video hashing method for scalable video segment copy detection. The objective of video segment copy detection is to find the video (s) present in a large database one of whose segments (cropped in time) is a (transformed) copy of the given query video. This transformation may be temporal (for example frame dropping change in frame rate) or spatial (brightness and contrast change addition of noise etc.) in nature although the primary focus of this report is detecting temporal attacks. The video hashing method proposed by us uses a deep learning neural network to learn variable length binary hash codes for the entire video considering both temporal and spatial features into account. This is in contrast to most existing video hashing methods as they use conventional image hashing techniques to obtain hash codes for a video after extracting features for every frame or certain key frames in which case the temporal information present in the video is not exploited. Our hashing method is specifically resilient to time cropping making it extremely useful in video segment copy detection. Experimental results obtained on the large augmented dataset consisting of around 25000 videos with segment copies demonstrate the efficacy of our proposed video hashing method.</p>\n", "tags": ["ARXIV", "Deep Learning", "Unsupervised"], "tsne_embedding": [-7.38114595413208, -25.9860897064209]}, {"key": "kulis2009kernelized", "year": "2009", "title": "Kernelized Locality-Sensitive Hashing for Scalable Image Search", "abstract": "<p>Fast retrieval methods are critical for large-scale and\ndata-driven vision applications. Recent work has explored\nways to embed high-dimensional features or complex distance\nfunctions into a low-dimensional Hamming space\nwhere items can be efficiently searched. However, existing\nmethods do not apply for high-dimensional kernelized\ndata when the underlying feature embedding for the kernel\nis unknown. We show how to generalize locality-sensitive\nhashing to accommodate arbitrary kernel functions, making\nit possible to preserve the algorithm\u2019s sub-linear time similarity\nsearch guarantees for a wide class of useful similarity\nfunctions. Since a number of successful image-based kernels\nhave unknown or incomputable embeddings, this is especially\nvaluable for image retrieval tasks. We validate our\ntechnique on several large-scale datasets, and show that it\nenables accurate and fast performance for example-based\nobject classification, feature matching, and content-based\nretrieval.</p>\n", "tags": ["Has Code", "ICCV", "Image Retrieval"], "tsne_embedding": [-15.787837028503418, 14.998811721801758]}, {"key": "kulis2009learning", "year": "2009", "title": "Learning To Hash With Binary Reconstructive Embeddings", "abstract": "<p>Fast retrieval methods are increasingly critical for many large-scale analysis tasks and there have been several recent methods that attempt to learn hash functions for fast and accurate nearest neighbor searches. In this paper we develop an algorithm for learning hash functions based on explicitly minimizing the reconstruction error between the original distances and the Hamming distances of the corresponding binary embeddings. We develop a scalable coordinate-descent algorithm for our proposed hashing objective that is able to efficiently learn hash functions in a variety of settings. Unlike existing methods such as semantic hashing and spectral hashing our method is easily kernelized and does not require restrictive assumptions about the underlying distribution of the data. We present results over several domains to demonstrate that our method outperforms existing state-of-the-art techniques.</p>\n", "tags": ["Independent", "NEURIPS"], "tsne_embedding": [-20.918142318725586, -12.997133255004883]}, {"key": "kumar2011learning", "year": "2011", "title": "Learning hash functions for cross-view similarity search", "abstract": "<p>Many applications in Multilingual and Multimodal\nInformation Access involve searching large\ndatabases of high dimensional data objects with\nmultiple (conditionally independent) views. In this\nwork we consider the problem of learning hash\nfunctions for similarity search across the views\nfor such applications. We propose a principled\nmethod for learning a hash function for each view\ngiven a set of multiview training data objects. The\nhash functions map similar objects to similar codes\nacross the views thus enabling cross-view similarity\nsearch. We present results from an extensive\nempirical study of the proposed approach\nwhich demonstrate its effectiveness on Japanese\nlanguage People Search and Multilingual People\nSearch problems.</p>\n", "tags": ["Cross Modal", "IJCAI", "Independent"], "tsne_embedding": [-17.0788516998291, 0.6334589123725891]}, {"key": "kurpicz2022pachash", "year": "2022", "title": "Pachash Packed And Compressed Hash Tables", "abstract": "<p>We introduce PaCHash a hash table that stores its objects contiguously in an array without intervening space even if the objects have variable size. In particular each object can be compressed using standard compression techniques. A small search data structure allows locating the objects in constant expected time. PaCHash is most naturally described as a static external hash table where it needs a constant number of bits of internal memory per block of external memory. Here in some sense PaCHash beats a lower bound on the space consumption of k-perfect hashing. An implementation for fast SSDs needs about 5 bits of internal memory per block of external memory requires only one disk access (of variable length) per search operation and has small internal search overhead compared to the disk access cost. Our experiments show that it has lower space consumption than all previous approaches even when considering objects of identical size.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-30.70370101928711, -18.05188751220703]}, {"key": "kusupati2021accurate", "year": "2021", "title": "LLC: Accurate, Multi-purpose Learnt Low-dimensional Binary Codes", "abstract": "<p>Learning binary representations of instances and classes is a classical problem with several high potential applications. In modern settings, the compression of high-dimensional \nneural representations to low-dimensional binary codes is a challenging task and often require large bit-codes to be accurate. In this work, we propose a novel method for \nLearning Low-dimensional binary Codes (LLC) for instances as well as classes. Our method does not require any side-information, like annotated attributes or label meta-data, \nand learns extremely low-dimensional binary codes (~20 bits for ImageNet-1K). The learnt codes are super-efficient while still ensuring nearly optimal classification accuracy for \nResNet50 on ImageNet-1K. We demonstrate that the learnt codes capture intrinsically important features in the data, by discovering an intuitive taxonomy over classes. We further \nquantitatively measure the quality of our codes by applying it to the efficient image retrieval as well as out-of-distribution (OOD) detection problems. For ImageNet-100 \nretrieval problem, our learnt binary codes outperform 16 bit HashNet using only 10 bits and also are as accurate as 10 dimensional real representations. Finally, our learnt \nbinary codes can perform OOD detection, out-of-the-box, as accurately as a baseline that needs ~3000 samples to tune its threshold, while we require none.</p>\n", "tags": ["Deep Learning", "Has Code", "NEURIPS"], "tsne_embedding": [3.113462448120117, 1.035957932472229]}, {"key": "labelme2007labelme", "year": "2007", "title": "LabelMe: a database and web-based tool for image annotation", "abstract": "<p>We seek to build a large collection of images with ground truth labels to be used for object\ndetection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation\nand instant sharing of such annotations. Using this annotation tool, we have collected a large\ndataset that spans many object categories, often containing multiple instances over a wide variety\nof images. We quantify the contents of the dataset and compare against existing state of the\nart datasets used for object recognition and detection. Also, we show how to extend the dataset\nto automatically enhance object labels with WordNet, discover object parts, recover a depth ordering\nof objects in a scene, and increase the number of labels using minimal user supervision\nand images from the web.</p>\n", "tags": ["Dataset", "Supervised"], "tsne_embedding": [8.774563789367676, 20.31416130065918]}, {"key": "lai2015simultaneous", "year": "2015", "title": "Simultaneous Feature Learning And Hash Coding With Deep Neural Networks", "abstract": "<p>Similarity-preserving hashing is a widely-used method for nearest neighbour search in large-scale image retrieval tasks. For most existing hashing methods an image is first encoded as a vector of hand-engineering visual features followed by another separate projection or quantization step that generates binary codes. However such visual feature vectors may not be optimally compatible with the coding process thus producing sub-optimal hashing codes. In this paper we propose a deep architecture for supervised hashing in which images are mapped into binary codes via carefully designed deep neural networks. The pipeline of the proposed deep architecture consists of three building blocks 1) a sub-network with a stack of convolution layers to produce the effective intermediate image features; 2) a divide-and-encode module to divide the intermediate image features into multiple branches each encoded into one hash bit; and 3) a triplet ranking loss designed to characterize that one image is more similar to the second image than to the third one. Extensive evaluations on several benchmark image datasets show that the proposed simultaneous feature learning and hash coding pipeline brings substantial improvements over other state-of-the-art supervised or unsupervised hashing methods.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Quantisation", "Unsupervised"], "tsne_embedding": [-10.526681900024414, 9.282689094543457]}, {"key": "lai2016instance", "year": "2016", "title": "Instance-aware Hashing For Multi-label Image Retrieval", "abstract": "<p>Similarity-preserving hashing is a commonly used method for nearest neighbour search in large-scale image retrieval. For image retrieval deep-networks-based hashing methods are appealing since they can simultaneously learn effective image representations and compact hash codes. This paper focuses on deep-networks-based hashing for multi-label images each of which may contain objects of multiple categories. In most existing hashing methods each image is represented by one piece of hash code which is referred to as semantic hashing. This setting may be suboptimal for multi-label image retrieval. To solve this problem we propose a deep architecture that learns textbfinstance-aware image representations for multi-label image data which are organized in multiple groups with each group containing the features for one category. The instance-aware representations not only bring advantages to semantic hashing but also can be used in category-aware hashing in which an image is represented by multiple pieces of hash codes and each piece of code corresponds to a category. Extensive evaluations conducted on several benchmark datasets demonstrate that for both semantic hashing and category-aware hashing the proposed method shows substantial improvement over the state-of-the-art supervised and unsupervised hashing methods.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Unsupervised"], "tsne_embedding": [-11.009315490722656, 9.391161918640137]}, {"key": "lai2017improved", "year": "2017", "title": "Improved Search In Hamming Space Using Deep Multi-index Hashing", "abstract": "<p>Similarity-preserving hashing is a widely-used method for nearest neighbour search in large-scale image retrieval tasks. There has been considerable research on generating efficient image representation via the deep-network-based hashing methods. However the issue of efficient searching in the deep representation space remains largely unsolved. To this end we propose a simple yet efficient deep-network-based multi-index hashing method for simultaneously learning the powerful image representation and the efficient searching. To achieve these two goals we introduce the multi-index hashing (MIH) mechanism into the proposed deep architecture which divides the binary codes into multiple substrings. Due to the non-uniformly distributed codes will result in inefficiency searching we add the two balanced constraints at feature-level and instance-level respectively. Extensive evaluations on several benchmark image retrieval datasets show that the learned balanced binary codes bring dramatic speedups and achieve comparable performance over the existing baselines.</p>\n", "tags": ["ARXIV", "Image Retrieval"], "tsne_embedding": [-12.287872314453125, 10.924907684326172]}, {"key": "lai2017transductive", "year": "2017", "title": "Transductive Zero-shot Hashing Via Coarse-to-fine Similarity Mining", "abstract": "<p>Zero-shot Hashing (ZSH) is to learn hashing models for novel/target classes without training data which is an important and challenging problem. Most existing ZSH approaches exploit transfer learning via an intermediate shared semantic representations between the seen/source classes and novel/target classes. However due to having disjoint the hash functions learned from the source dataset are biased when applied directly to the target classes. In this paper we study the transductive ZSH i.e. we have unlabeled data for novel classes. We put forward a simple yet efficient joint learning approach via coarse-to-fine similarity mining which transfers knowledges from source data to target data. It mainly consists of two building blocks in the proposed deep architecture 1) a shared two-streams network which the first stream operates on the source data and the second stream operates on the unlabeled data to learn the effective common image representations and 2) a coarse-to-fine module which begins with finding the most representative images from target classes and then further detect similarities among these images to transfer the similarities of the source data to the target data in a greedy fashion. Extensive evaluation results on several benchmark datasets demonstrate that the proposed hashing method achieves significant improvement over the state-of-the-art methods.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [-3.3952605724334717, 4.1163177490234375]}, {"key": "lamping2014fast", "year": "2014", "title": "A Fast Minimal Memory Consistent Hash Algorithm", "abstract": "<p>We present jump consistent hash a fast minimal memory consistent hash algorithm that can be expressed in about 5 lines of code. In comparison to the algorithm of Karger et al. jump consistent hash requires no storage is faster and does a better job of evenly dividing the key space among the buckets and of evenly dividing the workload when the number of buckets changes. Its main limitation is that the buckets must be numbered sequentially which makes it more suitable for data storage applications than for distributed web caching.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-30.932086944580078, -18.08782386779785]}, {"key": "lecroq2023optimal", "year": "2023", "title": "Optimal-hash Exact String Matching Algorithms", "abstract": "<p>String matching is the problem of finding all the occurrences of a pattern in a text. We propose improved versions of the fast family of string matching algorithms based on hashing (q)-grams. The improvement consists of considering minimal values (q) such that each (q)-grams of the pattern has a unique hash value. The new algorithms are fastest than algorithm of the HASH family for short patterns on large size alphabets.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-18.452835083007812, -15.149312019348145]}, {"key": "lee2011similarity", "year": "2011", "title": "Similarity Join Size Estimation Using Locality Sensitive Hashing", "abstract": "<p>Similarity joins are important operations with a broad range of applications. In this paper we study the problem of vector similarity join size estimation (VSJ). It is a generalization of the previously studied set similarity join size estimation (SSJ) problem and can handle more interesting cases such as TF-IDF vectors. One of the key challenges in similarity join size estimation is that the join size can change dramatically depending on the input similarity threshold. We propose a sampling based algorithm that uses the Locality-Sensitive-Hashing (LSH) scheme. The proposed algorithm LSH-SS uses an LSH index to enable effective sampling even at high thresholds. We compare the proposed technique with random sampling and the state-of-the-art technique for SSJ (adapted to VSJ) and demonstrate LSH-SS offers more accurate estimates at both high and low similarity thresholds and small variance using real-world data sets.</p>\n", "tags": ["Independent", "LSH", "VLDB"], "tsne_embedding": [-26.375713348388672, 4.0661845207214355]}, {"key": "lee2023recursion", "year": "2023", "title": "Recursion Of Thought A Divide-and-conquer Approach To Multi-context Reasoning With Language Models", "abstract": "<p>Generating intermediate steps or Chain of Thought (CoT) is an effective way to significantly improve language models (LM) multi-step reasoning capability. However the CoT lengths can grow rapidly with the problem complexity easily exceeding the maximum context size. Instead of increasing the context limit which has already been heavily investigated we explore an orthogonal direction making LMs divide a problem into multiple contexts. We propose a new inference framework called Recursion of Thought (RoT) which introduces several special tokens that the models can output to trigger context-related operations. Extensive experiments with multiple architectures including GPT-3 show that RoT dramatically improves LMs inference capability to solve problems whose solution consists of hundreds of thousands of tokens.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [33.261959075927734, -6.229609489440918]}, {"key": "lehmann2023shockhash", "year": "2023", "title": "Shockhash Near Optimal-space Minimal Perfect Hashing Beyond Brute-force", "abstract": "<p>A minimal perfect hash function (MPHF) maps a set S of n keys to the first n integers without collisions. There is a lower bound of nlog(e)=1.44n bits needed to represent an MPHF. This can be reached by a brute-force algorithm that tries e^n hash function seeds in expectation and stores the first seed leading to an MPHF. The most space-efficient previous algorithms for constructing MPHFs all use such a brute-force approach as a basic building block. In this paper we introduce ShockHash - Small heavily overloaded cuckoo hash tables for minimal perfect hashing. ShockHash uses two hash functions h_0 and h_1 hoping for the existence of a function f S-0 1 such that x - h_f(x)(x) is an MPHF on S. It then uses a 1-bit retrieval data structure to store f using n + o(n) bits. In graph terminology ShockHash generates n-edge random graphs until stumbling on a pseudoforest - where each component contains as many edges as nodes. Using cuckoo hashing ShockHash then derives an MPHF from the pseudoforest in linear time. We show that ShockHash needs to try only about (e/2)^n=1.359^n seeds in expectation. This reduces the space for storing the seed by roughly n bits (maintaining the asymptotically optimal space consumption) and speeds up construction by almost a factor of 2^n compared to brute-force. Bipartite ShockHash reduces the expected construction time again to 1.166^n by maintaining a pool of candidate hash functions and checking all possible pairs. ShockHash as a building block within the RecSplit framework can be constructed up to 3 orders of magnitude faster than competing approaches. It can build an MPHF for 10 million keys with 1.489 bits per key in about half an hour. When instead using ShockHash after an efficient k-perfect hash function it achieves space usage similar to the best competitors while being significantly faster to construct and query.</p>\n", "tags": ["ARXIV", "Graph", "Independent"], "tsne_embedding": [-32.29427719116211, -12.931377410888672]}, {"key": "lehmann2023sliding", "year": "2023", "title": "Sliding Block Hashing (slick) -- Basic Algorithmic Ideas", "abstract": "<p>We present (bf) Sliding Blo(bf) ck Hashing (Slick) a simple hash table data structure that combines high performance with very good space efficiency. This preliminary report outlines avenues for analysis and implementation that we intend to pursue.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-19.254108428955078, 21.72771453857422]}, {"key": "lei2020locality", "year": "2020", "title": "Locality-sensitive Hashing Scheme Based On Longest Circular Co-substring", "abstract": "<p>Locality-Sensitive Hashing (LSH) is one of the most popular methods for c-Approximate Nearest Neighbor Search (c-ANNS) in high-dimensional spaces. In this paper we propose a novel LSH scheme based on the Longest Circular Co-Substring (LCCS) search framework (LCCS-LSH) with a theoretical guarantee. We introduce a novel concept of LCCS and a new data structure named Circular Shift Array (CSA) for k-LCCS search. The insight of LCCS search framework is that close data objects will have a longer LCCS than the far-apart ones with high probability. LCCS-LSH is (emph)LSH-family-independent and it supports c-ANNS with different kinds of distance metrics. We also introduce a multi-probe version of LCCS-LSH and conduct extensive experiments over five real-life datasets. The experimental results demonstrate that LCCS-LSH outperforms state-of-the-art LSH schemes.</p>\n", "tags": ["ARXIV", "Independent", "LSH"], "tsne_embedding": [-28.09946632385254, 2.339229106903076]}, {"key": "lemire2006one", "year": "2006", "title": "One-pass One-hash N-gram Statistics Estimation", "abstract": "<p>In multimedia text or bioinformatics databases applications query sequences of n consecutive symbols called n-grams. Estimating the number of distinct n-grams is a view-size estimation problem. While view sizes can be estimated by sampling under statistical assumptions we desire an unassuming algorithm with universally valid accuracy bounds. Most related work has focused on repeatedly hashing the data which is prohibitive for large data sources. We prove that a one-pass one-hash algorithm is sufficient for accurate estimates if the hashing is sufficiently independent. To reduce costs further we investigate recursive random hashing algorithms and show that they are sufficiently independent in practice. We compare our running times with exact counts using suffix arrays and show that while we use hardly any storage we are an order of magnitude faster. The approach further is extended to a one-pass/one-hash computation of n-gram entropy and iceberg counts. The experiments use a large collection of English text from the Gutenberg Project as well as synthetic data.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-21.551597595214844, -14.700428009033203]}, {"key": "lemire2007recursive", "year": "2007", "title": "Recursive N-gram Hashing Is Pairwise Independent At Best", "abstract": "<p>Many applications use sequences of n consecutive symbols (n-grams). Hashing these n-grams can be a performance bottleneck. For more speed recursive hash families compute hash values by updating previous values. We prove that recursive hash families cannot be more than pairwise independent. While hashing by irreducible polynomials is pairwise independent our implementations either run in time O(n) or use an exponential amount of memory. As a more scalable alternative we make hashing by cyclic polynomials pairwise independent by ignoring n-1 bits. Experimentally we show that hashing by cyclic polynomials is is twice as fast as hashing by irreducible polynomials. We also show that randomized Karp-Rabin hash families are not pairwise independent.</p>\n", "tags": ["Independent"], "tsne_embedding": [-20.324857711791992, -15.309588432312012]}, {"key": "lemire2010universality", "year": "2010", "title": "The Universality Of Iterated Hashing Over Variable-length Strings", "abstract": "<p>Iterated hash functions process strings recursively one character at a time. At each iteration they compute a new hash value from the preceding hash value and the next character. We prove that iterated hashing can be pairwise independent but never 3-wise independent. We show that it can be almost universal over strings much longer than the number of hash values; we bound the maximal string length given the collision probability.</p>\n", "tags": ["Independent"], "tsne_embedding": [-19.084945678710938, -15.314018249511719]}, {"key": "lemire2015faster", "year": "2015", "title": "Faster 64-bit Universal Hashing Using Carry-less Multiplications", "abstract": "<p>Intel and AMD support the Carry-less Multiplication (CLMUL) instruction set in their x64 processors. We use CLMUL to implement an almost universal 64-bit hash family (CLHASH). We compare this new family with what might be the fastest almost universal family on x64 processors (VHASH). We find that CLHASH is at least 6037; faster. We also compare CLHASH with a popular hash function designed for speed (Googles CityHash). We find that CLHASH is 4037; faster than CityHash on inputs larger than 64 bytes and just as fast otherwise.</p>\n", "tags": ["Independent"], "tsne_embedding": [-26.718036651611328, -14.756714820861816]}, {"key": "leng2015hashing", "year": "2015", "title": "Hashing for Distributed Data", "abstract": "<p>Recently, hashing based approximate nearest\nneighbors search has attracted much attention.\nExtensive centralized hashing algorithms have\nbeen proposed and achieved promising performance. However, due to the large scale of many\napplications, the data is often stored or even collected in a distributed manner. Learning hash\nfunctions by aggregating all the data into a fusion\ncenter is infeasible because of the prohibitively\nexpensive communication and computation overhead.\nIn this paper, we develop a novel hashing\nmodel to learn hash functions in a distributed setting. We cast a centralized hashing model as a\nset of subproblems with consensus constraints.\nWe find these subproblems can be analytically\nsolved in parallel on the distributed compute nodes. Since no training data is transmitted across\nthe nodes in the learning process, the communication cost of our model is independent to the data size. Extensive experiments on several large\nscale datasets containing up to 100 million samples demonstrate the efficacy of our method.</p>\n", "tags": ["ICML", "Independent"], "tsne_embedding": [-7.232397556304932, -16.091747283935547]}, {"key": "leng2023mitigating", "year": "2023", "title": "Mitigating Object Hallucinations In Large Vision-language Models Through Visual Contrastive Decoding", "abstract": "<p>Large Vision-Language Models (LVLMs) have advanced considerably intertwining visual recognition and language understanding to generate content that is not only coherent but also contextually attuned. Despite their success LVLMs still suffer from the issue of object hallucinations where models generate plausible yet incorrect outputs that include objects that do not exist in the images. To mitigate this issue we introduce Visual Contrastive Decoding (VCD) a simple and training-free method that contrasts output distributions derived from original and distorted visual inputs. The proposed VCD effectively reduces the over-reliance on statistical bias and unimodal priors two essential causes of object hallucinations. This adjustment ensures the generated content is closely grounded to visual inputs resulting in contextually accurate outputs. Our experiments show that VCD without either additional training or the usage of external tools significantly mitigates the object hallucination issue across different LVLM families. Beyond mitigating object hallucinations VCD also excels in general LVLM benchmarks highlighting its wide-ranging applicability.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [50.011009216308594, 5.761733531951904]}, {"key": "lepikhin2020gshard", "year": "2020", "title": "Gshard Scaling Giant Models With Conditional Computation And Automatic Sharding", "abstract": "<p>Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality there are challenges on the path such as the computation cost ease of programming and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [18.327505111694336, -9.970054626464844]}, {"key": "lessley2018data", "year": "2018", "title": "Data-parallel Hashing Techniques For GPU Architectures", "abstract": "<p>Hash tables are one of the most fundamental data structures for effectively storing and accessing sparse data with widespread usage in domains ranging from computer graphics to machine learning. This study surveys the state-of-the-art research on data-parallel hashing techniques for emerging massively-parallel many-core GPU architectures. Key factors affecting the performance of different hashing schemes are discovered and used to suggest best practices and pinpoint areas for further research.</p>\n", "tags": ["ARXIV", "Graph"], "tsne_embedding": [0.175321564078331, -16.840435028076172]}, {"key": "lester2021power", "year": "2021", "title": "The Power Of Scale For Parameter-efficient Prompt Tuning", "abstract": "<p>In this work we explore prompt tuning a simple yet effective mechanism for learning soft prompts to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3 soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3s few-shot learning by a large margin. More remarkably through ablations on model size using T5 we show that prompt tuning becomes more competitive with scale as models exceed billions of parameters our method closes the gap and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed prefix tuning of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer as compared to full model tuning.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [19.665252685546875, -5.526000022888184]}, {"key": "leu2023fast", "year": "2023", "title": "Fast Consistent Hashing In Constant Time", "abstract": "<p>Consistent hashing is a technique that can minimize key remapping when the number of hash buckets changes. The paper proposes a fast consistent hash algorithm (called power consistent hash) that has O(1) expected time for key lookup independent of the number of buckets. Hash values are computed in real time. No search data structure is constructed to store bucket ranges or key mappings. The algorithm has a lightweight design using O(1) space with superior scalability. In particular it uses two auxiliary hash functions to achieve distribution uniformity and O(1) expected time for key lookup. Furthermore it performs consistent hashing such that only a minimal number of keys are remapped when the number of buckets changes. Consistent hashing has a wide range of use cases including load balancing distributed caching and distributed key-value stores. The proposed algorithm is faster than well-known consistent hash algorithms with O((log) n) lookup time.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-24.67768669128418, -18.89751434326172]}, {"key": "lewis2020retrieval", "year": "2020", "title": "Retrieval-augmented Generation For Knowledge-intensive NLP Tasks", "abstract": "<p>Large pre-trained language models have been shown to store factual knowledge in their parameters and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However their ability to access and precisely manipulate knowledge is still limited and hence on knowledge-intensive tasks their performance lags behind task-specific architectures. Additionally providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) \u2013 models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia accessed with a pre-trained neural retriever. We compare two RAG formulations one which conditions on the same retrieved passages across the whole generated sequence the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks we find that RAG models generate more specific diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [12.151227951049805, -5.808173656463623]}, {"key": "lewkowycz2022solving", "year": "2022", "title": "Solving Quantitative Reasoning Problems With Language Models", "abstract": "<p>Language models have achieved remarkable performance on a wide range of tasks that require natural language understanding. Nevertheless state-of-the-art models have generally struggled with tasks that require quantitative reasoning such as solving mathematics science and engineering problems at the college level. To help close this gap we introduce Minerva a large language model pretrained on general natural language data and further trained on technical content. The model achieves state-of-the-art performance on technical benchmarks without the use of external tools. We also evaluate our model on over two hundred undergraduate-level problems in physics biology chemistry economics and other sciences that require quantitative reasoning and find that the model can correctly answer nearly a third of them.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [27.39488410949707, 1.711918592453003]}, {"key": "li2006sparse", "year": "2006", "title": "Very Sparse Random Projections", "abstract": "<p>There has been considerable interest in random projections, an approximate algorithm for estimating distances between pairs of points in a high-dimensional vector space. Let A in Rn x D be our n points in D dimensions. The method multiplies A by a random matrix R in RD x k, reducing the D dimensions down to just k for speeding up the computation. R typically consists of entries of standard normal N(0,1). It is well known that random projections preserve pairwise distances (in the expectation). Achlioptas proposed sparse random projections by replacing the N(0,1) entries in R with entries in -1,0,1 with probabilities 1/6, 2/3, 1/6, achieving a threefold speedup in processing time.We recommend using R of entries in -1,0,1 with probabilities 1/2\u221aD, 1-1\u221aD, 1/2\u221aD for achieving a significant \u221aD-fold speedup, with little loss in accuracy.</p>\n", "tags": ["KDD"], "tsne_embedding": [-30.668495178222656, 8.803961753845215]}, {"key": "li2009b", "year": "2009", "title": "B-bit Minwise Hashing", "abstract": "<p>This paper establishes the theoretical framework of b-bit minwise hashing. The original minwise hashing method has become a standard technique for estimating set similarity (e.g. resemblance) with applications in information retrieval data management social networks and computational advertising. By only storing the lowest (b) bits of each (minwise) hashed value (e.g. b=1 or 2) one can gain substantial advantages in terms of computational efficiency and storage space. We prove the basic theoretical results and provide an unbiased estimator of the resemblance for any b. We demonstrate that even in the least favorable scenario using b=1 may reduce the storage space at least by a factor of 21.3 (or 10.7) compared to using b=64 (or b=32) if one is interested in resemblance 0.5.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-16.927873611450195, 3.5579535961151123]}, {"key": "li2010b", "year": "2010", "title": "B-bit Minwise Hashing For Estimating Three-way Similarities", "abstract": "<p>Computing two-way and multi-way set similarities is a fundamental problem. This study focuses on estimating 3-way resemblance (Jaccard similarity) using b-bit minwise hashing. While traditional minwise hashing methods store each hashed value using 64 bits b-bit minwise hashing only stores the lowest b bits (where b= 2 for 3-way). The extension to 3-way similarity from the prior work on 2-way similarity is technically non-trivial. We develop the precise estimator which is accurate and very complicated; and we recommend a much simplified estimator suitable for sparse data. Our analysis shows that (b)-bit minwise hashing can normally achieve a 10 to 25-fold improvement in the storage space required for a given estimator accuracy of the 3-way resemblance.</p>\n", "tags": ["NEURIPS"], "tsne_embedding": [-17.5771541595459, 2.9056692123413086]}, {"key": "li2011accurate", "year": "2011", "title": "Accurate Estimators For Improving Minwise Hashing And B-bit Minwise Hashing", "abstract": "<p>Minwise hashing is the standard technique in the context of search and databases for efficiently estimating set (e.g. high-dimensional 0/1 vector) similarities. Recently b-bit minwise hashing was proposed which significantly improves upon the original minwise hashing in practice by storing only the lowest b bits of each hashed value as opposed to using 64 bits. b-bit hashing is particularly effective in applications which mainly concern sets of high similarities (e.g. the resemblance 0.5). However there are other important applications in which not just pairs of high similarities matter. For example many learning algorithms require all pairwise similarities and it is expected that only a small fraction of the pairs are similar. Furthermore many applications care more about containment (e.g. how much one object is contained by another object) than the resemblance. In this paper we show that the estimators for minwise hashing and b-bit minwise hashing used in the current practice can be systematically improved and the improvements are most significant for set pairs of low resemblance and high containment.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-18.096820831298828, -5.1654052734375]}, {"key": "li2011b", "year": "2011", "title": "B-bit Minwise Hashing For Large-scale Linear SVM", "abstract": "<p>In this paper we propose to (seamlessly) integrate b-bit minwise hashing with linear SVM to substantially improve the training (and testing) efficiency using much smaller memory with essentially no loss of accuracy. Theoretically we prove that the resemblance matrix the minwise hashing matrix and the b-bit minwise hashing matrix are all positive definite matrices (kernels). Interestingly our proof for the positive definiteness of the b-bit minwise hashing kernel naturally suggests a simple strategy to integrate b-bit hashing with linear SVM. Our technique is particularly useful when the data can not fit in memory which is an increasingly critical issue in large-scale machine learning. Our preliminary experimental results on a publicly available webspam dataset (350K samples and 16 million dimensions) verified the effectiveness of our algorithm. For example the training time was reduced to merely a few seconds. In addition our technique can be easily extended to many other linear and nonlinear machine learning applications such as logistic regression.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [6.332892417907715, -12.044699668884277]}, {"key": "li2011hashing", "year": "2011", "title": "Hashing Algorithms For Large-scale Learning", "abstract": "<p>Minwise hashing is a standard technique in the context of search for efficiently computing set similarities. The recent development of b-bit minwise hashing provides a substantial improvement by storing only the lowest b bits of each hashed value. In this paper we demonstrate that b-bit minwise hashing can be naturally integrated with linear learning algorithms such as linear SVM and logistic regression to solve large-scale and high-dimensional statistical learning tasks especially when the data do not fit in memory. We compare (b)-bit minwise hashing with the Count-Min (CM) and Vowpal Wabbit (VW) algorithms which have essentially the same variances as random projections. Our theoretical and empirical comparisons illustrate that b-bit minwise hashing is significantly more accurate (at the same storage cost) than VW (and random projections) for binary data.</p>\n", "tags": ["NEURIPS", "Supervised"], "tsne_embedding": [-25.366777420043945, -7.141359329223633]}, {"key": "li2011learning", "year": "2011", "title": "Learning To Search Efficiently In High Dimensions", "abstract": "<p>High dimensional similarity search in large scale databases becomes an important challenge due to the advent of Internet. For such applications specialized data structures are required to achieve computational efficiency. Traditional approaches relied on algorithmic constructions that are often data independent (such as Locality Sensitive Hashing) or weakly dependent (such as kd-trees k-means trees). While supervised learning algorithms have been applied to related problems those proposed in the literature mainly focused on learning hash codes optimized for compact embedding of the data rather than search efficiency. Consequently such an embedding has to be used with linear scan or another search algorithm. Hence learning to hash does not directly address the search efficiency issue. This paper considers a new framework that applies supervised learning to directly optimize a data structure that supports efficient large scale search. Our approach takes both search quality and computational cost into consideration. Specifically we learn a boosted search forest that is optimized using pair-wise similarity labeled examples. The output of this search forest can be efficiently converted into an inverted indexing data structure which can leverage modern text search infrastructure to achieve both scalability and efficiency. Experimental results show that our approach significantly outperforms the start-of-the-art learning to hash methods (such as spectral hashing) as well as state-of-the-art high dimensional search algorithms (such as LSH and k-means trees).</p>\n", "tags": ["LSH", "NEURIPS", "Supervised"], "tsne_embedding": [8.142440795898438, -1.1128250360488892]}, {"key": "li2011training", "year": "2011", "title": "Training Logistic Regression And SVM On 200GB Data Using B-bit Minwise Hashing And Comparisons With Vowpal Wabbit (VW)", "abstract": "<p>We generated a dataset of 200 GB with 10^9 features to test our recent b-bit minwise hashing algorithms for training very large-scale logistic regression and SVM. The results confirm our prior work that compared with the VW hashing algorithm (which has the same variance as random projections) b-bit minwise hashing is substantially more accurate at the same storage. For example with merely 30 hashed values per data point b-bit minwise hashing can achieve similar accuracies as VW with 2^14 hashed values per data point. We demonstrate that the preprocessing cost of b-bit minwise hashing is roughly on the same order of magnitude as the data loading time. Furthermore by using a GPU the preprocessing cost can be reduced to a small fraction of the data loading time. Minwise hashing has been widely used in industry at least in the context of search. One reason for its popularity is that one can efficiently simulate permutations by (e.g.) universal hashing. In other words there is no need to store the permutation matrix. In this paper we empirically verify this practice by demonstrating that even using the simplest 2-universal hashing does not degrade the learning performance.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [-32.15184020996094, -14.747711181640625]}, {"key": "li2012b", "year": "2012", "title": "B-bit Minwise Hashing In Practice Large-scale Batch And Online Learning And Using Gpus For Fast Preprocessing With Simple Hash Functions", "abstract": "<p>In this paper we study several critical issues which must be tackled before one can apply b-bit minwise hashing to the volumes of data often used industrial applications especially in the context of search. 1. (b-bit) Minwise hashing requires an expensive preprocessing step that computes k (e.g. 500) minimal values after applying the corresponding permutations for each data vector. We developed a parallelization scheme using GPUs and observed that the preprocessing time can be reduced by a factor of 20-80 and becomes substantially smaller than the data loading time. 2. One major advantage of b-bit minwise hashing is that it can substantially reduce the amount of memory required for batch learning. However as online algorithms become increasingly popular for large-scale learning in the context of search it is not clear if b-bit minwise yields significant improvements for them. This paper demonstrates that b-bit minwise hashing provides an effective data size/dimension reduction scheme and hence it can dramatically reduce the data loading time for each epoch of the online training process. This is significant because online learning often requires many (e.g. 10 to 100) epochs to reach a sufficient accuracy. 3. Another critical issue is that for very large data sets it becomes impossible to store a (fully) random permutation matrix due to its space requirements. Our paper is the first study to demonstrate that b-bit minwise hashing implemented using simple hash functions e.g. the 2-universal (2U) and 4-universal (4U) hash families can produce very similar learning results as using fully random permutations. Experiments on datasets of up to 200GB are presented.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [7.1617350578308105, -13.442593574523926]}, {"key": "li2012one", "year": "2012", "title": "One Permutation Hashing", "abstract": "<p>While minwise hashing is promising for large-scale learning in massive binary data the preprocessing cost is prohibitive as it requires applying (e.g.) (k=500) permutations on the data. The testing time is also expensive if a new data point (e.g. a new document or a new image) has not been processed. In this paper we develop a simple textbfone permutation hashing scheme to address this important issue. While it is true that the preprocessing step can be parallelized it comes at the cost of additional hardware and implementation. Also reducing (k) permutations to just one would be much more textbfenergy-efficient which might be an important perspective as minwise hashing is commonly deployed in the search industry. While the theoretical probability analysis is interesting our experiments on similarity estimation and SVM amp; logistic regression also confirm the theoretical results.</p>\n", "tags": ["NEURIPS", "Supervised"], "tsne_embedding": [-27.66648292541504, -9.59623908996582]}, {"key": "li2013column", "year": "2013", "title": "Learning Hash Functions Using Column Generation", "abstract": "<p>Fast nearest neighbor searching is becoming\nan increasingly important tool in solving\nmany large-scale problems. Recently\na number of approaches to learning datadependent\nhash functions have been developed.\nIn this work, we propose a column\ngeneration based method for learning datadependent\nhash functions on the basis of\nproximity comparison information. Given a\nset of triplets that encode the pairwise proximity\ncomparison information, our method\nlearns hash functions that preserve the relative\ncomparison relationships in the data\nas well as possible within the large-margin\nlearning framework. The learning procedure\nis implemented using column generation and\nhence is named CGHash. At each iteration\nof the column generation procedure, the best\nhash function is selected. Unlike most other\nhashing methods, our method generalizes to\nnew data points naturally; and has a training\nobjective which is convex, thus ensuring\nthat the global optimum can be identi-\nfied. Experiments demonstrate that the proposed\nmethod learns compact binary codes\nand that its retrieval performance compares\nfavorably with state-of-the-art methods when\ntested on a few benchmark datasets.</p>\n", "tags": ["Has Code", "ICML"], "tsne_embedding": [-16.042348861694336, -11.034322738647461]}, {"key": "li2013learning", "year": "2013", "title": "Learning Hash Functions Using Column Generation", "abstract": "<p>Fast nearest neighbor searching is becoming an increasingly important tool in solving many large-scale problems. Recently a number of approaches to learning data-dependent hash functions have been developed. In this work we propose a column generation based method for learning data-dependent hash functions on the basis of proximity comparison information. Given a set of triplets that encode the pairwise proximity comparison information our method learns hash functions that preserve the relative comparison relationships in the data as well as possible within the large-margin learning framework. The learning procedure is implemented using column generation and hence is named CGHash. At each iteration of the column generation procedure the best hash function is selected. Unlike most other hashing methods our method generalizes to new data points naturally; and has a training objective which is convex thus ensuring that the global optimum can be identified. Experiments demonstrate that the proposed method learns compact binary codes and that its retrieval performance compares favorably with state-of-the-art methods when tested on a few benchmark datasets.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-16.033109664916992, -11.069916725158691]}, {"key": "li2015birds", "year": "2015", "title": "Two Birds, One Stone: Jointly Learning Binary Code for Large-scale Face Image Retrieval and Attributes Prediction", "abstract": "<p>We address the challenging large-scale content-based\nface image retrieval problem, intended as searching images\nbased on the presence of specific subject, given one face\nimage of him/her. To this end, one natural demand is a supervised binary code learning method. While the learned\ncodes might be discriminating, people often have a further\nexpectation that whether some semantic message (e.g., visual attributes) can be read from the human-incomprehensible\ncodes. For this purpose, we propose a novel binary code\nlearning framework by jointly encoding identity discriminability and a number of facial attributes into unified binary code. In this way, the learned binary codes can be applied to not only fine-grained face image retrieval, but also\nfacial attributes prediction, which is the very innovation of\nthis work, just like killing two birds with one stone. To evaluate the effectiveness of the proposed method, extensive experiments are conducted on a new purified large-scale web\ncelebrity database, named CFW 60K, with abundant manual identity and attributes annotation, and experimental results exhibit the superiority of our method over state-of-the-art.</p>\n", "tags": ["ICCV", "Image Retrieval", "Supervised"], "tsne_embedding": [-7.6058454513549805, 11.419266700744629]}, {"key": "li2015bit", "year": "2015", "title": "0-Bit Consistent Weighted Sampling", "abstract": "<p>We develop 0-bit consistent weighted sampling (CWS) for efficiently estimating min-max kernel, which is a generalization of the resemblance kernel originally designed for binary data. Because the estimator of 0-bit CWS constitutes a positive definite kernel, this method can be naturally applied to large-scale data mining problems. Basically, if we feed the sampled data from 0-bit CWS to a highly efficient linear classifier (e.g., linear SVM), we effectively (and approximately) train a nonlinear classifier based on the min-max kernel. The accuracy improves as we increase the sample size.</p>\n\n<p>In this paper, we first demonstrate, through an extensive classification study using kernel machines, that the min-max kernel often provides an effective measure of similarity for nonnegative data. This helps justify the use of min-max kernel. However, as the min-max kernel is nonlinear and might be difficult to be used for industrial applications with massive data, we propose to linearize the min-max kernel via 0-bit CWS, a simplification of the original CWS method.</p>\n\n<p>The previous remarkable work on consistent weighted sampling (CWS) produces samples in the form of (i<em>, t</em>) where the i* records the location (and in fact also the weights) information analogous to the samples produced by classical minwise hashing on binary data. Because the t* is theoretically unbounded, it was not immediately clear how to effectively implement CWS for building large-scale linear classifiers. We provide a simple solution by discarding t* (which we refer to as the \u201c0-bit\u201d scheme). Via an extensive empirical study, we show that this 0-bit scheme does not lose essential information. We then apply 0-bit CWS for building linear classifiers to approximate min-max kernel classifiers, as extensively validated on a wide range of public datasets.</p>\n\n<p>We expect this work will generate interests among data mining practitioners who would like to efficiently utilize the nonlinear information of non-binary and nonnegative data.</p>\n\n", "tags": [], "tsne_embedding": [4.127613544464111, -18.594614028930664]}, {"key": "li2015fast", "year": "2015", "title": "Fast K-nearest Neighbour Search Via Dynamic Continuous Indexing", "abstract": "<p>Existing methods for retrieving k-nearest neighbours suffer from the curse of dimensionality. We argue this is caused in part by inherent deficiencies of space partitioning which is the underlying strategy used by most existing methods. We devise a new strategy that avoids partitioning the vector space and present a novel randomized algorithm that runs in time linear in dimensionality of the space and sub-linear in the intrinsic dimensionality and the size of the dataset and takes space constant in dimensionality of the space and linear in the size of the dataset. The proposed algorithm allows fine-grained control over accuracy and speed on a per-query basis automatically adapts to variations in data density supports dynamic updates to the dataset and is easy-to-implement. We show appealing theoretical properties and demonstrate empirically that the proposed algorithm outperforms locality-sensitivity hashing (LSH) in terms of approximation quality speed and space efficiency.</p>\n", "tags": ["ARXIV", "Independent", "LSH"], "tsne_embedding": [-28.771902084350586, -7.379343032836914]}, {"key": "li2015feature", "year": "2015", "title": "Feature Learning Based Deep Supervised Hashing With Pairwise Labels", "abstract": "<p>Recent years have witnessed wide application of hashing for large-scale image retrieval. However most existing hashing methods are based on hand-crafted features which might not be optimally compatible with the hashing procedure. Recently deep hashing methods have been proposed to perform simultaneous feature learning and hash-code learning with deep neural networks which have shown better performance than traditional hashing methods with hand-crafted features. Most of these deep hashing methods are supervised whose supervised information is given with triplet labels. For another common application scenario with pairwise labels there have not existed methods for simultaneous feature learning and hash-code learning. In this paper we propose a novel deep hashing method called deep pairwise-supervised hashing(DPSH) to perform simultaneous feature learning and hash-code learning for applications with pairwise labels. Experiments on real datasets show that our DPSH method can outperform other methods to achieve the state-of-the-art performance in image retrieval applications.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Supervised"], "tsne_embedding": [1.8198702335357666, -21.788991928100586]}, {"key": "li2015rank", "year": "2015", "title": "Rank Subspace Learning For Compact Hash Codes", "abstract": "<p>The era of Big Data has spawned unprecedented interests in developing hashing algorithms for efficient storage and fast nearest neighbor search. Most existing work learn hash functions that are numeric quantizations of feature values in projected feature space. In this work we propose a novel hash learning framework that encodes features rank orders instead of numeric values in a number of optimal low-dimensional ranking subspaces. We formulate the ranking subspace learning problem as the optimization of a piece-wise linear convex-concave function and present two versions of our algorithm one with independent optimization of each hash bit and the other exploiting a sequential learning framework. Our work is a generalization of the Winner-Take-All (WTA) hash family and naturally enjoys all the numeric stability benefits of rank correlation measures while being optimized to achieve high precision at very short code length. We compare with several state-of-the-art hashing algorithms in both supervised and unsupervised domain showing superior performance in a number of data sets.</p>\n", "tags": ["ARXIV", "Quantisation", "Unsupervised"], "tsne_embedding": [-18.495473861694336, -11.753968238830566]}, {"key": "li2016feature", "year": "2016", "title": "Feature Learning based Deep Supervised Hashing with Pairwise Labels", "abstract": "<p>Recent years have witnessed wide application of\nhashing for large-scale image retrieval. However,\nmost existing hashing methods are based on handcrafted features which might not be optimally compatible with the hashing procedure. Recently, deep\nhashing methods have been proposed to perform simultaneous feature learning and hash-code learning with deep neural networks, which have shown\nbetter performance than traditional hashing methods with hand-crafted features. Most of these deep\nhashing methods are supervised whose supervised\ninformation is given with triplet labels. For another common application scenario with pairwise labels, there have not existed methods for simultaneous feature learning and hash-code learning. In this\npaper, we propose a novel deep hashing method,\ncalled deep pairwise-supervised hashing (DPSH),\nto perform simultaneous feature learning and hashcode learning for applications with pairwise labels.\nExperiments on real datasets show that our DPSH\nmethod can outperform other methods to achieve\nthe state-of-the-art performance in image retrieval\napplications.</p>\n", "tags": ["Deep Learning", "Has Code", "IJCAI", "Image Retrieval", "Supervised"], "tsne_embedding": [1.859818696975708, -21.776010513305664]}, {"key": "li2016generalized", "year": "2016", "title": "Generalized Intersection Kernel", "abstract": "<p>Following the very recent line of work on the generalized min-max (GMM) kernel this study proposes the generalized intersection (GInt) kernel and the related normalized generalized min-max (NGMM) kernel. In computer vision the (histogram) intersection kernel has been popular and the GInt kernel generalizes it to data which can have both negative and positive entries. Through an extensive empirical classification study on 40 datasets from the UCI repository we are able to show that this (tuning-free) GInt kernel performs fairly well. The empirical results also demonstrate that the NGMM kernel typically outperforms the GInt kernel. Interestingly the NGMM kernel has another interpretation \u2014 it is the asymmetrically transformed version of the GInt kernel based on the idea of asymmetric hashing. Just like the GMM kernel the NGMM kernel can be efficiently linearized through (e.g.) generalized consistent weighted sampling (GCWS) as empirically validated in our study. Owing to the discrete nature of hashed values it also provides a scheme for approximate near neighbor search.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [-26.569713592529297, -1.221543312072754]}, {"key": "li2016random", "year": "2016", "title": "2-bit Random Projections Nonlinear Estimators And Approximate Near Neighbor Search", "abstract": "<p>The method of random projections has become a standard tool for machine learning data mining and search with massive data at Web scale. The effective use of random projections requires efficient coding schemes for quantizing (real-valued) projected data into integers. In this paper we focus on a simple 2-bit coding scheme. In particular we develop accurate nonlinear estimators of data similarity based on the 2-bit strategy. This work will have important practical applications. For example in the task of near neighbor search a crucial step (often called re-ranking) is to compute or estimate data similarities once a set of candidate data points have been identified by hash table techniques. This re-ranking step can take advantage of the proposed coding scheme and estimator. As a related task in this paper we also study a simple uniform quantization scheme for the purpose of building hash tables with projected data. Our analysis shows that typically only a small number of bits are needed. For example when the target similarity level is high 2 or 3 bits might be sufficient. When the target similarity level is not so high it is preferable to use only 1 or 2 bits. Therefore a 2-bit scheme appears to be overall a good choice for the task of sublinear time approximate near neighbor search via hash tables. Combining these results we conclude that 2-bit random projections should be recommended for approximate near neighbor search and similarity estimation. Extensive experimental results are provided.</p>\n", "tags": ["ARXIV", "Independent", "Quantisation"], "tsne_embedding": [-21.956218719482422, -1.3497475385665894]}, {"key": "li2017deep", "year": "2017", "title": "Deep Supervised Discrete Hashing", "abstract": "<p>With the rapid growth of image and video data on the web hashing has been extensively studied for image or video search in recent years. Benefiting from recent advances in deep learning deep hashing methods have achieved promising results for image retrieval. However there are some limitations of previous deep hashing methods (e.g. the semantic information is not fully exploited). In this paper we develop a deep supervised discrete hashing algorithm based on the assumption that the learned binary codes should be ideal for classification. Both the pairwise label information and the classification information are used to learn the hash codes within one stream framework. We constrain the outputs of the last layer to be binary codes directly which is rarely investigated in deep hashing algorithm. Because of the discrete nature of hash codes an alternating minimization method is used to optimize the objective function. Experimental results have shown that our method outperforms current state-of-the-art methods on benchmark datasets.</p>\n", "tags": ["Deep Learning", "Image Retrieval", "NEURIPS", "Supervised"], "tsne_embedding": [-1.0887579917907715, 8.407358169555664]}, {"key": "li2018dual", "year": "2018", "title": "Dual Asymmetric Deep Hashing Learning", "abstract": "<p>Due to the impressive learning power deep learning has achieved a remarkable performance in supervised hash function learning. In this paper we propose a novel asymmetric supervised deep hashing method to preserve the semantic structure among different categories and generate the binary codes simultaneously. Specifically two asymmetric deep networks are constructed to reveal the similarity between each pair of images according to their semantic labels. The deep hash functions are then learned through two networks by minimizing the gap between the learned features and discrete codes. Furthermore since the binary codes in the Hamming space also should keep the semantic affinity existing in the original space another asymmetric pairwise loss is introduced to capture the similarity between the binary codes and real-value features. This asymmetric loss not only improves the retrieval performance but also contributes to a quick convergence at the training phase. By taking advantage of the two-stream deep structures and two types of asymmetric pairwise functions an alternating algorithm is designed to optimize the deep features and high-quality binary codes efficiently. Experimental results on three real-world datasets substantiate the effectiveness and superiority of our approach as compared with state-of-the-art.</p>\n", "tags": ["ARXIV", "Deep Learning", "Supervised"], "tsne_embedding": [1.20844304561615, 1.5987011194229126]}, {"key": "li2018scratch", "year": "2018", "title": "SCRATCH: A Scalable Discrete Matrix Factorization Hashing for Cross-Modal Retrieval", "abstract": "<p>In recent years, many hashing methods have been proposed for the cross-modal retrieval task. However, there are still some issues that need to be further explored. For example, some of them relax the binary constraints to generate the hash codes, which may generate large quantization error. Although some discrete schemes have been proposed, most of them are time-consuming. In addition, most of the existing supervised hashing methods use an n x n similarity matrix during the optimization, making them unscalable. To address these issues, in this paper, we present a novel supervised cross-modal hashing method\u2014Scalable disCRete mATrix faCtorization Hashing, SCRATCH for short. It leverages the collective matrix factorization on the kernelized features and the semantic embedding with labels to find a latent semantic space to preserve the intra- and inter-modality similarities. In addition, it incorporates the label matrix instead of the similarity matrix into the loss function. Based on the proposed loss function and the iterative optimization algorithm, it can learn the hash functions and binary codes simultaneously. Moreover, the binary codes can be generated discretely, reducing the quantization error generated by the relaxation scheme. Its time complexity is linear to the size of the dataset, making it scalable to large-scale datasets. Extensive experiments on three benchmark datasets, namely, Wiki, MIRFlickr-25K, and NUS-WIDE, have verified that our proposed SCRATCH model outperforms several state-of-the-art unsupervised and supervised hashing methods for cross-modal retrieval.</p>\n", "tags": ["Cross Modal", "MM", "Supervised"], "tsne_embedding": [-4.957406044006348, -2.9221720695495605]}, {"key": "li2018self", "year": "2018", "title": "Self-supervised Adversarial Hashing Networks For Cross-modal Retrieval", "abstract": "<p>Thanks to the success of deep learning cross-modal retrieval has made significant progress recently. However there still remains a crucial bottleneck how to bridge the modality gap to further enhance the retrieval accuracy. In this paper we propose a self-supervised adversarial hashing ((textbfSSAH)) approach which lies among the early attempts to incorporate adversarial learning into cross-modal hashing in a self-supervised fashion. The primary contribution of this work is that two adversarial networks are leveraged to maximize the semantic correlation and consistency of the representations between different modalities. In addition we harness a self-supervised semantic network to discover high-level semantic information in the form of multi-label annotations. Such information guides the feature learning process and preserves the modality relationships in both the common semantic space and the Hamming space. Extensive experiments carried out on three benchmark datasets validate that the proposed SSAH surpasses the state-of-the-art methods.</p>\n", "tags": ["ARXIV", "Cross Modal", "Deep Learning", "Supervised"], "tsne_embedding": [7.256927490234375, 6.030433654785156]}, {"key": "li2019coupled", "year": "2019", "title": "Coupled Cyclegan Unsupervised Hashing Network For Cross-modal Retrieval", "abstract": "<p>In recent years hashing has attracted more and more attention owing to its superior capacity of low storage cost and high query efficiency in large-scale cross-modal retrieval. Benefiting from deep leaning continuously compelling results in cross-modal retrieval community have been achieved. However existing deep cross-modal hashing methods either rely on amounts of labeled information or have no ability to learn an accuracy correlation between different modalities. In this paper we proposed Unsupervised coupled Cycle generative adversarial Hashing networks (UCH) for cross-modal retrieval where outer-cycle network is used to learn powerful common representation and inner-cycle network is explained to generate reliable hash codes. Specifically our proposed UCH seamlessly couples these two networks with generative adversarial mechanism which can be optimized simultaneously to learn representation and hash codes. Extensive experiments on three popular benchmark datasets show that the proposed UCH outperforms the state-of-the-art unsupervised cross-modal hashing methods.</p>\n", "tags": ["ARXIV", "Cross Modal", "Deep Learning", "Unsupervised"], "tsne_embedding": [-6.185563564300537, -10.68338680267334]}, {"key": "li2019deep", "year": "2019", "title": "Deep Multi-index Hashing For Person Re-identification", "abstract": "<p>Traditional person re-identification (ReID) methods typically represent person images as real-valued features which makes ReID inefficient when the gallery set is extremely large. Recently some hashing methods have been proposed to make ReID more efficient. However these hashing methods will deteriorate the accuracy in general and the efficiency of them is still not high enough. In this paper we propose a novel hashing method called deep multi-index hashing (DMIH) to improve both efficiency and accuracy for ReID. DMIH seamlessly integrates multi-index hashing and multi-branch based networks into the same framework. Furthermore a novel block-wise multi-index hashing table construction approach and a search-aware multi-index (SAMI) loss are proposed in DMIH to improve the search efficiency. Experiments on three widely used datasets show that DMIH can outperform other state-of-the-art baselines including both hashing methods and real-valued methods in terms of both efficiency and accuracy.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [1.626246452331543, 21.409746170043945]}, {"key": "li2019neighborhood", "year": "2019", "title": "Neighborhood Preserving Hashing for Scalable Video Retrieval", "abstract": "<p>In this paper, we propose a Neighborhood Preserving\nHashing (NPH) method for scalable video retrieval in an\nunsupervised manner. Unlike most existing deep video\nhashing methods which indiscriminately compress an entire video into a binary code, we embed the spatial-temporal\nneighborhood information into the encoding network such\nthat the neighborhood-relevant visual content of a video can\nbe preferentially encoded into a binary code under the guidance of the neighborhood information. Specifically, we propose a neighborhood attention mechanism which focuses\non partial useful content of each input frame conditioned\non the neighborhood information. We then integrate the\nneighborhood attention mechanism into an RNN-based reconstruction scheme to encourage the binary codes to capture the spatial-temporal structure in a video which is consistent with that in the neighborhood. As a consequence, the\nlearned hashing functions can map similar videos to similar\nbinary codes. Extensive experiments on three widely-used\nbenchmark datasets validate the effectiveness of our proposed approach.</p>\n", "tags": ["Deep Learning", "ICCV", "Supervised", "Video Retrieval"], "tsne_embedding": [-9.555399894714355, -24.89223289489746]}, {"key": "li2019push", "year": "2019", "title": "Push for Quantization: Deep Fisher Hashing", "abstract": "<p>Current massive datasets demand light-weight access for analysis. Discrete hashing methods are thus beneficial because they map high-dimensional data to compact binary codes that are efficient to store and process, while preserving semantic similarity. To optimize powerful deep learning methods for image hashing, gradient-based methods are required. Binary codes, however, are discrete and thus have no continuous derivatives. Relaxing the problem by solving it in a continuous space and then quantizing the solution is not guaranteed to yield separable binary codes. The quantization needs to be included in the optimization. In this paper we push for quantization: We optimize maximum class separability in the binary space. We introduce a margin on distances between dissimilar image pairs as measured in the binary space. In addition to pair-wise distances, we draw inspiration from Fisher\u2019s Linear Discriminant Analysis (Fisher LDA) to maximize the binary distances between classes and at the same time minimize the binary distance of images within the same class. Experiments on CIFAR-10, NUS-WIDE and ImageNet100 demonstrate compact codes comparing favorably to the current state of the art.</p>\n", "tags": ["BMVC", "CNN", "Image Retrieval", "Quantisation"], "tsne_embedding": [-21.93982696533203, 15.266586303710938]}, {"key": "li2019re", "year": "2019", "title": "Re-randomized Densification For One Permutation Hashing And Bin-wise Consistent Weighted Sampling", "abstract": "<p>Jaccard similarity is widely used as a distance measure in many machine learning and search applications. Typically hashing methods are essential for the use of Jaccard similarity to be practical in large-scale settings. For hashing binary (0/1) data the idea of one permutation hashing (OPH) with densification significantly accelerates traditional minwise hashing algorithms while providing unbiased and accurate estimates. In this paper we propose a strategy named re-randomization in the process of densification that could achieve the smallest variance among all densification schemes. The success of this idea naturally inspires us to generalize one permutation hashing to weighted (non-binary) data which results in the socalled bin-wise consistent weighted sampling (BCWS) algorithm. We analyze the behavior of BCWS and compare it with a recent alternative. Extensive experiments on various datasets illustrates the effectiveness of our proposed methods.</p>\n", "tags": ["Independent", "NEURIPS"], "tsne_embedding": [-24.77280044555664, -0.14310504496097565]}, {"key": "li2020deep", "year": "2020", "title": "Deep Unsupervised Image Hashing By Maximizing Bit Entropy", "abstract": "<p>Unsupervised hashing is important for indexing huge image or video collections without having expensive annotations available. Hashing aims to learn short binary codes for compact storage and efficient semantic retrieval. We propose an unsupervised deep hashing layer called Bi-half Net that maximizes entropy of the binary codes. Entropy is maximal when both possible values of the bit are uniformly (half-half) distributed. To maximize bit entropy we do not add a term to the loss function as this is difficult to optimize and tune. Instead we design a new parameter-free network layer to explicitly force continuous image features to approximate the optimal half-half bit distribution. This layer is shown to minimize a penalized term of the Wasserstein distance between the learned continuous image features and the optimal half-half bit distribution. Experimental results on the image datasets Flickr25k Nus-wide Cifar-10 Mscoco Mnist and the video datasets Ucf-101 and Hmdb-51 show that our approach leads to compact codes and compares favorably to the current state-of-the-art.</p>\n", "tags": ["ARXIV", "Unsupervised"], "tsne_embedding": [-13.306499481201172, -26.6317138671875]}, {"key": "li2020multiple", "year": "2020", "title": "Multiple Code Hashing For Efficient Image Retrieval", "abstract": "<p>Due to its low storage cost and fast query speed hashing has been widely used in large-scale image retrieval tasks. Hash bucket search returns data points within a given Hamming radius to each query which can enable search at a constant or sub-linear time cost. However existing hashing methods cannot achieve satisfactory retrieval performance for hash bucket search in complex scenarios since they learn only one hash code for each image. More specifically by using one hash code to represent one image existing methods might fail to put similar image pairs to the buckets with a small Hamming distance to the query when the semantic information of images is complex. As a result a large number of hash buckets need to be visited for retrieving similar images based on the learned codes. This will deteriorate the efficiency of hash bucket search. In this paper we propose a novel hashing framework called multiple code hashing (MCH) to improve the performance of hash bucket search. The main idea of MCH is to learn multiple hash codes for each image with each code representing a different region of the image. Furthermore we propose a deep reinforcement learning algorithm to learn the parameters in MCH. To the best of our knowledge this is the first work that proposes to learn multiple hash codes for each image in image retrieval. Experiments demonstrate that MCH can achieve a significant improvement in hash bucket search compared with existing methods that learn only one hash code for each image.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Independent"], "tsne_embedding": [-4.097595691680908, 1.103619933128357]}, {"key": "li2020selfsupervised", "year": "2021", "title": "Self-Supervised Video Hashing via Bidirectional Transformers", "abstract": "<p>Most existing unsupervised video hashing methods are built on unidirectional models with less reliable training objectives, which underuse the correlations among frames and the similarity structure between videos. To enable efficient scalable video retrieval, we propose a self-supervised video Hashing method based on Bidirectional Transformers (BTH). Based on the encoder-decoder structure of transformers, we design a visual cloze task to fully exploit the bidirectional correlations between frames. To unveil the similarity structure between unlabeled video data, we further develop a similarity reconstruction task by establishing reliable and effective similarity connections in the video space. Furthermore, we develop a cluster assignment task to exploit the structural statistics of the whole dataset such that more discriminative binary codes can be learned. Extensive experiments implemented on three public benchmark datasets, FCVID, ActivityNet and YFCC, demonstrate the superiority of our proposed approach.</p>\n", "tags": ["CVPR", "Deep Learning", "Self Supervised", "Video Retrieval"], "tsne_embedding": [-8.03706169128418, -24.318031311035156]}, {"key": "li2020task", "year": "2020", "title": "Task-adaptive Asymmetric Deep Cross-modal Hashing", "abstract": "<p>Supervised cross-modal hashing aims to embed the semantic correlations of heterogeneous modality data into the binary hash codes with discriminative semantic labels. Because of its advantages on retrieval and storage efficiency it is widely used for solving efficient cross-modal retrieval. However existing researches equally handle the different tasks of cross-modal retrieval and simply learn the same couple of hash functions in a symmetric way for them. Under such circumstance the uniqueness of different cross-modal retrieval tasks are ignored and sub-optimal performance may be brought. Motivated by this we present a Task-adaptive Asymmetric Deep Cross-modal Hashing (TA-ADCMH) method in this paper. It can learn task-adaptive hash functions for two sub-retrieval tasks via simultaneous modality representation and asymmetric hash learning. Unlike previous cross-modal hashing approaches our learning framework jointly optimizes semantic preserving that transforms deep features of multimedia data into binary hash codes and the semantic regression which directly regresses query modality representation to explicit label. With our model the binary codes can effectively preserve semantic correlations across different modalities meanwhile adaptively capture the query semantics. The superiority of TA-ADCMH is proved on two standard datasets from many aspects.</p>\n", "tags": ["ARXIV", "Cross Modal", "Supervised"], "tsne_embedding": [-7.421395301818848, -1.773618459701538]}, {"key": "li2021c", "year": "2021", "title": "C-minhash Practically Reducing Two Permutations To Just One", "abstract": "<p>Traditional minwise hashing (MinHash) requires applying K independent permutations to estimate the Jaccard similarity in massive binary (0/1) data where K can be (e.g.) 1024 or even larger depending on applications. The recent work on C-MinHash (Li and Li 2021) has shown with rigorous proofs that only two permutations are needed. An initial permutation is applied to break whatever structures which might exist in the data and a second permutation is re-used K times to produce K hashes via a circulant shifting fashion. (Li and Li 2021) has proved that perhaps surprisingly even though the K hashes are correlated the estimation variance is strictly smaller than the variance of the traditional MinHash. It has been demonstrated in (Li and Li 2021) that the initial permutation in C-MinHash is indeed necessary. For the ease of theoretical analysis they have used two independent permutations. In this paper we show that one can actually simply use one permutation. That is one single permutation is used for both the initial pre-processing step to break the structures in the data and the circulant hashing step to generate K hashes. Although the theoretical analysis becomes very complicated we are able to explicitly write down the expression for the expectation of the estimator. The new estimator is no longer unbiased but the bias is extremely small and has essentially no impact on the estimation accuracy (mean square errors). An extensive set of experiments are provided to verify our claim for using just one permutation.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-32.20024490356445, -8.910149574279785]}, {"key": "andoni2006near", "year": "2021", "title": "Deep Unsupervised Image Hashing by Maximizing Bit Entropy", "abstract": "<p>Unsupervised hashing is important for indexing huge image or video collections without having expensive annotations available. Hashing aims to learn short binary codes for compact storage and efficient semantic retrieval. We propose an unsupervised deep hashing layer called Bi-half Net that maximizes entropy of the binary codes. Entropy is maximal when both possible values of the bit are uniformly (half-half) distributed. To maximize bit entropy, we do not add a term to the loss function as this is difficult to optimize and tune. Instead, we design a new parameter-free network layer to explicitly force continuous image features to approximate the optimal half-half bit distribution. This layer is shown to minimize a penalized term of the Wasserstein distance between the learned continuous image features and the optimal half-half bit distribution. Experimental results on the image datasets Flickr25k, Nus-wide, Cifar-10, Mscoco, Mnist and the video datasets Ucf-101 and Hmdb-51 show that our approach leads to compact codes and compares favorably to the current state-of-the-art.</p>\n", "tags": ["AAAI", "Has Code", "Image Retrieval", "Supervised"], "tsne_embedding": [-13.303868293762207, -26.630508422851562]}, {"key": "li2022adaptive", "year": "2022", "title": "Adaptive Structural Similarity Preserving For Unsupervised Cross Modal Hashing", "abstract": "<p>Cross-modal hashing is an important approach for multimodal data management and application. Existing unsupervised cross-modal hashing algorithms mainly rely on data features in pre-trained models to mine their similarity relationships. However their optimization objectives are based on the static metric between the original uni-modal features without further exploring data correlations during the training. In addition most of them mainly focus on association mining and alignment among pairwise instances in continuous space but ignore the latent structural correlations contained in the semantic hashing space. In this paper we propose an unsupervised hash learning framework namely Adaptive Structural Similarity Preservation Hashing (ASSPH) to solve the above problems. Firstly we propose an adaptive learning scheme with limited data and training batches to enrich semantic correlations of unlabeled instances during the training process and meanwhile to ensure a smooth convergence of the training process. Secondly we present an asymmetric structural semantic representation learning scheme. We introduce structural semantic metrics based on graph adjacency relations during the semantic reconstruction and correlation mining stage and meanwhile align the structure semantics in the hash space with an asymmetric binary optimization process. Finally we conduct extensive experiments to validate the enhancements of our work in comparison with existing works.</p>\n", "tags": ["ARXIV", "Cross Modal", "Graph", "Unsupervised"], "tsne_embedding": [-3.902803421020508, -2.223339319229126]}, {"key": "li2022asymmetric", "year": "2022", "title": "Asymmetric Scalable Cross-modal Hashing", "abstract": "<p>Cross-modal hashing is a successful method to solve large-scale multimedia retrieval issue. A lot of matrix factorization-based hashing methods are proposed. However the existing methods still struggle with a few problems such as how to generate the binary codes efficiently rather than directly relax them to continuity. In addition most of the existing methods choose to use an n(times) n similarity matrix for optimization which makes the memory and computation unaffordable. In this paper we propose a novel Asymmetric Scalable Cross-Modal Hashing (ASCMH) to address these issues. It firstly introduces a collective matrix factorization to learn a common latent space from the kernelized features of different modalities and then transforms the similarity matrix optimization to a distance-distance difference problem minimization with the help of semantic labels and common latent space. Hence the computational complexity of the n(times) n asymmetric optimization is relieved. In the generation of hash codes we also employ an orthogonal constraint of label information which is indispensable for search accuracy. So the redundancy of computation can be much reduced. For efficient optimization and scalable to large-scale datasets we adopt the two-step approach rather than optimizing simultaneously. Extensive experiments on three benchmark datasets Wiki MIRFlickr-25K and NUS-WIDE demonstrate that our ASCMH outperforms the state-of-the-art cross-modal hashing methods in terms of accuracy and efficiency.</p>\n", "tags": ["ARXIV", "Cross Modal", "Independent"], "tsne_embedding": [-9.24328327178955, -3.2428345680236816]}, {"key": "li2022competition", "year": "2022", "title": "Competition-level Code Generation With Alphacode", "abstract": "<p>Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and accessible yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code and are now able to complete simple programming tasks. However these models still perform poorly when evaluated on more complex unseen problems that require problem-solving skills beyond simply translating instructions into code. For example competitive programming problems which require an understanding of algorithms and complex natural language remain extremely challenging. To address this gap we introduce AlphaCode a system for code generation that can create novel solutions to these problems that require deeper reasoning. In simulated evaluations on recent programming competitions on the Codeforces platform AlphaCode achieved on average a ranking of top 54.337; in competitions with more than 5000 participants. We found that three key components were critical to achieve good and reliable performance (1) an extensive and clean competitive programming dataset for training and evaluation (2) large and efficient-to-sample transformer-based architectures and (3) large-scale model sampling to explore the search space followed by filtering based on program behavior to a small set of submissions.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [26.634136199951172, -13.653221130371094]}, {"key": "li2022making", "year": "2022", "title": "Making Large Language Models Better Reasoners With Step-aware Verifier", "abstract": "<p>Few-shot learning is a challenging task that requires language models to generalize from limited examples. Large language models like GPT-3 and PaLM have made impressive progress in this area but they still face difficulties in reasoning tasks such as GSM8K a benchmark for arithmetic problems. To improve their reasoning skills previous work has proposed to guide the language model with prompts that elicit a series of reasoning steps before giving the final answer achieving a significant improvement on GSM8K from 17.937; to 58.137; in problem-solving rate. In this paper we present DIVERSE (Diverse Verifier on Reasoning Step) a novel approach that further enhances the reasoning capability of language models. DIVERSE has three main components first it generates diverse prompts to explore different reasoning paths for the same question; second it uses a verifier to filter out incorrect answers based on a weighted voting scheme; and third it verifies each reasoning step individually instead of the whole chain. We evaluate DIVERSE on the latest language model code-davinci-002 and show that it achieves new state-of-the-art results on six of eight reasoning benchmarks (e.g. GSM8K 74.437; to 83.237;).</p>\n", "tags": ["ARXIV"], "tsne_embedding": [30.394615173339844, 4.231785774230957]}, {"key": "li2022self", "year": "2022", "title": "Self-prompting Large Language Models For Zero-shot Open-domain QA", "abstract": "<p>Open-Domain Question Answering (ODQA) aims to answer questions without explicitly providing specific background documents. This task becomes notably challenging in a zero-shot setting where no data is available to train tailored retrieval-reader models. While recent Large Language Models (LLMs) like GPT-3 have demonstrated their effectiveness in zero-shot ODQA using direct prompting methods these methods still fall short of fully harnessing the potential of LLMs when implicitly invoked. In this paper we propose a Self-Prompting framework to explicitly utilize the massive knowledge encoded in the parameters of LLMs and their strong instruction understanding abilities. Concretely we prompt LLMs step by step to generate multiple pseudo QA pairs with background passages and explanations entirely from scratch. These generated elements are then utilized for in-context learning. Experimental results show that our method significantly surpasses previous state-of-the-art zero-shot methods on three widely-used ODQA datasets and even achieves comparable performance with various customized fine-tuned models on full training data. Our code is available at https://github.com/lockon-n/self-prompting.</p>\n", "tags": ["ARXIV", "Has Code", "Supervised"], "tsne_embedding": [27.166200637817383, -1.8134984970092773]}, {"key": "li2023blip", "year": "2023", "title": "BLIP-2 Bootstrapping Language-image Pre-training With Frozen Image Encoders And Large Language Models", "abstract": "<p>The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2 a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks despite having significantly fewer trainable parameters than existing methods. For example our model outperforms Flamingo80B by 8.737; on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the models emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [22.062950134277344, -4.78034782409668]}, {"key": "li2023dual", "year": "2023", "title": "Dual-stream Knowledge-preserving Hashing For Unsupervised Video Retrieval", "abstract": "<p>Unsupervised video hashing usually optimizes binary codes by learning to reconstruct input videos. Such reconstruction constraint spends much effort on frame-level temporal context changes without focusing on video-level global semantics that are more useful for retrieval. Hence we address this problem by decomposing video information into reconstruction-dependent and semantic-dependent information which disentangles the semantic extraction from reconstruction constraint. Specifically we first design a simple dual-stream structure including a temporal layer and a hash layer. Then with the help of semantic similarity knowledge obtained from self-supervision the hash layer learns to capture information for semantic retrieval while the temporal layer learns to capture the information for reconstruction. In this way the model naturally preserves the disentangled semantics into binary codes. Validated by comprehensive experiments our method consistently outperforms the state-of-the-arts on three video benchmarks.</p>\n", "tags": ["ARXIV", "Unsupervised", "Video Retrieval"], "tsne_embedding": [-8.035513877868652, -25.70889663696289]}, {"key": "li2023elegant", "year": "2023", "title": "E4srec An Elegant Effective Efficient Extensible Solution Of Large Language Models For Sequential Recommendation", "abstract": "<p>The recent advancements in Large Language Models (LLMs) have sparked interest in harnessing their potential within recommender systems. Since LLMs are designed for natural language tasks existing recommendation approaches have predominantly transformed recommendation tasks into open-domain natural language generation tasks. However this approach necessitates items to possess rich semantic information often generates out-of-range results and suffers from notably low efficiency and limited extensibility. Furthermore practical ID-based recommendation strategies reliant on a huge number of unique identities (IDs) to represent users and items have gained prominence in real-world recommender systems due to their effectiveness and efficiency. Nevertheless the incapacity of LLMs to model IDs presents a formidable challenge when seeking to leverage LLMs for personalized recommendations. In this paper we introduce an Elegant Effective Efficient Extensible solution for large language models for Sequential Recommendation (E4SRec) which seamlessly integrates LLMs with traditional recommender systems that exclusively utilize IDs to represent items. Specifically E4SRec takes ID sequences as inputs ensuring that the generated outputs fall within the candidate lists. Furthermore E4SRec possesses the capability to generate the entire ranking list in a single forward process and demands only a minimal set of pluggable parameters which are trained for each dataset while keeping the entire LLM frozen. We substantiate the effectiveness efficiency and extensibility of our proposed E4SRec through comprehensive experiments conducted on four widely-used real-world datasets. The implementation code is accessible at https://github.com/HestiaSky/E4SRec/.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [42.09956359863281, -6.513792991638184]}, {"key": "li2023evaluating", "year": "2023", "title": "Evaluating Object Hallucination In Large Vision-language Models", "abstract": "<p>Inspired by the superior language abilities of large language models (LLM) large vision-language models (LVLM) have been recently explored by integrating powerful LLMs for improving the performance on complex multimodal tasks. Despite the promising progress on LVLMs we find that LVLMs suffer from the hallucination problem i.e. they tend to generate objects that are inconsistent with the target images in the descriptions. To investigate it this work presents the first systematic study on object hallucination of LVLMs. We conduct the evaluation experiments on several representative LVLMs and show that they mostly suffer from severe object hallucination issue. We further discuss that the visual instructions may influence the hallucination and find that objects that frequently occur in the visual instructions or co-occur with the image objects are obviously prone to be hallucinated by LVLMs. Besides we find that existing evaluation methods might be affected by the input instructions and generation styles of LVLMs. Thus we further design an improved evaluation method for object hallucination by proposing a polling-based query method called POPE. Experiment results demonstrate that our POPE can evaluate the object hallucination in a more stable and flexible way. Our codes and data are publicly available at https://github.com/RUCAIBox/POPE.</p>\n", "tags": ["ARXIV", "Cross Modal", "Has Code"], "tsne_embedding": [50.7338981628418, 7.205543041229248]}, {"key": "li2023large", "year": "2023", "title": "Large Language Models For Generative Recommendation A Survey And Visionary Discussions", "abstract": "<p>Large language models (LLM) not only have revolutionized the field of natural language processing (NLP) but also have the potential to reshape many other fields e.g. recommender systems (RS). However most of the related work treats an LLM as a component of the conventional recommendation pipeline (e.g. as a feature extractor) which may not be able to fully leverage the generative power of LLM. Instead of separating the recommendation process into multiple stages such as score computation and re-ranking this process can be simplified to one stage with LLM directly generating recommendations from the complete pool of items. This survey reviews the progress methods and future directions of LLM-based generative recommendation by examining three questions 1) What generative recommendation is 2) Why RS should advance to generative recommendation and 3) How to implement LLM-based generative recommendation for various RS tasks. We hope that this survey can provide the context and guidance needed to explore this interesting and emerging topic.</p>\n", "tags": ["ARXIV", "Survey Paper"], "tsne_embedding": [40.19527053833008, -8.976367950439453]}, {"key": "li2023locality", "year": "2023", "title": "Locality Preserving Multiview Graph Hashing For Large Scale Remote Sensing Image Search", "abstract": "<p>Hashing is very popular for remote sensing image search. This article proposes a multiview hashing with learnable parameters to retrieve the queried images for a large-scale remote sensing dataset. Existing methods always neglect that real-world remote sensing data lies on a low-dimensional manifold embedded in high-dimensional ambient space. Unlike previous methods this article proposes to learn the consensus compact codes in a view-specific low-dimensional subspace. Furthermore we have added a hyperparameter learnable module to avoid complex parameter tuning. In order to prove the effectiveness of our method we carried out experiments on three widely used remote sensing data sets and compared them with seven state-of-the-art methods. Extensive experiments show that the proposed method can achieve competitive results compared to the other method.</p>\n", "tags": ["ARXIV", "Cross Modal", "Graph"], "tsne_embedding": [10.57509994506836, 14.911334991455078]}, {"key": "li2023preliminary", "year": "2023", "title": "A Preliminary Study Of Chatgpt On News Recommendation Personalization Provider Fairness Fake News", "abstract": "<p>Online news platforms commonly employ personalized news recommendation methods to assist users in discovering interesting articles and many previous works have utilized language model techniques to capture user interests and understand news content. With the emergence of large language models like GPT-3 and T-5 a new recommendation paradigm has emerged leveraging pre-trained language models for making recommendations. ChatGPT with its user-friendly interface and growing popularity has become a prominent choice for text-based tasks. Considering the growing reliance on ChatGPT for language tasks the importance of news recommendation in addressing social issues and the trend of using language models in recommendations this study conducts an initial investigation of ChatGPTs performance in news recommendations focusing on three perspectives personalized news recommendation news provider fairness and fake news detection. ChatGPT has the limitation that its output is sensitive to the input phrasing. We therefore aim to explore the constraints present in the generated responses of ChatGPT for each perspective. Additionally we investigate whether specific prompt formats can alleviate these constraints or if these limitations require further attention from researchers in the future. We also surpass fixed evaluations by developing a webpage to monitor ChatGPTs performance on weekly basis on the tasks and prompts we investigated. Our aim is to contribute to and encourage more researchers to engage in the study of enhancing news recommendation performance through the utilization of large language models such as ChatGPT.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [46.84544372558594, -5.327921390533447]}, {"key": "li2023self", "year": "2023", "title": "Self-checker Plug-and-play Modules For Fact-checking With Large Language Models", "abstract": "<p>Fact-checking is an essential task in NLP that is commonly utilized for validating the factual accuracy of claims. Prior work has mainly focused on fine-tuning pre-trained languages models on specific datasets which can be computationally intensive and time-consuming. With the rapid development of large language models (LLMs) such as ChatGPT and GPT-3 researchers are now exploring their in-context learning capabilities for a wide range of tasks. In this paper we aim to assess the capacity of LLMs for fact-checking by introducing Self-Checker a framework comprising a set of plug-and-play modules that facilitate fact-checking by purely prompting LLMs in an almost zero-shot setting. This framework provides a fast and efficient way to construct fact-checking systems in low-resource environments. Empirical results demonstrate the potential of Self-Checker in utilizing LLMs for fact-checking. However there is still significant room for improvement compared to SOTA fine-tuned models which suggests that LLM adoption could be a promising approach for future fact-checking research.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [28.76462745666504, -5.039320468902588]}, {"key": "li2024mend", "year": "2024", "title": "MEND Meta Demonstration Distillation For Efficient And Effective In-context Learning", "abstract": "<p>Large Language models (LLMs) have demonstrated impressive in-context learning (ICL) capabilities where a LLM makes predictions for a given test input together with a few input-output pairs (demonstrations). Nevertheless the inclusion of demonstrations leads to a quadratic increase in the computational overhead of the self-attention mechanism. Existing solutions attempt to distill lengthy demonstrations into compact vectors. However they often require task-specific retraining or compromise LLMs in-context learning performance. To mitigate these challenges we present Meta dEmonstratioN Distillation (MEND) where a language model learns to distill any lengthy demonstrations into vectors without retraining for a new downstream task. We exploit the knowledge distillation to enhance alignment between MEND and LLM achieving both efficiency and effectiveness simultaneously. MEND is endowed with the meta-knowledge of distilling demonstrations through a two-stage training process which includes meta-distillation pretraining and fine-tuning. Comprehensive evaluations across seven diverse ICL task partitions using decoder-only (GPT-2) and encoder-decoder (T5) attest to MENDs prowess. It not only matches but often outperforms the Vanilla ICL as well as other state-of-the-art distillation models while significantly reducing the computational demands. This innovation promises enhanced scalability and efficiency for the practical deployment of large language models</p>\n", "tags": ["ARXIV"], "tsne_embedding": [26.851009368896484, -10.541802406311035]}, {"key": "lian2023explainable", "year": "2023", "title": "Explainable Multimodal Emotion Recognition", "abstract": "<p>Multimodal emotion recognition is an important research topic in artificial intelligence whose main goal is to integrate multimodal clues to identify human emotional states. Current works generally assume accurate labels for benchmark datasets and focus on developing more effective architectures. However emotion annotation relies on subjective judgment. To obtain more reliable labels existing datasets usually restrict the label space to some basic categories then hire plenty of annotators and use majority voting to select the most likely label. However this process may result in some correct but non-candidate or non-majority labels being ignored. To ensure reliability without ignoring subtle emotions we propose a new task called Explainable Multimodal Emotion Recognition (EMER). Unlike traditional emotion recognition EMER takes a step further by providing explanations for these predictions. Through this task we can extract relatively reliable labels since each label has a certain basis. Meanwhile we borrow large language models (LLMs) to disambiguate unimodal clues and generate more complete multimodal explanations. From them we can extract richer emotions in an open-vocabulary manner. This paper presents our initial attempt at this task including introducing a new dataset establishing baselines and defining evaluation metrics. In addition EMER can serve as a benchmark task to evaluate the audio-video-text understanding performance of multimodal LLMs.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [44.35552215576172, 3.5762715339660645]}, {"key": "liang2022holistic", "year": "2022", "title": "Holistic Evaluation Of Language Models", "abstract": "<p>Language models (LMs) are becoming the foundation for almost all major language technologies but their capabilities limitations and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility noting whats missing or underrepresented (e.g. question answering for neglected English dialects metrics for trustworthiness). Second we adopt a multi-metric approach We measure 7 metrics (accuracy calibration robustness fairness bias toxicity and efficiency) for each of 16 core scenarios when possible (87.537; of the time). This ensures metrics beyond accuracy dont fall to the wayside and that trade-offs are clearly exposed. We also perform 7 targeted evaluations based on 26 targeted scenarios to analyze specific aspects (e.g. reasoning disinformation). Third we conduct a large-scale evaluation of 30 prominent language models (spanning open limited-access and closed models) on all 42 scenarios 21 of which were not previously used in mainstream LM evaluation. Prior to HELM models on average were evaluated on just 17.937; of the core HELM scenarios with some prominent models not sharing a single scenario in common. We improve this to 96.037; now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency we release all raw model prompts and completions publicly for further analysis as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community continuously updated with new scenarios metrics and models.</p>\n", "tags": [], "tsne_embedding": [43.55419921875, 4.392578601837158]}, {"key": "liao2023llara", "year": "2023", "title": "Llara Large Language-recommendation Assistant", "abstract": "<p>Sequential recommendation aims to predict users next interaction with items based on their past engagement sequence. Recently the advent of Large Language Models (LLMs) has sparked interest in leveraging them for sequential recommendation viewing it as language modeling. Previous studies represent items within LLMs input prompts as either ID indices or textual metadata. However these approaches often fail to either encapsulate comprehensive world knowledge or exhibit sufficient behavioral understanding. To combine the complementary strengths of conventional recommenders in capturing behavioral patterns of users and LLMs in encoding world knowledge about items we introduce Large Language-Recommendation Assistant (LLaRA). Specifically it uses a novel hybrid prompting method that integrates ID-based item embeddings learned by traditional recommendation models with textual item features. Treating the sequential behaviors of users as a distinct modality beyond texts we employ a projector to align the traditional recommenders ID embeddings with the LLMs input space. Moreover rather than directly exposing the hybrid prompt to LLMs a curriculum learning strategy is adopted to gradually ramp up training complexity. Initially we warm up the LLM using text-only prompts which better suit its inherent language modeling ability. Subsequently we progressively transition to the hybrid prompts training the model to seamlessly incorporate the behavioral knowledge from the traditional sequential recommender into the LLM. Empirical results validate the effectiveness of our proposed framework. Codes are available at https://github.com/ljy0ustc/LLaRA.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [41.011962890625, -10.194196701049805]}, {"key": "lieber2024jamba", "year": "2024", "title": "Jamba A Hybrid Transformer-mamba Language Model", "abstract": "<p>We present Jamba a new base large language model based on a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically Jamba interleaves blocks of Transformer and Mamba layers enjoying the benefits of both model families. MoE is added in some of these layers to increase model capacity while keeping active parameter usage manageable. This flexible architecture allows resource- and objective-specific configurations. In the particular configuration we have implemented we end up with a powerful model that fits in a single 80GB GPU. Built at large scale Jamba provides high throughput and small memory footprint compared to vanilla Transformers and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations. Remarkably the model presents strong results for up to 256K tokens context length. We study various architectural decisions such as how to combine Transformer and Mamba layers and how to mix experts and show that some of them are crucial in large scale modeling. We also describe several interesting properties of these architectures which the training and evaluation of Jamba have revealed and plan to release checkpoints from various ablation runs to encourage further exploration of this novel architecture. We make the weights of our implementation of Jamba publicly available under a permissive license.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [19.282669067382812, -11.205828666687012]}, {"key": "limasset2017fast", "year": "2017", "title": "Fast And Scalable Minimal Perfect Hashing For Massive Key Sets", "abstract": "<p>Minimal perfect hash functions provide space-efficient and collision-free hashing on static sets. Existing algorithms and implementations that build such functions have practical limitations on the number of input elements they can process due to high construction time RAM or external memory usage. We revisit a simple algorithm and show that it is highly competitive with the state of the art especially in terms of construction time and memory usage. We provide a parallel C++ implementation called BBhash. It is capable of creating a minimal perfect hash function of 10^10 elements in less than 7 minutes using 8 threads and 5 GB of memory and the resulting function uses 3.7 bits/element. To the best of our knowledge this is also the first implementation that has been successfully tested on an input of cardinality 10^12. Source code https://github.com/rizkg/BBHash\u201d</p>\n", "tags": ["ARXIV", "Has Code", "Independent"], "tsne_embedding": [-26.28615379333496, -19.197078704833984]}, {"key": "lin2012density", "year": "2012", "title": "Density Sensitive Hashing", "abstract": "<p>Nearest neighbors search is a fundamental problem in various research fields like machine learning data mining and pattern recognition. Recently hashing-based approaches e.g. Locality Sensitive Hashing (LSH) are proved to be effective for scalable high dimensional nearest neighbors search. Many hashing algorithms found their theoretic root in random projection. Since these algorithms generate the hash tables (projections) randomly a large number of hash tables (i.e. long codewords) are required in order to achieve both high precision and recall. To address this limitation we propose a novel hashing algorithm called em Density Sensitive Hashing (DSH) in this paper. DSH can be regarded as an extension of LSH. By exploring the geometric structure of the data DSH avoids the purely random projections selection and uses those projective functions which best agree with the distribution of the data. Extensive experimental results on real-world data sets have shown that the proposed method achieves better performance compared to the state-of-the-art hashing approaches.</p>\n", "tags": ["ARXIV", "Independent", "LSH"], "tsne_embedding": [-28.408002853393555, 3.232675075531006]}, {"key": "lin2013general", "year": "2013", "title": "A General Two-step Approach To Learning-based Hashing", "abstract": "<p>Most existing approaches to hashing apply a single form of hash function and an optimization process which is typically deeply coupled to this specific form. This tight coupling restricts the flexibility of the method to respond to the data and can result in complex optimization problems that are difficult to solve. Here we propose a flexible yet simple framework that is able to accommodate different types of loss functions and hash functions. This framework allows a number of existing approaches to hashing to be placed in context and simplifies the development of new problem-specific hashing methods. Our framework decomposes hashing learning problem into two steps hash bit learning and hash function learning based on the learned bits. The first step can typically be formulated as binary quadratic problems and the second step can be accomplished by training standard binary classifiers. Both problems have been extensively studied in the literature. Our extensive experiments demonstrate that the proposed framework is effective flexible and outperforms the state-of-the-art.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-0.3161379098892212, -14.432297706604004]}, {"key": "lin2013twostep", "year": "2013", "title": "A General Two-Step Approach to Learning-Based Hashing", "abstract": "<p>Most existing approaches to hashing apply a single form of hash function, and an optimization process which\nis typically deeply coupled to this specific form. This tight coupling restricts the flexibility of the method to\nrespond to the data, and can result in complex optimization problems that are difficult to solve. Here we propose\na flexible yet simple framework that is able to accommodate different types of loss functions and hash functions.\nThis framework allows a number of existing approaches to hashing to be placed in context, and simplifies the\ndevelopment of new problem-specific hashing methods. Our framework decomposes hashing learning problem\ninto two steps: hash bit learning and hash function learning based on the learned bits. The first step can typically\nbe formulated as binary quadratic problems, and the second step can be accomplished by training standard binary\nclassifiers. Both problems have been extensively studied in the literature. Our extensive experiments demonstrate\nthat the proposed framework is effective, flexible and outperforms the state-of-the-art.</p>\n", "tags": [], "tsne_embedding": [-0.3175785541534424, -14.408849716186523]}, {"key": "lin2014fast", "year": "2014", "title": "Fast supervised hashing with decision trees for high-dimensional data", "abstract": "<p>Supervised hashing aims to map the original features to\ncompact binary codes that are able to preserve label based\nsimilarity in the Hamming space. Non-linear hash functions\nhave demonstrated their advantage over linear ones due to\ntheir powerful generalization capability. In the literature,\nkernel functions are typically used to achieve non-linearity\nin hashing, which achieve encouraging retrieval performance at the price of slow evaluation and training time.\nHere we propose to use boosted decision trees for achieving\nnon-linearity in hashing, which are fast to train and evaluate, hence more suitable for hashing with high dimensional\ndata. In our approach, we first propose sub-modular formulations for the hashing binary code inference problem\nand an efficient GraphCut based block search method for\nsolving large-scale inference.\nThen we learn hash functions by training boosted decision trees to fit the binary\ncodes. Experiments demonstrate that our proposed method\nsignificantly outperforms most state-of-the-art methods in\nretrieval precision and training time. Especially for highdimensional data, our method is orders of magnitude faster\nthan many methods in terms of training time.</p>\n", "tags": ["CVPR", "Has Code", "Supervised"], "tsne_embedding": [-0.3112320601940155, -6.721473693847656]}, {"key": "lin2014optimising", "year": "2014", "title": "Optimizing Ranking Measures for Compact Binary Code Learning", "abstract": "<p>Hashing has proven a valuable tool for large-scale information retrieval. Despite much success, existing hashing methods optimize over simple objectives such as the reconstruction error or graph Laplacian related loss functions, instead of the performance evaluation criteria of interest\u2014multivariate performance measures such as the AUC and NDCG. Here we present a general framework (termed StructHash) that allows one to directly optimize multivariate performance measures.\nThe resulting optimization problem can involve exponentially or infinitely many variables and constraints, which is more challenging than standard structured output learning. To solve the StructHash optimization problem, we use a combination of column generation and cutting-plane techniques. We demonstrate the generality of StructHash by applying it to ranking prediction and image retrieval, and show that it outperforms a few state-of-the-art hashing methods.</p>\n", "tags": ["ECCV", "Image Retrieval"], "tsne_embedding": [-22.239124298095703, -10.660130500793457]}, {"key": "lin2014optimizing", "year": "2014", "title": "Optimizing Ranking Measures For Compact Binary Code Learning", "abstract": "<p>Hashing has proven a valuable tool for large-scale information retrieval. Despite much success existing hashing methods optimize over simple objectives such as the reconstruction error or graph Laplacian related loss functions instead of the performance evaluation criteria of interest\u2014multivariate performance measures such as the AUC and NDCG. Here we present a general framework (termed StructHash) that allows one to directly optimize multivariate performance measures. The resulting optimization problem can involve exponentially or infinitely many variables and constraints which is more challenging than standard structured output learning. To solve the StructHash optimization problem we use a combination of column generation and cutting-plane techniques. We demonstrate the generality of StructHash by applying it to ranking prediction and image retrieval and show that it outperforms a few state-of-the-art hashing methods.</p>\n", "tags": ["ARXIV", "Deep Learning", "Graph", "Image Retrieval"], "tsne_embedding": [-22.21438980102539, -10.662516593933105]}, {"key": "lin2014supervised", "year": "2014", "title": "Supervised Hashing Using Graph Cuts And Boosted Decision Trees", "abstract": "<p>Embedding image features into a binary Hamming space can improve both the speed and accuracy of large-scale query-by-example image retrieval systems. Supervised hashing aims to map the original features to compact binary codes in a manner which preserves the label-based similarities of the original data. Most existing approaches apply a single form of hash function and an optimization process which is typically deeply coupled to this specific form. This tight coupling restricts the flexibility of those methods and can result in complex optimization problems that are difficult to solve. In this work we proffer a flexible yet simple framework that is able to accommodate different types of loss functions and hash functions. The proposed framework allows a number of existing approaches to hashing to be placed in context and simplifies the development of new problem-specific hashing methods. Our framework decomposes the into two steps binary code (hash bits) learning and hash function learning. The first step can typically be formulated as a binary quadratic problem and the second step can be accomplished by training standard binary classifiers. For solving large-scale binary code inference we show how to ensure that the binary quadratic problems are submodular such that an efficient graph cut approach can be used. To achieve efficiency as well as efficacy on large-scale high-dimensional data we propose to use boosted decision trees as the hash functions which are nonlinear highly descriptive and very fast to train and evaluate. Experiments demonstrate that our proposed method significantly outperforms most state-of-the-art methods especially on high-dimensional data.</p>\n", "tags": ["ARXIV", "Graph", "Image Retrieval", "Supervised"], "tsne_embedding": [-1.2725625038146973, -0.44223371148109436]}, {"key": "lin2015deep", "year": "2015", "title": "Deep learning of binary hash codes for fast image retrieval", "abstract": "<p>Approximate nearest neighbor search is an efficient strategy for large-scale image retrieval. Encouraged by the recent advances in convolutional neural networks (CNNs), we propose an effective deep learning framework to generate binary hash codes for fast image retrieval. Our idea is that when the data labels are available, binary codes can be learned by employing a hidden layer for representing the latent concepts that dominate the class labels.\nhe utilization of the CNN also allows for learning image representations. Unlike other supervised methods that require pair-wised inputs for binary code learning, our method learns hash codes and image representations in a point-wised manner, making it suitable for large-scale datasets. Experimental results show that our method outperforms several state-of-the-art hashing algorithms on the CIFAR-10 and MNIST datasets. We further demonstrate its scalability and efficacy on a large-scale dataset of 1 million clothing images.</p>\n", "tags": ["CVPR", "Deep Learning", "Image Retrieval", "Supervised"], "tsne_embedding": [4.595757007598877, 13.453964233398438]}, {"key": "lin2015deephash", "year": "2015", "title": "Deephash Getting Regularization Depth And Fine-tuning Right", "abstract": "<p>This work focuses on representing very high-dimensional global image descriptors using very compact 64-1024 bit binary hashes for instance retrieval. We propose DeepHash a hashing scheme based on deep networks. Key to making DeepHash work at extremely low bitrates are three important considerations \u2013 regularization depth and fine-tuning \u2013 each requiring solutions specific to the hashing problem. In-depth evaluation shows that our scheme consistently outperforms state-of-the-art methods across all data sets for both Fisher Vectors and Deep Convolutional Neural Network features by up to 20 percent over other schemes. The retrieval performance with 256-bit hashes is close to that of the uncompressed floating point features \u2013 a remarkable 512 times compression.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [-16.876981735229492, 5.102524280548096]}, {"key": "lin2015implicit", "year": "2015", "title": "Implicit Sparse Code Hashing", "abstract": "<p>We address the problem of converting large-scale high-dimensional image data into binary codes so that approximate nearest-neighbor search over them can be efficiently performed. Different from most of the existing unsupervised approaches for yielding binary codes our method is based on a dimensionality-reduction criterion that its resulting mapping is designed to preserve the image relationships entailed by the inner products of sparse codes rather than those implied by the Euclidean distances in the ambient space. While the proposed formulation does not require computing any sparse codes the underlying computation model still inevitably involves solving an unmanageable eigenproblem when extremely high-dimensional descriptors are used. To overcome the difficulty we consider the column-sampling technique and presume a special form of rotation matrix to facilitate subproblem decomposition. We test our method on several challenging image datasets and demonstrate its effectiveness by comparing with state-of-the-art binary coding techniques.</p>\n", "tags": ["ARXIV", "Unsupervised"], "tsne_embedding": [-21.139883041381836, 13.060059547424316]}, {"key": "lin2015semantics", "year": "2015", "title": "Semantics-Preserving Hashing for Cross-View Retrieval", "abstract": "<p>With benefits of low storage costs and high query speeds,\nhashing methods are widely researched for efficiently retrieving large-scale data, which commonly contains multiple views, e.g. a news report with images, videos and texts.\nIn this paper, we study the problem of cross-view retrieval\nand propose an effective Semantics-Preserving Hashing\nmethod, termed SePH. Given semantic affinities of training data as supervised information, SePH transforms them\ninto a probability distribution and approximates it with tobe-learnt hash codes in Hamming space via minimizing the\nKullback-Leibler divergence. Then kernel logistic regression with a sampling strategy is utilized to learn the nonlinear projections from features in each view to the learnt\nhash codes. And for any unseen instance, predicted hash\ncodes and their corresponding output probabilities from observed views are utilized to determine its unified hash code,\nusing a novel probabilistic approach. Extensive experiments conducted on three benchmark datasets well demonstrate the effectiveness and reasonableness of SePH.</p>\n", "tags": ["CVPR", "Cross Modal", "Supervised"], "tsne_embedding": [-5.668457508087158, -7.749504566192627]}, {"key": "lin2015tiny", "year": "2015", "title": "Tiny Descriptors For Image Retrieval With Unsupervised Triplet Hashing", "abstract": "<p>A typical image retrieval pipeline starts with the comparison of global descriptors from a large database to find a short list of candidate matches. A good image descriptor is key to the retrieval pipeline and should reconcile two contradictory requirements providing recall rates as high as possible and being as compact as possible for fast matching. Following the recent successes of Deep Convolutional Neural Networks (DCNN) for large scale image classification descriptors extracted from DCNNs are increasingly used in place of the traditional hand crafted descriptors such as Fisher Vectors (FV) with better retrieval performances. Nevertheless the dimensionality of a typical DCNN descriptor \u2013extracted either from the visual feature pyramid or the fully-connected layers\u2013 remains quite high at several thousands of scalar values. In this paper we propose Unsupervised Triplet Hashing (UTH) a fully unsupervised method to compute extremely compact binary hashes \u2013in the 32-256 bits range\u2013 from high-dimensional global descriptors. UTH consists of two successive deep learning steps. First Stacked Restricted Boltzmann Machines (SRBM) a type of unsupervised deep neural nets are used to learn binary embedding functions able to bring the descriptor size down to the desired bitrate. SRBMs are typically able to ensure a very high compression rate at the expense of loosing some desirable metric properties of the original DCNN descriptor space. Then triplet networks a rank learning scheme based on weight sharing nets is used to fine-tune the binary embedding functions to retain as much as possible of the useful metric properties of the original space. A thorough empirical evaluation conducted on multiple publicly available dataset using DCNN descriptors shows that our method is able to significantly outperform state-of-the-art unsupervised schemes in the target bit range.</p>\n", "tags": ["ARXIV", "Deep Learning", "Image Retrieval", "Unsupervised"], "tsne_embedding": [-13.75811767578125, 9.315202713012695]}, {"key": "lin2019supervised", "year": "2019", "title": "Supervised Online Hashing Via Similarity Distribution Learning", "abstract": "<p>Online hashing has attracted extensive research attention when facing streaming data. Most online hashing methods learning binary codes based on pairwise similarities of training instances fail to capture the semantic relationship and suffer from a poor generalization in large-scale applications due to large variations. In this paper we propose to model the similarity distributions between the input data and the hashing codes upon which a novel supervised online hashing method dubbed as Similarity Distribution based Online Hashing (SDOH) is proposed to keep the intrinsic semantic relationship in the produced Hamming space. Specifically we first transform the discrete similarity matrix into a probability matrix via a Gaussian-based normalization to address the extremely imbalanced distribution issue. And then we introduce a scaling Student t-distribution to solve the challenging initialization problem and efficiently bridge the gap between the known and unknown distributions. Lastly we align the two distributions via minimizing the Kullback-Leibler divergence (KL-diverence) with stochastic gradient descent (SGD) by which an intuitive similarity constraint is imposed to update hashing model on the new streaming data with a powerful generalizing ability to the past data. Extensive experiments on three widely-used benchmarks validate the superiority of the proposed SDOH over the state-of-the-art methods in the online retrieval task.</p>\n", "tags": ["ARXIV", "Streaming Data", "Supervised"], "tsne_embedding": [-8.891708374023438, 2.0041935443878174]}, {"key": "lin2019towards", "year": "2019", "title": "Towards Optimal Discrete Online Hashing With Balanced Similarity", "abstract": "<p>When facing large-scale image datasets online hashing serves as a promising solution for online retrieval and prediction tasks. It encodes the online streaming data into compact binary codes and simultaneously updates the hash functions to renew codes of the existing dataset. To this end the existing methods update hash functions solely based on the new data batch without investigating the correlation between such new data and the existing dataset. In addition existing works update the hash functions using a relaxation process in its corresponding approximated continuous space. And it remains as an open problem to directly apply discrete optimizations in online hashing. In this paper we propose a novel supervised online hashing method termed Balanced Similarity for Online Discrete Hashing (BSODH) to solve the above problems in a unified framework. BSODH employs a well-designed hashing algorithm to preserve the similarity between the streaming data and the existing dataset via an asymmetric graph regularization. We further identify the data-imbalance problem brought by the constructed asymmetric graph which restricts the application of discrete optimization in our problem. Therefore a novel balanced similarity is further proposed which uses two equilibrium factors to balance the similar and dissimilar weights and eventually enables the usage of discrete optimizations. Extensive experiments conducted on three widely-used benchmarks demonstrate the advantages of the proposed method over the state-of-the-art methods.</p>\n", "tags": ["ARXIV", "Graph", "Streaming Data", "Supervised"], "tsne_embedding": [-5.6660966873168945, 22.033218383789062]}, {"key": "lin2020fast", "year": "2020", "title": "Fast Class-wise Updating For Online Hashing", "abstract": "<p>Online image hashing has received increasing research attention recently which processes large-scale data in a streaming fashion to update the hash functions on-the-fly. To this end most existing works exploit this problem under a supervised setting i.e. using class labels to boost the hashing performance which suffers from the defects in both adaptivity and efficiency First large amounts of training batches are required to learn up-to-date hash functions which leads to poor online adaptivity. Second the training is time-consuming which contradicts with the core need of online learning. In this paper a novel supervised online hashing scheme termed Fast Class-wise Updating for Online Hashing (FCOH) is proposed to address the above two challenges by introducing a novel and efficient inner product operation. To achieve fast online adaptivity a class-wise updating method is developed to decompose the binary code learning and alternatively renew the hash functions in a class-wise fashion which well addresses the burden on large amounts of training batches. Quantitatively such a decomposition further leads to at least 7537; storage saving. To further achieve online efficiency we propose a semi-relaxation optimization which accelerates the online training by treating different binary constraints independently. Without additional constraints and variables the time complexity is significantly reduced. Such a scheme is also quantitatively shown to well preserve past information during updating hashing functions. We have quantitatively demonstrated that the collective effort of class-wise updating and semi-relaxation optimization provides a superior performance comparing to various state-of-the-art methods which is verified through extensive experiments on three widely-used datasets.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [3.734490156173706, -11.757309913635254]}, {"key": "lin2021deep", "year": "2021", "title": "Deep Self-adaptive Hashing For Image Retrieval", "abstract": "<p>Hashing technology has been widely used in image retrieval due to its computational and storage efficiency. Recently deep unsupervised hashing methods have attracted increasing attention due to the high cost of human annotations in the real world and the superiority of deep learning technology. However most deep unsupervised hashing methods usually pre-compute a similarity matrix to model the pairwise relationship in the pre-trained feature space. Then this similarity matrix would be used to guide hash learning in which most of the data pairs are treated equivalently. The above process is confronted with the following defects 1) The pre-computed similarity matrix is inalterable and disconnected from the hash learning process which cannot explore the underlying semantic information. 2) The informative data pairs may be buried by the large number of less-informative data pairs. To solve the aforementioned problems we propose a Deep Self-Adaptive Hashing (DSAH) model to adaptively capture the semantic information with two special designs Adaptive Neighbor Discovery (AND) and Pairwise Information Content (PIC). Firstly we adopt the AND to initially construct a neighborhood-based similarity matrix and then refine this initial similarity matrix with a novel update strategy to further investigate the semantic structure behind the learned representation. Secondly we measure the priorities of data pairs with PIC and assign adaptive weights to them which is relies on the assumption that more dissimilar data pairs contain more discriminative information for hash learning. Extensive experiments on several datasets demonstrate that the above two technologies facilitate the deep hashing model to achieve superior performance.</p>\n", "tags": ["ARXIV", "Deep Learning", "Image Retrieval", "Unsupervised"], "tsne_embedding": [-5.866132736206055, 2.076622486114502]}, {"key": "lin2022deep", "year": "2022", "title": "Deep Unsupervised Hashing With Latent Semantic Components", "abstract": "<p>Deep unsupervised hashing has been appreciated in the regime of image retrieval. However most prior arts failed to detect the semantic components and their relationships behind the images which makes them lack discriminative power. To make up the defect we propose a novel Deep Semantic Components Hashing (DSCH) which involves a common sense that an image normally contains a bunch of semantic components with homology and co-occurrence relationships. Based on this prior DSCH regards the semantic components as latent variables under the Expectation-Maximization framework and designs a two-step iterative algorithm with the objective of maximum likelihood of training data. Firstly DSCH constructs a semantic component structure by uncovering the fine-grained semantics components of images with a Gaussian Mixture Modal~(GMM) where an image is represented as a mixture of multiple components and the semantics co-occurrence are exploited. Besides coarse-grained semantics components are discovered by considering the homology relationships between fine-grained components and the hierarchy organization is then constructed. Secondly DSCH makes the images close to their semantic component centers at both fine-grained and coarse-grained levels and also makes the images share similar semantic components close to each other. Extensive experiments on three benchmark datasets demonstrate that the proposed hierarchical semantic components indeed facilitate the hashing model to achieve superior performance.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Unsupervised"], "tsne_embedding": [-4.373928070068359, 3.6362314224243164]}, {"key": "lin2023bridging", "year": "2023", "title": "Bridging Items And Language A Transition Paradigm For Large Language Model-based Recommendation", "abstract": "<p>Harnessing Large Language Models (LLMs) for recommendation is rapidly emerging which relies on two fundamental steps to bridge the recommendation item space and the language space 1) item indexing utilizes identifiers to represent items in the language space and 2) generation grounding associates LLMs generated token sequences to in-corpus items. However previous methods exhibit inherent limitations in the two steps. Existing ID-based identifiers (e.g. numeric IDs) and description-based identifiers (e.g. titles) either lose semantics or lack adequate distinctiveness. Moreover prior generation grounding methods might generate invalid identifiers thus misaligning with in-corpus items. To address these issues we propose a novel Transition paradigm for LLM-based Recommender (named TransRec) to bridge items and language. Specifically TransRec presents multi-facet identifiers which simultaneously incorporate ID title and attribute for item indexing to pursue both distinctiveness and semantics. Additionally we introduce a specialized data structure for TransRec to ensure generating valid identifiers only and utilize substring indexing to encourage LLMs to generate from any position of identifiers. Lastly TransRec presents an aggregated grounding module to leverage generated multi-facet identifiers to rank in-corpus items efficiently. We instantiate TransRec on two backbone models BART-large and LLaMA-7B. Extensive results on three real-world datasets under diverse settings validate the superiority of TransRec.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [41.178863525390625, -7.924516677856445]}, {"key": "lin2023how", "year": "2023", "title": "How Can Recommender Systems Benefit From Large Language Models A Survey", "abstract": "<p>With the rapid development of online services recommender systems (RS) have become increasingly indispensable for mitigating information overload. Despite remarkable progress conventional recommendation models (CRM) still have some limitations e.g. lacking open-world knowledge and difficulties in comprehending users underlying preferences and motivations. Meanwhile large language models (LLM) have shown impressive general intelligence and human-like capabilities which mainly stem from their extensive open-world knowledge reasoning ability as well as their comprehension of human culture and society. Consequently the emergence of LLM is inspiring the design of recommender systems and pointing out a promising research direction i.e. whether we can incorporate LLM and benefit from their knowledge and capabilities to compensate for the limitations of CRM. In this paper we conduct a comprehensive survey on this research direction from the perspective of the whole pipeline in real-world recommender systems. Specifically we summarize existing works from two orthogonal aspects where and how to adapt LLM to RS. For the WHERE question we discuss the roles that LLM could play in different stages of the recommendation pipeline i.e. feature engineering feature encoder scoring/ranking function user interaction and pipeline controller. For the HOW question we investigate the training and inference strategies resulting in two fine-grained taxonomy criteria i.e. whether to tune LLM or not and whether to involve conventional recommendation models for inference. Then we highlight key challenges in adapting LLM to RS from three aspects i.e. efficiency effectiveness and ethics. Finally we summarize the survey and discuss the future prospects. We actively maintain a GitHub repository for papers and other related resources https://github.com/CHIANGEL/Awesome-LLM-for-RecSys/.</p>\n", "tags": ["ARXIV", "Has Code", "Survey Paper"], "tsne_embedding": [40.85532760620117, -12.154956817626953]}, {"key": "lin2023llm", "year": "2023", "title": "Llm-eval Unified Multi-dimensional Automatic Evaluation For Open-domain Conversations With Large Language Models", "abstract": "<p>We propose LLM-Eval a unified multi-dimensional automatic evaluation method for open-domain conversations with large language models (LLMs). Existing evaluation methods often rely on human annotations ground-truth responses or multiple LLM prompts which can be expensive and time-consuming. To address these issues we design a single prompt-based evaluation method that leverages a unified evaluation schema to cover multiple dimensions of conversation quality in a single model call. We extensively evaluate the performance of LLM-Eval on various benchmark datasets demonstrating its effectiveness efficiency and adaptability compared to state-of-the-art evaluation methods. Our analysis also highlights the importance of choosing suitable LLMs and decoding strategies for accurate evaluation results. LLM-Eval offers a versatile and robust solution for evaluating open-domain conversation systems streamlining the evaluation process and providing consistent performance across diverse scenarios.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [37.566871643066406, -8.1182861328125]}, {"key": "liong2015using", "year": "2015", "title": "Deep Hashing for Compact Binary Codes Learning", "abstract": "<p>In this paper, we propose a new deep hashing (DH) approach\nto learn compact binary codes for large scale visual\nsearch. Unlike most existing binary codes learning methods\nwhich seek a single linear projection to map each sample\ninto a binary vector, we develop a deep neural network\nto seek multiple hierarchical non-linear transformations to\nlearn these binary codes, so that the nonlinear relationship\nof samples can be well exploited. Our model is learned under\nthree constraints at the top layer of the deep network:\n1) the loss between the original real-valued feature descriptor\nand the learned binary vector is minimized, 2) the binary\ncodes distribute evenly on each bit, and 3) different bits\nare as independent as possible. To further improve the discriminative\npower of the learned binary codes, we extend\nDH into supervised DH (SDH) by including one discriminative\nterm into the objective function of DH which simultaneously\nmaximizes the inter-class variations and minimizes\nthe intra-class variations of the learned binary codes. Experimental\nresults show the superiority of the proposed approach\nover the state-of-the-arts.</p>\n", "tags": ["CVPR", "Deep Learning", "Supervised"], "tsne_embedding": [-1.4862806797027588, 10.41633129119873]}, {"key": "liong2017crossmodal", "year": "2017", "title": "Cross-Modal Deep Variational Hashing", "abstract": "<p>In this paper, we propose a cross-modal deep variational hashing (CMDVH) method for cross-modality multimedia retrieval. Unlike existing cross-modal hashing methods\nwhich learn a single pair of projections to map each example as a binary vector, we design a couple of deep neural\nnetwork to learn non-linear transformations from imagetext input pairs, so that unified binary codes can be obtained. We then design the modality-specific neural networks in a probabilistic manner where we model a latent\nvariable as close as possible from the inferred binary codes,\nwhich is approximated by a posterior distribution regularized by a known prior. Experimental results on three benchmark datasets show the efficacy of the proposed approach.</p>\n", "tags": ["Cross Modal", "Deep Learning", "ICCV"], "tsne_embedding": [-22.767269134521484, 9.682684898376465]}, {"key": "liong2020deep", "year": "2020", "title": "Deep Variational and Structural Hashing", "abstract": "<p>In this paper, we propose a deep variational and structural hashing (DVStH) method to learn compact binary codes for multimedia retrieval. Unlike most existing deep hashing methods which use a series of convolution and fully-connected layers to learn binary features, we develop a probabilistic framework to infer latent feature representation inside the network. Then, we design a struct layer rather than a bottleneck hash layer, to obtain binary codes through a simple encoding procedure. By doing these, we are able to obtain binary codes discriminatively and generatively. To make it applicable to cross-modal scalable multimedia retrieval, we extend our method to a cross-modal deep variational and structural hashing (CM-DVStH). We design a deep fusion network with a struct layer to maximize the correlation between image-text input pairs during the training stage so that a unified binary vector can be obtained. We then design modality-specific hashing networks to handle the out-of-sample extension scenario. Specifically, we train a network for each modality which outputs a latent representation that is as close as possible to the binary codes which are inferred from the fusion network. Experimental results on five benchmark datasets are presented to show the efficacy of the proposed approach.</p>\n", "tags": ["Deep Learning", "TPAMI"], "tsne_embedding": [-11.296293258666992, 6.105167865753174]}, {"key": "lisa2021prefix", "year": "2021", "title": "Prefix-tuning Optimizing Continuous Prompts For Generation", "abstract": "<p>Fine-tuning is the de facto way to leverage large pretrained language models to perform downstream tasks. However it modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper we propose prefix-tuning a lightweight alternative to fine-tuning for natural language generation tasks which keeps language model parameters frozen but optimizes a small continuous task-specific vector (called the prefix). Prefix-tuning draws inspiration from prompting allowing subsequent tokens to attend to this prefix as if it were virtual tokens. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We find that by learning only 0.137; of the parameters prefix-tuning obtains comparable performance in the full data setting outperforms fine-tuning in low-data settings and extrapolates better to examples with topics unseen during training.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [23.281463623046875, -2.6158671379089355]}, {"key": "liu2011learning", "year": "2011", "title": "Hashing with Graphs", "abstract": "<p>Hashing is becoming increasingly popular for\nefficient nearest neighbor search in massive\ndatabases. However, learning short codes\nthat yield good search performance is still\na challenge. Moreover, in many cases realworld\ndata lives on a low-dimensional manifold,\nwhich should be taken into account\nto capture meaningful nearest neighbors. In\nthis paper, we propose a novel graph-based\nhashing method which automatically discovers\nthe neighborhood structure inherent in\nthe data to learn appropriate compact codes.\nTo make such an approach computationally\nfeasible, we utilize Anchor Graphs to obtain\ntractable low-rank adjacency matrices. Our\nformulation allows constant time hashing of a\nnew data point by extrapolating graph Laplacian\neigenvectors to eigenfunctions. Finally,\nwe describe a hierarchical threshold learning\nprocedure in which each eigenfunction yields\nmultiple bits, leading to higher search accuracy.\nExperimental comparison with the\nother state-of-the-art methods on two large\ndatasets demonstrates the efficacy of the proposed\nmethod.</p>\n", "tags": ["Has Code", "ICML"], "tsne_embedding": [-4.7184624671936035, 28.49471092224121]}, {"key": "liu2012compact", "year": "2012", "title": "Compact Hyperplane Hashing With Bilinear Functions", "abstract": "<p>Hyperplane hashing aims at rapidly searching nearest points to a hyperplane and has shown practical impact in scaling up active learning with SVMs. Unfortunately the existing randomized methods need long hash codes to achieve reasonable search accuracy and thus suffer from reduced search speed and large memory overhead. To this end this paper proposes a novel hyperplane hashing technique which yields compact hash codes. The key idea is the bilinear form of the proposed hash functions which leads to higher collision probability than the existing hyperplane hash functions when using random projections. To further increase the performance we propose a learning based framework in which the bilinear functions are directly learned from the data. This results in short yet discriminative codes and also boosts the search performance over the random projection based solutions. Large-scale active learning experiments carried out on two datasets with up to one million samples demonstrate the overall superiority of the proposed approach.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [-24.560441970825195, -8.4579496383667]}, {"key": "liu2012supervised", "year": "2012", "title": "Supervised Hashing with Kernels", "abstract": "<p>Recent years have witnessed the growing popularity of\nhashing in large-scale vision problems. It has been shown\nthat the hashing quality could be boosted by leveraging supervised\ninformation into hash function learning. However,\nthe existing supervised methods either lack adequate performance\nor often incur cumbersome model training. In this\npaper, we propose a novel kernel-based supervised hashing\nmodel which requires a limited amount of supervised information,\ni.e., similar and dissimilar data pairs, and a feasible\ntraining cost in achieving high quality hashing. The idea\nis to map the data to compact binary codes whose Hamming\ndistances are minimized on similar pairs and simultaneously\nmaximized on dissimilar pairs. Our approach is\ndistinct from prior works by utilizing the equivalence between\noptimizing the code inner products and the Hamming\ndistances. This enables us to sequentially and efficiently\ntrain the hash functions one bit at a time, yielding very\nshort yet discriminative codes. We carry out extensive experiments\non two image benchmarks with up to one million\nsamples, demonstrating that our approach significantly outperforms\nthe state-of-the-arts in searching both metric distance\nneighbors and semantically similar neighbors, with\naccuracy gains ranging from 13% to 46%.</p>\n", "tags": [], "tsne_embedding": [-1.2101507186889648, -6.14752721786499]}, {"key": "liu2013hashbit", "year": "2013", "title": "Hash Bit Selection: a Unified Solution for Selection Problems in Hashing", "abstract": "<p>Hashing based methods recently have been shown promising for large-scale nearest neighbor search. However, good designs involve difficult decisions of many unknowns \u2013 data features, hashing algorithms, parameter settings, kernels, etc. In this paper, we provide a unified solution as hash bit selection, i.e., selecting the most informative hash bits from a pool of candidates that may have been generated under various conditions mentioned above. We represent the candidate bit pool as a vertex- and edge-weighted graph with the pooled bits as vertices. Then we formulate the bit selection problem as quadratic programming over the graph, and solve it efficiently by replicator dynamics. Extensive experiments show that our bit selection approach can achieve superior performance over both naive selection methods and state-of-the-art methods under each scenario, usually with significant accuracy gains from 10% to 50% relatively.</p>\n\n", "tags": [], "tsne_embedding": [-3.9214208126068115, 30.498735427856445]}, {"key": "liu2014collaborative", "year": "2014", "title": "Collaborative Hashing", "abstract": "<p>Hashing technique has become a promising approach for\nfast similarity search. Most of existing hashing research\npursue the binary codes for the same type of entities by\npreserving their similarities. In practice, there are many\nscenarios involving nearest neighbor search on the data\ngiven in matrix form, where two different types of, yet\nnaturally associated entities respectively correspond to its\ntwo dimensions or views. To fully explore the duality\nbetween the two views, we propose a collaborative hashing\nscheme for the data in matrix form to enable fast search\nin various applications such as image search using bag of\nwords and recommendation using user-item ratings. By\nsimultaneously preserving both the entity similarities in\neach view and the interrelationship between views, our\ncollaborative hashing effectively learns the compact binary\ncodes and the explicit hash functions for out-of-sample\nextension in an alternating optimization way. Extensive\nevaluations are conducted on three well-known datasets\nfor search inside a single view and search across different\nviews, demonstrating that our proposed method outperforms\nstate-of-the-art baselines, with significant accuracy\ngains ranging from 7.67% to 45.87% relatively.</p>\n", "tags": [], "tsne_embedding": [-17.038837432861328, -0.04971972852945328]}, {"key": "liu2014discrete", "year": "2014", "title": "Discrete Graph Hashing", "abstract": "<p>Hashing has emerged as a popular technique for fast nearest neighbor search in gigantic databases. In particular learning based hashing has received considerable attention due to its appealing storage and search efficiency. However the performance of most unsupervised learning based hashing methods deteriorates rapidly as the hash code length increases. We argue that the degraded performance is due to inferior optimization procedures used to achieve discrete binary codes. This paper presents a graph-based unsupervised hashing model to preserve the neighborhood structure of massive data in a discrete code space. We cast the graph hashing problem into a discrete optimization framework which directly learns the binary codes. A tractable alternating maximization algorithm is then proposed to explicitly deal with the discrete constraints yielding high-quality codes to well capture the local neighborhoods. Extensive experiments performed on four large datasets with up to one million samples show that our discrete optimization based graph hashing method obtains superior search accuracy over state-of-the-art unsupervised hashing methods especially for longer codes.</p>\n", "tags": ["Graph", "NEURIPS", "Unsupervised"], "tsne_embedding": [-4.382436275482178, 28.54634666442871]}, {"key": "liu2015multi", "year": "2015", "title": "Multi-View Complementary Hash Tables for Nearest Neighbor Search", "abstract": "<p>Recent years have witnessed the success of hashing techniques in fast nearest neighbor search. In practice many\napplications (e.g., visual search, object detection, image\nmatching, etc.) have enjoyed the benefits of complementary hash tables and information fusion over multiple views.\nHowever, most of prior research mainly focused on compact hash code cleaning, and rare work studies how to build\nmultiple complementary hash tables, much less to adaptively integrate information stemming from multiple views.\nIn\nthis paper we first present a novel multi-view complementary hash table method that learns complementary hash tables from the data with multiple views. For single multiview table, using exemplar based feature fusion, we approximate the inherent data similarities with a low-rank matrix,\nand learn discriminative hash functions in an efficient way.\nTo build complementary tables and meanwhile maintain scalable training and fast out-of-sample extension, an exemplar reweighting scheme is introduced to update the induced low-rank similarity in the sequential table construction framework, which indeed brings mutual benefits between tables by placing greater importance on exemplars\nshared by mis-separated neighbors. Extensive experiments\non three large-scale image datasets demonstrate that the\nproposed method significantly outperforms various naive\nsolutions and state-of-the-art multi-table methods.</p>\n", "tags": ["Cross Modal", "ICCV", "Image Retrieval"], "tsne_embedding": [-18.750131607055664, -0.31870487332344055]}, {"key": "liu2016accurate", "year": "2016", "title": "Accurate Deep Representation Quantization With Gradient Snapping Layer For Similarity Search", "abstract": "<p>Recent advance of large scale similarity search involves using deeply learned representations to improve the search accuracy and use vector quantization methods to increase the search speed. However how to learn deep representations that strongly preserve similarities between data pairs and can be accurately quantized via vector quantization remains a challenging task. Existing methods simply leverage quantization loss and similarity loss which result in unexpectedly biased back-propagating gradients and affect the search performances. To this end we propose a novel gradient snapping layer (GSL) to directly regularize the back-propagating gradient towards a neighboring codeword the generated gradients are un-biased for reducing similarity loss and also propel the learned representations to be accurately quantized. Joint deep representation and vector quantization learning can be easily performed by alternatively optimize the quantization codebook and the deep neural network. The proposed framework is compatible with various existing vector quantization approaches. Experimental results demonstrate that the proposed framework is effective flexible and outperforms the state-of-the-art large scale similarity search methods.</p>\n", "tags": ["ARXIV", "Quantisation", "Supervised"], "tsne_embedding": [9.069771766662598, 2.3114147186279297]}, {"key": "liu2016dual", "year": "2016", "title": "Dual Purpose Hashing", "abstract": "<p>Recent years have seen more and more demand for a unified framework to address multiple realistic image retrieval tasks concerning both category and attributes. Considering the scale of modern datasets hashing is favorable for its low complexity. However most existing hashing methods are designed to preserve one single kind of similarity thus improper for dealing with the different tasks simultaneously. To overcome this limitation we propose a new hashing method named Dual Purpose Hashing (DPH) which jointly preserves the category and attribute similarities by exploiting the Convolutional Neural Network (CNN) models to hierarchically capture the correlations between category and attributes. Since images with both category and attribute labels are scarce our method is designed to take the abundant partially labelled images on the Internet as training inputs. With such a framework the binary codes of new-coming images can be readily obtained by quantizing the network outputs of a binary-like layer and the attributes can be recovered from the codes easily. Experiments on two large-scale datasets show that our dual purpose hash codes can achieve comparable or even better performance than those state-of-the-art methods specifically designed for each individual retrieval task while being more compact than the compared methods.</p>\n", "tags": ["ARXIV", "CNN", "Image Retrieval", "Supervised"], "tsne_embedding": [1.3237518072128296, 7.325031757354736]}, {"key": "liu2016ordinal", "year": "2016", "title": "Ordinal Constrained Binary Code Learning For Nearest Neighbor Search", "abstract": "<p>Recent years have witnessed extensive attention in binary code learning a.k.a. hashing for nearest neighbor search problems. It has been seen that high-dimensional data points can be quantized into binary codes to give an efficient similarity approximation via Hamming distance. Among existing schemes ranking-based hashing is recent promising that targets at preserving ordinal relations of ranking in the Hamming space to minimize retrieval loss. However the size of the ranking tuples which shows the ordinal relations is quadratic or cubic to the size of training samples. By given a large-scale training data set it is very expensive to embed such ranking tuples in binary code learning. Besides it remains a dificulty to build ranking tuples efficiently for most ranking-preserving hashing which are deployed over an ordinal graph-based setting. To handle these problems we propose a novel ranking-preserving hashing method dubbed Ordinal Constraint Hashing (OCH) which efficiently learns the optimal hashing functions with a graph-based approximation to embed the ordinal relations. The core idea is to reduce the size of ordinal graph with ordinal constraint projection which preserves the ordinal relations through a small data set (such as clusters or random samples). In particular to learn such hash functions effectively we further relax the discrete constraints and design a specific stochastic gradient decent algorithm for optimization. Experimental results on three large-scale visual search benchmark datasets i.e. LabelMe Tiny100K and GIST1M show that the proposed OCH method can achieve superior performance over the state-of-the-arts approaches.</p>\n", "tags": ["ARXIV", "Graph", "Supervised"], "tsne_embedding": [7.098787307739258, -3.238844394683838]}, {"key": "liu2016supervised", "year": "2016", "title": "Supervised Matrix Factorization For Cross-modality Hashing", "abstract": "<p>Matrix factorization has been recently utilized for the task of multi-modal hashing for cross-modality visual search where basis functions are learned to map data from different modalities to the same Hamming embedding. In this paper we propose a novel cross-modality hashing algorithm termed Supervised Matrix Factorization Hashing (SMFH) which tackles the multi-modal hashing problem with a collective non-matrix factorization across the different modalities. In particular SMFH employs a well-designed binary code learning algorithm to preserve the similarities among multi-modal original features through a graph regularization. At the same time semantic labels when available are incorporated into the learning procedure. We conjecture that all these would facilitate to preserve the most relevant information during the binary quantization process and hence improve the retrieval accuracy. We demonstrate the superior performance of SMFH on three cross-modality visual search benchmarks i.e. the PASCAL-Sentence Wiki and NUS-WIDE with quantitative comparison to various state-of-the-art methods</p>\n", "tags": ["ARXIV", "Cross Modal", "Graph", "Quantisation", "Supervised"], "tsne_embedding": [-1.7492198944091797, 21.889205932617188]}, {"key": "liu2017deep", "year": "2017", "title": "Deep Hashing With Category Mask For Fast Video Retrieval", "abstract": "<p>This paper proposes an end-to-end deep hashing framework with category mask for fast video retrieval. We train our network in a supervised way by fully exploiting inter-class diversity and intra-class identity. Classification loss is optimized to maximize inter-class diversity while intra-pair is introduced to learn representative intra-class identity. We investigate the binary bits distribution related to categories and find out that the effectiveness of binary bits is highly correlated with data categories and some bits may degrade classification performance of some categories. We then design hash code generation scheme with category mask to filter out bits with negative contribution. Experimental results demonstrate the proposed method outperforms several state-of-the-arts under various evaluation metrics on public datasets.</p>\n", "tags": ["ARXIV", "Supervised", "Video Retrieval"], "tsne_embedding": [-8.99185848236084, -23.275379180908203]}, {"key": "liu2017discretely", "year": "2017", "title": "Discretely Coding Semantic Rank Orders for Supervised Image Hashing", "abstract": "<p>Learning to hash has been recognized to accomplish highly efficient storage and retrieval for large-scale visual data. Particularly, ranking-based hashing techniques have recently attracted broad research attention because ranking accuracy among the retrieved data is well explored and their objective is more applicable to realistic search tasks. However, directly optimizing discrete hash codes without continuous-relaxations on a nonlinear ranking objective is infeasible by either traditional optimization methods or even recent discrete hashing algorithms. To address this challenging issue, in this paper, we introduce a novel supervised hashing method, dubbed Discrete Semantic Ranking Hashing (DSeRH), which aims to directly embed semantic rank orders into binary codes. In DSeRH, a generalized Adaptive Discrete Minimization (ADM) approach is proposed to discretely optimize binary codes with the quadratic nonlinear ranking objective in an iterative manner and is guaranteed to converge quickly. Additionally, instead of using 0/1 independent labels to form rank orders as in previous works, we generate the listwise rank orders from the high-level semantic word embeddings which can quantitatively capture the intrinsic correlation between different categories. We evaluate our DSeRH, coupled with both linear and deep convolutional neural network (CNN) hash functions, on three image datasets, i.e., CIFAR-10, SUN397 and ImageNet100, and the results manifest that DSeRH can outperform the state-of-the-art ranking-based hashing methods.</p>\n", "tags": ["CVPR", "Image Retrieval", "Supervised"], "tsne_embedding": [-5.562595367431641, 13.479279518127441]}, {"key": "liu2017end", "year": "2017", "title": "End-to-end Binary Representation Learning Via Direct Binary Embedding", "abstract": "<p>Learning binary representation is essential to large-scale computer vision tasks. Most existing algorithms require a separate quantization constraint to learn effective hashing functions. In this work we present Direct Binary Embedding (DBE) a simple yet very effective algorithm to learn binary representation in an end-to-end fashion. By appending an ingeniously designed DBE layer to the deep convolutional neural network (DCNN) DBE learns binary code directly from the continuous DBE layer activation without quantization error. By employing the deep residual network (ResNet) as DCNN component DBE captures rich semantics from images. Furthermore in the effort of handling multilabel images we design a joint cross entropy loss that includes both softmax cross entropy and weighted binary cross entropy in consideration of the correlation and independence of labels respectively. Extensive experiments demonstrate the significant superiority of DBE over state-of-the-art methods on tasks of natural object recognition image retrieval and image annotation.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Quantisation", "Supervised"], "tsne_embedding": [-1.1949952840805054, 10.21334171295166]}, {"key": "liu2018discriminative", "year": "2018", "title": "Discriminative Cross-view Binary Representation Learning", "abstract": "<p>Learning compact representation is vital and challenging for large scale multimedia data. Cross-view/cross-modal hashing for effective binary representation learning has received significant attention with exponentially growing availability of multimedia content. Most existing cross-view hashing algorithms emphasize the similarities in individual views which are then connected via cross-view similarities. In this work we focus on the exploitation of the discriminative information from different views and propose an end-to-end method to learn semantic-preserving and discriminative binary representation dubbed Discriminative Cross-View Hashing (DCVH) in light of learning multitasking binary representation for various tasks including cross-view retrieval image-to-image retrieval and image annotation/tagging. The proposed DCVH has the following key components. First it uses convolutional neural network (CNN) based nonlinear hashing functions and multilabel classification for both images and texts simultaneously. Such hashing functions achieve effective continuous relaxation during training without explicit quantization loss by using Direct Binary Embedding (DBE) layers. Second we propose an effective view alignment via Hamming distance minimization which is efficiently accomplished by bit-wise XOR operation. Extensive experiments on two image-text benchmark datasets demonstrate that DCVH outperforms state-of-the-art cross-view hashing algorithms as well as single-view image hashing algorithms. In addition DCVH can provide competitive performance for image annotation/tagging.</p>\n", "tags": ["CNN", "Cross Modal", "Image Retrieval", "Quantisation", "Supervised"], "tsne_embedding": [-0.6610408425331116, 14.800145149230957]}, {"key": "liu2018fusion", "year": "2018", "title": "Fusion Hashing A General Framework For Self-improvement Of Hashing", "abstract": "<p>Hashing has been widely used for efficient similarity search based on its query and storage efficiency. To obtain better precision most studies focus on designing different objective functions with different constraints or penalty terms that consider neighborhood information. In this paper in contrast to existing hashing methods we propose a novel generalized framework called fusion hashing (FH) to improve the precision of existing hashing methods without adding new constraints or penalty terms. In the proposed FH given an existing hashing method we first execute it several times to get several different hash codes for a set of training samples. We then propose two novel fusion strategies that combine these different hash codes into one set of final hash codes. Based on the final hash codes we learn a simple linear hash function for the samples that can significantly improve model precision. In general the proposed FH can be adopted in existing hashing method and achieve more precise and stable performance compared to the original hashing method with little extra expenditure in terms of time and space. Extensive experiments were performed based on three benchmark datasets and the results demonstrate the superior performance of the proposed framework</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [3.2749006748199463, -15.22259521484375]}, {"key": "liu2018mtfh", "year": "2018", "title": "MTFH A Matrix Tri-factorization Hashing Framework For Efficient Cross-modal Retrieval", "abstract": "<p>Hashing has recently sparked a great revolution in cross-modal retrieval because of its low storage cost and high query speed. Recent cross-modal hashing methods often learn unified or equal-length hash codes to represent the multi-modal data and make them intuitively comparable. However such unified or equal-length hash representations could inherently sacrifice their representation scalability because the data from different modalities may not have one-to-one correspondence and could be encoded more efficiently by different hash codes of unequal lengths. To mitigate these problems this paper exploits a related and relatively unexplored problem encode the heterogeneous data with varying hash lengths and generalize the cross-modal retrieval in various challenging scenarios. To this end a generalized and flexible cross-modal hashing framework termed Matrix Tri-Factorization Hashing (MTFH) is proposed to work seamlessly in various settings including paired or unpaired multi-modal data and equal or varying hash length encoding scenarios. More specifically MTFH exploits an efficient objective function to flexibly learn the modality-specific hash codes with different length settings while synchronously learning two semantic correlation matrices to semantically correlate the different hash representations for heterogeneous data comparable. As a result the derived hash codes are more semantically meaningful for various challenging cross-modal retrieval tasks. Extensive experiments evaluated on public benchmark datasets highlight the superiority of MTFH under various retrieval scenarios and show its competitive performance with the state-of-the-arts.</p>\n", "tags": ["Cross Modal", "Independent", "TPAMI"], "tsne_embedding": [-10.827252388000488, -3.514880418777466]}, {"key": "liu2019cross", "year": "2019", "title": "Cross-modal Zero-shot Hashing", "abstract": "<p>Hashing has been widely studied for big data retrieval due to its low storage cost and fast query speed. Zero-shot hashing (ZSH) aims to learn a hashing model that is trained using only samples from seen categories but can generalize well to samples of unseen categories. ZSH generally uses category attributes to seek a semantic embedding space to transfer knowledge from seen categories to unseen ones. As a result it may perform poorly when labeled data are insufficient. ZSH methods are mainly designed for single-modality data which prevents their application to the widely spread multi-modal data. On the other hand existing cross-modal hashing solutions assume that all the modalities share the same category labels while in practice the labels of different data modalities may be different. To address these issues we propose a general Cross-modal Zero-shot Hashing (CZHash) solution to effectively leverage unlabeled and labeled multi-modality data with different label spaces. CZHash first quantifies the composite similarity between instances using label and feature information. It then defines an objective function to achieve deep feature learning compatible with the composite similarity preserving category attribute space learning and hashing coding function learning. CZHash further introduces an alternative optimization procedure to jointly optimize these learning objectives. Experiments on benchmark multi-modal datasets show that CZHash significantly outperforms related representative hashing approaches both on effectiveness and adaptability.</p>\n", "tags": ["ARXIV", "Cross Modal", "Supervised"], "tsne_embedding": [-3.092115640640259, 4.254996299743652]}, {"key": "liu2019deep", "year": "2019", "title": "Deep Triplet Quantization", "abstract": "<p>Deep hashing establishes efficient and effective image retrieval by end-to-end learning of deep representations and hash codes from similarity data. We present a compact coding solution focusing on deep learning to quantization approach that has shown superior performance over hashing solutions for similarity retrieval. We propose Deep Triplet Quantization (DTQ) a novel approach to learning deep quantization models from the similarity triplets. To enable more effective triplet training we design a new triplet selection approach Group Hard that randomly selects hard triplets in each image group. To generate compact binary codes we further apply a triplet quantization with weak orthogonality during triplet training. The quantization loss reduces the codebook redundancy and enhances the quantizability of deep representations through back-propagation. Extensive experiments demonstrate that DTQ can generate high-quality and compact binary codes which yields state-of-the-art image retrieval performance on three benchmark datasets NUS-WIDE CIFAR-10 and MS-COCO.</p>\n", "tags": ["ARXIV", "Deep Learning", "Image Retrieval", "Independent", "Quantisation"], "tsne_embedding": [-7.463630199432373, 9.915010452270508]}, {"key": "liu2019guided", "year": "2019", "title": "Guided Similarity Separation For Image Retrieval", "abstract": "<p>Despite recent progress in computer vision image retrieval remains a challenging open problem. Numerous variations such as view angle lighting and occlusion make it difficult to design models that are both robust and efficient. Many leading methods traverse the nearest neighbor graph to exploit higher order neighbor information and uncover the highly complex underlying manifold. In this work we propose a different approach where we leverage graph convolutional networks to directly encode neighbor information into image descriptors. We further leverage ideas from clustering and manifold learning and introduce an unsupervised loss based on pairwise separation of image similarities. Empirically we demonstrate that our model is able to successfully learn a new descriptor space that significantly improves retrieval accuracy while still allowing efficient inner product inference. Experiments on five public benchmarks show highly competitive performance with up to 2437; relative improvement in mAP over leading baselines. Full code for this work is available here https://github.com/layer6ai-labs/GSS.</p>\n", "tags": ["Graph", "Has Code", "Image Retrieval", "NEURIPS", "Unsupervised"], "tsne_embedding": [-5.433600902557373, 26.504384994506836]}, {"key": "liu2019moboost", "year": "2019", "title": "MoBoost: A Self-improvement Framework for Linear-based Hashing", "abstract": "<p>The linear model is commonly utilized in hashing methods owing to its efficiency. To obtain better accuracy, linear-based hashing methods focus on designing a generalized linear objective function with different constraints or penalty terms that consider neighborhood information. In this study, we propose a novel generalized framework called Model Boost (MoBoost), which can achieve the self-improvement of the linear-based hashing. The proposed MoBoost is used to improve model parameter optimization for linear-based hashing methods without adding new constraints or penalty terms. In the proposed MoBoost, given a linear-based hashing method, we first execute the method several times to get several different hash codes for training samples, and then combine these different hash codes into one set utilizing one novel fusion strategy. Based on this set of hash codes, we learn some new parameters for the linear hash function that can significantly improve accuracy. The proposed MoBoost can be generally adopted in existing linear-based hashing methods, achieving more precise and stable performance compared to the original methods while imposing negligible added expenditure in terms of time and space. Extensive experiments are performed based on three benchmark datasets, and the results demonstrate the superior performance of the proposed framework.</p>\n", "tags": ["Image Retrieval"], "tsne_embedding": [3.3053977489471436, -15.170639991760254]}, {"key": "liu2019mutual", "year": "2019", "title": "Mutual Linear Regression-based Discrete Hashing", "abstract": "<p>Label information is widely used in hashing methods because of its effectiveness of improving the precision. The existing hashing methods always use two different projections to represent the mutual regression between hash codes and class labels. In contrast to the existing methods we propose a novel learning-based hashing method termed stable supervised discrete hashing with mutual linear regression (S2DHMLR) in this study where only one stable projection is used to describe the linear correlation between hash codes and corresponding labels. To the best of our knowledge this strategy has not been used for hashing previously. In addition we further use a boosting strategy to improve the final performance of the proposed method without adding extra constraints and with little extra expenditure in terms of time and space. Extensive experiments conducted on three image benchmarks demonstrate the superior performance of the proposed method.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [-8.745196342468262, -11.290947914123535]}, {"key": "liu2019optimal", "year": "2019", "title": "Optimal Projection Guided Transfer Hashing For Image Retrieval", "abstract": "<p>Recently learning to hash has been widely studied for image retrieval thanks to the computation and storage efficiency of binary codes. For most existing learning to hash methods sufficient training images are required and used to learn precise hashing codes. However in some real-world applications there are not always sufficient training images in the domain of interest. In addition some existing supervised approaches need a amount of labeled data which is an expensive process in term of time label and human expertise. To handle such problems inspired by transfer learning we propose a simple yet effective unsupervised hashing method named Optimal Projection Guided Transfer Hashing (GTH) where we borrow the images of other different but related domain i.e. source domain to help learn precise hashing codes for the domain of interest i.e. target domain. Besides we propose to seek for the maximum likelihood estimation (MLE) solution of the hashing functions of target and source domains due to the domain gap. Furthermorean alternating optimization method is adopted to obtain the two projections of target and source domains such that the domain hashing disparity is reduced gradually. Extensive experiments on various benchmark databases verify that our method outperforms many state-of-the-art learning to hash methods. The implementation details are available at https://github.com/liuji93/GTH.\u201d</p>\n", "tags": ["ARXIV", "Has Code", "Image Retrieval", "Unsupervised"], "tsne_embedding": [-1.3344829082489014, 0.028194649145007133]}, {"key": "liu2019query", "year": "2019", "title": "Query-adaptive Hash Code Ranking For Large-scale Multi-view Visual Search", "abstract": "<p>Hash based nearest neighbor search has become attractive in many applications. However the quantization in hashing usually degenerates the discriminative power when using Hamming distance ranking. Besides for large-scale visual search existing hashing methods cannot directly support the efficient search over the data with multiple sources and while the literature has shown that adaptively incorporating complementary information from diverse sources or views can significantly boost the search performance. To address the problems this paper proposes a novel and generic approach to building multiple hash tables with multiple views and generating fine-grained ranking results at bitwise and tablewise levels. For each hash table a query-adaptive bitwise weighting is introduced to alleviate the quantization loss by simultaneously exploiting the quality of hash functions and their complement for nearest neighbor search. From the tablewise aspect multiple hash tables are built for different data views as a joint index over which a query-specific rank fusion is proposed to rerank all results from the bitwise ranking by diffusing in a graph. Comprehensive experiments on image search over three well-known benchmarks show that the proposed method achieves up to 17.1137; and 20.2837; performance gains on single and multiple table search over state-of-the-art methods.</p>\n", "tags": ["ARXIV", "Cross Modal", "Graph", "Independent", "Quantisation"], "tsne_embedding": [-18.916929244995117, -0.13683141767978668]}, {"key": "liu2019ranking", "year": "2019", "title": "Ranking-based Deep Cross-modal Hashing", "abstract": "<p>Cross-modal hashing has been receiving increasing interests for its low storage cost and fast query speed in multi-modal data retrievals. However most existing hashing methods are based on hand-crafted or raw level features of objects which may not be optimally compatible with the coding process. Besides these hashing methods are mainly designed to handle simple pairwise similarity. The complex multilevel ranking semantic structure of instances associated with multiple labels has not been well explored yet. In this paper we propose a ranking-based deep cross-modal hashing approach (RDCMH). RDCMH firstly uses the feature and label information of data to derive a semi-supervised semantic ranking list. Next to expand the semantic representation power of hand-crafted features RDCMH integrates the semantic ranking information into deep cross-modal hashing and jointly optimizes the compatible parameters of deep feature representations and of hashing functions. Experiments on real multi-modal datasets show that RDCMH outperforms other competitive baselines and achieves the state-of-the-art performance in cross-modal retrieval applications.</p>\n", "tags": ["ARXIV", "Cross Modal", "Supervised"], "tsne_embedding": [-9.901023864746094, -1.1789202690124512]}, {"key": "liu2019weakly", "year": "2019", "title": "Weakly-paired Cross-modal Hashing", "abstract": "<p>Hashing has been widely adopted for large-scale data retrieval in many domains due to its low storage cost and high retrieval speed. Existing cross-modal hashing methods optimistically assume that the correspondence between training samples across modalities are readily available. This assumption is unrealistic in practical applications. In addition these methods generally require the same number of samples across different modalities which restricts their flexibility. We propose a flexible cross-modal hashing approach (Flex-CMH) to learn effective hashing codes from weakly-paired data whose correspondence across modalities are partially (or even totally) unknown. FlexCMH first introduces a clustering-based matching strategy to explore the local structure of each cluster and thus to find the potential correspondence between clusters (and samples therein) across modalities. To reduce the impact of an incomplete correspondence it jointly optimizes in a unified objective function the potential correspondence the cross-modal hashing functions derived from the correspondence and a hashing quantitative loss. An alternative optimization technique is also proposed to coordinate the correspondence and hash functions and to reinforce the reciprocal effects of the two objectives. Experiments on publicly multi-modal datasets show that FlexCMH achieves significantly better results than state-of-the-art methods and it indeed offers a high degree of flexibility for practical cross-modal hashing tasks.</p>\n", "tags": ["ARXIV", "Cross Modal", "Unsupervised"], "tsne_embedding": [5.006091117858887, -4.5587592124938965]}, {"key": "liu2020joint", "year": "2020", "title": "Joint-modal Distribution-based Similarity Hashing for Large-scale Unsupervised Deep Cross-modal Retrieval", "abstract": "<p>Hashing-based cross-modal search which aims to map multiple modality features into binary codes has attracted increasingly attention due to its storage and search efficiency especially in large-scale database retrieval. Recent unsupervised deep cross-modal hashing methods have shown promising results. However, existing approaches typically suffer from two limitations: (1) They usually learn cross-modal similarity information separately or in a redundant fusion manner, which may fail to capture semantic correlations among instances from different modalities sufficiently and effectively. (2) They seldom consider the sampling and weighting schemes for unsupervised cross-modal hashing, resulting in the lack of satisfactory discriminative ability in hash codes. To overcome these limitations, we propose a novel unsupervised deep cross-modal hashing method called Joint-modal Distribution-based Similarity Hashing (JDSH) for large-scale cross-modal retrieval. Firstly, we propose a novel cross-modal joint-training method by constructing a joint-modal similarity matrix to fully preserve the cross-modal semantic correlations among instances. Secondly, we propose a sampling and weighting scheme termed the Distribution-based Similarity Decision and Weighting (DSDW) method for unsupervised cross-modal hashing, which is able to generate more discriminative hash codes by pushing semantic similar instance pairs closer and pulling semantic dissimilar instance pairs apart. The experimental results demonstrate the superiority of JDSH compared with several unsupervised cross-modal hashing methods on two public datasets NUS-WIDE and MIRFlickr.</p>\n", "tags": ["Cross Modal", "SIGIR", "Supervised"], "tsne_embedding": [-12.884109497070312, -2.375101327896118]}, {"key": "liu2020model", "year": "2020", "title": "Model Optimization Boosting Framework for Linear Model Hash Learning", "abstract": "<p>Efficient hashing techniques have attracted extensive research interests in both storage and retrieval of high dimensional data, such as images and videos. In existing hashing methods, a linear model is commonly utilized owing to its efficiency. To obtain better accuracy, linear-based hashing methods focus on designing a generalized linear objective function with different constraints or penalty terms that consider the inherent characteristics and neighborhood information of samples. Differing from existing hashing methods, in this study, we propose a self-improvement framework called Model Boost (MoBoost) to improve model parameter optimization for linear-based hashing methods without adding new constraints or penalty terms. In the proposed MoBoost, for a linear-based hashing method, we first repeatedly execute the hashing method to obtain several hash codes to training samples. Then, utilizing two novel fusion strategies, these codes are fused into a single set. We also propose two new criteria to evaluate the goodness of hash bits during the fusion process. Based on the fused set of hash codes, we learn new parameters for the linear hash function that can significantly improve the accuracy. In general, the proposed MoBoost can be adopted by existing linear-based hashing methods, achieving more precise and stable performance compared to the original methods, and adopting the proposed MoBoost will incur negligible time and space costs. To evaluate the proposed MoBoost, we performed extensive experiments on four benchmark datasets, and the results demonstrate superior performance.</p>\n", "tags": ["Image Retrieval", "TIP"], "tsne_embedding": [3.2757503986358643, -15.068370819091797]}, {"key": "liu2020reinforcing", "year": "2020", "title": "Reinforcing Short-length Hashing", "abstract": "<p>Due to the compelling efficiency in retrieval and storage similarity-preserving hashing has been widely applied to approximate nearest neighbor search in large-scale image retrieval. However existing methods have poor performance in retrieval using an extremely short-length hash code due to weak ability of classification and poor distribution of hash bit. To address this issue in this study we propose a novel reinforcing short-length hashing (RSLH). In this proposed RSLH mutual reconstruction between the hash representation and semantic labels is performed to preserve the semantic information. Furthermore to enhance the accuracy of hash representation a pairwise similarity matrix is designed to make a balance between accuracy and training expenditure on memory. In addition a parameter boosting strategy is integrated to reinforce the precision with hash bits fusion. Extensive experiments on three large-scale image benchmarks demonstrate the superior performance of RSLH under various short-length hashing scenarios.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Supervised"], "tsne_embedding": [-15.916117668151855, 4.444779872894287]}, {"key": "liu2021fddh", "year": "2021", "title": "FDDH Fast Discriminative Discrete Hashing For Large-scale Cross-modal Retrieval", "abstract": "<p>Cross-modal hashing favored for its effectiveness and efficiency has received wide attention to facilitating efficient retrieval across different modalities. Nevertheless most existing methods do not sufficiently exploit the discriminative power of semantic information when learning the hash codes while often involving time-consuming training procedure for handling the large-scale dataset. To tackle these issues we formulate the learning of similarity-preserving hash codes in terms of orthogonally rotating the semantic data so as to minimize the quantization loss of mapping such data to hamming space and propose an efficient Fast Discriminative Discrete Hashing (FDDH) approach for large-scale cross-modal retrieval. More specifically FDDH introduces an orthogonal basis to regress the targeted hash codes of training examples to their corresponding semantic labels and utilizes -dragging technique to provide provable large semantic margins. Accordingly the discriminative power of semantic information can be explicitly captured and maximized. Moreover an orthogonal transformation scheme is further proposed to map the nonlinear embedding data into the semantic subspace which can well guarantee the semantic consistency between the data feature and its semantic representation. Consequently an efficient closed form solution is derived for discriminative hash code learning which is very computationally efficient. In addition an effective and stable online learning strategy is presented for optimizing modality-specific projection functions featuring adaptivity to different training sizes and streaming data. The proposed FDDH approach theoretically approximates the bi-Lipschitz continuity runs sufficiently fast and also significantly improves the retrieval performance over the state-of-the-art methods. The source code is released at https://github.com/starxliu/FDDH.\u201d</p>\n", "tags": ["Cross Modal", "Has Code", "Independent", "Quantisation", "Streaming Data", "TNNLS"], "tsne_embedding": [-5.770694255828857, -1.2986611127853394]}, {"key": "liu2021p", "year": "2021", "title": "P-tuning V2 Prompt Tuning Can Be Comparable To Fine-tuning Universally Across Scales And Tasks", "abstract": "<p>Prompt tuning which only tunes continuous prompts with a frozen language model substantially reduces per-task storage and memory usage at training. However in the context of NLU prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.137;-337; tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning citeli2021prefixqin2021learning optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2 we believe it can serve as an alternative to finetuning and a strong baseline for future research.Our code and data are released at https://github.com/THUDM/P-tuning-v2.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [20.682172775268555, -4.592623710632324]}, {"key": "liu2021pre", "year": "2021", "title": "Pre-train Prompt And Predict A Systematic Survey Of Prompting Methods In Natural Language Processing", "abstract": "<p>This paper surveys and organizes research works in a new paradigm in natural language processing which we dub prompt-based learning. Unlike traditional supervised learning which trains a model to take in an input x and predict an output y as P(yx) prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks the original input x is modified using a template into a textual string prompt x that has some unfilled slots and then the language model is used to probabilistically fill the unfilled information to obtain a final string x from which the final output y can be derived. This framework is powerful and attractive for a number of reasons it allows the language model to be pre-trained on massive amounts of raw text and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm describe a unified set of mathematical notations that can cover a wide variety of existing work and organize existing work along several dimensions e.g.the choice of pre-trained models prompts and tuning strategies. To make the field more accessible to interested beginners we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources e.g. a website http://pretrain.nlpedia.ai/ including constantly-updated survey and paperlist.</p>\n", "tags": ["ARXIV", "Supervised", "Survey Paper"], "tsne_embedding": [29.504985809326172, -5.482748985290527]}, {"key": "liu2021ternary", "year": "2021", "title": "Ternary Hashing", "abstract": "<p>This paper proposes a novel ternary hash encoding for learning to hash methods which provides a principled more efficient coding scheme with performances better than those of the state-of-the-art binary hashing counterparts. Two kinds of axiomatic ternary logic Kleene logic and (L)ukasiewicz logic are adopted to calculate the Ternary Hamming Distance (THD) for both the learning/encoding and testing/querying phases. Our work demonstrates that with an efficient implementation of ternary logic on standard binary machines the proposed ternary hashing is compared favorably to the binary hashing methods with consistent improvements of retrieval mean average precision (mAP) ranging from 137; to 5.937; as shown in CIFAR10 NUS-WIDE and ImageNet100 datasets.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [11.130167007446289, 5.788618087768555]}, {"key": "liu2022prototype", "year": "2022", "title": "Prototype-based Layered Federated Cross-modal Hashing", "abstract": "<p>Recently deep cross-modal hashing has gained increasing attention. However in many practical cases data are distributed and cannot be collected due to privacy concerns which greatly reduces the cross-modal hashing performance on each client. And due to the problems of statistical heterogeneity model heterogeneity and forcing each client to accept the same parameters applying federated learning to cross-modal hash learning becomes very tricky. In this paper we propose a novel method called prototype-based layered federated cross-modal hashing. Specifically the prototype is introduced to learn the similarity between instances and classes on server reducing the impact of statistical heterogeneity (non-IID) on different clients. And we monitor the distance between local and global prototypes to further improve the performance. To realize personalized federated learning a hypernetwork is deployed on server to dynamically update different layers weights of local model. Experimental results on benchmark datasets show that our method outperforms state-of-the-art methods.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [5.575374603271484, -6.309393405914307]}, {"key": "liu2023blockwise", "year": "2023", "title": "Blockwise Parallel Transformer For Large Context Models", "abstract": "<p>Transformers have emerged as the cornerstone of state-of-the-art natural language processing models showcasing exceptional performance across a wide range of AI applications. However the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach Blockwise Parallel Transformer (BPT) that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency BPT enables training sequences 32 times longer than vanilla Transformers and up to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [17.776241302490234, -12.899312019348145]}, {"key": "liu2023can", "year": "2023", "title": "Can LSH (locality-sensitive Hashing) Be Replaced By Neural Network", "abstract": "<p>With the rapid development of GPU (Graphics Processing Unit) technologies and neural networks we can explore more appropriate data structures and algorithms. Recent progress shows that neural networks can partly replace traditional data structures. In this paper we proposed a novel DNN (Deep Neural Network)-based learned locality-sensitive hashing called LLSH to efficiently and flexibly map high-dimensional data to low-dimensional space. LLSH replaces the traditional LSH (Locality-sensitive Hashing) function families with parallel multi-layer neural networks which reduces the time and memory consumption and guarantees query accuracy simultaneously. The proposed LLSH demonstrate the feasibility of replacing the hash index with learning-based neural networks and open a new door for developers to design and configure data organization more accurately to improve information-searching performance. Extensive experiments on different types of datasets show the superiority of the proposed method in query accuracy time consumption and memory usage.</p>\n", "tags": ["ARXIV", "Graph", "LSH", "Supervised"], "tsne_embedding": [7.345075607299805, -6.854866027832031]}, {"key": "liu2023hs", "year": "2023", "title": "HS-GCN Hamming Spatial Graph Convolutional Networks For Recommendation", "abstract": "<p>An efficient solution to the large-scale recommender system is to represent users and items as binary hash codes in the Hamming space. Towards this end existing methods tend to code users by modeling their Hamming similarities with the items they historically interact with which are termed as the first-order similarities in this work. Despite their efficiency these methods suffer from the suboptimal representative capacity since they forgo the correlation established by connecting multiple first-order similarities i.e. the relation among the indirect instances which could be defined as the high-order similarity. To tackle this drawback we propose to model both the first- and the high-order similarities in the Hamming space through the user-item bipartite graph. Therefore we develop a novel learning to hash framework namely Hamming Spatial Graph Convolutional Networks (HS-GCN) which explicitly models the Hamming similarity and embeds it into the codes of users and items. Extensive experiments on three public benchmark datasets demonstrate that our proposed model significantly outperforms several state-of-the-art hashing models and obtains performance comparable with the real-valued recommendation models.</p>\n", "tags": ["ARXIV", "Graph", "Independent"], "tsne_embedding": [-11.980203628540039, -5.523425579071045]}, {"key": "liu2023is", "year": "2023", "title": "Is Chatgpt A Good Recommender A Preliminary Study", "abstract": "<p>Recommendation systems have witnessed significant advancements and have been widely used over the past decades. However most traditional recommendation methods are task-specific and therefore lack efficient generalization ability. Recently the emergence of ChatGPT has significantly advanced NLP tasks by enhancing the capabilities of conversational models. Nonetheless the application of ChatGPT in the recommendation domain has not been thoroughly investigated. In this paper we employ ChatGPT as a general-purpose recommendation model to explore its potential for transferring extensive linguistic and world knowledge acquired from large-scale corpora to recommendation scenarios. Specifically we design a set of prompts and evaluate ChatGPTs performance on five recommendation scenarios. Unlike traditional recommendation methods we do not fine-tune ChatGPT during the entire evaluation process relying only on the prompts themselves to convert recommendation tasks into natural language tasks. Further we explore the use of few-shot prompting to inject interaction information that contains user potential interest to help ChatGPT better understand user needs and interests. Comprehensive experimental results on Amazon Beauty dataset show that ChatGPT has achieved promising results in certain tasks and is capable of reaching the baseline level in others. We conduct human evaluations on two explainability-oriented tasks to more accurately evaluate the quality of contents generated by different models. And the human evaluations show ChatGPT can truly understand the provided information and generate clearer and more reasonable results. We hope that our study can inspire researchers to further explore the potential of language models like ChatGPT to improve recommendation performance and contribute to the advancement of the recommendation systems field.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [44.747947692871094, -5.911077499389648]}, {"key": "liu2023mitigating", "year": "2023", "title": "Mitigating Hallucination In Large Multi-modal Models Via Robust Instruction Tuning", "abstract": "<p>Despite the promising progress in multi-modal tasks current large multi-modal models (LMMs) are prone to hallucinating inconsistent descriptions with respect to the associated image and human instructions. This paper addresses this issue by introducing the first large and diverse visual instruction tuning dataset named Large-scale Robust Visual (LRV)-Instruction. Our dataset comprises 400k visual instructions generated by GPT4 covering 16 vision-and-language tasks with open-ended instructions and answers. Unlike existing studies that primarily focus on positive instruction samples we design LRV-Instruction to include both positive and negative instructions for more robust visual instruction tuning. Our negative instructions are designed at three semantic levels (i) Nonexistent Object Manipulation (ii) Existent Object Manipulation and (iii) Knowledge Manipulation. To efficiently measure the hallucination generated by LMMs we propose GPT4-Assisted Visual Instruction Evaluation (GAVIE) a stable approach to evaluate visual instruction tuning like human experts. GAVIE does not require human-annotated groundtruth answers and can adapt to diverse instruction formats. We conduct comprehensive experiments to investigate the hallucination of LMMs. Our results demonstrate existing LMMs exhibit significant hallucinations when presented with our negative instructions particularly Existent Object and Knowledge Manipulation instructions. Moreover we successfully mitigate hallucination by finetuning MiniGPT4 and mPLUG-Owl on LRV-Instruction while improving performance on several public datasets compared to state-of-the-art methods. Additionally we observed that a balanced ratio of positive and negative instances in the training data leads to a more robust model. Code and data are available at https://github.com/FuxiaoLiu/LRV-Instruction.</p>\n", "tags": ["ARXIV", "Has Code", "Supervised"], "tsne_embedding": [46.94813537597656, 7.095206260681152]}, {"key": "liu2023once", "year": "2023", "title": "ONCE Boosting Content-based Recommendation With Both Open- And Closed-source Large Language Models", "abstract": "<p>Personalized content-based recommender systems have become indispensable tools for users to navigate through the vast amount of content available on platforms like daily news websites and book recommendation services. However existing recommenders face significant challenges in understanding the content of items. Large language models (LLMs) which possess deep semantic comprehension and extensive knowledge from pretraining have proven to be effective in various natural language processing tasks. In this study we explore the potential of leveraging both open- and closed-source LLMs to enhance content-based recommendation. With open-source LLMs we utilize their deep layers as content encoders enriching the representation of content at the embedding level. For closed-source LLMs we employ prompting techniques to enrich the training data at the token level. Through comprehensive experiments we demonstrate the high effectiveness of both types of LLMs and show the synergistic relationship between them. Notably we observed a significant relative improvement of up to 19.3237; compared to existing state-of-the-art recommendation models. These findings highlight the immense potential of both open- and closed-source of LLMs in enhancing content-based recommendation systems. We will make our code and LLM-generated data available for other researchers to reproduce our results.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [39.185115814208984, -11.039478302001953]}, {"key": "liu2023pre", "year": "2023", "title": "Pre-train Prompt And Recommendation A Comprehensive Survey Of Language Modelling Paradigm Adaptations In Recommender Systems", "abstract": "<p>The emergence of Pre-trained Language Models (PLMs) has achieved tremendous success in the field of Natural Language Processing (NLP) by learning universal representations on large corpora in a self-supervised manner. The pre-trained models and the learned representations can be beneficial to a series of downstream NLP tasks. This training paradigm has recently been adapted to the recommendation domain and is considered a promising approach by both academia and industry. In this paper we systematically investigate how to extract and transfer knowledge from pre-trained models learned by different PLM-related training paradigms to improve recommendation performance from various perspectives such as generality sparsity efficiency and effectiveness. Specifically we propose a comprehensive taxonomy to divide existing PLM-based recommender systems w.r.t. their training strategies and objectives. Then we analyze and summarize the connection between PLM-based training paradigms and different input data types for recommender systems. Finally we elaborate on open issues and future research directions in this vibrant field.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [39.8074836730957, -10.325218200683594]}, {"key": "liu2023scissorhands", "year": "2023", "title": "Scissorhands Exploiting The Persistence Of Importance Hypothesis For LLM KV Cache Compression At Test Time", "abstract": "<p>Large language models(LLMs) have sparked a new wave of exciting AI applications. Hosting these models at scale requires significant memory resources. One crucial memory bottleneck for the deployment stems from the context window. It is commonly recognized that model weights are memory hungry; however the size of key-value embedding stored during the generation process (KV cache) can easily surpass the model size. The enormous size of the KV cache puts constraints on the inference batch size which is crucial for high throughput inference workload. Inspired by an interesting observation of the attention scores we hypothesize the persistence of importance only pivotal tokens which had a substantial influence at one step will significantly influence future generations. Based on our empirical verification and theoretical analysis around this hypothesis we propose Scissorhands a system that maintains the memory usage of the KV cache at a fixed budget without finetuning the model. In essence Scissorhands manages the KV cache by storing the pivotal tokens with a higher probability. We validate that Scissorhands reduces the inference memory usage of the KV cache by up to 5X without compromising model quality. We further demonstrate that Scissorhands can be combined with 4-bit quantization traditionally used to compress model weights to achieve up to 20X compression.</p>\n", "tags": ["ARXIV", "Quantisation"], "tsne_embedding": [12.562829971313477, -11.119677543640137]}, {"key": "liu2023sparse", "year": "2023", "title": "Sparse-inductive Generative Adversarial Hashing For Nearest Neighbor Search", "abstract": "<p>Unsupervised hashing has received extensive research focus on the past decade which typically aims at preserving a predefined metric (i.e. Euclidean metric) in the Hamming space. To this end the encoding functions of the existing hashing are typically quasi-isometric which devote to reducing the quantization loss from the target metric space to the discrete Hamming space. However it is indeed problematic to directly minimize such error since such mentioned two metric spaces are heterogeneous and the quasi-isometric mapping is non-linear. The former leads to inconsistent feature distributions while the latter leads to problematic optimization issues. In this paper we propose a novel unsupervised hashing method termed Sparsity-Induced Generative Adversarial Hashing (SiGAH) to encode large-scale high-dimensional features into binary codes which well solves the two problems through a generative adversarial training framework. Instead of minimizing the quantization loss our key innovation lies in enforcing the learned Hamming space to have similar data distribution to the target metric space via a generative model. In particular we formulate a ReLU-based neural network as a generator to output binary codes and an MSE-loss based auto-encoder network as a discriminator upon which a generative adversarial learning is carried out to train hash functions. Furthermore to generate the synthetic features from the hash codes a compressed sensing procedure is introduced into the generative model which enforces the reconstruction boundary of binary codes to be consistent with that of original features. Finally such generative adversarial framework can be trained via the Adam optimizer. Experimental results on four benchmarks i.e. Tiny100K GIST1M Deep1M and MNIST have shown that the proposed SiGAH has superior performance over the state-of-the-art approaches.</p>\n", "tags": ["ARXIV", "Cross Modal", "Quantisation", "Unsupervised"], "tsne_embedding": [-4.765506267547607, -5.238173007965088]}, {"key": "liu2023visual", "year": "2023", "title": "Visual Instruction Tuning", "abstract": "<p>Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks but the idea is less explored in the multimodal field. In this paper we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data we introduce LLaVA Large Language and Vision Assistant an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions and yields a 85.137; relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.5337;. We make GPT-4 generated visual instruction tuning data our model and code base publicly available.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [23.148466110229492, -10.23919677734375]}, {"key": "liu2024paying", "year": "2024", "title": "Paying More Attention To Image A Training-free Method For Alleviating Hallucination In Lvlms", "abstract": "<p>Existing Large Vision-Language Models (LVLMs) primarily align image features of vision encoder with Large Language Models (LLMs) to leverage their superior text generation capabilities. However the scale disparity between vision encoder and language model may led to LLMs assuming a predominant role in multi-modal comprehension. This imbalance in LVLMs may result in the instances of hallucinatory. Concretely LVLMs may generate consistent descriptions with or without visual input indicating that certain outputs are influenced solely by context text. We refer to this phenomenon as text inertia. To counteract this issue we introduce a training-free algorithm to find an equilibrium point between image comprehension and language inference. Specifically we adaptively involve adjusting and amplifying the attention weights assigned to image tokens thereby granting greater prominence to visual elements. Meanwhile we subtract the logits of multi-modal inputs from ones of pure text input which can help LVLMs be not biased towards LLMs. By enhancing images tokens and reducing the stubborn output of LLM we can let LVLM pay more attention to images towards alleviating text inertia and reducing the hallucination in LVLMs. Our extensive experiments shows that this method substantially reduces the frequency of hallucinatory outputs in various LVLMs in terms of different metrics. Project page is available at https://lalbj.github.io/projects/PAI/.</p>\n", "tags": ["ARXIV", "Cross Modal", "Has Code"], "tsne_embedding": [45.887664794921875, 5.2029900550842285]}, {"key": "liu2024rec", "year": "2024", "title": "Rec-gpt4v Multimodal Recommendation With Large Vision-language Models", "abstract": "<p>The development of large vision-language models (LVLMs) offers the potential to address challenges faced by traditional multimodal recommendations thanks to their proficient understanding of static images and textual dynamics. However the application of LVLMs in this field is still limited due to the following complexities First LVLMs lack user preference knowledge as they are trained from vast general datasets. Second LVLMs suffer setbacks in addressing multiple image dynamics in scenarios involving discrete noisy and redundant image sequences. To overcome these issues we propose the novel reasoning scheme named Rec-GPT4V Visual-Summary Thought (VST) of leveraging large vision-language models for multimodal recommendation. We utilize user history as in-context user preferences to address the first challenge. Next we prompt LVLMs to generate item image summaries and utilize image comprehension in natural language space combined with item titles to query the user preferences over candidate items. We conduct comprehensive experiments across four datasets with three LVLMs GPT4-V LLaVa-7b and LLaVa-13b. The numerical results indicate the efficacy of VST.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [40.51957321166992, 4.948526382446289]}, {"key": "liu2024survey", "year": "2024", "title": "A Survey On Hallucination In Large Vision-language Models", "abstract": "<p>Recent development of Large Vision-Language Models (LVLMs) has attracted growing attention within the AI landscape for its practical implementation potential. However hallucination or more specifically the misalignment between factual visual content and corresponding textual generation poses a significant challenge of utilizing LVLMs. In this comprehensive survey we dissect LVLM-related hallucinations in an attempt to establish an overview and facilitate future mitigation. Our scrutiny starts with a clarification of the concept of hallucinations in LVLMs presenting a variety of hallucination symptoms and highlighting the unique challenges inherent in LVLM hallucinations. Subsequently we outline the benchmarks and methodologies tailored specifically for evaluating hallucinations unique to LVLMs. Additionally we delve into an investigation of the root causes of these hallucinations encompassing insights from the training data and model components. We also critically review existing methods for mitigating hallucinations. The open questions and future directions pertaining to hallucinations within LVLMs are discussed to conclude this survey.</p>\n", "tags": ["ARXIV", "Cross Modal", "Supervised", "Survey Paper"], "tsne_embedding": [52.686309814453125, 8.17953872680664]}, {"key": "loncaric2018convolutional", "year": "2018", "title": "Convolutional Hashing For Automated Scene Matching", "abstract": "<p>We present a powerful new loss function and training scheme for learning binary hash functions. In particular we demonstrate our method by creating for the first time a neural network that outperforms state-of-the-art Haar wavelets and color layout descriptors at the task of automated scene matching. By accurately relating distance on the manifold of network outputs to distance in Hamming space we achieve a 100-fold reduction in nontrivial false positive rate and significantly higher true positive rate. We expect our insights to provide large wins for hashing models applied to other information retrieval hashing tasks as well.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [-12.140945434570312, -15.851688385009766]}, {"key": "loncaric2018learning", "year": "2018", "title": "Learning Hash Codes Via Hamming Distance Targets", "abstract": "<p>We present a powerful new loss function and training scheme for learning binary hash codes with any differentiable model and similarity function. Our loss function improves over prior methods by using log likelihood loss on top of an accurate approximation for the probability that two inputs fall within a Hamming distance target. Our novel training scheme obtains a good estimate of the true gradient by better sampling inputs and evaluating loss terms between all pairs of inputs in each minibatch. To fully leverage the resulting hashes we use multi-indexing. We demonstrate that these techniques provide large improvements to a similarity search tasks. We report the best results to date on competitive information retrieval tasks for ImageNet and SIFT 1M improving MAP from 7337; to 8437; and reducing query cost by a factor of 2-8 respectively.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-16.685291290283203, 17.970264434814453]}, {"key": "long2015composite", "year": "2015", "title": "Composite Correlation Quantization For Efficient Multimodal Retrieval", "abstract": "<p>Efficient similarity retrieval from large-scale multimodal database is pervasive in modern search engines and social networks. To support queries across content modalities the system should enable cross-modal correlation and computation-efficient indexing. While hashing methods have shown great potential in achieving this goal current attempts generally fail to learn isomorphic hash codes in a seamless scheme that is they embed multiple modalities in a continuous isomorphic space and separately threshold embeddings into binary codes which incurs substantial loss of retrieval accuracy. In this paper we approach seamless multimodal hashing by proposing a novel Composite Correlation Quantization (CCQ) model. Specifically CCQ jointly finds correlation-maximal mappings that transform different modalities into isomorphic latent space and learns composite quantizers that convert the isomorphic latent features into compact binary codes. An optimization framework is devised to preserve both intra-modal similarity and inter-modal correlation through minimizing both reconstruction and quantization errors which can be trained from both paired and partially paired data in linear time. A comprehensive set of experiments clearly show the superior effectiveness and efficiency of CCQ against the state of the art hashing methods for both unimodal and cross-modal retrieval.</p>\n", "tags": ["ARXIV", "Cross Modal", "Independent", "Quantisation"], "tsne_embedding": [-13.3768892288208, -1.5559215545654297]}, {"key": "long2018deep", "year": "2018", "title": "Deep Domain Adaptation Hashing with Adversarial Learning", "abstract": "<p>The recent advances in deep neural networks have demonstrated high capability in a wide variety of scenarios. Nevertheless, fine-tuning deep models in a new domain still requires a significant amount of labeled data despite expensive labeling efforts. A valid question is how to leverage the source knowledge plus unlabeled or only sparsely labeled target data for learning a new model in target domain. The core problem is to bring the source and target distributions closer in the feature space. In the paper, we facilitate this issue in an adversarial learning framework, in which a domain discriminator is devised to handle domain shift. Particularly, we explore the learning in the context of hashing problem, which has been studied extensively due to its great efficiency in gigantic data. Specifically, a novel Deep Domain Adaptation Hashing with Adversarial learning (DeDAHA) architecture is presented, which mainly consists of three components: a deep convolutional neural networks (CNN) for learning basic image/frame representation followed by an adversary stream on one hand to optimize the domain discriminator, and on the other, to interact with each domain-specific hashing stream for encoding image representation to hash codes. The whole architecture is trained end-to-end by jointly optimizing two types of losses, i.e., triplet ranking loss to preserve the relative similarity ordering in the input triplets and adversarial loss to maximally fool the domain discriminator with the learnt source and target feature distributions. Extensive experiments are conducted on three domain transfer tasks, including cross-domain digits retrieval, image to image and image to video transfers, on several benchmarks. Our DeDAHA framework achieves superior results when compared to the state-of-the-art techniques.</p>\n", "tags": ["CNN", "Deep Learning", "GAN", "SIGIR"], "tsne_embedding": [7.442785263061523, 2.657729148864746]}, {"key": "long2018filter", "year": "2018", "title": "A Filter Of Minhash For Image Similarity Measures", "abstract": "<p>Image similarity measures play an important role in nearest neighbor search and duplicate detection for large-scale image datasets. Recently Minwise Hashing (or Minhash) and its related hashing algorithms have achieved great performances in large-scale image retrieval systems. However there are a large number of comparisons for image pairs in these applications which may spend a lot of computation time and affect the performance. In order to quickly obtain the pairwise images that theirs similarities are higher than the specific threshold T (e.g. 0.5) we propose a dynamic threshold filter of Minwise Hashing for image similarity measures. It greatly reduces the calculation time by terminating the unnecessary comparisons in advance. We also find that the filter can be extended to other hashing algorithms on when the estimator satisfies the binomial distribution such as b-Bit Minwise Hashing One Permutation Hashing etc. In this pager we use the Bag-of-Visual-Words (BoVW) model based on the Scale Invariant Feature Transform (SIFT) to represent the image features. We have proved that the filter is correct and effective through the experiment on real image datasets.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Independent"], "tsne_embedding": [-16.839017868041992, 9.718899726867676]}, {"key": "long2022adaptive", "year": "2022", "title": "Adaptive Asymmetric Label-guided Hashing For Multimedia Search", "abstract": "<p>With the rapid growth of multimodal media data on the Web in recent years hash learning methods as a way to achieve efficient and flexible cross-modal retrieval of massive multimedia data have received a lot of attention from the current Web resource retrieval research community. Existing supervised hashing methods simply transform label information into pairwise similarity information to guide hash learning leading to a potential risk of semantic error in the face of multi-label data. In addition most existing hash optimization methods solve NP-hard optimization problems by employing approximate approximation strategies based on relaxation strategies leading to a large quantization error. In order to address above obstacles we present a simple yet efficient Adaptive Asymmetric Label-guided Hashing named A2LH for Multimedia Search. Specifically A2LH is a two-step hashing method. In the first step we design an association representation model between the different modality representations and semantic label representation separately and use the semantic label representation as an intermediate bridge to solve the semantic gap existing between different modalities. In addition we present an efficient discrete optimization algorithm for solving the quantization error problem caused by relaxation-based optimization algorithms. In the second step we leverage the generated hash codes to learn the hash mapping functions. The experimental results show that our proposed method achieves optimal performance on all compared baseline methods.</p>\n", "tags": ["ARXIV", "Cross Modal", "Quantisation", "Supervised"], "tsne_embedding": [-7.663444995880127, -2.369805335998535]}, {"key": "longpre2023flan", "year": "2023", "title": "The Flan Collection Designing Data And Methods For Effective Instruction Tuning", "abstract": "<p>We study the design decisions of publicly available instruction tuning methods and break down the development of Flan 2022 (Chung et al. 2022). Through careful ablation studies on the Flan Collection of tasks and methods we tease apart the effect of design decisions which enable Flan-T5 to outperform prior work by 3-1737;+ across evaluation settings. We find task balancing and enrichment techniques are overlooked but critical to effective instruction tuning and in particular training with mixed prompt settings (zero-shot few-shot and chain-of-thought) actually yields stronger (237;+) performance in all settings. In further experiments we show Flan-T5 requires less finetuning to converge higher and faster than T5 on single downstream tasks motivating instruction-tuned models as more computationally-efficient starting checkpoints for new tasks. Finally to accelerate research on instruction tuning we make the Flan 2022 collection of datasets templates and methods publicly available at https://github.com/google-research/FLAN/tree/main/flan/v2.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [23.74333381652832, -0.09717550873756409]}, {"key": "lovenia2023negative", "year": "2023", "title": "Negative Object Presence Evaluation (NOPE) To Measure Object Hallucination In Vision-language Models", "abstract": "<p>Object hallucination poses a significant challenge in vision-language (VL) models often leading to the generation of nonsensical or unfaithful responses with non-existent objects. However the absence of a general measurement for evaluating object hallucination in VL models has hindered our understanding and ability to mitigate this issue. In this work we present NOPE (Negative Object Presence Evaluation) a novel benchmark designed to assess object hallucination in VL models through visual question answering (VQA). We propose a cost-effective and scalable approach utilizing large language models to generate 29.5k synthetic negative pronoun (NegP) data of high quality for NOPE. We extensively investigate the performance of 10 state-of-the-art VL models in discerning the non-existence of objects in visual questions where the ground truth answers are denoted as NegP (e.g. none). Additionally we evaluate their standard performance on visual questions on 9 other VQA datasets. Through our experiments we demonstrate that no VL model is immune to the vulnerability of object hallucination as all models achieve accuracy below 1037; on NegP. Furthermore we uncover that lexically diverse visual questions question types with large scopes and scene-relevant objects capitalize the risk of object hallucination in VL models.</p>\n", "tags": ["ARXIV", "Cross Modal", "Supervised"], "tsne_embedding": [51.738746643066406, 9.791393280029297]}, {"key": "lu2020label", "year": "2020", "title": "Label Self-Adaption Hashing for Image Retrieval", "abstract": "<p>Hashing has attracted widespread attention in image retrieval because of its fast retrieval speed and low storage cost. Compared with supervised methods, unsupervised hashing methods are more reasonable and suitable for large-scale image retrieval since it is always difficult and expensive to collect true labels of the massive data. Without label information, however, unsupervised hashing methods can not guarantee the quality of learned binary codes. To resolve this dilemma, this paper proposes a novel unsupervised hashing method called Label Self-Adaption Hashing (LSAH), which contains effective hashing function learning part and self-adaption label generation part. In the first part, we utilize anchor graph to keep the local structure of the data and introduce joint sparsity into the model to extract effective features for high-quality binary code learning. In the second part, a self-adaptive cluster label matrix is learned from the data under the assumption that the nearest neighbor points should have a large probability to be in the same cluster. Therefore, the proposed LSAH can make full use of the potential discriminative information of the data to guide the learning of binary code. It is worth noting that LSAH can learn effective binary codes, hashing function and cluster labels simultaneously in a unified optimization framework. To solve the resulting optimization problem, an Augmented Lagrange Multiplier based iterative algorithm is elaborately designed. Extensive experiments on three large-scale data sets indicate the promising performance of the proposed LSAH.</p>\n", "tags": ["ICPR", "Image Retrieval", "Supervised"], "tsne_embedding": [4.963881015777588, -2.932018756866455]}, {"key": "lu2021deep", "year": "2021", "title": "Deep Asymmetric Hashing With Dual Semantic Regression And Class Structure Quantization", "abstract": "<p>Recently deep hashing methods have been widely used in image retrieval task. Most existing deep hashing approaches adopt one-to-one quantization to reduce information loss. However such class-unrelated quantization cannot give discriminative feedback for network training. In addition these methods only utilize single label to integrate supervision information of data for hashing function learning which may result in inferior network generalization performance and relatively low-quality hash codes since the inter-class information of data is totally ignored. In this paper we propose a dual semantic asymmetric hashing (DSAH) method which generates discriminative hash codes under three-fold constraints. Firstly DSAH utilizes class prior to conduct class structure quantization so as to transmit class information during the quantization process. Secondly a simple yet effective label mechanism is designed to characterize both the intra-class compactness and inter-class separability of data thereby achieving semantic-sensitive binary code learning. Finally a meaningful pairwise similarity preserving loss is devised to minimize the distances between class-related network outputs based on an affinity graph. With these three main components high-quality hash codes can be generated through network. Extensive experiments conducted on various datasets demonstrate the superiority of DSAH in comparison with state-of-the-art deep hashing methods.</p>\n", "tags": ["ARXIV", "Graph", "Image Retrieval", "Quantisation", "Supervised"], "tsne_embedding": [-2.0579662322998047, 5.141040802001953]}, {"key": "lu2021slosh", "year": "2021", "title": "SLOSH Set Locality Sensitive Hashing Via Sliced-wasserstein Embeddings", "abstract": "<p>Learning from set-structured data is an essential problem with many applications in machine learning and computer vision. This paper focuses on non-parametric and data-independent learning from set-structured data using approximate nearest neighbor (ANN) solutions particularly locality-sensitive hashing. We consider the problem of set retrieval from an input set query. Such retrieval problem requires 1) an efficient mechanism to calculate the distances/dissimilarities between sets and 2) an appropriate data structure for fast nearest neighbor search. To that end we propose Sliced-Wasserstein set embedding as a computationally efficient set-2-vector mechanism that enables downstream ANN with theoretical guarantees. The set elements are treated as samples from an unknown underlying distribution and the Sliced-Wasserstein distance is used to compare sets. We demonstrate the effectiveness of our algorithm denoted as Set-LOcality Sensitive Hashing (SLOSH) on various set retrieval datasets and compare our proposed embedding with standard set embedding approaches including Generalized Mean (GeM) embedding/pooling Featurewise Sort Pooling (FSPool) and Covariance Pooling and show consistent improvement in retrieval results. The code for replicating our results is available here hrefhttps://github.com/mint-vu/SLOSH}{https://github.com/mint-vu/SLOSH}.</p>\n", "tags": ["ARXIV", "Has Code", "Independent"], "tsne_embedding": [-12.797174453735352, -9.562710762023926]}, {"key": "lu2022asymmetric", "year": "2022", "title": "Asymmetric Transfer Hashing With Adaptive Bipartite Graph Learning", "abstract": "<p>Thanks to the efficient retrieval speed and low storage consumption learning to hash has been widely used in visual retrieval tasks. However existing hashing methods assume that the query and retrieval samples lie in homogeneous feature space within the same domain. As a result they cannot be directly applied to heterogeneous cross-domain retrieval. In this paper we propose a Generalized Image Transfer Retrieval (GITR) problem which encounters two crucial bottlenecks 1) the query and retrieval samples may come from different domains leading to an inevitable domain distribution gap; 2) the features of the two domains may be heterogeneous or misaligned bringing up an additional feature gap. To address the GITR problem we propose an Asymmetric Transfer Hashing (ATH) framework with its unsupervised/semi-supervised/supervised realizations. Specifically ATH characterizes the domain distribution gap by the discrepancy between two asymmetric hash functions and minimizes the feature gap with the help of a novel adaptive bipartite graph constructed on cross-domain data. By jointly optimizing asymmetric hash functions and the bipartite graph not only can knowledge transfer be achieved but information loss caused by feature alignment can also be avoided. Meanwhile to alleviate negative transfer the intrinsic geometrical structure of single-domain data is preserved by involving a domain affinity graph. Extensive experiments on both single-domain and cross-domain benchmarks under different GITR subtasks indicate the superiority of our ATH method in comparison with the state-of-the-art hashing methods.</p>\n", "tags": ["ARXIV", "Cross Modal", "Graph", "Unsupervised"], "tsne_embedding": [-5.583848476409912, 1.8991485834121704]}, {"key": "lu2022dynamic", "year": "2022", "title": "Dynamic Prompt Learning Via Policy Gradient For Semi-structured Mathematical Reasoning", "abstract": "<p>Mathematical reasoning a core ability of human intelligence presents unique challenges for machines in abstract thinking and logical reasoning. Recent large pre-trained language models such as GPT-3 have achieved remarkable progress on mathematical reasoning tasks written in text form such as math word problems (MWP). However it is unknown if the models can handle more complex problems that involve math reasoning over heterogeneous information such as tabular data. To fill the gap we present Tabular Math Word Problems (TabMWP) a new dataset containing 38431 open-domain grade-level problems that require mathematical reasoning on both textual and tabular data. Each question in TabMWP is aligned with a tabular context which is presented as an image semi-structured text and a structured table. There are two types of questions free-text and multi-choice and each problem is annotated with gold solutions to reveal the multi-step reasoning process. We evaluate different pre-trained models on TabMWP including the GPT-3 model in a few-shot setting. As earlier studies suggest since few-shot GPT-3 relies on the selection of in-context examples its performance is unstable and can degrade to near chance. The unstable issue is more severe when handling complex problems like TabMWP. To mitigate this we further propose a novel approach PromptPG which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example. Experimental results show that our method outperforms the best baseline by 5.3137; on the accuracy metric and reduces the prediction variance significantly compared to random selection which verifies its effectiveness in selecting in-context examples.</p>\n", "tags": ["ARXIV", "Cross Modal", "Supervised"], "tsne_embedding": [28.389665603637695, 3.6943910121917725]}, {"key": "lu2023attributes", "year": "2023", "title": "Attributes Grouping And Mining Hashing For Fine-grained Image Retrieval", "abstract": "<p>In recent years hashing methods have been popular in the large-scale media search for low storage and strong representation capabilities. To describe objects with similar overall appearance but subtle differences more and more studies focus on hashing-based fine-grained image retrieval. Existing hashing networks usually generate both local and global features through attention guidance on the same deep activation tensor which limits the diversity of feature representations. To handle this limitation we substitute convolutional descriptors for attention-guided features and propose an Attributes Grouping and Mining Hashing (AGMH) which groups and embeds the category-specific visual attributes in multiple descriptors to generate a comprehensive feature representation for efficient fine-grained image retrieval. Specifically an Attention Dispersion Loss (ADL) is designed to force the descriptors to attend to various local regions and capture diverse subtle details. Moreover we propose a Stepwise Interactive External Attention (SIEA) to mine critical attributes in each descriptor and construct correlations between fine-grained attributes and objects. The attention mechanism is dedicated to learning discrete attributes which will not cost additional computations in hash codes generation. Finally the compact binary codes are learned by preserving pairwise similarities. Experimental results demonstrate that AGMH consistently yields the best performance against state-of-the-art methods on fine-grained benchmark datasets.</p>\n", "tags": ["AAAI", "Image Retrieval", "Independent"], "tsne_embedding": [-8.21892261505127, 15.890105247497559]}, {"key": "lu2023chameleon", "year": "2023", "title": "Chameleon Plug-and-play Compositional Reasoning With Large Language Models", "abstract": "<p>Large language models (LLMs) have achieved remarkable progress in solving various natural language processing tasks due to emergent reasoning abilities. However LLMs have inherent limitations as they are incapable of accessing up-to-date information (stored on the Web or in task-specific knowledge bases) using external tools and performing precise mathematical and logical reasoning. In this paper we present Chameleon an AI system that mitigates these limitations by augmenting LLMs with plug-and-play modules for compositional reasoning. Chameleon synthesizes programs by composing various tools (e.g. LLMs off-the-shelf vision models web search engines Python functions and heuristic-based modules) for accomplishing complex reasoning tasks. At the heart of Chameleon is an LLM-based planner that assembles a sequence of tools to execute to generate the final response. We showcase the effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning tasks ScienceQA and TabMWP. Chameleon powered by GPT-4 achieves an 86.5437; overall accuracy on ScienceQA improving the best published few-shot result by 11.3737;. On TabMWP GPT-4-powered Chameleon improves the accuracy by 17.037; lifting the state of the art to 98.7837;. Our analysis also shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions compared to a ChatGPT-powered planner. The project is available at https://chameleon-llm.github.io.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [28.450496673583984, -14.01950454711914]}, {"key": "lu2023evaluation", "year": "2023", "title": "Evaluation And Enhancement Of Semantic Grounding In Large Vision-language Models", "abstract": "<p>Large Vision-Language Models (LVLMs) offer remarkable benefits for a variety of vision-language tasks. However a challenge hindering their application in real-world scenarios particularly regarding safety robustness and reliability is their constrained semantic grounding ability which pertains to connecting language to the physical-world entities or concepts referenced in images. Therefore a crucial need arises for a comprehensive study to assess the semantic grounding ability of widely used LVLMs. Despite the significance sufficient investigation in this direction is currently lacking. Our work bridges this gap by designing a pipeline for generating large-scale evaluation datasets covering fine-grained semantic information such as color number material etc. along with a thorough assessment of seven popular LVLMs semantic grounding ability. Results highlight prevalent misgrounding across various aspects and degrees. To address this issue we propose a data-centric enhancement method that aims to improve LVLMs semantic grounding ability through multimodal instruction tuning on fine-grained conversations. Experiments on enhanced LVLMs demonstrate notable improvements in addressing misgrounding issues.</p>\n", "tags": ["ARXIV", "Cross Modal", "Survey Paper"], "tsne_embedding": [43.993003845214844, 4.611172199249268]}, {"key": "lu2024aligning", "year": "2024", "title": "Aligning Large Language Models For Controllable Recommendations", "abstract": "<p>Inspired by the exceptional general intelligence of Large Language Models (LLMs) researchers have begun to explore their application in pioneering the next generation of recommender systems - systems that are conversational explainable and controllable. However existing literature primarily concentrates on integrating domain-specific knowledge into LLMs to enhance accuracy often neglecting the ability to follow instructions. To address this gap we initially introduce a collection of supervised learning tasks augmented with labels derived from a conventional recommender model aimed at explicitly improving LLMs proficiency in adhering to recommendation-specific instructions. Subsequently we develop a reinforcement learning-based alignment procedure to further strengthen LLMs aptitude in responding to users intentions and mitigating formatting errors. Through extensive experiments on two real-world datasets our method markedly advances the capability of LLMs to comply with instructions within recommender systems while sustaining a high level of accuracy performance.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [38.27861404418945, -9.573091506958008]}, {"key": "lunga2017hashed", "year": "2017", "title": "Hashed Binary Search Sampling For Convolutional Network Training With Large Overhead Image Patches", "abstract": "<p>Very large overhead imagery associated with ground truth maps has the potential to generate billions of training image patches for machine learning algorithms. However random sampling selection criteria often leads to redundant and noisy-image patches for model training. With minimal research efforts behind this challenge the current status spells missed opportunities to develop supervised learning algorithms that generalize over wide geographical scenes. In addition much of the computational cycles for large scale machine learning are poorly spent crunching through noisy and redundant image patches. We demonstrate a potential framework to address these challenges specifically while evaluating a human settlement detection task. A novel binary search tree sampling scheme is fused with a kernel based hashing procedure that maps image patches into hash-buckets using binary codes generated from image content. The framework exploits inherent redundancy within billions of image patches to promote mostly high variance preserving samples for accelerating algorithmic training and increasing model generalization.</p>\n", "tags": ["ARXIV", "Graph", "Supervised"], "tsne_embedding": [2.3904759883880615, 10.033602714538574]}, {"key": "luo2018collaborative", "year": "2018", "title": "Collaborative Learning For Extremely Low Bit Asymmetric Hashing", "abstract": "<p>Hashing techniques are in great demand for a wide range of real-world applications such as image retrieval and network compression. Nevertheless existing approaches could hardly guarantee a satisfactory performance with the extremely low-bit (e.g. 4-bit) hash codes due to the severe information loss and the shrink of the discrete solution space. In this paper we propose a novel textitCollaborative Learning strategy that is tailored for generating high-quality low-bit hash codes. The core idea is to jointly distill bit-specific and informative representations for a group of pre-defined code lengths. The learning of short hash codes among the group can benefit from the manifold shared with other long codes where multiple views from different hash codes provide the supplementary guidance and regularization making the convergence faster and more stable. To achieve that an asymmetric hashing framework with two variants of multi-head embedding structures is derived termed as Multi-head Asymmetric Hashing (MAH) leading to great efficiency of training and querying. Extensive experiments on three benchmark datasets have been conducted to verify the superiority of the proposed MAH and have shown that the 8-bit hash codes generated by MAH achieve (94.3) of the MAP (Mean Average Precision (MAP)) score on the CIFAR-10 dataset which significantly surpasses the performance of the 48-bit codes by the state-of-the-arts in image retrieval tasks.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Independent"], "tsne_embedding": [-3.6327755451202393, 0.7939600348472595]}, {"key": "luo2018fast", "year": "2018", "title": "Fast Scalable Supervised Hashing", "abstract": "<p>Despite significant progress in supervised hashing, there are three\ncommon limitations of existing methods. First, most pioneer methods discretely learn hash codes bit by bit, making the learning\nprocedure rather time-consuming. Second, to reduce the large complexity of the n by n pairwise similarity matrix, most methods apply\nsampling strategies during training, which inevitably results in information loss and suboptimal performance; some recent methods\ntry to replace the large matrix with a smaller one, but the size is\nstill large. Third, among the methods that leverage the pairwise\nsimilarity matrix, most of them only encode the semantic label\ninformation in learning the hash codes, failing to fully capture\nthe characteristics of data. In this paper, we present a novel supervised hashing method, called Fast Scalable Supervised Hashing\n(FSSH), which circumvents the use of the large similarity matrix by\nintroducing a pre-computed intermediate term whose size is independent with the size of training data. Moreover, FSSH can learn\nthe hash codes with not only the semantic information but also\nthe features of data. Extensive experiments on three widely used\ndatasets demonstrate its superiority over several state-of-the-art\nmethods in both accuracy and scalability. Our experiment codes\nare available at: https://lcbwlx.wixsite.com/fssh.</p>\n", "tags": ["Has Code", "SIGIR", "Supervised"], "tsne_embedding": [2.4054267406463623, -6.703869819641113]}, {"key": "luo2020cimon", "year": "2020", "title": "CIMON Towards High-quality Hash Codes", "abstract": "<p>Recently hashing is widely used in approximate nearest neighbor search for its storage and computational efficiency. Most of the unsupervised hashing methods learn to map images into semantic similarity-preserving hash codes by constructing local semantic similarity structure from the pre-trained model as the guiding information i.e. treating each point pair similar if their distance is small in feature space. However due to the inefficient representation ability of the pre-trained model many false positives and negatives in local semantic similarity will be introduced and lead to error propagation during the hash code learning. Moreover few of the methods consider the robustness of models which will cause instability of hash codes to disturbance. In this paper we propose a new method named textbfComprehensive stextbfImilarity textbfMining and ctextbfOnsistency leartextbfNing (CIMON). First we use global refinement and similarity statistical distribution to obtain reliable and smooth guidance. Second both semantic and contrastive consistency learning are introduced to derive both disturb-invariant and discriminative hash codes. Extensive experiments on several benchmark datasets show that the proposed method outperforms a wide range of state-of-the-art methods in both retrieval performance and robustness.</p>\n", "tags": ["ARXIV", "Unsupervised"], "tsne_embedding": [-6.815147876739502, 3.4615185260772705]}, {"key": "luo2020survey", "year": "2020", "title": "A Survey On Deep Hashing Methods", "abstract": "<p>Nearest neighbor search aims to obtain the samples in the database with the smallest distances from them to the queries which is a basic task in a range of fields including computer vision and data mining. Hashing is one of the most widely used methods for its computational and storage efficiency. With the development of deep learning deep hashing methods show more advantages than traditional methods. In this survey we detailedly investigate current deep hashing algorithms including deep supervised hashing and deep unsupervised hashing. Specifically we categorize deep supervised hashing methods into pairwise methods ranking-based methods pointwise methods as well as quantization according to how measuring the similarities of the learned hash codes. Moreover deep unsupervised hashing is categorized into similarity reconstruction-based methods pseudo-label-based methods and prediction-free self-supervised learning-based methods based on their semantic learning manners. We also introduce three related important topics including semi-supervised deep hashing domain adaption deep hashing and multi-modal deep hashing. Meanwhile we present some commonly used public datasets and the scheme to measure the performance of deep hashing algorithms. Finally we discuss some potential research directions in conclusion.</p>\n", "tags": ["ARXIV", "Deep Learning", "Quantisation", "Survey Paper", "Unsupervised"], "tsne_embedding": [-13.597151756286621, -8.541373252868652]}, {"key": "luo2021deep", "year": "2021", "title": "Deep Unsupervised Hashing By Distilled Smooth Guidance", "abstract": "<p>Hashing has been widely used in approximate nearest neighbor search for its storage and computational efficiency. Deep supervised hashing methods are not widely used because of the lack of labeled data especially when the domain is transferred. Meanwhile unsupervised deep hashing models can hardly achieve satisfactory performance due to the lack of reliable similarity signals. To tackle this problem we propose a novel deep unsupervised hashing method namely Distilled Smooth Guidance (DSG) which can learn a distilled dataset consisting of similarity signals as well as smooth confidence signals. To be specific we obtain the similarity confidence weights based on the initial noisy similarity signals learned from local structures and construct a priority loss function for smooth similarity-preserving learning. Besides global information based on clustering is utilized to distill the image pairs by removing contradictory similarity signals. Extensive experiments on three widely used benchmark datasets show that the proposed DSG consistently outperforms the state-of-the-art search methods.</p>\n", "tags": ["ICME", "Unsupervised"], "tsne_embedding": [-11.670252799987793, 0.9434627294540405]}, {"key": "luo2023survey", "year": "2023", "title": "A Survey on Deep Hashing Methods", "abstract": "<p>Nearest neighbor search aims at obtaining the samples in the database with the smallest distances from them to the queries, which is a basic task in a range of fields, including computer vision and data mining. Hashing is one of the most widely used methods for its computational and storage efficiency. With the development of deep learning, deep hashing methods show more advantages than traditional methods. In this survey, we detailedly investigate current deep hashing algorithms including deep supervised hashing and deep unsupervised hashing. Specifically, we categorize deep supervised hashing methods into pairwise methods, ranking-based methods, pointwise methods as well as quantization according to how measuring the similarities of the learned hash codes. Moreover, deep unsupervised hashing is categorized into similarity reconstruction-based methods, pseudo-label-based methods, and prediction-free self-supervised learning-based methods based on their semantic learning manners. We also introduce three related important topics including semi-supervised deep hashing, domain adaption deep hashing, and multi-modal deep hashing. Meanwhile, we present some commonly used public datasets and the scheme to measure the performance of deep hashing algorithms. Finally, we discuss some potential research directions in conclusion.</p>\n", "tags": ["Semi Supervised", "Survey Paper"], "tsne_embedding": [-13.647292137145996, -8.566923141479492]}, {"key": "luo2024integrating", "year": "2024", "title": "Integrating Large Language Models Into Recommendation Via Mutual Augmentation And Adaptive Aggregation", "abstract": "<p>Conventional recommendation methods have achieved notable advancements by harnessing collaborative or sequential information from user behavior. Recently large language models (LLMs) have gained prominence for their capabilities in understanding and reasoning over textual semantics and have found utility in various domains including recommendation. Conventional recommendation methods and LLMs each have their strengths and weaknesses. While conventional methods excel at mining collaborative information and modeling sequential behavior they struggle with data sparsity and the long-tail problem. LLMs on the other hand are proficient at utilizing rich textual contexts but face challenges in mining collaborative or sequential information. Despite their individual successes there is a significant gap in leveraging their combined potential to enhance recommendation performance. In this paper we introduce a general and model-agnostic framework known as textbfLarge textbflanguage model with textbfmutual augmentation and textbfadaptive aggregation for textbfRecommendation (textbfLlama4Rec). Llama4Rec synergistically combines conventional and LLM-based recommendation models. Llama4Rec proposes data augmentation and prompt augmentation strategies tailored to enhance the conventional model and LLM respectively. An adaptive aggregation module is adopted to combine the predictions of both kinds of models to refine the final recommendation results. Empirical studies on three real-world datasets validate the superiority of Llama4Rec demonstrating its consistent outperformance of baseline methods and significant improvements in recommendation performance.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [42.69425582885742, -7.610082626342773]}, {"key": "lv2007probe", "year": "2007", "title": "Multi-Probe LSH: Efficient Indexing for High-Dimensional Similarity Search", "abstract": "<p>Similarity indices for high-dimensional data are very desirable for building content-based search systems for featurerich data such as audio, images, videos, and other sensor\ndata. Recently, locality sensitive hashing (LSH) and its\nvariations have been proposed as indexing techniques for\napproximate similarity search. A significant drawback of\nthese approaches is the requirement for a large number of\nhash tables in order to achieve good search quality. This paper proposes a new indexing scheme called multi-probe LSH\nthat overcomes this drawback. Multi-probe LSH is built on\nthe well-known LSH technique, but it intelligently probes\nmultiple buckets that are likely to contain query results in\na hash table. Our method is inspired by and improves upon\nrecent theoretical work on entropy-based LSH designed to\nreduce the space requirement of the basic LSH method. We\nhave implemented the multi-probe LSH method and evaluated the implementation with two different high-dimensional\ndatasets. Our evaluation shows that the multi-probe LSH\nmethod substantially improves upon previously proposed\nmethods in both space and time efficiency. To achieve the\nsame search quality, multi-probe LSH has a similar timeefficiency as the basic LSH method while reducing the number of hash tables by an order of magnitude. In comparison\nwith the entropy-based LSH method, to achieve the same\nsearch quality, multi-probe LSH uses less query time and 5\nto 8 times fewer number of hash tables.</p>\n", "tags": ["Has Code", "LSH", "VLDB"], "tsne_embedding": [-20.01599884033203, -1.6898938417434692]}, {"key": "lyu2023faithful", "year": "2023", "title": "Faithful Chain-of-thought Reasoning", "abstract": "<p>While Chain-of-Thought (CoT) prompting boosts Language Models (LM) performance on a gamut of complex reasoning tasks the generated reasoning chain does not necessarily reflect how the model arrives at the answer (aka. faithfulness). We propose Faithful CoT a reasoning framework involving two stages Translation (Natural Language query (rightarrow) symbolic reasoning chain) and Problem Solving (reasoning chain (rightarrow) answer) using an LM and a deterministic solver respectively. This guarantees that the reasoning chain provides a faithful explanation of the final answer. Aside from interpretability Faithful CoT also improves empirical performance it outperforms standard CoT on 9 of 10 benchmarks from 4 diverse domains with a relative accuracy gain of 6.337; on Math Word Problems (MWP) 3.437; on Planning 5.537; on Multi-hop Question Answering (QA) and 21.437; on Relational Inference. Furthermore with GPT-4 and Codex it sets the new state-of-the-art few-shot performance on 7 datasets (with 95.0+ accuracy on 6 of them) showing a strong synergy between faithfulness and accuracy.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [24.373050689697266, 2.557650089263916]}, {"key": "m2023chemcrow", "year": "2023", "title": "Chemcrow Augmenting Large-language Models With Chemistry Tools", "abstract": "<p>Over the last decades excellent computational chemistry tools have been developed. Integrating them into a single platform with enhanced accessibility could help reaching their full potential by overcoming steep learning curves. Recently large-language models (LLMs) have shown strong performance in tasks across domains but struggle with chemistry-related problems. Moreover these models lack access to external knowledge sources limiting their usefulness in scientific applications. In this study we introduce ChemCrow an LLM chemistry agent designed to accomplish tasks across organic synthesis drug discovery and materials design. By integrating 18 expert-designed tools ChemCrow augments the LLM performance in chemistry and new capabilities emerge. Our agent autonomously planned and executed the syntheses of an insect repellent three organocatalysts and guided the discovery of a novel chromophore. Our evaluation including both LLM and expert assessments demonstrates ChemCrows effectiveness in automating a diverse set of chemical tasks. Surprisingly we find that GPT-4 as an evaluator cannot distinguish between clearly wrong GPT-4 completions and Chemcrows performance. Our work not only aids expert chemists and lowers barriers for non-experts but also fosters scientific advancement by bridging the gap between experimental and computational chemistry.</p>\n", "tags": ["ARXIV", "Deep Learning"], "tsne_embedding": [24.6689453125, -14.663064002990723]}, {"key": "m2023zero", "year": "2023", "title": "Zero-shot Recommendations With Pre-trained Large Language Models For Multimodal Nudging", "abstract": "<p>We present a method for zero-shot recommendation of multimodal non-stationary content that leverages recent advancements in the field of generative AI. We propose rendering inputs of different modalities as textual descriptions and to utilize pre-trained LLMs to obtain their numerical representations by computing semantic embeddings. Once unified representations of all content items are obtained the recommendation can be performed by computing an appropriate similarity metric between them without any additional learning. We demonstrate our approach on a synthetic multimodal nudging environment where the inputs consist of tabular textual and visual data.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [20.465173721313477, 9.737753868103027]}, {"key": "ma2018progressive", "year": "2018", "title": "Progressive Generative Hashing for Image Retrieval", "abstract": "<p>Recent years have witnessed the success of the emerging hashing techniques in large-scale image\nretrieval. Owing to the great learning capacity,\ndeep hashing has become one of the most promising solutions, and achieved attractive performance\nin practice. However, without semantic label information, the unsupervised deep hashing still remains\nan open question. In this paper, we propose a novel\nprogressive generative hashing (PGH) framework\nto help learn a discriminative hashing network in an\nunsupervised way. Different from existing studies,\nit first treats the hash codes as a kind of semantic\ncondition for the similar image generation, and simultaneously feeds the original image and its codes\ninto the generative adversarial networks (GANs).\nThe real images together with the synthetic ones\ncan further help train a discriminative hashing network based on a triplet loss. By iteratively inputting\nthe learnt codes into the hash conditioned GANs, we can progressively enable the hashing network\nto discover the semantic relations. Extensive experiments on the widely-used image datasets demonstrate that PGH can significantly outperform stateof-the-art unsupervised hashing methods.</p>\n", "tags": ["Deep Learning", "GAN", "IJCAI", "Image Retrieval", "Supervised"], "tsne_embedding": [6.304718017578125, 7.4897050857543945]}, {"key": "ma2019hierarchy", "year": "2019", "title": "Hierarchy Neighborhood Discriminative Hashing For An Unified View Of Single-label And Multi-label Image Retrieval", "abstract": "<p>Recently deep supervised hashing methods have become popular for large-scale image retrieval task. To preserve the semantic similarity notion between examples they typically utilize the pairwise supervision or the triplet supervised information for hash learning. However these methods usually ignore the semantic class information which can help the improvement of the semantic discriminative ability of hash codes. In this paper we propose a novel hierarchy neighborhood discriminative hashing method. Specifically we construct a bipartite graph to build coarse semantic neighbourhood relationship between the sub-class feature centers and the embeddings features. Moreover we utilize the pairwise supervised information to construct the fined semantic neighbourhood relationship between embeddings features. Finally we propose a hierarchy neighborhood discriminative hashing loss to unify the single-label and multilabel image retrieval problem with a one-stream deep neural network architecture. Experimental results on two largescale datasets demonstrate that the proposed method can outperform the state-of-the-art hashing methods.</p>\n", "tags": ["ARXIV", "Graph", "Image Retrieval", "Supervised"], "tsne_embedding": [-9.05334186553955, 10.055523872375488]}, {"key": "ma2021rank", "year": "2021", "title": "Rank-consistency Deep Hashing For Scalable Multi-label Image Search", "abstract": "<p>As hashing becomes an increasingly appealing technique for large-scale image retrieval multi-label hashing is also attracting more attention for the ability to exploit multi-level semantic contents. In this paper we propose a novel deep hashing method for scalable multi-label image search. Unlike existing approaches with conventional objectives such as contrast and triplet losses we employ a rank list rather than pairs or triplets to provide sufficient global supervision information for all the samples. Specifically a new rank-consistency objective is applied to align the similarity orders from two spaces the original space and the hamming space. A powerful loss function is designed to penalize the samples whose semantic similarity and hamming distance are mismatched in two spaces. Besides a multi-label softmax cross-entropy loss is presented to enhance the discriminative power with a concise formulation of the derivative function. In order to manipulate the neighborhood structure of the samples with different labels we design a multi-label clustering loss to cluster the hashing vectors of the samples with the same labels by reducing the distances between the samples and their multiple corresponding class centers. The state-of-the-art experimental results achieved on three public multi-label datasets MIRFLICKR-25K IAPRTC12 and NUS-WIDE demonstrate the effectiveness of the proposed method.</p>\n", "tags": ["Image Retrieval", "Unsupervised"], "tsne_embedding": [-17.70222282409668, 7.814050674438477]}, {"key": "ma2023llm", "year": "2023", "title": "Llm-pruner On The Structural Pruning Of Large Language Models", "abstract": "<p>Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However such impressive capability typically comes with a substantial model size which presents significant challenges in both the deployment inference and training stages. With LLM being a general-purpose task solver we explore its compression in a task-agnostic manner which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM which makes both data transfer and model post-training over-burdensome. Thus we tackle the compression of LLMs within the bound of two constraints being task-agnostic and minimizing the reliance on the original training dataset. Our method named LLM-Pruner adopts structural pruning that selectively removes non-critical coupled structures based on gradient information maximally preserving the majority of the LLMs functionality. To this end the performance of pruned models can be efficiently recovered through tuning techniques LoRA in merely 3 hours requiring only 50K data. We validate the LLM-Pruner on three LLMs including LLaMA Vicuna and ChatGLM and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at https://github.com/horseee/LLM-Pruner</p>\n", "tags": ["ARXIV", "Has Code", "Supervised"], "tsne_embedding": [18.66411590576172, -4.553014278411865]}, {"key": "ma2024harr", "year": "2024", "title": "HARR: Learning Discriminative and High-quality Hash Codes for Image Retrieval", "abstract": "<p>This article studies deep unsupervised hashing, which has attracted increasing attention in large-scale image retrieval. The majority of recent approaches usually reconstruct semantic similarity information, which then guides the hash code learning. However, they still fail to achieve satisfactory performance in reality for two reasons. On the one hand, without accurate supervised information, these methods usually fail to produce independent and robust hash codes with semantics information well preserved, which may hinder effective image retrieval. On the other hand, due to discrete constraints, how to effectively optimize the hashing network in an end-to-end manner with small quantization errors remains a problem. To address these difficulties, we propose a novel unsupervised hashing method called HARR to learn discriminative and high-quality hash codes. To comprehensively explore semantic similarity structure, HARR adopts the Winner-Take-All hash to model the similarity structure. Then similarity-preserving hash codes are learned under the reliable guidance of the reconstructed similarity structure. Additionally, we improve the quality of hash codes by a bit correlation reduction module, which forces the cross-correlation matrix between a batch of hash codes under different augmentations to approach the identity matrix. In this way, the generated hash bits are expected to be invariant to disturbances with minimal redundancy, which can be further interpreted as an instantiation of the information bottleneck principle. Finally, for effective hashing network training, we minimize the cosine distances between real-value network outputs and their binary codes for small quantization errors. Extensive experiments demonstrate the effectiveness of our proposed HARR.</p>\n", "tags": ["Deep Learning", "Supervised", "TOMM"], "tsne_embedding": [-6.325290203094482, 2.263850212097168]}, {"key": "madaan2022language", "year": "2022", "title": "Language Models Of Code Are Few-shot Commonsense Learners", "abstract": "<p>We address the general task of structured commonsense reasoning given a natural language input the goal is to generate a graph such as an event \u2013 or a reasoning-graph. To employ large language models (LMs) for this task existing approaches serialize the output graph as a flat list of nodes and edges. Although feasible these serialized graphs strongly deviate from the natural language corpora that LMs were pre-trained on hindering LMs from generating them correctly. In this paper we show that when we instead frame structured commonsense reasoning tasks as code generation tasks pre-trained LMs of code are better structured commonsense reasoners than LMs of natural language even when the downstream task does not involve source code at all. We demonstrate our approach across three diverse structured commonsense reasoning tasks. In all these natural language tasks we show that using our approach a code generation LM (CODEX) outperforms natural-LMs that are fine-tuned on the target task (e.g. T5) and other strong LMs such as GPT-3 in the few-shot setting.</p>\n", "tags": ["ARXIV", "Graph"], "tsne_embedding": [2.9261696338653564, 26.438779830932617]}, {"key": "madaan2022text", "year": "2022", "title": "Text And Patterns For Effective Chain Of Thought It Takes Two To Tango", "abstract": "<p>The past decade has witnessed dramatic gains in natural language processing and an unprecedented scaling of large language models. These developments have been accelerated by the advent of few-shot techniques such as chain of thought (CoT) prompting. Specifically CoT pushes the performance of large language models in a few-shot setup by augmenting the prompts with intermediate steps. Despite impressive results across various tasks the reasons behind their success have not been explored. This work uses counterfactual prompting to develop a deeper understanding of CoT-based few-shot prompting mechanisms in large language models. We first systematically identify and define the key components of a prompt symbols patterns and text. Then we devise and conduct an exhaustive set of experiments across four different tasks by querying the model with counterfactual prompts where only one of these components is altered. Our experiments across three models (PaLM GPT-3 and CODEX) reveal several surprising findings and brings into question the conventional wisdom around few-shot prompting. First the presence of factual patterns in a prompt is practically immaterial to the success of CoT. Second our results conclude that the primary role of intermediate steps may not be to facilitate learning how to solve a task. The intermediate steps are rather a beacon for the model to realize what symbols to replicate in the output to form a factual answer. Further text imbues patterns with commonsense knowledge and meaning. Our empirical and qualitative analysis reveals that a symbiotic relationship between text and patterns explains the success of few-shot prompting text helps extract commonsense from the question to help patterns and patterns enforce task understanding and direct text generation.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [35.532676696777344, 2.266017198562622]}, {"key": "magliani2018efficient", "year": "2018", "title": "Efficient Nearest Neighbors Search For Large-scale Landmark Recognition", "abstract": "<p>The problem of landmark recognition has achieved excellent results in small-scale datasets. When dealing with large-scale retrieval issues that were irrelevant with small amount of data quickly become fundamental for an efficient retrieval phase. In particular computational time needs to be kept as low as possible whilst the retrieval accuracy has to be preserved as much as possible. In this paper we propose a novel multi-index hashing method called Bag of Indexes (BoI) for Approximate Nearest Neighbors (ANN) search. It allows to drastically reduce the query time and outperforms the accuracy results compared to the state-of-the-art methods for large-scale landmark recognition. It has been demonstrated that this family of algorithms can be applied on different embedding techniques like VLAD and R-MAC obtaining excellent results in very short times on different public datasets Holidays+Flickr1M Oxford105k and Paris106k.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-26.819456100463867, -11.932141304016113]}, {"key": "maier2017dynamic", "year": "2017", "title": "Dynamic Space Efficient Hashing", "abstract": "<p>We consider space efficient hash tables that can grow and shrink dynamically and are always highly space efficient i.e. their space consumption is always close to the lower bound even while growing and when taking into account storage that is only needed temporarily. None of the traditionally used hash tables have this property. We show how known approaches like linear probing and bucket cuckoo hashing can be adapted to this scenario by subdividing them into many subtables or using virtual memory overcommitting. However these rather straightforward solutions suffer from slow amortized insertion times due to frequent reallocation in small increments. Our main result is DySECT ((bf) Dynamic (bf) Space (bf) Efficient (bf) Cuckoo (bf) Table) which avoids these problems. DySECT consists of many subtables which grow by doubling their size. The resulting inhomogeneity in subtable sizes is equalized by the flexibility available in bucket cuckoo hashing where each element can go to several buckets each of which containing several cells. Experiments indicate that DySECT works well with load factors up to 9837;. With up to 2.7 times better performance than the next best solution.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-27.76852798461914, -16.118513107299805]}, {"key": "maity2023image", "year": "2023", "title": "Image Hash Minimization For Tamper Detection", "abstract": "<p>Tamper detection using image hash is a very common problem of modern days. Several research and advancements have already been done to address this problem. However most of the existing methods lack the accuracy of tamper detection when the tampered area is low as well as requiring long image hashes. In this paper we propose a novel method objectively to minimize the hash length while enhancing the performance at low tampered area.</p>\n", "tags": [], "tsne_embedding": [0.5127617716789246, -22.235239028930664]}, {"key": "manakul2023selfcheckgpt", "year": "2023", "title": "Selfcheckgpt Zero-resource Black-box Hallucination Detection For Generative Large Language Models", "abstract": "<p>Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate often complex modules. In this work we propose SelfCheckGPT a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept sampled responses are likely to be similar and contain consistent facts. However for hallucinated facts stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [33.93756103515625, 0.5489763021469116]}, {"key": "mandal2020novel", "year": "2020", "title": "A Novel Incremental Cross-modal Hashing Approach", "abstract": "<p>Cross-modal retrieval deals with retrieving relevant items from one modality when provided with a search query from another modality. Hashing techniques where the data is represented as binary bits have specifically gained importance due to the ease of storage fast computations and high accuracy. In real world the number of data categories is continuously increasing which requires algorithms capable of handling this dynamic scenario. In this work we propose a novel incremental cross-modal hashing algorithm termed iCMH which can adapt itself to handle incoming data of new categories. The proposed approach consists of two sequential stages namely learning the hash codes and training the hash functions. At every stage a small amount of old category data termed exemplars is is used so as not to forget the old data while trying to learn for the new incoming data i.e. to avoid catastrophic forgetting. In the first stage the hash codes for the exemplars is used and simultaneously hash codes for the new data is computed such that it maintains the semantic relations with the existing data. For the second stage we propose both a non-deep and deep architectures to learn the hash functions effectively. Extensive experiments across a variety of cross-modal datasets and comparisons with state-of-the-art cross-modal algorithms shows the usefulness of our approach.</p>\n", "tags": ["ARXIV", "Cross Modal", "Independent"], "tsne_embedding": [1.5912708044052124, -11.418828964233398]}, {"key": "markchit2019effective", "year": "2019", "title": "Effective And Efficient Indexing In Cross-modal Hashing-based Datasets", "abstract": "<p>To overcome the barrier of storage and computation the hashing technique has been widely used for nearest neighbor search in multimedia retrieval applications recently. Particularly cross-modal retrieval that searches across different modalities becomes an active but challenging problem. Although dozens of cross-modal hashing algorithms are proposed to yield compact binary codes the exhaustive search is impractical for the real-time purpose and Hamming distance computation suffers inaccurate results. In this paper we propose a novel search method that utilizes a probability-based index scheme over binary hash codes in cross-modal retrieval. The proposed hash code indexing scheme exploits a few binary bits of the hash code as the index code. We construct an inverted index table based on index codes and train a neural network to improve the indexing accuracy and efficiency. Experiments are performed on two benchmark datasets for retrieval across image and text modalities where hash codes are generated by three cross-modal hashing methods. Results show the proposed method effectively boost the performance on these hash methods.</p>\n", "tags": ["ARXIV", "Cross Modal", "Supervised"], "tsne_embedding": [5.287434101104736, 1.8288906812667847]}, {"key": "masci2012multimodal", "year": "2012", "title": "Multimodal Similarity-preserving Hashing", "abstract": "<p>We introduce an efficient computational framework for hashing data belonging to multiple modalities into a single representation space where they become mutually comparable. The proposed approach is based on a novel coupled siamese neural network architecture and allows unified treatment of intra- and inter-modality similarity learning. Unlike existing cross-modality similarity learning approaches our hashing functions are not limited to binarized linear projections and can assume arbitrarily complex forms. We show experimentally that our method significantly outperforms state-of-the-art hashing approaches on multimedia retrieval tasks.</p>\n", "tags": ["ARXIV", "Cross Modal", "Supervised"], "tsne_embedding": [-16.537153244018555, -2.856046438217163]}, {"key": "masci2013sparse", "year": "2013", "title": "Sparse Similarity-preserving Hashing", "abstract": "<p>In recent years a lot of attention has been devoted to efficient nearest neighbor search by means of similarity-preserving hashing. One of the plights of existing hashing techniques is the intrinsic trade-off between performance and computational complexity while longer hash codes allow for lower false positive rates it is very difficult to increase the embedding dimensionality without incurring in very high false negatives rates or prohibiting computational costs. In this paper we propose a way to overcome this limitation by enforcing the hash codes to be sparse. Sparse high-dimensional codes enjoy from the low false positive rates typical of long hashes while keeping the false negative rates similar to those of a shorter dense hashing scheme with equal number of degrees of freedom. We use a tailored feed-forward neural network for the hashing function. Extensive experimental evaluation involving visual and multi-modal data shows the benefits of the proposed method.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [-19.923593521118164, 3.4837374687194824]}, {"key": "masson2024fliphash", "year": "2024", "title": "Fliphash A Constant-time Consistent Range-hashing Algorithm", "abstract": "<p>Consistent range-hashing is a technique used in distributed systems either directly or as a subroutine for consistent hashing commonly to realize an even and stable data distribution over a variable number of resources. We introduce FlipHash a consistent range-hashing algorithm with constant time complexity and low memory requirements. Like Jump Consistent Hash FlipHash is intended for applications where resources can be indexed sequentially. Under this condition it ensures that keys are hashed evenly across resources and that changing the number of resources only causes keys to be remapped from a removed resource or to an added one but never shuffled across persisted ones. FlipHash differentiates itself with its low computational cost achieving constant-time complexity. We show that FlipHash beats Jump Consistent Hashs cost which is logarithmic in the number of resources both theoretically and in experiments over practical settings.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-24.628143310546875, -18.80765724182129]}, {"key": "mathur2018multichannel", "year": "2018", "title": "Multichannel Distributed Local Pattern For Content Based Indexing And Retrieval", "abstract": "<p>A novel color feature descriptor Multichannel Distributed Local Pattern (MDLP) is proposed in this manuscript. The MDLP combines the salient features of both local binary and local mesh patterns in the neighborhood. The multi-distance information computed by the MDLP aids in robust extraction of the texture arrangement. Further MDLP features are extracted for each color channel of an image. The retrieval performance of the MDLP is evaluated on the three benchmark datasets for CBIR namely Corel-5000 Corel-10000 and MIT-Color Vistex respectively. The proposed technique attains substantial improvement as compared to other state-of- the-art feature descriptors in terms of various evaluation parameters such as ARP and ARR on the respective databases.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-13.045541763305664, 21.052989959716797]}, {"key": "mccauley2019approximate", "year": "2019", "title": "Approximate Similarity Search Under Edit Distance Using Locality-sensitive Hashing", "abstract": "<p>Edit distance similarity search also called approximate pattern matching is a fundamental problem with widespread database applications. The goal of the problem is to preprocess n strings of length d to quickly answer queries q of the form if there is a database string within edit distance r of q return a database string within edit distance cr of q. Previous approaches to this problem either rely on very large (superconstant) approximation ratios c or very small search radii r. Outside of a narrow parameter range these solutions are not competitive with trivially searching through all n strings. In this work give a simple and easy-to-implement hash function that can quickly answer queries for a wide range of parameters. Specifically our strategy can answer queries in time (tildeO)(d3^rn^1/c). The best known practical results require c (gg) r to achieve any correctness guarantee; meanwhile the best known theoretical results are very involved and difficult to implement and require query time at least 24^r. Our results significantly broaden the range of parameters for which we can achieve nontrivial bounds while retaining the practicality of a locality-sensitive hash function. We also show how to apply our ideas to the closely-related Approximate Nearest Neighbor problem for edit distance obtaining similar time bounds.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-20.465652465820312, -7.487350940704346]}, {"key": "mckeown2022hamming", "year": "2022", "title": "Hamming Distributions Of Popular Perceptual Hashing Techniques", "abstract": "<p>Content-based file matching has been widely deployed for decades largely for the detection of sources of copyright infringement extremist materials and abusive sexual media. Perceptual hashes such as Microsofts PhotoDNA are one automated mechanism for facilitating detection allowing for machines to approximately match visual features of an image or video in a robust manner. However there does not appear to be much public evaluation of such approaches particularly when it comes to how effective they are against content-preserving modifications to media files. In this paper we present a million-image scale evaluation of several perceptual hashing archetypes for popular algorithms (including Facebooks PDQ Apples Neuralhash and the popular pHash library) against seven image variants. The focal point is the distribution of Hamming distance scores between both unrelated images and image variants to better understand the problems faced by each approach.</p>\n", "tags": [], "tsne_embedding": [-0.9412999153137207, -21.87308692932129]}, {"key": "mendelson2018anchorhash", "year": "2018", "title": "Anchorhash A Scalable Consistent Hash", "abstract": "<p>Consistent hashing (CH) is a central building block in many networking applications from datacenter load-balancing to distributed storage. Unfortunately state-of-the-art CH solutions cannot ensure full consistency under arbitrary changes and/or cannot scale while maintaining reasonable memory footprints and update times. We present AnchorHash a scalable and fully-consistent hashing algorithm. AnchorHash achieves high key lookup rates a low memory footprint and low update times. We formally establish its strong theoretical guarantees and present advanced implementations with a memory footprint of only a few bytes per resource. Moreover extensive evaluations indicate that it outperforms state-of-the-art algorithms and that it can scale on a single core to 100 million resources while still achieving a key lookup rate of more than 15 million keys per second.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-25.886362075805664, -18.984594345092773]}, {"key": "mialon2023augmented", "year": "2023", "title": "Augmented Language Models A Survey", "abstract": "<p>This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter. LMs can leverage these augmentations separately or in combination via heuristics or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective such augmented LMs can use various possibly non-parametric external modules to expand their context processing ability thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows ALMs to learn to reason use tools and even act while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks. In this work after reviewing current advance in ALMs we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability consistency and scalability issues.</p>\n", "tags": ["ARXIV", "Survey Paper"], "tsne_embedding": [35.34840774536133, -3.7440133094787598]}, {"key": "microsoftspacev1B", "year": "2021", "title": "Microsoft SPACEV-1B", "abstract": "<p>Microsoft SPACEV-1B is a new web search related dataset released by Microsoft Bing for this competition. It consists of document and query vectors encoded by Microsoft SpaceV Superior model to capture generic intent representation.</p>\n", "tags": ["Dataset"], "tsne_embedding": [12.962642669677734, 22.995012283325195]}, {"key": "microsoftturinganns1B", "year": "2021", "title": "Microsoft Turing-ANNS-1B", "abstract": "<p>Microsoft Turing-ANNS-1B is a new dataset being released by the Microsoft Turing team for this competition. It consists of Bing queries encoded by Turing AGI v5 that trains Transformers to capture similarity of intent in web search queries. An early version of the RNN-based AGI Encoder is described in a SIGIR\u201919 paper and a blogpost.</p>\n", "tags": ["Dataset"], "tsne_embedding": [13.150036811828613, 23.220293045043945]}, {"key": "mikriukov2022deep", "year": "2022", "title": "Deep Unsupervised Contrastive Hashing For Large-scale Cross-modal Text-image Retrieval In Remote Sensing", "abstract": "<p>Due to the availability of large-scale multi-modal data (e.g. satellite images acquired by different sensors text sentences etc) archives the development of cross-modal retrieval systems that can search and retrieve semantically relevant data across different modalities based on a query in any modality has attracted great attention in RS. In this paper we focus our attention on cross-modal text-image retrieval where queries from one modality (e.g. text) can be matched to archive entries from another (e.g. image). Most of the existing cross-modal text-image retrieval systems require a high number of labeled training samples and also do not allow fast and memory-efficient retrieval due to their intrinsic characteristics. These issues limit the applicability of the existing cross-modal retrieval systems for large-scale applications in RS. To address this problem in this paper we introduce a novel deep unsupervised cross-modal contrastive hashing (DUCH) method for RS text-image retrieval. The proposed DUCH is made up of two main modules 1) feature extraction module (which extracts deep representations of the text-image modalities); and 2) hashing module (which learns to generate cross-modal binary hash codes from the extracted representations). Within the hashing module we introduce a novel multi-objective loss function including i) contrastive objectives that enable similarity preservation in both intra- and inter-modal similarities; ii) an adversarial objective that is enforced across two modalities for cross-modal representation consistency; iii) binarization objectives for generating representative hash codes. Experimental results show that the proposed DUCH outperforms state-of-the-art unsupervised cross-modal hashing methods on two multi-modal (image and text) benchmark archives in RS. Our code is publicly available at https://git.tu-berlin.de/rsim/duch.\u201d</p>\n", "tags": ["ARXIV", "Cross Modal", "Has Code", "Image Retrieval", "Unsupervised"], "tsne_embedding": [8.011995315551758, -17.680095672607422]}, {"key": "mikriukov2022unsupervised", "year": "2022", "title": "Unsupervised Contrastive Hashing For Cross-modal Retrieval In Remote Sensing", "abstract": "<p>The development of cross-modal retrieval systems that can search and retrieve semantically relevant data across different modalities based on a query in any modality has attracted great attention in remote sensing (RS). In this paper we focus our attention on cross-modal text-image retrieval where queries from one modality (e.g. text) can be matched to archive entries from another (e.g. image). Most of the existing cross-modal text-image retrieval systems in RS require a high number of labeled training samples and also do not allow fast and memory-efficient retrieval. These issues limit the applicability of the existing cross-modal retrieval systems for large-scale applications in RS. To address this problem in this paper we introduce a novel unsupervised cross-modal contrastive hashing (DUCH) method for text-image retrieval in RS. To this end the proposed DUCH is made up of two main modules 1) feature extraction module which extracts deep representations of two modalities; 2) hashing module that learns to generate cross-modal binary hash codes from the extracted representations. We introduce a novel multi-objective loss function including i) contrastive objectives that enable similarity preservation in intra- and inter-modal similarities; ii) an adversarial objective that is enforced across two modalities for cross-modal representation consistency; and iii) binarization objectives for generating hash codes. Experimental results show that the proposed DUCH outperforms state-of-the-art methods. Our code is publicly available at https://git.tu-berlin.de/rsim/duch.\u201d</p>\n", "tags": ["ARXIV", "Cross Modal", "Has Code", "Image Retrieval", "Unsupervised"], "tsne_embedding": [8.01888656616211, -17.672286987304688]}, {"key": "min2021metaicl", "year": "2021", "title": "Metaicl Learning To Learn In Context", "abstract": "<p>We introduce MetaICL (Meta-training for In-Context Learning) a new meta-training framework for few-shot learning where a pretrained language model is tuned to do in-context learning on a large set of training tasks. This meta-training enables the model to more effectively learn a new task in context at test time by simply conditioning on a few training examples with no parameter updates or task-specific templates. We experiment on a large diverse collection of tasks consisting of 142 NLP datasets including classification question answering natural language inference paraphrase detection and more across seven different meta-training/target splits. MetaICL outperforms a range of baselines including in-context learning without meta-training and multi-task learning followed by zero-shot transfer. We find that the gains are particularly significant for target tasks that have domain shifts from the meta-training tasks and that using a diverse set of the meta-training tasks is key to improvements. We also show that MetaICL approaches (and sometimes beats) the performance of models fully finetuned on the target task and outperforms much bigger models with nearly 8x parameters. Finally we show that MetaICL is complementary to human-written instructions and the best performance can be achieved by combining both approaches.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [25.38160514831543, -3.257221221923828]}, {"key": "ming2008improvement", "year": "2008", "title": "The Improvement Of The Bound On Hash Family", "abstract": "<p>In this paper we study the bound on three kinds of hash family using the Singleton bound. To (epsilon)-U(N; n m) hash family in the caes of nm^21 and 1(geq)(epsilon)(geq) (epsilon)_1(n m) we get that the new bound is better. To (epsilon)-(bigtriangleup) U(N; n m) hash family in the case of nm1 and 1(geq)(epsilon)(geq)(epsilon)_3(nm) the new bound is better. To (epsilon)-SU(N; n m) hash family in the case of n2^m2 and 1(geq)(epsilon)(geq) (epsilon)_4(n m) we get that the new bound is better.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-33.95448303222656, -0.3044572174549103]}, {"key": "mirflickr2008new", "year": "2008", "title": "The MIR Flickr Retrieval Evaluation.", "abstract": "<p>In most well known image retrieval test sets, the imagery\ntypically cannot be freely distributed or is not representative of a\nlarge community of users. In this paper we present a collection\nfor the MIR community comprising 25000 images from the Flickr\nwebsite which are redistributable for research purposes and\nrepresent a real community of users both in the image content and\nimage tags. We have extracted the tags and EXIF image metadata,\nand also make all of these publicly available. In addition we\ndiscuss several challenges for benchmarking retrieval and\nclassification methods.</p>\n", "tags": ["Dataset"], "tsne_embedding": [10.672900199890137, 16.640146255493164]}, {"key": "mishchuk2017working", "year": "2017", "title": "Working Hard To Know Your Neighbors Margins Local Descriptor Learning Loss", "abstract": "<p>We introduce a loss for metric learning which is inspired by the Lowes matching criterion for SIFT. We show that the proposed loss that maximizes the distance between the closest positive and closest negative example in the batch is better than complex regularization methods; it works well for both shallow and deep convolution network architectures. Applying the novel loss to the L2Net CNN architecture results in a compact descriptor named HardNet. It has the same dimensionality as SIFT (128) and shows state-of-art performance in wide baseline stereo patch verification and instance retrieval benchmarks.</p>\n", "tags": ["CNN", "NEURIPS"], "tsne_embedding": [3.755500555038452, 16.404325485229492]}, {"key": "mishra2021cross", "year": "2021", "title": "Cross-task Generalization Via Natural Language Crowdsourcing Instructions", "abstract": "<p>Humans (e.g. crowdworkers) have a remarkable ability in solving different tasks by simply reading textual instructions that define them and looking at a few examples. Despite the success of the conventional supervised learning on individual datasets such models often struggle with generalization across tasks (e.g. a question-answering system cannot solve classification tasks). A long-standing challenge in AI is to build a model that learns a new task by understanding the human-readable instructions that define it. To study this we introduce NATURAL INSTRUCTIONS a dataset of 61 distinct tasks their human-authored instructions and 193k task instances (input-output pairs). The instructions are obtained from crowdsourcing instructions used to create existing NLP datasets and mapped to a unified schema. Using this meta-dataset we measure cross-task generalization by training models on seen tasks and measuring generalization to the remaining unseen ones. We adopt generative pre-trained language models to encode task-specific instructions along with input and generate task output. Our results indicate that models benefit from instructions when evaluated in terms of generalization to unseen tasks (1937; better for models utilizing instructions). These models however are far behind an estimated performance upperbound indicating significant room for more progress in this direction.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [29.639759063720703, -7.294417381286621]}, {"key": "misra2018bernoulli", "year": "2018", "title": "Bernoulli Embeddings For Graphs", "abstract": "<p>Just as semantic hashing can accelerate information retrieval binary valued embeddings can significantly reduce latency in the retrieval of graphical data. We introduce a simple but effective model for learning such binary vectors for nodes in a graph. By imagining the embeddings as independent coin flips of varying bias continuous optimization techniques can be applied to the approximate expected loss. Embeddings optimized in this fashion consistently outperform the quantization of both spectral graph embeddings and various learned real-valued embeddings on both ranking and pre-ranking tasks for a variety of datasets.</p>\n", "tags": ["ARXIV", "Graph", "Independent", "Quantisation"], "tsne_embedding": [-3.2369725704193115, 25.284019470214844]}, {"key": "mitzenmacher2012balanced", "year": "2012", "title": "Balanced Allocations And Double Hashing", "abstract": "<p>Double hashing has recently found more common usage in schemes that use multiple hash functions. In double hashing for an item x one generates two hash values f(x) and g(x) and then uses combinations (f(x) +k g(x)) (bmod) n for k=012\u2026 to generate multiple hash values from the initial two. We first perform an empirical study showing that surprisingly the performance difference between double hashing and fully random hashing appears negligible in the standard balanced allocation paradigm where each item is placed in the least loaded of d choices as well as several related variants. We then provide theoretical results that explain the behavior of double hashing in this context.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-25.174665451049805, -6.196959972381592]}, {"key": "mnist1999mnist", "year": "1999", "title": "The MNIST Database of Handwritten Digits", "abstract": "<p>The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\nIt is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.</p>\n", "tags": ["Dataset"], "tsne_embedding": [24.970460891723633, -5.2154998779296875]}, {"key": "modarressi2023ret", "year": "2023", "title": "RET-LLM Towards A General Read-write Memory For Large Language Models", "abstract": "<p>Large language models (LLMs) have significantly advanced the field of natural language processing (NLP) through their extensive parameters and comprehensive data utilization. However existing LLMs lack a dedicated memory unit limiting their ability to explicitly store and retrieve knowledge for various tasks. In this paper we propose RET-LLM a novel framework that equips LLMs with a general write-read memory unit allowing them to extract store and recall knowledge from the text as needed for task performance. Inspired by Davidsonian semantics theory we extract and save knowledge in the form of triplets. The memory unit is designed to be scalable aggregatable updatable and interpretable. Through qualitative evaluations we demonstrate the superiority of our proposed framework over baseline approaches in question answering tasks. Moreover our framework exhibits robust performance in handling temporal-based question answering tasks showcasing its ability to effectively manage time-dependent information.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [33.5419921875, -2.942901849746704]}, {"key": "moran2013aneighbourhood", "year": "2013", "title": "Neighbourhood Preserving Quantisation for LSH", "abstract": "<p>We introduce a scheme for optimally allocating multiple bits per hyperplane for Locality Sensitive Hashing (LSH). Existing approaches binarise LSH projections by thresholding at zero yielding a single bit per dimension. We demonstrate that this is a sub-optimal bit allocation approach that can easily destroy the neighbourhood structure in the original feature space. Our proposed method, dubbed Neighbourhood Preserving Quantization (NPQ), assigns multiple bits per hyperplane based upon adaptively learned thresholds. NPQ exploits a pairwise affinity matrix to discretise each dimension such that nearest neighbours in the original feature space fall within the same quantisation thresholds and are therefore assigned identical bits. NPQ is not only applicable to LSH, but can also be applied to any low-dimensional projection scheme. Despite using half the number of hyperplanes, NPQ is shown to improve LSH-based retrieval accuracy by up to 65% compared to the state-of-the-art.</p>\n", "tags": ["Has Code", "LSH", "Quantisation", "SIGIR"], "tsne_embedding": [-31.591588973999023, 0.47302424907684326]}, {"key": "moran2013bvariable", "year": "2013", "title": "Variable Bit Quantisation for LSH", "abstract": "<p>We introduce a scheme for optimally allocating\na variable number of bits per\nLSH hyperplane. Previous approaches assign\na constant number of bits per hyperplane.\nThis neglects the fact that a subset\nof hyperplanes may be more informative\nthan others. Our method, dubbed Variable\nBit Quantisation (VBQ), provides a datadriven\nnon-uniform bit allocation across\nhyperplanes. Despite only using a fraction\nof the available hyperplanes, VBQ outperforms\nuniform quantisation by up to 168%\nfor retrieval across standard text and image\ndatasets.</p>\n", "tags": ["ACL", "LSH", "Quantisation"], "tsne_embedding": [-23.27396011352539, 13.390926361083984]}, {"key": "moran2015agraph", "year": "2015", "title": "Graph Regularised Hashing", "abstract": "<p>In this paper we propose a two-step iterative scheme, Graph Regularised Hashing (GRH), for incrementally adjusting the positioning of the hashing hypersurfaces to better conform to the supervisory signal: in the first step the binary bits are regularised using a data similarity graph so that similar data points receive similar bits. In the second step the regularised hashcodes form targets for a set of binary classifiers which shift the position of each hypersurface so as to separate opposite bits with maximum margin. GRH exhibits superior retrieval accuracy to competing hashing methods.</p>\n", "tags": ["Has Code", "Image Retrieval"], "tsne_embedding": [-3.051866054534912, 21.774219512939453]}, {"key": "moran2015bregularised", "year": "2015", "title": "Regularised Cross-Modal Hashing", "abstract": "<p>In this paper we propose Regularised Cross-Modal Hashing (RCMH) a new cross-modal hashing scheme that projects annotation and visual feature descriptors into a common Hamming space. RCMH optimises the intra-modality similarity of data-points in the annotation modality using an iterative three-step hashing algorithm: in the first step each training image is assigned a K-bit hashcode based on hyperplanes learnt at the previous iteration; in the second step the binary bits are smoothed by a formulation of graph regularisation so that similar data-points have similar bits; in the third step a set of binary classifiers are trained to predict the regularised bits with maximum margin. Visual descriptors are projected into the annotation Hamming space by a set of binary classifiers learnt using the bits of the corresponding annotations as labels. RCMH is shown to consistently improve retrieval effectiveness over state-of-the-art baselines.</p>\n", "tags": ["Cross Modal", "Has Code", "Image Retrieval", "SIGIR", "Text Retrieval"], "tsne_embedding": [-2.589414119720459, 21.777843475341797]}, {"key": "moran2016enhancing", "year": "2016", "title": "Enhancing First Story Detection using Word Embeddings", "abstract": "<p>In this paper we show how word embeddings can be used to increase the effectiveness of a state-of-the art Locality Sensitive Hashing (LSH) based first story detection (FSD) system over a standard tweet corpus. Vocabulary mismatch, in which related tweets use different words, is a serious hindrance to the effectiveness of a modern FSD system. In this case, a tweet could be flagged as a first story even if a related tweet, which uses different but synonymous words, was already returned as a first story. In this work, we propose a novel approach to mitigate this problem of lexical variation, based on tweet expansion. In particular, we propose to expand tweets with semantically related paraphrases identified via automatically mined word embeddings over a background tweet corpus. Through experimentation on a large data stream comprised of 50 million tweets, we show that FSD effectiveness can be improved by 9.5% over a state-of-the-art FSD system.</p>\n", "tags": ["LSH", "SIGIR", "Streaming Data", "Text Retrieval"], "tsne_embedding": [16.7944278717041, 13.687821388244629]}, {"key": "moran2016learning", "year": "2016", "title": "Learning to Project and Binarise for Hashing-Based Approximate Nearest Neighbour Search", "abstract": "<p>In this paper we focus on improving the effectiveness of hashing-based approximate nearest neighbour search. Generating similarity preserving hashcodes for images has been shown to be an effective and efficient method for searching through large datasets. Hashcode generation generally involves two steps: bucketing the input feature space with a set of hyperplanes, followed by quantising the projection of the data-points onto the normal vectors to those hyperplanes. This procedure results in the makeup of the hashcodes depending on the positions of the data-points with respect to the hyperplanes in the feature space, allowing a degree of locality to be encoded into the hashcodes. In this paper we study the effect of learning both the hyperplanes and the thresholds as part of the same model. Most previous research either learn the hyperplanes assuming a fixed set of thresholds, or vice-versa. In our experiments over two standard image datasets we find statistically significant increases in retrieval effectiveness versus a host of state-of-the-art data-dependent and independent hashing models.</p>\n", "tags": ["Image Retrieval", "Independent", "SIGIR"], "tsne_embedding": [-16.54629898071289, 17.959598541259766]}, {"key": "morgado2020deep", "year": "2020", "title": "Deep Hashing With Hash-consistent Large Margin Proxy Embeddings", "abstract": "<p>Image hash codes are produced by binarizing the embeddings of convolutional neural networks (CNN) trained for either classification or retrieval. While proxy embeddings achieve good performance on both tasks they are non-trivial to binarize due to a rotational ambiguity that encourages non-binary embeddings. The use of a fixed set of proxies (weights of the CNN classification layer) is proposed to eliminate this ambiguity and a procedure to design proxy sets that are nearly optimal for both classification and hashing is introduced. The resulting hash-consistent large margin (HCLM) proxies are shown to encourage saturation of hashing units thus guaranteeing a small binarization error while producing highly discriminative hash-codes. A semantic extension (sHCLM) aimed to improve hashing performance in a transfer scenario is also proposed. Extensive experiments show that sHCLM embeddings achieve significant improvements over state-of-the-art hashing procedures on several small and large datasets both within and beyond the set of training classes.</p>\n", "tags": ["ARXIV", "CNN", "Supervised"], "tsne_embedding": [3.4648218154907227, 14.9301118850708]}, {"key": "morozov2019unsupervised", "year": "2019", "title": "Unsupervised Neural Quantization For Compressed-domain Similarity Search", "abstract": "<p>We tackle the problem of unsupervised visual descriptors compression which is a key ingredient of large-scale image retrieval systems. While the deep learning machinery has benefited literally all computer vision pipelines the existing state-of-the-art compression methods employ shallow architectures and we aim to close this gap by our paper. In more detail we introduce a DNN architecture for the unsupervised compressed-domain retrieval based on multi-codebook quantization. The proposed architecture is designed to incorporate both fast data encoding and efficient distances computation via lookup tables. We demonstrate the exceptional advantage of our scheme over existing quantization approaches on several datasets of visual descriptors via outperforming the previous state-of-the-art by a large margin.</p>\n", "tags": ["ARXIV", "Deep Learning", "Image Retrieval", "Quantisation", "Unsupervised"], "tsne_embedding": [-20.53371810913086, 19.797819137573242]}, {"key": "morvan2018needs", "year": "2018", "title": "On The Needs For Rotations In Hypercubic Quantization Hashing", "abstract": "<p>The aim of this paper is to endow the well-known family of hypercubic quantization hashing methods with theoretical guarantees. In hypercubic quantization applying a suitable (random or learned) rotation after dimensionality reduction has been experimentally shown to improve the results accuracy in the nearest neighbors search problem. We prove in this paper that the use of these rotations is optimal under some mild assumptions getting optimal binary sketches is equivalent to applying a rotation uniformizing the diagonal of the covariance matrix between data points. Moreover for two closed points the probability to have dissimilar binary sketches is upper bounded by a factor of the initial distance between the data points. Relaxing these assumptions we obtain a general concentration result for random matrices. We also provide some experiments illustrating these theoretical points and compare a set of algorithms in both the batch and online settings.</p>\n", "tags": ["ARXIV", "Quantisation", "Unsupervised"], "tsne_embedding": [-26.70505142211914, 1.2019879817962646]}, {"key": "mor\u00e8re2016nested", "year": "2016", "title": "Nested Invariance Pooling And RBM Hashing For Image Instance Retrieval", "abstract": "<p>The goal of this work is the computation of very compact binary hashes for image instance retrieval. Our approach has two novel contributions. The first one is Nested Invariance Pooling (NIP) a method inspired from i-theory a mathematical theory for computing group invariant transformations with feed-forward neural networks. NIP is able to produce compact and well-performing descriptors with visual representations extracted from convolutional neural networks. We specifically incorporate scale translation and rotation invariances but the scheme can be extended to any arbitrary sets of transformations. We also show that using moments of increasing order throughout nesting is important. The NIP descriptors are then hashed to the target code size (32-256 bits) with a Restricted Boltzmann Machine with a novel batch-level regularization scheme specifically designed for the purpose of hashing (RBMH). A thorough empirical evaluation with state-of-the-art shows that the results obtained both with the NIP descriptors and the NIP+RBMH hashes are consistently outstanding across a wide range of datasets.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [-14.041712760925293, 9.575441360473633]}, {"key": "mu2016deep", "year": "2016", "title": "Deep Hashing A Joint Approach For Image Signature Learning", "abstract": "<p>Similarity-based image hashing represents crucial technique for visual data storage reduction and expedited image search. Conventional hashing schemes typically feed hand-crafted features into hash functions which separates the procedures of feature extraction and hash function learning. In this paper we propose a novel algorithm that concurrently performs feature engineering and non-linear supervised hashing function learning. Our technical contributions in this paper are two-folds 1) deep network optimization is often achieved by gradient propagation which critically requires a smooth objective function. The discrete nature of hash codes makes them not amenable for gradient-based optimization. To address this issue we propose an exponentiated hashing loss function and its bilinear smooth approximation. Effective gradient calculation and propagation are thereby enabled; 2) pre-training is an important trick in supervised deep learning. The impact of pre-training on the hash code quality has never been discussed in current deep hashing literature. We propose a pre-training scheme inspired by recent advance in deep network based image classification and experimentally demonstrate its effectiveness. Comprehensive quantitative evaluations are conducted on several widely-used image benchmarks. On all benchmarks our proposed deep hashing algorithm outperforms all state-of-the-art competitors by significant margins. In particular our algorithm achieves a near-perfect 0.99 in terms of Hamming ranking accuracy with only 12 bits on MNIST and a new record of 0.74 on the CIFAR10 dataset. In comparison the best accuracies obtained on CIFAR10 by existing hashing algorithms without or with deep networks are known to be 0.36 and 0.58 respectively.</p>\n", "tags": ["ARXIV", "Deep Learning", "Supervised"], "tsne_embedding": [-1.0691720247268677, 1.0350557565689087]}, {"key": "mu2023embodiedgpt", "year": "2023", "title": "Embodiedgpt Vision-language Pre-training Via Embodied Chain Of Thought", "abstract": "<p>Embodied AI is a crucial frontier in robotics capable of planning and executing action sequences for robots to accomplish long-horizon tasks in physical environments. In this work we introduce EmbodiedGPT an end-to-end multi-modal foundation model for embodied AI empowering embodied agents with multi-modal understanding and execution capabilities. To achieve this we have made the following efforts (i) We craft a large-scale embodied planning dataset termed EgoCOT. The dataset consists of carefully selected videos from the Ego4D dataset along with corresponding high-quality language instructions. Specifically we generate a sequence of sub-goals with the Chain of Thoughts mode for effective embodied planning. (ii) We introduce an efficient training approach to EmbodiedGPT for high-quality plan generation by adapting a 7B large language model (LLM) to the EgoCOT dataset via prefix tuning. (iii) We introduce a paradigm for extracting task-related features from LLM-generated planning queries to form a closed loop between high-level planning and low-level control. Extensive experiments show the effectiveness of EmbodiedGPT on embodied tasks including embodied planning embodied control visual captioning and visual question answering. Notably EmbodiedGPT significantly enhances the success rate of the embodied control task by extracting more effective features. It has achieved a remarkable 1.6 times increase in success rate on the Franka Kitchen benchmark and a 1.3 times increase on the Meta-World benchmark compared to the BLIP-2 baseline fine-tuned with the Ego4D dataset.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [30.251094818115234, -15.180872917175293]}, {"key": "muennighoff2022crosslingual", "year": "2022", "title": "Crosslingual Generalization Through Multitask Finetuning", "abstract": "<p>Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition we introduce xP3 a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.</p>\n", "tags": ["ARXIV", "Has Code", "Supervised"], "tsne_embedding": [27.55764389038086, -3.662982225418091]}, {"key": "mukherjee2012efficient", "year": "2012", "title": "An Efficient Cryptographic Hash Algorithm (BSA)", "abstract": "<p>Recent cryptanalytic attacks have exposed the vulnerabilities of some widely used cryptographic hash functions like MD5 and SHA-1. Attacks in the line of differential attacks have been used to expose the weaknesses of several other hash functions like RIPEMD HAVAL. In this paper we propose a new efficient hash algorithm that provides a near random hash output and overcomes some of the earlier weaknesses. Extensive simulations and comparisons with some existing hash functions have been done to prove the effectiveness of the BSA which is an acronym for the name of the 3 authors.</p>\n", "tags": ["Graph", "Independent"], "tsne_embedding": [-5.542132377624512, -14.139130592346191]}, {"key": "mukherjee2015nmf", "year": "2015", "title": "An NMF perspective on Binary Hashing", "abstract": "<p>The pervasiveness of massive data repositories has led\nto much interest in efficient methods for indexing, search,\nand retrieval. For image data, a rapidly developing body of\nwork for these applications shows impressive performance\nwith methods that broadly fall under the umbrella term of\nBinary Hashing. Given a distance matrix, a binary hashing\nalgorithm solves for a binary code for the given set of examples, whose Hamming distance nicely approximates the\noriginal distances. The formulation is non-convex \u2014 so existing solutions adopt spectral relaxations or perform coordinate descent (or quantization) on a surrogate objective\nthat is numerically more tractable. In this paper, we first\nderive an Augmented Lagrangian approach to optimize the\nstandard binary Hashing objective (i.e., maintain fidelity\nwith a given distance matrix). With appropriate step sizes,\nwe find that this scheme already yields results that match or\nsubstantially outperform state of the art methods on most\nbenchmarks used in the literature. Then, to allow the model\nto scale to large datasets, we obtain an interesting reformulation of the binary hashing objective as a non-negative matrix factorization. Later, this leads to a simple multiplicative updates algorithm \u2014 whose parallelization properties\nare exploited to obtain a fast GPU based implementation.\nWe give a probabilistic analysis of our initialization scheme\nand present a range of experiments to show that the method\nis simple to implement and competes favorably with available methods (both for optimization and generalization).</p>\n\n", "tags": ["ICCV"], "tsne_embedding": [-23.274784088134766, 15.803938865661621]}, {"key": "muramatsu2008hash", "year": "2008", "title": "Hash Property And Coding Theorems For Sparse Matrices And Maximum-likelihood Coding", "abstract": "<p>The aim of this paper is to prove the achievability of several coding problems by using sparse matrices (the maximum column weight grows logarithmically in the block length) and maximal-likelihood (ML) coding. These problems are the Slepian-Wolf problem the Gelfand-Pinsker problem the Wyner-Ziv problem and the One-helps-one problem (source coding with partial side information at the decoder). To this end the notion of a hash property for an ensemble of functions is introduced and it is proved that an ensemble of q-ary sparse matrices satisfies the hash property. Based on this property it is proved that the rate of codes using sparse matrices and maximal-likelihood (ML) coding can achieve the optimal rate.</p>\n", "tags": [], "tsne_embedding": [-27.275564193725586, 9.774203300476074]}, {"key": "najgebauer2015fast", "year": "2015", "title": "Fast Dictionary Matching For Content-based Image Retrieval", "abstract": "<p>This paper describes a method for searching for common sets of descriptors between collections of images. The presented method operates on local interest keypoints which are generated using the SURF algorithm. The use of a dictionary of descriptors allowed achieving good performance of the content-based image retrieval. The method can be used to initially determine a set of similar pairs of keypoints between images. For this purpose we use a certain level of tolerance between values of descriptors as values of feature descriptors are almost never equal but similar between different images. After that the method compares the structure of rotation and location of interest points in one image with the point structure in other images. Thus we were able to find similar areas in images and determine the level of similarity between them even when images contain different scenes.</p>\n", "tags": ["ARXIV", "Image Retrieval"], "tsne_embedding": [-23.12691879272461, 22.18621063232422]}, {"key": "najibi2015large", "year": "2015", "title": "On Large-scale Retrieval Binary Or N-ary Coding", "abstract": "<p>The growing amount of data available in modern-day datasets makes the need to efficiently search and retrieve information. To make large-scale search feasible Distance Estimation and Subset Indexing are the main approaches. Although binary coding has been popular for implementing both techniques n-ary coding (known as Product Quantization) is also very effective for Distance Estimation. However their relative performance has not been studied for Subset Indexing. We investigate whether binary or n-ary coding works better under different retrieval strategies. This leads to the design of a new n-ary coding method Linear Subspace Quantization (LSQ) which unlike other n-ary encoders can be used as a similarity-preserving embedding. Experiments on image retrieval show that when Distance Estimation is used n-ary LSQ outperforms other methods. However when Subset Indexing is applied interestingly binary codings are more effective and binary LSQ achieves the best accuracy.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Quantisation"], "tsne_embedding": [-20.831525802612305, 18.074522018432617]}, {"key": "nakano2021webgpt", "year": "2021", "title": "Webgpt Browser-assisted Question-answering With Human Feedback", "abstract": "<p>We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans we are able to train models on the task using imitation learning and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5 a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning and then performing rejection sampling against a reward model trained to predict human preferences. This models answers are preferred by humans 5637; of the time to those of our human demonstrators and 6937; of the time to the highest-voted answer from Reddit.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [44.721649169921875, -3.5385305881500244]}, {"key": "ndungu2023deep", "year": "2023", "title": "Deep Supervised Hashing For Fast Retrieval Of Radio Image Cubes", "abstract": "<p>The shear number of sources that will be detected by next-generation radio surveys will be astronomical which will result in serendipitous discoveries. Data-dependent deep hashing algorithms have been shown to be efficient at image retrieval tasks in the fields of computer vision and multimedia. However there are limited applications of these methodologies in the field of astronomy. In this work we utilize deep hashing to rapidly search for similar images in a large database. The experiment uses a balanced dataset of 2708 samples consisting of four classes Compact FRI FRII and Bent. The performance of the method was evaluated using the mean average precision (mAP) metric where a precision of 88.537; was achieved. The experimental results demonstrate the capability to search and retrieve similar radio images efficiently and at scale. The retrieval is based on the Hamming distance between the binary hash of the query image and those of the reference images in the database.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Supervised"], "tsne_embedding": [-25.048660278320312, 14.850961685180664]}, {"key": "netay2024hashing", "year": "2024", "title": "Hashing Geographical Point Data Using The Space-filling H-curve", "abstract": "<p>We construct geohashing procedure based on using of space-filling H-curve. This curve provides a way to construct geohash with less computations than the construction based on usage of Hilbert curve. At the same time H-curve has better clustering properties.</p>\n", "tags": ["ARXIV", "Graph", "Unsupervised"], "tsne_embedding": [-30.890472412109375, 5.217094421386719]}, {"key": "neyshabur2013power", "year": "2013", "title": "The Power Of Asymmetry In Binary Hashing", "abstract": "<p>When approximating binary similarity using the hamming distance between short binary hashes we shown that even if the similarity is symmetric we can have shorter and more accurate hashes by using two distinct code maps. I.e.~by approximating the similarity between (x) and (x) as the hamming distance between (f(x)) and (g(x)) for two distinct binary codes (fg) rather than as the hamming distance between (f(x)) and (f(x)).</p>\n", "tags": ["NEURIPS"], "tsne_embedding": [-32.37977600097656, 9.179336547851562]}, {"key": "neyshabur2014symmetric", "year": "2014", "title": "On Symmetric And Asymmetric Lshs For Inner Product Search", "abstract": "<p>We consider the problem of designing locality sensitive hashes (LSH) for inner product similarity and of the power of asymmetric hashes in this context. Shrivastava and Li argue that there is no symmetric LSH for the problem and propose an asymmetric LSH based on different mappings for query and database points. However we show there does exist a simple symmetric LSH that enjoys stronger guarantees and better empirical performance than the asymmetric LSH they suggest. We also show a variant of the settings where asymmetry is in-fact needed but there a different asymmetric LSH is required.</p>\n", "tags": ["ARXIV", "Independent", "LSH"], "tsne_embedding": [-15.727330207824707, -7.184638500213623]}, {"key": "ng2023unsupervised", "year": "2023", "title": "Unsupervised Hashing With Similarity Distribution Calibration", "abstract": "<p>Unsupervised hashing methods typically aim to preserve the similarity between data points in a feature space by mapping them to binary hash codes. However these methods often overlook the fact that the similarity between data points in the continuous feature space may not be preserved in the discrete hash code space due to the limited similarity range of hash codes. The similarity range is bounded by the code length and can lead to a problem known as similarity collapse. That is the positive and negative pairs of data points become less distinguishable from each other in the hash space. To alleviate this problem in this paper a novel Similarity Distribution Calibration (SDC) method is introduced. SDC aligns the hash code similarity distribution towards a calibration distribution (e.g. beta distribution) with sufficient spread across the entire similarity range thus alleviating the similarity collapse problem. Extensive experiments show that our SDC outperforms significantly the state-of-the-art alternatives on coarse category-level and instance-level image retrieval. Code is available at https://github.com/kamwoh/sdc.</p>\n", "tags": ["ARXIV", "Has Code", "Image Retrieval", "Unsupervised"], "tsne_embedding": [-15.774495124816895, 6.217165946960449]}, {"key": "ng2024concepthash", "year": "2024", "title": "Concepthash Interpretable Fine-grained Hashing Via Concept Discovery", "abstract": "<p>Existing fine-grained hashing methods typically lack code interpretability as they compute hash code bits holistically using both global and local features. To address this limitation we propose ConceptHash a novel method that achieves sub-code level interpretability. In ConceptHash each sub-code corresponds to a human-understandable concept such as an object part and these concepts are automatically discovered without human annotations. Specifically we leverage a Vision Transformer architecture and introduce concept tokens as visual prompts along with image patch tokens as model inputs. Each concept is then mapped to a specific sub-code at the model output providing natural sub-code interpretability. To capture subtle visual differences among highly similar sub-categories (e.g. bird species) we incorporate language guidance to ensure that the learned hash codes are distinguishable within fine-grained object classes while maintaining semantic alignment. This approach allows us to develop hash codes that exhibit similarity within families of species while remaining distinct from species in other families. Extensive experiments on four fine-grained image retrieval benchmarks demonstrate that ConceptHash outperforms previous methods by a significant margin offering unique sub-code interpretability as an additional benefit. Code at https://github.com/kamwoh/concepthash.\u201d</p>\n", "tags": ["ARXIV", "Has Code", "Image Retrieval", "Independent"], "tsne_embedding": [-5.225525379180908, 4.950594902038574]}, {"key": "nguyen2013approximate", "year": "2013", "title": "Approximate Nearest Neighbor Search In (ell_p)", "abstract": "<p>We present a new locality sensitive hashing (LSH) algorithm for (c)-approximate nearest neighbor search in (ell_p) with (1&lt;p&lt;2). For a database of (n) points in (ell_p) we achieve (O(dn^rho)) query time and (O(dn+n^1+rho)) space where (rho le O((ln c)^2/c^p)). This improves upon the previous best upper bound (rhole 1/c) by Datar et al. (SOCG 2004) and is close to the lower bound (rho ge 1/c^p) by ODonnell Wu and Zhou (ITCS 2011). The proof is a simple generalization of the LSH scheme for (ell_2) by Andoni and Indyk (FOCS 2006).</p>\n", "tags": ["ARXIV", "FOCS", "Independent", "LSH"], "tsne_embedding": [-33.271095275878906, 0.8496230244636536]}, {"key": "nijkamp2022codegen", "year": "2022", "title": "Codegen An Open Large Language Model For Code With Multi-turn Program Synthesis", "abstract": "<p>Program synthesis strives to generate a computer program as a solution to a given problem specification expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis though limited training resources and data impede open access to such models. To democratize this we train and release a family of large language models up to 16.1B parameters called CODEGEN on natural language and programming language data and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis where a single program is factorized into multiple prompts specifying subproblems. To this end we construct an open benchmark Multi-Turn Programming Benchmark (MTPB) consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multi-turn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution https://github.com/salesforce/CodeGen.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [26.67552375793457, -12.993803024291992]}, {"key": "ning2016scalable", "year": "2016", "title": "Scalable Image Retrieval By Sparse Product Quantization", "abstract": "<p>Fast Approximate Nearest Neighbor (ANN) search technique for high-dimensional feature indexing and retrieval is the crux of large-scale image retrieval. A recent promising technique is Product Quantization which attempts to index high-dimensional image features by decomposing the feature space into a Cartesian product of low dimensional subspaces and quantizing each of them separately. Despite the promising results reported their quantization approach follows the typical hard assignment of traditional quantization methods which may result in large quantization errors and thus inferior search performance. Unlike the existing approaches in this paper we propose a novel approach called Sparse Product Quantization (SPQ) to encoding the high-dimensional feature vectors into sparse representation. We optimize the sparse representations of the feature vectors by minimizing their quantization errors making the resulting representation is essentially close to the original data in practice. Experiments show that the proposed SPQ technique is not only able to compress data but also an effective encoding technique. We obtain state-of-the-art results for ANN search on four public image datasets and the promising results of content-based image retrieval further validate the efficacy of our proposed method.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Quantisation"], "tsne_embedding": [-16.778669357299805, 11.991104125976562]}, {"key": "niu2016constructions", "year": "2016", "title": "Constructions And Bounds For Separating Hash Families", "abstract": "<p>In this paper we present a new construction for strong separating hash families by using hypergraphs and obtain some optimal separating hash families. We also improve some previously known bounds of separating hash families.</p>\n", "tags": ["ARXIV", "Graph"], "tsne_embedding": [-36.75750732421875, 5.112471103668213]}, {"key": "nllb2022no", "year": "2022", "title": "No Language Left Behind Scaling Human-centered Machine Translation", "abstract": "<p>Driven by the goal of eradicating language barriers on a global scale machine translation has solidified itself as a key focus of artificial intelligence research today. However such efforts have coalesced around a small subset of languages leaving behind the vast majority of mostly low-resource languages. What does it take to break the 200 language barrier while ensuring safe high quality results all while keeping ethical considerations in mind In No Language Left Behind we took on this challenge by first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then we created datasets and models aimed at narrowing the performance gap between low and high-resource languages. More specifically we developed a conditional compute model based on Sparsely Gated Mixture of Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training improvements to counteract overfitting while training on thousands of tasks. Critically we evaluated the performance of over 40000 different translation directions using a human-translated benchmark Flores-200 and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety. Our model achieves an improvement of 4437; BLEU relative to the previous state-of-the-art laying important groundwork towards realizing a universal translation system. Finally we open source all contributions described in this work accessible at https://github.com/facebookresearch/fairseq/tree/nllb.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [24.36517333984375, -12.352960586547852]}, {"key": "noma2014eclipse", "year": "2014", "title": "Eclipse Hashing Alexandrov Compactification And Hashing With Hyperspheres For Fast Similarity Search", "abstract": "<p>The similarity searches that use high-dimensional feature vectors consisting of a vast amount of data have a wide range of application. One way of conducting a fast similarity search is to transform the feature vectors into binary vectors and perform the similarity search by using the Hamming distance. Such a transformation is a hashing method and the choice of hashing function is important. Hashing methods using hyperplanes or hyperspheres are proposed. One study reported here is inspired by Spherical LSH and we use hypersperes to hash the feature vectors. Our method called Eclipse-hashing performs a compactification of R^n by using the inverse stereographic projection which is a kind of Alexandrov compactification. By using Eclipse-hashing one can obtain the hypersphere-hash function without explicitly using hyperspheres. Hence the number of nonlinear operations is reduced and the processing time of hashing becomes shorter. Furthermore we also show that as a result of improving the approximation accuracy Eclipse-hashing is more accurate than hyperplane-hashing.</p>\n", "tags": ["ARXIV", "Graph", "Independent", "LSH"], "tsne_embedding": [-29.686046600341797, 6.269761085510254]}, {"key": "norouzi2011minimal", "year": "2011", "title": "Minimal Loss Hashing", "abstract": "<p>We propose a method for learning similaritypreserving\nhash functions that map highdimensional\ndata onto binary codes. The\nformulation is based on structured prediction\nwith latent variables and a hinge-like\nloss function. It is efficient to train for large\ndatasets, scales well to large code lengths,\nand outperforms state-of-the-art methods.</p>\n", "tags": [], "tsne_embedding": [-15.282604217529297, -13.104418754577637]}, {"key": "norouzi2012hamming", "year": "2012", "title": "Hamming Distance Metric Learning", "abstract": "<p>Motivated by large-scale multimedia applications we propose to learn mappings from high-dimensional data to binary codes that preserve semantic similarity. Binary codes are well suited to large-scale applications as they are storage efficient and permit exact sub-linear kNN search. The framework is applicable to broad families of mappings and uses a flexible form of triplet ranking loss. We overcome discontinuous optimization of the discrete mappings by minimizing a piecewise-smooth upper bound on empirical loss inspired by latent structural SVMs. We develop a new loss-augmented inference algorithm that is quadratic in the code length. We show strong retrieval performance on CIFAR-10 and MNIST with promising classification results using no more than kNN on the binary codes.</p>\n", "tags": ["NEURIPS", "Supervised"], "tsne_embedding": [-18.37487030029297, 20.86475372314453]}, {"key": "norouzi2013fast", "year": "2013", "title": "Fast Exact Search In Hamming Space With Multi-index Hashing", "abstract": "<p>There is growing interest in representing image data and feature descriptors using compact binary codes for fast near neighbor search. Although binary codes are motivated by their use as direct indices (addresses) into a hash table codes longer than 32 bits are not being used as such as it was thought to be ineffective. We introduce a rigorous way to build multiple hash tables on binary code substrings that enables exact k-nearest neighbor search in Hamming space. The approach is storage efficient and straightforward to implement. Theoretical analysis shows that the algorithm exhibits sub-linear run-time behavior for uniformly distributed codes. Empirical results show dramatic speedups over a linear scan baseline for datasets of up to one billion codes of 64 128 or 256 bits.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-20.420120239257812, -9.453803062438965]}, {"key": "nuswide2009nuswide", "year": "2009", "title": "NUS-WIDE: a real-world web image database from National University of Singapore", "abstract": "<p>This paper introduces a web image dataset created by NUS\u2019s Lab for Media Search. The dataset includes: (1) 269,648 images and the associated tags from Flickr, with a total of 5,018 unique tags; (2) six types of low-level features extracted from these images, including 64-D color histogram, 144-D color correlogram, 73-D edge direction histogram, 128-D wavelet texture, 225-D block-wise color moments extracted over 5x5 fixed grid partitions, and 500-D bag of words based on SIFT descriptions; and (3) ground-truth for 81 concepts that can be used for evaluation. Based on this dataset, we highlight characteristics of Web image collections and identify four research issues on web image annotation and retrieval. We also provide the baseline results for web image annotation by learning from the tags using the traditional k-NN algorithm. The benchmark results indicate that it is possible to learn effective models from sufficiently large image dataset to facilitate general image retrieval.</p>\n", "tags": ["Dataset"], "tsne_embedding": [7.624655246734619, 20.593807220458984]}, {"key": "openai2023gpt", "year": "2023", "title": "GPT-4 Technical Report", "abstract": "<p>We report the development of GPT-4 a large-scale multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios GPT-4 exhibits human-level performance on various professional and academic benchmarks including passing a simulated bar exam with a score around the top 1037; of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4s performance based on models trained with no more than 1/1000th the compute of GPT-4.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [22.040109634399414, -10.481928825378418]}, {"key": "orvieto2023resurrecting", "year": "2023", "title": "Resurrecting Recurrent Neural Networks For Long Sequences", "abstract": "<p>Recurrent Neural Networks (RNNs) offer fast inference on long sequences but are hard to optimize and slow to train. Deep state-space models (SSMs) have recently been shown to perform remarkably well on long sequence modeling tasks and have the added benefits of fast parallelizable training and RNN-like fast inference. However while SSMs are superficially similar to RNNs there are important differences that make it unclear where their performance boost over RNNs comes from. In this paper we show that careful design of deep RNNs using standard signal propagation arguments can recover the impressive performance of deep SSMs on long-range reasoning tasks while also matching their training speed. To achieve this we analyze and ablate a series of changes to standard RNNs including linearizing and diagonalizing the recurrence using better parameterizations and initializations and ensuring proper normalization of the forward pass. Our results provide new insights on the origins of the impressive performance of deep SSMs while also introducing an RNN block called the Linear Recurrent Unit that matches both their performance on the Long Range Arena benchmark and their computational efficiency.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [9.535158157348633, 3.100912094116211]}, {"key": "ou2013comparing", "year": "2013", "title": "Comparing apples to oranges: a scalable solution with heterogeneous hashing", "abstract": "<p>Although hashing techniques have been popular for the large scale similarity search problem, most of the existing methods for designing optimal hash functions focus on homogeneous similarity assessment, i.e., the data entities to be indexed are of the same type. Realizing that heterogeneous entities and relationships are also ubiquitous in the real world applications, there is an emerging need to retrieve and search similar or relevant data entities from multiple heterogeneous domains, e.g., recommending relevant posts and images to a certain Facebook user. In this paper, we address the problem of ``comparing apples to oranges\u2019\u2019 under the large scale setting. Specifically, we propose a novel Relation-aware Heterogeneous Hashing (RaHH), which provides a general framework for generating hash codes of data entities sitting in multiple heterogeneous domains. Unlike some existing hashing methods that map heterogeneous data in a common Hamming space, the RaHH approach constructs a Hamming space for each type of data entities, and learns optimal mappings between them simultaneously. This makes the learned hash codes flexibly cope with the characteristics of different data domains. Moreover, the RaHH framework encodes both homogeneous and heterogeneous relationships between the data entities to design hash functions with improved accuracy. To validate the proposed RaHH method, we conduct extensive evaluations on two large datasets; one is crawled from a popular social media sites, Tencent Weibo, and the other is an open dataset of Flickr(NUS-WIDE). The experimental results clearly demonstrate that the RaHH outperforms several state-of-the-art hashing methods with significant performance gains.</p>\n", "tags": ["Cross Modal", "KDD"], "tsne_embedding": [-12.528874397277832, -3.9008901119232178]}, {"key": "ou2021refining", "year": "2021", "title": "Refining BERT Embeddings For Document Hashing Via Mutual Information Maximization", "abstract": "<p>Existing unsupervised document hashing methods are mostly established on generative models. Due to the difficulties of capturing long dependency structures these methods rarely model the raw documents directly but instead to model the features extracted from them (e.g. bag-of-words (BOW) TFIDF). In this paper we propose to learn hash codes from BERT embeddings after observing their tremendous successes on downstream tasks. As a first try we modify existing generative hashing models to accommodate the BERT embeddings. However little improvement is observed over the codes learned from the old BOW or TFIDF features. We attribute this to the reconstruction requirement in the generative hashing which will enforce irrelevant information that is abundant in the BERT embeddings also compressed into the codes. To remedy this issue a new unsupervised hashing paradigm is further proposed based on the mutual information (MI) maximization principle. Specifically the method first constructs appropriate global and local codes from the documents and then seeks to maximize their mutual information. Experimental results on three benchmark datasets demonstrate that the proposed method is able to generate hash codes that outperform existing ones learned from BOW features by a substantial margin.</p>\n", "tags": ["ARXIV", "Unsupervised"], "tsne_embedding": [12.391815185546875, -6.924354553222656]}, {"key": "ouyang2021contextual", "year": "2021", "title": "Contextual Similarity Aggregation With Self-attention For Visual Re-ranking", "abstract": "<p>In content-based image retrieval the first-round retrieval result by simple visual feature comparison may be unsatisfactory which can be refined by visual re-ranking techniques. In image retrieval it is observed that the contextual similarity among the top-ranked images is an important clue to distinguish the semantic relevance. Inspired by this observation in this paper we propose a visual re-ranking method by contextual similarity aggregation with self-attention. In our approach for each image in the top-K ranking list we represent it into an affinity feature vector by comparing it with a set of anchor images. Then the affinity features of the top-K images are refined by aggregating the contextual information with a transformer encoder. Finally the affinity features are used to recalculate the similarity scores between the query and the top-K images for re-ranking of the latter. To further improve the robustness of our re-ranking model and enhance the performance of our method a new data augmentation scheme is designed. Since our re-ranking model is not directly involved with the visual feature used in the initial retrieval it is ready to be applied to retrieval result lists obtained from various retrieval algorithms. We conduct comprehensive experiments on four benchmark datasets to demonstrate the generality and effectiveness of our proposed visual re-ranking method.</p>\n", "tags": ["Image Retrieval", "NEURIPS"], "tsne_embedding": [-11.075011253356934, 18.41689109802246]}, {"key": "ouyang2022training", "year": "2022", "title": "Training Language Models To Follow Instructions With Human Feedback", "abstract": "<p>Making language models bigger does not inherently make them better at following a users intent. For example large language models can generate outputs that are untruthful toxic or simply not helpful to the user. In other words these models are not aligned with their users. In this paper we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API we collect a dataset of labeler demonstrations of the desired model behavior which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3 despite having 100x fewer parameters. Moreover InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [41.58805847167969, -1.752895712852478]}, {"key": "ozdemir2016scalable", "year": "2016", "title": "Scalable Gaussian Processes For Supervised Hashing", "abstract": "<p>We propose a flexible procedure for large-scale image search by hash functions with kernels. Our method treats binary codes and pairwise semantic similarity as latent and observed variables respectively in a probabilistic model based on Gaussian processes for binary classification. We present an efficient inference algorithm with the sparse pseudo-input Gaussian process (SPGP) model and parallelization. Experiments on three large-scale image dataset demonstrate the effectiveness of the proposed hashing method Gaussian Process Hashing (GPH) for short binary codes and the datasets without predefined classes in comparison to the state-of-the-art supervised hashing methods.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [-21.897472381591797, 9.798051834106445]}, {"key": "ozdemir2016supervised", "year": "2016", "title": "Supervised Incremental Hashing", "abstract": "<p>We propose an incremental strategy for learning hash functions with kernels for large-scale image search. Our method is based on a two-stage classification framework that treats binary codes as intermediate variables between the feature space and the semantic space. In the first stage of classification binary codes are considered as class labels by a set of binary SVMs; each corresponds to one bit. In the second stage binary codes become the input space of a multi-class SVM. Hash functions are learned by an efficient algorithm where the NP-hard problem of finding optimal binary codes is solved via cyclic coordinate descent and SVMs are trained in a parallelized incremental manner. For modifications like adding images from a previously unseen class we describe an incremental procedure for effective and efficient updates to the previous hash functions. Experiments on three large-scale image datasets demonstrate the effectiveness of the proposed hashing method Supervised Incremental Hashing (SIH) over the state-of-the-art supervised hashing methods.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [-13.2783784866333, 13.84693431854248]}, {"key": "ozsari2003hash", "year": "2003", "title": "A Hash Of Hash Functions", "abstract": "<p>In this paper we present a general review of hash functions in a cryptographic sense. We give special emphasis on some particular topics such as cipher block chaining message authentication code (CBC MAC) and its variants. This paper also broadens the information given in some well known surveys by including more details on block-cipher based hash functions and security of different hash schemes.</p>\n", "tags": ["ARXIV", "Graph", "Independent", "Survey Paper"], "tsne_embedding": [-5.24550199508667, -14.645543098449707]}, {"key": "pachori2016zero", "year": "2016", "title": "Zero Shot Hashing", "abstract": "<p>This paper provides a framework to hash images containing instances of unknown object classes. In many object recognition problems we might have access to huge amount of data. It may so happen that even this huge data doesnt cover the objects belonging to classes that we see in our day to day life. Zero shot learning exploits auxiliary information (also called as signatures) in order to predict the labels corresponding to unknown classes. In this work we attempt to generate the hash codes for images belonging to unseen classes information of which is available only through the textual corpus. We formulate this as an unsupervised hashing formulation as the exact labels are not available for the instances of unseen classes. We show that the proposed solution is able to generate hash codes which can predict labels corresponding to unseen classes with appreciably good precision.</p>\n", "tags": ["ARXIV", "Unsupervised"], "tsne_embedding": [-2.766791820526123, 5.40640926361084]}, {"key": "pachori2017hashing", "year": "2017", "title": "Hashing In The Zero Shot Framework With Domain Adaptation", "abstract": "<p>Techniques to learn hash codes which can store and retrieve large dimensional multimedia data efficiently have attracted broad research interests in the recent years. With rapid explosion of newly emerged concepts and online data existing supervised hashing algorithms suffer from the problem of scarcity of ground truth annotations due to the high cost of obtaining manual annotations. Therefore we propose an algorithm to learn a hash function from training images belonging to seen classes which can efficiently encode images of unseen classes to binary codes. Specifically we project the image features from visual space and semantic features from semantic space into a common Hamming subspace. Earlier works to generate hash codes have tried to relax the discrete constraints on hash codes and solve the continuous optimization problem. However it often leads to quantization errors. In this work we use the max-margin classifier to learn an efficient hash function. To address the concern of domain-shift which may arise due to the introduction of new classes we also introduce an unsupervised domain adaptation model in the proposed hashing framework. Results on the three datasets show the advantage of using domain adaptation in learning a high-quality hash function and superiority of our method for the task of image retrieval performance as compared to several state-of-the-art hashing methods.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Quantisation", "Unsupervised"], "tsne_embedding": [-0.854561448097229, -0.47553861141204834]}, {"key": "packer2023memgpt", "year": "2023", "title": "Memgpt Towards Llms As Operating Systems", "abstract": "<p>Large language models (LLMs) have revolutionized AI but are constrained by limited context windows hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows we propose virtual context management a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique we introduce MemGPT (Memory-GPT) a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLMs limited context window and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance document analysis where MemGPT is able to analyze large documents that far exceed the underlying LLMs context window and multi-session chat where MemGPT can create conversational agents that remember reflect and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [25.536035537719727, -17.748619079589844]}, {"key": "pacuk2016locality", "year": "2016", "title": "Locality-sensitive Hashing Without False Negatives For L_p", "abstract": "<p>In this paper we show a construction of locality-sensitive hash functions without false negatives i.e. which ensure collision for every pair of points within a given radius R in d dimensional space equipped with l_p norm when p (in) 1(infty). Furthermore we show how to use these hash functions to solve the c-approximate nearest neighbor search problem without false negatives. Namely if there is a point at distance R we will certainly report it and points at distance greater than cR will not be reported for c=(Omega)((sqrtd)d^1-(frac1)p). The constructed algorithms work - with preprocessing time (mathcalO)(n (log)(n)) and sublinear expected query time - with preprocessing time (mathcalO)((mathrmpoly)(n)) and expected query time (mathcalO)((log)(n)). Our paper reports progress on answering the open problem presented by Pagh 8 who considered the nearest neighbor search without false negatives for the Hamming distance.</p>\n", "tags": ["Independent"], "tsne_embedding": [-33.21306228637695, -2.628926992416382]}, {"key": "pagh2006linear", "year": "2006", "title": "Linear Probing With Constant Independence", "abstract": "<p>Hashing with linear probing dates back to the 1950s and is among the most studied algorithms. In recent years it has become one of the most important hash table organizations since it uses the cache of modern computers very well. Unfortunately previous analysis rely either on complicated and space consuming hash functions or on the unrealistic assumption of free access to a truly random hash function. Already Carter and Wegman in their seminal paper on universal hashing raised the question of extending their analysis to linear probing. However we show in this paper that linear probing using a pairwise independent family may have expected (em) logarithmic cost per operation. On the positive side we show that 5-wise independence is enough to ensure constant expected time per operation. This resolves the question of finding a space and time efficient hash function that provably ensures good performance for linear probing.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-32.063758850097656, -18.167423248291016]}, {"key": "pagh2015coveringlsh", "year": "2015", "title": "Coveringlsh Locality-sensitive Hashing Without False Negatives", "abstract": "<p>We consider a new construction of locality-sensitive hash functions for Hamming space that is emphcovering in the sense that is it guaranteed to produce a collision for every pair of vectors within a given radius (r). The construction is emphefficient in the sense that the expected number of hash collisions between vectors at distance~(cr) for a given (c1) comes close to that of the best possible data independent LSH without the covering guarantee namely the seminal LSH construction of Indyk and Motwani (STOC 98). The efficiency of the new construction essentially emphmatches their bound when the search radius is not too large \u2014 e.g. when (cr = o(log(n)/loglog n)) where (n) is the number of points in the data set and when (cr = log(n)/k) where (k) is an integer constant. In general it differs by at most a factor (ln(4)) in the exponent of the time bounds. As a consequence LSH-based similarity search in Hamming space can avoid the problem of false negatives at little or no cost in efficiency.</p>\n", "tags": ["ARXIV", "Independent", "LSH"], "tsne_embedding": [-32.10009002685547, -1.9443844556808472]}, {"key": "panigrahy2005entropy", "year": "2005", "title": "Entropy Based Nearest Neighbor Search In High Dimensions", "abstract": "<p>In this paper we study the problem of finding the approximate nearest neighbor of a query point in the high dimensional space focusing on the Euclidean space. The earlier approaches use locality-preserving hash functions (that tend to map nearby points to the same value) to construct several hash tables to ensure that the query point hashes to the same bucket as its nearest neighbor in at least one table. Our approach is different \u2013 we use one (or a few) hash table and hash several randomly chosen points in the neighborhood of the query point showing that at least one of them will hash to the bucket containing its nearest neighbor. We show that the number of randomly chosen points in the neighborhood of the query point q required depends on the entropy of the hash value h(p) of a random point p at the same distance from q at its nearest neighbor given q and the locality preserving hash function h chosen randomly from the hash family. Precisely we show that if the entropy I(h(p)qh) = M and g is a bound on the probability that two far-off points will hash to the same bucket then we can find the approximate nearest neighbor in O(n^(rho)) time and near linear (tilde) O(n) space where (rho) = M/(log)(1/g). Alternatively we can build a data structure of size (tilde) O(n^1/(1-(rho))) to answer queries in (tilde) O(d) time. By applying this analysis to the locality preserving hash functions in and adjusting the parameters we show that the c nearest neighbor can be computed in time (tilde) O(n^(rho)) and near linear space where (rho) (approx) 2.06/c as c becomes large.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-36.515350341796875, 0.2124820053577423]}, {"key": "paria2020minimizing", "year": "2020", "title": "Minimizing Flops To Learn Efficient Sparse Representations", "abstract": "<p>Deep representation learning has become one of the most widely adopted approaches for visual search recommendation and identification. Retrieval of such representations from a large database is however computationally challenging. Approximate methods based on learning compact representations have been widely explored for this problem such as locality sensitive hashing product quantization and PCA. In this work in contrast to learning compact representations we propose to learn high dimensional and sparse representations that have similar representational capacity as dense embeddings while being more efficient due to sparse matrix multiplication operations which can be much faster than dense multiplication. Following the key insight that the number of operations decreases quadratically with the sparsity of embeddings provided the non-zero entries are distributed uniformly across dimensions we propose a novel approach to learn such distributed sparse embeddings via the use of a carefully constructed regularization function that directly minimizes a continuous relaxation of the number of floating-point operations (FLOPs) incurred during retrieval. Our experiments show that our approach is competitive to the other baselines and yields a similar or better speed-vs-accuracy tradeoff on practical datasets.</p>\n", "tags": ["ARXIV", "Quantisation", "Unsupervised"], "tsne_embedding": [-24.472877502441406, -10.09483814239502]}, {"key": "parkerholder2018compressing", "year": "2018", "title": "Compressing Deep Neural Networks A New Hashing Pipeline Using Kacs Random Walk Matrices", "abstract": "<p>The popularity of deep learning is increasing by the day. However despite the recent advancements in hardware deep neural networks remain computationally intensive. Recent work has shown that by preserving the angular distance between vectors random feature maps are able to reduce dimensionality without introducing bias to the estimator. We test a variety of established hashing pipelines as well as a new approach using Kacs random walk matrices. We demonstrate that this method achieves similar accuracy to existing pipelines.</p>\n", "tags": ["ARXIV", "Deep Learning", "Supervised"], "tsne_embedding": [7.8279829025268555, -8.353523254394531]}, {"key": "parsana2007kernels", "year": "2007", "title": "Kernels On Attributed Pointsets With Applications", "abstract": "<p>This paper introduces kernels on attributed pointsets which are sets of vectors embedded in an euclidean space. The embedding gives the notion of neighborhood which is used to define positive semidefinite kernels on pointsets. Two novel kernels on neighborhoods are proposed one evaluating the attribute similarity and the other evaluating shape similarity. Shape similarity function is motivated from spectral graph matching techniques. The kernels are tested on three real life applications face recognition photo album tagging and shot annotation in video sequences with encouraging results.</p>\n", "tags": ["Graph", "NEURIPS"], "tsne_embedding": [-25.12295150756836, 9.848159790039062]}, {"key": "passalis2019deep", "year": "2019", "title": "Deep Supervised Hashing Leveraging Quadratic Spherical Mutual Information For Content-based Image Retrieval", "abstract": "<p>Several deep supervised hashing techniques have been proposed to allow for efficiently querying large image databases. However deep supervised image hashing techniques are developed to a great extent heuristically often leading to suboptimal results. Contrary to this we propose an efficient deep supervised hashing algorithm that optimizes the learned codes using an information-theoretic measure the Quadratic Mutual Information (QMI). The proposed method is adapted to the needs of large-scale hashing and information retrieval leading to a novel information-theoretic measure the Quadratic Spherical Mutual Information (QSMI). Apart from demonstrating the effectiveness of the proposed method under different scenarios and outperforming existing state-of-the-art image hashing techniques this paper provides a structured way to model the process of information retrieval and develop novel methods adapted to the needs of each application.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Supervised"], "tsne_embedding": [-11.575238227844238, 11.947759628295898]}, {"key": "patrascu2010power", "year": "2010", "title": "The Power Of Simple Tabulation Hashing", "abstract": "<p>Randomized algorithms are often enjoyed for their simplicity but the hash functions used to yield the desired theoretical guarantees are often neither simple nor practical. Here we show that the simplest possible tabulation hashing provides unexpectedly strong guarantees. The scheme itself dates back to Carter and Wegman (STOC77). Keys are viewed as consisting of c characters. We initialize c tables T_1 \u2026 T_c mapping characters to random hash codes. A key x=(x_1 \u2026 x_q) is hashed to T_1x_1 xor \u2026 xor T_cx_c. While this scheme is not even 4-independent we show that it provides many of the guarantees that are normally obtained via higher independence e.g. Chernoff-type concentration min-wise hashing for estimating set intersection and cuckoo hashing.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-20.16689682006836, -21.72385025024414]}, {"key": "pauleve2010locality", "year": "2010", "title": "Locality sensitive hashing: a comparison of hash function types and querying mechanisms", "abstract": "<p>It is well known that high-dimensional nearest-neighbor retrieval is very expensive. Dramatic performance gains are obtained using\napproximate search schemes, such as the popular Locality-Sensitive Hashing (LSH). Several extensions have been proposed to\naddress the limitations of this algorithm, in particular, by choosing more appropriate hash functions to better partition the vector\nspace. All the proposed extensions, however, rely on a structured quantizer for hashing, poorly fitting real data sets, limiting\nits performance in practice. In this paper, we compare several families of space hashing functions in a real setup, namely when\nsearching for high-dimension SIFT descriptors. The comparison of random projections, lattice quantizers, k-means and hierarchical\nk-means reveal that unstructured quantizer significantly improves the accuracy of LSH, as it closely fits the data in the feature space.\nWe then compare two querying mechanisms introduced in the literature with the one originally proposed in LSH, and discuss their\nrespective merits and limitations.</p>\n", "tags": ["Pattern Recognition Letters"], "tsne_embedding": [-23.453914642333984, -4.603760242462158]}, {"key": "peng2023check", "year": "2023", "title": "Check Your Facts And Try Again Improving Large Language Models With External Knowledge And Automated Feedback", "abstract": "<p>Large language models (LLMs) such as ChatGPT are able to generate human-like fluent responses for many downstream tasks e.g. task-oriented dialog and question answering. However applying LLMs to real-world mission-critical applications remains challenging mainly due to their tendency to generate hallucinations and their inability to use external knowledge. This paper proposes a LLM-Augmenter system which augments a black-box LLM with a set of plug-and-play modules. Our system makes the LLM generate responses grounded in external knowledge e.g. stored in task-specific databases. It also iteratively revises LLM prompts to improve model responses using feedback generated by utility functions e.g. the factuality score of a LLM-generated response. The effectiveness of LLM-Augmenter is empirically validated on two types of scenarios task-oriented dialog and open-domain question answering. LLM-Augmenter significantly reduces ChatGPTs hallucinations without sacrificing the fluency and informativeness of its responses. We make the source code and models publicly available.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [37.6064567565918, -5.983325958251953]}, {"key": "peng2023rwkv", "year": "2023", "title": "RWKV Reinventing Rnns For The Transformer Era", "abstract": "<p>Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture Receptance Weighted Key Value (RWKV) that combines the efficient parallelizable training of transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters by far the largest dense RNN ever trained and find RWKV performs on par with similarly sized Transformers suggesting future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [17.52961540222168, -13.593768119812012]}, {"key": "petrovic2010streaming", "year": "2010", "title": "Streaming First Story Detection with application to Twitter", "abstract": "<p>With the recent rise in popularity and size of\nsocial media, there is a growing need for systems\nthat can extract useful information from\nthis amount of data. We address the problem\nof detecting new events from a stream of\nTwitter posts. To make event detection feasible\non web-scale corpora, we present an algorithm\nbased on locality-sensitive hashing which\nis able overcome the limitations of traditional\napproaches, while maintaining competitive results.\nIn particular, a comparison with a stateof-the-art\nsystem on the first story detection\ntask shows that we achieve over an order of\nmagnitude speedup in processing time, while\nretaining comparable performance. Event detection\nexperiments on a collection of 160 million\nTwitter posts show that celebrity deaths\nare the fastest spreading news on Twitter.</p>\n", "tags": ["LSH", "NAACL", "Streaming Data", "Text Retrieval"], "tsne_embedding": [16.3188533782959, 14.271513938903809]}, {"key": "petrovic2012paraphrases", "year": "2012", "title": "Using paraphrases for improving first story detection in news and Twitter", "abstract": "<p>First story detection (FSD) involves identifying\nfirst stories about events from a continuous\nstream of documents. A major problem in this\ntask is the high degree of lexical variation in\ndocuments which makes it very difficult to detect\nstories that talk about the same event but\nexpressed using different words. We suggest\nusing paraphrases to alleviate this problem,\nmaking this the first work to use paraphrases\nfor FSD. We show a novel way of integrating\nparaphrases with locality sensitive hashing\n(LSH) in order to obtain an efficient FSD system\nthat can scale to very large datasets. Our\nsystem achieves state-of-the-art results on the\nfirst story detection task, beating both the best\nsupervised and unsupervised systems. To test\nour approach on large data, we construct a corpus\nof events for Twitter, consisting of 50 million\ndocuments, and show that paraphrasing is\nalso beneficial in this domain.</p>\n", "tags": ["LSH", "NAACL", "Streaming Data", "Supervised", "Text Retrieval"], "tsne_embedding": [16.526988983154297, 14.022247314453125]}, {"key": "pham2016scalability", "year": "2016", "title": "Scalability And Total Recall With Fast Coveringlsh", "abstract": "<p>Locality-sensitive hashing (LSH) has emerged as the dominant algorithmic technique for similarity search with strong performance guarantees in high-dimensional spaces. A drawback of traditional LSH schemes is that they may have emphfalse negatives i.e. the recall is less than 10037;. This limits the applicability of LSH in settings requiring precise performance guarantees. Building on the recent theoretical CoveringLSH construction that eliminates false negatives we propose a fast and practical covering LSH scheme for Hamming space called emphFast CoveringLSH (fcLSH). Inheriting the design benefits of CoveringLSH our method avoids false negatives and always reports all near neighbors. Compared to CoveringLSH we achieve an asymptotic improvement to the hash function computation time from ((dL)) to ((d + L)) where (d) is the dimensionality of data and (L) is the number of hash tables. Our experiments on synthetic and real-world data sets demonstrate that emphfcLSH is comparable (and often superior) to traditional hashing-based approaches for search radius up to 20 in high-dimensional Hamming space.</p>\n", "tags": ["ARXIV", "Independent", "LSH"], "tsne_embedding": [-28.379711151123047, 0.7663381695747375]}, {"key": "pham2022locality", "year": "2022", "title": "Falconn++ A Locality-sensitive Filtering Approach For Approximate Nearest Neighbor Search", "abstract": "<p>We present Falconn++ a novel locality-sensitive filtering (LSF) approach for approximate nearest neighbor search on angular distance. Falconn++ can filter out potential far away points in any hash bucket before querying which results in higher quality candidates compared to other hashing-based solutions. Theoretically Falconn++ asymptotically achieves lower query time complexity than Falconn an optimal locality-sensitive hashing scheme on angular distance. Empirically Falconn++ achieves a higher recall-speed tradeoff than Falconn on many real-world data sets. Falconn++ is also competitive with HNSW an efficient representative of graph-based solutions on high search recall regimes.</p>\n", "tags": ["Graph", "NEURIPS"], "tsne_embedding": [-9.079187393188477, 28.812841415405273]}, {"key": "pi2024strengthening", "year": "2024", "title": "Strengthening Multimodal Large Language Model With Bootstrapped Preference Optimization", "abstract": "<p>Multimodal Large Language Models (MLLMs) excel in generating responses based on visual inputs. However they often suffer from a bias towards generating responses similar to their pretraining corpus overshadowing the importance of visual information. We treat this bias as a preference for pretraining statistics which hinders the models grounding in visual input. To mitigate this issue we propose Bootstrapped Preference Optimization (BPO) which conducts preference learning with datasets containing negative responses bootstrapped from the model itself. Specifically we propose the following two strategies 1) using distorted image inputs to the MLLM for eliciting responses that contain signified pretraining bias; 2) leveraging text-based LLM to explicitly inject erroneous but common elements into the original response. Those undesirable responses are paired with original annotated responses from the datasets to construct the preference dataset which is subsequently utilized to perform preference learning. Our approach effectively suppresses pretrained LLM bias enabling enhanced grounding in visual inputs. Extensive experimentation demonstrates significant performance improvements across multiple benchmarks advancing the state-of-the-art in multimodal conversational systems.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [46.15071105957031, 4.271398067474365]}, {"key": "platt2003fast", "year": "2003", "title": "Fast Embedding Of Sparse Similarity Graphs", "abstract": "<p>This paper applies fast sparse multidimensional scaling (MDS) to a large graph of music similarity with 267K vertices that represent artists al- bums and tracks; and 3.22M edges that represent similarity between those entities. Once vertices are assigned locations in a Euclidean space the locations can be used to browse music and to generate playlists. MDS on very large sparse graphs can be effectively performed by a family of algorithms called Rectangular Dijsktra (RD) MDS algorithms. These RD algorithms operate on a dense rectangular slice of the distance matrix created by calling Dijsktra a constant number of times. Two RD algorithms are compared Landmark MDS which uses the Nystr\u00f6m ap- proximation to perform MDS; and a new algorithm called Fast Sparse Embedding which uses FastMap. These algorithms compare favorably to Laplacian Eigenmaps both in terms of speed and embedding quality.</p>\n", "tags": ["Graph", "NEURIPS"], "tsne_embedding": [-24.562170028686523, 5.547730445861816]}, {"key": "portegys2015general", "year": "2015", "title": "General Graph Identification By Hashing", "abstract": "<p>A method for identifying graphs using MD5 hashing is presented. This allows fast graph equality comparisons and can also be used to facilitate graph isomorphism testing. The graphs can be labeled or unlabeled. The method identifies vertices by hashing the graph configuration in their neighborhoods. With each vertex hashed the entire graph can be identified by hashing the vertex hashes.</p>\n", "tags": ["ARXIV", "Graph"], "tsne_embedding": [-2.0929644107818604, 29.643516540527344]}, {"key": "press2022measuring", "year": "2022", "title": "Measuring And Narrowing The Compositionality Gap In Language Models", "abstract": "<p>We investigate the ability of language models to perform compositional reasoning tasks where the overall solution depends on correctly composing the answers to sub-problems. We measure how often models can correctly answer all sub-problems but not generate the overall solution a ratio we call the compositionality gap. We evaluate this ratio by asking multi-hop questions with answers that require composing multiple facts unlikely to have been observed together during pretraining. In the GPT-3 family of models as model size increases we show that the single-hop question answering performance improves faster than the multi-hop performance does therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge they show no corresponding improvement in their ability to perform this kind of compositional reasoning. We then demonstrate how elicitive prompting (such as chain of thought) narrows the compositionality gap by reasoning explicitly. We present a new method self-ask that further improves on chain of thought. In our method the model explicitly asks itself (and answers) follow-up questions before answering the initial question. We finally show that self-asks structured prompting lets us easily plug in a search engine to answer the follow-up questions which additionally improves accuracy.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [30.76498794555664, 2.6320488452911377]}, {"key": "pronobis2016sharing", "year": "2016", "title": "Sharing Hash Codes For Multiple Purposes", "abstract": "<p>Locality sensitive hashing (LSH) is a powerful tool for sublinear-time approximate nearest neighbor search and a variety of hashing schemes have been proposed for different dissimilarity measures. However hash codes significantly depend on the dissimilarity which prohibits users from adjusting the dissimilarity at query time. In this paper we propose multiple purpose LSH (mp-LSH) which shares the hash codes for different dissimilarities. mp-LSH supports L2 cosine and inner product dissimilarities and their corresponding weighted sums where the weights can be adjusted at query time. It also allows us to modify the importance of pre-defined groups of features. Thus mp-LSH enables us for example to retrieve similar items to a query with the user preference taken into account to find a similar material to a query with some properties (stability utility etc.) optimized and to turn on or off a part of multi-modal information (brightness color audio text etc.) in image/video retrieval. We theoretically and empirically analyze the performance of three variants of mp-LSH and demonstrate their usefulness on real-world data sets.</p>\n", "tags": ["ARXIV", "Independent", "LSH", "Video Retrieval"], "tsne_embedding": [-17.981887817382812, 6.857372283935547]}, {"key": "q2023mistral", "year": "2023", "title": "Mistral 7B", "abstract": "<p>We introduce Mistral 7B v0.1 a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks and Llama 1 34B in reasoning mathematics and code generation. Our model leverages grouped-query attention (GQA) for faster inference coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions Mistral 7B \u2013 Instruct that surpasses the Llama 2 13B \u2013 Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [20.65517807006836, 9.608553886413574]}, {"key": "qasemizadeh2017sketching", "year": "2017", "title": "Sketching Word Vectors Through Hashing", "abstract": "<p>We propose a new fast word embedding technique using hash functions. The method is a derandomization of a new type of random projections By disregarding the classic constraint used in designing random projections (i.e. preserving pairwise distances in a particular normed space) our solution exploits extremely sparse non-negative random projections. Our experiments show that the proposed method can achieve competitive results comparable to neural embedding learning techniques however with only a fraction of the computational complexity of these methods. While the proposed derandomization enhances the computational and space complexity of our method the possibility of applying weighting methods such as positive pointwise mutual information (PPMI) to our models after their construction (and at a reduced dimensionality) imparts a high discriminatory power to the resulting embeddings. Obviously this method comes with other known benefits of random projection-based techniques such as ease of update.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-28.042131423950195, -6.480258464813232]}, {"key": "qi2017efficient", "year": "2017", "title": "An Efficient Deep Learning Hashing Neural Network For Mobile Visual Search", "abstract": "<p>Mobile visual search applications are emerging that enable users to sense their surroundings with smart phones. However because of the particular challenges of mobile visual search achieving a high recognition bitrate has becomes a consistent target of previous related works. In this paper we propose a few-parameter low-latency and high-accuracy deep hashing approach for constructing binary hash codes for mobile visual search. First we exploit the architecture of the MobileNet model which significantly decreases the latency of deep feature extraction by reducing the number of model parameters while maintaining accuracy. Second we add a hash-like layer into MobileNet to train the model on labeled mobile visual data. Evaluations show that the proposed system can exceed state-of-the-art accuracy performance in terms of the MAP. More importantly the memory consumption is much less than that of other deep learning models. The proposed method requires only 13 MB of memory for the neural network and achieves a MAP of 97.8037; on the mobile location recognition dataset used for testing.</p>\n", "tags": ["ARXIV", "Deep Learning", "Supervised"], "tsne_embedding": [-3.596184253692627, -12.393498420715332]}, {"key": "qian2023creator", "year": "2023", "title": "CREATOR Tool Creation For Disentangling Abstract And Concrete Reasoning Of Large Language Models", "abstract": "<p>Large Language Models (LLMs) have made significant progress in utilizing tools but their ability is limited by API availability and the instability of implicit reasoning particularly when both planning and execution are involved. To overcome these limitations we propose CREATOR a novel framework that enables LLMs to create their own tools using documentation and code realization. CREATOR disentangles abstract tool creation and concrete decision execution resulting in improved performance. We evaluate CREATOR on MATH and TabMWP benchmarks respectively consisting of challenging math competition problems and diverse tabular contents. Remarkably CREATOR outperforms existing chain-of-thought program-of-thought and tool-using baselines. Additionally we introduce the Creation Challenge dataset featuring 2K diverse questions to emphasize the necessity and benefits of LLMs tool creation ability. Further research demonstrates that leveraging LLMs as tool creators facilitates knowledge transfer and LLMs exhibit varying levels of tool creation abilities enabling them to adapt to diverse situations. The tool creation ability revolutionizes the LLMs problem-solving paradigm driving us closer to the next frontier of artificial intelligence. All the codes and data are released.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [27.686723709106445, -13.85226821899414]}, {"key": "qiao2019deep", "year": "2019", "title": "Deep Heterogeneous Hashing For Face Video Retrieval", "abstract": "<p>Retrieving videos of a particular person with face image as a query via hashing technique has many important applications. While face images are typically represented as vectors in Euclidean space characterizing face videos with some robust set modeling techniques (e.g. covariance matrices as exploited in this study which reside on Riemannian manifold) has recently shown appealing advantages. This hence results in a thorny heterogeneous spaces matching problem. Moreover hashing with handcrafted features as done in many existing works is clearly inadequate to achieve desirable performance for this task. To address such problems we present an end-to-end Deep Heterogeneous Hashing (DHH) method that integrates three stages including image feature learning video modeling and heterogeneous hashing in a single framework to learn unified binary codes for both face images and videos. To tackle the key challenge of hashing on the manifold a well-studied Riemannian kernel mapping is employed to project data (i.e. covariance matrices) into Euclidean space and thus enables to embed the two heterogeneous representations into a common Hamming space where both intra-space discriminability and inter-space compatibility are considered. To perform network optimization the gradient of the kernel mapping is innovatively derived via structured matrix backpropagation in a theoretically principled way. Experiments on three challenging datasets show that our method achieves quite competitive performance compared with existing hashing methods.</p>\n", "tags": ["Cross Modal", "ICIP", "Video Retrieval"], "tsne_embedding": [-3.170452117919922, 2.7031428813934326]}, {"key": "qiao2022reasoning", "year": "2022", "title": "Reasoning With Language Model Prompting A Survey", "abstract": "<p>Reasoning as an essential ability for complex problem-solving can provide back-end support for various real-world applications such as medical diagnosis negotiation etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).</p>\n", "tags": ["ARXIV", "Has Code", "Survey Paper"], "tsne_embedding": [36.029239654541016, -11.795943260192871]}, {"key": "qin2023tool", "year": "2023", "title": "Tool Learning With Foundation Models", "abstract": "<p>Humans possess an extraordinary ability to create and utilize tools allowing them to overcome physical limitations and explore new frontiers. With the advent of foundation models AI systems have the potential to be equally adept in tool use as humans. This paradigm i.e. tool learning with foundation models combines the strengths of specialized tools and foundation models to achieve enhanced accuracy efficiency and automation in problem-solving. Despite its immense potential there is still a lack of a comprehensive understanding of key challenges opportunities and future endeavors in this field. To this end we present a systematic investigation of tool learning in this paper. We first introduce the background of tool learning including its cognitive origins the paradigm shift of foundation models and the complementary roles of tools and models. Then we recapitulate existing tool learning research into tool-augmented and tool-oriented learning. We formulate a general tool learning framework starting from understanding the user instruction models should learn to decompose a complex task into several subtasks dynamically adjust their plan through reasoning and effectively conquer each sub-task by selecting appropriate tools. We also discuss how to train models for improved tool-use capabilities and facilitate the generalization in tool learning. Considering the lack of a systematic tool learning evaluation in prior works we experiment with 18 representative tools and show the potential of current foundation models in skillfully utilizing tools. Finally we discuss several open problems that require further investigation for tool learning. In general we hope this paper could inspire future research in integrating tools with foundation models.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [29.55980110168457, -15.132128715515137]}, {"key": "qiu2014random", "year": "2014", "title": "Random Forests Can Hash", "abstract": "<p>Hash codes are a very efficient data representation needed to be able to cope with the ever growing amounts of data. We introduce a random forest semantic hashing scheme with information-theoretic code aggregation showing for the first time how random forest a technique that together with deep learning have shown spectacular results in classification can also be extended to large-scale retrieval. Traditional random forest fails to enforce the consistency of hashes generated from each tree for the same class data i.e. to preserve the underlying similarity and it also lacks a principled way for code aggregation across trees. We start with a simple hashing scheme where independently trained random trees in a forest are acting as hashing functions. We the propose a subspace model as the splitting function and show that it enforces the hash consistency in a tree for data from the same class. We also introduce an information-theoretic approach for aggregating codes of individual trees into a single hash code producing a near-optimal unique hash for each class. Experiments on large-scale public datasets are presented showing that the proposed approach significantly outperforms state-of-the-art hashing methods for retrieval tasks.</p>\n", "tags": ["ARXIV", "Deep Learning", "Independent"], "tsne_embedding": [1.5720477104187012, 30.953346252441406]}, {"key": "qiu2017deep", "year": "2017", "title": "Deep Semantic Hashing with Generative Adversarial Networks ", "abstract": "<p>Hashing has been a widely-adopted technique for nearest\nneighbor search in large-scale image retrieval tasks. Recent research has shown that leveraging supervised information can\nlead to high quality hashing. However, the cost of annotating\ndata is often an obstacle when applying supervised hashing\nto a new domain. Moreover, the results can suffer from the\nrobustness problem as the data at training and test stage\nmay come from different distributions. This paper studies\nthe exploration of generating synthetic data through semisupervised generative adversarial networks (GANs), which\nleverages largely unlabeled and limited labeled training data\nto produce highly compelling data with intrinsic invariance\nand global coherence, for better understanding statistical\nstructures of natural data. We demonstrate that the above\ntwo limitations can be well mitigated by applying the synthetic data for hashing. Specifically, a novel deep semantic\nhashing with GANs (DSH-GANs) is presented, which mainly\nconsists of four components: a deep convolution neural networks (CNN) for learning image representations, an adversary\nstream to distinguish synthetic images from real ones, a hash\nstream for encoding image representations to hash codes and\na classification stream. The whole architecture is trained endto-end by jointly optimizing three losses, i.e., adversarial loss\nto correct label of synthetic or real for each sample, triplet\nranking loss to preserve the relative similarity ordering in the\ninput real-synthetic triplets and classification loss to classify\neach sample accurately. Extensive experiments conducted on\nboth CIFAR-10 and NUS-WIDE image benchmarks validate the capability of exploiting synthetic images for hashing. Our\nframework also achieves superior results when compared to\nstate-of-the-art deep hash models.</p>\n", "tags": ["CNN", "Deep Learning", "GAN", "Image Retrieval", "SIGIR", "Supervised"], "tsne_embedding": [4.225627422332764, 3.976349353790283]}, {"key": "qiu2017foresthash", "year": "2017", "title": "Foresthash Semantic Hashing With Shallow Random Forests And Tiny Convolutional Networks", "abstract": "<p>Hash codes are efficient data representations for coping with the ever growing amounts of data. In this paper we introduce a random forest semantic hashing scheme that embeds tiny convolutional neural networks (CNN) into shallow random forests with near-optimal information-theoretic code aggregation among trees. We start with a simple hashing scheme where random trees in a forest act as hashing functions by setting 1 for the visited tree leaf and 0 for the rest. We show that traditional random forests fail to generate hashes that preserve the underlying similarity between the trees rendering the random forests approach to hashing challenging. To address this we propose to first randomly group arriving classes at each tree split node into two groups obtaining a significantly simplified two-class classification problem which can be handled using a light-weight CNN weak learner. Such random class grouping scheme enables code uniqueness by enforcing each class to share its code with different classes in different trees. A non-conventional low-rank loss is further adopted for the CNN weak learners to encourage code consistency by minimizing intra-class variations and maximizing inter-class distance for the two random class groups. Finally we introduce an information-theoretic approach for aggregating codes of individual trees into a single hash code producing a near-optimal unique hash for each class. The proposed approach significantly outperforms state-of-the-art hashing methods for image retrieval tasks on large-scale public datasets while performing at the level of other state-of-the-art image classification techniques while utilizing a more compact and efficient scalable representation. This work proposes a principled and robust procedure to train and deploy in parallel an ensemble of light-weight CNNs instead of simply going deeper.</p>\n", "tags": ["ARXIV", "CNN", "Image Retrieval", "Independent"], "tsne_embedding": [1.6002167463302612, 30.97186279296875]}, {"key": "qiu2018deep", "year": "2018", "title": "Deep Semantic Hashing With Generative Adversarial Networks", "abstract": "<p>Hashing has been a widely-adopted technique for nearest neighbor search in large-scale image retrieval tasks. Recent research has shown that leveraging supervised information can lead to high quality hashing. However the cost of annotating data is often an obstacle when applying supervised hashing to a new domain. Moreover the results can suffer from the robustness problem as the data at training and test stage could come from similar but different distributions. This paper studies the exploration of generating synthetic data through semi-supervised generative adversarial networks (GANs) which leverages largely unlabeled and limited labeled training data to produce highly compelling data with intrinsic invariance and global coherence for better understanding statistical structures of natural data. We demonstrate that the above two limitations can be well mitigated by applying the synthetic data for hashing. Specifically a novel deep semantic hashing with GANs (DSH-GANs) is presented which mainly consists of four components a deep convolution neural networks (CNN) for learning image representations an adversary stream to distinguish synthetic images from real ones a hash stream for encoding image representations to hash codes and a classification stream. The whole architecture is trained end-to-end by jointly optimizing three losses i.e. adversarial loss to correct label of synthetic or real for each sample triplet ranking loss to preserve the relative similarity ordering in the input real-synthetic triplets and classification loss to classify each sample accurately. Extensive experiments conducted on both CIFAR-10 and NUS-WIDE image benchmarks validate the capability of exploiting synthetic images for hashing. Our framework also achieves superior results when compared to state-of-the-art deep hash models.</p>\n", "tags": ["ARXIV", "CNN", "Image Retrieval", "Supervised"], "tsne_embedding": [4.182186603546143, 3.9936959743499756]}, {"key": "qiu2021unsupervised", "year": "2021", "title": "Unsupervised Hashing With Contrastive Information Bottleneck", "abstract": "<p>Many unsupervised hashing methods are implicitly established on the idea of reconstructing the input data which basically encourages the hashing codes to retain as much information of original data as possible. However this requirement may force the models spending lots of their effort on reconstructing the unuseful background information while ignoring to preserve the discriminative semantic information that is more important for the hashing task. To tackle this problem inspired by the recent success of contrastive learning in learning continuous representations we propose to adapt this framework to learn binary hashing codes. Specifically we first propose to modify the objective function to meet the specific requirement of hashing and then introduce a probabilistic binary representation layer into the model to facilitate end-to-end training of the entire model. We further prove the strong connection between the proposed contrastive-learning-based hashing method and the mutual information and show that the proposed model can be considered under the broader framework of the information bottleneck (IB). Under this perspective a more general hashing model is naturally obtained. Extensive experimental results on three benchmark image datasets demonstrate that the proposed hashing method significantly outperforms existing baselines.</p>\n", "tags": ["ARXIV", "Unsupervised"], "tsne_embedding": [-3.902839422225952, -3.9884445667266846]}, {"key": "qiu2023smile", "year": "2023", "title": "SMILE Single-turn To Multi-turn Inclusive Language Expansion Via Chatgpt For Mental Health Support", "abstract": "<p>Developing specialized dialogue systems for mental health support requires multi-turn conversation data which has recently garnered increasing attention. However gathering and releasing large-scale and real-life multi-turn conversations to facilitate advancements in mental health presents challenges due to data privacy protection as well as the time and cost involved. To address the challenges related to data scarcity we introduce SMILE a single-turn to multi-turn inclusive language expansion technique that prompts ChatGPT to rewrite public single-turn dialogues into multi-turn ones. Our work begins with the analysis of language transformation validating the feasibility of the proposed method when compared with other baseline methods. We then conduct a study on dialogue diversity including lexical features semantic features and dialogue topics demonstrating the effectiveness of our proposed method. Furthermore we implement an expert evaluation and the results demonstrate that the dialogues generated with our proposed method are of higher quality than those generated with other baseline methods. Thus we employ our method to generate a large-scale diverse and high-quality dialogue dataset named SmileChat comprising 55165 dialogues in total with an average of 10.4 turns per dialogue. Finally we utilize the collected corpus to develop a mental health chatbot MeChat. To better assess the overall quality of SmileChat we collect a real-life chat dataset comprising 82 counseling dialogues for model evaluation. Both automatic and human evaluations demonstrate that our trained dialogue system exhibits significant improvements showcasing that SmileChat is high-quality and practical.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [36.378238677978516, -9.023862838745117]}, {"key": "qu2024alleviating", "year": "2024", "title": "Alleviating Hallucination In Large Vision-language Models With Active Retrieval Augmentation", "abstract": "<p>Despite the remarkable ability of large vision-language models (LVLMs) in image comprehension these models frequently generate plausible yet factually incorrect responses a phenomenon known as hallucination.Recently in large language models (LLMs) augmenting LLMs by retrieving information from external knowledge resources has been proven as a promising solution to mitigate hallucinations.However the retrieval augmentation in LVLM significantly lags behind the widespread applications of LVLM. Moreover when transferred to augmenting LVLMs sometimes the hallucination degree of the model is even exacerbated.Motivated by the research gap and counter-intuitive phenomenon we introduce a novel framework the Active Retrieval-Augmented large vision-language model (ARA) specifically designed to address hallucinations by incorporating three critical dimensions (i) dissecting the retrieval targets based on the inherent hierarchical structures of images. (ii) pinpointing the most effective retrieval methods and filtering out the reliable retrieval results. (iii) timing the retrieval process to coincide with episodes of low certainty while circumventing unnecessary retrieval during periods of high certainty. To assess the capability of our proposed ARA model in reducing hallucination we employ three widely used LVLM models (LLaVA-1.5 Qwen-VL and mPLUG-Owl2) across four benchmarks. Our empirical observations suggest that by utilizing fitting retrieval mechanisms and timing the retrieval judiciously we can effectively mitigate the hallucination problem. We hope that this study can provide deeper insights into how to adapt the retrieval augmentation to LVLMs for reducing hallucinations with more effective retrieval and minimal retrieval occurrences.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [49.15785598754883, 7.666995048522949]}, {"key": "rafailov2023direct", "year": "2023", "title": "Direct Preference Optimization Your Language Model Is Secretly A Reward Model", "abstract": "<p>While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences often with reinforcement learning from human feedback (RLHF). However RLHF is a complex and often unstable procedure first fitting a reward model that reflects the human preferences and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm which we call Direct Preference Optimization (DPO) is stable performant and computationally lightweight eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.</p>\n", "tags": ["ARXIV", "Unsupervised"], "tsne_embedding": [42.22039031982422, -1.6912600994110107]}, {"key": "raff2017lempel", "year": "2017", "title": "Lempel-ziv Jaccard Distance An Effective Alternative To Ssdeep And Sdhash", "abstract": "<p>Recent work has proposed the Lempel-Ziv Jaccard Distance (LZJD) as a method to measure the similarity between binary byte sequences for malware classification. We propose and test LZJDs effectiveness as a similarity digest hash for digital forensics. To do so we develop a high performance Java implementation with the same command-line arguments as sdhash making it easy to integrate into existing workflows. Our testing shows that LZJD is effective for this task and significantly outperforms sdhash and ssdeep in its ability to match related file fragments and files corrupted with random noise. In addition LZJD is up to 60x faster than sdhash at comparison time.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [-24.71477508544922, -0.7668439745903015]}, {"key": "raffel2019exploring", "year": "2019", "title": "Exploring The Limits Of Transfer Learning With A Unified Text-to-text Transformer", "abstract": "<p>Transfer learning where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches methodology and practice. In this paper we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives architectures unlabeled data sets transfer approaches and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new Colossal Clean Crawled Corpus we achieve state-of-the-art results on many benchmarks covering summarization question answering text classification and more. To facilitate future work on transfer learning for NLP we release our data set pre-trained models and code.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [30.183551788330078, -6.201456069946289]}, {"key": "raginsky2009locality", "year": "2009", "title": "Locality-sensitive Binary Codes From Shift-invariant Kernels", "abstract": "<p>This paper addresses the problem of designing binary codes for high-dimensional data such that vectors that are similar in the original space map to similar binary strings. We introduce a simple distribution-free encoding scheme based on random projections such that the expected Hamming distance between the binary codes of two vectors is related to the value of a shift-invariant kernel (e.g. a Gaussian kernel) between the vectors. We present a full theoretical analysis of the convergence properties of the proposed scheme and report favorable experimental performance as compared to a recent state-of-the-art method spectral hashing.</p>\n", "tags": ["Independent", "NEURIPS"], "tsne_embedding": [-25.688907623291016, 8.949220657348633]}, {"key": "rajbhandari2019zero", "year": "2019", "title": "Zero Memory Optimizations Toward Training Trillion Parameter Models", "abstract": "<p>Large deep learning models offer significant accuracy gains but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory while obtaining computation communication and development efficiency. We develop a novel solution Zero Redundancy Optimizer (ZeRO) to optimize memory vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates ZeRO has the potential to scale beyond 1 Trillion parameters using todays hardware. We implement and evaluate ZeRO it trains large models of over 100B parameter with super-linear speedup on 400 GPUs achieving throughput of 15 Petaflops. This represents an 8x increase in model size and 10x increase in achievable performance over state-of-the-art. In terms of usability ZeRO can train large models of up to 13B parameters (e.g. larger than Megatron GPT 8.3B and T5 11B) without requiring model parallelism which is harder for scientists to apply. Last but not the least researchers have used the system breakthroughs of ZeRO to create the worlds largest language model (Turing-NLG 17B parameters) with record breaking accuracy.</p>\n", "tags": ["ARXIV", "Deep Learning"], "tsne_embedding": [17.839229583740234, -10.391582489013672]}, {"key": "ram2009rank", "year": "2009", "title": "Rank-approximate Nearest Neighbor Search Retaining Meaning And Speed In High Dimensions", "abstract": "<p>The long-standing problem of efficient nearest-neighbor (NN) search has ubiquitous applications ranging from astrophysics to MP3 fingerprinting to bioinformatics to movie recommendations. As the dimensionality of the dataset increases exact NN search becomes computationally prohibitive; (1+eps)-distance-approximate NN search can provide large speedups but risks losing the meaning of NN search present in the ranks (ordering) of the distances. This paper presents a simple practical algorithm allowing the user to for the first time directly control the true accuracy of NN search (in terms of ranks) while still achieving the large speedups over exact NN. Experiments with high-dimensional datasets show that it often achieves faster and more accurate results than the best-known distance-approximate method with much more stable behavior.</p>\n", "tags": ["NEURIPS"], "tsne_embedding": [-30.492536544799805, -6.145547389984131]}, {"key": "ramos2024boost", "year": "2024", "title": "BlockBoost: Scalable and Efficient Blocking through Boosting", "abstract": "<p>As datasets grow larger, matching and merging entries from different databases has become a costly task in modern data pipelines. To avoid expensive comparisons between entries, blocking similar items is a popular preprocessing step. In this paper, we introduce BlockBoost, a novel boosting-based method that generates compact binary hash codes for database entries, through which blocking can be performed efficiently. The algorithm is fast and scalable, resulting in computational costs that are orders of magnitude lower than current benchmarks. Unlike existing alternatives, BlockBoost comes with associated feature importance measures for interpretability, and possesses strong theoretical guarantees, including lower bounds on critical performance metrics like recall and reduction ratio. Finally, we show that BlockBoost delivers great empirical results, outperforming state-of-the-art blocking benchmarks in terms of both performance metrics and computational cost.</p>\n", "tags": ["AISTATS", "Has Code"], "tsne_embedding": [-25.547075271606445, -12.812583923339844]}, {"key": "rao2015diverse", "year": "2015", "title": "Diverse Yet Efficient Retrieval Using Hash Functions", "abstract": "<p>Typical retrieval systems have three requirements a) Accurate retrieval i.e. the method should have high precision b) Diverse retrieval i.e. the obtained set of points should be diverse c) Retrieval time should be small. However most of the existing methods address only one or two of the above mentioned requirements. In this work we present a method based on randomized locality sensitive hashing which tries to address all of the above requirements simultaneously. While earlier hashing approaches considered approximate retrieval to be acceptable only for the sake of efficiency we argue that one can further exploit approximate retrieval to provide impressive trade-offs between accuracy and diversity. We extend our method to the problem of multi-label prediction where the goal is to output a diverse and accurate set of labels for a given document in real-time. Moreover we introduce a new notion to simultaneously evaluate a methods performance for both the precision and diversity measures. Finally we present empirical results on several different retrieval tasks and show that our method retrieves diverse and accurate images/labels while ensuring (100x)-speed-up over the existing diverse retrieval approaches.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Independent"], "tsne_embedding": [-9.082673072814941, -10.829507827758789]}, {"key": "rastegari2016xnor", "year": "2016", "title": "XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks ", "abstract": "<p>We propose two efficient approximations to standard convolutional neural networks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks, the filters are approximated with binary values\nresulting in 32x memory saving. In XNOR-Networks, both the filters and the input to convolutional layers are binary. XNOR-Networks approximate convolutions using primarily binary operations. This\nresults in 58x faster convolutional operations and 32x memory savings. XNOR-Nets offer the possibility of running state-of-the-art networks on CPUs (rather than GPUs) in real-time. Our binary\nnetworks are simple, accurate, efficient, and work on challenging visual tasks. We evaluate our approach on the ImageNet classification task. The classification accuracy with a Binary-Weight-Network\nversion of AlexNet is only 2.9\\% less than the full-precision AlexNet (in top-1 measure). We compare our method with recent network binarization methods, BinaryConnect and BinaryNets, and outperform these methods by large margins on ImageNet, more than 16\\% in top-1 accuracy.</p>\n", "tags": ["Deep Learning", "Has Code", "Image Retrieval"], "tsne_embedding": [-21.086753845214844, 11.335123062133789]}, {"key": "raziperchikolaei2015optimizing", "year": "2015", "title": "Optimizing Affinity-based Binary Hashing Using Auxiliary Coordinates", "abstract": "<p>In supervised binary hashing one wants to learn a function that maps a high-dimensional feature vector to a vector of binary codes for application to fast image retrieval. This typically results in a difficult optimization problem nonconvex and nonsmooth because of the discrete variables involved. Much work has simply relaxed the problem during training solving a continuous optimization and truncating the codes a posteriori. This gives reasonable results but is quite suboptimal. Recent work has tried to optimize the objective directly over the binary codes and achieved better results but the hash function was still learned a posteriori which remains suboptimal. We propose a general framework for learning hash functions using affinity-based loss functions that uses auxiliary coordinates. This closes the loop and optimizes jointly over the hash functions and the binary codes so that they gradually match each other. The resulting algorithm can be seen as a corrected iterated version of the procedure of optimizing first over the codes and then learning the hash function. Compared to this our optimization is guaranteed to obtain better hash functions while being not much slower as demonstrated experimentally in various supervised datasets. In addition our framework facilitates the design of optimization algorithms for arbitrary types of loss and hash functions.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Supervised"], "tsne_embedding": [-0.9541102647781372, -2.3238043785095215]}, {"key": "raziperchikolaei2016optimizing", "year": "2016", "title": "Optimizing Affinity-based Binary Hashing Using Auxiliary Coordinates", "abstract": "<p>In supervised binary hashing one wants to learn a function that maps a high-dimensional feature vector to a vector of binary codes for application to fast image retrieval. This typically results in a difficult optimization problem nonconvex and nonsmooth because of the discrete variables involved. Much work has simply relaxed the problem during training solving a continuous optimization and truncating the codes a posteriori. This gives reasonable results but is quite suboptimal. Recent work has tried to optimize the objective directly over the binary codes and achieved better results but the hash function was still learned a posteriori which remains suboptimal. We propose a general framework for learning hash functions using affinity-based loss functions that uses auxiliary coordinates. This closes the loop and optimizes jointly over the hash functions and the binary codes so that they gradually match each other. The resulting algorithm can be seen as an iterated version of the procedure of optimizing first over the codes and then learning the hash function. Compared to this our optimization is guaranteed to obtain better hash functions while being not much slower as demonstrated experimentally in various supervised datasets. In addition our framework facilitates the design of optimization algorithms for arbitrary types of loss and hash functions.</p>\n", "tags": ["Image Retrieval", "NEURIPS", "Supervised"], "tsne_embedding": [-0.9312431812286377, -2.319105625152588]}, {"key": "ren2020hm", "year": "2020", "title": "HM-ANN Efficient Billion-point Nearest Neighbor Search On Heterogeneous Memory", "abstract": "<p>The state-of-the-art approximate nearest neighbor search (ANNS) algorithms face a fundamental tradeoff between query latency and accuracy because of small main memory capacity To store indices in main memory for short query latency the ANNS algorithms have to limit dataset size or use a quantization scheme which hurts search accuracy. The emergence of heterogeneous memory (HM) brings a solution to significantly increase memory capacity and break the above tradeoff Using HM billions of data points can be placed in the main memory on a single machine without using any data compression. However HM consists of both fast (but small) memory and slow (but large) memory and using HM inappropriately slows down query significantly. In this work we present a novel graph-based similarity search algorithm called HM-ANN which takes both memory and data heterogeneity into consideration and enables billion-scale similarity search on a single node without using compression. On two billion-sized datasets BIGANN and DEEP1B HM-ANN outperforms state-of-the-art compression-based solutions such as Lamp;C and IMI+OPQ in recall-vs-latency by a large margin obtaining 4637; higher recall under the same search latency. We also extend existing graph-based methods such as HNSW and NSG with two strong baseline implementations on HM. At billion-point scale HM-ANN is 2X and 5.8X faster than our HNSWand NSG baselines respectively to reach the same accuracy.</p>\n", "tags": ["Cross Modal", "Graph", "NEURIPS", "Quantisation"], "tsne_embedding": [-28.272512435913086, -13.458357810974121]}, {"key": "ren2023pangu", "year": "2023", "title": "Pangu-\u03c3 Towards Trillion Parameter Language Model With Sparse Heterogeneous Computing", "abstract": "<p>The scaling of large language models has greatly improved natural language understanding generation and reasoning. In this work we develop a system that trained a trillion-parameter language model on a cluster of Ascend 910 AI processors and MindSpore framework and present the language model with 1.085T parameters named PanGu-Sigma. With parameter inherent from PanGu-alpha we extend the dense Transformer model to sparse one with Random Routed Experts (RRE) and efficiently train the model over 329B tokens by using Expert Computation and Storage Separation(ECSS). This resulted in a 6.3x increase in training throughput through heterogeneous computing. Our experimental findings show that PanGu-Sigma provides state-of-the-art performance in zero-shot learning of various Chinese NLP downstream tasks. Moreover it demonstrates strong abilities when fine-tuned in application data of open-domain dialogue question answering machine translation and code generation.</p>\n", "tags": ["ARXIV", "Cross Modal", "Independent"], "tsne_embedding": [23.449426651000977, -7.8743414878845215]}, {"key": "ren2023representation", "year": "2023", "title": "Representation Learning With Large Language Models For Recommendation", "abstract": "<p>Recommender systems have seen significant advancements with the influence of deep learning and graph neural networks particularly in capturing complex user-item relationships. However these graph-based recommenders heavily depend on ID-based data potentially disregarding valuable textual information associated with users and items resulting in less informative learned representations. Moreover the utilization of implicit feedback data introduces potential noise and bias posing challenges for the effectiveness of user preference learning. While the integration of large language models (LLMs) into traditional ID-based recommenders has gained attention challenges such as scalability issues limitations in text-only reliance and prompt input constraints need to be addressed for effective implementation in practical recommender systems. To address these challenges we propose a model-agnostic framework RLMRec that aims to enhance existing recommenders with LLM-empowered representation learning. It proposes a recommendation paradigm that integrates representation learning with LLMs to capture intricate semantic aspects of user behaviors and preferences. RLMRec incorporates auxiliary textual signals develops a user/item profiling paradigm empowered by LLMs and aligns the semantic space of LLMs with the representation space of collaborative relational signals through a cross-view alignment framework. This work further establish a theoretical foundation demonstrating that incorporating textual signals through mutual information maximization enhances the quality of representations. In our evaluation we integrate RLMRec with state-of-the-art recommender models while also analyzing its efficiency and robustness to noise data. Our implementation codes are available at https://github.com/HKUDS/RLMRec.</p>\n", "tags": ["ARXIV", "Cross Modal", "Deep Learning", "Graph", "Has Code", "Supervised"], "tsne_embedding": [42.435733795166016, -11.846587181091309]}, {"key": "roberts2020how", "year": "2020", "title": "How Much Knowledge Can You Pack Into The Parameters Of A Language Model", "abstract": "<p>It has recently been observed that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural language queries. In this short paper we measure the practical utility of this approach by fine-tuning pre-trained models to answer questions without access to any external context or knowledge. We show that this approach scales with model size and performs competitively with open-domain systems that explicitly retrieve answers from an external knowledge source when answering questions. To facilitate reproducibility and future work we release our code and trained models at https://goo.gle/t5-cbqa.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [26.492229461669922, -6.608293056488037]}, {"key": "rong2018locality", "year": "2018", "title": "Locality-Sensitive Hashing for Earthquake Detection: A Case Study of Scaling Data-Driven Science", "abstract": "<p>In this work, we report on a novel application of Locality Sensitive\nHashing (LSH) to seismic data at scale. Based on the high waveform similarity between reoccurring earthquakes, our application\nidentifies potential earthquakes by searching for similar time series\nsegments via LSH. However, a straightforward implementation of\nthis LSH-enabled application has difficulty scaling beyond 3 months\nof continuous time series data measured at a single seismic station.\nAs a case study of a data-driven science workflow, we illustrate how\ndomain knowledge can be incorporated into the workload to improve\nboth the efficiency and result quality. We describe several end-toend optimizations of the analysis pipeline from pre-processing to\npost-processing, which allow the application to scale to time series data measured at multiple seismic stations. Our optimizations\nenable an over 100\u00d7 speedup in the end-to-end analysis pipeline.\nThis improved scalability enabled seismologists to perform seismic\nanalysis on more than ten years of continuous time series data from\nover ten seismic stations, and has directly enabled the discovery of\n597 new earthquakes near the Diablo Canyon nuclear power plant\nin California and 6123 new earthquakes in New Zealand.</p>\n", "tags": ["Case Study", "LSH", "VLDB"], "tsne_embedding": [-25.210390090942383, 16.078266143798828]}, {"key": "rose2023personalisation", "year": "2023", "title": "Personalisation Within Bounds A Risk Taxonomy And Policy Framework For The Alignment Of Large Language Models With Personalised Feedback", "abstract": "<p>Large language models (LLMs) are used to generate content for a wide range of tasks and are set to reach a growing audience in coming years due to integration in product interfaces like ChatGPT or search engines like Bing. This intensifies the need to ensure that models are aligned with human preferences and do not produce unsafe inaccurate or toxic outputs. While alignment techniques like reinforcement learning with human feedback (RLHF) and red-teaming can mitigate some safety concerns and improve model capabilities it is unlikely that an aggregate fine-tuning process can adequately represent the full range of users preferences and values. Different people may legitimately disagree on their preferences for language and conversational norms as well as on values or ideologies which guide their communication. Personalising LLMs through micro-level preference learning processes may result in models that are better aligned with each user. However there are several normative challenges in defining the bounds of a societally-acceptable and safe degree of personalisation. In this paper we ask how and in what ways LLMs should be personalised. First we review literature on current paradigms for aligning LLMs with human feedback and identify issues including (i) a lack of clarity regarding what alignment means; (ii) a tendency of technology providers to prescribe definitions of inherently subjective preferences and values; and (iii) a tyranny of the crowdworker exacerbated by a lack of documentation in who we are really aligning to. Second we present a taxonomy of benefits and risks associated with personalised LLMs for individuals and society at large. Finally we propose a three-tiered policy framework that allows users to experience the benefits of personalised alignment while restraining unsafe and undesirable LLM-behaviours within (supra-)national and organisational bounds.</p>\n", "tags": ["ARXIV", "Survey Paper"], "tsne_embedding": [46.302303314208984, -12.803908348083496]}, {"key": "rose2023visual", "year": "2023", "title": "Visual Chain Of Thought Bridging Logical Gaps With Multimodal Infillings", "abstract": "<p>Recent advances in large language models elicit reasoning in a chain-of-thought that allows models to decompose problems in a human-like fashion. Though this paradigm improves multi-step reasoning ability in language models it is limited by being unimodal and applied mainly to question-answering tasks. We claim that incorporating visual augmentation into reasoning is essential especially for complex imaginative tasks. Consequently we introduce VCoT a novel method that leverages chain-of-thought prompting with vision-language grounding to recursively bridge the logical gaps within sequential data. Our method uses visual guidance to generate synthetic multimodal infillings that add consistent and novel information to reduce the logical gaps for downstream tasks that can benefit from temporal reasoning as well as provide interpretability into models multi-step reasoning. We apply VCoT to the Visual Storytelling and WikiHow summarization datasets and demonstrate through human evaluation that VCoT offers novel and consistent synthetic data augmentation beating chain-of-thought baselines which can be used to enhance downstream performance.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [38.869483947753906, 3.578688859939575]}, {"key": "roy2019metric", "year": "2019", "title": "Metric-learning Based Deep Hashing Network For Content Based Retrieval Of Remote Sensing Images", "abstract": "<p>Hashing methods have been recently found very effective in retrieval of remote sensing (RS) images due to their computational efficiency and fast search speed. The traditional hashing methods in RS usually exploit hand-crafted features to learn hash functions to obtain binary codes which can be insufficient to optimally represent the information content of RS images. To overcome this problem in this paper we introduce a metric-learning based hashing network which learns 1) a semantic-based metric space for effective feature representation; and 2) compact binary hash codes for fast archive search. Our network considers an interplay of multiple loss functions that allows to jointly learn a metric based semantic space facilitating similar images to be clustered together in that target space and at the same time producing compact final activations that lose negligible information when binarized. Experiments carried out on two benchmark RS archives point out that the proposed network significantly improves the retrieval performance under the same retrieval time when compared to the state-of-the-art hashing methods in RS.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-15.50267505645752, 4.645805358886719]}, {"key": "rubin2021learning", "year": "2021", "title": "Learning To Retrieve Prompts For In-context Learning", "abstract": "<p>In-context learning is a recent paradigm in natural language understanding where a large pre-trained language model (LM) observes a test instance and a few training examples as its input and directly decodes the output without any update to its parameters. However performance has been shown to strongly depend on the selected training examples (termed prompt). In this work we propose an efficient method for retrieving prompts for in-context learning using annotated data and a LM. Given an input-output pair we estimate the probability of the output given the input and a candidate training example as the prompt and label training examples as positive or negative based on this probability. We then train an efficient dense retriever from this data which is used to retrieve training examples as prompts at test time. We evaluate our approach on three sequence-to-sequence tasks where language utterances are mapped to meaning representations and find that it substantially outperforms prior work and multiple baselines across the board.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [26.17844581604004, -3.2496540546417236]}, {"key": "ruoss2023randomized", "year": "2023", "title": "Randomized Positional Encodings Boost Length Generalization Of Transformers", "abstract": "<p>Transformers have impressive generalization capabilities on tasks with a fixed context length. However they fail to generalize to sequences of arbitrary length even for seemingly simple tasks such as duplicating a string. Moreover simply training on longer sequences is inefficient due to the quadratic computation complexity of the global attention mechanism. In this work we demonstrate that this failure mode is linked to positional encodings being out-of-distribution for longer sequences (even for relative encodings) and introduce a novel family of positional encodings that can overcome this problem. Concretely our randomized positional encoding scheme simulates the positions of longer sequences and randomly selects an ordered subset to fit the sequences length. Our large-scale empirical evaluation of 6000 models across 15 algorithmic reasoning tasks shows that our method allows Transformers to generalize to sequences of unseen length (increasing test accuracy by 12.037; on average).</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [16.962080001831055, -13.71578598022461]}, {"key": "ryali2020bio", "year": "2020", "title": "Bio-inspired Hashing For Unsupervised Similarity Search", "abstract": "<p>The fruit fly Drosophilas olfactory circuit has inspired a new locality sensitive hashing (LSH) algorithm FlyHash. In contrast with classical LSH algorithms that produce low dimensional hash codes FlyHash produces sparse high-dimensional hash codes and has also been shown to have superior empirical performance compared to classical LSH algorithms in similarity search. However FlyHash uses random projections and cannot learn from data. Building on inspiration from FlyHash and the ubiquity of sparse expansive representations in neurobiology our work proposes a novel hashing algorithm BioHash that produces sparse high dimensional hash codes in a data-driven manner. We show that BioHash outperforms previously published benchmarks for various hashing methods. Since our learning algorithm is based on a local and biologically plausible synaptic plasticity rule our work provides evidence for the proposal that LSH might be a computational reason for the abundance of sparse expansive motifs in a variety of biological systems. We also propose a convolutional variant BioConvHash that further improves performance. From the perspective of computer science BioHash and BioConvHash are fast scalable and yield compressed binary representations that are useful for similarity search.</p>\n", "tags": ["ICML", "LSH", "Unsupervised"], "tsne_embedding": [9.001322746276855, -22.179027557373047]}, {"key": "sablayrolles2016how", "year": "2016", "title": "How Should We Evaluate Supervised Hashing", "abstract": "<p>Hashing produces compact representations for documents to perform tasks like classification or retrieval based on these short codes. When hashing is supervised the codes are trained using labels on the training data. This paper first shows that the evaluation protocols used in the literature for supervised hashing are not satisfactory we show that a trivial solution that encodes the output of a classifier significantly outperforms existing supervised or semi-supervised methods while using much shorter codes. We then propose two alternative protocols for supervised hashing one based on retrieval on a disjoint set of classes and another based on transfer learning to new classes. We provide two baseline methods for image-related tasks to assess the performance of (semi-)supervised hashing without coding and with unsupervised codes. These baselines give a lower- and upper-bound on the performance of a supervised hashing scheme.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [-1.618115782737732, 6.102144718170166]}, {"key": "sackman2015perfect", "year": "2015", "title": "Perfect Consistent Hashing", "abstract": "<p>Consistent Hashing functions are widely used for load balancing across a variety of applications. However the original presentation and typical implementations of Consistent Hashing rely on randomised allocation of hash codes to keys which results in a flawed and approximately-uniform allocation of keys to hash codes. We analyse the desired properties and present an algorithm that perfectly achieves them without resorting to any random distributions. The algorithm is simple and adds to our understanding of what is necessary to create a consistent hash function.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-23.320077896118164, -19.4114990234375]}, {"key": "saha2023can", "year": "2023", "title": "Can Language Models Teach Weaker Agents Teacher Explanations Improve Students Via Personalization", "abstract": "<p>A hallmark property of explainable AI models is the ability to teach other agents communicating knowledge of how to perform a task. While Large Language Models perform complex reasoning by generating explanations for their predictions it is unclear whether they also make good teachers for weaker agents. To address this we consider a student-teacher framework between two LLM agents and study if when and how the teacher should intervene with natural language explanations to improve the students performance. Since communication is expensive we define a budget such that the teacher only communicates explanations for a fraction of the data after which the student should perform well on its own. We decompose the teaching problem along four axes (1) if teachers test time intervention improve student predictions (2) when it is worth explaining a data point (3) how the teacher should personalize explanations to better teach the student and (4) if teacher explanations also improve students on future unexplained data. We first show that teacher LLMs can indeed intervene on student reasoning to improve their performance. Next inspired by the Theory of Mind abilities of effective teachers we propose building two few-shot mental models of the student. The first model defines an Intervention Function that simulates the utility of an intervention allowing the teacher to intervene when this utility is the highest and improving student performance at lower budgets. The second model enables the teacher to personalize explanations for a particular student and outperform unpersonalized teachers. We also demonstrate that in multi-turn interactions teacher explanations generalize and learning from explained data improves student performance on future unexplained data. Finally we verify that misaligned teachers can lower student performance to random chance by intentionally misleading them.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [29.5080623626709, 11.150339126586914]}, {"key": "salakhutdinov2007semantic", "year": "2007", "title": "Semantic Hashing", "abstract": "<p>We show how to learn a deep graphical model of the word-count\nvectors obtained from a large set of documents. The values of the\nlatent variables in the deepest layer are easy to infer and give a\nmuch better representation of each document than Latent Semantic\nAnalysis. When the deepest layer is forced to use a small number of\nbinary variables (e.g. 32), the graphical model performs \u201csemantic\nhashing\u201d: Documents are mapped to memory addresses in such a\nway that semantically similar documents are located at nearby addresses.\nDocuments similar to a query document can then be found\nby simply accessing all the addresses that differ by only a few bits\nfrom the address of the query document. This way of extending the\nefficiency of hash-coding to approximate matching is much faster\nthan locality sensitive hashing, which is the fastest current method.\nBy using semantic hashing to filter the documents given to TF-IDF,\nwe achieve higher accuracy than applying TF-IDF to the entire document\nset.</p>\n", "tags": ["Deep Learning", "Has Code", "NEURIPS"], "tsne_embedding": [-17.397748947143555, -14.814305305480957]}, {"key": "salvi2016bloom", "year": "2016", "title": "Bloom Filters And Compact Hash Codes For Efficient And Distributed Image Retrieval", "abstract": "<p>This paper presents a novel method for efficient image retrieval based on a simple and effective hashing of CNN features and the use of an indexing structure based on Bloom filters. These filters are used as gatekeepers for the database of image features allowing to avoid to perform a query if the query features are not stored in the database and speeding up the query process without affecting retrieval performance. Thanks to the limited memory requirements the system is suitable for mobile applications and distributed databases associating each filter to a distributed portion of the database. Experimental validation has been performed on three standard image retrieval datasets outperforming state-of-the-art hashing methods in terms of precision while the proposed indexing method obtains a (2times) speedup.</p>\n", "tags": ["ARXIV", "CNN", "Image Retrieval", "Independent"], "tsne_embedding": [2.87186336517334, 15.336292266845703]}, {"key": "sanh2021multitask", "year": "2021", "title": "Multitask Prompted Training Enables Zero-shot Task Generalization", "abstract": "<p>Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al. 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models pretraining (Radford et al. 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning To test this question at scale we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks. We fine-tune a pretrained encoder-decoder model (Raffel et al. 2020; Lester et al. 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets often outperforming models up to 16x its size. Further our approach attains strong performance on a subset of tasks from the BIG-bench benchmark outperforming models up to 6x its size. All trained models are available at https://github.com/bigscience-workshop/t-zero and all prompts are available at https://github.com/bigscience-workshop/promptsource.</p>\n", "tags": ["ARXIV", "Has Code", "Supervised"], "tsne_embedding": [27.53720474243164, -3.487900733947754]}, {"key": "satuluri2011bayesian", "year": "2011", "title": "Bayesian Locality Sensitive Hashing For Fast Similarity Search", "abstract": "<p>Given a collection of objects and an associated similarity measure the all-pairs similarity search problem asks us to find all pairs of objects with similarity greater than a certain user-specified threshold. Locality-sensitive hashing (LSH) based methods have become a very popular approach for this problem. However most such methods only use LSH for the first phase of similarity search - i.e. efficient indexing for candidate generation. In this paper we present BayesLSH a principled Bayesian algorithm for the subsequent phase of similarity search - performing candidate pruning and similarity estimation using LSH. A simpler variant BayesLSH-Lite which calculates similarities exactly is also presented. BayesLSH is able to quickly prune away a large majority of the false positive candidate pairs leading to significant speedups over baseline approaches. For BayesLSH we also provide probabilistic guarantees on the quality of the output both in terms of accuracy and recall. Finally the quality of BayesLSHs output can be easily tuned and does not require any manual setting of the number of hashes to use for similarity estimation unlike standard approaches. For two state-of-the-art candidate generation algorithms AllPairs and LSH BayesLSH enables significant speedups typically in the range 2x-20x for a wide variety of datasets.</p>\n", "tags": ["Independent", "LSH"], "tsne_embedding": [-21.441247940063477, 0.08206402510404587]}, {"key": "schaeffer2023are", "year": "2023", "title": "Are Emergent Abilities Of Large Language Models A Mirage", "abstract": "<p>Recent work claims that large language models display emergent abilities abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold their sharpness transitioning seemingly instantaneously from not present to present and their unpredictability appearing at seemingly unforeseeable model scales. Here we present an alternative explanation for emergent abilities that for a particular task and model family when analyzing fixed model outputs emergent abilities appear due to the researchers choice of metric rather than due to fundamental changes in model behavior with scale. Specifically nonlinear or discontinuous metrics produce apparent emergent abilities whereas linear or continuous metrics produce smooth continuous predictable changes in model performance. We present our alternative explanation in a simple mathematical model then test it in three complementary ways we (1) make test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities; (2) make test and confirm two predictions about metric choices in a meta-analysis of emergent abilities on BIG-Bench; and (3) show to choose metrics to produce never-before-seen seemingly emergent abilities in multiple vision tasks across diverse deep networks. Via all three analyses we provide evidence that alleged emergent abilities evaporate with different metrics or with better statistics and may not be a fundamental property of scaling AI models.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [28.106555938720703, 4.319392681121826]}, {"key": "schlegel2018adding", "year": "2018", "title": "Adding Cues To Binary Feature Descriptors For Visual Place Recognition", "abstract": "<p>In this paper we propose an approach to embed continuous and selector cues in binary feature descriptors used for visual place recognition. The embedding is achieved by extending each feature descriptor with a binary string that encodes a cue and supports the Hamming distance metric. Augmenting the descriptors in such a way has the advantage of being transparent to the procedure used to compare them. We present two concrete applications of our methodology demonstrating the two considered types of cues. In addition to that we conducted on these applications a broad quantitative and comparative evaluation covering five benchmark datasets and several state-of-the-art image retrieval approaches in combination with various binary descriptor types.</p>\n", "tags": ["ARXIV", "Image Retrieval"], "tsne_embedding": [-13.021415710449219, 20.122634887695312]}, {"key": "schlemper2019deep", "year": "2019", "title": "Deep Hashing Using Entropy Regularised Product Quantisation Network", "abstract": "<p>In large scale systems approximate nearest neighbour search is a crucial algorithm to enable efficient data retrievals. Recently deep learning-based hashing algorithms have been proposed as a promising paradigm to enable data dependent schemes. Often their efficacy is only demonstrated on data sets with fixed limited numbers of classes. In practical scenarios those labels are not always available or one requires a method that can handle a higher input variability as well as a higher granularity. To fulfil those requirements we look at more flexible similarity measures. In this work we present a novel flexible end-to-end trainable network for large-scale data hashing. Our method works by transforming the data distribution to behave as a uniform distribution on a product of spheres. The transformed data is subsequently hashed to a binary form in a way that maximises entropy of the output (i.e. to fully utilise the available bit-rate capacity) while maintaining the correctness (i.e. close items hash to the same key in the map). We show that the method outperforms baseline approaches such as locality-sensitive hashing and product quantisation in the limited capacity regime.</p>\n", "tags": ["ARXIV", "Deep Learning", "Independent", "Quantisation"], "tsne_embedding": [-26.958099365234375, -0.5712921619415283]}, {"key": "schwengber2023deep", "year": "2023", "title": "Deep Hashing Via Householder Quantization", "abstract": "<p>Hashing is at the heart of large-scale image similarity search and recent methods have been substantially improved through deep learning techniques. Such algorithms typically learn continuous embeddings of the data. To avoid a subsequent costly binarization step a common solution is to employ loss functions that combine a similarity learning term (to ensure similar images are grouped to nearby embeddings) and a quantization penalty term (to ensure that the embedding entries are close to binarized entries e.g. -1 or 1). Still the interaction between these two terms can make learning harder and the embeddings worse. We propose an alternative quantization strategy that decomposes the learning problem in two stages first perform similarity learning over the embedding space with no quantization; second find an optimal orthogonal transformation of the embeddings so each coordinate of the embedding is close to its sign and then quantize the transformed embedding through the sign function. In the second step we parametrize orthogonal transformations using Householder matrices to efficiently leverage stochastic gradient descent. Since similarity measures are usually invariant under orthogonal transformations this quantization strategy comes at no cost in terms of performance. The resulting algorithm is unsupervised fast hyperparameter-free and can be run on top of any existing deep hashing or metric learning algorithm. We provide extensive experimental results showing that this approach leads to state-of-the-art performance on widely used image datasets and unlike other quantization strategies brings consistent improvements in performance to existing deep hashing algorithms.</p>\n", "tags": ["ARXIV", "Deep Learning", "Quantisation", "Unsupervised"], "tsne_embedding": [-8.725337028503418, 3.2478859424591064]}, {"key": "shand2020locality", "year": "2020", "title": "Locality-sensitive Hashing In Function Spaces", "abstract": "<p>We discuss the problem of performing similarity search over function spaces. To perform search over such spaces in a reasonable amount of time we use it locality-sensitive hashing (LSH). We present two methods that allow LSH functions on (^N) to be extended to (L^p) spaces one using function approximation in an orthonormal basis and another using (quasi-)Monte Carlo-style techniques. We use the presented hashing schemes to construct an LSH family for Wasserstein distance over one-dimensional continuous probability distributions.</p>\n", "tags": ["ARXIV", "Independent", "LSH"], "tsne_embedding": [-33.56474685668945, 5.511063098907471]}, {"key": "shao2018h", "year": "2018", "title": "H-CNN Spatial Hashing Based CNN For 3D Shape Analysis", "abstract": "<p>We present a novel spatial hashing based data structure to facilitate 3D shape analysis using convolutional neural networks (CNNs). Our method well utilizes the sparse occupancy of 3D shape boundary and builds hierarchical hash tables for an input model under different resolutions. Based on this data structure we design two efficient GPU algorithms namely hash2col and col2hash so that the CNN operations like convolution and pooling can be efficiently parallelized. The spatial hashing is nearly minimal and our data structure is almost of the same size as the raw input. Compared with state-of-the-art octree-based methods our data structure significantly reduces the memory footprint during the CNN training. As the input geometry features are more compactly packed CNN operations also run faster with our data structure. The experiment shows that under the same network structure our method yields comparable or better benchmarks compared to the state-of-the-art while it has only one-third memory consumption. Such superior memory performance allows the CNN to handle high-resolution shape analysis.</p>\n", "tags": ["ARXIV", "CNN", "Supervised"], "tsne_embedding": [3.724365472793579, 14.784565925598145]}, {"key": "shao2022johnson", "year": "2022", "title": "Johnson-lindenstrauss Embeddings For Noisy Vectors -- Taking Advantage Of The Noise", "abstract": "<p>This paper investigates theoretical properties of subsampling and hashing as tools for approximate Euclidean norm-preserving embeddings for vectors with (unknown) additive Gaussian noises. Such embeddings are sometimes called Johnson-lindenstrauss embeddings due to their celebrated lemma. Previous work shows that as sparse embeddings the success of subsampling and hashing closely depends on the (l_infty) to (l_2) ratios of the vector to be mapped. This paper shows that the presence of noise removes such constrain in high-dimensions in other words sparse embeddings such as subsampling and hashing with comparable embedding dimensions to dense embeddings have similar approximate norm-preserving dimensionality-reduction properties. The key is that the noise should be treated as an information to be exploited not simply something to be removed. Theoretical bounds for subsampling and hashing to recover the approximate norm of a high dimension vector in the presence of noise are derived with numerical illustrations showing better performances are achieved in the presence of noise.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-35.77111053466797, 10.158600807189941]}, {"key": "shao2024visual", "year": "2024", "title": "Visual Cot Advancing Multi-modal Language Models With A Comprehensive Dataset And Benchmark For Chain-of-thought Reasoning", "abstract": "<p>Multi-Modal Large Language Models (MLLMs) have demonstrated impressive performance in various VQA tasks. However they often lack interpretability and struggle with complex visual inputs especially when the resolution of the input image is high or when the interested region that could provide key information for answering the question is small. To address these challenges we collect and introduce the large-scale Visual CoT dataset comprising 438k question-answer pairs annotated with intermediate bounding boxes highlighting key regions essential for answering the questions. Additionally about 98k pairs of them are annotated with detailed reasoning steps. Importantly we propose a multi-turn processing pipeline that dynamically focuses on visual inputs and provides interpretable thoughts. We also introduce the related benchmark to evaluate the MLLMs in scenarios requiring specific local region identification. Extensive experiments demonstrate the effectiveness of our framework and shed light on better inference strategies. The Visual CoT dataset benchmark and pre-trained models are released to foster further research in this direction.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [40.34467315673828, 4.920467853546143]}, {"key": "shapira2023clever", "year": "2023", "title": "Clever Hans Or Neural Theory Of Mind Stress Testing Social Reasoning In Large Language Models", "abstract": "<p>The escalating debate on AIs capabilities warrants developing reliable metrics to assess machine intelligence. Recently many anecdotal examples were used to suggest that newer large language models (LLMs) like ChatGPT and GPT-4 exhibit Neural Theory-of-Mind (N-ToM); however prior work reached conflicting conclusions regarding those abilities. We investigate the extent of LLMs N-ToM through an extensive evaluation on 6 tasks and find that while LLMs exhibit certain N-ToM abilities this behavior is far from being robust. We further examine the factors impacting performance on N-ToM tasks and discover that LLMs struggle with adversarial examples indicating reliance on shallow heuristics rather than robust ToM abilities. We caution against drawing conclusions from anecdotal examples limited benchmark testing and using human-designed psychological tests to evaluate models.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [40.930091857910156, 1.0877115726470947]}, {"key": "sharma2018improving", "year": "2018", "title": "Improving Similarity Search With High-dimensional Locality-sensitive Hashing", "abstract": "<p>We propose a new class of data-independent locality-sensitive hashing (LSH) algorithms based on the fruit fly olfactory circuit. The fundamental difference of this approach is that instead of assigning hashes as dense points in a low dimensional space hashes are assigned in a high dimensional space which enhances their separability. We show theoretically and empirically that this new family of hash functions is locality-sensitive and preserves rank similarity for inputs in any p space. We then analyze different variations on this strategy and show empirically that they outperform existing LSH methods for nearest-neighbors search on six benchmark datasets. Finally we propose a multi-probe version of our algorithm that achieves higher performance for the same query time or conversely that maintains performance of prior approaches while taking significantly less indexing time and memory. Overall our approach leverages the advantages of separability provided by high-dimensional spaces while still remaining computationally efficient</p>\n", "tags": ["ARXIV", "Independent", "LSH"], "tsne_embedding": [-29.52533721923828, -1.8428422212600708]}, {"key": "shen2013inductive", "year": "2013", "title": "Inductive Hashing On Manifolds", "abstract": "<p>Learning based hashing methods have attracted considerable attention due to their ability to greatly increase the scale at which existing algorithms may operate. Most of these methods are designed to generate binary codes that preserve the Euclidean distance in the original space. Manifold learning techniques in contrast are better able to model the intrinsic structure embedded in the original high-dimensional data. The complexity of these models and the problems with out-of-sample data have previously rendered them unsuitable for application to large-scale embedding however. In this work we consider how to learn compact binary embeddings on their intrinsic manifolds. In order to address the above-mentioned difficulties we describe an efficient inductive solution to the out-of-sample data problem and a process by which non-parametric manifold learning may be used as the basis of a hashing method. Our proposed approach thus allows the development of a range of new hashing techniques exploiting the flexibility of the wide variety of manifold learning approaches available. We particularly show that hashing on the basis of t-SNE .</p>\n", "tags": ["ARXIV", "Unsupervised"], "tsne_embedding": [-3.9372925758361816, -5.844791889190674]}, {"key": "shen2014hashing", "year": "2014", "title": "Hashing On Nonlinear Manifolds", "abstract": "<p>Learning based hashing methods have attracted considerable attention due to their ability to greatly increase the scale at which existing algorithms may operate. Most of these methods are designed to generate binary codes preserving the Euclidean similarity in the original space. Manifold learning techniques in contrast are better able to model the intrinsic structure embedded in the original high-dimensional data. The complexities of these models and the problems with out-of-sample data have previously rendered them unsuitable for application to large-scale embedding however. In this work how to learn compact binary embeddings on their intrinsic manifolds is considered. In order to address the above-mentioned difficulties an efficient inductive solution to the out-of-sample data problem and a process by which non-parametric manifold learning may be used as the basis of a hashing method is proposed. The proposed approach thus allows the development of a range of new hashing techniques exploiting the flexibility of the wide variety of manifold learning approaches available. It is particularly shown that hashing on the basis of t-SNE outperforms state-of-the-art hashing methods on large-scale benchmark datasets and is very effective for image classification with very short code lengths. The proposed hashing framework is shown to be easily improved for example by minimizing the quantization error with learned orthogonal rotations. In addition a supervised inductive manifold hashing framework is developed by incorporating the label information which is shown to greatly advance the semantic retrieval performance.</p>\n", "tags": ["ARXIV", "Quantisation", "Supervised"], "tsne_embedding": [-4.10593318939209, -5.626586437225342]}, {"key": "shen2018nash", "year": "2018", "title": "NASH: Toward End-to-End Neural Architecture for Generative Semantic Hashing", "abstract": "<p>Semantic hashing has become a powerful paradigm for fast similarity search\nin many information retrieval systems.\nWhile fairly successful, previous techniques generally require two-stage training, and the binary constraints are handled ad-hoc. In this paper, we present\nan end-to-end Neural Architecture for Semantic Hashing (NASH), where the binary\nhashing codes are treated as Bernoulli latent variables. A neural variational inference framework is proposed for training, where gradients are directly backpropagated through the discrete latent\nvariable to optimize the hash function.\nWe also draw connections between proposed method and rate-distortion theory, which provides a theoretical foundation for the effectiveness of the proposed framework. Experimental results on\nthree public datasets demonstrate that our\nmethod significantly outperforms several\nstate-of-the-art models on both unsupervised and supervised scenarios.</p>\n", "tags": ["ACL", "Deep Learning", "Supervised"], "tsne_embedding": [-6.174363136291504, -12.49609088897705]}, {"key": "shen2018unsupervised", "year": "2018", "title": "Unsupervised Deep Hashing with Similarity-Adaptive and Discrete Optimization", "abstract": "<p>Recent vision and learning studies show that learning compact hash codes can facilitate massive data processing\nwith significantly reduced storage and computation. Particularly, learning deep hash functions has greatly improved the retrieval\nperformance, typically under the semantic supervision. In contrast, current unsupervised deep hashing algorithms can hardly achieve\nsatisfactory performance due to either the relaxed optimization or absence of similarity-sensitive objective. In this work, we propose a\nsimple yet effective unsupervised hashing framework, named Similarity-Adaptive Deep Hashing (SADH), which alternatingly proceeds\nover three training modules: deep hash model training, similarity graph updating and binary code optimization. The key difference from\nthe widely-used two-step hashing method is that the output representations of the learned deep model help update the similarity graph\nmatrix, which is then used to improve the subsequent code optimization. In addition, for producing high-quality binary codes, we devise\nan effective discrete optimization algorithm which can directly handle the binary constraints with a general hashing loss. Extensive\nexperiments validate the efficacy of SADH, which consistently outperforms the state-of-the-arts by large gaps.</p>\n", "tags": ["Deep Learning", "Has Code", "Image Retrieval", "Supervised", "TPAMI"], "tsne_embedding": [-1.6220998764038086, -3.9943127632141113]}, {"key": "shen2018zero", "year": "2018", "title": "Zero-shot Sketch-image Hashing", "abstract": "<p>Recent studies show that large-scale sketch-based image retrieval (SBIR) can be efficiently tackled by cross-modal binary representation learning methods where Hamming distance matching significantly speeds up the process of similarity search. Providing training and test data subjected to a fixed set of pre-defined categories the cutting-edge SBIR and cross-modal hashing works obtain acceptable retrieval performance. However most of the existing methods fail when the categories of query sketches have never been seen during training. In this paper the above problem is briefed as a novel but realistic zero-shot SBIR hashing task. We elaborate the challenges of this special task and accordingly propose a zero-shot sketch-image hashing (ZSIH) model. An end-to-end three-network architecture is built two of which are treated as the binary encoders. The third network mitigates the sketch-image heterogeneity and enhances the semantic relations among data by utilizing the Kronecker fusion layer and graph convolution respectively. As an important part of ZSIH we formulate a generative hashing scheme in reconstructing semantic knowledge representations for zero-shot retrieval. To the best of our knowledge ZSIH is the first zero-shot hashing work suitable for SBIR and cross-modal search. Comprehensive experiments are conducted on two extended datasets i.e. Sketchy and TU-Berlin with a novel zero-shot train-test split. The proposed model remarkably outperforms related works.</p>\n", "tags": ["ARXIV", "Cross Modal", "Graph", "Image Retrieval"], "tsne_embedding": [-4.032543659210205, -1.2087275981903076]}, {"key": "shen2019embarass", "year": "2019", "title": "Embarrassingly Simple Binary Representation Learning", "abstract": "<p>Recent binary representation learning models usually require sophisticated binary optimization, similarity measure or even generative models as auxiliaries. However, one may wonder whether these non-trivial components are needed to formulate practical and effective hashing models. In this paper, we answer the above question by proposing an embarrassingly simple approach to binary representation learning. With a simple classification objective, our model only incorporates two additional fully-connected layers onto the top of an arbitrary backbone network, whilst complying with the binary constraints during training. The proposed model lower-bounds the Information Bottleneck (IB) between data samples and their semantics, and can be related to many recent `learning to hash\u2019 paradigms. We show that, when properly designed, even such a simple network can generate effective binary codes, by fully exploring data semantics without any held-out alternating updating steps or auxiliary models. Experiments are conducted on conventional large-scale benchmarks, i.e., CIFAR-10, NUS-WIDE, and ImageNet, where the proposed simple model outperforms the state-of-the-art methods.</p>\n", "tags": ["Deep Learning", "Has Code", "ICCV"], "tsne_embedding": [-1.1537795066833496, -14.586690902709961]}, {"key": "shen2020auto", "year": "2020", "title": "Auto-Encoding Twin-Bottleneck Hashing", "abstract": "<p>Conventional unsupervised hashing methods usually take advantage of similarity graphs, which are either pre-computed in the high-dimensional space or obtained from random anchor points. On the one hand, existing methods uncouple the procedures of hash function learning and graph construction. On the other hand, graphs empirically built upon original data could introduce biased prior knowledge of data relevance, leading to sub-optimal retrieval performance. In this paper, we tackle the above problems by proposing an efficient and adaptive code-driven graph, which is updated by decoding in the context of an auto-encoder. Specifically, we introduce into our framework twin bottlenecks (i.e., latent variables) that exchange crucial information collaboratively. One bottleneck (i.e., binary codes) conveys the high-level intrinsic data structure captured by the code-driven graph to the other (i.e., continuous variables for low-level detail information), which in turn propagates the updated network feedback for the encoder to learn more discriminative binary codes. The auto-encoding learning objective literally rewards the code-driven graph to learn an optimal encoder. Moreover, the proposed model can be simply optimized by gradient descent without violating the binary constraints. Experiments on benchmarked datasets clearly show the superiority of our framework over the state-of-the-art hashing methods.</p>\n", "tags": ["CVPR", "Deep Learning", "Has Code", "Supervised"], "tsne_embedding": [-0.37623199820518494, 25.53240966796875]}, {"key": "shen2021re", "year": "2021", "title": "Re-ranking For Image Retrieval And Transductive Few-shot Classification", "abstract": "<p>In the problems of image retrieval and few-shot classification the mainstream approaches focus on learning a better feature representation. However directly tackling the distance or similarity measure between images could also be efficient. To this end we revisit the idea of re-ranking the top-k retrieved images in the context of image retrieval (e.g. the k-reciprocal nearest neighbors) and generalize this idea to transductive few-shot learning. We propose to meta-learn the re-ranking updates such that the similarity graph converges towards the target similarity graph induced by the image labels. Specifically the re-ranking module takes as input an initial similarity graph between the query image and the contextual images using a pre-trained feature extractor and predicts an improved similarity graph by leveraging the structure among the involved images. We show that our re-ranking approach can be applied to unseen images and can further boost existing approaches for both image retrieval and few-shot learning problems. Our approach operates either independently or in conjunction with classical re-ranking approaches yielding clear and consistent improvements on image retrieval (CUB Cars SOP rOxford5K and rParis6K) and transductive few-shot classification (Mini-ImageNet tiered-ImageNet and CIFAR-FS) benchmarks. Our code is available at https://imagine.enpc.fr/~shenx/SSR/.</p>\n", "tags": ["Graph", "Has Code", "Image Retrieval", "NEURIPS", "Supervised"], "tsne_embedding": [-11.078892707824707, 18.366655349731445]}, {"key": "shen2022semicon", "year": "2022", "title": "SEMICON A Learning-to-hash Solution For Large-scale Fine-grained Image Retrieval", "abstract": "<p>In this paper we propose Suppression-Enhancing Mask based attention and Interactive Channel transformatiON (SEMICON) to learn binary hash codes for dealing with large-scale fine-grained image retrieval tasks. In SEMICON we first develop a suppression-enhancing mask (SEM) based attention to dynamically localize discriminative image regions. More importantly different from existing attention mechanism simply erasing previous discriminative regions our SEM is developed to restrain such regions and then discover other complementary regions by considering the relation between activated regions in a stage-by-stage fashion. In each stage the interactive channel transformation (ICON) module is afterwards designed to exploit correlations across channels of attended activation tensors. Since channels could generally correspond to the parts of fine-grained objects the part correlation can be also modeled accordingly which further improves fine-grained retrieval accuracy. Moreover to be computational economy ICON is realized by an efficient two-step process. Finally the hash learning of our SEMICON consists of both global- and local-level branches for better representing fine-grained objects and then generating binary hash codes explicitly corresponding to multiple levels. Experiments on five benchmark fine-grained datasets show our superiority over competing methods.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Independent"], "tsne_embedding": [-9.307635307312012, 16.69432258605957]}, {"key": "shen2023hugginggpt", "year": "2023", "title": "Hugginggpt Solving AI Tasks With Chatgpt And Its Friends In Hugging Face", "abstract": "<p>Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding generation interaction and reasoning we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks with language serving as a generic interface to empower this. Based on this philosophy we present HuggingGPT an LLM-powered agent that leverages LLMs (e.g. ChatGPT) to connect various AI models in machine learning communities (e.g. Hugging Face) to solve AI tasks. Specifically we use ChatGPT to conduct task planning when receiving a user request select models according to their function descriptions available in Hugging Face execute each subtask with the selected AI model and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language vision speech and other challenging tasks which paves a new way towards the realization of artificial general intelligence.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [38.680362701416016, -1.4738969802856445]}, {"key": "shi2016functional", "year": "2016", "title": "Functional Hashing For Compressing Neural Networks", "abstract": "<p>As the complexity of deep neural networks (DNNs) trend to grow to absorb the increasing sizes of data memory and energy consumption has been receiving more and more attentions for industrial applications especially on mobile devices. This paper presents a novel structure based on functional hashing to compress DNNs namely FunHashNN. For each entry in a deep net FunHashNN uses multiple low-cost hash functions to fetch values in the compression space and then employs a small reconstruction network to recover that entry. The reconstruction network is plugged into the whole network and trained jointly. FunHashNN includes the recently proposed HashedNets as a degenerated case and benefits from larger value capacity and less reconstruction loss. We further discuss extensions with dual space hashing and multi-hops. On several benchmark datasets FunHashNN demonstrates high compression ratios with little loss on prediction accuracy.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [7.8555402755737305, -10.24152660369873]}, {"key": "shi2018scalable", "year": "2018", "title": "A Scalable Optimization Mechanism For Pairwise Based Discrete Hashing", "abstract": "<p>Maintaining the pair similarity relationship among originally high-dimensional data into a low-dimensional binary space is a popular strategy to learn binary codes. One simiple and intutive method is to utilize two identical code matrices produced by hash functions to approximate a pairwise real label matrix. However the resulting quartic problem is difficult to directly solve due to the non-convex and non-smooth nature of the objective. In this paper unlike previous optimization methods using various relaxation strategies we aim to directly solve the original quartic problem using a novel alternative optimization mechanism to linearize the quartic problem by introducing a linear regression model. Additionally we find that gradually learning each batch of binary codes in a sequential mode i.e. batch by batch is greatly beneficial to the convergence of binary code learning. Based on this significant discovery and the proposed strategy we introduce a scalable symmetric discrete hashing algorithm that gradually and smoothly updates each batch of binary codes. To further improve the smoothness we also propose a greedy symmetric discrete hashing algorithm to update each bit of batch binary codes. Moreover we extend the proposed optimization mechanism to solve the non-convex optimization problems for binary code learning in many other pairwise based hashing algorithms. Extensive experiments on benchmark single-label and multi-label databases demonstrate the superior performance of the proposed mechanism over recent state-of-the-art methods.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [2.074848175048828, -10.345908164978027]}, {"key": "shi2019variable", "year": "2019", "title": "Variable-Length Quantization Strategy for Hashing", "abstract": "<p>Hashing is widely used to solve fast Approximate Nearest Neighbor (ANN) search problems, involves converting the original real-valued samples to binary-valued representations. The conventional quantization strategies, such as Single-Bit Quantization and Multi-Bit quantization, are considered ineffective, because of their serious information loss. To address this issue, we propose a novel variable-length quantization (VLQ) strategy for hashing. In the proposed VLQ technique, we divide all samples into different regions in each dimension firstly given the real-valued features of samples. Then we compute the dispersion degrees of these regions. Subsequently, we attempt to optimally assign different number of bits to each dimensions to obtain the minimum dispersion degree. Our experiments show that the VLQ strategy achieves not only superior performance over the state-of-the-art methods, but also has a faster retrieval speed on public datasets.</p>\n", "tags": ["ICIP", "Quantisation"], "tsne_embedding": [-23.42508888244629, -6.599162578582764]}, {"key": "shi2022deep", "year": "2022", "title": "Deep Manifold Hashing A Divide-and-conquer Approach For Semi-paired Unsupervised Cross-modal Retrieval", "abstract": "<p>Hashing that projects data into binary codes has shown extraordinary talents in cross-modal retrieval due to its low storage usage and high query speed. Despite their empirical success on some scenarios existing cross-modal hashing methods usually fail to cross modality gap when fully-paired data with plenty of labeled information is nonexistent. To circumvent this drawback motivated by the Divide-and-Conquer strategy we propose Deep Manifold Hashing (DMH) a novel method of dividing the problem of semi-paired unsupervised cross-modal retrieval into three sub-problems and building one simple yet efficiency model for each sub-problem. Specifically the first model is constructed for obtaining modality-invariant features by complementing semi-paired data based on manifold learning whereas the second model and the third model aim to learn hash codes and hash functions respectively. Extensive experiments on three benchmarks demonstrate the superiority of our DMH compared with the state-of-the-art fully-paired and semi-paired unsupervised cross-modal hashing methods.</p>\n", "tags": ["ARXIV", "Cross Modal", "Unsupervised"], "tsne_embedding": [-6.208134651184082, -10.100393295288086]}, {"key": "shi2022efficient", "year": "2022", "title": "Efficient Cross-modal Retrieval Via Deep Binary Hashing And Quantization", "abstract": "<p>Cross-modal retrieval aims to search for data with similar semantic meanings across different content modalities. However cross-modal retrieval requires huge amounts of storage and retrieval time since it needs to process data in multiple modalities. Existing works focused on learning single-source compact features such as binary hash codes that preserve similarities between different modalities. In this work we propose a jointly learned deep hashing and quantization network (HQ) for cross-modal retrieval. We simultaneously learn binary hash codes and quantization codes to preserve semantic information in multiple modalities by an end-to-end deep learning architecture. At the retrieval step binary hashing is used to retrieve a subset of items from the search space then quantization is used to re-rank the retrieved items. We theoretically and empirically show that this two-stage retrieval approach provides faster retrieval results while preserving accuracy. Experimental results on the NUS-WIDE MIR-Flickr and Amazon datasets demonstrate that HQ achieves boosts of more than 737; in precision compared to supervised neural network-based compact coding models.</p>\n", "tags": ["BMVC", "Cross Modal", "Deep Learning", "Quantisation", "Supervised"], "tsne_embedding": [-12.780815124511719, 4.028364181518555]}, {"key": "shi2022language", "year": "2022", "title": "Language Models Are Multilingual Chain-of-thought Reasoners", "abstract": "<p>We evaluate the reasoning abilities of large language models in multilingual settings. We introduce the Multilingual Grade School Math (MGSM) benchmark by manually translating 250 grade-school math problems from the GSM8K dataset (Cobbe et al. 2021) into ten typologically diverse languages. We find that the ability to solve MGSM problems via chain-of-thought prompting emerges with increasing model scale and that models have strikingly strong multilingual reasoning abilities even in underrepresented languages such as Bengali and Swahili. Finally we show that the multilingual reasoning abilities of language models extend to other tasks such as commonsense reasoning and word-in-context semantic judgment. The MGSM benchmark is publicly available at https://github.com/google-research/url-nlp.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [31.6215877532959, -4.906789779663086]}, {"key": "shi2023large", "year": "2023", "title": "Large Language Models Can Be Easily Distracted By Irrelevant Context", "abstract": "<p>Large language models have achieved impressive performance on various natural language processing tasks. However so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work we investigate the distractibility of large language models i.e. how the model problem-solving accuracy can be influenced by irrelevant context. In particular we introduce Grade-School Math with Irrelevant Context (GSM-IC) an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [26.10472869873047, -0.047176726162433624]}, {"key": "shi2023pretraining", "year": "2023", "title": "In-context Pretraining Language Modeling Beyond Document Boundaries", "abstract": "<p>Large language models (LMs) are currently trained to predict tokens given document prefixes enabling them to directly perform long-form generation and prompting-style tasks which can be reduced to document completion. Existing pretraining pipelines train LMs by concatenating random sets of short documents to create input contexts but the prior documents provide no signal for predicting the next document. We instead present In-Context Pretraining a new approach where language models are pretrained on a sequence of related documents thereby explicitly encouraging them to read and reason across document boundaries. We can do In-Context Pretraining by simply changing the document ordering so that each context contains related documents and directly applying existing pretraining pipelines. However this document sorting problem is challenging. There are billions of documents and we would like the sort to maximize contextual similarity for every document without repeating any data. To do this we introduce approximate algorithms for finding related documents with efficient nearest neighbor search and constructing coherent input contexts with a graph traversal algorithm. Our experiments show In-Context Pretraining offers a simple and scalable approach to significantly enhance LMsperformance we see notable improvements in tasks that require more complex contextual reasoning including in-context learning (+837;) reading comprehension (+1537;) faithfulness to previous contexts (+1637;) long-context reasoning (+537;) and retrieval augmentation (+937;).</p>\n", "tags": ["ARXIV", "Graph", "Independent"], "tsne_embedding": [12.413902282714844, -3.761871576309204]}, {"key": "shi2023replug", "year": "2023", "title": "REPLUG Retrieval-augmented Black-box Language Models", "abstract": "<p>We introduce REPLUG a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross attention mechanisms to encode the retrieved text REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing retrieval and language models. Furthermore we show that the LM can be used to supervise the retrieval model which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.337; as well as the performance of Codex on five-shot MMLU by 5.137;.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [15.757511138916016, 7.524494647979736]}, {"key": "shin2022effect", "year": "2022", "title": "On The Effect Of Pretraining Corpora On In-context Learning By A Large-scale Language Model", "abstract": "<p>Many recent studies on large-scale language models have reported successful in-context zero- and few-shot learning ability. However the in-depth analysis of when in-context learning occurs is still lacking. For example it is unknown how in-context learning performance changes as the training corpus varies. Here we investigate the effects of the source and size of the pretraining corpus on in-context learning in HyperCLOVA a Korean-centric GPT-3 model. From our in-depth investigation we introduce the following observations (1) in-context learning performance heavily depends on the corpus domain source and the size of the pretraining corpus does not necessarily determine the emergence of in-context learning (2) in-context learning ability can emerge when a language model is trained on a combination of multiple corpora even when each corpus does not result in in-context learning on its own (3) pretraining with a corpus related to a downstream task does not always guarantee the competitive in-context learning performance of the downstream task especially in the few-shot setting and (4) the relationship between language modeling (measured in perplexity) and in-context learning does not always correlate e.g. low perplexity does not always imply high in-context few-shot learning performance.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [23.66128921508789, -4.355504989624023]}, {"key": "shinde2010similarity", "year": "2010", "title": "Similarity Search And Locality Sensitive Hashing Using Tcams", "abstract": "<p>Similarity search methods are widely used as kernels in various machine learning applications. Nearest neighbor search (NNS) algorithms are often used to retrieve similar entries given a query. While there exist efficient techniques for exact query lookup using hashing similarity search using exact nearest neighbors is known to be a hard problem and in high dimensions best known solutions offer little improvement over a linear scan. Fast solutions to the approximate NNS problem include Locality Sensitive Hashing (LSH) based techniques which need storage polynomial in n with exponent greater than 1 and query time sublinear but still polynomial in n where n is the size of the database. In this work we present a new technique of solving the approximate NNS problem in Euclidean space using a Ternary Content Addressable Memory (TCAM) which needs near linear space and has O(1) query time. In fact this method also works around the best known lower bounds in the cell probe model for the query time using a data structure near linear in the size of the data base. TCAMs are high performance associative memories widely used in networking applications such as access control lists. A TCAM can query for a bit vector within a database of ternary vectors where every bit position represents 0 1 or . The is a wild card representing either a 0 or a 1. We leverage TCAMs to design a variant of LSH called Ternary Locality Sensitive Hashing (TLSH) wherein we hash database entries represented by vectors in the Euclidean space into 01. By using the added functionality of a TLSH scheme with respect to the character we solve an instance of the approximate nearest neighbor problem with 1 TCAM access and storage nearly linear in the size of the database. We believe that this work can open new avenues in very high speed data mining.</p>\n", "tags": ["ARXIV", "Independent", "LSH"], "tsne_embedding": [-19.921459197998047, -6.091454982757568]}, {"key": "shinn2023reflexion", "year": "2023", "title": "Reflexion Language Agents With Verbal Reinforcement Learning", "abstract": "<p>Large language models (LLMs) have been increasingly used to interact with external environments (e.g. games compilers APIs) as goal-driven agents. However it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion a novel framework to reinforce language agents not by updating weights but instead through linguistic feedback. Concretely Reflexion agents verbally reflect on task feedback signals then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making coding language reasoning). For example Reflexion achieves a 9137; pass@1 accuracy on the HumanEval coding benchmark surpassing the previous state-of-the-art GPT-4 that achieves 8037;. We also conduct ablation and analysis studies using different feedback signals feedback incorporation methods and agent types and provide insights into how they affect performance.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [33.18401336669922, -8.864481925964355]}, {"key": "shoeybi2019megatron", "year": "2019", "title": "Megatron-lm Training Multi-billion Parameter Language Models Using Model Parallelism", "abstract": "<p>Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However very large models can be quite difficult to train due to memory constraints. In this work we present our techniques for training very large transformer models and implement a simple efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes is orthogonal and complimentary to pipeline model parallelism and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 7637; scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs which is 3037; of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA) we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.537; compared to SOTA accuracy of 63.237;) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.937; compared to SOTA accuracy of 89.437;).</p>\n", "tags": ["ARXIV"], "tsne_embedding": [18.384105682373047, -10.826261520385742]}, {"key": "shrivastava2013beyond", "year": "2013", "title": "Beyond Pairwise Provably Fast Algorithms For Approximate (k)-way Similarity Search", "abstract": "<p>We go beyond the notion of pairwise similarity and look into search problems with (k)-way similarity functions. In this paper we focus on problems related to emph3-way Jaccard similarity (^3way= S_1 cup S_2 cup S_3) (S_1 S_2 S_3 in ) where () is a size (n) collection of sets (or binary vectors). We show that approximate (^3way) similarity search problems admit fast algorithms with provable guarantees analogous to the pairwise case. Our analysis and speedup guarantees naturally extend to (k)-way resemblance. In the process we extend traditional framework of emphlocality sensitive hashing (LSH) to handle higher order similarities which could be of independent theoretical interest. The applicability of (^3way) search is shown on the Google sets application. In addition we demonstrate the advantage of (^3way) resemblance over the pairwise case in improving retrieval quality.</p>\n", "tags": ["Independent", "LSH", "NEURIPS"], "tsne_embedding": [-16.509788513183594, -0.5163165926933289]}, {"key": "shrivastava2014asymmetric", "year": "2014", "title": "Asymmetric LSH (ALSH) For Sublinear Time Maximum Inner Product Search (MIPS)", "abstract": "<p>We present the first provably sublinear time hashing algorithm for approximate emphMaximum Inner Product Search (MIPS). Searching with (un-normalized) inner product as the underlying similarity measure is a known difficult problem and finding hashing schemes for MIPS was considered hard. While the existing Locality Sensitive Hashing (LSH) framework is insufficient for solving MIPS in this paper we extend the LSH framework to allow asymmetric hashing schemes. Our proposal is based on a key observation that the problem of finding maximum inner products after independent asymmetric transformations can be converted into the problem of approximate near neighbor search in classical settings. This key observation makes efficient sublinear hashing scheme for MIPS possible. Under the extended asymmetric LSH (ALSH) framework this paper provides an example of explicit construction of provably fast hashing scheme for MIPS. Our proposed algorithm is simple and easy to implement. The proposed hashing scheme leads to significant computational savings over the two popular conventional LSH schemes (i) Sign Random Projection (SRP) and (ii) hashing based on (p)-stable distributions for (L_2) norm (L2LSH) in the collaborative filtering task of item recommendations on Netflix and Movielens (10M) datasets.</p>\n", "tags": ["Independent", "LSH", "NEURIPS"], "tsne_embedding": [-29.138158798217773, 0.44281402230262756]}, {"key": "shrivastava2014densifying", "year": "2014", "title": "Densifying One Permutation Hashing via Rotation for Fast Near Neighbor Search", "abstract": "<p>The query complexity of locality sensitive hashing\n(LSH) based similarity search is dominated\nby the number of hash evaluations, and this number\ngrows with the data size (Indyk &amp; Motwani,\n1998). In industrial applications such as search\nwhere the data are often high-dimensional and\nbinary (e.g., text n-grams), minwise hashing is\nwidely adopted, which requires applying a large\nnumber of permutations on the data. This is\ncostly in computation and energy-consumption.\nIn this paper, we propose a hashing technique\nwhich generates all the necessary hash evaluations\nneeded for similarity search, using one\nsingle permutation. The heart of the proposed\nhash function is a \u201crotation\u201d scheme which densifies\nthe sparse sketches of one permutation\nhashing (Li et al., 2012) in an unbiased fashion\nthereby maintaining the LSH property. This\nmakes the obtained sketches suitable for hash table\nconstruction. This idea of rotation presented\nin this paper could be of independent interest for\ndensifying other types of sparse sketches.\nUsing our proposed hashing method, the query\ntime of a (K, L)-parameterized LSH is reduced\nfrom the typical O(dKL) complexity to merely\nO(KL + dL), where d is the number of nonzeros\nof the data vector, K is the number of hashes\nin each hash table, and L is the number of hash\ntables. Our experimental evaluation on real data\nconfirms that the proposed scheme significantly\nreduces the query processing time over minwise\nhashing without loss in retrieval accuracies.</p>\n", "tags": [], "tsne_embedding": [-18.435930252075195, -3.489842176437378]}, {"key": "shrivastava2014improved", "year": "2014", "title": "Improved Densification Of One Permutation Hashing", "abstract": "<p>The existing work on densification of one permutation hashing reduces the query processing cost of the ((KL))-parameterized Locality Sensitive Hashing (LSH) algorithm with minwise hashing from (O(dKL)) to merely (O(d + KL)) where (d) is the number of nonzeros of the data vector (K) is the number of hashes in each hash table and (L) is the number of hash tables. While that is a substantial improvement our analysis reveals that the existing densification scheme is sub-optimal. In particular there is no enough randomness in that procedure which affects its accuracy on very sparse datasets. In this paper we provide a new densification procedure which is provably better than the existing scheme. This improvement is more significant for very sparse datasets which are common over the web. The improved technique has the same cost of (O(d + KL)) for query processing thereby making it strictly preferable over the existing procedure. Experimental evaluations on public datasets in the task of hashing based near neighbor search support our theoretical findings.</p>\n", "tags": ["ARXIV", "Independent", "LSH"], "tsne_embedding": [-29.377765655517578, -8.905439376831055]}, {"key": "shrivastava2016exact", "year": "2016", "title": "Exact Weighted Minwise Hashing In Constant Time", "abstract": "<p>Weighted minwise hashing (WMH) is one of the fundamental subroutine required by many celebrated approximation algorithms commonly adopted in industrial practice for large scale-search and learning. The resource bottleneck of the algorithms is the computation of multiple (typically a few hundreds to thousands) independent hashes of the data. The fastest hashing algorithm is by Ioffe (cite)ProcIoffe_ICDM10 which requires one pass over the entire data vector O(d) (d is the number of non-zeros) for computing one hash. However the requirement of multiple hashes demands hundreds or thousands passes over the data. This is very costly for modern massive dataset. In this work we break this expensive barrier and show an expected constant amortized time algorithm which computes k independent and unbiased WMH in time O(k) instead of O(dk) required by Ioffes method. Moreover our proposal only needs a few bits (5 - 9 bits) of storage per hash value compared to around 64 bits required by the state-of-art-methodologies. Experimental evaluations on real datasets show that for computing 500 WMH our proposal can be 60000x faster than the Ioffes method without losing any accuracy. Our method is also around 100x faster than approximate heuristics capitalizing on the efficient densified one permutation hashing schemes (cite)ProcOneHashLSH_ICML14. Given the simplicity of our approach and its significant advantages we hope that it will replace existing implementations in practice.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-31.660228729248047, -13.839335441589355]}, {"key": "shrivastava2016simple", "year": "2016", "title": "Simple And Efficient Weighted Minwise Hashing", "abstract": "<p>Weighted minwise hashing (WMH) is one of the fundamental subroutine required by many celebrated approximation algorithms commonly adopted in industrial practice for large -scale search and learning. The resource bottleneck with WMH is the computation of multiple (typically a few hundreds to thousands) independent hashes of the data. We propose a simple rejection type sampling scheme based on a carefully designed red-green map where we show that the number of rejected sample has exactly the same distribution as weighted minwise sampling. The running time of our method for many practical datasets is an order of magnitude smaller than existing methods. Experimental evaluations on real datasets show that for computing 500 WMH our proposal can be 60000x faster than the Ioffes method without losing any accuracy. Our method is also around 100x faster than approximate heuristics capitalizing on the efficient densified one permutation hashing schemes~citeProcOneHashLSHICML14ProcShrivastavaUAI14. Given the simplicity of our approach and its significant advantages we hope that it will replace existing implementations in practice.</p>\n", "tags": ["Independent", "NEURIPS"], "tsne_embedding": [-31.55040168762207, -14.111708641052246]}, {"key": "shrivastava2017optimal", "year": "2017", "title": "Optimal Densification For Fast And Accurate Minwise Hashing", "abstract": "<p>Minwise hashing is a fundamental and one of the most successful hashing algorithm in the literature. Recent advances based on the idea of densification~(cite)ProcOneHashLSH_ICML14ProcShrivastava_UAI14 have shown that it is possible to compute k minwise hashes of a vector with d nonzeros in mere (d + k) computations a significant improvement over the classical O(dk). These advances have led to an algorithmic improvement in the query complexity of traditional indexing algorithms based on minwise hashing. Unfortunately the variance of the current densification techniques is unnecessarily high which leads to significantly poor accuracy compared to vanilla minwise hashing especially when the data is sparse. In this paper we provide a novel densification scheme which relies on carefully tailored 2-universal hashes. We show that the proposed scheme is variance-optimal and without losing the runtime efficiency it is significantly more accurate than existing densification techniques. As a result we obtain a significantly efficient hashing scheme which has the same variance and collision probability as minwise hashing. Experimental evaluations on real sparse and high-dimensional datasets validate our claims. We believe that given the significant advantages our method will replace minwise hashing implementations in practice.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-29.368432998657227, -8.951434135437012]}, {"key": "shukor2023unival", "year": "2023", "title": "Unival Unified Model For Image Video Audio And Language Tasks", "abstract": "<p>Large Language Models (LLMs) have made the ambitious quest for generalist agents significantly far from being a fantasy. A key hurdle for building such general models is the diversity and heterogeneity of tasks and modalities. A promising solution is unification allowing the support of a myriad of tasks and modalities within one unified framework. While few large models (e.g. Flamingo (Alayrac et al. 2022) trained on massive datasets can support more than two modalities current small to mid-scale unified models are still limited to 2 modalities usually image-text or video-text. The question that we ask is is it possible to build efficiently a unified model that can support all modalities To answer this we propose UnIVAL a step further towards this ambitious goal. Without relying on fancy datasets sizes or models with billions of parameters the ~ 0.25B parameter UnIVAL model goes beyond two modalities and unifies text images video and audio into a single model. Our model is efficiently pretrained on many tasks based on task balancing and multimodal curriculum learning. UnIVAL shows competitive performance to existing state-of-the-art approaches across image and video-text tasks. The feature representations learned from image and video-text modalities allows the model to achieve competitive performance when finetuned on audio-text tasks despite not being pretrained on audio. Thanks to the unified model we propose a novel study on multimodal model merging via weight interpolation of models trained on different multimodal tasks showing their benefits in particular for out-of-distribution generalization. Finally we motivate unification by showing the synergy between tasks. The model weights and code are released here https://github.com/mshukor/UnIVAL.</p>\n", "tags": ["ARXIV", "Cross Modal", "Has Code"], "tsne_embedding": [19.135021209716797, -18.614212036132812]}, {"key": "si2022prompting", "year": "2022", "title": "Prompting GPT-3 To Be Reliable", "abstract": "<p>Large language models (LLMs) show impressive abilities via few-shot prompting. Commercialized APIs such as OpenAI GPT-3 further increase their use in real-world language applications. However the crucial problem of how to improve the reliability of GPT-3 is still under-explored. While reliability is a broad and vaguely defined term we decompose reliability into four main facets that correspond to the existing framework of ML safety and are well-recognized to be important generalizability social biases calibration and factuality. Our core contribution is to establish simple and effective prompts that improve GPT-3s reliability as it 1) generalizes out-of-distribution 2) balances demographic distribution and uses natural language instructions to reduce social biases 3) calibrates output probabilities and 4) updates the LLMs factual knowledge and reasoning chains. With appropriate prompts GPT-3 is more reliable than smaller-scale supervised models on all these facets. We release all processed datasets evaluation scripts and model predictions. Our systematic empirical study not only sheds new insights on the reliability of prompting LLMs but more importantly our prompting strategies can help practitioners more reliably use LLMs like GPT-3.</p>\n", "tags": ["ARXIV", "Graph", "Supervised"], "tsne_embedding": [35.12432098388672, 1.1693180799484253]}, {"key": "sift1m2009searching", "year": "2009", "title": "Searching with quantization: approximate nearest neighbor search using short codes and distance estimators", "abstract": "<p>We propose an approximate nearest neighbor search method based\non quantization. It uses, in particular, product quantizer to produce short codes\nand corresponding distance estimators approximating the Euclidean distance\nbetween the orginal vectors. The method is advantageously used in an asymmetric\nmanner, by computing the distance between a vector and code, unlike\ncompeting techniques such as spectral hashing that only compare codes.\nOur approach approximates the Euclidean distance based on memory efficient codes and, thus, permits efficient nearest neighbor search. Experiments\nperformed on SIFT and GIST image descriptors show excellent search accuracy.\nThe method is shown to outperform two state-of-the-art approaches of the literature.\nTimings measured when searching a vector set of 2 billion vectors are\nshown to be excellent given the high accuracy of the method.</p>\n", "tags": [], "tsne_embedding": [-30.22218894958496, 9.834263801574707]}, {"key": "silavong2021deskew", "year": "2021", "title": "DeSkew-LSH based Code-to-Code Recommendation Engine", "abstract": "<p>Machine learning on source code (MLOnCode) is a popular research field that has been driven by the availability of large-scale code repositories and the development of powerful probabilistic and deep learning models for mining source code. Code-to-code recommendation is a task in MLOnCode that aims to recommend relevant, diverse and concise code snippets that usefully extend the code currently being written by a developer in their development environment (IDE). Code-to-code recommendation engines hold the promise of increasing developer productivity by reducing context switching from the IDE and increasing code-reuse. Existing code-to-code recommendation engines do not scale gracefully to large codebases, exhibiting a linear growth in query time as the code repository increases in size. In addition, existing code-to-code recommendation engines fail to account for the global statistics of code repositories in the ranking function, such as the distribution of code snippet lengths, leading to sub-optimal retrieval results. We address both of these weaknesses with \\emph{Senatus}, a new code-to-code recommendation engine. At the core of Senatus is \\emph{De-Skew} LSH a new locality sensitive hashing (LSH) algorithm that indexes the data for fast (sub-linear time) retrieval while also counteracting the skewness in the snippet length distribution using novel abstract syntax tree-based feature scoring and selection algorithms. We evaluate Senatus via automatic evaluation and with an expert developer user study and find the recommendations to be of higher quality than competing baselines, while achieving faster search. For example, on the CodeSearchNet dataset we show that Senatus improves performance by 6.7% F1 and query time 16x is faster compared to Facebook Aroma on the task of code-to-code recommendation.</p>\n", "tags": ["LSH", "Text Retrieval"], "tsne_embedding": [10.166220664978027, -8.976028442382812]}, {"key": "sileo2021zero", "year": "2021", "title": "Zero-shot Recommendation As Language Modeling", "abstract": "<p>Recommendation is the task of ranking items (e.g. movies or products) according to individual user needs. Current systems rely on collaborative filtering and content-based techniques which both require structured training data. We propose a framework for recommendation with off-the-shelf pretrained language models (LM) that only used unstructured text corpora as training data. If a user (u) liked textitMatrix and textitInception we construct a textual prompt e.g. textitMovies like Matrix Inception (&lt;m) to estimate the affinity between (u) and (m) with LM likelihood. We motivate our idea with a corpus analysis evaluate several prompt structures and we compare LM-based recommendation with standard matrix factorization trained on different data regimes. The code for our experiments is publicly available (https://colab.research.google.com/drive/1f1mlZ-FGaLGdo5rPzxf3vemKllbh2esT?usp=sharing).</p>\n", "tags": ["ARXIV", "Has Code", "Supervised"], "tsne_embedding": [43.84751510620117, -9.964323997497559]}, {"key": "singh2019adversarially", "year": "2019", "title": "Adversarially Trained Deep Neural Semantic Hashing Scheme For Subjective Search In Fashion Inventory", "abstract": "<p>The simple approach of retrieving a closest match of a query image from one in the gallery compares an image pair using sum of absolute difference in pixel or feature space. The process is computationally expensive ill-posed to illumination background composition pose variation as well as inefficient to be deployed on gallery sets with more than 1000 elements. Hashing is a faster alternative which involves representing images in reduced dimensional simple feature spaces. Encoding images into binary hash codes enables similarity comparison in an image-pair using the Hamming distance measure. The challenge however lies in encoding the images using a semantic hashing scheme that lets subjective neighbors lie within the tolerable Hamming radius. This work presents a solution employing adversarial learning of a deep neural semantic hashing network for fashion inventory retrieval. It consists of a feature extracting convolutional neural network (CNN) learned to (i) minimize error in classifying type of clothing (ii) minimize hamming distance between semantic neighbors and maximize distance between semantically dissimilar images (iii) maximally scramble a discriminators ability to identify the corresponding hash code-image pair when processing a semantically similar query-gallery image pair. Experimental validation for fashion inventory search yields a mean average precision (mAP) of 90.6537; in finding the closest match as compared to 53.2637; obtained by the prior art of deep Cauchy hashing for hamming space retrieval.</p>\n", "tags": ["ARXIV", "CNN", "Supervised"], "tsne_embedding": [-9.645041465759277, 14.420947074890137]}, {"key": "singh2022simultaneously", "year": "2022", "title": "Simultaneously Learning Robust Audio Embeddings And Balanced Hash Codes For Query-by-example", "abstract": "<p>Audio fingerprinting systems must efficiently and robustly identify query snippets in an extensive database. To this end state-of-the-art systems use deep learning to generate compact audio fingerprints. These systems deploy indexing methods which quantize fingerprints to hash codes in an unsupervised manner to expedite the search. However these methods generate imbalanced hash codes leading to their suboptimal performance. Therefore we propose a self-supervised learning framework to compute fingerprints and balanced hash codes in an end-to-end manner to achieve both fast and accurate retrieval performance. We model hash codes as a balanced clustering process which we regard as an instance of the optimal transport problem. Experimental results indicate that the proposed approach improves retrieval efficiency while preserving high accuracy particularly at high distortion levels compared to the competing methods. Moreover our system is efficient and scalable in computational load and memory storage.</p>\n", "tags": ["ARXIV", "Deep Learning", "Unsupervised"], "tsne_embedding": [-6.720386505126953, -5.4002556800842285]}, {"key": "sivertsen2017fast", "year": "2017", "title": "Fast Nearest Neighbor Preserving Embeddings", "abstract": "<p>We show an analog to the Fast Johnson-Lindenstrauss Transform for Nearest Neighbor Preserving Embeddings in (ell_2). These are sparse randomized embeddings that preserve the (approximate) nearest neighbors. The dimensionality of the embedding space is bounded not by the size of the embedded set n but by its doubling dimension lambda. For most large real-world datasets this will mean a considerably lower-dimensional embedding space than possible when preserving all distances. The resulting embeddings can be used with existing approximate nearest neighbor data structures to yield speed improvements.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-31.68916893005371, 3.434197425842285]}, {"key": "sivertsen2019similarity", "year": "2019", "title": "Similarity Problems In High Dimensions", "abstract": "<p>The main contribution of this dissertation is the introduction of new or improved approximation algorithms and data structures for several similarity search problems. We examine the furthest neighbor query the annulus query distance sensitive membership nearest neighbor preserving embeddings and set similarity queries in the large-scale high-dimensional setting.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-33.4927864074707, 5.6317949295043945]}, {"key": "smith2022using", "year": "2022", "title": "Using Deepspeed And Megatron To Train Megatron-turing NLG 530B A Large-scale Generative Language Model", "abstract": "<p>Pretrained general-purpose language models can achieve state-of-the-art accuracies in various natural language processing domains by adapting to downstream tasks via zero-shot few-shot and fine-tuning techniques. Because of their success the size of these models has increased rapidly requiring high-performance hardware software and algorithmic techniques to enable training such large models. As the result of a joint effort between Microsoft and NVIDIA we present details on the training of the largest monolithic transformer based language model Megatron-Turing NLG 530B (MT-NLG) with 530 billion parameters. In this paper we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using DeepSpeed and Megatron. Next we detail the training process the design of our training corpus and our data curation techniques which we believe is a key ingredient to the success of the model. Finally we discuss various evaluation results as well as other interesting observations and new properties exhibited by MT-NLG. We demonstrate that MT-NLG achieves superior zero- one- and few-shot learning accuracies on several NLP benchmarks and establishes new state-of-the-art results. We believe that our contributions will help further the development of large-scale training infrastructures large-scale language models and natural language generations.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [27.0614070892334, -12.250411033630371]}, {"key": "soltan2022alexatm", "year": "2022", "title": "Alexatm 20B Few-shot Learning Using A Large-scale Multilingual Seq2seq Model", "abstract": "<p>In this work we demonstrate that multilingual large-scale sequence-to-sequence (seq2seq) models pre-trained on a mixture of denoising and Causal Language Modeling (CLM) tasks are more efficient few-shot learners than decoder-only models on various tasks. In particular we train a 20 billion parameter multilingual seq2seq model called Alexa Teacher Model (AlexaTM 20B) and show that it achieves state-of-the-art (SOTA) performance on 1-shot summarization tasks outperforming a much larger 540B PaLM decoder model. AlexaTM 20B also achieves SOTA in 1-shot machine translation especially for low-resource languages across almost all language pairs supported by the model (Arabic English French German Hindi Italian Japanese Marathi Portuguese Spanish Tamil and Telugu) on Flores-101 dataset. We also show in zero-shot setting AlexaTM 20B outperforms GPT3 (175B) on SuperGLUE and SQuADv2 datasets and provides SOTA performance on multilingual tasks such as XNLI XCOPA Paws-X and XWinograd. Overall our results present a compelling case for seq2seq models as a powerful alternative to decoder-only models for Large-scale Language Model (LLM) training.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [22.014347076416016, -1.3310203552246094]}, {"key": "song2011random", "year": "2011", "title": "Random Maximum Margin Hashing", "abstract": "<p>Following the success of hashing methods for multidimensional\nindexing, more and more works are interested\nin embedding visual feature space in compact hash codes.\nSuch approaches are not an alternative to using index structures\nbut a complementary way to reduce both the memory\nusage and the distance computation cost. Several data\ndependent hash functions have notably been proposed to\nclosely fit data distribution and provide better selectivity\nthan usual random projections such as LSH. However, improvements\noccur only for relatively small hash code sizes\nup to 64 or 128 bits. As discussed in the paper, this is mainly\ndue to the lack of independence between the produced hash\nfunctions. We introduce a new hash function family that\nattempts to solve this issue in any kernel space. Rather\nthan boosting the collision probability of close points, our\nmethod focus on data scattering. By training purely random\nsplits of the data, regardless the closeness of the training\nsamples, it is indeed possible to generate consistently\nmore independent hash functions. On the other side, the\nuse of large margin classifiers allows to maintain good generalization\nperformances. Experiments show that our new\nRandom Maximum Margin Hashing scheme (RMMH) outperforms\nfour state-of-the-art hashing methods, notably in\nkernel spaces.</p>\n", "tags": [], "tsne_embedding": [-26.877010345458984, -6.857411861419678]}, {"key": "song2013intermedia", "year": "2013", "title": "Inter-Media Hashing for Large-Scale Retrieval from Heterogeneous Data Sources", "abstract": "<p>In this paper, we present a new multimedia retrieval paradigm to innovate large-scale search of heterogenous multimedia data. It is able to return results of different media types from heterogeneous data sources, e.g., using a query image to retrieve relevant text documents or images from different data sources. This utilizes the widely available data from different sources and caters for the current users\u2019 demand of receiving a result list simultaneously containing multiple types of data to obtain a comprehensive understanding of the query\u2019s results. To enable large-scale inter-media retrieval, we propose a novel inter-media hashing (IMH) model to explore the correlations among multiple media types from different data sources and tackle the scalability issue. To this end, multimedia data from heterogeneous data sources are transformed into a common Hamming space, in which fast search can be easily implemented by XOR and bit-count operations. Furthermore, we integrate a linear regression model to learn hashing functions so that the hash codes for new data points can be efficiently generated. Experiments conducted on real-world large-scale multimedia datasets demonstrate the superiority of our proposed method compared with state-of-the-art techniques.</p>\n", "tags": ["Cross Modal", "Has Code", "Image Retrieval"], "tsne_embedding": [-15.715341567993164, -4.330690383911133]}, {"key": "song2015rank", "year": "2015", "title": "Top Rank Supervised Binary Coding for Visual Search", "abstract": "<p>In recent years, binary coding techniques are becoming\nincreasingly popular because of their high efficiency in handling large-scale computer vision applications. It has been\ndemonstrated that supervised binary coding techniques that\nleverage supervised information can significantly enhance\nthe coding quality, and hence greatly benefit visual search\ntasks. Typically, a modern binary coding method seeks\nto learn a group of coding functions which compress data\nsamples into binary codes. However, few methods pursued\nthe coding functions such that the precision at the top of\na ranking list according to Hamming distances of the generated binary codes is optimized.\nIn this paper, we propose a novel supervised binary coding approach, namely\nTop Rank Supervised Binary Coding (Top-RSBC), which\nexplicitly focuses on optimizing the precision of top positions in a Hamming-distance ranking list towards preserving the supervision information. The core idea is to train\nthe disciplined coding functions, by which the mistakes at\nthe top of a Hamming-distance ranking list are penalized\nmore than those at the bottom. To solve such coding functions, we relax the original discrete optimization objective\nwith a continuous surrogate, and derive a stochastic gradient descent to optimize the surrogate objective. To further reduce the training time cost, we also design an online\nlearning algorithm to optimize the surrogate objective more\nefficiently. Empirical studies based upon three benchmark\nimage datasets demonstrate that the proposed binary coding approach achieves superior image search accuracy over\nthe state-of-the-arts.</p>\n", "tags": ["ICCV", "Supervised"], "tsne_embedding": [2.173403024673462, 2.631326198577881]}, {"key": "song2017deep", "year": "2017", "title": "Deep Discrete Hashing With Self-supervised Pairwise Labels", "abstract": "<p>Hashing methods have been widely used for applications of large-scale image retrieval and classification. Non-deep hashing methods using handcrafted features have been significantly outperformed by deep hashing methods due to their better feature representation and end-to-end learning framework. However the most striking successes in deep hashing have mostly involved discriminative models which require labels. In this paper we propose a novel unsupervised deep hashing method named Deep Discrete Hashing (DDH) for large-scale image retrieval and classification. In the proposed framework we address two main problems 1) how to directly learn discrete binary codes 2) how to equip the binary representation with the ability of accurate image retrieval and classification in an unsupervised way We resolve these problems by introducing an intermediate variable and a loss function steering the learning process which is based on the neighborhood structure in the original space. Experimental results on standard datasets (CIFAR-10 NUS-WIDE and Oxford-17) demonstrate that our DDH significantly outperforms existing hashing methods by large margin in terms of~mAP for image retrieval and object recognition. Code is available at urlhttps://github.com/htconquer/ddh}.</p>\n", "tags": ["ARXIV", "Has Code", "Image Retrieval", "Supervised"], "tsne_embedding": [-1.400635838508606, 8.752900123596191]}, {"key": "song2018self", "year": "2018", "title": "Self-Supervised Video Hashing with Hierarchical Binary Auto-encoder", "abstract": "<p>Existing video hash functions are built on three isolated stages: frame pooling, relaxed learning, and binarization, which have not adequately explored the temporal order of video frames in a joint binary optimization model, resulting in severe information loss. In this paper, we propose a novel unsupervised video hashing framework dubbed Self-Supervised Video Hashing (SSVH), that is able to capture the temporal nature of videos in an end-to-end learning-to-hash fashion. We specifically address two central problems: 1) how to design an encoder-decoder architecture to generate binary codes for videos; and 2) how to equip the binary codes with the ability of accurate video retrieval. We design a hierarchical binary autoencoder to model the temporal dependencies in videos with multiple granularities, and embed the videos into binary codes with less computations than the stacked architecture. Then, we encourage the binary codes to simultaneously reconstruct the visual content and neighborhood structure of the videos. Experiments on two real-world datasets (FCVID and YFCC) show that our SSVH method can significantly outperform the state-of-the-art methods and achieve the currently best performance on the task of unsupervised video retrieval.</p>\n", "tags": ["Self Supervised", "TIP", "Video Retrieval"], "tsne_embedding": [-8.134076118469238, -25.317285537719727]}, {"key": "song2019deep", "year": "2019", "title": "Deep Hashing Learning For Visual And Semantic Retrieval Of Remote Sensing Images", "abstract": "<p>Driven by the urgent demand for managing remote sensing big data large-scale remote sensing image retrieval (RSIR) attracts increasing attention in the remote sensing field. In general existing retrieval methods can be regarded as visual-based retrieval approaches which search and return a set of similar images from a database to a given query image. Although retrieval methods have achieved great success there is still a question that needs to be responded to Can we obtain the accurate semantic labels of the returned similar images to further help analyzing and processing imagery Inspired by the above question in this paper we redefine the image retrieval problem as visual and semantic retrieval of images. Specifically we propose a novel deep hashing convolutional neural network (DHCNN) to simultaneously retrieve the similar images and classify their semantic labels in a unified framework. In more detail a convolutional neural network (CNN) is used to extract high-dimensional deep features. Then a hash layer is perfectly inserted into the network to transfer the deep features into compact hash codes. In addition a fully connected layer with a softmax function is performed on hash layer to generate class distribution. Finally a loss function is elaborately designed to simultaneously consider the label loss of each image and similarity loss of pairs of images. Experimental results on two remote sensing datasets demonstrate that the proposed method achieves the state-of-art retrieval and classification performance.</p>\n", "tags": ["ARXIV", "CNN", "Image Retrieval", "Supervised"], "tsne_embedding": [-2.0853633880615234, 15.363243103027344]}, {"key": "song2020deep", "year": "2020", "title": "Deep Robust Multilevel Semantic Cross-modal Hashing", "abstract": "<p>Hashing based cross-modal retrieval has recently made significant progress. But straightforward embedding data from different modalities into a joint Hamming space will inevitably produce false codes due to the intrinsic modality discrepancy and noises. We present a novel Robust Multilevel Semantic Hashing (RMSH) for more accurate cross-modal retrieval. It seeks to preserve fine-grained similarity among data with rich semantics while explicitly require distances between dissimilar points to be larger than a specific value for strong robustness. For this we give an effective bound of this value based on the information coding-theoretic analysis and the above goals are embodied into a margin-adaptive triplet loss. Furthermore we introduce pseudo-codes via fusing multiple hash codes to explore seldom-seen semantics alleviating the sparsity problem of similarity information. Experiments on three benchmarks show the validity of the derived bounds and our method achieves state-of-the-art performance.</p>\n", "tags": ["ARXIV", "Cross Modal", "Independent"], "tsne_embedding": [-14.9874906539917, 0.11166699230670929]}, {"key": "song2022asymmetric", "year": "2022", "title": "Asymmetric Hash Code Learning For Remote Sensing Image Retrieval", "abstract": "<p>Remote sensing image retrieval (RSIR) aiming at searching for a set of similar items to a given query image is a very important task in remote sensing applications. Deep hashing learning as the current mainstream method has achieved satisfactory retrieval performance. On one hand various deep neural networks are used to extract semantic features of remote sensing images. On the other hand the hashing techniques are subsequently adopted to map the high-dimensional deep features to the low-dimensional binary codes. This kind of methods attempts to learn one hash function for both the query and database samples in a symmetric way. However with the number of database samples increasing it is typically time-consuming to generate the hash codes of large-scale database images. In this paper we propose a novel deep hashing method named asymmetric hash code learning (AHCL) for RSIR. The proposed AHCL generates the hash codes of query and database images in an asymmetric way. In more detail the hash codes of query images are obtained by binarizing the output of the network while the hash codes of database images are directly learned by solving the designed objective function. In addition we combine the semantic information of each image and the similarity information of pairs of images as supervised information to train a deep hashing network which improves the representation ability of deep features and hash codes. The experimental results on three public datasets demonstrate that the proposed method outperforms symmetric methods in terms of retrieval accuracy and efficiency. The source code is available at https://github.com/weiweisong415/Demo AHCL for TGRS2022.</p>\n", "tags": ["ARXIV", "Has Code", "Image Retrieval", "Supervised"], "tsne_embedding": [4.588912487030029, 5.261307239532471]}, {"key": "sridhar2010comparison", "year": "2010", "title": "Comparison Of Modified Dual Ternary Indexing And Multi-key Hashing Algorithms For Music Information Retrieval", "abstract": "<p>In this work we have compared two indexing algorithms that have been used to index and retrieve Carnatic music songs. We have compared a modified algorithm of the Dual ternary indexing algorithm for music indexing and retrieval with the multi-key hashing indexing algorithm proposed by us. The modification in the dual ternary algorithm was essential to handle variable length query phrase and to accommodate features specific to Carnatic music. The dual ternary indexing algorithm is adapted for Carnatic music by segmenting using the segmentation technique for Carnatic music. The dual ternary algorithm is compared with the multi-key hashing algorithm designed by us for indexing and retrieval in which features like MFCC spectral flux melody string and spectral centroid are used as features for indexing data into a hash table. The way in which collision resolution was handled by this hash table is different than the normal hash table approaches. It was observed that multi-key hashing based retrieval had a lesser time complexity than dual-ternary based indexing The algorithms were also compared for their precision and recall in which multi-key hashing had a better recall than modified dual ternary indexing for the sample data considered.</p>\n", "tags": ["IJCAI"], "tsne_embedding": [-19.122394561767578, 7.138473033905029]}, {"key": "struppek2021learning", "year": "2021", "title": "Learning To Break Deep Perceptual Hashing The Use Case Neuralhash", "abstract": "<p>Apple recently revealed its deep perceptual hashing system NeuralHash to detect child sexual abuse material (CSAM) on user devices before files are uploaded to its iCloud service. Public criticism quickly arose regarding the protection of user privacy and the systems reliability. In this paper we present the first comprehensive empirical analysis of deep perceptual hashing based on NeuralHash. Specifically we show that current deep perceptual hashing may not be robust. An adversary can manipulate the hash values by applying slight changes in images either induced by gradient-based approaches or simply by performing standard image transformations forcing or preventing hash collisions. Such attacks permit malicious actors easily to exploit the detection system from hiding abusive material to framing innocent users everything is possible. Moreover using the hash values inferences can still be made about the data stored on user devices. In our view based on our results deep perceptual hashing in its current form is generally not ready for robust client-side scanning and should not be used from a privacy perspective.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-2.113598108291626, -21.261777877807617]}, {"key": "su2018greedy", "year": "2018", "title": "Greedy Hash Towards Fast Optimization For Accurate Hash Coding In CNN", "abstract": "<p>To convert the input into binary code hashing algorithm has been widely used for approximate nearest neighbor search on large-scale image sets due to its computation and storage efficiency. Deep hashing further improves the retrieval quality by combining the hash coding with deep neural network. However a major difficulty in deep hashing lies in the discrete constraints imposed on the network output which generally makes the optimization NP hard. In this work we adopt the greedy principle to tackle this NP hard problem by iteratively updating the network toward the probable optimal discrete solution in each iteration. A hash coding layer is designed to implement our approach which strictly uses the sign function in forward propagation to maintain the discrete constraints while in back propagation the gradients are transmitted intactly to the front layer to avoid the vanishing gradients. In addition to the theoretical derivation we provide a new perspective to visualize and understand the effectiveness and efficiency of our algorithm. Experiments on benchmark datasets show that our scheme outperforms state-of-the-art hashing methods in both supervised and unsupervised tasks.</p>\n", "tags": ["CNN", "NEURIPS", "Unsupervised"], "tsne_embedding": [0.21659468114376068, -5.21404504776001]}, {"key": "su2019unsupervised", "year": "2019", "title": "Deep Joint-Semantics Reconstructing Hashing for Large-Scale Unsupervised Cross-Modal Retrieval", "abstract": "<p><img src=\"https://github.com/zzs1994/DJSRH/blob/master/page_image/DJRSH.png?raw=true\" alt=\"Deep Joint-Semantics Reconstructing Hashing for Large-Scale Unsupervised Cross-Modal Retrieval\" title=\"Deep Joint-Semantics Reconstructing Hashing for Large-Scale Unsupervised Cross-Modal Retrieval\" /></p>\n\n<p>Cross-modal hashing encodes the multimedia data into a common binary hash space in which the correlations among the samples from different modalities can be effectively measured. Deep cross-modal hashing further improves the retrieval performance as the deep neural networks can generate more semantic relevant features and hash codes. In this paper, we study the unsupervised deep cross-modal hash coding and propose Deep Joint Semantics Reconstructing Hashing (DJSRH), which has the following two main advantages. First, to learn binary codes that preserve the neighborhood structure of the original data, DJSRH constructs a novel joint-semantics affinity matrix which elaborately integrates the original neighborhood information from different modalities and accordingly is capable to capture the latent intrinsic semantic affinity for the input multi-modal instances. Second, DJSRH later trains the networks to generate binary codes that maximally reconstruct above joint-semantics relations via the proposed reconstructing framework, which is more competent for the batch-wise training as it reconstructs the specific similarity value unlike the common Laplacian constraint merely preserving the similarity order. Extensive experiments demonstrate the significant improvement by DJSRH in various cross-modal retrieval tasks.</p>\n", "tags": ["Cross Modal", "Deep Learning", "Has Code", "ICCV", "Supervised"], "tsne_embedding": [-11.64333724975586, 3.4309489727020264]}, {"key": "su2021hard", "year": "2021", "title": "Hard Example Guided Hashing For Image Retrieval", "abstract": "<p>Compared with the traditional hashing methods deep hashing methods generate hash codes with rich semantic information and greatly improves the performances in the image retrieval field. However it is unsatisfied for current deep hashing methods to predict the similarity of hard examples. It exists two main factors affecting the ability of learning hard examples which are weak key features extraction and the shortage of hard examples. In this paper we give a novel end-to-end model to extract the key feature from hard examples and obtain hash code with the accurate semantic information. In addition we redesign a hard pair-wise loss function to assess the hard degree and update penalty weights of examples. It effectively alleviates the shortage problem in hard examples. Experimental results on CIFAR-10 and NUS-WIDE demonstrate that our model outperformances the mainstream hashing-based image retrieval methods.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Independent"], "tsne_embedding": [-18.119237899780273, 14.596940994262695]}, {"key": "su2022welm", "year": "2022", "title": "Welm A Well-read Pre-trained Language Model For Chinese", "abstract": "<p>Large Language Models pre-trained with self-supervised learning have demonstrated impressive zero-shot generalization capabilities on a wide spectrum of tasks. In this work we present WeLM a well-read pre-trained language model for Chinese that is able to seamlessly perform different types of tasks with zero or few-shot demonstrations. WeLM is trained with 10B parameters by reading a curated high-quality corpus covering a wide range of topics. We show that WeLM is equipped with broad knowledge on various domains and languages. On 18 monolingual (Chinese) tasks WeLM can significantly outperform existing pre-trained models with similar sizes and match the performance of models up to 25 times larger. WeLM also exhibits strong capabilities in multi-lingual and code-switching understanding outperforming existing multilingual language models pre-trained on 30 languages. Furthermore We collected human-written prompts for a large set of supervised datasets in Chinese and fine-tuned WeLM with multi-prompted training. The resulting model can attain strong generalization on unseen types of tasks and outperform the unsupervised WeLM in zero-shot learning. Finally we demonstrate that WeLM has basic skills at explaining and calibrating the decisions from itself which can be promising directions for future research. Our models can be applied from https://welm.weixin.qq.com/docs/api/.</p>\n", "tags": ["ARXIV", "Has Code", "Unsupervised"], "tsne_embedding": [23.64723014831543, -6.922964572906494]}, {"key": "subramanya2019diskann", "year": "2019", "title": "DiskANN: Fast Accurate Billion-point Nearest Neighbor Search on a Single Node", "abstract": "<p>Current state-of-the-art approximate nearest neighbor search (ANNS) algorithms generate indices that must be stored in main memory for fast high-recall search. This makes them expensive and limits the size of the dataset. We present a new graph-based indexing and search system called DiskANN that can index, store, and search a billion point database on a single workstation with just 64GB RAM and an inexpensive solid-state drive (SSD). Contrary to current wisdom, we demonstrate that the SSD-based indices built by DiskANN can meet all three desiderata for large-scale ANNS: high-recall, low query latency and high density (points indexed per node). On the billion point SIFT1B bigann dataset, DiskANN serves &gt; 5000 queries a second with &lt; 3ms mean latency and 95%+ 1-recall@1 on a 16 core machine, where state-of-the-art billion-point ANNS algorithms with similar memory footprint like FAISS and IVFOADC+G+P plateau at around 50% 1-recall@1. Alternately, in the high recall regime, DiskANN can index and serve 5 \u2212 10x more points per node compared to state-of-the-art graph- based methods such as HNSW and NSG. Finally, as part of our overall DiskANN system, we introduce Vamana, a new graph-based ANNS index that is more versatile than the graph indices even for in-memory indices.</p>\n\n", "tags": ["Graph", "Has Code", "NEURIPS"], "tsne_embedding": [-27.777835845947266, -14.275495529174805]}, {"key": "sumbul2020deep", "year": "2020", "title": "Deep Learning For Image Search And Retrieval In Large Remote Sensing Archives", "abstract": "<p>This chapter presents recent advances in content based image search and retrieval (CBIR) systems in remote sensing (RS) for fast and accurate information discovery from massive data archives. Initially we analyze the limitations of the traditional CBIR systems that rely on the hand-crafted RS image descriptors. Then we focus our attention on the advances in RS CBIR systems for which deep learning (DL) models are at the forefront. In particular we present the theoretical properties of the most recent DL based CBIR systems for the characterization of the complex semantic content of RS images. After discussing their strengths and limitations we present the deep hashing based CBIR systems that have high time-efficient search capability within huge data archives. Finally the most promising research directions in RS CBIR are discussed.</p>\n", "tags": ["ARXIV", "Deep Learning"], "tsne_embedding": [10.511661529541016, 11.341300010681152]}, {"key": "sun2019supervised", "year": "2019", "title": "Supervised Hierarchical Cross-Modal Hashing", "abstract": "<p>Recently, due to the unprecedented growth of multimedia data,\ncross-modal hashing has gained increasing attention for the\nefficient cross-media retrieval. Typically, existing methods on crossmodal hashing treat labels of one instance independently but\noverlook the correlations among labels. Indeed, in many real-world\nscenarios, like the online fashion domain, instances (items) are\nlabeled with a set of categories correlated by certain hierarchy. In\nthis paper, we propose a new end-to-end solution for supervised\ncross-modal hashing, named HiCHNet, which explicitly exploits the\nhierarchical labels of instances. In particular, by the pre-established\nlabel hierarchy, we comprehensively characterize each modality\nof the instance with a set of layer-wise hash representations. In\nessence, hash codes are encouraged to not only preserve the layerwise semantic similarities encoded by the label hierarchy, but also\nretain the hierarchical discriminative capabilities. Due to the lack\nof benchmark datasets, apart from adapting the existing dataset\nFashionVC from fashion domain, we create a dataset from the\nonline fashion platform Ssense consisting of 15, 696 image-text\npairs labeled by 32 hierarchical categories. Extensive experiments\non two real-world datasets demonstrate the superiority of our model\nover the state-of-the-art methods.</p>\n", "tags": ["Cross Modal", "Deep Learning", "SIGIR", "Supervised"], "tsne_embedding": [-14.413656234741211, -4.824809551239014]}, {"key": "sun2022deep", "year": "2022", "title": "Deep Normalized Cross-Modal Hashing With Bi-Direction Relation Reasoning", "abstract": "<p>Due to the continuous growth of large-scale multi-modal data and increasing requirements for retrieval speed, deep cross-modal hashing has gained increasing attention recently. Most of existing studies take a similarity matrix as supervision to optimize their models, and the inner product between continuous surrogates of hash codes is utilized to depict the similarity in the Hamming space. However, all of them merely consider the relevant information to build the similarity matrix, ignoring the contribution of the irrelevant one, i.e., the categories that samples do not belong to. Therefore, they cannot effectively alleviate the effect of dissimilar samples. Moreover, due to the modality distribution difference, directly utilizing continuous surrogates of hash codes to calculate similarity may induce suboptimal retrieval performance. To tackle these issues, in this paper, we propose a novel deep normalized cross-modal hashing scheme with bi-direction relation reasoning, named Bi_NCMH. Specifically, we build the multi-level semantic similarity matrix by considering bi-direction relation, i.e., consistent and inconsistent relation. It hence can holistically characterize relations among instances. Besides, we execute feature normalization on continuous surrogates of hash codes to eliminate the deviation caused by modality gap, which further reduces the negative impact of binarization on retrieval performance. Extensive experiments on two cross-modal benchmark datasets demonstrate the superiority of our model over several state-of-the-art baselines.</p>\n", "tags": ["CVPR", "Cross Modal", "Deep Learning"], "tsne_embedding": [-14.253449440002441, -1.1631004810333252]}, {"key": "sun2023aligning", "year": "2023", "title": "Aligning Large Multimodal Models With Factually Augmented RLHF", "abstract": "<p>Large Multimodal Models (LMM) are built across modalities and the misalignment between two modalities can result in hallucination generating textual outputs that are not grounded by the multimodal information in context. To address the multimodal misalignment issue we adapt the Reinforcement Learning from Human Feedback (RLHF) from the text domain to the task of vision-language alignment where human annotators are asked to compare two responses and pinpoint the more hallucinated one and the vision-language model is trained to maximize the simulated human rewards. We propose a new alignment algorithm called Factually Augmented RLHF that augments the reward model with additional factual information such as image captions and ground-truth multi-choice options which alleviates the reward hacking phenomenon in RLHF and further improves the performance. We also enhance the GPT-4-generated training data (for vision instruction tuning) with previously available human-written image-text pairs to improve the general capabilities of our model. To evaluate the proposed approach in real-world scenarios we develop a new evaluation benchmark MMHAL-BENCH with a special focus on penalizing hallucinations. As the first LMM trained with RLHF our approach achieves remarkable improvement on the LLaVA-Bench dataset with the 9437; performance level of the text-only GPT-4 (while previous best methods can only achieve the 8737; level) and an improvement by 6037; on MMHAL-BENCH over other baselines. We opensource our code model data at https://llava-rlhf.github.io.</p>\n", "tags": ["ARXIV", "Cross Modal", "Has Code", "Supervised"], "tsne_embedding": [47.64175033569336, 5.419919967651367]}, {"key": "sun2023is", "year": "2023", "title": "Is Chatgpt Good At Search Investigating Large Language Models As Re-ranking Agents", "abstract": "<p>Large Language Models (LLMs) have demonstrated remarkable zero-shot generalization across various language-related tasks including search engines. However existing work utilizes the generative ability of LLMs for Information Retrieval (IR) rather than direct passage ranking. The discrepancy between the pre-training objectives of LLMs and the ranking objective poses another challenge. In this paper we first investigate generative LLMs such as ChatGPT and GPT-4 for relevance ranking in IR. Surprisingly our experiments reveal that properly instructed LLMs can deliver competitive even superior results to state-of-the-art supervised methods on popular IR benchmarks. Furthermore to address concerns about data contamination of LLMs we collect a new test set called NovelEval based on the latest knowledge and aiming to verify the models ability to rank unknown knowledge. Finally to improve efficiency in real-world applications we delve into the potential for distilling the ranking capabilities of ChatGPT into small specialized models using a permutation distillation scheme. Our evaluation results turn out that a distilled 440M model outperforms a 3B supervised model on the BEIR benchmark. The code to reproduce our results is available at www.github.com/sunnweiwei/RankGPT.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [35.88180923461914, -6.024649143218994]}, {"key": "sun2023principle", "year": "2023", "title": "Principle-driven Self-alignment Of Language Models From Scratch With Minimal Human Supervision", "abstract": "<p>Recent AI-assistant agents such as ChatGPT predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions ensuring they are helpful ethical and reliable. However this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality reliability diversity self-consistency and undesirable biases. To address these challenges we propose a novel approach called SELF-ALIGN which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision. Our approach encompasses four stages first we use an LLM to generate synthetic prompts and a topic-guided method to augment the prompt diversity; second we use a small set of human-written principles for AI models to follow and guide the LLM through in-context learning from demonstrations (of principles application) to produce helpful ethical and reliable responses to users queries; third we fine-tune the original LLM with the high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally we offer a refinement step to address the issues of overly-brief or indirect responses. Applying SELF-ALIGN to the LLaMA-65b base language model we develop an AI assistant named Dromedary. With fewer than 300 lines of human annotations (including &lt; 200 seed prompts 16 generic principles and 5 exemplars for in-context learning). Dromedary significantly surpasses the performance of several state-of-the-art AI systems including Text-Davinci-003 and Alpaca on benchmark datasets with various settings.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [41.29753875732422, -2.9980592727661133]}, {"key": "sun2024temporal", "year": "2024", "title": "Temporal Insight Enhancement Mitigating Temporal Hallucination In Multimodal Large Language Models", "abstract": "<p>Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced the comprehension of multimedia content bringing together diverse modalities such as text images and videos. However a critical challenge faced by these models especially when processing video inputs is the occurrence of hallucinations - erroneous perceptions or interpretations particularly at the event level. This study introduces an innovative method to address event-level hallucinations in MLLMs focusing on specific temporal understanding in video content. Our approach leverages a novel framework that extracts and utilizes event-specific information from both the event query and the provided video to refine MLLMs response. We propose a unique mechanism that decomposes on-demand event queries into iconic actions. Subsequently we employ models like CLIP and BLIP2 to predict specific timestamps for event occurrences. Our evaluation conducted using the Charades-STA dataset demonstrates a significant reduction in temporal hallucinations and an improvement in the quality of event-related responses. This research not only provides a new perspective in addressing a critical limitation of MLLMs but also contributes a quantitatively measurable method for evaluating MLLMs in the context of temporal-related questions.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [49.848575592041016, 10.230512619018555]}, {"key": "sunarso2013scalable", "year": "2013", "title": "Scalable Protein Sequence Similarity Search Using Locality-sensitive Hashing And Mapreduce", "abstract": "<p>Metagenomics is the study of environments through genetic sampling of their microbiota. Metagenomic studies produce large datasets that are estimated to grow at a faster rate than the available computational capacity. A key step in the study of metagenome data is sequence similarity searching which is computationally intensive over large datasets. Tools such as BLAST require large dedicated computing infrastructure to perform such analysis and may not be available to every researcher. In this paper we propose a novel approach called ScalLoPS that performs searching on protein sequence datasets using LSH (Locality-Sensitive Hashing) that is implemented using the MapReduce distributed framework. ScalLoPS is designed to scale across computing resources sourced from cloud computing providers. We present the design and implementation of ScalLoPS followed by evaluation with datasets derived from both traditional as well as metagenomic studies. Our experiments show that with this method approximates the quality of BLAST results while improving the scalability of protein sequence search.</p>\n", "tags": ["ARXIV", "Independent", "LSH"], "tsne_embedding": [12.763904571533203, -19.26028060913086]}, {"key": "sundaram2013streaming", "year": "2013", "title": "Streaming Similarity Search over one Billion Tweets using Parallel Locality-Sensitive Hashing", "abstract": "<p>Finding nearest neighbors has become an important operation on databases, with applications to text search, multimedia indexing,\nand many other areas. One popular algorithm for similarity search, especially for high dimensional data (where spatial indexes like kdtrees do not perform well) is Locality Sensitive Hashing (LSH), an\napproximation algorithm for finding similar objects. In this paper, we describe a new variant of LSH, called Parallel\nLSH (PLSH) designed to be extremely efficient, capable of scaling out on multiple nodes and multiple cores, and which supports highthroughput streaming of new data. Our approach employs several\nnovel ideas, including: cache-conscious hash table layout, using a 2-level merge algorithm for hash table construction; an efficient\nalgorithm for duplicate elimination during hash-table querying; an insert-optimized hash table structure and efficient data expiration\nalgorithm for streaming data; and a performance model that accurately estimates performance of the algorithm and can be used to\noptimize parameter settings. We show that on a workload where we perform similarity search on a dataset of &gt; 1 Billion tweets, with\nhundreds of millions of new tweets per day, we can achieve query times of 1\u20132.5 ms. We show that this is an order of magnitude faster\nthan existing indexing schemes, such as inverted indexes. To the best of our knowledge, this is the fastest implementation of LSH,\nwith table construction times up to 3.7x faster and query times that are 8.3x faster than a basic implementation.</p>\n", "tags": ["LSH", "Streaming Data", "VLDB"], "tsne_embedding": [-19.90395164489746, -0.9890613555908203]}, {"key": "svenstrup2017hash", "year": "2017", "title": "Hash Embeddings For Efficient Word Representations", "abstract": "<p>We present hash embeddings an efficient method for representing words in a continuous vector form. A hash embedding may be seen as an interpolation between a standard word embedding and a word embedding created using a random hash function (the hashing trick). In hash embeddings each token is represented by (k) (d)-dimensional embeddings vectors and one (k) dimensional weight vector. The final (d) dimensional representation of the token is the product of the two. Rather than fitting the embedding vectors for each token these are selected by the hashing trick from a shared pool of (B) embedding vectors. Our experiments show that hash embeddings can easily deal with huge vocabularies consisting of millions of tokens. When using a hash embedding there is no need to create a dictionary before training nor to perform any kind of vocabulary pruning after training. We show that models trained using hash embeddings exhibit at least the same level of performance as models trained using regular embeddings across a wide range of tasks. Furthermore the number of parameters needed by such an embedding is only a fraction of what is required by a regular embedding. Since standard embeddings and embeddings constructed using the hashing trick are actually just special cases of a hash embedding hash embeddings can be considered an extension and improvement over the existing regular embedding types.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-15.274134635925293, -20.799076080322266]}, {"key": "taherkhani2020error", "year": "2020", "title": "Error-corrected Margin-based Deep Cross-modal Hashing For Facial Image Retrieval", "abstract": "<p>Cross-modal hashing facilitates mapping of heterogeneous multimedia data into a common Hamming space which can beutilized for fast and flexible retrieval across different modalities. In this paper we propose a novel cross-modal hashingarchitecture-deep neural decoder cross-modal hashing (DNDCMH) which uses a binary vector specifying the presence of certainfacial attributes as an input query to retrieve relevant face images from a database. The DNDCMH network consists of two separatecomponents an attribute-based deep cross-modal hashing (ADCMH) module which uses a margin (m)-based loss function toefficiently learn compact binary codes to preserve similarity between modalities in the Hamming space and a neural error correctingdecoder (NECD) which is an error correcting decoder implemented with a neural network. The goal of NECD network in DNDCMH isto error correct the hash codes generated by ADCMH to improve the retrieval efficiency. The NECD network is trained such that it hasan error correcting capability greater than or equal to the margin (m) of the margin-based loss function. This results in NECD cancorrect the corrupted hash codes generated by ADCMH up to the Hamming distance of m. We have evaluated and comparedDNDCMH with state-of-the-art cross-modal hashing methods on standard datasets to demonstrate the superiority of our method.</p>\n", "tags": ["ARXIV", "Cross Modal", "Image Retrieval", "Supervised"], "tsne_embedding": [-10.147018432617188, 6.663760662078857]}, {"key": "talreja2019using", "year": "2019", "title": "Using Deep Cross Modal Hashing And Error Correcting Codes For Improving The Efficiency Of Attribute Guided Facial Image Retrieval", "abstract": "<p>With benefits of fast query speed and low storage cost hashing-based image retrieval approaches have garnered considerable attention from the research community. In this paper we propose a novel Error-Corrected Deep Cross Modal Hashing (CMH-ECC) method which uses a bitmap specifying the presence of certain facial attributes as an input query to retrieve relevant face images from the database. In this architecture we generate compact hash codes using an end-to-end deep learning module which effectively captures the inherent relationships between the face and attribute modality. We also integrate our deep learning module with forward error correction codes to further reduce the distance between different modalities of the same subject. Specifically the properties of deep hashing and forward error correction codes are exploited to design a cross modal hashing framework with high retrieval performance. Experimental results using two standard datasets with facial attributes-image modalities indicate that our CMH-ECC face image retrieval model outperforms most of the current attribute-based face image retrieval approaches.</p>\n", "tags": ["ARXIV", "Cross Modal", "Deep Learning", "Image Retrieval", "Independent"], "tsne_embedding": [-9.053146362304688, 11.938196182250977]}, {"key": "tan2017supervised", "year": "2017", "title": "Supervised Hashing With End-to-end Binary Deep Neural Network", "abstract": "<p>Image hashing is a popular technique applied to large scale content-based visual retrieval due to its compact and efficient binary codes. Our work proposes a new end-to-end deep network architecture for supervised hashing which directly learns binary codes from input images and maintains good properties over binary codes such as similarity preservation independence and balancing. Furthermore we also propose a new learning scheme that can cope with the binary constrained loss function. The proposed algorithm not only is scalable for learning over large-scale datasets but also outperforms state-of-the-art supervised hashing methods which are illustrated throughout extensive experiments from various image retrieval benchmarks.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Supervised"], "tsne_embedding": [-3.567394971847534, 11.359564781188965]}, {"key": "tan2019drill", "year": "2019", "title": "Drill-down Interactive Retrieval Of Complex Scenes Using Natural Language Queries", "abstract": "<p>This paper explores the task of interactive image retrieval using natural language queries where a user progressively provides input queries to refine a set of retrieval results. Moreover our work explores this problem in the context of complex image scenes containing multiple objects. We propose Drill-down an effective framework for encoding multiple queries with an efficient compact state representation that significantly extends current methods for single-round image retrieval. We show that using multiple rounds of natural language queries as input can be surprisingly effective to find arbitrarily specific images of complex scenes. Furthermore we find that existing image datasets with textual captions can provide a surprisingly effective form of weak supervision for this task. We compare our method with existing sequential encoding and embedding networks demonstrating superior performance on two proposed benchmarks automatic image retrieval on a simulated scenario that uses region captions as queries and interactive image retrieval using real queries from human evaluators.</p>\n", "tags": ["Image Retrieval", "NEURIPS"], "tsne_embedding": [11.376147270202637, 18.31320571899414]}, {"key": "tan2020learning", "year": "2020", "title": "Learning To Hash With Graph Neural Networks For Recommender Systems", "abstract": "<p>Graph representation learning has attracted much attention in supporting high quality candidate search at scale. Despite its effectiveness in learning embedding vectors for objects in the user-item interaction network the computational costs to infer users preferences in continuous embedding space are tremendous. In this work we investigate the problem of hashing with graph neural networks (GNNs) for high quality retrieval and propose a simple yet effective discrete representation learning framework to jointly learn continuous and discrete codes. Specifically a deep hashing with GNNs (HashGNN) is presented which consists of two components a GNN encoder for learning node representations and a hash layer for encoding representations to hash codes. The whole architecture is trained end-to-end by jointly optimizing two losses i.e. reconstruction loss from reconstructing observed links and ranking loss from preserving the relative ordering of hash codes. A novel discrete optimization strategy based on straight through estimator (STE) with guidance is proposed. The principal idea is to avoid gradient magnification in back-propagation of STE with continuous embedding guidance in which we begin from learning an easier network that mimic the continuous embedding and let it evolve during the training until it finally goes back to STE. Comprehensive experiments over three publicly available and one real-world Alibaba company datasets demonstrate that our model not only can achieve comparable performance compared with its continuous counterpart but also runs multiple times faster during inference.</p>\n", "tags": ["ARXIV", "Graph", "Supervised"], "tsne_embedding": [-1.4664807319641113, 26.933425903320312]}, {"key": "tan2021bcd", "year": "2021", "title": "BCD A Cross-architecture Binary Comparison Database Experiment Using Locality Sensitive Hashing Algorithms", "abstract": "<p>Given a binary executable without source code it is difficult to determine what each function in the binary does by reverse engineering it and even harder without prior experience and context. In this paper we performed a comparison of different hashing functions effectiveness at detecting similar lifted snippets of LLVM IR code and present the design and implementation of a framework for cross-architecture binary code similarity search database using MinHash as the chosen hashing algorithm over SimHash SSDEEP and TLSH. The motivation is to help reverse engineers to quickly gain context of functions in an unknown binary by comparing it against a database of known functions. The code for this project is open source and can be found at https://github.com/h4sh5/bcddb\u201d</p>\n", "tags": ["ARXIV", "Has Code", "Independent"], "tsne_embedding": [-2.5006003379821777, -15.809758186340332]}, {"key": "tan2023fast", "year": "2023", "title": "Fast Locality Sensitive Hashing With Theoretical Guarantee", "abstract": "<p>Locality-sensitive hashing (LSH) is an effective randomized technique widely used in many machine learning tasks. The cost of hashing is proportional to data dimensions and thus often the performance bottleneck when dimensionality is high and the number of hash functions involved is large. Surprisingly however little work has been done to improve the efficiency of LSH computation. In this paper we design a simple yet efficient LSH scheme named FastLSH under l2 norm. By combining random sampling and random projection FastLSH reduces the time complexity from O(n) to O(m) (m&lt;n) where n is the data dimensionality and m is the number of sampled dimensions. Moreover FastLSH has provable LSH property which distinguishes it from the non-LSH fast sketches. We conduct comprehensive experiments over a collection of real and synthetic datasets for the nearest neighbor search task. Experimental results demonstrate that FastLSH is on par with the state-of-the-arts in terms of answer quality space occupation and query efficiency while enjoying up to 80x speedup in hash function evaluation. We believe that FastLSH is a promising alternative to the classic LSH scheme.</p>\n", "tags": ["ARXIV", "Independent", "LSH"], "tsne_embedding": [-24.819660186767578, -8.53193187713623]}, {"key": "tanaka2021fake", "year": "2021", "title": "Fake-image Detection With Robust Hashing", "abstract": "<p>In this paper we investigate whether robust hashing has a possibility to robustly detect fake-images even when multiple manipulation techniques such as JPEG compression are applied to images for the first time. In an experiment the proposed fake detection with robust hashing is demonstrated to outperform state-of-the-art one under the use of various datasets including fake images generated with GANs.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-2.669161319732666, -23.61130142211914]}, {"key": "tang2021when", "year": "2021", "title": "When Similarity Digest Meets Vector Management System A Survey On Similarity Hash Function", "abstract": "<p>The booming vector manage system calls for feasible similarity hash function as a front-end to perform similarity analysis. In this paper we make a systematical survey on the existent well-known similarity hash functions to tease out the satisfied ones. We conclude that the similarity hash function MinHash and Nilsimsa can be directly marshaled into the pipeline of similarity analysis using vector manage system. After that we make a brief and empirical discussion on the performance drawbacks of the these functions and highlight MinHash the variant of SimHash and feature hashing are the best for vector management system for large-scale similarity analysis.</p>\n", "tags": ["ARXIV", "Independent", "Survey Paper"], "tsne_embedding": [-15.67746639251709, -1.2538399696350098]}, {"key": "tang2023chain", "year": "2023", "title": "Chain-of-thought Prompting Under Streaming Batch A Case Study", "abstract": "<p>Recently Large Language Models (LLMs) have demonstrated remarkable capabilities. Chain-of-Thought (CoT) has been proposed as a way of assisting LLMs in performing complex reasoning. However developing effective prompts can be a challenging and labor-intensive task. Many studies come out of some way to automatically construct CoT from test data. Most of them assume that all test data is visible before testing and only select a small subset to generate rationales which is an unrealistic assumption. In this paper we present a case study on how to construct and optimize chain-of-thought prompting using batch data in streaming settings.</p>\n", "tags": ["ARXIV", "Case Study"], "tsne_embedding": [33.29630661010742, -6.277350902557373]}, {"key": "tang2023science", "year": "2023", "title": "The Science Of Detecting Llm-generated Texts", "abstract": "<p>The emergence of large language models (LLMs) has resulted in the production of LLM-generated texts that is highly sophisticated and almost indistinguishable from texts written by humans. However this has also sparked concerns about the potential misuse of such texts such as spreading misinformation and causing disruptions in the education system. Although many detection approaches have been proposed a comprehensive understanding of the achievements and challenges is still lacking. This survey aims to provide an overview of existing LLM-generated text detection techniques and enhance the control and regulation of language generation models. Furthermore we emphasize crucial considerations for future research including the development of comprehensive evaluation metrics and the threat posed by open-source LLMs to drive progress in the area of LLM-generated text detection.</p>\n", "tags": ["ARXIV", "Survey Paper"], "tsne_embedding": [38.37376022338867, -7.530190944671631]}, {"key": "taquet2010invariant", "year": "2010", "title": "Invariant Spectral Hashing Of Image Saliency Graph", "abstract": "<p>Image hashing is the process of associating a short vector of bits to an image. The resulting summaries are useful in many applications including image indexing image authentication and pattern recognition. These hashes need to be invariant under transformations of the image that result in similar visual content but should drastically differ for conceptually distinct contents. This paper proposes an image hashing method that is invariant under rotation scaling and translation of the image. The gist of our approach relies on the geometric characterization of salient point distribution in the image. This is achieved by the definition of a saliency graph connecting these points jointly with an image intensity function on the graph nodes. An invariant hash is then obtained by considering the spectrum of this function in the eigenvector basis of the Laplacian graph that is its graph Fourier transform. Interestingly this spectrum is invariant under any relabeling of the graph nodes. The graph reveals geometric information of the image making the hash robust to image transformation yet distinct for different visual content. The efficiency of the proposed method is assessed on a set of MRI 2-D slices and on a database of faces.</p>\n", "tags": ["ARXIV", "Graph"], "tsne_embedding": [-2.5743165016174316, 24.607091903686523]}, {"key": "tay2022transcending", "year": "2022", "title": "Transcending Scaling Laws With 0.1 Extra Compute", "abstract": "<p>Scaling language models improves performance but comes with significant computational costs. This paper proposes UL2R a method that substantially improves existing language models and their scaling curves with a relatively tiny amount of extra compute. The key idea is to continue training a state-of-the-art large language model (e.g. PaLM) on a few more steps with UL2s mixture-of-denoiser objective. We show that with almost negligible extra computational costs and no new sources of data we are able to substantially improve the scaling properties of large language models on downstream metrics. In this paper we continue training PaLM with UL2R introducing a new set of models at 8B 62B and 540B scale which we call U-PaLM. Impressively at 540B scale we show an approximately 2x computational savings rate where U-PaLM achieves the same performance as the final PaLM 540B model at around half its computational budget (i.e. saving (sim)4.4 million TPUv4 hours). We further show that this improved scaling curve leads to emergent abilities on challenging BIG-Bench tasks \u2013 for instance U-PaLM does much better than PaLM on some tasks or demonstrates better quality at much smaller scale (62B as opposed to 540B). Overall we show that U-PaLM outperforms PaLM on many few-shot setups i.e. English NLP tasks (e.g. commonsense reasoning question answering) reasoning tasks with chain-of-thought (e.g. GSM8K) multilingual tasks (MGSM TydiQA) MMLU and challenging BIG-Bench tasks. Finally we provide qualitative examples showing the new capabilities of U-PaLM for single and multi-span infilling.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [19.75682258605957, 0.496650755405426]}, {"key": "tay2022unifying", "year": "2022", "title": "UL2 Unifying Language Learning Paradigms", "abstract": "<p>Existing pre-trained models are generally geared towards a particular class of problems. To date there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives \u2013 two concepts that are commonly conflated. Next we present a generalized amp; unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD) a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 amp; GPT-like models across multiple diverse setups. By scaling our model up to 20B parameters we achieve SOTA performance on 50 well-established supervised finetuning based NLP tasks. Our model also achieve strong results at in-context learning outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. On 0-shot MMLU UL2 20B outperforms T0 and T5 models. UL2 20B also works well with chain-of-thought prompting and reasoning making it an appealing choice for research into reasoning at a small to medium scale of 20B parameters. Finally we apply FLAN instruction tuning to the UL2 20B model achieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release Flax-based T5X checkpoints for the UL2 20B amp; Flan-UL2 20B.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [36.189453125, -17.079320907592773]}, {"key": "taylor2022galactica", "year": "2022", "title": "Galactica A Large Language Model For Science", "abstract": "<p>Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica a large language model that can store combine and reason about scientific knowledge. We train on a large scientific corpus of papers reference material knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations Galactica outperforms the latest GPT-3 by 68.237; versus 49.037;. Galactica also performs well on reasoning outperforming Chinchilla on mathematical MMLU by 41.337; to 35.737; and PaLM 540B on MATH with a score of 20.437; versus 8.837;. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.637; and 52.937;. And despite not being trained on a general corpus Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [33.029354095458984, -13.881738662719727]}, {"key": "teixeira2013scalable", "year": "2013", "title": "Scalable Locality-sensitive Hashing For Similarity Search In High-dimensional Large-scale Multimedia Datasets", "abstract": "<p>Similarity search is critical for many database applications including the increasingly popular online services for Content-Based Multimedia Retrieval (CBMR). These services which include image search engines must handle an overwhelming volume of data while keeping low response times. Thus scalability is imperative for similarity search in Web-scale applications but most existing methods are sequential and target shared-memory machines. Here we address these issues with a distributed efficient and scalable index based on Locality-Sensitive Hashing (LSH). LSH is one of the most efficient and popular techniques for similarity search but its poor referential locality properties has made its implementation a challenging problem. Our solution is based on a widely asynchronous dataflow parallelization with a number of optimizations that include a hierarchical parallelization to decouple indexing and data storage locality-aware data partition strategies to reduce message passing and multi-probing to limit memory usage. The proposed parallelization attained an efficiency of 9037; in a distributed system with about 800 CPU cores. In particular the original locality-aware data partition reduced the number of messages exchanged in 3037;. Our parallel LSH was evaluated using the largest public dataset for similarity search (to the best of our knowledge) with (10^9) 128-d SIFT descriptors extracted from Web images. This is two orders of magnitude larger than datasets that previous LSH parallelizations could handle.</p>\n", "tags": ["ARXIV", "Independent", "LSH"], "tsne_embedding": [-20.091026306152344, -3.631614923477173]}, {"key": "tepper2020procrustean", "year": "2020", "title": "Procrustean Orthogonal Sparse Hashing", "abstract": "<p>Hashing is one of the most popular methods for similarity search because of its speed and efficiency. Dense binary hashing is prevalent in the literature. Recently insect olfaction was shown to be structurally and functionally analogous to sparse hashing 6. Here we prove that this biological mechanism is the solution to a well-posed optimization problem. Furthermore we show that orthogonality increases the accuracy of sparse hashing. Next we present a novel method Procrustean Orthogonal Sparse Hashing (POSH) that unifies these findings learning an orthogonal transform from training data compatible with the sparse hashing mechanism. We provide theoretical evidence of the shortcomings of Optimal Sparse Lifting (OSL) 22 and BioHash 30 two related olfaction-inspired methods and propose two new methods Binary OSL and SphericalHash to address these deficiencies. We compare POSH Binary OSL and SphericalHash to several state-of-the-art hashing methods and provide empirical results for the superiority of the proposed methods across a wide range of standard benchmarks and parameter settings.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [8.991470336914062, -22.195581436157227]}, {"key": "thoppilan2022lamda", "year": "2022", "title": "Lamda Language Models For Dialog Applications", "abstract": "<p>We present LaMDA Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge safety involves ensuring that the models responses are consistent with a set of human values such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge factual grounding involves enabling the model to consult external knowledge sources such as an information retrieval system a language translator and a calculator. We quantify factuality using a groundedness metric and we find that our approach enables the model to generate responses grounded in known sources rather than responses that merely sound plausible. Finally we explore the use of LaMDA in the domains of education and content recommendations and analyze their helpfulness and role consistency.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [48.58192443847656, -1.1148368120193481]}, {"key": "thorup2015fast", "year": "2015", "title": "Fast And Powerful Hashing Using Tabulation", "abstract": "<p>Randomized algorithms are often enjoyed for their simplicity but the hash functions employed to yield the desired probabilistic guarantees are often too complicated to be practical. Here we survey recent results on how simple hashing schemes based on tabulation provide unexpectedly strong guarantees. Simple tabulation hashing dates back to Zobrist 1970. Keys are viewed as consisting of c characters and we have precomputed character tables h_1\u2026h_c mapping characters to random hash values. A key x=(x_1\u2026x_c) is hashed to h_1x_1 (oplus) h_2x_2\u2026..(oplus) h_cx_c. This schemes is very fast with character tables in cache. While simple tabulation is not even 4-independent it does provide many of the guarantees that are normally obtained via higher independence e.g. linear probing and Cuckoo hashing. Next we consider twisted tabulation where one input character is twisted in a simple way. The resulting hash function has powerful distributional properties Chernoff-Hoeffding type tail bounds and a very small bias for min-wise hashing. This also yields an extremely fast pseudo-random number generator that is provably good for many classic randomized algorithms and data-structures. Finally we consider double tabulation where we compose two simple tabulation functions applying one to the output of the other and show that this yields very high independence in the classic framework of Carter and Wegman 1977. In fact w.h.p. for a given set of size proportional to that of the space consumed double tabulation gives fully-random hashing. We also mention some more elaborate tabulation schemes getting near-optimal independence for given time and space. While these tabulation schemes are all easy to implement and use their analysis is not.</p>\n", "tags": ["ARXIV", "Independent", "Survey Paper"], "tsne_embedding": [-20.162548065185547, -21.71454429626465]}, {"key": "thorup2015linear", "year": "2015", "title": "Linear Probing With 5-independent Hashing", "abstract": "<p>These lecture notes show that linear probing takes expected constant time if the hash function is 5-independent. This result was first proved by Pagh et al. STOC07SICOMP09. The simple proof here is essentially taken from Patrascu and Thorup ICALP10. We will also consider a smaller space version of linear probing that may have false positives like Bloom filters. These lecture notes illustrate the use of higher moments in data structures and could be used in a course on randomized algorithms.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-32.311805725097656, -18.241113662719727]}, {"key": "tian2017semi", "year": "2017", "title": "Semi-supervised Multimodal Hashing", "abstract": "<p>Retrieving nearest neighbors across correlated data in multiple modalities such as image-text pairs on Facebook and video-tag pairs on YouTube has become a challenging task due to the huge amount of data. Multimodal hashing methods that embed data into binary codes can boost the retrieving speed and reduce storage requirement. As unsupervised multimodal hashing methods are usually inferior to supervised ones while the supervised ones requires too much manually labeled data the proposed method in this paper utilizes a part of labels to design a semi-supervised multimodal hashing method. It first computes the transformation matrices for data matrices and label matrix. Then with these transformation matrices fuzzy logic is introduced to estimate a label matrix for unlabeled data. Finally it uses the estimated label matrix to learn hashing functions for data in each modality to generate a unified binary code matrix. Experiments show that the proposed semi-supervised method with 5037; labels can get a medium performance among the compared supervised ones and achieve an approximate performance to the best supervised method with 9037; labels. With only 1037; labels the proposed method can still compete with the worst compared supervised one.</p>\n", "tags": ["ARXIV", "Cross Modal", "Supervised"], "tsne_embedding": [-11.251985549926758, -7.983756065368652]}, {"key": "tian2018learning", "year": "2018", "title": "Learning Decorrelated Hashing Codes For Multimodal Retrieval", "abstract": "<p>In social networks heterogeneous multimedia data correlate to each other such as videos and their corresponding tags in YouTube and image-text pairs in Facebook. Nearest neighbor retrieval across multiple modalities on large data sets becomes a hot yet challenging problem. Hashing is expected to be an efficient solution since it represents data as binary codes. As the bit-wise XOR operations can be fast handled the retrieval time is greatly reduced. Few existing multimodal hashing methods consider the correlation among hashing bits. The correlation has negative impact on hashing codes. When the hashing code length becomes longer the retrieval performance improvement becomes slower. In this paper we propose a minimum correlation regularization (MCR) for multimodal hashing. First the sigmoid function is used to embed the data matrices. Then the MCR is applied on the output of sigmoid function. As the output of sigmoid function approximates a binary code matrix the proposed MCR can efficiently decorrelate the hashing codes. Experiments show the superiority of the proposed method becomes greater as the code length increases.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [-18.60484504699707, 4.677286624908447]}, {"key": "tian2019global", "year": "2019", "title": "Global Hashing System For Fast Image Search", "abstract": "<p>Hashing methods have been widely investigated for fast approximate nearest neighbor searching in large data sets. Most existing methods use binary vectors in lower dimensional spaces to represent data points that are usually real vectors of higher dimensionality. We divide the hashing process into two steps. Data points are first embedded in a low-dimensional space and the global positioning system method is subsequently introduced but modified for binary embedding. We devise dataindependent and data-dependent methods to distribute the satellites at appropriate locations. Our methods are based on finding the tradeoff between the information losses in these two steps. Experiments show that our data-dependent method outperforms other methods in different-sized data sets from 100k to 10M. By incorporating the orthogonality of the code matrix both our data-independent and data-dependent methods are particularly impressive in experiments on longer bits.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-24.486967086791992, 13.713030815124512]}, {"key": "tian2021one", "year": "2021", "title": "One Loss For All Deep Hashing With A Single Cosine Similarity Based Learning Objective", "abstract": "<p>A deep hashing model typically has two main learning objectives to make the learned binary hash codes discriminative and to minimize a quantization error. With further constraints such as bit balance and code orthogonality it is not uncommon for existing models to employ a large number (4) of losses. This leads to difficulties in model training and subsequently impedes their effectiveness. In this work we propose a novel deep hashing model with only (). Specifically we show that maximizing the cosine similarity between the continuous codes and their corresponding () can ensure both hash code discriminativeness and quantization error minimization. Further with this learning objective code balancing can be achieved by simply using a Batch Normalization (BN) layer and multi-label classification is also straightforward with label smoothing. The result is a one-loss deep hashing model that removes all the hassles of tuning the weights of various losses. Importantly extensive experiments show that our model is highly effective outperforming the state-of-the-art multi-loss hashing models on three large-scale instance retrieval benchmarks often by significant margins.</p>\n", "tags": ["NEURIPS", "Quantisation", "Supervised"], "tsne_embedding": [2.757333278656006, -2.960740804672241]}, {"key": "tiny2008million", "year": "2008", "title": "80 million tiny images: a large dataset for non-parametric object and scene recognition", "abstract": "<p>With the advent of the Internet, billions of images\nare now freely available online and constitute a dense sampling\nof the visual world. Using a variety of non-parametric methods,\nwe explore this world with the aid of a large dataset of 79,302,017\nimages collected from the Web. Motivated by psychophysical\nresults showing the remarkable tolerance of the human visual\nsystem to degradations in image resolution, the images in the\ndataset are stored as 32 \u00d7 32 color images. Each image is\nloosely labeled with one of the 75,062 non-abstract nouns in\nEnglish, as listed in the Wordnet lexical database. Hence the\nimage database gives a comprehensive coverage of all object\ncategories and scenes. The semantic information from Wordnet\ncan be used in conjunction with nearest-neighbor methods to\nperform object classification over a range of semantic levels\nminimizing the effects of labeling noise. For certain classes that\nare particularly prevalent in the dataset, such as people, we are\nable to demonstrate a recognition performance comparable to\nclass-specific Viola-Jones style detectors.</p>\n", "tags": [], "tsne_embedding": [8.31982421875, 21.120309829711914]}, {"key": "tito2017hash", "year": "2017", "title": "Hash Embeddings For Efficient Word Representations", "abstract": "<p>We present hash embeddings an efficient method for representing words in a continuous vector form. A hash embedding may be seen as an interpolation between a standard word embedding and a word embedding created using a random hash function (the hashing trick). In hash embeddings each token is represented by (k) (d)-dimensional embeddings vectors and one (k) dimensional weight vector. The final (d) dimensional representation of the token is the product of the two. Rather than fitting the embedding vectors for each token these are selected by the hashing trick from a shared pool of (B) embedding vectors. Our experiments show that hash embeddings can easily deal with huge vocabularies consisting of millions tokens. When using a hash embedding there is no need to create a dictionary before training nor to perform any kind of vocabulary pruning after training. We show that models trained using hash embeddings exhibit at least the same level of performance as models trained using regular embeddings across a wide range of tasks. Furthermore the number of parameters needed by such an embedding is only a fraction of what is required by a regular embedding. Since standard embeddings and embeddings constructed using the hashing trick are actually just special cases of a hash embedding hash embeddings can be considered an extension and improvement over the existing regular embedding types.</p>\n", "tags": ["Independent", "NEURIPS"], "tsne_embedding": [-15.275712013244629, -20.79867935180664]}, {"key": "touvron2023llama", "year": "2023", "title": "Llama Open And Efficient Foundation Language Models", "abstract": "<p>We introduce LLaMA a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens and show that it is possible to train state-of-the-art models using publicly available datasets exclusively without resorting to proprietary and inaccessible datasets. In particular LLaMA-13B outperforms GPT-3 (175B) on most benchmarks and LLaMA-65B is competitive with the best models Chinchilla-70B and PaLM-540B. We release all our models to the research community.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [21.233518600463867, 5.52611780166626]}, {"key": "tu2018object", "year": "2018", "title": "Object Detection Based Deep Unsupervised Hashing", "abstract": "<p>Recently similarity-preserving hashing methods have been extensively studied for large-scale image retrieval. Compared with unsupervised hashing supervised hashing methods for labeled data have usually better performance by utilizing semantic label information. Intuitively for unlabeled data it will improve the performance of unsupervised hashing methods if we can first mine some supervised semantic label information from unlabeled data and then incorporate the label information into the training process. Thus in this paper we propose a novel Object Detection based Deep Unsupervised Hashing method (ODDUH). Specifically a pre-trained object detection model is utilized to mining supervised label information which is used to guide the learning process to generate high-quality hash codes.Extensive experiments on two public datasets demonstrate that the proposed method outperforms the state-of-the-art unsupervised hashing methods in the image retrieval task.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Unsupervised"], "tsne_embedding": [-3.6690144538879395, 7.039222240447998]}, {"key": "tu2019deep", "year": "2019", "title": "Deep Cross-modal Hashing With Hashing Functions And Unified Hash Codes Jointly Learning", "abstract": "<p>Due to their high retrieval efficiency and low storage cost cross-modal hashing methods have attracted considerable attention. Generally compared with shallow cross-modal hashing methods deep cross-modal hashing methods can achieve a more satisfactory performance by integrating feature learning and hash codes optimizing into a same framework. However most existing deep cross-modal hashing methods either cannot learn a unified hash code for the two correlated data-points of different modalities in a database instance or cannot guide the learning of unified hash codes by the feedback of hashing function learning procedure to enhance the retrieval accuracy. To address the issues above in this paper we propose a novel end-to-end Deep Cross-Modal Hashing with Hashing Functions and Unified Hash Codes Jointly Learning (DCHUC). Specifically by an iterative optimization algorithm DCHUC jointly learns unified hash codes for image-text pairs in a database and a pair of hash functions for unseen query image-text pairs. With the iterative optimization algorithm the learned unified hash codes can be used to guide the hashing function learning procedure; Meanwhile the learned hashing functions can feedback to guide the unified hash codes optimizing procedure. Extensive experiments on three public datasets demonstrate that the proposed method outperforms the state-of-the-art cross-modal hashing methods.</p>\n", "tags": ["ARXIV", "Cross Modal", "Independent"], "tsne_embedding": [-2.8005576133728027, 0.4747141897678375]}, {"key": "tu2020deep", "year": "2020", "title": "Deep Cross-modal Hashing Via Margin-dynamic-softmax Loss", "abstract": "<p>Due to their high retrieval efficiency and low storage cost for cross-modal search task cross-modal hashing methods have attracted considerable attention. For the supervised cross-modal hashing methods how to make the learned hash codes preserve semantic information sufficiently contained in the label of datapoints is the key to further enhance the retrieval performance. Hence almost all supervised cross-modal hashing methods usually depends on defining a similarity between datapoints with the label information to guide the hashing model learning fully or partly. However the defined similarity between datapoints can only capture the label information of datapoints partially and misses abundant semantic information then hinders the further improvement of retrieval performance. Thus in this paper different from previous works we propose a novel cross-modal hashing method without defining the similarity between datapoints called Deep Cross-modal Hashing via (textit)Margin-dynamic-softmax Loss (DCHML). Specifically DCHML first trains a proxy hashing network to transform each category information of a dataset into a semantic discriminative hash code called proxy hash code. Each proxy hash code can preserve the semantic information of its corresponding category well. Next without defining the similarity between datapoints to supervise the training process of the modality-specific hashing networks we propose a novel (textit)margin-dynamic-softmax loss to directly utilize the proxy hashing codes as supervised information. Finally by minimizing the novel (textit)margin-dynamic-softmax loss the modality-specific hashing networks can be trained to generate hash codes which can simultaneously preserve the cross-modal similarity and abundant semantic information well.</p>\n", "tags": ["ARXIV", "Cross Modal", "Supervised"], "tsne_embedding": [-7.346450328826904, 0.3704027831554413]}, {"key": "tu2022unsupervised", "year": "2022", "title": "Unsupervised Hashing With Semantic Concept Mining", "abstract": "<p>Recently to improve the unsupervised image retrieval performance plenty of unsupervised hashing methods have been proposed by designing a semantic similarity matrix which is based on the similarities between image features extracted by a pre-trained CNN model. However most of these methods tend to ignore high-level abstract semantic concepts contained in images. Intuitively concepts play an important role in calculating the similarity among images. In real-world scenarios each image is associated with some concepts and the similarity between two images will be larger if they share more identical concepts. Inspired by the above intuition in this work we propose a novel Unsupervised Hashing with Semantic Concept Mining called UHSCM which leverages a VLP model to construct a high-quality similarity matrix. Specifically a set of randomly chosen concepts is first collected. Then by employing a vision-language pretraining (VLP) model with the prompt engineering which has shown strong power in visual representation learning the set of concepts is denoised according to the training images. Next the proposed method UHSCM applies the VLP model with prompting again to mine the concept distribution of each image and construct a high-quality semantic similarity matrix based on the mined concept distributions. Finally with the semantic similarity matrix as guiding information a novel hashing loss with a modified contrastive loss based regularization item is proposed to optimize the hashing network. Extensive experiments on three benchmark datasets show that the proposed method outperforms the state-of-the-art baselines in the image retrieval task.</p>\n", "tags": ["ARXIV", "CNN", "Cross Modal", "Image Retrieval", "Unsupervised"], "tsne_embedding": [-2.991781234741211, 14.80865478515625]}, {"key": "valsesia2019analysis", "year": "2019", "title": "Analysis Of Sparsehash An Efficient Embedding Of Set-similarity Via Sparse Projections", "abstract": "<p>Embeddings provide compact representations of signals in order to perform efficient inference in a wide variety of tasks. In particular random projections are common tools to construct Euclidean distance-preserving embeddings while hashing techniques are extensively used to embed set-similarity metrics such as the Jaccard coefficient. In this letter we theoretically prove that a class of random projections based on sparse matrices called SparseHash can preserve the Jaccard coefficient between the supports of sparse signals which can be used to estimate set similarities. Moreover besides the analysis we provide an efficient implementation and we test the performance in several numerical experiments both on synthetic and real datasets.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-26.04302215576172, 7.369321823120117]}, {"key": "van2015image", "year": "2015", "title": "Image Retrieval Based On Binary Signature Ang S-kgraph", "abstract": "<p>In this paper we introduce an optimum approach for querying similar images on large digital-image databases. Our work is based on RBIR (region-based image retrieval) method which uses multiple regions as the key to retrieval images. This method significantly improves the accuracy of queries. However this also increases the cost of computing. To reduce this expensive computational cost we implement binary signature encoder which maps an image to its identification in binary. In order to fasten the lookup binary signatures of images are classified by the help of S-kGraph. Finally our work is evaluated on CORELs images.</p>\n", "tags": ["Graph", "Image Retrieval"], "tsne_embedding": [-19.20208168029785, 18.487350463867188]}, {"key": "vanblokland2020indexing", "year": "2020", "title": "An Indexing Scheme And Descriptor For 3D Object Retrieval Based On Local Shape Querying", "abstract": "<p>A binary descriptor indexing scheme based on Hamming distance called the Hamming tree for local shape queries is presented. A new binary clutter resistant descriptor named Quick Intersection Count Change Image (QUICCI) is also introduced. This local shape descriptor is extremely small and fast to compare. Additionally a novel distance function called Weighted Hamming applicable to QUICCI images is proposed for retrieval applications. The effectiveness of the indexing scheme and QUICCI is demonstrated on 828 million QUICCI images derived from the SHREC2017 dataset while the clutter resistance of QUICCI is shown using the clutterbox experiment.</p>\n", "tags": [], "tsne_embedding": [-11.056961059570312, 22.4394474029541]}, {"key": "vanblokland2021partial", "year": "2021", "title": "Partial 3D Object Retrieval Using Local Binary QUICCI Descriptors And Dissimilarity Tree Indexing", "abstract": "<p>A complete pipeline is presented for accurate and efficient partial 3D object retrieval based on Quick Intersection Count Change Image (QUICCI) binary local descriptors and a novel indexing tree. It is shown how a modification to the QUICCI query descriptor makes it ideal for partial retrieval. An indexing structure called Dissimilarity Tree is proposed which can significantly accelerate searching the large space of local descriptors; this is applicable to QUICCI and other binary descriptors. The index exploits the distribution of bits within descriptors for efficient retrieval. The retrieval pipeline is tested on the artificial part of SHREC16 dataset with near-ideal retrieval results.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-12.82665729522705, 22.124710083007812]}, {"key": "vaswani2017attention", "year": "2017", "title": "Attention Is All You Need", "abstract": "<p>The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture the Transformer based solely on attention mechanisms dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task improving over the existing best results including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [18.69598960876465, -8.4694242477417]}, {"key": "vats2024exploring", "year": "2024", "title": "Exploring The Impact Of Large Language Models On Recommender Systems An Extensive Review", "abstract": "<p>The paper underscores the significance of Large Language Models (LLMs) in reshaping recommender systems attributing their value to unique reasoning abilities absent in traditional recommenders. Unlike conventional systems lacking direct user interaction data LLMs exhibit exceptional proficiency in recommending items showcasing their adeptness in comprehending intricacies of language. This marks a fundamental paradigm shift in the realm of recommendations. Amidst the dynamic research landscape researchers actively harness the language comprehension and generation capabilities of LLMs to redefine the foundations of recommendation tasks. The investigation thoroughly explores the inherent strengths of LLMs within recommendation frameworks encompassing nuanced contextual comprehension seamless transitions across diverse domains adoption of unified approaches holistic learning strategies leveraging shared data reservoirs transparent decision-making and iterative improvements. Despite their transformative potential challenges persist including sensitivity to input prompts occasional misinterpretations and unforeseen recommendations necessitating continuous refinement and evolution in LLM-driven recommender systems.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [38.33356475830078, -9.133184432983398]}, {"key": "venkateswara2017deep", "year": "2017", "title": "Deep Hashing Network For Unsupervised Domain Adaptation", "abstract": "<p>In recent years deep neural networks have emerged as a dominant machine learning tool for a wide variety of application domains. However training a deep neural network requires a large amount of labeled data which is an expensive process in terms of time labor and human expertise. Domain adaptation or transfer learning algorithms address this challenge by leveraging labeled data in a different but related source domain to develop a model for the target domain. Further the explosive growth of digital data has posed a fundamental challenge concerning its storage and retrieval. Due to its storage and retrieval efficiency recent years have witnessed a wide application of hashing in a variety of computer vision applications. In this paper we first introduce a new dataset Office-Home to evaluate domain adaptation algorithms. The dataset contains images of a variety of everyday objects from multiple domains. We then propose a novel deep learning framework that can exploit labeled source data and unlabeled target data to learn informative hash codes to accurately classify unseen target data. To the best of our knowledge this is the first research effort to exploit the feature learning capabilities of deep neural networks to learn representative hash codes to address the domain adaptation problem. Our extensive empirical studies on multiple transfer tasks corroborate the usefulness of the framework in learning efficient hash codes which outperform existing competitive baselines for unsupervised domain adaptation.</p>\n", "tags": ["ARXIV", "Deep Learning", "Unsupervised"], "tsne_embedding": [7.856091022491455, 1.326190710067749]}, {"key": "verdoliva2015reliable", "year": "2015", "title": "A Reliable Order-statistics-based Approximate Nearest Neighbor Search Algorithm", "abstract": "<p>We propose a new algorithm for fast approximate nearest neighbor search based on the properties of ordered vectors. Data vectors are classified based on the index and sign of their largest components thereby partitioning the space in a number of cones centered in the origin. The query is itself classified and the search starts from the selected cone and proceeds to neighboring ones. Overall the proposed algorithm corresponds to locality sensitive hashing in the space of directions with hashing based on the order of components. Thanks to the statistical features emerging through ordering it deals very well with the challenging case of unstructured data and is a valuable building block for more complex techniques dealing with structured data. Experiments on both simulated and real-world data prove the proposed algorithm to provide a state-of-the-art performance.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-26.327796936035156, 6.583993911743164]}, {"key": "vuli\u01072020probing", "year": "2020", "title": "Probing Pretrained Language Models For Lexical Semantics", "abstract": "<p>The success of large pretrained language models (LMs) such as BERT and RoBERTa has sparked interest in probing their representations in order to unveil what types of knowledge they implicitly capture. While prior research focused on morphosyntactic semantic and world knowledge it remains unclear to which extent LMs also derive lexical type-level knowledge from words in context. In this work we present a systematic empirical analysis across six typologically diverse languages and five different lexical tasks addressing the following questions 1) How do different lexical knowledge extraction strategies (monolingual versus multilingual source LM out-of-context versus in-context encoding inclusion of special tokens and layer-wise averaging) impact performance How consistent are the observed effects across tasks and languages 2) Is lexical knowledge stored in few parameters or is it scattered throughout the network 3) How do these representations fare against traditional static word vectors in lexical tasks 4) Does the lexical information emerging from independently trained monolingual LMs display latent similarities Our main results indicate patterns and best practices that hold universally but also point to prominent variations across languages and tasks. Moreover we validate the claim that lower Transformer layers carry more type-level lexical knowledge but also show that this knowledge is distributed across multiple layers.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [35.01673889160156, 9.196746826171875]}, {"key": "w2021scaling", "year": "2021", "title": "Scaling Language Models Methods Analysis Insights From Training Gopher", "abstract": "<p>Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper we present an analysis of Transformer-based language model performance across a wide range of model scales \u2013 from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension fact-checking and the identification of toxic language but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and models behaviour covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [40.02999496459961, -0.6672358512878418]}, {"key": "wang2010semisupervised", "year": "2010", "title": "Semi-supervised hashing for scalable image retrieval", "abstract": "<p>Large scale image search has recently attracted considerable\nattention due to easy availability of huge amounts of\ndata. Several hashing methods have been proposed to allow\napproximate but highly efficient search. Unsupervised\nhashing methods show good performance with metric distances\nbut, in image search, semantic similarity is usually\ngiven in terms of labeled pairs of images. There exist supervised\nhashing methods that can handle such semantic similarity\nbut they are prone to overfitting when labeled data\nis small or noisy. Moreover, these methods are usually very\nslow to train. In this work, we propose a semi-supervised\nhashing method that is formulated as minimizing empirical\nerror on the labeled data while maximizing variance\nand independence of hash bits over the labeled and unlabeled\ndata. The proposed method can handle both metric as\nwell as semantic similarity. The experimental results on two\nlarge datasets (up to one million samples) demonstrate its\nsuperior performance over state-of-the-art supervised and\nunsupervised methods.</p>\n", "tags": ["CVPR", "Image Retrieval", "Semi Supervised"], "tsne_embedding": [-15.870375633239746, 10.618355751037598]}, {"key": "wang2010sequential", "year": "2010", "title": "Sequential projection learning for hashing with compact codes", "abstract": "<p>Hashing based Approximate Nearest Neighbor\n(ANN) search has attracted much attention\ndue to its fast query time and drastically\nreduced storage. However, most of the hashing\nmethods either use random projections or\nextract principal directions from the data to\nderive hash functions. The resulting embedding\nsuffers from poor discrimination when\ncompact codes are used. In this paper, we\npropose a novel data-dependent projection\nlearning method such that each hash function\nis designed to correct the errors made by\nthe previous one sequentially. The proposed\nmethod easily adapts to both unsupervised\nand semi-supervised scenarios and shows significant\nperformance gains over the state-ofthe-art\nmethods on two large datasets containing\nup to 1 million points.</p>\n", "tags": ["ICML", "Semi Supervised"], "tsne_embedding": [-23.809743881225586, -7.860633373260498]}, {"key": "wang2014geometric", "year": "2014", "title": "Geometric VLAD For Large Scale Image Search", "abstract": "<p>We present a novel compact image descriptor for large scale image search. Our proposed descriptor - Geometric VLAD (gVLAD) is an extension of VLAD (Vector of Locally Aggregated Descriptors) that incorporates weak geometry information into the VLAD framework. The proposed geometry cues are derived as a membership function over keypoint angles which contain evident and informative information but yet often discarded. A principled technique for learning the membership function by clustering angles is also presented. Further to address the overhead of iterative codebook training over real-time datasets a novel codebook adaptation strategy is outlined. Finally we demonstrate the efficacy of proposed gVLAD based retrieval framework where we achieve more than 1537; improvement in mAP over existing benchmarks.</p>\n", "tags": ["ARXIV", "Unsupervised"], "tsne_embedding": [-7.818047523498535, 24.745601654052734]}, {"key": "wang2014hashing", "year": "2014", "title": "Hashing For Similarity Search A Survey", "abstract": "<p>Similarity search (nearest neighbor search) is a problem of pursuing the data items whose distances to a query item are the smallest from a large database. Various methods have been developed to address this problem and recently a lot of efforts have been devoted to approximate search. In this paper we present a survey on one of the main solutions hashing which has been widely studied since the pioneering work locality sensitive hashing. We divide the hashing algorithms two main categories locality sensitive hashing which designs hash functions without exploring the data distribution and learning to hash which learns hash functions according the data distribution and review them from various aspects including hash function design and distance measure and search scheme in the hash coding space.</p>\n", "tags": ["ARXIV", "Independent", "Survey Paper"], "tsne_embedding": [-15.073216438293457, -8.371417045593262]}, {"key": "wang2015hamming", "year": "2015", "title": "Hamming Compatible Quantization for Hashing", "abstract": "<p>Hashing is one of the effective techniques for fast\nApproximate Nearest Neighbour (ANN) search.\nTraditional single-bit quantization (SBQ) in most\nhashing methods incurs lots of quantization error\nwhich seriously degrades the search performance.\nTo address the limitation of SBQ, researchers have\nproposed promising multi-bit quantization (MBQ)\nmethods to quantize each projection dimension\nwith multiple bits. However, some MBQ methods\nneed to adopt specific distance for binary code\nmatching instead of the original Hamming distance,\nwhich would significantly decrease the retrieval\nspeed. Two typical MBQ methods Hierarchical\nQuantization and Double Bit Quantization\nretain the Hamming distance, but both of them only\nconsider the projection dimensions during quantization,\nignoring the neighborhood structure of raw\ndata inherent in Euclidean space. In this paper,\nwe propose a multi-bit quantization method named\nHamming Compatible Quantization (HCQ) to preserve\nthe capability of similarity metric between\nEuclidean space and Hamming space by utilizing\nthe neighborhood structure of raw data. Extensive\nexperiment results have shown our approach significantly\nimproves the performance of various stateof-the-art\nhashing methods while maintaining fast\nretrieval speed.</p>\n\n", "tags": [], "tsne_embedding": [-32.19342041015625, 10.698694229125977]}, {"key": "wang2015learning", "year": "2015", "title": "Learning To Hash For Indexing Big Data - A Survey", "abstract": "<p>The explosive growth in big data has attracted much attention in designing efficient indexing and search methods recently. In many critical applications such as large-scale search and pattern matching finding the nearest neighbors to a query is a fundamental research problem. However the straightforward solution using exhaustive comparison is infeasible due to the prohibitive computational complexity and memory requirement. In response Approximate Nearest Neighbor (ANN) search based on hashing techniques has become popular due to its promising performance in both efficiency and accuracy. Prior randomized hashing methods e.g. Locality-Sensitive Hashing (LSH) explore data-independent hash functions with random projections or permutations. Although having elegant theoretic guarantees on the search quality in certain metric spaces performance of randomized hashing has been shown insufficient in many real-world applications. As a remedy new approaches incorporating data-driven learning methods in development of advanced hash functions have emerged. Such learning to hash methods exploit information such as data distributions or class labels when optimizing the hash codes or functions. Importantly the learned hash codes are able to preserve the proximity of neighboring data in the original feature spaces in the hash code spaces. The goal of this paper is to provide readers with systematic understanding of insights pros and cons of the emerging techniques. We provide a comprehensive survey of the learning to hash framework and representative techniques of various types including unsupervised semi-supervised and supervised. In addition we also summarize recent hashing approaches utilizing the deep learning models. Finally we discuss the future direction and trends of research in this area.</p>\n", "tags": ["ARXIV", "Deep Learning", "LSH", "Survey Paper", "Unsupervised"], "tsne_embedding": [6.997706890106201, -4.274274826049805]}, {"key": "wang2015semantic", "year": "2015", "title": "Semantic Topic Multimodal Hashing for Cross-Media Retrieval", "abstract": "<p>Multimodal hashing is essential to cross-media\nsimilarity search for its low storage cost and fast\nquery speed. Most existing multimodal hashing\nmethods embedded heterogeneous data into a common low-dimensional Hamming space, and then\nrounded the continuous embeddings to obtain the\nbinary codes. Yet they usually neglect the inherent discrete nature of hashing for relaxing the discrete constraints, which will cause degraded retrieval performance especially for long codes. For\nthis purpose, a novel Semantic Topic Multimodal\nHashing (STMH) is developed by considering latent semantic information in coding procedure.\nIt\nfirst discovers clustering patterns of texts and robust factorizes the matrix of images to obtain multiple semantic topics of texts and concepts of images.\nThen the learned multimodal semantic features are\ntransformed into a common subspace by their correlations. Finally, each bit of unified hash code\ncan be generated directly by figuring out whether a\ntopic or concept is contained in a text or an image.\nTherefore, the obtained model by STMH is more\nsuitable for hashing scheme as it directly learns discrete hash codes in the coding process. Experimental results demonstrate that the proposed method\noutperforms several state-of-the-art methods.</p>\n", "tags": ["Cross Modal", "IJCAI"], "tsne_embedding": [-9.28547191619873, -1.6304115056991577]}, {"key": "wang2016affinity", "year": "2016", "title": "Affinity Preserving Quantization for Hashing: A Vector Quantization Approach to Learning Compact Binary Codes", "abstract": "<p>Hashing techniques are powerful for approximate nearest\nneighbour (ANN) search. Existing quantization methods in\nhashing are all focused on scalar quantization (SQ) which\nis inferior in utilizing the inherent data distribution. In this\npaper, we propose a novel vector quantization (VQ) method\nnamed affinity preserving quantization (APQ) to improve the\nquantization quality of projection values, which has significantly\nboosted the performance of state-of-the-art hashing\ntechniques. In particular, our method incorporates the neighbourhood\nstructure in the pre- and post-projection data space\ninto vector quantization. APQ minimizes the quantization errors\nof projection values as well as the loss of affinity property\nof original space. An effective algorithm has been proposed\nto solve the joint optimization problem in APQ, and\nthe extension to larger binary codes has been resolved by applying\nproduct quantization to APQ. Extensive experiments\nhave shown that APQ consistently outperforms the state-of-the-art\nquantization methods, and has significantly improved\nthe performance of various hashing techniques.</p>\n", "tags": ["AAAI", "Quantisation"], "tsne_embedding": [-30.056385040283203, 2.3249073028564453]}, {"key": "wang2016algorithm", "year": "2016", "title": "An Algorithm For L1 Nearest Neighbor Search Via Monotonic Embedding", "abstract": "<p>Fast algorithms for nearest neighbor (NN) search have in large part focused on L2 distance. Here we develop an approach for L1 distance that begins with an explicit and exact embedding of the points into L2. We show how this embedding can efficiently be combined with random projection methods for L2 NN search such as locality-sensitive hashing or random projection trees. We rigorously establish the correctness of the methodology and show by experimentation that it is competitive in practice with available alternatives.</p>\n", "tags": ["Independent", "NEURIPS"], "tsne_embedding": [-27.28424072265625, 6.151080131530762]}, {"key": "wang2016deep", "year": "2016", "title": "Deep Supervised Hashing With Triplet Labels", "abstract": "<p>Hashing is one of the most popular and powerful approximate nearest neighbor search techniques for large-scale image retrieval. Most traditional hashing methods first represent images as off-the-shelf visual features and then produce hashing codes in a separate stage. However off-the-shelf visual features may not be optimally compatible with the hash code learning procedure which may result in sub-optimal hash codes. Recently deep hashing methods have been proposed to simultaneously learn image features and hash codes using deep neural networks and have shown superior performance over traditional hashing methods. Most deep hashing methods are given supervised information in the form of pairwise labels or triplet labels. The current state-of-the-art deep hashing method DPSH~citeli2015feature which is based on pairwise labels performs image feature learning and hash code learning simultaneously by maximizing the likelihood of pairwise similarities. Inspired by DPSH~citeli2015feature we propose a triplet label based deep hashing method which aims to maximize the likelihood of the given triplet labels. Experimental results show that our method outperforms all the baselines on CIFAR-10 and NUS-WIDE datasets including the state-of-the-art method DPSH~citeli2015feature and all the previous triplet label based deep hashing methods.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Supervised"], "tsne_embedding": [-12.086309432983398, 9.08113956451416]}, {"key": "wang2016survey", "year": "2016", "title": "A Survey On Learning To Hash", "abstract": "<p>Nearest neighbor search is a problem of finding the data points from the database such that the distances from them to the query point are the smallest. Learning to hash is one of the major solutions to this problem and has been widely studied recently. In this paper we present a comprehensive survey of the learning to hash algorithms categorize them according to the manners of preserving the similarities into pairwise similarity preserving multiwise similarity preserving implicit similarity preserving as well as quantization and discuss their relations. We separate quantization from pairwise similarity preserving as the objective function is very different though quantization as we show can be derived from preserving the pairwise similarities. In addition we present the evaluation protocols and the general performance analysis and point out that the quantization algorithms perform superiorly in terms of search accuracy search time cost and space cost. Finally we introduce a few emerging topics.</p>\n", "tags": ["ARXIV", "Quantisation", "Survey Paper"], "tsne_embedding": [-14.779691696166992, -8.351299285888672]}, {"key": "wang2016unsupervised", "year": "2016", "title": "Unsupervised Cross-media Hashing With Structure Preservation", "abstract": "<p>Recent years have seen the exponential growth of heterogeneous multimedia data. The need for effective and accurate data retrieval from heterogeneous data sources has attracted much research interest in cross-media retrieval. Here given a query of any media type cross-media retrieval seeks to find relevant results of different media types from heterogeneous data sources. To facilitate large-scale cross-media retrieval we propose a novel unsupervised cross-media hashing method. Our method incorporates local affinity and distance repulsion constraints into a matrix factorization framework. Correspondingly the proposed method learns hash functions that generates unified hash codes from different media types while ensuring intrinsic geometric structure of the data distribution is preserved. These hash codes empower the similarity between data of different media types to be evaluated directly. Experimental results on two large-scale multimedia datasets demonstrate the effectiveness of the proposed method where we outperform the state-of-the-art methods.</p>\n", "tags": ["ARXIV", "Cross Modal", "Unsupervised"], "tsne_embedding": [-15.35181999206543, -4.265684127807617]}, {"key": "wang2017supervised", "year": "2017", "title": "Supervised Deep Hashing For Hierarchical Labeled Data", "abstract": "<p>Recently hashing methods have been widely used in large-scale image retrieval. However most existing hashing methods did not consider the hierarchical relation of labels which means that they ignored the rich information stored in the hierarchy. Moreover most of previous works treat each bit in a hash code equally which does not meet the scenario of hierarchical labeled data. In this paper we propose a novel deep hashing method called supervised hierarchical deep hashing (SHDH) to perform hash code learning for hierarchical labeled data. Specifically we define a novel similarity formula for hierarchical labeled data by weighting each layer and design a deep convolutional neural network to obtain a hash code for each data point. Extensive experiments on several real-world public datasets show that the proposed method outperforms the state-of-the-art baselines in the image retrieval task.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Supervised"], "tsne_embedding": [1.3480255603790283, -21.928226470947266]}, {"key": "wang2017survey", "year": "2017", "title": "A Survey on Learning to Hash", "abstract": "<p>Nearest neighbor search is a problem of finding the data points from the database such that the distances from them to the\nquery point are the smallest. Learning to hash is one of the major solutions to this problem and has been widely studied recently. In this\npaper, we present a comprehensive survey of the learning to hash algorithms, categorize them according to the manners of preserving\nthe similarities into: pairwise similarity preserving, multiwise similarity preserving, implicit similarity preserving, as well as quantization,\nand discuss their relations. We separate quantization from pairwise similarity preserving as the objective function is very different\nthough quantization, as we show, can be derived from preserving the pairwise similarities. In addition, we present the evaluation\nprotocols, and the general performance analysis, and point out that the quantization algori</p>\n", "tags": ["Image Retrieval", "Survey Paper"], "tsne_embedding": [-14.943493843078613, -8.13750171661377]}, {"key": "wang2019cluster", "year": "2019", "title": "Cluster-wise Unsupervised Hashing For Cross-modal Similarity Search", "abstract": "<p>Large-scale cross-modal hashing similarity retrieval has attracted more and more attention in modern search applications such as search engines and autopilot showing great superiority in computation and storage. However current unsupervised cross-modal hashing methods still have some limitations (1)many methods relax the discrete constraints to solve the optimization objective which may significantly degrade the retrieval performance;(2)most existing hashing model project heterogenous data into a common latent space which may always lose sight of diversity in heterogenous data;(3)transforming real-valued data point to binary codes always results in abundant loss of information producing the suboptimal continuous latent space. To overcome above problems in this paper a novel Cluster-wise Unsupervised Hashing (CUH) method is proposed. Specifically CUH jointly performs the multi-view clustering that projects the original data points from different modalities into its own low-dimensional latent semantic space and finds the cluster centroid points and the common clustering indicators in its own low-dimensional space and learns the compact hash codes and the corresponding linear hash functions. An discrete optimization framework is developed to learn the unified binary codes across modalities under the guidance cluster-wise code-prototypes. The reasonableness and effectiveness of CUH is well demonstrated by comprehensive experiments on diverse benchmark datasets.</p>\n", "tags": ["ARXIV", "Cross Modal", "Unsupervised"], "tsne_embedding": [-13.560090065002441, -2.878037929534912]}, {"key": "wang2019deep", "year": "2019", "title": "Deep Policy Hashing Network With Listwise Supervision", "abstract": "<p>Deep-networks-based hashing has become a leading approach for large-scale image retrieval which learns a similarity-preserving network to map similar images to nearby hash codes. The pairwise and triplet losses are two widely used similarity preserving manners for deep hashing. These manners ignore the fact that hashing is a prediction task on the list of binary codes. However learning deep hashing with listwise supervision is challenging in 1) how to obtain the rank list of whole training set when the batch size of the deep network is always small and 2) how to utilize the listwise supervision. In this paper we present a novel deep policy hashing architecture with two systems are learned in parallel a query network and a shared and slowly changing database network. The following three steps are repeated until convergence 1) the database network encodes all training samples into binary codes to obtain a whole rank list 2) the query network is trained based on policy learning to maximize a reward that indicates the performance of the whole ranking list of binary codes e.g. mean average precision (MAP) and 3) the database network is updated as the query network. Extensive evaluations on several benchmark datasets show that the proposed method brings substantial improvements over state-of-the-art hashing methods.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Independent"], "tsne_embedding": [4.476651668548584, 5.9864182472229]}, {"key": "wang2019fusion", "year": "2019", "title": "Fusion-supervised Deep Cross-modal Hashing", "abstract": "<p>Deep hashing has recently received attention in cross-modal retrieval for its impressive advantages. However existing hashing methods for cross-modal retrieval cannot fully capture the heterogeneous multi-modal correlation and exploit the semantic information. In this paper we propose a novel emphFusion-supervised Deep Cross-modal Hashing (FDCH) approach. Firstly FDCH learns unified binary codes through a fusion hash network with paired samples as input which effectively enhances the modeling of the correlation of heterogeneous multi-modal data. Then these high-quality unified hash codes further supervise the training of the modality-specific hash networks for encoding out-of-sample queries. Meanwhile both pair-wise similarity information and classification information are embedded in the hash networks under one stream framework which simultaneously preserves cross-modal similarity and keeps semantic consistency. Experimental results on two benchmark datasets demonstrate the state-of-the-art performance of FDCH.</p>\n", "tags": ["ARXIV", "Cross Modal", "Supervised"], "tsne_embedding": [-12.107863426208496, 3.0782737731933594]}, {"key": "wang2019semi", "year": "2019", "title": "Semi-supervised Deep Quantization for Cross-modal Search", "abstract": "<p>The problem of cross-modal similarity search, which aims at making efficient and accurate queries across multiple domains, has become a significant and important research topic. Composite quantization, a compact coding solution superior to hashing techniques, has shown its effectiveness for similarity search. However, most existing works utilizing composite quantization to search multi-domain content only consider either pairwise similarity information or class label information across different domains, which fails to tackle the semi-supervised problem in composite quantization. In this paper, we address the semi-supervised quantization problem by considering: (i) pairwise similarity information (without class label information) across different domains, which captures the intra-document relation, (ii) cross-domain data with class label which can help capture inter-document relation, and (iii) cross-domain data with neither pairwise similarity nor class label which enables the full use of abundant unlabelled information. To the best of our knowledge, we are the first to consider both supervised information (pairwise similarity + class label) and unsupervised information (neither pairwise similarity nor class label) simultaneously in composite quantization. A challenging problem arises: how can we jointly handle these three sorts of information across multiple domains in an efficient way? To tackle this challenge, we propose a novel semi-supervised deep quantization (SSDQ) model that takes both supervised and unsupervised information into account. The proposed SSDQ model is capable of incorporating the above three kinds of information into one single framework when utilizing composite quantization for accurate and efficient queries across different domains. More specifically, we employ a modified deep autoencoder for better latent representation and formulate pairwise similarity loss, supervised quantization loss as well as unsupervised distribution match loss to handle all three types of information. The extensive experiments demonstrate the significant improvement of SSDQ over several state-of-the-art methods on various datasets.</p>\n", "tags": ["Cross Modal", "Deep Learning", "MM", "Quantisation", "Semi Supervised"], "tsne_embedding": [-13.479496955871582, -0.5747752785682678]}, {"key": "wang2019supervised", "year": "2019", "title": "Supervised Quantization For Similarity Search", "abstract": "<p>In this paper we address the problem of searching for semantically similar images from a large database. We present a compact coding approach supervised quantization. Our approach simultaneously learns feature selection that linearly transforms the database points into a low-dimensional discriminative subspace and quantizes the data points in the transformed space. The optimization criterion is that the quantized points not only approximate the transformed points accurately but also are semantically separable the points belonging to a class lie in a cluster that is not overlapped with other clusters corresponding to other classes which is formulated as a classification problem. The experiments on several standard datasets show the superiority of our approach over the state-of-the art supervised hashing and unsupervised quantization algorithms.</p>\n", "tags": ["ARXIV", "Quantisation", "Supervised"], "tsne_embedding": [-21.391563415527344, 5.867271900177002]}, {"key": "wang2020asymmetric", "year": "2020", "title": "Asymmetric Correlation Quantization Hashing For Cross-modal Retrieval", "abstract": "<p>Due to the superiority in similarity computation and database storage for large-scale multiple modalities data cross-modal hashing methods have attracted extensive attention in similarity retrieval across the heterogeneous modalities. However there are still some limitations to be further taken into account (1) most current CMH methods transform real-valued data points into discrete compact binary codes under the binary constraints limiting the capability of representation for original data on account of abundant loss of information and producing suboptimal hash codes; (2) the discrete binary constraint learning model is hard to solve where the retrieval performance may greatly reduce by relaxing the binary constraints for large quantization error; (3) handling the learning problem of CMH in a symmetric framework leading to difficult and complex optimization objective. To address above challenges in this paper a novel Asymmetric Correlation Quantization Hashing (ACQH) method is proposed. Specifically ACQH learns the projection matrixs of heterogeneous modalities data points for transforming query into a low-dimensional real-valued vector in latent semantic space and constructs the stacked compositional quantization embedding in a coarse-to-fine manner for indicating database points by a series of learnt real-valued codeword in the codebook with the help of pointwise label information regression simultaneously. Besides the unified hash codes across modalities can be directly obtained by the discrete iterative optimization framework devised in the paper. Comprehensive experiments on diverse three benchmark datasets have shown the effectiveness and rationality of ACQH.</p>\n", "tags": ["ARXIV", "Cross Modal", "Quantisation", "Supervised"], "tsne_embedding": [-11.20497989654541, -2.920311212539673]}, {"key": "wang2020deep", "year": "2020", "title": "Deep Hashing with Active Pairwise Supervision", "abstract": "<p>In this paper, we propose a Deep Hashing method with Active Pairwise Supervision(DH-APS). Conventional methods with passive\npairwise supervision obtain labeled data for training and require large\namount of annotations to reach their full potential, which are not feasible in realistic retrieval tasks. On the contrary, we actively select a small\nquantity of informative samples for annotation to provide effective pairwise supervision so that discriminative hash codes can be obtained with\nlimited annotation budget. Specifically, we generalize the structural risk\nminimization principle and obtain three criteria for the pairwise supervision acquisition: uncertainty, representativeness and diversity. Accordingly, samples involved in the following training pairs should be labeled:\npairs with most uncertain similarity, pairs that minimize the discrepancy\nbetween labeled and unlabeled data, and pairs which are most different\nfrom the annotated data, so that the discriminality and generalization ability of the learned hash codes are significantly strengthened. Moreover,\nour DH-APS can also be employed as a plug-and-play module for semisupervised hashing methods to further enhance the performance. Experiments demonstrate that the presented DH-APS achieves the accuracy\nof supervised hashing methods with only 30% labeled training samples\nand improves the semi-supervised binary codes by a sizable margin.</p>\n", "tags": ["Deep Learning", "ECCV", "Semi Supervised"], "tsne_embedding": [-0.8875617980957031, 4.282310962677002]}, {"key": "wang2020online", "year": "2020", "title": "Online Collective Matrix Factorization Hashing for Large-Scale Cross-Media Retrieval", "abstract": "<p>Cross-modal hashing has been widely investigated recently for its efficiency in large-scale cross-media retrieval. However, most existing cross-modal hashing methods learn hash functions in a batch-based learning mode. Such mode is not suitable for large-scale data sets due to the large memory consumption and loses its efficiency when training streaming data. Online cross-modal hashing can deal with the above problems by learning hash model in an online learning process. However, existing online cross-modal hashing methods cannot update hash codes of old data by the newly learned model. In this paper, we propose Online Collective Matrix Factorization Hashing (OCMFH) based on collective matrix factorization hashing (CMFH), which can adaptively update hash codes of old data according to dynamic changes of hash model without accessing to old data. Specifically, it learns discriminative hash codes for streaming data by collective matrix factorization in an online optimization scheme. Unlike conventional CMFH which needs to load the entire data points into memory, the proposed OCMFH retrains hash functions only by newly arriving data points. Meanwhile, it generates hash codes of new data and updates hash codes of old data by the latest updated hash model. In such way, hash codes of new data and old data are well-matched. Furthermore, a zero mean strategy is developed to solve the mean-varying problem in the online hash learning process. Extensive experiments on three benchmark data sets demonstrate the effectiveness and efficiency of OCMFH on online cross-media retrieval.</p>\n", "tags": ["Cross Modal", "SIGIR"], "tsne_embedding": [2.4351956844329834, -12.780559539794922]}, {"key": "wang2021contrastive", "year": "2021", "title": "Contrastive Quantization With Code Memory For Unsupervised Image Retrieval", "abstract": "<p>The high efficiency in computation and storage makes hashing (including binary hashing and quantization) a common strategy in large-scale retrieval systems. To alleviate the reliance on expensive annotations unsupervised deep hashing becomes an important research problem. This paper provides a novel solution to unsupervised deep quantization namely Contrastive Quantization with Code Memory (MeCoQ). Different from existing reconstruction-based strategies we learn unsupervised binary descriptors by contrastive learning which can better capture discriminative visual semantics. Besides we uncover that codeword diversity regularization is critical to prevent contrastive learning-based quantization from model degeneration. Moreover we introduce a novel quantization code memory module that boosts contrastive learning with lower feature drift than conventional feature memories. Extensive experiments on benchmark datasets show that MeCoQ outperforms state-of-the-art methods. Code and configurations are publicly available at https://github.com/gimpong/AAAI22-MeCoQ.\u201d</p>\n", "tags": ["ARXIV", "Has Code", "Image Retrieval", "Quantisation", "Unsupervised"], "tsne_embedding": [1.709102749824524, -5.469008445739746]}, {"key": "wang2021cross", "year": "2021", "title": "Cross-modal Zero-shot Hashing By Label Attributes Embedding", "abstract": "<p>Cross-modal hashing (CMH) is one of the most promising methods in cross-modal approximate nearest neighbor search. Most CMH solutions ideally assume the labels of training and testing set are identical. However the assumption is often violated causing a zero-shot CMH problem. Recent efforts to address this issue focus on transferring knowledge from the seen classes to the unseen ones using label attributes. However the attributes are isolated from the features of multi-modal data. To reduce the information gap we introduce an approach called LAEH (Label Attributes Embedding for zero-shot cross-modal Hashing). LAEH first gets the initial semantic attribute vectors of labels by word2vec model and then uses a transformation network to transform them into a common subspace. Next it leverages the hash vectors and the feature similarity matrix to guide the feature extraction network of different modalities. At the same time LAEH uses the attribute similarity as the supplement of label similarity to rectify the label embedding and common subspace. Experiments show that LAEH outperforms related representative zero-shot and cross-modal hashing methods.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [-4.479532241821289, 6.096113681793213]}, {"key": "wang2021ernie", "year": "2021", "title": "ERNIE 3.0 Titan Exploring Larger-scale Knowledge Enhanced Pre-training For Language Understanding And Generation", "abstract": "<p>Pre-trained language models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. GPT-3 has shown that scaling up pre-trained language models can further exploit their enormous potential. A unified framework named ERNIE 3.0 was recently proposed for pre-training large-scale knowledge enhanced models and trained a model with 10 billion parameters. ERNIE 3.0 outperformed the state-of-the-art models on various NLP tasks. In order to explore the performance of scaling up ERNIE 3.0 we train a hundred-billion-parameter model called ERNIE 3.0 Titan with up to 260 billion parameters on the PaddlePaddle platform. Furthermore we design a self-supervised adversarial loss and a controllable language modeling loss to make ERNIE 3.0 Titan generate credible and controllable texts. To reduce the computation overhead and carbon emission we propose an online distillation framework for ERNIE 3.0 Titan where the teacher model will teach students and train itself simultaneously. ERNIE 3.0 Titan is the largest Chinese dense pre-trained model so far. Empirical results show that the ERNIE 3.0 Titan outperforms the state-of-the-art models on 68 NLP datasets.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [22.343597412109375, -7.993112087249756]}, {"key": "wang2021mathematical", "year": "2021", "title": "Mathematical Models For Local Sensing Hashes", "abstract": "<p>As data volumes continue to grow searches in data are becoming increasingly time-consuming. Classical index structures for neighbor search are no longer sustainable due to the curse of dimensionality. Instead approximated index structures offer a good opportunity to significantly accelerate the neighbor search for clustering and outlier detection and to have the lowest possible error rate in the results of the algorithms. Local sensing hashes is one of those. We indicate directions to mathematically model the properties of it.</p>\n", "tags": ["ARXIV", "Unsupervised"], "tsne_embedding": [-6.1255035400390625, 30.029401779174805]}, {"key": "wang2021meta", "year": "2021", "title": "Meta Cross-modal Hashing On Long-tailed Data", "abstract": "<p>Due to the advantage of reducing storage while speeding up query time on big heterogeneous data cross-modal hashing has been extensively studied for approximate nearest neighbor search of multi-modal data. Most hashing methods assume that training data is class-balanced.However in practice real world data often have a long-tailed distribution. In this paper we introduce a meta-learning based cross-modal hashing method (MetaCMH) to handle long-tailed data. Due to the lack of training samples in the tail classes MetaCMH first learns direct features from data in different modalities and then introduces an associative memory module to learn the memory features of samples of the tail classes. It then combines the direct and memory features to obtain meta features for each sample. For samples of the head classes of the long tail distribution the weight of the direct features is larger because there are enough training data to learn them well; while for rare classes the weight of the memory features is larger. Finally MetaCMH uses a likelihood loss function to preserve the similarity in different modalities and learns hash functions in an end-to-end fashion. Experiments on long-tailed datasets show that MetaCMH performs significantly better than state-of-the-art methods especially on the tail classes.</p>\n", "tags": ["ARXIV", "Cross Modal", "Supervised"], "tsne_embedding": [3.1528122425079346, -5.217398643493652]}, {"key": "wang2021prototype", "year": "2021", "title": "Prototype-Supervised Adversarial Network for Targeted Attack of Deep Hashing", "abstract": "<p>Due to its powerful capability of representation learning and high-efficiency computation, deep hashing has made significant progress in large-scale image retrieval. However, deep hashing networks are vulnerable to adversarial examples, which is a practical secure problem but seldom studied in hashing-based retrieval field. In this paper, we propose a novel prototype-supervised adversarial network (ProS-GAN), which formulates a flexible generative architecture for efficient and effective targeted hashing attack. To the best of our knowledge, this is the first generation-based method to attack deep hashing networks. Generally, our proposed framework consists of three parts, i.e., a PrototypeNet, a generator and a discriminator. Specifically, the designed PrototypeNet embeds the target label into the semantic representation and learns the prototype code as the category-level representative of the target label. Moreover, the semantic representation and the original image are jointly fed into the generator for flexible targeted attack. Particularly, the prototype code is adopted to supervise the generator to construct the targeted adversarial example by minimizing the Hamming distance between the hash code of the adversarial example and the prototype code. Furthermore, the generator is against the discriminator to simultaneously encourage the adversarial examples visually realistic and the semantic representation informative. Extensive experiments verify that the proposed framework can efficiently produce adversarial examples with better targeted attack performance and transferability over state-of-the-art targeted attack methods of deep hashing.</p>\n", "tags": ["CVPR", "Deep Learning", "Has Code", "Supervised"], "tsne_embedding": [6.135072231292725, 4.723323822021484]}, {"key": "wang2022binary", "year": "2022", "title": "Binary Representation Via Jointly Personalized Sparse Hashing", "abstract": "<p>Unsupervised hashing has attracted much attention for binary representation learning due to the requirement of economical storage and efficiency of binary codes. It aims to encode high-dimensional features in the Hamming space with similarity preservation between instances. However most existing methods learn hash functions in manifold-based approaches. Those methods capture the local geometric structures (i.e. pairwise relationships) of data and lack satisfactory performance in dealing with real-world scenarios that produce similar features (e.g. color and shape) with different semantic information. To address this challenge in this work we propose an effective unsupervised method namely Jointly Personalized Sparse Hashing (JPSH) for binary representation learning. To be specific firstly we propose a novel personalized hashing module i.e. Personalized Sparse Hashing (PSH). Different personalized subspaces are constructed to reflect category-specific attributes for different clusters adaptively mapping instances within the same cluster to the same Hamming space. In addition we deploy sparse constraints for different personalized subspaces to select important features. We also collect the strengths of the other clusters to build the PSH module with avoiding over-fitting. Then to simultaneously preserve semantic and pairwise similarities in our JPSH we incorporate the PSH and manifold-based hash learning into the seamless formulation. As such JPSH not only distinguishes the instances from different clusters but also preserves local neighborhood structures within the cluster. Finally an alternating optimization algorithm is adopted to iteratively capture analytical solutions of the JPSH model. Extensive experiments on four benchmark datasets verify that the JPSH outperforms several hashing algorithms on the similarity search task.</p>\n", "tags": ["ARXIV", "Unsupervised"], "tsne_embedding": [-11.893153190612793, -3.3420088291168213]}, {"key": "wang2022hcfrec", "year": "2022", "title": "Hcfrec Hash Collaborative Filtering Via Normalized Flow With Structural Consensus For Efficient Recommendation", "abstract": "<p>The ever-increasing data scale of user-item interactions makes it challenging for an effective and efficient recommender system. Recently hash-based collaborative filtering (Hash-CF) approaches employ efficient Hamming distance of learned binary representations of users and items to accelerate recommendations. However Hash-CF often faces two challenging problems i.e. optimization on discrete representations and preserving semantic information in learned representations. To address the above two challenges we propose HCFRec a novel Hash-CF approach for effective and efficient recommendations. Specifically HCFRec not only innovatively introduces normalized flow to learn the optimal hash code by efficiently fit a proposed approximate mixture multivariate normal distribution a continuous but approximately discrete distribution but also deploys a cluster consistency preserving mechanism to preserve the semantic structure in representations for more accurate recommendations. Extensive experiments conducted on six real-world datasets demonstrate the superiority of our HCFRec compared to the state-of-art methods in terms of effectiveness and efficiency.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-20.008607864379883, 1.813852071762085]}, {"key": "wang2022rationale", "year": "2022", "title": "Rationale-augmented Ensembles In Language Models", "abstract": "<p>Recent research has shown that rationales or step-by-step chains of thought can be used to improve performance in multi-step reasoning tasks. We reconsider rationale-augmented prompting for few-shot in-context learning where (input - output) prompts are expanded to (input rationale - output) prompts. For rationale-augmented prompting we demonstrate how existing approaches which rely on manual prompt engineering are subject to sub-optimal rationales that may harm performance. To mitigate this brittleness we propose a unified framework of rationale-augmented ensembles where we identify rationale sampling in the output space as the key component to robustly improve performance. This framework is general and can easily be extended to common natural language processing tasks even those that do not traditionally leverage intermediate steps such as question answering word sense disambiguation and sentiment analysis. We demonstrate that rationale-augmented ensembles achieve more accurate and interpretable results than existing prompting approaches\u2013including standard prompting without rationales and rationale-based chain-of-thought prompting\u2013while simultaneously improving interpretability of model predictions through the associated rationales.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [37.94016647338867, 0.9462701082229614]}, {"key": "wang2022self", "year": "2022", "title": "Self-consistency Improves Chain Of Thought Reasoning In Language Models", "abstract": "<p>Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper we propose a new decoding strategy self-consistency to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks including GSM8K (+17.937;) SVAMP (+11.037;) AQuA (+12.237;) StrategyQA (+6.437;) and ARC-challenge (+3.937;).</p>\n", "tags": ["ARXIV"], "tsne_embedding": [29.795269012451172, 5.915177345275879]}, {"key": "wang2022super", "year": "2022", "title": "Super-naturalinstructions Generalization Via Declarative Instructions On 1600+ NLP Tasks", "abstract": "<p>How well can NLP models generalize to a variety of unseen tasks when provided with task instructions To address this question we first introduce Super-NaturalInstructions a benchmark of 1616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types including but not limited to classification extraction infilling sequence tagging text rewriting and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions \u2013 training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones. Furthermore we build Tk-Instruct a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 937; on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters such as the number of observed tasks the number of instances per task and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [28.499282836914062, -6.949732303619385]}, {"key": "wang2022towards", "year": "2022", "title": "Towards Unified Conversational Recommender Systems Via Knowledge-enhanced Prompt Learning", "abstract": "<p>Conversational recommender systems (CRS) aim to proactively elicit user preference and recommend high-quality items through natural language conversations. Typically a CRS consists of a recommendation module to predict preferred items for users and a conversation module to generate appropriate responses. To develop an effective CRS it is essential to seamlessly integrate the two modules. Existing works either design semantic alignment strategies or share knowledge resources and representations between the two modules. However these approaches still rely on different architectures or techniques to develop the two modules making it difficult for effective module integration. To address this problem we propose a unified CRS model named UniCRS based on knowledge-enhanced prompt learning. Our approach unifies the recommendation and conversation subtasks into the prompt learning paradigm and utilizes knowledge-enhanced prompts based on a fixed pre-trained language model (PLM) to fulfill both subtasks in a unified approach. In the prompt design we include fused knowledge representations task-specific soft tokens and the dialogue context which can provide sufficient contextual information to adapt the PLM for the CRS task. Besides for the recommendation subtask we also incorporate the generated response template as an important part of the prompt to enhance the information interaction between the two subtasks. Extensive experiments on two public CRS datasets have demonstrated the effectiveness of our approach.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [40.139984130859375, -10.827560424804688]}, {"key": "wang2023aligning", "year": "2023", "title": "Aligning Large Language Models With Human A Survey", "abstract": "<p>Large Language Models (LLMs) trained on extensive textual corpora have emerged as leading solutions for a broad array of Natural Language Processing (NLP) tasks. Despite their notable performance these models are prone to certain limitations such as misunderstanding human instructions generating potentially biased content or factually incorrect (hallucinated) information. Hence aligning LLMs with human expectations has become an active area of interest within the research community. This survey presents a comprehensive overview of these alignment technologies including the following aspects. (1) Data collection the methods for effectively collecting high-quality instructions for LLM alignment including the use of NLP benchmarks human annotations and leveraging strong LLMs. (2) Training methodologies a detailed review of the prevailing training methods employed for LLM alignment. Our exploration encompasses Supervised Fine-tuning both Online and Offline human preference training along with parameter-efficient training mechanisms. (3) Model Evaluation the methods for evaluating the effectiveness of these human-aligned LLMs presenting a multifaceted approach towards their assessment. In conclusion we collate and distill our findings shedding light on several promising future research avenues in the field. This survey therefore serves as a valuable resource for anyone invested in understanding and advancing the alignment of LLMs to better suit human-oriented tasks and expectations. An associated GitHub link collecting the latest papers is available at https://github.com/GaryYufei/AlignLLMHumanSurvey.</p>\n", "tags": ["ARXIV", "Has Code", "Supervised", "Survey Paper"], "tsne_embedding": [38.589332580566406, -10.427719116210938]}, {"key": "wang2023amber", "year": "2023", "title": "AMBER An Llm-free Multi-dimensional Benchmark For Mllms Hallucination Evaluation", "abstract": "<p>Despite making significant progress in multi-modal tasks current Multi-modal Large Language Models (MLLMs) encounter the significant challenge of hallucinations which may lead to harmful consequences. Therefore evaluating MLLMs hallucinations is becoming increasingly important in model improvement and practical application deployment. Previous works are limited in high evaluation costs (e.g. relying on humans or advanced LLMs) and insufficient evaluation dimensions (e.g. types of tasks and hallucinations). In this paper we propose an LLM-free multi-dimensional benchmark AMBER which can be used to evaluate both generative task and discriminative task including existence attribute and relation hallucination. Based on AMBER we design a low-cost and efficient evaluation pipeline. Additionally we conduct a comprehensive evaluation and detailed analysis of mainstream MLLMs including GPT-4V(ision) and also give guideline suggestions for mitigating hallucinations. The data and code of AMBER are available at https://github.com/junyangwang0410/AMBER.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [52.95186996459961, 5.928735256195068]}, {"key": "wang2023caption", "year": "2023", "title": "Caption Anything Interactive Image Description With Diverse Multimodal Controls", "abstract": "<p>Controllable image captioning is an emerging multimodal topic that aims to describe the image with natural language following human purpose () looking at the specified regions or telling in a particular text style. State-of-the-art methods are trained on annotated pairs of input controls and output captions. However the scarcity of such well-annotated multimodal data largely limits their usability and scalability for interactive AI systems. Leveraging unimodal instruction-following foundation models is a promising alternative that benefits from broader sources of data. In this paper we present Caption AnyThing (CAT) a foundation model augmented image captioning framework supporting a wide range of multimodel controls 1) visual controls including points boxes and trajectories; 2) language controls such as sentiment length language and factuality. Powered by Segment Anything Model (SAM) and ChatGPT we unify the visual and language prompts into a modularized framework enabling the flexible combination between different controls. Extensive case studies demonstrate the user intention alignment capabilities of our framework shedding light on effective user interaction modeling in vision-language applications. Our code is publicly available at https://github.com/ttengwang/Caption-Anything.</p>\n", "tags": ["ARXIV", "Case Study", "Cross Modal", "Has Code"], "tsne_embedding": [39.022220611572266, 7.706523418426514]}, {"key": "wang2023describe", "year": "2023", "title": "Describe Explain Plan And Select Interactive Planning With Large Language Models Enables Open-world Multi-task Agents", "abstract": "<p>We investigate the challenge of task planning for multi-task embodied agents in open-world environments. Two main difficulties are identified 1) executing plans in an open-world environment (e.g. Minecraft) necessitates accurate and multi-step reasoning due to the long-term nature of tasks and 2) as vanilla planners do not consider how easy the current agent can achieve a given sub-task when ordering parallel sub-goals within a complicated plan the resulting plan could be inefficient or even infeasible. To this end we propose ()escribe ()xplain ()lan and ()elect (()) an interactive planning approach based on Large Language Models (LLMs). DEPS facilitates better error correction on initial LLM-generated () by integrating () of the plan execution process and providing self-() of feedback when encountering failures during the extended planning phases. Furthermore it includes a goal () which is a trainable module that ranks parallel candidate sub-goals based on the estimated steps of completion consequently refining the initial plan. Our experiments mark the milestone of the first zero-shot multi-task agent that can robustly accomplish 70+ Minecraft tasks and nearly double the overall performances. Further testing reveals our methods general effectiveness in popularly adopted non-open-ended domains as well (i.e. ALFWorld and tabletop manipulation). The ablation and exploratory studies detail how our design beats the counterparts and provide a promising update on the () grand challenge with our approach. The code is released at https://github.com/CraftJarvis/MC-Planner.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [33.78022766113281, -16.974626541137695]}, {"key": "wang2023element", "year": "2023", "title": "Element-aware Summarization With Large Language Models Expert-aligned Evaluation And Chain-of-thought Method", "abstract": "<p>Automatic summarization generates concise summaries that contain key ideas of source documents. As the most mainstream datasets for the news sub-domain CNN/DailyMail and BBC XSum have been widely used for performance benchmarking. However the reference summaries of those datasets turn out to be noisy mainly in terms of factual hallucination and information redundancy. To address this challenge we first annotate new expert-writing Element-aware test sets following the Lasswell Communication Model proposed by Lasswell (1948) allowing reference summaries to focus on more fine-grained news elements objectively and comprehensively. Utilizing the new test sets we observe the surprising zero-shot summary ability of LLMs which addresses the issue of the inconsistent results between human preference and automatic evaluation metrics of LLMs zero-shot summaries in prior work. Further we propose a Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by step which helps them integrate more fine-grained details of source documents into the final summaries that correlate with the human writing mindset. Experimental results show our method outperforms state-of-the-art fine-tuned PLMs and zero-shot LLMs by +4.33/+4.77 in ROUGE-L on the two datasets respectively. Dataset and code are publicly available at https://github.com/Alsace08/SumCoT.</p>\n", "tags": ["ARXIV", "CNN", "Has Code"], "tsne_embedding": [31.135845184326172, -0.3200414478778839]}, {"key": "wang2023evaluation", "year": "2023", "title": "Evaluation And Analysis Of Hallucination In Large Vision-language Models", "abstract": "<p>Large Vision-Language Models (LVLMs) have recently achieved remarkable success. However LVLMs are still plagued by the hallucination problem which limits the practicality in many scenarios. Hallucination refers to the information of LVLMs responses that does not exist in the visual input which poses potential risks of substantial consequences. There has been limited work studying hallucination evaluation in LVLMs. In this paper we propose Hallucination Evaluation based on Large Language Models (HaELM) an LLM-based hallucination evaluation framework. HaELM achieves an approximate 9537; performance comparable to ChatGPT and has additional advantages including low cost reproducibility privacy preservation and local deployment. Leveraging the HaELM we evaluate the hallucination in current LVLMs. Furthermore we analyze the factors contributing to hallucination in LVLMs and offer helpful suggestions to mitigate the hallucination problem. Our training data and human annotation hallucination data will be made public soon.</p>\n", "tags": ["ARXIV", "Cross Modal", "Supervised"], "tsne_embedding": [52.02348327636719, 6.940612316131592]}, {"key": "wang2023explainable", "year": "2023", "title": "Llm4vis Explainable Visualization Recommendation Using Chatgpt", "abstract": "<p>Data visualization is a powerful tool for exploring and communicating insights in various domains. To automate visualization choice for datasets a task known as visualization recommendation has been proposed. Various machine-learning-based approaches have been developed for this purpose but they often require a large corpus of dataset-visualization pairs for training and lack natural explanations for their results. To address this research gap we propose LLM4Vis a novel ChatGPT-based prompting approach to perform visualization recommendation and return human-like explanations using very few demonstration examples. Our approach involves feature description demonstration example selection explanation generation demonstration example construction and inference steps. To obtain demonstration examples with high-quality explanations we propose a new explanation generation bootstrapping to iteratively refine generated explanations by considering the previous generation and template-based hint. Evaluations on the VizML dataset show that LLM4Vis outperforms or performs similarly to supervised learning models like Random Forest Decision Tree and MLP in both few-shot and zero-shot settings. The qualitative evaluation also shows the effectiveness of explanations generated by LLM4Vis. We make our code publicly available at hrefhttps://github.com/demoleiwang/LLM4Vis}{https://github.com/demoleiwang/LLM4Vis}.</p>\n", "tags": ["ARXIV", "Has Code", "Supervised"], "tsne_embedding": [36.24571228027344, -5.900766849517822]}, {"key": "wang2023generative", "year": "2023", "title": "Generative Recommendation Towards Next-generation Recommender Paradigm", "abstract": "<p>Recommender systems typically retrieve items from an item corpus for personalized recommendations. However such a retrieval-based recommender paradigm faces two limitations 1) the human-generated items in the corpus might fail to satisfy the users diverse information needs and 2) users usually adjust the recommendations via inefficient passive feedback e.g. clicks. Nowadays AI-Generated Content (AIGC) has revealed significant success offering the potential to overcome these limitations 1) generative AI can produce personalized items to satisfy users information needs and 2) the newly emerged large language models significantly reduce the efforts of users to precisely express information needs via natural language instructions. In this light the boom of AIGC points the way towards the next-generation recommender paradigm with two new objectives 1) generating personalized content through generative AI and 2) integrating user instructions to guide content generation. To this end we propose a novel Generative Recommender paradigm named GeneRec which adopts an AI generator to personalize content generation and leverages user instructions. Specifically we pre-process users instructions and traditional feedback via an instructor to output the generation guidance. Given the guidance we instantiate the AI generator through an AI editor and an AI creator to repurpose existing items and create new items. Eventually GeneRec can perform content retrieval repurposing and creation to satisfy users information needs. Besides to ensure the trustworthiness of the generated items we emphasize various fidelity checks. Moreover we provide a roadmap to envision future developments of GeneRec and several domain-specific applications of GeneRec with potential research tasks. Lastly we study the feasibility of implementing AI editor and AI creator on micro-video generation.</p>\n", "tags": ["ARXIV", "Text Retrieval"], "tsne_embedding": [42.732627868652344, -11.200934410095215]}, {"key": "wang2023graph", "year": "2023", "title": "Graph-collaborated Auto-encoder Hashing For Multi-view Binary Clustering", "abstract": "<p>Unsupervised hashing methods have attracted widespread attention with the explosive growth of large-scale data which can greatly reduce storage and computation by learning compact binary codes. Existing unsupervised hashing methods attempt to exploit the valuable information from samples which fails to take the local geometric structure of unlabeled samples into consideration. Moreover hashing based on auto-encoders aims to minimize the reconstruction loss between the input data and binary codes which ignores the potential consistency and complementarity of multiple sources data. To address the above issues we propose a hashing algorithm based on auto-encoders for multi-view binary clustering which dynamically learns affinity graphs with low-rank constraints and adopts collaboratively learning between auto-encoders and affinity graphs to learn a unified binary code called Graph-Collaborated Auto-Encoder Hashing for Multi-view Binary Clustering (GCAE). Specifically we propose a multi-view affinity graphs learning model with low-rank constraint which can mine the underlying geometric information from multi-view data. Then we design an encoder-decoder paradigm to collaborate the multiple affinity graphs which can learn a unified binary code effectively. Notably we impose the decorrelation and code balance constraints on binary codes to reduce the quantization errors. Finally we utilize an alternating iterative optimization scheme to obtain the multi-view clustering results. Extensive experimental results on 5 public datasets are provided to reveal the effectiveness of the algorithm and its superior performance over other state-of-the-art alternatives.</p>\n", "tags": ["ARXIV", "Cross Modal", "Graph", "Quantisation", "Unsupervised"], "tsne_embedding": [-1.4963854551315308, 27.191591262817383]}, {"key": "wang2023humanoid", "year": "2023", "title": "Humanoid Agents Platform For Simulating Human-like Generative Agents", "abstract": "<p>Just as computational simulations of atoms molecules and cells have shaped the way we study the sciences true-to-life simulations of human-like agents can be valuable tools for studying human behavior. We propose Humanoid Agents a system that guides Generative Agents to behave more like humans by introducing three elements of System 1 processing Basic needs (e.g. hunger health and energy) Emotion and Closeness in Relationships. Humanoid Agents are able to use these dynamic elements to adapt their daily activities and conversations with other agents as supported with empirical experiments. Our system is designed to be extensible to various settings three of which we demonstrate as well as to other elements influencing human behavior (e.g. empathy moral values and cultural background). Our platform also includes a Unity WebGL game interface for visualization and an interactive analytics dashboard to show agent statuses over time. Our platform is available on https://www.humanoidagents.com/ and code is on https://github.com/HumanoidAgents/HumanoidAgents</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [30.73358726501465, -17.34079933166504]}, {"key": "wang2023idea", "year": "2023", "title": "IDEA: An Invariant Perspective for Efficient Domain Adaptive Image Retrieval", "abstract": "<p>In this paper, we investigate the problem of unsupervised domain adaptive hashing, which leverage knowledge from a label-rich source domain to expedite learning to hash on a label-scarce target domain. Although numerous existing approaches attempt to incorporate transfer learning techniques into deep hashing frameworks, they often neglect the essential invariance for adequate alignment between these two domains. Worse yet, these methods fail to distinguish between causal and non-causal effects embedded in images, rendering cross-domain retrieval ineffective. To address these challenges, we propose an Invariance-acquired Domain AdaptivE HAshing (IDEA) model. Our IDEA first decomposes each image into a causal feature representing label information, and a non-causal feature indicating domain information. Subsequently, we generate discriminative hash codes using causal features with consistency learning on both source and target domains. More importantly, we employ a generative model for synthetic samples to simulate the intervention of various non-causal effects, ultimately minimizing their impact on hash codes for domain invariance. Comprehensive experiments conducted on benchmark datasets validate the superior performance of our IDEA compared to a variety of competitive baselines.</p>\n", "tags": ["Deep Learning", "Supervised", "TOMM"], "tsne_embedding": [-3.6748926639556885, 5.857583045959473]}, {"key": "wang2023is", "year": "2023", "title": "Is Chatgpt A Good Sentiment Analyzer A Preliminary Study", "abstract": "<p>Recently ChatGPT has drawn great attention from both the research community and the public. We are particularly interested in whether it can serve as a universal sentiment analyzer. To this end in this work we provide a preliminary evaluation of ChatGPT on the understanding of emphopinions emphsentiments and emphemotions contained in the text. Specifically we evaluate it in three settings including emphstandard evaluation emphpolarity shift evaluation and emphopen-domain evaluation. We conduct an evaluation on 7 representative sentiment analysis tasks covering 17 benchmark datasets and compare ChatGPT with fine-tuned BERT and corresponding state-of-the-art (SOTA) models on them. We also attempt several popular prompting techniques to elicit the ability further. Moreover we conduct human evaluation and present some qualitative case studies to gain a deep comprehension of its sentiment analysis capabilities.</p>\n", "tags": ["ARXIV", "Case Study"], "tsne_embedding": [42.979736328125, 3.4061989784240723]}, {"key": "wang2023jarvis", "year": "2023", "title": "JARVIS-1 Open-world Multi-task Agents With Memory-augmented Multimodal Language Models", "abstract": "<p>Achieving human-like planning and control with multimodal observations in an open world is a key milestone for more functional generalist agents. Existing approaches can handle certain long-horizon tasks in an open world. However they still struggle when the number of open-world tasks could potentially be infinite and lack the capability to progressively enhance task completion as game time progresses. We introduce JARVIS-1 an open-world agent that can perceive multimodal input (visual observations and human instructions) generate sophisticated plans and perform embodied control all within the popular yet challenging open-world Minecraft universe. Specifically we develop JARVIS-1 on top of pre-trained multimodal language models which map visual observations and textual instructions to plans. The plans will be ultimately dispatched to the goal-conditioned controllers. We outfit JARVIS-1 with a multimodal memory which facilitates planning using both pre-trained knowledge and its actual game survival experiences. JARVIS-1 is the existing most general agent in Minecraft capable of completing over 200 different tasks using control and observation space similar to humans. These tasks range from short-horizon tasks e.g. chopping trees to long-horizon tasks e.g. obtaining a diamond pickaxe. JARVIS-1 performs exceptionally well in short-horizon tasks achieving nearly perfect performance. In the classic long-term task of () JARVIS-1 surpasses the reliability of current state-of-the-art agents by 5 times and can successfully complete longer-horizon and more challenging tasks. The project page is available at https://craftjarvis.org/JARVIS-1</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [33.194793701171875, -17.23292350769043]}, {"key": "wang2023mitigating", "year": "2023", "title": "Mitigating Fine-grained Hallucination By Fine-tuning Large Vision-language Models With Caption Rewrites", "abstract": "<p>Large language models (LLMs) have shown remarkable performance in natural language processing (NLP) tasks. To comprehend and execute diverse human instructions over image data instruction-tuned large vision-language models (LVLMs) have been introduced. However LVLMs may suffer from different types of object hallucinations. Nevertheless LVLMs are evaluated for coarse-grained object hallucinations only (i.e. generated objects non-existent in the input image). The fine-grained object attributes and behaviors non-existent in the image may still be generated but not measured by the current evaluation methods. In this paper we thus focus on reducing fine-grained hallucinations of LVLMs. We propose textitReCaption a framework that consists of two components rewriting captions using ChatGPT and fine-tuning the instruction-tuned LVLMs on the rewritten captions. We also propose a fine-grained probing-based evaluation method named textitFine-Grained Object Hallucination Evaluation (textitFGHE). Our experiment results demonstrate that ReCaption effectively reduces fine-grained object hallucination for different LVLM options and improves their text generation quality. The code can be found at https://github.com/Anonymousanoy/FOHE.</p>\n", "tags": ["ARXIV", "Cross Modal", "Has Code"], "tsne_embedding": [46.38648986816406, 5.879617691040039]}, {"key": "wang2023neural", "year": "2023", "title": "Neural Codec Language Models Are Zero-shot Text To Speech Synthesizers", "abstract": "<p>We introduce a language modeling approach for text to speech synthesis (TTS). Specifically we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition we find Vall-E could preserve the speakers emotion and acoustic environment of the acoustic prompt in synthesis. See https://aka.ms/valle for demos of our work.</p>\n", "tags": ["ARXIV", "Has Code", "Supervised"], "tsne_embedding": [21.9477481842041, -2.985877752304077]}, {"key": "wang2023recmind", "year": "2023", "title": "Recmind Large Language Model Powered Agent For Recommendation", "abstract": "<p>While the recommendation system (RS) has advanced significantly through deep learning current RS approaches usually train and fine-tune models on task-specific datasets limiting their generalizability to new recommendation tasks and their ability to leverage external knowledge due to model scale and data size constraints. Thus we designed an LLM-powered autonomous recommender agent RecMind which is capable of leveraging external knowledge utilizing tools with careful planning to provide zero-shot personalized recommendations. We propose a Self-Inspiring algorithm to improve the planning ability. At each intermediate step the LLM self-inspires to consider all previously explored states to plan for the next step. This mechanism greatly improves the models ability to comprehend and utilize historical information in planning for recommendation. We evaluate RecMinds performance in various recommendation scenarios. Our experiment shows that RecMind outperforms existing zero/few-shot LLM-based recommendation baseline methods in various tasks and achieves comparable performance to a fully trained recommendation model P5.</p>\n", "tags": ["ARXIV", "Deep Learning"], "tsne_embedding": [44.27659225463867, -7.645244121551514]}, {"key": "wang2023reliable", "year": "2023", "title": "Reliable And Efficient Evaluation Of Adversarial Robustness For Deep Hashing-based Retrieval", "abstract": "<p>Deep hashing has been extensively applied to massive image retrieval due to its efficiency and effectiveness. Recently several adversarial attacks have been presented to reveal the vulnerability of deep hashing models against adversarial examples. However existing attack methods suffer from degraded performance or inefficiency because they underutilize the semantic relations between original samples or spend a lot of time learning these relations with a deep neural network. In this paper we propose a novel Pharos-guided Attack dubbed PgA to evaluate the adversarial robustness of deep hashing networks reliably and efficiently. Specifically we design pharos code to represent the semantics of the benign image which preserves the similarity to semantically relevant samples and dissimilarity to irrelevant ones. It is proven that we can quickly calculate the pharos code via a simple math formula. Accordingly PgA can directly conduct a reliable and efficient attack on deep hashing-based retrieval by maximizing the similarity between the hash code of the adversarial example and the pharos code. Extensive experiments on the benchmark datasets verify that the proposed algorithm outperforms the prior state-of-the-arts in both attack strength and speed.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Supervised"], "tsne_embedding": [-2.0503711700439453, 1.304078459739685]}, {"key": "wang2023rethinking", "year": "2023", "title": "Rethinking The Evaluation For Conversational Recommendation In The Era Of Large Language Models", "abstract": "<p>The recent success of large language models (LLMs) has shown great potential to develop more powerful conversational recommender systems (CRSs) which rely on natural language conversations to satisfy user needs. In this paper we embark on an investigation into the utilization of ChatGPT for conversational recommendation revealing the inadequacy of the existing evaluation protocol. It might over-emphasize the matching with the ground-truth items or utterances generated by human annotators while neglecting the interactive nature of being a capable CRS. To overcome the limitation we further propose an interactive Evaluation approach based on LLMs named iEvaLM that harnesses LLM-based user simulators. Our evaluation approach can simulate various interaction scenarios between users and systems. Through the experiments on two publicly available CRS datasets we demonstrate notable improvements compared to the prevailing evaluation protocol. Furthermore we emphasize the evaluation of explainability and ChatGPT showcases persuasive explanation generation for its recommendations. Our study contributes to a deeper comprehension of the untapped potential of LLMs for CRSs and provides a more flexible and easy-to-use evaluation framework for future research endeavors. The codes and data are publicly available at https://github.com/RUCAIBox/iEvaLM-CRS.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [36.5289421081543, -8.49025821685791]}, {"key": "wang2023t", "year": "2023", "title": "T-sciq Teaching Multimodal Chain-of-thought Reasoning Via Mixed Large Language Model Signals For Science Question Answering", "abstract": "<p>Large Language Models (LLMs) have recently demonstrated exceptional performance in various Natural Language Processing (NLP) tasks. They have also shown the ability to perform chain-of-thought (CoT) reasoning to solve complex problems. Recent studies have explored CoT reasoning in complex multimodal scenarios such as the science question answering task by fine-tuning multimodal models with high-quality human-annotated CoT rationales. However collecting high-quality COT rationales is usually time-consuming and costly. Besides the annotated rationales are hardly accurate due to the external essential information missed. To address these issues we propose a novel method termed T-SciQ that aims at teaching science question answering with LLM signals. The T-SciQ approach generates high-quality CoT rationales as teaching signals and is advanced to train much smaller models to perform CoT reasoning in complex modalities. Additionally we introduce a novel data mixing strategy to produce more effective teaching data samples for simple and complex science question answer problems. Extensive experimental results show that our T-SciQ method achieves a new state-of-the-art performance on the ScienceQA benchmark with an accuracy of 96.1837;. Moreover our approach outperforms the most powerful fine-tuned baseline by 4.537;. The code is publicly available at https://github.com/T-SciQ/T-SciQ.</p>\n", "tags": ["ARXIV", "Cross Modal", "Has Code"], "tsne_embedding": [37.791297912597656, -4.4329705238342285]}, {"key": "wang2023uncertainty", "year": "2023", "title": "Uncertainty-aware Unsupervised Video Hashing", "abstract": "<p>Learning to hash has become popular for video retrieval due to its fast speed and low storage consumption. Previous efforts formulate video hashing as training a binary auto-encoder, for which noncontinuous latent representations are optimized by the biased straight-through (ST) back-propagation heuristic. We propose to formulate video hashing as learning a discrete variational auto-encoder with the factorized Bernoulli latent distribution, termed as Bernoulli variational auto-encoder (BerVAE). The corresponding evidence lower bound (ELBO) in our BerVAE implementation leads to closed-form gradient expression, which can be applied to achieve principled training along with some other unbiased gradient estimators. BerVAE enables uncertainty-aware video hashing by predicting the probability distribution of video hash code-words, thus providing reliable uncertainty quantification. Experiments on both simulated and real-world large-scale video data demonstrate that our BerVAE trained with unbiased gradient estimators can achieve the state-of-the-art retrieval performance. Furthermore, we show that quantified uncertainty is highly correlated to video retrieval performance, which can be leveraged to further improve the retrieval accuracy. Our code is available at https://github.com/wangyucheng1234/BerVAE</p>\n", "tags": ["AISTATS", "Has Code", "Supervised", "Video Retrieval"], "tsne_embedding": [0.4979923367500305, -9.094301223754883]}, {"key": "wang2023vigc", "year": "2023", "title": "VIGC Visual Instruction Generation And Correction", "abstract": "<p>The integration of visual encoders and large language models (LLMs) has driven recent progress in multimodal large language models (MLLMs). However the scarcity of high-quality instruction-tuning data for vision-language tasks remains a challenge. The current leading paradigm such as LLaVA relies on language-only GPT-4 to generate data which requires pre-annotated image captions and detection bounding boxes suffering from understanding image details. A practical solution to this problem would be to utilize the available multimodal large language models (MLLMs) to generate instruction data for vision-language tasks. However its worth noting that the currently accessible MLLMs are not as powerful as their LLM counterparts as they tend to produce inadequate responses and generate false information. As a solution for addressing the current issue this paper proposes the Visual Instruction Generation and Correction (VIGC) framework that enables multimodal large language models to generate instruction-tuning data and progressively enhance its quality on-the-fly. Specifically Visual Instruction Generation (VIG) guides the vision-language model to generate diverse instruction-tuning data. To ensure generation quality Visual Instruction Correction (VIC) adopts an iterative update mechanism to correct any inaccuracies in data produced by VIG effectively reducing the risk of hallucination. Leveraging the diverse high-quality data generated by VIGC we finetune mainstream models and validate data quality based on various evaluations. Experimental results demonstrate that VIGC not only compensates for the shortcomings of language-only data generation methods but also effectively enhances the benchmark performance. The models datasets and code are available at https://opendatalab.github.io/VIGC.</p>\n", "tags": ["ARXIV", "Cross Modal", "Has Code"], "tsne_embedding": [45.595176696777344, 6.74075984954834]}, {"key": "wang2023voyager", "year": "2023", "title": "Voyager An Open-ended Embodied Agent With Large Language Models", "abstract": "<p>We introduce Voyager the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world acquires diverse skills and makes novel discoveries without human intervention. Voyager consists of three key components 1) an automatic curriculum that maximizes exploration 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors and 3) a new iterative prompting mechanism that incorporates environment feedback execution errors and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended interpretable and compositional which compounds the agents abilities rapidly and alleviates catastrophic forgetting. Empirically Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items travels 2.3x longer distances and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [26.954408645629883, -15.540624618530273]}, {"key": "wang2023zero", "year": "2023", "title": "Zero-shot Next-item Recommendation Using Large Pretrained Language Models", "abstract": "<p>Large language models (LLMs) have achieved impressive zero-shot performance in various natural language processing (NLP) tasks demonstrating their capabilities for inference without training examples. Despite their success no research has yet explored the potential of LLMs to perform next-item recommendations in the zero-shot setting. We have identified two major challenges that must be addressed to enable LLMs to act effectively as recommenders. First the recommendation space can be extremely large for LLMs and LLMs do not know about the target users past interacted items and preferences. To address this gap we propose a prompting strategy called Zero-Shot Next-Item Recommendation (NIR) prompting that directs LLMs to make next-item recommendations. Specifically the NIR-based strategy involves using an external module to generate candidate items based on user-filtering or item-filtering. Our strategy incorporates a 3-step prompting that guides GPT-3 to carry subtasks that capture the users preferences select representative previously watched movies and recommend a ranked list of 10 movies. We evaluate the proposed approach using GPT-3 on MovieLens 100K dataset and show that it achieves strong zero-shot performance even outperforming some strong sequential recommendation models trained on the entire training dataset. These promising results highlight the ample research opportunities to use LLMs as recommenders. The code can be found at https://github.com/AGI-Edgerunners/LLM-Next-Item-Rec.</p>\n", "tags": ["ARXIV", "Has Code", "Supervised"], "tsne_embedding": [45.638031005859375, -8.386670112609863]}, {"key": "wang2024can", "year": "2024", "title": "Can Small Language Models Be Good Reasoners For Sequential Recommendation", "abstract": "<p>Large language models (LLMs) open up new horizons for sequential recommendations owing to their remarkable language comprehension and generation capabilities. However there are still numerous challenges that should be addressed to successfully implement sequential recommendations empowered by LLMs. Firstly user behavior patterns are often complex and relying solely on one-step reasoning from LLMs may lead to incorrect or task-irrelevant responses. Secondly the prohibitively resource requirements of LLM (e.g. ChatGPT-175B) are overwhelmingly high and impractical for real sequential recommender systems. In this paper we propose a novel Step-by-step knowLedge dIstillation fraMework for recommendation (SLIM) paving a promising path for sequential recommenders to enjoy the exceptional reasoning capabilities of LLMs in a slim (i.e. resource-efficient) manner. We introduce CoT prompting based on user behavior sequences for the larger teacher model. The rationales generated by the teacher model are then utilized as labels to distill the downstream smaller student model (e.g. LLaMA2-7B). In this way the student model acquires the step-by-step reasoning capabilities in recommendation tasks. We encode the generated rationales from the student model into a dense vector which empowers recommendation in both ID-based and ID-agnostic scenarios. Extensive experiments demonstrate the effectiveness of SLIM over state-of-the-art baselines and further analysis showcasing its ability to generate meaningful recommendation reasoning at affordable costs.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [29.665157318115234, 10.771506309509277]}, {"key": "wang2024large", "year": "2024", "title": "Large Language Models As Data Augmenters For Cold-start Item Recommendation", "abstract": "<p>The reasoning and generalization capabilities of LLMs can help us better understand user preferences and item characteristics offering exciting prospects to enhance recommendation systems. Though effective while user-item interactions are abundant conventional recommendation systems struggle to recommend cold-start items without historical interactions. To address this we propose utilizing LLMs as data augmenters to bridge the knowledge gap on cold-start items during training. We employ LLMs to infer user preferences for cold-start items based on textual description of user historical behaviors and new item descriptions. The augmented training signals are then incorporated into learning the downstream recommendation models through an auxiliary pairwise loss. Through experiments on public Amazon datasets we demonstrate that LLMs can effectively augment the training signals for cold-start items leading to significant improvements in cold-start item recommendation for various recommendation models.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [42.835758209228516, -13.6661958694458]}, {"key": "wang2024llm", "year": "2024", "title": "Llm-enhanced User-item Interactions Leveraging Edge Information For Optimized Recommendations", "abstract": "<p>The extraordinary performance of large language models has not only reshaped the research landscape in the field of NLP but has also demonstrated its exceptional applicative potential in various domains. However the potential of these models in mining relationships from graph data remains under-explored. Graph neural networks as a popular research area in recent years have numerous studies on relationship mining. Yet current cutting-edge research in graph neural networks has not been effectively integrated with large language models leading to limited efficiency and capability in graph relationship mining tasks. A primary challenge is the inability of LLMs to deeply exploit the edge information in graphs which is critical for understanding complex node relationships. This gap limits the potential of LLMs to extract meaningful insights from graph structures limiting their applicability in more complex graph-based analysis. We focus on how to utilize existing LLMs for mining and understanding relationships in graph data applying these techniques to recommendation tasks. We propose an innovative framework that combines the strong contextual representation capabilities of LLMs with the relationship extraction and analysis functions of GNNs for mining relationships in graph data. Specifically we design a new prompt construction framework that integrates relational information of graph data into natural language expressions aiding LLMs in more intuitively grasping the connectivity information within graph data. Additionally we introduce graph relationship understanding and analysis functions into LLMs to enhance their focus on connectivity information in graph data. Our evaluation on real-world datasets demonstrates the frameworks ability to understand connectivity information in graph data.</p>\n", "tags": ["ARXIV", "Graph", "Supervised"], "tsne_embedding": [0.9542189836502075, 27.473344802856445]}, {"key": "wang2024mitigating", "year": "2024", "title": "Mitigating Hallucinations In Large Vision-language Models With Instruction Contrastive Decoding", "abstract": "<p>Large Vision-Language Models (LVLMs) are increasingly adept at generating contextually detailed and coherent responses from visual inputs. However their application in multimodal decision-making and open-ended generation is hindered by a notable rate of hallucinations where generated text inaccurately represents the visual contents. To address this issue this paper introduces the Instruction Contrastive Decoding (ICD) method a novel approach designed to reduce hallucinations during LVLM inference. Our method is inspired by our observation that what we call disturbance instructions significantly exacerbate hallucinations in multimodal fusion modules. ICD contrasts distributions from standard and instruction disturbance thereby increasing alignment uncertainty and effectively subtracting hallucinated concepts from the original distribution. Through comprehensive experiments on discriminative benchmarks (POPE and MME) and a generative benchmark (LLaVa-Bench) we demonstrate that ICD significantly mitigates both object-level and attribute-level hallucinations. Moreover our method not only addresses hallucinations but also significantly enhances the general perception and recognition capabilities of LVLMs.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [49.74551773071289, 6.711115837097168]}, {"key": "wang2024neural", "year": "2024", "title": "Neural Locality Sensitive Hashing For Entity Blocking", "abstract": "<p>Locality-sensitive hashing (LSH) is a fundamental algorithmic technique widely employed in large-scale data processing applications such as nearest-neighbor search entity resolution and clustering. However its applicability in some real-world scenarios is limited due to the need for careful design of hashing functions that align with specific metrics. Existing LSH-based Entity Blocking solutions primarily rely on generic similarity metrics such as Jaccard similarity whereas practical use cases often demand complex and customized similarity rules surpassing the capabilities of generic similarity metrics. Consequently designing LSH functions for these customized similarity rules presents considerable challenges. In this research we propose a neuralization approach to enhance locality-sensitive hashing by training deep neural networks to serve as hashing functions for complex metrics. We assess the effectiveness of this approach within the context of the entity resolution problem which frequently involves the use of task-specific metrics in real-world applications. Specifically we introduce NLSHBlock (Neural-LSH Block) a novel blocking methodology that leverages pre-trained language models fine-tuned with a novel LSH-based loss function. Through extensive evaluations conducted on a diverse range of real-world datasets we demonstrate the superiority of NLSHBlock over existing methods exhibiting significant performance improvements. Furthermore we showcase the efficacy of NLSHBlock in enhancing the performance of the entity matching phase particularly within the semi-supervised setting.</p>\n", "tags": ["ARXIV", "LSH", "Supervised"], "tsne_embedding": [7.860725402832031, 1.9376899003982544]}, {"key": "wang2024rethinking", "year": "2024", "title": "Rethinking Large Language Model Architectures For Sequential Recommendations", "abstract": "<p>Recently sequential recommendation has been adapted to the LLM paradigm to enjoy the power of LLMs. LLM-based methods usually formulate recommendation information into natural language and the model is trained to predict the next item in an auto-regressive manner. Despite their notable success the substantial computational overhead of inference poses a significant obstacle to their real-world applicability. In this work we endeavor to streamline existing LLM-based recommendation models and propose a simple yet highly effective model Lite-LLM4Rec. The primary goal of Lite-LLM4Rec is to achieve efficient inference for the sequential recommendation task. Lite-LLM4Rec circumvents the beam search decoding by using a straight item projection head for ranking scores generation. This design stems from our empirical observation that beam search decoding is ultimately unnecessary for sequential recommendations. Additionally Lite-LLM4Rec introduces a hierarchical LLM structure tailored to efficiently handle the extensive contextual information associated with items thereby reducing computational overhead while enjoying the capabilities of LLMs. Experiments on three publicly available datasets corroborate the effectiveness of Lite-LLM4Rec in both performance and inference efficiency (notably 46.837; performance improvement and 97.2837; efficiency improvement on ML-1m) over existing LLM-based methods. Our implementations will be open sourced.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [43.08549880981445, -6.975274562835693]}, {"key": "wang2024rreh", "year": "2024", "title": "RREH Reconstruction Relations Embedded Hashing For Semi-paired Cross-modal Retrieval", "abstract": "<p>Known for efficient computation and easy storage hashing has been extensively explored in cross-modal retrieval. The majority of current hashing models are predicated on the premise of a direct one-to-one mapping between data points. However in real practice data correspondence across modalities may be partially provided. In this research we introduce an innovative unsupervised hashing technique designed for semi-paired cross-modal retrieval tasks named Reconstruction Relations Embedded Hashing (RREH). RREH assumes that multi-modal data share a common subspace. For paired data RREH explores the latent consistent information of heterogeneous modalities by seeking a shared representation. For unpaired data to effectively capture the latent discriminative features the high-order relationships between unpaired data and anchors are embedded into the latent subspace which are computed by efficient linear reconstruction. The anchors are sampled from paired data which improves the efficiency of hash learning. The RREH trains the underlying features and the binary encodings in a unified framework with high-order reconstruction relations preserved. With the well devised objective function and discrete optimization algorithm RREH is designed to be scalable making it suitable for large-scale datasets and facilitating efficient cross-modal retrieval. In the evaluation process the proposed is tested with partially paired data to establish its superiority over several existing methods.</p>\n", "tags": ["ARXIV", "Cross Modal", "Unsupervised"], "tsne_embedding": [-13.439943313598633, 0.891494631767273]}, {"key": "wei2021anet", "year": "2021", "title": "A-Net: Learning Attribute-Aware Hash Codes for Large-Scale Fine-Grained Image Retrieval", "abstract": "<p>Our work focuses on tackling large-scale fine-grained image retrieval as ranking the images depicting the concept of interests (i.e., the same sub-category labels) highest based on the fine-grained details in the query. It is desirable to alleviate the challenges of both fine-grained nature of small inter-class variations with large intra-class variations and explosive growth of fine-grained data for such a practical task. In this paper, we propose an Attribute-Aware hashing Network (A-Net) for generating attribute-aware hash codes to not only make the retrieval process efficient, but also establish explicit correspondences between hash codes and visual attributes. Specifically, based on the captured visual representations by attention, we develop an encoder-decoder structure network of a reconstruction task to unsupervisedly distill high-level attribute-specific vectors from the appearance-specific visual representations without attribute annotations. A-Net is also equipped with a feature decorrelation constraint upon these attribute vectors to enhance their representation abilities. Finally, the required hash codes are generated by the attribute vectors driven by preserving original similarities. Qualitative experiments on five benchmark fine-grained datasets show our superiority over competing methods. More importantly, quantitative results demonstrate the obtained hash codes can strongly correspond to certain kinds of crucial properties of fine-grained objects.</p>\n", "tags": ["Image Retrieval", "NEURIPS", "Supervised"], "tsne_embedding": [-6.123707294464111, 17.660612106323242]}, {"key": "wei2021finetuned", "year": "2021", "title": "Finetuned Language Models Are Zero-shot Learners", "abstract": "<p>This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning \u2013 finetuning language models on a collection of tasks described via instructions \u2013 substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model which we call FLAN on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI RTE BoolQ AI2-ARC OpenbookQA and StoryCloze. Ablation studies reveal that number of finetuning datasets model scale and natural language instructions are key to the success of instruction tuning.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [21.49767303466797, 0.10278106480836868]}, {"key": "wei2021learning", "year": "2021", "title": "A(^2)-net Learning Attribute-aware Hash Codes For Large-scale Fine-grained Image Retrieval", "abstract": "<p>Our work focuses on tackling large-scale fine-grained image retrieval as ranking the images depicting the concept of interests (i.e. the same sub-category labels) highest based on the fine-grained details in the query. It is desirable to alleviate the challenges of both fine-grained nature of small inter-class variations with large intra-class variations and explosive growth of fine-grained data for such a practical task. In this paper we propose an Attribute-Aware hashing Network (A(^2)-Net) for generating attribute-aware hash codes to not only make the retrieval process efficient but also establish explicit correspondences between hash codes and visual attributes. Specifically based on the captured visual representations by attention we develop an encoder-decoder structure network of a reconstruction task to unsupervisedly distill high-level attribute-specific vectors from the appearance-specific visual representations without attribute annotations. A(^2)-Net is also equipped with a feature decorrelation constraint upon these attribute vectors to enhance their representation abilities. Finally the required hash codes are generated by the attribute vectors driven by preserving original similarities. Qualitative experiments on five benchmark fine-grained datasets show our superiority over competing methods. More importantly quantitative results demonstrate the obtained hash codes can strongly correspond to certain kinds of crucial properties of fine-grained objects.</p>\n", "tags": ["Image Retrieval", "NEURIPS", "Supervised"], "tsne_embedding": [-6.120734214782715, 17.639970779418945]}, {"key": "wei2022chain", "year": "2022", "title": "Chain-of-thought Prompting Elicits Reasoning In Large Language Models", "abstract": "<p>We explore how generating a chain of thought \u2013 a series of intermediate reasoning steps \u2013 significantly improves the ability of large language models to perform complex reasoning. In particular we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic commonsense and symbolic reasoning tasks. The empirical gains can be striking. For instance prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems surpassing even finetuned GPT-3 with a verifier.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [29.563186645507812, 5.693675994873047]}, {"key": "wei2022emergent", "year": "2022", "title": "Emergent Abilities Of Large Language Models", "abstract": "<p>Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [38.12052917480469, 1.4202876091003418]}, {"key": "wei2022hyperbolic", "year": "2022", "title": "Hyperbolic Hierarchical Contrastive Hashing", "abstract": "<p>Hierarchical semantic structures naturally existing in real-world datasets can assist in capturing the latent distribution of data to learn robust hash codes for retrieval systems. Although hierarchical semantic structures can be simply expressed by integrating semantically relevant data into a high-level taxon with coarser-grained semantics the construction embedding and exploitation of the structures remain tricky for unsupervised hash learning. To tackle these problems we propose a novel unsupervised hashing method named Hyperbolic Hierarchical Contrastive Hashing (HHCH). We propose to embed continuous hash codes into hyperbolic space for accurate semantic expression since embedding hierarchies in hyperbolic space generates less distortion than in hyper-sphere space and Euclidean space. In addition we extend the K-Means algorithm to hyperbolic space and perform the proposed hierarchical hyperbolic K-Means algorithm to construct hierarchical semantic structures adaptively. To exploit the hierarchical semantic structures in hyperbolic space we designed the hierarchical contrastive learning algorithm including hierarchical instance-wise and hierarchical prototype-wise contrastive learning. Extensive experiments on four benchmark datasets demonstrate that the proposed method outperforms the state-of-the-art unsupervised hashing methods. Codes will be released.</p>\n", "tags": ["ICIP", "Unsupervised"], "tsne_embedding": [-8.504115104675293, -1.0249178409576416]}, {"key": "wei2023attribute", "year": "2023", "title": "Attribute-aware Deep Hashing With Self-consistency For Large-scale Fine-grained Image Retrieval", "abstract": "<p>Our work focuses on tackling large-scale fine-grained image retrieval as ranking the images depicting the concept of interests (i.e. the same sub-category labels) highest based on the fine-grained details in the query. It is desirable to alleviate the challenges of both fine-grained nature of small inter-class variations with large intra-class variations and explosive growth of fine-grained data for such a practical task. In this paper we propose attribute-aware hashing networks with self-consistency for generating attribute-aware hash codes to not only make the retrieval process efficient but also establish explicit correspondences between hash codes and visual attributes. Specifically based on the captured visual representations by attention we develop an encoder-decoder structure network of a reconstruction task to unsupervisedly distill high-level attribute-specific vectors from the appearance-specific visual representations without attribute annotations. Our models are also equipped with a feature decorrelation constraint upon these attribute vectors to strengthen their representative abilities. Then driven by preserving original entities similarity the required hash codes can be generated from these attribute-specific vectors and thus become attribute-aware. Furthermore to combat simplicity bias in deep hashing we consider the model design from the perspective of the self-consistency principle and propose to further enhance models self-consistency by equipping an additional image reconstruction path. Comprehensive quantitative experiments under diverse empirical settings on six fine-grained retrieval datasets and two generic retrieval datasets show the superiority of our models over competing methods.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Supervised"], "tsne_embedding": [-6.005475997924805, 17.64056396484375]}, {"key": "wei2023llmrec", "year": "2023", "title": "Llmrec Large Language Models With Graph Augmentation For Recommendation", "abstract": "<p>The problem of data sparsity has long been a challenge in recommendation systems and previous studies have attempted to address this issue by incorporating side information. However this approach often introduces side effects such as noise availability issues and low data quality which in turn hinder the accurate modeling of user preferences and adversely impact recommendation performance. In light of the recent advancements in large language models (LLMs) which possess extensive knowledge bases and strong reasoning capabilities we propose a novel framework called LLMRec that enhances recommender systems by employing three simple yet effective LLM-based graph augmentation strategies. Our approach leverages the rich content available within online platforms (e.g. Netflix MovieLens) to augment the interaction graph in three ways (i) reinforcing user-item interaction egde (ii) enhancing the understanding of item node attributes and (iii) conducting user node profiling intuitively from the natural language perspective. By employing these strategies we address the challenges posed by sparse implicit feedback and low-quality side information in recommenders. Besides to ensure the quality of the augmentation we develop a denoised data robustification mechanism that includes techniques of noisy implicit feedback pruning and MAE-based feature enhancement that help refine the augmented data and improve its reliability. Furthermore we provide theoretical analysis to support the effectiveness of LLMRec and clarify the benefits of our method in facilitating model optimization. Experimental results on benchmark datasets demonstrate the superiority of our LLM-based augmentation approach over state-of-the-art techniques. To ensure reproducibility we have made our code and augmented data publicly available at https://github.com/HKUDS/LLMRec.git</p>\n", "tags": ["ARXIV", "Graph", "Has Code"], "tsne_embedding": [41.82212829589844, -12.209206581115723]}, {"key": "wei2023symbol", "year": "2023", "title": "Symbol Tuning Improves In-context Learning In Language Models", "abstract": "<p>We present symbol tuning - finetuning language models on in-context input-label pairs where natural language labels (e.g. positive/negative sentiment) are replaced with arbitrary symbols (e.g. foo/bar). Symbol tuning leverages the intuition that when a model cannot use instructions or natural language labels to figure out a task it must instead do so by learning the input-label mappings. We experiment with symbol tuning across Flan-PaLM models up to 540B parameters and observe benefits across various settings. First symbol tuning boosts performance on unseen in-context learning tasks and is much more robust to underspecified prompts such as those without instructions or without natural language labels. Second symbol-tuned models are much stronger at algorithmic reasoning tasks with up to 18.237; better performance on the List Functions benchmark and up to 15.337; better performance on the Simple Turing Concepts benchmark. Finally symbol-tuned models show large improvements in following flipped-labels presented in-context meaning that they are more capable of using in-context information to override prior semantic knowledge.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [22.437660217285156, 1.8129702806472778]}, {"key": "wei2024contrastive", "year": "2024", "title": "Contrastive Masked Auto-encoders Based Self-supervised Hashing For 2D Image And 3D Point Cloud Cross-modal Retrieval", "abstract": "<p>Implementing cross-modal hashing between 2D images and 3D point-cloud data is a growing concern in real-world retrieval systems. Simply applying existing cross-modal approaches to this new task fails to adequately capture latent multi-modal semantics and effectively bridge the modality gap between 2D and 3D. To address these issues without relying on hand-crafted labels we propose contrastive masked autoencoders based self-supervised hashing (CMAH) for retrieval between images and point-cloud data. We start by contrasting 2D-3D pairs and explicitly constraining them into a joint Hamming space. This contrastive learning process ensures robust discriminability for the generated hash codes and effectively reduces the modality gap. Moreover we utilize multi-modal auto-encoders to enhance the models understanding of multi-modal semantics. By completing the masked image/point-cloud data modeling task the model is encouraged to capture more localized clues. In addition the proposed multi-modal fusion block facilitates fine-grained interactions among different modalities. Extensive experiments on three public datasets demonstrate that the proposed CMAH significantly outperforms all baseline methods.</p>\n", "tags": ["ARXIV", "Cross Modal", "Supervised"], "tsne_embedding": [-7.478749752044678, 13.504768371582031]}, {"key": "weinberger2009feature", "year": "2009", "title": "Feature Hashing For Large Scale Multitask Learning", "abstract": "<p>Empirical evidence suggests that hashing is an effective strategy for dimensionality reduction and practical nonparametric estimation. In this paper we provide exponential tail bounds for feature hashing and show that the interaction between random subspaces is negligible with high probability. We demonstrate the feasibility of this approach with experimental results for a new use case \u2013 multitask learning with hundreds of thousands of tasks.</p>\n", "tags": ["ARXIV", "Unsupervised"], "tsne_embedding": [-20.954120635986328, -11.492472648620605]}, {"key": "weiss2008spectral", "year": "2008", "title": "Spectral Hashing", "abstract": "<p>Semantic hashing seeks compact binary codes of datapoints so that the Hamming distance between codewords correlates with semantic similarity. Hinton et al. used a clever implementation of autoencoders to find such codes. In this paper we show that the problem of finding a best code for a given dataset is closely related to the problem of graph partitioning and can be shown to be NP hard. By relaxing the original problem we obtain a spectral method whose solutions are simply a subset of thresh- olded eigenvectors of the graph Laplacian. By utilizing recent results on convergence of graph Laplacian eigenvectors to the Laplace-Beltrami eigen- functions of manifolds we show how to efficiently calculate the code of a novel datapoint. Taken together both learning the code and applying it to a novel point are extremely simple. Our experiments show that our codes significantly outperform the state-of-the art.</p>\n", "tags": ["Graph", "NEURIPS", "Unsupervised"], "tsne_embedding": [-2.83595871925354, 26.13796043395996]}, {"key": "weiss2012multi", "year": "2012", "title": "Multidimensional Spectral Hashing", "abstract": "<p>en a surge of interest in methods based on \u201csemantic hashing\u201d,\ni.e. compact binary codes of data-points so that the Hamming distance\nbetween codewords correlates with similarity. In reviewing and\ncomparing existing methods, we show that their relative performance can\nchange drastically depending on the definition of ground-truth neighbors.\nMotivated by this finding, we propose a new formulation for learning binary\ncodes which seeks to reconstruct the affinity between datapoints,\nrather than their distances. We show that this criterion is intractable\nto solve exactly, but a spectral relaxation gives an algorithm where the\nbits correspond to thresholded eigenvectors of the affinity matrix, and\nas the number of datapoints goes to infinity these eigenvectors converge\nto eigenfunctions of Laplace-Beltrami operators, similar to the recently\nproposed Spectral Hashing (SH) method. Unlike SH whose performance\nmay degrade as the number of bits increases, the optimal code using\nour formulation is guaranteed to faithfully reproduce the affinities as\nthe number of bits increases. We show that the number of eigenfunctions\nneeded may increase exponentially with dimension, but introduce a \u201ckernel\ntrick\u201d to allow us to compute with an exponentially large number of\nbits but using only memory and computation that grows linearly with\ndimension. Experiments shows that MDSH outperforms the state-of-the\nart, especially in the challenging regime of small distance thresholds.</p>\n", "tags": ["ECCV", "Image Retrieval"], "tsne_embedding": [-29.96988296508789, -0.24659208953380585]}, {"key": "weng2019efficient", "year": "2019", "title": "Efficient Querying From Weighted Binary Codes", "abstract": "<p>Binary codes are widely used to represent the data due to their small storage and efficient computation. However there exists an ambiguity problem that lots of binary codes share the same Hamming distance to a query. To alleviate the ambiguity problem weighted binary codes assign different weights to each bit of binary codes and compare the binary codes by the weighted Hamming distance. Till now performing the querying from the weighted binary codes efficiently is still an open issue. In this paper we propose a new method to rank the weighted binary codes and return the nearest weighted binary codes of the query efficiently. In our method based on the multi-index hash tables two algorithms the table bucket finding algorithm and the table merging algorithm are proposed to select the nearest weighted binary codes of the query in a non-exhaustive and accurate way. The proposed algorithms are justified by proving their theoretic properties. The experiments on three large-scale datasets validate both the search efficiency and the search accuracy of our method. Especially for the number of weighted binary codes up to one billion our method shows a great improvement of more than 1000 times faster than the linear scan.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-7.149444103240967, -10.165842056274414]}, {"key": "weng2019online", "year": "2019", "title": "Online Hashing With Efficient Updating Of Binary Codes", "abstract": "<p>Online hashing methods are efficient in learning the hash functions from the streaming data. However when the hash functions change the binary codes for the database have to be recomputed to guarantee the retrieval accuracy. Recomputing the binary codes by accumulating the whole database brings a timeliness challenge to the online retrieval process. In this paper we propose a novel online hashing framework to update the binary codes efficiently without accumulating the whole database. In our framework the hash functions are fixed and the projection functions are introduced to learn online from the streaming data. Therefore inefficient updating of the binary codes by accumulating the whole database can be transformed to efficient updating of the binary codes by projecting the binary codes into another binary space. The queries and the binary code database are projected asymmetrically to further improve the retrieval accuracy. The experiments on two multi-label image databases demonstrate the effectiveness and the efficiency of our method for multi-label image retrieval.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Independent", "Streaming Data"], "tsne_embedding": [-9.510518074035645, -19.065122604370117]}, {"key": "weng2020fast", "year": "2020", "title": "Fast Search On Binary Codes By Weighted Hamming Distance", "abstract": "<p>Weighted Hamming distance as a similarity measure between binary codes and binary queries provides superior accuracy in search tasks than Hamming distance. However how to efficiently and accurately find K binary codes that have the smallest weighted Hamming distance to the query remains an open issue. In this paper a fast search algorithm is proposed to perform the non-exhaustive search for K nearest binary codes by weighted Hamming distance. By using binary codes as direct bucket indices in a hash table the search algorithm generates a sequence to probe the buckets based on the independence characteristic of the weights for each bit. Furthermore a fast search framework based on the proposed search algorithm is designed to solve the problem of long binary codes. Specifically long binary codes are split into substrings and multiple hash tables are built on them. Then the search algorithm probes the buckets to obtain candidates according to the generated substring indices and a merging algorithm is proposed to find the nearest binary codes by merging the candidates. Theoretical analysis and experimental results demonstrate that the search algorithm improves the search accuracy compared to other non-exhaustive algorithms and provides orders-of-magnitude faster search than the linear scan baseline.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-28.262048721313477, 11.688045501708984]}, {"key": "weng2020online", "year": "2020", "title": "Online Hashing with Efficient Updating of Binary Codes", "abstract": "<p>Online hashing methods are efficient in learning the hash functions from the streaming data. However, when the hash functions change, the binary codes for the database have to be recomputed to guarantee the retrieval accuracy. Recomputing the binary codes by accumulating the whole database brings a timeliness challenge to the online retrieval process. In this paper, we propose a novel online hashing framework to update the binary codes efficiently without accumulating the whole database. In our framework, the hash functions are fixed and the projection functions are introduced to learn online from the streaming data. Therefore, inefficient updating of the binary codes by accumulating the whole database can be transformed to efficient updating of the binary codes by projecting the binary codes into another binary space. The queries and the binary code database are projected asymmetrically to further improve the retrieval accuracy. The experiments on two multi-label image databases demonstrate the effectiveness and the efficiency of our method for multi-label image retrieval.</p>\n", "tags": ["AAAI"], "tsne_embedding": [-9.553390502929688, -19.052736282348633]}, {"key": "weng2020random", "year": "2020", "title": "Random VLAD Based Deep Hashing For Efficient Image Retrieval", "abstract": "<p>Image hash algorithms generate compact binary representations that can be quickly matched by Hamming distance thus become an efficient solution for large-scale image retrieval. This paper proposes RV-SSDH a deep image hash algorithm that incorporates the classical VLAD (vector of locally aggregated descriptors) architecture into neural networks. Specifically a novel neural network component is formed by coupling a random VLAD layer with a latent hash layer through a transform layer. This component can be combined with convolutional layers to realize a hash algorithm. We implement RV-SSDH as a point-wise algorithm that can be efficiently trained by minimizing classification error and quantization loss. Comprehensive experiments show this new architecture significantly outperforms baselines such as NetVLAD and SSDH and offers a cost-effective trade-off in the state-of-the-art. In addition the proposed random VLAD layer leads to satisfactory accuracy with low complexity thus shows promising potentials as an alternative to NetVLAD.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Independent", "Quantisation"], "tsne_embedding": [-18.33786392211914, -8.584403991699219]}, {"key": "weng2021online", "year": "2021", "title": "Online Hashing With Similarity Learning", "abstract": "<p>Online hashing methods usually learn the hash functions online aiming to efficiently adapt to the data variations in the streaming environment. However when the hash functions are updated the binary codes for the whole database have to be updated to be consistent with the hash functions resulting in the inefficiency in the online image retrieval process. In this paper we propose a novel online hashing framework without updating binary codes. In the proposed framework the hash functions are fixed and a parametric similarity function for the binary codes is learnt online to adapt to the streaming data. Specifically a parametric similarity function that has a bilinear form is adopted and a metric learning algorithm is proposed to learn the similarity function online based on the characteristics of the hashing methods. The experiments on two multi-label image datasets show that our method is competitive or outperforms the state-of-the-art online hashing methods in terms of both accuracy and efficiency for multi-label image retrieval.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Independent", "Streaming Data"], "tsne_embedding": [-9.384256362915039, -19.002012252807617]}, {"key": "weng2022large", "year": "2022", "title": "Large Language Models Are Better Reasoners With Self-verification", "abstract": "<p>Recently with the chain of thought (CoT) prompting large language models (LLMs) e.g. GPT-3 have shown strong reasoning ability in several natural language processing tasks such as arithmetic commonsense and logical reasoning. However LLMs with CoT require multi-step prompting and multi-token prediction which is highly sensitive to individual mistakes and vulnerable to error accumulation. The above issues make the LLMs need the ability to verify the answers. In fact after inferring conclusions in some thinking decision tasks people often check them by re-verifying steps to avoid some mistakes. In this paper we propose and prove that LLMs also have similar self-verification abilities. We take the conclusion obtained by CoT as one of the conditions for solving the original problem. By performing a backward verification of the answers that LLM deduced for itself we can obtain interpretable answer validation scores to select the candidate answer with the highest score. Experimental results demonstrate that the proposed method can improve the reasoning performance on various arithmetic commonsense and logical reasoning datasets. Our code is publicly available at https://github.com/WENGSYX/Self-Verification.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [33.52178955078125, 2.7291431427001953]}, {"key": "weng2023constant", "year": "2023", "title": "Constant Sequence Extension For Fast Search Using Weighted Hamming Distance", "abstract": "<p>Representing visual data using compact binary codes is attracting increasing attention as binary codes are used as direct indices into hash table(s) for fast non-exhaustive search. Recent methods show that ranking binary codes using weighted Hamming distance (WHD) rather than Hamming distance (HD) by generating query-adaptive weights for each bit can better retrieve query-related items. However search using WHD is slower than that using HD. One main challenge is that the complexity of extending a monotone increasing sequence using WHD to probe buckets in hash table(s) for existing methods is at least proportional to the square of the sequence length while that using HD is proportional to the sequence length. To overcome this challenge we propose a novel fast non-exhaustive search method using WHD. The key idea is to design a constant sequence extension algorithm to perform each sequence extension in constant computational complexity and the total complexity is proportional to the sequence length which is justified by theoretical analysis. Experimental results show that our method is faster than other WHD-based search methods. Also compared with the HD-based non-exhaustive search method our method has comparable efficiency but retrieves more query-related items for the dataset of up to one billion items.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-30.007221221923828, 17.41907501220703]}, {"key": "wiki2010new", "year": "2010", "title": "A New Approach to Cross-Modal Multimedia Retrieval", "abstract": "<p>The collected documents are selected sections from the Wikipedia\u2019s featured articles collection. This is a continuously growing dataset, that at the time of collection (October 2009) had 2,669 articles spread over 29 categories. Some of the categories are very scarce, therefore we considered only the 10 most populated ones. The articles generally have multiple sections and pictures. We have split them into sections based on section headings, and assign each image to the section in which it was placed by the author(s). Then this dataset was prunned to keep only sections that contained a single image and at least 70 words. \nThe final corpus contains 2,866 multimedia documents. The median text length is 200 words.</p>\n", "tags": ["Dataset", "ICME"], "tsne_embedding": [-17.02291488647461, 22.287887573242188]}, {"key": "wolf2023fundamental", "year": "2023", "title": "Fundamental Limitations Of Alignment In Large Language Models", "abstract": "<p>An important aspect in developing language models that interact with humans is aligning their behavior to be useful and unharmful for their human users. This is usually achieved by tuning the model in a way that enhances desired behaviors and inhibits undesired ones a process referred to as alignment. In this paper we propose a theoretical approach called Behavior Expectation Bounds (BEB) which allows us to formally investigate several inherent characteristics and limitations of alignment in large language models. Importantly we prove that within the limits of this framework for any behavior that has a finite probability of being exhibited by the model there exist prompts that can trigger the model into outputting this behavior with probability that increases with the length of the prompt. This implies that any alignment process that attenuates an undesired behavior but does not remove it altogether is not safe against adversarial prompting attacks. Furthermore our framework hints at the mechanism by which leading alignment approaches such as reinforcement learning from human feedback make the LLM prone to being prompted into the undesired behaviors. This theoretical result is being experimentally demonstrated in large scale by the so called contemporary chatGPT jailbreaks where adversarial users trick the LLM into breaking its alignment guardrails by triggering it into acting as a malicious persona. Our results expose fundamental limitations in alignment of LLMs and bring to the forefront the need to devise reliable mechanisms for ensuring AI safety.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [42.240203857421875, -2.4615707397460938]}, {"key": "won2022scaling", "year": "2022", "title": "Scaling Instruction-finetuned Language Models", "abstract": "<p>Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks (2) scaling the model size and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM T5 U-PaLM) prompting setups (zero-shot few-shot CoT) and evaluation benchmarks (MMLU BBH TyDiQA MGSM open-ended generation). For instance Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.437; on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks such as 75.237; on five-shot MMLU. We also publicly release Flan-T5 checkpoints which achieve strong few-shot performance even compared to much larger models such as PaLM 62B. Overall instruction finetuning is a general method for improving the performance and usability of pretrained language models.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [21.129514694213867, 0.31421560049057007]}, {"key": "workshop2022bloom", "year": "2022", "title": "BLOOM A 176b-parameter Open-access Multilingual Language Model", "abstract": "<p>Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology we present BLOOM a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs we publicly release our models and code under the Responsible AI License.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [29.13642120361328, -9.548310279846191]}, {"key": "wu2016robust", "year": "2016", "title": "Robust Hashing For Multi-view Data Jointly Learning Low-rank Kernelized Similarity Consensus And Hash Functions", "abstract": "<p>Learning hash functions/codes for similarity search over multi-view data is attracting increasing attention where similar hash codes are assigned to the data objects characterizing consistently neighborhood relationship across views. Traditional methods in this category inherently suffer three limitations 1) they commonly adopt a two-stage scheme where similarity matrix is first constructed followed by a subsequent hash function learning; 2) these methods are commonly developed on the assumption that data samples with multiple representations are noise-freewhich is not practical in real-life applications; 3) they often incur cumbersome training model caused by the neighborhood graph construction using all N points in the database (O(N)). In this paper we motivate the problem of jointly and efficiently training the robust hash functions over data objects with multi-feature representations which may be noise corrupted. To achieve both the robustness and training efficiency we propose an approach to effectively and efficiently learning low-rank kernelized (footnote)We use kernelized similarity rather than kernel as it is not a squared symmetric matrix for data-landmark affinity matrix. hash functions shared across views. Specifically we utilize landmark graphs to construct tractable similarity matrices in multi-views to automatically discover neighborhood structure in the data. To learn robust hash functions a latent low-rank kernel function is used to construct hash functions in order to accommodate linearly inseparable data. In particular a latent kernelized similarity matrix is recovered by rank minimization on multiple kernel-based similarity matrices. Extensive experiments on real-world multi-view datasets validate the efficacy of our method in the presence of error corruptions.</p>\n", "tags": ["ARXIV", "Cross Modal", "Graph", "Independent"], "tsne_embedding": [5.795926094055176, -3.4937045574188232]}, {"key": "wu2017deep", "year": "2017", "title": "Deep Supervised Hashing for Multi-Label and Large-Scale Image Retrieval", "abstract": "<p>One of the most challenging tasks in large-scale multi-label image retrieval is to map images into binary codes while preserving multilevel semantic similarity. Recently, several deep supervised hashing methods have been proposed to learn hash functions that preserve multilevel semantic similarity with deep convolutional neural networks. However, these triplet label based methods try to preserve the ranking order of images according to their similarity degrees to the queries while not putting direct constraints on the distance between the codes of very similar images. Besides, the current evaluation criteria are not able to measure the performance of existing hashing methods on preserving fine-grained multilevel semantic similarity. To tackle these issues, we propose a novel Deep Multilevel Semantic Similarity Preserving Hashing (DMSSPH) method to learn compact similarity-preserving binary codes for the huge body of multi-label image data with deep convolutional neural networks. In our approach, we make the best of the supervised information in the form of pairwise labels to maximize the discriminability of output binary codes. Extensive evaluations conducted on several benchmark datasets demonstrate that the proposed method significantly outperforms the state-of-the-art supervised and unsupervised hashing methods at the accuracies of top returned images, especially for shorter binary codes. Meanwhile, the proposed method shows better performance on preserving fine-grained multilevel semantic similarity according to the results under the Jaccard coefficient based evaluation criteria we propose.</p>\n", "tags": ["Deep Learning", "Image Retrieval", "Supervised"], "tsne_embedding": [-8.833621978759766, 6.794628620147705]}, {"key": "wu2017improved", "year": "2017", "title": "Improved Consistent Weighted Sampling Revisited", "abstract": "<p>Min-Hash is a popular technique for efficiently estimating the Jaccard similarity of binary sets. Consistent Weighted Sampling (CWS) generalizes the Min-Hash scheme to sketch weighted sets and has drawn increasing interest from the community. Due to its constant-time complexity independent of the values of the weights Improved CWS (ICWS) is considered as the state-of-the-art CWS algorithm. In this paper we revisit ICWS and analyze its underlying mechanism to show that there actually exists dependence between the two components of the hash-code produced by ICWS which violates the condition of independence. To remedy the problem we propose an Improved ICWS (I(^2)CWS) algorithm which not only shares the same theoretical computational complexity as ICWS but also abides by the required conditions of the CWS scheme. The experimental results on a number of synthetic data sets and real-world text data sets demonstrate that our I(^2)CWS algorithm can estimate the Jaccard similarity more accurately and also compete with or outperform the compared methods including ICWS in classification and top-(K) retrieval after relieving the underlying dependence.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [-13.406837463378906, 2.339830160140991]}, {"key": "wu2018cycle", "year": "2018", "title": "Cycle-consistent Deep Generative Hashing For Cross-modal Retrieval", "abstract": "<p>In this paper we propose a novel deep generative approach to cross-modal retrieval to learn hash functions in the absence of paired training samples through the cycle consistency loss. Our proposed approach employs adversarial training scheme to lean a couple of hash functions enabling translation between modalities while assuming the underlying semantic relationship. To induce the hash codes with semantics to the input-output pair cycle consistency loss is further proposed upon the adversarial training to strengthen the correlations between inputs and corresponding outputs. Our approach is generative to learn hash functions such that the learned hash codes can maximally correlate each input-output correspondence meanwhile can also regenerate the inputs so as to minimize the information loss. The learning to hash embedding is thus performed to jointly optimize the parameters of the hash functions across modalities as well as the associated generative models. Extensive experiments on a variety of large-scale cross-modal data sets demonstrate that our proposed method achieves better retrieval results than the state-of-the-arts.</p>\n", "tags": ["ARXIV", "Cross Modal", "Independent"], "tsne_embedding": [-3.567883014678955, -3.05452823638916]}, {"key": "wu2018learning", "year": "2018", "title": "Learning Effective Binary Visual Representations With Deep Networks", "abstract": "<p>Although traditionally binary visual representations are mainly designed to reduce computational and storage costs in the image retrieval research this paper argues that binary visual representations can be applied to large scale recognition and detection problems in addition to hashing in retrieval. Furthermore the binary nature may make it generalize better than its real-valued counterparts. Existing binary hashing methods are either two-stage or hinging on loss term regularization or saturated functions hence converge slowly and only emit soft binary values. This paper proposes Approximately Binary Clamping (ABC) which is non-saturating end-to-end trainable with fast convergence and can output true binary visual representations. ABC achieves comparable accuracy in ImageNet classification as its real-valued counterpart and even generalizes better in object detection. On benchmark image retrieval datasets ABC also outperforms existing hashing methods.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Supervised"], "tsne_embedding": [-6.025726795196533, 15.416484832763672]}, {"key": "wu2019deep", "year": "2019", "title": "Deep Incremental Hashing Network for Efficient Image Retrieval", "abstract": "<p>Hashing has shown great potential in large-scale image retrieval due to its storage and computation efficiency, especially the recent deep supervised hashing methods. To achieve promising performance, deep supervised hashing methods require a large amount of training data from different classes. However, when images of new categories emerge, existing deep hashing methods have to retrain the CNN model and generate hash codes for all the database images again, which is impractical for large-scale retrieval system.\nIn this paper, we propose a novel deep hashing framework, called Deep Incremental Hashing Network (DIHN), for learning hash codes in an incremental manner. DIHN learns the hash codes for the new coming images directly, while keeping the old ones unchanged. Simultaneously, a deep hash function for query set is learned by preserving the similarities between training points. Extensive experiments on two widely used image retrieval benchmarks demonstrate that the proposed DIHN framework can significantly decrease the training time while keeping the state-of-the-art retrieval accuracy.</p>\n", "tags": [], "tsne_embedding": [3.364570379257202, 11.874237060546875]}, {"key": "wu2021online", "year": "2021", "title": "Online Enhanced Semantic Hashing Towards Effective And Efficient Retrieval For Streaming Multi-modal Data", "abstract": "<p>With the vigorous development of multimedia equipment and applications efficient retrieval of large-scale multi-modal data has become a trendy research topic. Thereinto hashing has become a prevalent choice due to its retrieval efficiency and low storage cost. Although multi-modal hashing has drawn lots of attention in recent years there still remain some problems. The first point is that existing methods are mainly designed in batch mode and not able to efficiently handle streaming multi-modal data. The second point is that all existing online multi-modal hashing methods fail to effectively handle unseen new classes which come continuously with streaming data chunks. In this paper we propose a new model termed Online enhAnced SemantIc haShing (OASIS). We design novel semantic-enhanced representation for data which could help handle the new coming classes and thereby construct the enhanced semantic objective function. An efficient and effective discrete online optimization algorithm is further proposed for OASIS. Extensive experiments show that our method can exceed the state-of-the-art models. For good reproducibility and benefiting the community our code and data are already available in supplementary material and will be made publicly available.</p>\n", "tags": ["ARXIV", "Streaming Data"], "tsne_embedding": [-7.625260353088379, -8.155010223388672]}, {"key": "wu2022self", "year": "2022", "title": "Self-adaptive In-context Learning An Information Compression Perspective For In-context Example Selection And Ordering", "abstract": "<p>Despite the surprising few-shot performance of in-context learning (ICL) it is still a common practice to randomly sample examples to serve as context. This paper advocates a new principle for ICL self-adaptive in-context learning. The self-adaption mechanism is introduced to help each sample find an in-context example permutation (i.e. selection and ordering) that can derive the correct prediction thus maximizing performance. To validate the effectiveness of self-adaptive ICL we propose a general select-then-rank framework and instantiate it with new selection and ranking algorithms. Upon extensive evaluation on eight different NLP datasets our self-adaptive ICL method achieves a 4037; relative improvement over the common practice setting. Further analysis reveals the enormous potential of self-adaptive ICL that it might be able to close the gap between ICL and finetuning given more advanced algorithms. Our code is released to facilitate future research in this area https://github.com/Shark-NLP/self-adaptive-ICL</p>\n", "tags": ["ARXIV", "Has Code", "Independent"], "tsne_embedding": [11.607529640197754, 1.2106142044067383]}, {"key": "wu2023plan", "year": "2023", "title": "Plan Eliminate And Track -- Language Models Are Good Teachers For Embodied Agents", "abstract": "<p>Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLMs ability to generate abstract plans to simplify challenging control tasks either by action scoring or action modeling (fine-tuning). However the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent e.g. limited input lengths fine-tuning inefficiency bias from pre-training and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor we propose to instead use the knowledge in LLMs to simplify the control problem rather than solving it. We propose the Plan Eliminate and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally the Track module determines whether the agent has accomplished each sub-task. On the AlfWorld instruction following benchmark the PET framework leads to a significant 1537; improvement over SOTA for generalization to human goal specifications.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [34.042449951171875, -16.682903289794922]}, {"key": "wu2023survey", "year": "2023", "title": "A Survey On Large Language Models For Recommendation", "abstract": "<p>Large Language Models (LLMs) have emerged as powerful tools in the field of Natural Language Processing (NLP) and have recently gained significant attention in the domain of Recommendation Systems (RS). These models trained on massive amounts of data using self-supervised learning have demonstrated remarkable success in learning universal representations and have the potential to enhance various aspects of recommendation systems by some effective transfer techniques such as fine-tuning and prompt tuning and so on. The crucial aspect of harnessing the power of language models in enhancing recommendation quality is the utilization of their high-quality representations of textual features and their extensive coverage of external knowledge to establish correlations between items and users. To provide a comprehensive understanding of the existing LLM-based recommendation systems this survey presents a taxonomy that categorizes these models into two major paradigms respectively Discriminative LLM for Recommendation (DLLM4Rec) and Generative LLM for Recommendation (GLLM4Rec) with the latter being systematically sorted out for the first time. Furthermore we systematically review and analyze existing LLM-based recommendation systems within each paradigm providing insights into their methodologies techniques and performance. Additionally we identify key challenges and several valuable findings to provide researchers and practitioners with inspiration. We have also created a GitHub repository to index relevant papers on LLMs for recommendation https://github.com/WLiK/LLM4Rec.</p>\n", "tags": ["ARXIV", "Has Code", "Supervised", "Survey Paper"], "tsne_embedding": [39.49952697753906, -10.040258407592773]}, {"key": "wu2023visual", "year": "2023", "title": "Visual Chatgpt Talking Drawing And Editing With Visual Foundation Models", "abstract": "<p>ChatGPT is attracting a cross-field interest as it provides a language interface with remarkable conversational competency and reasoning capabilities across many domains. However since ChatGPT is trained with languages it is currently not capable of processing or generating images from the visual world. At the same time Visual Foundation Models such as Visual Transformers or Stable Diffusion although showing great visual understanding and generation capabilities they are only experts on specific tasks with one-round fixed inputs and outputs. To this end We build a system called textbfVisual ChatGPT incorporating different Visual Foundation Models to enable the user to interact with ChatGPT by 1) sending and receiving not only languages but also images 2) providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps. 3) providing feedback and asking for corrected results. We design a series of prompts to inject the visual model information into ChatGPT considering models of multiple inputs/outputs and models that require visual feedback. Experiments show that Visual ChatGPT opens the door to investigating the visual roles of ChatGPT with the help of Visual Foundation Models. Our system is publicly available at urlhttps://github.com/microsoft/visual-chatgpt}.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [39.19938659667969, 6.383162021636963]}, {"key": "wu2024empirical", "year": "2024", "title": "An Empirical Analysis Of Compute-optimal Inference For Problem-solving With Language Models", "abstract": "<p>The optimal training configurations of large language models (LLMs) with respect to model sizes and compute budgets have been extensively studied. But how to optimally configure LLMs during inference has not been explored in sufficient depth. We study compute-optimal inference designing models and inference strategies that optimally trade off additional inference-time compute for improved performance. As a first step towards understanding and designing compute-optimal inference methods we assessed the effectiveness and computational efficiency of multiple inference strategies such as Greedy Search Majority Voting Best-of-N Weighted Voting and their variants on two different Tree Search algorithms involving different model sizes and computational budgets. We found that a smaller language model with a novel tree search algorithm typically achieves a Pareto-optimal trade-off. These results highlight the potential benefits of deploying smaller models equipped with more sophisticated decoding algorithms in budget-constrained scenarios e.g. on end-devices to enhance problem-solving accuracy. For instance we show that the Llemma-7B model can achieve competitive accuracy to a Llemma-34B model on MATH500 while using (2times) less FLOPs. Our findings could potentially apply to any generation task with a well-defined measure of success.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [8.873005867004395, -3.498537540435791]}, {"key": "wu2024evaluating", "year": "2024", "title": "Evaluating And Analyzing Relationship Hallucinations In Large Vision-language Models", "abstract": "<p>The issue of hallucinations is a prevalent concern in existing Large Vision-Language Models (LVLMs). Previous efforts have primarily focused on investigating object hallucinations which can be easily alleviated by introducing object detectors. However these efforts neglect hallucinations in inter-object relationships which is essential for visual comprehension. In this work we introduce R-Bench a novel benchmark for evaluating Vision Relationship Hallucination. R-Bench features image-level questions that focus on the existence of relationships and instance-level questions that assess local visual comprehension. We identify three types of relationship co-occurrences that lead to hallucinations relationship-relationship subject-relationship and relationship-object. The visual instruction tuning datasets long-tail distribution significantly impacts LVLMs understanding of visual relationships. Furthermore our analysis reveals that current LVLMs tend to disregard visual content and overly rely on the common sense knowledge of Large Language Models. They also struggle with reasoning about spatial relationships based on contextual information.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [49.872528076171875, 8.098515510559082]}, {"key": "wu2024logical", "year": "2024", "title": "Logical Closed Loop Uncovering Object Hallucinations In Large Vision-language Models", "abstract": "<p>Object hallucination has been an Achilles heel which hinders the broader applications of large vision-language models (LVLMs). Object hallucination refers to the phenomenon that the LVLMs claim non-existent objects in the image. To mitigate the object hallucinations instruction tuning and external model-based detection methods have been proposed which either require large-scare computational resources or depend on the detection result of external models. However there remains an under-explored field to utilize the LVLM itself to alleviate object hallucinations. In this work we adopt the intuition that the LVLM tends to respond logically consistently for existent objects but inconsistently for hallucinated objects. Therefore we propose a Logical Closed Loop-based framework for Object Hallucination Detection and Mitigation namely LogicCheckGPT. In specific we devise logical consistency probing to raise questions with logical correlations inquiring about attributes from objects and vice versa. Whether their responses can form a logical closed loop serves as an indicator of object hallucination. As a plug-and-play method it can be seamlessly applied to all existing LVLMs. Comprehensive experiments conducted on three benchmarks across four LVLMs have demonstrated significant improvements brought by our method indicating its effectiveness and generality.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [51.44734191894531, 5.787734508514404]}, {"key": "wurzer2016randomised", "year": "2016", "title": "Randomised Relevance Model", "abstract": "<p>Relevance Models are well-known retrieval models and capable of producing competitive results. However because they use query expansion they can be very slow. We address this slowness by incorporating two variants of locality sensitive hashing (LSH) into the query expansion process. Results on two document collections suggest that we can obtain large reductions in the amount of work with a small reduction in effectiveness. Our approach is shown to be additive when pruning query terms.</p>\n", "tags": ["ARXIV", "Independent", "LSH"], "tsne_embedding": [-25.156633377075195, -11.1112699508667]}, {"key": "wurzer2022parameterizing", "year": "2022", "title": "Parameterizing Kterm Hashing", "abstract": "<p>Kterm Hashing provides an innovative approach to novelty detection on massive data streams. Previous research focused on maximizing the efficiency of Kterm Hashing and succeeded in scaling First Story Detection to Twitter-size data stream without sacrificing detection accuracy. In this paper we focus on improving the effectiveness of Kterm Hashing. Traditionally all kterms are considered as equally important when calculating a documents degree of novelty with respect to the past. We believe that certain kterms are more important than others and hypothesize that uniform kterm weights are sub-optimal for determining novelty in data streams. To validate our hypothesis we parameterize Kterm Hashing by assigning weights to kterms based on their characteristics. Our experiments apply Kterm Hashing in a First Story Detection setting and reveal that parameterized Kterm Hashing can surpass state-of-the-art detection accuracy and significantly outperform the uniformly weighted approach.</p>\n", "tags": ["SIGIR"], "tsne_embedding": [-28.488929748535156, -5.282887935638428]}, {"key": "wygocki2017fast", "year": "2017", "title": "On Fast Bounded Locality Sensitive Hashing", "abstract": "<p>In this paper we examine the hash functions expressed as scalar products i.e. f(x)=&lt;vx for some bounded random vector v. Such hash functions have numerous applications but often there is a need to optimize the choice of the distribution of v. In the present work we focus on so-called anti-concentration bounds i.e. the upper bounds of (mathbbP)(left)&lt;vx &lt; (alpha) (right). In many applications v is a vector of independent random variables with standard normal distribution. In such case the distribution of &lt;vx is also normal and it is easy to approximate (mathbbP)(left)&lt;vx &lt; (alpha) (right). Here we consider two bounded distributions in the context of the anti-concentration bounds. Particularly we analyze v being a random vector from the unit ball in l_(infty) and v being a random vector from the unit sphere in l_2. We show optimal up to a constant anti-concentration measures for functions f(x)=&lt;vx. As a consequence of our research we obtain new best results for (newline) (textit)c-approximate nearest neighbors without false negatives for l_p in high dimensional space for all p(in)1(infty) for c=(Omega)((max)(sqrtd)d^1/p). These results improve over those presented in 16. Finally our paper reports progress on answering the open problem by Pagh~17 who considered the nearest neighbor search without false negatives for the Hamming distance.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-35.0151481628418, -3.7711431980133057]}, {"key": "w\u00f6hnert2023study", "year": "2023", "title": "A Study On The Use Of Perceptual Hashing To Detect Manipulation Of Embedded Messages In Images", "abstract": "<p>Typically metadata of images are stored in a specific data segment of the image file. However to securely detect changes data can also be embedded within images. This follows the goal to invisibly and robustly embed as much information as possible to ideally even survive compression. This work searches for embedding principles which allow to distinguish between unintended changes by lossy image compression and malicious manipulation of the embedded message based on the change of its perceptual or robust hash. Different embedding and compression algorithms are compared. The study shows that embedding a message via integer wavelet transform and compression with Karhunen-Loeve-transform yields the best results. However it was not possible to distinguish between manipulation and compression in all cases.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-14.894780158996582, 14.145133972167969]}, {"key": "xi2023rise", "year": "2023", "title": "The Rise And Potential Of Large Language Model Based Agents A Survey", "abstract": "<p>For a long time humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment make decisions and take actions. Many efforts have been made to develop intelligent agents but they mainly focus on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually what the community lacks is a general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile capabilities they demonstrate large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI) offering hope for building general AI agents. Many researchers have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. In this paper we perform a comprehensive survey on LLM-based agents. We start by tracing the concept of agents from its philosophical origins to its development in AI and explain why LLMs are suitable foundations for agents. Building upon this we present a general framework for LLM-based agents comprising three main components brain perception and action and the framework can be tailored for different applications. Subsequently we explore the extensive applications of LLM-based agents in three aspects single-agent scenarios multi-agent scenarios and human-agent cooperation. Following this we delve into agent societies exploring the behavior and personality of LLM-based agents the social phenomena that emerge from an agent society and the insights they offer for human society. Finally we discuss several key topics and open problems within the field. A repository for the related papers at https://github.com/WooooDyy/LLM-Agent-Paper-List.</p>\n", "tags": ["ARXIV", "Has Code", "Survey Paper"], "tsne_embedding": [30.993118286132812, -16.07436752319336]}, {"key": "xi2023self", "year": "2023", "title": "Self-polish Enhance Reasoning In Large Language Models Via Problem Refinement", "abstract": "<p>To enhance the multi-step reasoning capabilities of large language models researchers have extensively explored prompting methods notably the Chain-of-Thought (CoT) method which explicitly elicits human-like rationales. However they have inadvertently overlooked the potential of enhancing model reasoning performance by formulating higher-quality problems. In this work we start from the problem side and propose Self-Polish (SP) a novel method that facilitates the models reasoning by guiding it to progressively refine the given problems to be more comprehensible and solvable. We also explore several automatic prompting varients and propose the Self-Polish prompt bank for the community. SP is orthogonal to all other prompting methods of answer/reasoning side like CoT allowing for seamless integration with state-of-the-art techniques for further improvement. Thorough experiments show that the proposed method attains notable and consistent effectiveness on five reasoning benchmarks across different models. Furthermore our method also showcases impressive performance on robustness evaluation. Codes and prompts are available at https://github.com/WooooDyy/Self-Polish.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [36.65146255493164, 3.719773054122925]}, {"key": "xi2024agentgym", "year": "2024", "title": "Agentgym Evolving Large Language Model-based Agents Across Diverse Environments", "abstract": "<p>Building generalist agents that can handle diverse tasks and evolve themselves across different environments is a long-term goal in the AI community. Large language models (LLMs) are considered a promising foundation to build such agents due to their generalized capabilities. Current approaches either have LLM-based agents imitate expert-provided trajectories step-by-step requiring human supervision which is hard to scale and limits environmental exploration; or they let agents explore and learn in isolated environments resulting in specialist agents with limited generalization. In this paper we take the first step towards building generally-capable LLM-based agents with self-evolution ability. We identify a trinity of ingredients 1) diverse environments for agent exploration and learning 2) a trajectory set to equip agents with basic capabilities and prior knowledge and 3) an effective and scalable evolution method. We propose AgentGym a new framework featuring a variety of environments and tasks for broad real-time uni-format and concurrent agent exploration. AgentGym also includes a database with expanded instructions a benchmark suite and high-quality trajectories across environments. Next we propose a novel method AgentEvol to investigate the potential of agent self-evolution beyond previously seen data across tasks and environments. Experimental results show that the evolved agents can achieve results comparable to SOTA models. We release the AgentGym suite including the platform dataset benchmark checkpoints and algorithm implementations. The AgentGym suite is available on https://github.com/WooooDyy/AgentGym.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [30.875577926635742, -15.907876014709473]}, {"key": "xi2024training", "year": "2024", "title": "Training Large Language Models For Reasoning Through Reverse Curriculum Reinforcement Learning", "abstract": "<p>In this paper we propose R(^3) Learning Reasoning through Reverse Curriculum Reinforcement Learning (RL) a novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models. The core challenge in applying RL to complex reasoning is to identify a sequence of actions that result in positive rewards and provide appropriate supervision for optimization. Outcome supervision provides sparse rewards for final results without identifying error locations whereas process supervision offers step-wise rewards but requires extensive manual annotation. R(^3) overcomes these limitations by learning from correct demonstrations. Specifically R(^3) progressively slides the start state of reasoning from a demonstrations end to its beginning facilitating easier model exploration at all stages. Thus R(^3) establishes a step-wise curriculum allowing outcome supervision to offer step-level signals and precisely pinpoint errors. Using Llama2-7B our method surpasses RL baseline on eight reasoning tasks by (4.1) points on average. Notebaly in program-based reasoning on GSM8K it exceeds the baseline by (4.2) points across three backbone models and without any extra data Codellama-7B + R(^3) performs comparable to larger models or closed-source models.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [30.62869644165039, 5.418755054473877]}, {"key": "xia2014supervised", "year": "2014", "title": "Supervised Hashing via Image Representation Learning", "abstract": "<p>Hashing is a popular approximate nearest neighbor\nsearch approach for large-scale image retrieval.\nSupervised hashing, which incorporates similarity/dissimilarity\ninformation on entity pairs to improve\nthe quality of hashing function learning, has recently\nreceived increasing attention. However, in the existing\nsupervised hashing methods for images, an input\nimage is usually encoded by a vector of hand-crafted\nvisual features. Such hand-crafted feature vectors\ndo not necessarily preserve the accurate semantic\nsimilarities of images pairs, which may often degrade\nthe performance of hashing function learning. In this\npaper, we propose a supervised hashing method for\nimage retrieval, in which we automatically learn a good\nimage representation tailored to hashing as well as a\nset of hash functions. The proposed method has two\nstages. In the first stage, given the pairwise similarity\nmatrix S over training images, we propose a scalable\ncoordinate descent method to decompose S into a\nproduct of HHT where H is a matrix with each of its\nrows being the approximate hash code associated to\na training image. In the second stage, we propose to\nsimultaneously learn a good feature representation for\nthe input images as well as a set of hash functions, via\na deep convolutional network tailored to the learned\nhash codes in H and optionally the discrete class labels\nof the images. Extensive empirical evaluations on three\nbenchmark datasets with different kinds of images\nshow that the proposed method has superior performance\ngains over several state-of-the-art supervised\nand unsupervised hashing methods.</p>\n", "tags": [], "tsne_embedding": [-7.406062602996826, 8.183759689331055]}, {"key": "xia2016unsupervised", "year": "2016", "title": "Unsupervised Deep Hashing For Large-scale Visual Search", "abstract": "<p>Learning based hashing plays a pivotal role in large-scale visual search. However most existing hashing algorithms tend to learn shallow models that do not seek representative binary codes. In this paper we propose a novel hashing approach based on unsupervised deep learning to hierarchically transform features into hash codes. Within the heterogeneous deep hashing framework the autoencoder layers with specific constraints are considered to model the nonlinear mapping between features and binary codes. Then a Restricted Boltzmann Machine (RBM) layer with constraints is utilized to reduce the dimension in the hamming space. Extensive experiments on the problem of visual search demonstrate the competitiveness of our proposed approach compared to state-of-the-art.</p>\n", "tags": ["Cross Modal", "Deep Learning", "Unsupervised"], "tsne_embedding": [0.4015893042087555, 8.9132661819458]}, {"key": "xiao2023unsupervised", "year": "2023", "title": "Unsupervised Multi-criteria Adversarial Detection In Deep Image Retrieval", "abstract": "<p>The vulnerability in the algorithm supply chain of deep learning has imposed new challenges to image retrieval systems in the downstream. Among a variety of techniques deep hashing is gaining popularity. As it inherits the algorithmic backend from deep learning a handful of attacks are recently proposed to disrupt normal image retrieval. Unfortunately the defense strategies in softmax classification are not readily available to be applied in the image retrieval domain. In this paper we propose an efficient and unsupervised scheme to identify unique adversarial behaviors in the hamming space. In particular we design three criteria from the perspectives of hamming distance quantization loss and denoising to defend against both untargeted and targeted attacks which collectively limit the adversarial space. The extensive experiments on four datasets demonstrate 2-2337; improvements of detection rates with minimum computational overhead for real-time image queries.</p>\n", "tags": ["ARXIV", "Deep Learning", "Image Retrieval", "Quantisation", "Unsupervised"], "tsne_embedding": [5.685303688049316, 11.59933090209961]}, {"key": "xiao2024detecting", "year": "2024", "title": "Detecting And Mitigating Hallucination In Large Vision Language Models Via Fine-grained AI Feedback", "abstract": "<p>The rapidly developing Large Vision Language Models (LVLMs) have shown notable capabilities on a range of multi-modal tasks but still face the hallucination phenomena where the generated texts do not align with the given contexts significantly restricting the usages of LVLMs. Most previous work detects and mitigates hallucination at the coarse-grained level or requires expensive annotation (e.g. labeling by proprietary models or human experts). To address these issues we propose detecting and mitigating hallucinations in LVLMs via fine-grained AI feedback. The basic idea is that we generate a small-size sentence-level hallucination annotation dataset by proprietary models whereby we train a hallucination detection model which can perform sentence-level hallucination detection covering primary hallucination types (i.e. object attribute and relationship). Then we propose a detect-then-rewrite pipeline to automatically construct preference dataset for training hallucination mitigating model. Furthermore we propose differentiating the severity of hallucinations and introducing a Hallucination Severity-Aware Direct Preference Optimization (HSA-DPO) for mitigating hallucination in LVLMs by incorporating the severity of hallucinations into preference learning. Extensive experiments demonstrate the effectiveness of our method.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [50.24549102783203, 7.191526412963867]}, {"key": "xie2013kernelized", "year": "2013", "title": "Kernelized Locality-sensitive Hashing For Semi-supervised Agglomerative Clustering", "abstract": "<p>Large scale agglomerative clustering is hindered by computational burdens. We propose a novel scheme where exact inter-instance distance calculation is replaced by the Hamming distance between Kernelized Locality-Sensitive Hashing (KLSH) hashed values. This results in a method that drastically decreases computation time. Additionally we take advantage of certain labeled data points via distance metric learning to achieve a competitive precision and recall comparing to K-Means but in much less computation time.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [-25.66868019104004, -10.768970489501953]}, {"key": "xin2022jiuzhang", "year": "2022", "title": "Jiuzhang A Chinese Pre-trained Language Model For Mathematical Problem Understanding", "abstract": "<p>This paper aims to advance the mathematical intelligence of machines by presenting the first Chinese mathematical pre-trained language model~(PLM) for effectively understanding and representing mathematical problems. Unlike other standard NLP tasks mathematical texts are difficult to understand since they involve mathematical terminology symbols and formulas in the problem statement. Typically it requires complex mathematical logic and background knowledge for solving mathematical problems. Considering the complex nature of mathematical texts we design a novel curriculum pre-training approach for improving the learning of mathematical PLMs consisting of both basic and advanced courses. Specially we first perform token-level pre-training based on a position-biased masking strategy and then design logic-based pre-training tasks that aim to recover the shuffled sentences and formulas respectively. Finally we introduce a more difficult pre-training task that enforces the PLM to detect and correct the errors in its generated solutions. We conduct extensive experiments on offline evaluation (including nine math-related tasks) and online (A/B) test. Experimental results demonstrate the effectiveness of our approach compared with a number of competitive baselines. Our code is available at textcolorblueurlhttps://github.com/RUCAIBox/JiuZhang}}.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [26.98065757751465, 3.056847095489502]}, {"key": "xin2023survey", "year": "2023", "title": "A Survey Of Large Language Models", "abstract": "<p>Language is essentially a complex intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach language modeling has been widely studied for language understanding and generation in the past two decades evolving from statistical language models to neural language models. Recently pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement they further study the scaling effect by increasing the model size to an even larger size. Interestingly when the parameter scale exceeds a certain level these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently the research on LLMs has been largely advanced by both academia and industry and a remarkable progress is the launch of ChatGPT which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community which would revolutionize the way how we develop and use AI algorithms. In this survey we review the recent advances of LLMs by introducing the background key findings and mainstream techniques. In particular we focus on four major aspects of LLMs namely pre-training adaptation tuning utilization and capacity evaluation. Besides we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.</p>\n", "tags": ["ARXIV", "Survey Paper"], "tsne_embedding": [32.78147888183594, -11.965749740600586]}, {"key": "xiong2014using", "year": "2014", "title": "Adaptive Quantization for Hashing: An Information-Based Approach to Learning Binary Codes", "abstract": "<p>Large-scale data mining and retrieval applications have\nincreasingly turned to compact binary data representations\nas a way to achieve both fast queries and efficient\ndata storage; many algorithms have been proposed for\nlearning effective binary encodings. Most of these algorithms\nfocus on learning a set of projection hyperplanes\nfor the data and simply binarizing the result from each\nhyperplane, but this neglects the fact that informativeness\nmay not be uniformly distributed across the projections.\nIn this paper, we address this issue by proposing\na novel adaptive quantization (AQ) strategy that\nadaptively assigns varying numbers of bits to different\nhyperplanes based on their information content. Our\nmethod provides an information-based schema that preserves\nthe neighborhood structure of data points, and\nwe jointly find the globally optimal bit-allocation for\nall hyperplanes. In our experiments, we compare with\nstate-of-the-art methods on four large-scale datasets\nand find that our adaptive quantization approach significantly\nimproves on traditional hashing methods.</p>\n", "tags": ["Has Code", "Quantisation"], "tsne_embedding": [-22.914356231689453, 12.181916236877441]}, {"key": "xirau2014fast", "year": "2014", "title": "Fast Approximate Nearest-Neighbor Field by Cascaded Spherical Hashing", "abstract": "<p>We present an efficient and fast algorithm for computing approximate nearest neighbor fields between two images. Our method builds on the concept of Coherency-Sensitive Hashing (CSH), but uses a recent hashing scheme, Spherical Hashing (SpH), which is known to be better adapted to the nearest-neighbor problem for natural images. Cascaded Spherical Hashing concatenates different configurations of SpH to build larger Hash Tables with less elements in each bin to achieve higher selectivity. Our method amply outperforms existing techniques like PatchMatch and CSH, and the experimental results show that our algorithm is faster and more accurate than existing methods.</p>\n", "tags": ["Image Retrieval"], "tsne_embedding": [-29.1517333984375, 4.782376766204834]}, {"key": "xu2013harmonious", "year": "2013", "title": "Harmonious Hashing", "abstract": "<p>Hashing-based fast nearest neighbor search technique\nhas attracted great attention in both research\nand industry areas recently. Many existing hashing\napproaches encode data with projection-based hash\nfunctions and represent each projected dimension\nby 1-bit. However, the dimensions with high variance\nhold large energy or information of data but\ntreated equivalently as dimensions with low variance,\nwhich leads to a serious information loss. In\nthis paper, we introduce a novel hashing algorithm\ncalled Harmonious Hashing which aims at learning\nhash functions with low information loss. Specifically,\nwe learn a set of optimized projections to\npreserve the maximum cumulative energy and meet\nthe constraint of equivalent variance on each dimension\nas much as possible. In this way, we could\nminimize the information loss after binarization.\nDespite the extreme simplicity, our method outperforms\nsuperiorly to many state-of-the-art hashing\nmethods in large-scale and high-dimensional nearest\nneighbor search experiments.</p>\n", "tags": [], "tsne_embedding": [-28.080480575561523, -6.09753942489624]}, {"key": "xu2015convolutional", "year": "2015", "title": "Convolutional Neural Networks for Text Hashing", "abstract": "<p>Hashing, as a popular approximate nearest neighbor\nsearch, has been widely used for large-scale similarity search. Recently, a spectrum of machine learning\nmethods are utilized to learn similarity-preserving\nbinary codes. However, most of them directly encode the explicit features, keywords, which fail to\npreserve the accurate semantic similarities in binary code beyond keyword matching, especially on\nshort texts. Here we propose a novel text hashing\nframework with convolutional neural networks. In\nparticular, we first embed the keyword features into\ncompact binary code with a locality preserving constraint. Meanwhile word features and position features are together fed into a convolutional network to\nlearn the implicit features which are further incorporated with the explicit features to fit the pre-trained\nbinary code. Such base method can be successfully\naccomplished without any external tags/labels, and\nother three model variations are designed to integrate tags/labels. Experimental results show the\nsuperiority of our proposed approach over several\nstate-of-the-art hashing methods when tested on one\nshort text dataset as well as one normal text dataset.</p>\n", "tags": [], "tsne_embedding": [-5.893045425415039, -3.9721791744232178]}, {"key": "xu2015short", "year": "2015", "title": "Short Text Hashing Improved By Integrating Multi-granularity Topics And Tags", "abstract": "<p>Due to computational and storage efficiencies of compact binary codes hashing has been widely used for large-scale similarity search. Unfortunately many existing hashing methods based on observed keyword features are not effective for short texts due to the sparseness and shortness. Recently some researchers try to utilize latent topics of certain granularity to preserve semantic similarity in hash codes beyond keyword matching. However topics of certain granularity are not adequate to represent the intrinsic semantic information. In this paper we present a novel unified approach for short text Hashing using Multi-granularity Topics and Tags dubbed HMTT. In particular we propose a selection method to choose the optimal multi-granularity topics depending on the type of dataset and design two distinct hashing strategies to incorporate multi-granularity topics. We also propose a simple and effective method to exploit tags to enhance the similarity of related texts. We carry out extensive experiments on one short text dataset as well as on one normal text dataset. The results demonstrate that our approach is effective and significantly outperforms baselines on several evaluation metrics.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-9.998242378234863, -1.4156204462051392]}, {"key": "xu2016binary", "year": "2016", "title": "Binary Subspace Coding For Query-by-image Video Retrieval", "abstract": "<p>The query-by-image video retrieval (QBIVR) task has been attracting considerable research attention recently. However most existing methods represent a video by either aggregating or projecting all its frames into a single datum point which may easily cause severe information loss. In this paper we propose an efficient QBIVR framework to enable an effective and efficient video search with image query. We first define a similarity-preserving distance metric between an image and its orthogonal projection in the subspace of the video which can be equivalently transformed to a Maximum Inner Product Search (MIPS) problem. Besides to boost the efficiency of solving the MIPS problem we propose two asymmetric hashing schemes which bridge the domain gap of images and videos. The first approach termed Inner-product Binary Coding (IBC) preserves the inner relationships of images and videos in a common Hamming space. To further improve the retrieval efficiency we devise a Bilinear Binary Coding (BBC) approach which employs compact bilinear projections instead of a single large projection matrix. Extensive experiments have been conducted on four real-world video datasets to verify the effectiveness of our proposed approaches as compared to the state-of-the-arts.</p>\n", "tags": ["ARXIV", "Independent", "Video Retrieval"], "tsne_embedding": [-6.815699100494385, -24.755615234375]}, {"key": "xu2018error", "year": "2018", "title": "Error Correction Maximization For Deep Image Hashing", "abstract": "<p>We propose to use the concept of the Hamming bound to derive the optimal criteria for learning hash codes with a deep network. In particular when the number of binary hash codes (typically the number of image categories) and code length are known it is possible to derive an upper bound on the minimum Hamming distance between the hash codes. This upper bound can then be used to define the loss function for learning hash codes. By encouraging the margin (minimum Hamming distance) between the hash codes of different image categories to match the upper bound we are able to learn theoretically optimal hash codes. Our experiments show that our method significantly outperforms competing deep learning-based approaches and obtains top performance on benchmark datasets.</p>\n", "tags": ["ARXIV", "Deep Learning", "Independent"], "tsne_embedding": [-5.867895126342773, 8.824326515197754]}, {"key": "xu2018gpu", "year": "2018", "title": "GPU Accelerated Cascade Hashing Image Matching For Large Scale 3D Reconstruction", "abstract": "<p>Image feature point matching is a key step in Structure from Motion(SFM). However it is becoming more and more time consuming because the number of images is getting larger and larger. In this paper we proposed a GPU accelerated image matching method with improved Cascade Hashing. Firstly we propose a Disk-Memory-GPU data exchange strategy and optimize the load order of data so that the proposed method can deal with big data. Next we parallelize the Cascade Hashing method on GPU. An improved parallel reduction and an improved parallel hashing ranking are proposed to fulfill this task. Finally extensive experiments show that our image matching is about 20 times faster than SiftGPU on the same graphics card nearly 100 times faster than the CPU CasHash method and hundreds of times faster than the CPU Kd-Tree based matching method. Further more we introduce the epipolar constraint to the proposed method and use the epipolar geometry to guide the feature matching procedure which further reduces the matching cost.</p>\n", "tags": ["ARXIV", "Graph"], "tsne_embedding": [-23.994037628173828, -15.2805757522583]}, {"key": "xu2019online", "year": "2019", "title": "Online Multi-modal Hashing with Dynamic Query-adaption", "abstract": "<p>Multi-modal hashing is an effective technique to support large-scale multimedia retrieval, due to its capability of encoding heterogeneous multi-modal features into compact and similarity-preserving binary codes. Although great progress has been achieved so far, existing methods still suffer from several problems, including: 1) All existing methods simply adopt fixed modality combination weights in online hashing process to generate the query hash codes. This strategy cannot adaptively capture the variations of different queries. 2) They either suffer from insufficient semantics (for unsupervised methods) or require high computation and storage cost (for the supervised methods, which rely on pair-wise semantic matrix). 3) They solve the hash codes with relaxed optimization strategy or bit-by-bit discrete optimization, which results in significant quantization loss or consumes considerable computation time. To address the above limitations, in this paper, we propose an Online Multi-modal Hashing with Dynamic Query-adaption (OMH-DQ) method in a novel fashion. Specifically, a self-weighted fusion strategy is designed to adaptively preserve the multi-modal feature information into hash codes by exploiting their complementarity. The hash codes are learned with the supervision of pair-wise semantic labels to enhance their discriminative capability, while avoiding the challenging symmetric similarity matrix factorization. Under such learning framework, the binary hash codes can be directly obtained with efficient operations and without quantization errors. Accordingly, our method can benefit from the semantic labels, and simultaneously, avoid the high computation complexity. Moreover, to accurately capture the query variations, at the online retrieval stage, we design a parameter-free online hashing module which can adaptively learn the query hash codes according to the dynamic query contents. Extensive experiments demonstrate the state-of-the-art performance of the proposed approach from various aspects.</p>\n", "tags": ["Cross Modal", "SIGIR", "Supervised"], "tsne_embedding": [-10.150875091552734, -3.4161999225616455]}, {"key": "xu2021hhf", "year": "2021", "title": "HHF Hashing-guided Hinge Function For Deep Hashing Retrieval", "abstract": "<p>Deep hashing has shown promising performance in large-scale image retrieval. However latent codes extracted by Deep Neural Networks (DNNs) will inevitably lose semantic information during the binarization process which damages the retrieval accuracy and makes it challenging. Although many existing approaches perform regularization to alleviate quantization errors we figure out an incompatible conflict between metric learning and quantization learning. The metric loss penalizes the inter-class distances to push different classes unconstrained far away. Worse still it tends to map the latent code deviate from ideal binarization point and generate severe ambiguity in the binarization process. Based on the minimum distance of the binary linear code we creatively propose Hashing-guided Hinge Function (HHF) to avoid such conflict. In detail the carefully-designed inflection point which relies on the hash bit length and category numbers is explicitly adopted to balance the metric term and quantization term. Such a modification prevents the network from falling into local metric optimal minima in deep hashing. Extensive experiments in CIFAR-10 CIFAR-100 ImageNet and MS-COCO show that HHF consistently outperforms existing techniques and is robust and flexible to transplant into other methods. Code is available at https://github.com/JerryXu0129/HHF.\u201d</p>\n", "tags": ["ARXIV", "Has Code", "Image Retrieval", "Quantisation", "Supervised"], "tsne_embedding": [-7.838580131530762, 3.8783135414123535]}, {"key": "xu2022loss", "year": "2022", "title": "Hyp^2 Loss Beyond Hypersphere Metric Space For Multi-label Image Retrieval", "abstract": "<p>Image retrieval has become an increasingly appealing technique with broad multimedia application prospects where deep hashing serves as the dominant branch towards low storage and efficient retrieval. In this paper we carried out in-depth investigations on metric learning in deep hashing for establishing a powerful metric space in multi-label scenarios where the pair loss suffers high computational overhead and converge difficulty while the proxy loss is theoretically incapable of expressing the profound label dependencies and exhibits conflicts in the constructed hypersphere space. To address the problems we propose a novel metric learning framework with Hybrid Proxy-Pair Loss (HyP^2 Loss) that constructs an expressive metric space with efficient training complexity w.r.t. the whole dataset. The proposed HyP^2 Loss focuses on optimizing the hypersphere space by learnable proxies and excavating data-to-data correlations of irrelevant pairs which integrates sufficient data correspondence of pair-based methods and high-efficiency of proxy-based methods. Extensive experiments on four standard multi-label benchmarks justify the proposed method outperforms the state-of-the-art is robust among different hash bits and achieves significant performance gains with a faster more stable convergence speed. Our code is available at https://github.com/JerryXu0129/HyP2-Loss.\u201d</p>\n", "tags": ["ARXIV", "Has Code", "Image Retrieval"], "tsne_embedding": [-7.020232677459717, 6.391818523406982]}, {"key": "xu2023deep", "year": "2023", "title": "Deep Lifelong Cross-modal Hashing", "abstract": "<p>Hashing methods have made significant progress in cross-modal retrieval tasks with fast query speed and low storage cost. Among them deep learning-based hashing achieves better performance on large-scale data due to its excellent extraction and representation ability for nonlinear heterogeneous features. However there are still two main challenges in catastrophic forgetting when data with new categories arrive continuously and time-consuming for non-continuous hashing retrieval to retrain for updating. To this end we in this paper propose a novel deep lifelong cross-modal hashing to achieve lifelong hashing retrieval instead of re-training hash function repeatedly when new data arrive. Specifically we design lifelong learning strategy to update hash functions by directly training the incremental data instead of retraining new hash functions using all the accumulated data which significantly reduce training time. Then we propose lifelong hashing loss to enable original hash codes participate in lifelong learning but remain invariant and further preserve the similarity and dis-similarity among original and incremental hash codes to maintain performance. Additionally considering distribution heterogeneity when new data arriving continuously we introduce multi-label semantic similarity to supervise hash learning and it has been proven that the similarity improves performance with detailed analysis. Experimental results on benchmark datasets show that the proposed methods achieves comparative performance comparing with recent state-of-the-art cross-modal hashing methods and it yields substantial average increments over 2037; in retrieval accuracy and almost reduces over 8037; training time when new data arrives continuously.</p>\n", "tags": ["ARXIV", "Cross Modal", "Deep Learning", "Supervised"], "tsne_embedding": [1.901801347732544, -11.387871742248535]}, {"key": "xu2023lemur", "year": "2023", "title": "Lemur Harmonizing Natural Language And Code For Language Agents", "abstract": "<p>We introduce Lemur and Lemur-Chat openly accessible language models optimized for both natural language and coding capabilities to serve as the backbone of versatile language agents. The evolution from language chat models to functional language agents demands that models not only master human interaction reasoning and planning but also ensure grounding in the relevant environments. This calls for a harmonious blend of language and coding capabilities in the models. Lemur and Lemur-Chat are proposed to address this necessity demonstrating balanced proficiencies in both domains unlike existing open-source models that tend to specialize in either. Through meticulous pre-training using a code-intensive corpus and instruction fine-tuning on text and code data our models achieve state-of-the-art averaged performance across diverse text and coding benchmarks among open-source models. Comprehensive experiments demonstrate Lemurs superiority over existing open-source models and its proficiency across various agent tasks involving human communication tool usage and interaction under fully- and partially- observable environments. The harmonization between natural and programming languages enables Lemur-Chat to significantly narrow the gap with proprietary models on agent abilities providing key insights into developing advanced open-source agents adept at reasoning planning and operating seamlessly across environments. https://github.com/OpenLemur/Lemur</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [29.31540298461914, -11.654845237731934]}, {"key": "xu2023wizardlm", "year": "2023", "title": "Wizardlm Empowering Large Language Models To Follow Complex Instructions", "abstract": "<p>Training large language models (LLMs) with open-domain instruction following data brings colossal success. However manually creating such instruction data is very time-consuming and labor-intensive. Moreover humans may struggle to produce high-complexity instructions. In this paper we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicunas testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation WizardLM achieves more than 9037; capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at https://github.com/nlpxucan/WizardLM</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [26.899160385131836, -10.878182411193848]}, {"key": "xu2024survey", "year": "2024", "title": "A Survey Of Resource-efficient LLM And Multimodal Foundation Models", "abstract": "<p>Large foundation models including large language models (LLMs) vision transformers (ViTs) diffusion and LLM-based multimodal models are revolutionizing the entire machine learning lifecycle from training to deployment. However the substantial advancements in versatility and performance these models offer come at a significant cost in terms of hardware resources. To support the growth of these large models in a scalable and environmentally sustainable way there has been a considerable focus on developing resource-efficient strategies. This survey delves into the critical importance of such research examining both algorithmic and systemic aspects. It offers a comprehensive analysis and valuable insights gleaned from existing literature encompassing a broad array of topics from cutting-edge model architectures and training/serving algorithms to practical system designs and implementations. The goal of this survey is to provide an overarching understanding of how current approaches are tackling the resource challenges posed by large foundation models and to potentially inspire future breakthroughs in this field.</p>\n", "tags": ["ARXIV", "Cross Modal", "Survey Paper"], "tsne_embedding": [28.560636520385742, -17.237117767333984]}, {"key": "xue2020massively", "year": "2020", "title": "Mt5 A Massively Multilingual Pre-trained Text-to-text Transformer", "abstract": "<p>The recent Text-to-Text Transfer Transformer (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper we introduce mT5 a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent accidental translation in the zero-shot setting where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [28.98454475402832, -3.178715229034424]}, {"key": "xue2022cross", "year": "2022", "title": "Cross-scale Context Extracted Hashing For Fine-grained Image Binary Encoding", "abstract": "<p>Deep hashing has been widely applied to large-scale image retrieval tasks owing to efficient computation and low storage cost by encoding high-dimensional image data into binary codes. Since binary codes do not contain as much information as float features the essence of binary encoding is preserving the main context to guarantee retrieval quality. However the existing hashing methods have great limitations on suppressing redundant background information and accurately encoding from Euclidean space to Hamming space by a simple sign function. In order to solve these problems a Cross-Scale Context Extracted Hashing Network (CSCE-Net) is proposed in this paper. Firstly we design a two-branch framework to capture fine-grained local information while maintaining high-level global semantic information. Besides Attention guided Information Extraction module (AIE) is introduced between two branches which suppresses areas of low context information cooperated with global sliding windows. Unlike previous methods our CSCE-Net learns a content-related Dynamic Sign Function (DSF) to replace the original simple sign function. Therefore the proposed CSCE-Net is context-sensitive and able to perform well on accurate image binary encoding. We further demonstrate that our CSCE-Net is superior to the existing hashing methods which improves retrieval performance on standard benchmarks.</p>\n", "tags": ["ARXIV", "Image Retrieval"], "tsne_embedding": [-7.630793571472168, 15.665108680725098]}, {"key": "yan2018norm", "year": "2018", "title": "Norm-ranging LSH For Maximum Inner Product Search", "abstract": "<p>Neyshabur and Srebro proposed SIMPLE-LSH which is the state-of-the-art hashing based algorithm for maximum inner product search (MIPS). We found that the performance of SIMPLE-LSH in both theory and practice suffers from long tails in the 2-norm distribution of real datasets. We propose NORM-RANGING LSH which addresses the excessive normalization problem caused by long tails by partitioning a dataset into sub-datasets and building a hash index for each sub-dataset independently. We prove that NORM-RANGING LSH achieves lower query time complexity than SIMPLE-LSH under mild conditions. We also show that the idea of dataset partitioning can improve another hashing based MIPS algorithm. Experiments show that NORM-RANGING LSH probes much less items than SIMPLE-LSH at the same recall thus significantly benefiting MIPS based applications.</p>\n", "tags": ["Independent", "LSH", "NEURIPS"], "tsne_embedding": [-25.700214385986328, -7.866710186004639]}, {"key": "yan2019deep", "year": "2019", "title": "Deep Hashing by Discriminating Hard Examples", "abstract": "<p>This paper tackles a rarely explored but critical problem within learning to hash, i.e., to learn hash codes that effectively discriminate hard similar and dissimilar examples, to empower large-scale image retrieval. Hard similar examples refer to image pairs from the same semantic class that demonstrate some shared appearance but have different fine-grained appearance. Hard dissimilar examples are image pairs that come from different semantic classes but exhibit similar appearance. These hard examples generally have a small distance due to the shared appearance. Therefore, effective encoding of the hard examples can well discriminate the relevant images within a small Hamming distance, enabling more accurate retrieval in the top-ranked returned images. However, most existing hashing methods cannot capture this key information as their optimization is dominated byeasy examples, i.e., distant similar/dissimilar pairs that share no or limited appearance. To address this problem, we introduce a novel Gamma distribution-enabled and symmetric Kullback-Leibler divergence-based loss, which is dubbed dual hinge loss because it works similarly as imposing two smoothed hinge losses on the respective similar and dissimilar pairs. Specifically, the loss enforces exponentially variant penalization on the hard similar (dissimilar) examples to emphasize and learn their fine-grained difference. It meanwhile imposes a bounding penalization on easy similar (dissimilar) examples to prevent the dominance of the easy examples in the optimization while preserving the high-level similarity (dissimilarity). This enables our model to well encode the key information carried by both easy and hard examples. Extensive empirical results on three widely-used image retrieval datasets show that (i) our method consistently and substantially outperforms state-of-the-art competing methods using hash codes of the same length and (ii) our method can use significantly (e.g., 50%-75%) shorter hash codes to perform substantially better than, or comparably well to, the competing methods.</p>\n", "tags": ["Deep Learning", "Image Retrieval", "MM"], "tsne_embedding": [-5.224206447601318, 0.6123942136764526]}, {"key": "yan2020deep", "year": "2020", "title": "Deep Multi-view Enhancement Hashing For Image Retrieval", "abstract": "<p>Hashing is an efficient method for nearest neighbor search in large-scale data space by embedding high-dimensional feature descriptors into a similarity preserving Hamming space with a low dimension. However large-scale high-speed retrieval through binary code has a certain degree of reduction in retrieval accuracy compared to traditional retrieval methods. We have noticed that multi-view methods can well preserve the diverse characteristics of data. Therefore we try to introduce the multi-view deep neural network into the hash learning field and design an efficient and innovative retrieval model which has achieved a significant improvement in retrieval performance. In this paper we propose a supervised multi-view hash model which can enhance the multi-view information through neural networks. This is a completely new hash learning method that combines multi-view and deep learning methods. The proposed method utilizes an effective view stability evaluation method to actively explore the relationship among views which will affect the optimization direction of the entire network. We have also designed a variety of multi-data fusion methods in the Hamming space to preserve the advantages of both convolution and multi-view. In order to avoid excessive computing resources on the enhancement procedure during retrieval we set up a separate structure called memory network which participates in training together. The proposed method is systematically evaluated on the CIFAR-10 NUS-WIDE and MS-COCO datasets and the results show that our method significantly outperforms the state-of-the-art single-view and multi-view hashing methods.</p>\n", "tags": ["ARXIV", "Cross Modal", "Deep Learning", "Image Retrieval", "Supervised"], "tsne_embedding": [5.596864223480225, 2.0528759956359863]}, {"key": "yan2020image", "year": "2020", "title": "Image Retrieval For Structure-from-motion Via Graph Convolutional Network", "abstract": "<p>Conventional image retrieval techniques for Structure-from-Motion (SfM) suffer from the limit of effectively recognizing repetitive patterns and cannot guarantee to create just enough match pairs with high precision and high recall. In this paper we present a novel retrieval method based on Graph Convolutional Network (GCN) to generate accurate pairwise matches without costly redundancy. We formulate image retrieval task as a node binary classification problem in graph data a node is marked as positive if it shares the scene overlaps with the query image. The key idea is that we find that the local context in feature space around a query image contains rich information about the matchable relation between this image and its neighbors. By constructing a subgraph surrounding the query image as input data we adopt a learnable GCN to exploit whether nodes in the subgraph have overlapping regions with the query photograph. Experiments demonstrate that our method performs remarkably well on the challenging dataset of highly ambiguous and duplicated scenes. Besides compared with state-of-the-art matchable retrieval methods the proposed approach significantly reduces useless attempted matches without sacrificing the accuracy and completeness of reconstruction.</p>\n", "tags": ["ARXIV", "Graph", "Image Retrieval", "Supervised"], "tsne_embedding": [-1.5093696117401123, 24.913864135742188]}, {"key": "yan2021binary", "year": "2021", "title": "Binary Code Based Hash Embedding For Web-scale Applications", "abstract": "<p>Nowadays deep learning models are widely adopted in web-scale applications such as recommender systems and online advertising. In these applications embedding learning of categorical features is crucial to the success of deep learning models. In these models a standard method is that each categorical feature value is assigned a unique embedding vector which can be learned and optimized. Although this method can well capture the characteristics of the categorical features and promise good performance it can incur a huge memory cost to store the embedding table especially for those web-scale applications. Such a huge memory cost significantly holds back the effectiveness and usability of EDRMs. In this paper we propose a binary code based hash embedding method which allows the size of the embedding table to be reduced in arbitrary scale without compromising too much performance. Experimental evaluation results show that one can still achieve 9937; performance even if the embedding table size is reduced 1000(times) smaller than the original one with our proposed method.</p>\n", "tags": ["ARXIV", "Deep Learning"], "tsne_embedding": [1.0982749462127686, -6.586628437042236]}, {"key": "yan2024fiha", "year": "2024", "title": "FIHA Autonomous Hallucination Evaluation In Vision-language Models With Davidson Scene Graphs", "abstract": "<p>The rapid development of Large Vision-Language Models (LVLMs) often comes with widespread hallucination issues making cost-effective and comprehensive assessments increasingly vital. Current approaches mainly rely on costly annotations and are not comprehensive \u2013 in terms of evaluating all aspects such as relations attributes and dependencies between aspects. Therefore we introduce the FIHA (autonomous Fine-graIned Hallucination evAluation evaluation in LVLMs) which could access hallucination LVLMs in the LLM-free and annotation-free way and model the dependency between different types of hallucinations. FIHA can generate Qamp;A pairs on any image dataset at minimal cost enabling hallucination assessment from both image and caption. Based on this approach we introduce a benchmark called FIHA-v1 which consists of diverse questions on various images from MSCOCO and Foggy. Furthermore we use the Davidson Scene Graph (DSG) to organize the structure among Qamp;A pairs in which we can increase the reliability of the evaluation. We evaluate representative models using FIHA-v1 highlighting their limitations and challenges. We released our code and data.</p>\n", "tags": ["ARXIV", "Cross Modal", "Graph"], "tsne_embedding": [50.738948822021484, 7.987593650817871]}, {"key": "yandexdeep1B", "year": "2021", "title": "Yandex DEEP-1B", "abstract": "<p>Yandex DEEP-1B image descriptor dataset consisting of the projected and normalized outputs from the last fully-connected layer of the GoogLeNet model, which was pretrained on the Imagenet classification task.</p>\n", "tags": ["Dataset"], "tsne_embedding": [11.168879508972168, 24.042966842651367]}, {"key": "yandextexttoimage1B", "year": "2021", "title": "Yandex Text-to-Image-1B", "abstract": "<p>Yandex Text-to-Image-1B is a new cross-model dataset (text and visual), where database and query vectors have different distributions in a shared representation space. The base set consists of Image embeddings produced by the Se-ResNext-101 model, and queries are textual embeddings produced by a variant of the DSSM model. Since the distributions are different, a 50M sample of the query distribution is provided.</p>\n", "tags": ["Dataset"], "tsne_embedding": [-14.74196720123291, 23.57556915283203]}, {"key": "yang2015supervised", "year": "2015", "title": "Supervised Learning Of Semantics-preserving Hash Via Deep Convolutional Neural Networks", "abstract": "<p>This paper presents a simple yet effective supervised deep hash approach that constructs binary hash codes from labeled data for large-scale image search. We assume that the semantic labels are governed by several latent attributes with each attribute on or off and classification relies on these attributes. Based on this assumption our approach dubbed supervised semantics-preserving deep hashing (SSDH) constructs hash functions as a latent layer in a deep network and the binary codes are learned by minimizing an objective function defined over classification error and other desirable hash codes properties. With this design SSDH has a nice characteristic that classification and retrieval are unified in a single learning model. Moreover SSDH performs joint learning of image representations hash codes and classification in a point-wised manner and thus is scalable to large-scale datasets. SSDH is simple and can be realized by a slight enhancement of an existing deep architecture for classification; yet it is effective and outperforms other hashing approaches on several benchmarks and large datasets. Compared with state-of-the-art approaches SSDH achieves higher retrieval accuracy while the classification performance is not sacrificed.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [-3.7465929985046387, 10.873478889465332]}, {"key": "yang2016zero", "year": "2016", "title": "Zero-shot Hashing Via Transferring Supervised Knowledge", "abstract": "<p>Hashing has shown its efficiency and effectiveness in facilitating large-scale multimedia applications. Supervised knowledge e.g. semantic labels or pair-wise relationship) associated to data is capable of significantly improving the quality of hash codes and hash functions. However confronted with the rapid growth of newly-emerging concepts and multimedia data on the Web existing supervised hashing approaches may easily suffer from the scarcity and validity of supervised information due to the expensive cost of manual labelling. In this paper we propose a novel hashing scheme termed emphzero-shot hashing (ZSH) which compresses images of unseen categories to binary codes with hash functions learned from limited training data of seen categories. Specifically we project independent data labels i.e. 0/1-form label vectors) into semantic embedding space where semantic relationships among all the labels can be precisely characterized and thus seen supervised knowledge can be transferred to unseen classes. Moreover in order to cope with the semantic shift problem we rotate the embedded space to more suitably align the embedded semantics with the low-level visual feature space thereby alleviating the influence of semantic gap. In the meantime to exert positive effects on learning high-quality hash functions we further propose to preserve local structural property and discrete nature in binary codes. Besides we develop an efficient alternating algorithm to solve the ZSH model. Extensive experiments conducted on various real-life datasets show the superior zero-shot image retrieval performance of ZSH as compared to several state-of-the-art hashing methods.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Supervised"], "tsne_embedding": [0.5499109625816345, 3.3459877967834473]}, {"key": "yang2018deep", "year": "2018", "title": "Deep Attention-guided Hashing", "abstract": "<p>With the rapid growth of multimedia data (e.g. image audio and video etc.) on the web learning-based hashing techniques such as Deep Supervised Hashing (DSH) have proven to be very efficient for large-scale multimedia search. The recent successes seen in Learning-based hashing methods are largely due to the success of deep learning-based hashing methods. However there are some limitations to previous learning-based hashing methods (e.g. the learned hash codes containing repetitive and highly correlated information). In this paper we propose a novel learning-based hashing method named Deep Attention-guided Hashing (DAgH). DAgH is implemented using two stream frameworks. The core idea is to use guided hash codes which are generated by the hashing network of the first stream framework (called first hashing network) to guide the training of the hashing network of the second stream framework (called second hashing network). Specifically in the first network it leverages an attention network and hashing network to generate the attention-guided hash codes from the original images. The loss function we propose contains two components the semantic loss and the attention loss. The attention loss is used to punish the attention network to obtain the salient region from pairs of images; in the second network these attention-guided hash codes are used to guide the training of the second hashing network (i.e. these codes are treated as supervised labels to train the second network). By doing this DAgH can make full use of the most critical information contained in images to guide the second hashing network in order to learn efficient hash codes in a true end-to-end fashion. Results from our experiments demonstrate that DAgH can generate high quality hash codes and it outperforms current state-of-the-art methods on three benchmark datasets CIFAR-10 NUS-WIDE and ImageNet.</p>\n", "tags": ["ARXIV", "Deep Learning", "Supervised"], "tsne_embedding": [6.928925037384033, 3.2378990650177]}, {"key": "yang2019adaptive", "year": "2019", "title": "Adaptive Labeling for Deep Learning to Hash", "abstract": "<p>Hash function learning has been widely used for largescale image retrieval because of the efficiency of computation and storage. We introduce AdaLabelHash, a binary\nhash function learning approach via deep neural networks\nin this paper. In AdaLabelHash, class label representations are variables that are adapted during the backward\nnetwork training procedure. We express the labels as hypercube vertices in a K-dimensional space, and the class\nlabel representations together with the network weights are\nupdated in the learning process. As the label representations (or referred to as codewords in this work), are learned\nfrom data, semantically similar classes will be assigned\nwith the codewords that are close to each other in terms\nof Hamming distance in the label space. The codewords\nthen serve as the desired output of the hash function learning, and yield compact and discriminating binary hash representations. AdaLabelHash is easy to implement, which\ncan jointly learn label representations and infer compact\nbinary codes from data. It is applicable to both supervised\nand semi-supervised hash. Experimental results on standard benchmarks demonstrate the satisfactory performance\nof AdaLabelHash.</p>\n", "tags": ["Deep Learning", "Semi Supervised"], "tsne_embedding": [-2.9272141456604004, 10.354512214660645]}, {"key": "yang2019asymmetric", "year": "2019", "title": "Asymmetric Deep Semantic Quantization For Image Retrieval", "abstract": "<p>Due to its fast retrieval and storage efficiency capabilities hashing has been widely used in nearest neighbor retrieval tasks. By using deep learning based techniques hashing can outperform non-learning based hashing technique in many applications. However we argue that the current deep learning based hashing methods ignore some critical problems (e.g. the learned hash codes are not discriminative due to the hashing methods being unable to discover rich semantic information and the training strategy having difficulty optimizing the discrete binary codes). In this paper we propose a novel image hashing method termed as (textbf)(underlineA)symmetric (textbf)(underlineD)eep (textbf)(underlineS)emantic (textbf)(underlineQ)uantization ((textbfADSQ)). (textbfADSQ) is implemented using three stream frameworks which consist of one (emphLabelNet) and two (emphImgNets). The (emphLabelNet) leverages the power of three fully-connected layers which are used to capture rich semantic information between image pairs. For the two (emphImgNets) they each adopt the same convolutional neural network structure but with different weights (i.e. asymmetric convolutional neural networks). The two (emphImgNets) are used to generate discriminative compact hash codes. Specifically the function of the (emphLabelNet) is to capture rich semantic information that is used to guide the two (emphImgNets) in minimizing the gap between the real-continuous features and the discrete binary codes. Furthermore (textbfADSQ) can utilize the most critical semantic information to guide the feature learning process and consider the consistency of the common semantic space and Hamming space. Experimental results on three benchmarks (i.e. CIFAR-10 NUS-WIDE and ImageNet) demonstrate that the proposed (textbfADSQ) can outperforms current state-of-the-art methods.</p>\n", "tags": ["ARXIV", "Deep Learning", "Image Retrieval", "Quantisation", "Supervised"], "tsne_embedding": [-6.233790397644043, -0.5719195008277893]}, {"key": "yang2019distill", "year": "2019", "title": "DistillHash: Unsupervised Deep Hashing by Distilling Data Pairs", "abstract": "<p>Due to the high storage and search efficiency, hashing\nhas become prevalent for large-scale similarity search. Particularly, deep hashing methods have greatly improved the\nsearch performance under supervised scenarios. In contrast, unsupervised deep hashing models can hardly achieve\nsatisfactory performance due to the lack of reliable supervisory similarity signals.\n To address this issue, we propose\na novel deep unsupervised hashing model, dubbed DistillHash, which can learn a distilled data set consisted of data\npairs, which have confidence similarity signals. Specifically, we investigate the relationship between the initial\nnoisy similarity signals learned from local structures and\nthe semantic similarity labels assigned by a Bayes optimal\nclassifier. We show that under a mild assumption, some\ndata pairs, of which labels are consistent with those assigned by the Bayes optimal classifier, can be potentially\ndistilled. Inspired by this fact, we design a simple yet effective strategy to distill data pairs automatically and further\nadopt a Bayesian learning framework to learn hash functions from the distilled data set. Extensive experimental results on three widely used benchmark datasets show that the\nproposed DistillHash consistently accomplishes the stateof-the-art search performance.</p>\n", "tags": ["CVPR", "Supervised"], "tsne_embedding": [-11.335504531860352, 0.6351114511489868]}, {"key": "yang2019distillhash", "year": "2019", "title": "Distillhash Unsupervised Deep Hashing By Distilling Data Pairs", "abstract": "<p>Due to the high storage and search efficiency hashing has become prevalent for large-scale similarity search. Particularly deep hashing methods have greatly improved the search performance under supervised scenarios. In contrast unsupervised deep hashing models can hardly achieve satisfactory performance due to the lack of reliable supervisory similarity signals. To address this issue we propose a novel deep unsupervised hashing model dubbed DistillHash which can learn a distilled data set consisted of data pairs which have confidence similarity signals. Specifically we investigate the relationship between the initial noisy similarity signals learned from local structures and the semantic similarity labels assigned by a Bayes optimal classifier. We show that under a mild assumption some data pairs of which labels are consistent with those assigned by the Bayes optimal classifier can be potentially distilled. Inspired by this fact we design a simple yet effective strategy to distill data pairs automatically and further adopt a Bayesian learning framework to learn hash functions from the distilled data set. Extensive experimental results on three widely used benchmark datasets show that the proposed DistillHash consistently accomplishes the state-of-the-art search performance.</p>\n", "tags": ["ARXIV", "Unsupervised"], "tsne_embedding": [-11.319519996643066, 0.6159355640411377]}, {"key": "yang2019feature", "year": "2019", "title": "Feature Pyramid Hashing", "abstract": "<p>In recent years deep-networks-based hashing has become a leading approach for large-scale image retrieval. Most deep hashing approaches use the high layer to extract the powerful semantic representations. However these methods have limited ability for fine-grained image retrieval because the semantic features extracted from the high layer are difficult in capturing the subtle differences. To this end we propose a novel two-pyramid hashing architecture to learn both the semantic information and the subtle appearance details for fine-grained image search. Inspired by the feature pyramids of convolutional neural network a vertical pyramid is proposed to capture the high-layer features and a horizontal pyramid combines multiple low-layer features with structural information to capture the subtle differences. To fuse the low-level features a novel combination strategy called consensus fusion is proposed to capture all subtle information from several low-layers for finer retrieval. Extensive evaluation on two fine-grained datasets CUB-200-2011 and Stanford Dogs demonstrate that the proposed method achieves significant performance compared with the state-of-art baselines.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Supervised"], "tsne_embedding": [-10.458908081054688, 8.739089012145996]}, {"key": "yang2020camera", "year": "2020", "title": "Camera-based Piano Sheet Music Identification", "abstract": "<p>This paper presents a method for large-scale retrieval of piano sheet music images. Our work differs from previous studies on sheet music retrieval in two ways. First we investigate the problem at a much larger scale than previous studies using all solo piano sheet music images in the entire IMSLP dataset as a searchable database. Second we use cell phone images of sheet music as our input queries which lends itself to a practical user-facing application. We show that a previously proposed fingerprinting method for sheet music retrieval is far too slow for a real-time application and we diagnose its shortcomings. We propose a novel hashing scheme called dynamic n-gram fingerprinting that significantly reduces runtime while simultaneously boosting retrieval accuracy. In experiments on IMSLP data our proposed method achieves a mean reciprocal rank of 0.85 and an average runtime of 0.98 seconds per query.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-27.53026008605957, -11.591224670410156]}, {"key": "yang2020nonlinear", "year": "2020", "title": "Nonlinear Robust Discrete Hashing for Cross-Modal Retrieval", "abstract": "<p>Hashing techniques have recently been successfully applied to solve similarity search problems in the information retrieval field because of their significantly reduced storage and high-speed search capabilities. However, the hash codes learned from most recent cross-modal hashing methods lack the ability to comprehensively preserve adequate information, resulting in a less than desirable performance. To solve this limitation, we propose a novel method termed Nonlinear Robust Discrete Hashing (NRDH), for cross-modal retrieval. The main idea behind NRDH is motivated by the success of neural networks, i.e., nonlinear descriptors, in the field of representation learning, and the use of nonlinear descriptors instead of simple linear transformations is more in line with the complex relationships that exist between common latent representation and heterogeneous multimedia data in the real world. In NRDH, we first learn a common latent representation through nonlinear descriptors to encode complementary and consistent information from the features of the heterogeneous multimedia data. Moreover, an asymmetric learning scheme is proposed to correlate the learned hash codes with the common latent representation. Empirically, we demonstrate that NRDH is able to successfully generate a comprehensive common latent representation that significantly improves the quality of the learned hash codes. Then, NRDH adopts a linear learning strategy to fast learn the hash function with the learned hash codes. Extensive experiments performed on two benchmark datasets highlight the superiority of NRDH over several state-of-the-art methods.</p>\n", "tags": ["Cross Modal", "SIGIR"], "tsne_embedding": [-8.519731521606445, -4.938564300537109]}, {"key": "yang2022fedhap", "year": "2022", "title": "Fedhap Federated Hashing With Global Prototypes For Cross-silo Retrieval", "abstract": "<p>Deep hashing has been widely applied in large-scale data retrieval due to its superior retrieval efficiency and low storage cost. However data are often scattered in data silos with privacy concerns so performing centralized data storage and retrieval is not always possible. Leveraging the concept of federated learning (FL) to perform deep hashing is a recent research trend. However existing frameworks mostly rely on the aggregation of the local deep hashing models which are trained by performing similarity learning with local skewed data only. Therefore they cannot work well for non-IID clients in a real federated environment. To overcome these challenges we propose a novel federated hashing framework that enables participating clients to jointly train the shared deep hashing model by leveraging the prototypical hash codes for each class. Globally the transmission of global prototypes with only one prototypical hash code per class will minimize the impact of communication cost and privacy risk. Locally the use of global prototypes are maximized by jointly training a discriminator network and the local hashing network. Extensive experiments on benchmark datasets are conducted to demonstrate that our method can significantly improve the performance of the deep hashing model in the federated environments with non-IID data distributions.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [5.444958209991455, -6.0569233894348145]}, {"key": "yang2023large", "year": "2023", "title": "Large Language Model Can Interpret Latent Space Of Sequential Recommender", "abstract": "<p>Sequential recommendation is to predict the next item of interest for a user based on her/his interaction history with previous items. In conventional sequential recommenders a common approach is to model item sequences using discrete IDs learning representations that encode sequential behaviors and reflect user preferences. Inspired by recent success in empowering large language models (LLMs) to understand and reason over diverse modality data (e.g. image audio 3D points) a compelling research question arises Can LLMs understand and work with hidden representations from ID-based sequential recommenders.To answer this we propose a simple framework RecInterpreter which examines the capacity of open-source LLMs to decipher the representation space of sequential recommenders. Specifically with the multimodal pairs (ie representations of interaction sequence and text narrations) RecInterpreter first uses a lightweight adapter to map the representations into the token embedding space of the LLM. Subsequently it constructs a sequence-recovery prompt that encourages the LLM to generate textual descriptions for items within the interaction sequence. Taking a step further we propose a sequence-residual prompt instead which guides the LLM in identifying the residual item by contrasting the representations before and after integrating this residual into the existing sequence. Empirical results showcase that our RecInterpreter enhances the exemplar LLM LLaMA to understand hidden representations from ID-based sequential recommenders especially when guided by our sequence-residual prompts. Furthermore RecInterpreter enables LLaMA to instantiate the oracle items generated by generative recommenders like DreamRec concreting the item a user would ideally like to interact with next. Codes are available at https://github.com/YangZhengyi98/RecInterpreter.</p>\n", "tags": ["ARXIV", "Cross Modal", "Has Code"], "tsne_embedding": [41.73728561401367, -9.85533618927002]}, {"key": "yang2023mm", "year": "2023", "title": "MM-REACT Prompting Chatgpt For Multimodal Reasoning And Action", "abstract": "<p>We propose MM-REACT a system paradigm that integrates ChatGPT with a pool of vision experts to achieve multimodal reasoning and action. In this paper we define and explore a comprehensive list of advanced vision tasks that are intriguing to solve but may exceed the capabilities of existing vision and vision-language models. To achieve such advanced visual intelligence MM-REACT introduces a textual prompt design that can represent text descriptions textualized spatial coordinates and aligned file names for dense visual signals such as images and videos. MM-REACTs prompt design allows language models to accept associate and process multimodal information thereby facilitating the synergetic combination of ChatGPT and various vision experts. Zero-shot experiments demonstrate MM-REACTs effectiveness in addressing the specified capabilities of interests and its wide application in different scenarios that require advanced visual understanding. Furthermore we discuss and compare MM-REACTs system paradigm with an alternative approach that extends language models for multimodal scenarios through joint finetuning. Code demo video and visualization are available at https://multimodal-react.github.io/</p>\n", "tags": ["ARXIV", "Cross Modal", "Has Code", "MM"], "tsne_embedding": [38.96418762207031, 6.402056694030762]}, {"key": "yang2023teaching", "year": "2023", "title": "Gpt4tools Teaching Large Language Model To Use Tools Via Self-instruction", "abstract": "<p>This paper aims to efficiently enable Large Language Models (LLMs) to use multimodal tools. Advanced proprietary LLMs such as ChatGPT and GPT-4 have shown great potential for tool usage through sophisticated prompt engineering. Nevertheless these models typically rely on prohibitive computational costs and publicly inaccessible data. To address these challenges we propose the GPT4Tools based on self-instruct to enable open-source LLMs such as LLaMA and OPT to use tools. It generates an instruction-following dataset by prompting an advanced teacher with various multi-modal contexts. By using the Low-Rank Adaptation (LoRA) optimization our approach facilitates the open-source LLMs to solve a range of visual problems including visual comprehension and image generation. Moreover we provide a benchmark to evaluate the ability of LLMs to use tools which is performed in both zero-shot and fine-tuning ways. Extensive experiments demonstrate the effectiveness of our method on various language models which not only significantly improves the accuracy of invoking seen tools but also enables the zero-shot capacity for unseen tools. The code and demo are available at https://github.com/StevenGrove/GPT4Tools.</p>\n", "tags": ["ARXIV", "Cross Modal", "Has Code"], "tsne_embedding": [25.408113479614258, -10.17866325378418]}, {"key": "yao2019efficient", "year": "2019", "title": "Efficient Discrete Supervised Hashing For Large-scale Cross-modal Retrieval", "abstract": "<p>Supervised cross-modal hashing has gained increasing research interest on large-scale retrieval task owning to its satisfactory performance and efficiency. However it still has some challenging issues to be further studied 1) most of them fail to well preserve the semantic correlations in hash codes because of the large heterogenous gap; 2) most of them relax the discrete constraint on hash codes leading to large quantization error and consequent low performance; 3) most of them suffer from relatively high memory cost and computational complexity during training procedure which makes them unscalable. In this paper to address above issues we propose a supervised cross-modal hashing method based on matrix factorization dubbed Efficient Discrete Supervised Hashing (EDSH). Specifically collective matrix factorization on heterogenous features and semantic embedding with class labels are seamlessly integrated to learn hash codes. Therefore the feature based similarities and semantic correlations can be both preserved in hash codes which makes the learned hash codes more discriminative. Then an efficient discrete optimal algorithm is proposed to handle the scalable issue. Instead of learning hash codes bit-by-bit hash codes matrix can be obtained directly which is more efficient. Extensive experimental results on three public real-world datasets demonstrate that EDSH produces a superior performance in both accuracy and scalability over some existing cross-modal hashing methods.</p>\n", "tags": ["ARXIV", "Cross Modal", "Quantisation", "Supervised"], "tsne_embedding": [0.4576190114021301, -3.3432278633117676]}, {"key": "yao2022react", "year": "2022", "title": "React Synergizing Reasoning And Acting In Language Models", "abstract": "<p>While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner allowing for greater synergy between the two reasoning traces help the model induce track and update action plans as well as handle exceptions while actions allow it to interface with external sources such as knowledge bases or environments to gather additional information. We apply our approach named ReAct to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely on question answering (HotpotQA) and fact verification (Fever) ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop) ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 3437; and 1037; respectively while being prompted with only one or two in-context examples. Project site with code https://react-lm.github.io</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [35.17396545410156, -3.312067747116089]}, {"key": "yao2023tree", "year": "2023", "title": "Tree Of Thoughts Deliberate Problem Solving With Large Language Models", "abstract": "<p>Language models are increasingly being deployed for general problem solving across a wide range of tasks but are still confined to token-level left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration strategic lookahead or where initial decisions play a pivotal role. To surmount these challenges we introduce a new framework for language model inference Tree of Thoughts (ToT) which generalizes over the popular Chain of Thought approach to prompting language models and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models problem-solving abilities on three novel tasks requiring non-trivial planning or search Game of 24 Creative Writing and Mini Crosswords. For instance in Game of 24 while GPT-4 with chain-of-thought prompting only solved 437; of tasks our method achieved a success rate of 7437;. Code repo with all prompts https://github.com/princeton-nlp/tree-of-thought-llm.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [30.98228645324707, 4.180603504180908]}, {"key": "ye2020nearoptimal", "year": "2020", "title": "Unsupervised Few-Bits Semantic Hashing with Implicit Topics Modeling", "abstract": "<p>Semantic hashing is a powerful paradigm for\nrepresenting texts as compact binary hash\ncodes. The explosion of short text data has\nspurred the demand of few-bits hashing. However, the performance of existing semantic\nhashing methods cannot be guaranteed when\napplied to few-bits hashing because of severe\ninformation loss. In this paper, we present a\nsimple but effective unsupervised neural generative semantic hashing method with a focus on\nfew-bits hashing. Our model is built upon variational autoencoder and represents each hash\nbit as a Bernoulli variable, which allows the\nmodel to be end-to-end trainable. To address\nthe issue of information loss, we introduce a\nset of auxiliary implicit topic vectors. With\nthe aid of these topic vectors, the generated\nhash codes are not only low-dimensional representations of the original texts but also capture their implicit topics. We conduct comprehensive experiments on four datasets. The results demonstrate that our approach achieves\nsignificant improvements over state-of-the-art\nsemantic hashing methods in few-bits hashing.</p>\n", "tags": [], "tsne_embedding": [-5.861032485961914, -9.329948425292969]}, {"key": "ye2021crossfit", "year": "2021", "title": "Crossfit A Few-shot Learning Challenge For Cross-task Generalization In NLP", "abstract": "<p>Humans can learn a new language task efficiently with only few examples by leveraging their knowledge obtained when learning prior tasks. In this paper we explore whether and how such cross-task generalization ability can be acquired and further applied to build better few-shot learners across diverse NLP tasks. We introduce CrossFit a problem setup for studying cross-task generalization ability which standardizes seen/unseen task partitions data access during different learning stages and the evaluation protocols. To instantiate different seen/unseen task partitions in CrossFit and facilitate in-depth analysis we present the NLP Few-shot Gym a repository of 160 diverse few-shot NLP tasks created from open-access NLP datasets and converted to a unified text-to-text format. Our analysis reveals that the few-shot learning ability on unseen tasks can be improved via an upstream learning stage using a set of seen tasks. We also observe that the selection of upstream learning tasks can significantly influence few-shot performance on unseen tasks asking further analysis on task similarity and transferability.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [26.66520118713379, -5.030780792236328]}, {"key": "ye2022unreliability", "year": "2022", "title": "The Unreliability Of Explanations In Few-shot Prompting For Textual Reasoning", "abstract": "<p>Does prompting a large language model (LLM) like GPT-3 with explanations improve in-context learning We study this question on two NLP tasks that involve reasoning over text namely question answering and natural language inference. We test the performance of four LLMs on three textual reasoning datasets using prompts that include explanations in multiple different styles. For these tasks we find that including explanations in the prompts for OPT GPT-3 (davinci) and InstructGPT (text-davinci-001) only yields small to moderate accuracy improvements over standard few-show learning. However text-davinci-002 is able to benefit more substantially. We further show that explanations generated by the LLMs may not entail the models predictions nor be factually grounded in the input even on simple tasks with extractive explanations. However these flawed explanations can still be useful as a way to verify LLMs predictions post-hoc. Through analysis in our three settings we show that explanations judged by humans to be good\u2013logically consistent with the input and the prediction\u2013more likely cooccur with accurate predictions. Following these observations we train calibrators using automatically extracted scores that assess the reliability of explanations allowing us to improve performance post-hoc across all of our datasets.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [34.27410125732422, 2.1575214862823486]}, {"key": "ye2023comprehensive", "year": "2023", "title": "A Comprehensive Capability Analysis Of GPT-3 And GPT-3.5 Series Models", "abstract": "<p>GPT series models such as GPT-3 CodeX InstructGPT ChatGPT and so on have gained considerable attention due to their exceptional natural language processing capabilities. However despite the abundance of research on the difference in capabilities between GPT series models and fine-tuned models there has been limited attention given to the evolution of GPT series models capabilities over time. To conduct a comprehensive analysis of the capabilities of GPT series models we select six representative models comprising two GPT-3 series models (i.e. davinci and text-davinci-001) and four GPT-3.5 series models (i.e. code-davinci-002 text-davinci-002 text-davinci-003 and gpt-3.5-turbo). We evaluate their performance on nine natural language understanding (NLU) tasks using 21 datasets. In particular we compare the performance and robustness of different models for each task under zero-shot and few-shot scenarios. Our extensive experiments reveal that the overall ability of GPT series models on NLU tasks does not increase gradually as the models evolve especially with the introduction of the RLHF training strategy. While this strategy enhances the models ability to generate human-like responses it also compromises their ability to solve some tasks. Furthermore our findings indicate that there is still room for improvement in areas such as model robustness.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [34.50362777709961, 5.981894493103027]}, {"key": "ye2023large", "year": "2023", "title": "Large Language Models Are Versatile Decomposers Decompose Evidence And Questions For Table-based Reasoning", "abstract": "<p>Table-based reasoning has shown remarkable progress in combining deep models with discrete reasoning which requires reasoning over both free-form natural language (NL) questions and structured tabular data. However previous table-based reasoning solutions usually suffer from significant performance degradation on huge evidence (tables). In addition most existing methods struggle to reason over complex questions since the required information is scattered in different places. To alleviate the above challenges we exploit large language models (LLMs) as decomposers for effective table-based reasoning which (i) decompose huge evidence (a huge table) into sub-evidence (a small table) to mitigate the interference of useless information for table reasoning; and (ii) decompose complex questions into simpler sub-questions for text reasoning. Specifically we first use the LLMs to break down the evidence (tables) involved in the current question retaining the relevant evidence and excluding the remaining irrelevant evidence from the huge table. In addition we propose a parsing-execution-filling strategy to alleviate the hallucination dilemma of the chain of thought by decoupling logic and numerical computation in each step. Extensive experiments show that our method can effectively leverage decomposed evidence and questions and outperforms the strong baselines on TabFact WikiTableQuestion and FetaQA datasets. Notably our model outperforms human performance for the first time on the TabFact dataset.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [31.435562133789062, 1.6510281562805176]}, {"key": "ye2023mplug", "year": "2023", "title": "Mplug-owl Modularization Empowers Large Language Models With Multimodality", "abstract": "<p>Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks while recent research has also explored the use of LLMs for multi-modal generation. In this study we introduce mPLUG-Owl a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM a visual knowledge module and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage language-only and multi-modal supervised datasets are used to jointly fine-tune a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing the visual knowledge module. We carefully build a visually-related instruction evaluation set OwlEval. Experimental results show that our model outperforms existing multi-modal models demonstrating mPLUG-Owls impressive instruction and visual understanding ability multi-turn conversation ability and knowledge reasoning ability. Besides we observe some unexpected and exciting abilities such as multi-image correlation and scene text understanding which makes it possible to leverage it for harder real scenarios such as vision-only document comprehension. Our code pre-trained model instruction-tuned models and evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The online demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.</p>\n", "tags": ["ARXIV", "Cross Modal", "Has Code", "Supervised"], "tsne_embedding": [25.37938117980957, -10.19951343536377]}, {"key": "yin2023survey", "year": "2023", "title": "A Survey On Multimodal Large Language Models", "abstract": "<p>Recently Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM such as writing stories based on images and OCR-free math reasoning are rare in traditional multimodal methods suggesting a potential path to artificial general intelligence. To this end both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V pushing the limit of research at a surprising speed. In this paper we aim to trace and summarize the recent progress of MLLMs. First of all we present the basic formulation of MLLM and delineate its related concepts including architecture training strategy and data as well as evaluation. Then we introduce research topics about how MLLMs can be extended to support more granularity modalities languages and scenarios. We continue with multimodal hallucination and extended techniques including Multimodal ICL (M-ICL) Multimodal CoT (M-CoT) and LLM-Aided Visual Reasoning (LAVR). To conclude the paper we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.</p>\n", "tags": ["ARXIV", "Cross Modal", "Has Code", "Survey Paper"], "tsne_embedding": [33.25270462036133, -12.392934799194336]}, {"key": "yin2023woodpecker", "year": "2023", "title": "Woodpecker Hallucination Correction For Multimodal Large Language Models", "abstract": "<p>Hallucination is a big shadow hanging over the rapidly evolving Multimodal Large Language Models (MLLMs) referring to the phenomenon that the generated text is inconsistent with the image content. In order to mitigate hallucinations existing studies mainly resort to an instruction-tuning manner that requires retraining the models with specific data. In this paper we pave a different way introducing a training-free method named Woodpecker. Like a woodpecker heals trees it picks out and corrects hallucinations from the generated text. Concretely Woodpecker consists of five stages key concept extraction question formulation visual knowledge validation visual claim generation and hallucination correction. Implemented in a post-remedy manner Woodpecker can easily serve different MLLMs while being interpretable by accessing intermediate outputs of the five stages. We evaluate Woodpecker both quantitatively and qualitatively and show the huge potential of this new paradigm. On the POPE benchmark our method obtains a 30.6637;/24.3337; improvement in accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released at https://github.com/BradyFU/Woodpecker.</p>\n", "tags": ["ARXIV", "Cross Modal", "Has Code"], "tsne_embedding": [50.24007797241211, 3.1869583129882812]}, {"key": "yoshikawa2015cross", "year": "2015", "title": "Cross-domain Matching For Bag-of-words Data Via Kernel Embeddings Of Latent Distributions", "abstract": "<p>We propose a kernel-based method for finding matching between instances across different domains such as multilingual documents and images with annotations. Each instance is assumed to be represented as a multiset of features e.g. a bag-of-words representation for documents. The major difficulty in finding cross-domain relationships is that the similarity between instances in different domains cannot be directly measured. To overcome this difficulty the proposed method embeds all the features of different domains in a shared latent space and regards each instance as a distribution of its own features in the shared latent space. To represent the distributions efficiently and nonparametrically we employ the framework of the kernel embeddings of distributions. The embedding is estimated so as to minimize the difference between distributions of paired instances while keeping unpaired instances apart. In our experiments we show that the proposed method can achieve high performance on finding correspondence between multi-lingual Wikipedia articles between documents and tags and between images and tags.</p>\n", "tags": ["Cross Modal", "NEURIPS"], "tsne_embedding": [-11.649479866027832, 14.24828052520752]}, {"key": "you2023idealgpt", "year": "2023", "title": "Idealgpt Iteratively Decomposing Vision And Language Reasoning Via Large Language Models", "abstract": "<p>The field of vision-and-language (VL) understanding has made unprecedented progress with end-to-end large pre-trained VL models (VLMs). However they still fall short in zero-shot reasoning tasks that require multi-step inferencing. To achieve this goal previous works resort to a divide-and-conquer pipeline. In this paper we argue that previous efforts have several inherent shortcomings 1) They rely on domain-specific sub-question decomposing models. 2) They force models to predict the final answer even if the sub-questions or sub-answers provide insufficient information. We address these limitations via IdealGPT a framework that iteratively decomposes VL reasoning using large language models (LLMs). Specifically IdealGPT utilizes an LLM to generate sub-questions a VLM to provide corresponding sub-answers and another LLM to reason to achieve the final answer. These three modules perform the divide-and-conquer procedure iteratively until the model is confident about the final answer to the main question. We evaluate IdealGPT on multiple challenging VL reasoning tasks under a zero-shot setting. In particular our IdealGPT outperforms the best existing GPT-4-like models by an absolute 1037; on VCR and 1537; on SNLI-VE. Code is available at https://github.com/Hxyou/IdealGPT</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [26.13584327697754, 7.2635064125061035]}, {"key": "yu2014using", "year": "2014", "title": "Circulant Binary Embedding", "abstract": "<p>Binary embedding of high-dimensional data requires\nlong codes to preserve the discriminative\npower of the input space. Traditional binary coding\nmethods often suffer from very high computation\nand storage costs in such a scenario. To\naddress this problem, we propose Circulant Binary\nEmbedding (CBE) which generates binary\ncodes by projecting the data with a circulant matrix.\nThe circulant structure enables the use of\nFast Fourier Transformation to speed up the computation.\nCompared to methods that use unstructured\nmatrices, the proposed method improves\nthe time complexity from O(d^2\n) to O(d log d),\nand the space complexity from O(d^2) to O(d)\nwhere d is the input dimensionality. We also\npropose a novel time-frequency alternating optimization\nto learn data-dependent circulant projections,\nwhich alternatively minimizes the objective\nin original and Fourier domains. We show\nby extensive experiments that the proposed approach\ngives much better performance than the\nstate-of-the-art approaches for fixed time, and\nprovides much faster computation with no performance\ndegradation for fixed number of bits.</p>\n", "tags": ["Has Code", "ICML"], "tsne_embedding": [-29.98543930053711, 17.392425537109375]}, {"key": "yu2016variable", "year": "2016", "title": "Variable-length Hashing", "abstract": "<p>Hashing has emerged as a popular technique for large-scale similarity search. Most learning-based hashing methods generate compact yet correlated hash codes. However this redundancy is storage-inefficient. Hence we propose a lossless variable-length hashing (VLH) method that is both storage- and search-efficient. Storage efficiency is achieved by converting the fixed-length hash code into a variable-length code. Search efficiency is obtained by using a multiple hash table structure. With VLH we are able to deliberately add redundancy into hash codes to improve retrieval performance with little sacrifice in storage efficiency or search complexity. In particular we propose a block K-means hashing (B-KMH) method to obtain significantly improved retrieval performance with no increase in storage and marginal increase in computational cost.</p>\n", "tags": ["ARXIV", "Unsupervised"], "tsne_embedding": [-30.455646514892578, -8.334702491760254]}, {"key": "yu2018discriminative", "year": "2018", "title": "Discriminative Supervised Hashing For Cross-modal Similarity Search", "abstract": "<p>With the advantage of low storage cost and high retrieval efficiency hashing techniques have recently been an emerging topic in cross-modal similarity search. As multiple modal data reflect similar semantic content many researches aim at learning unified binary codes. However discriminative hashing features learned by these methods are not adequate. This results in lower accuracy and robustness. We propose a novel hashing learning framework which jointly performs classifier learning subspace learning and matrix factorization to preserve class-specific semantic content termed Discriminative Supervised Hashing (DSH) to learn the discrimative unified binary codes for multi-modal data. Besides reducing the loss of information and preserving the non-linear structure of data DSH non-linearly projects different modalities into the common space in which the similarity among heterogeneous data points can be measured. Extensive experiments conducted on the three publicly available datasets demonstrate that the framework proposed in this paper outperforms several state-of -the-art methods.</p>\n", "tags": ["ARXIV", "Cross Modal", "Supervised"], "tsne_embedding": [-9.921990394592285, -0.057613126933574677]}, {"key": "yu2018learning", "year": "2018", "title": "Learning Discriminative Hashing Codes For Cross-modal Retrieval Based On Multi-view Features", "abstract": "<p>Hashing techniques have been applied broadly in retrieval tasks due to their low storage requirements and high speed of processing. Many hashing methods based on a single view have been extensively studied for information retrieval. However the representation capacity of a single view is insufficient and some discriminative information is not captured which results in limited improvement. In this paper we employ multiple views to represent images and texts for enriching the feature information. Our framework exploits the complementary information among multiple views to better learn the discriminative compact hash codes. A discrete hashing learning framework that jointly performs classifier learning and subspace learning is proposed to complete multiple search tasks simultaneously. Our framework includes two stages namely a kernelization process and a quantization process. Kernelization aims to find a common subspace where multi-view features can be fused. The quantization stage is designed to learn discriminative unified hashing codes. Extensive experiments are performed on single-label datasets (WiKi and MMED) and multi-label datasets (MIRFlickr and NUS-WIDE) and the experimental results indicate the superiority of our method compared with the state-of-the-art methods.</p>\n", "tags": ["ARXIV", "Cross Modal", "Independent", "Quantisation"], "tsne_embedding": [-6.265267372131348, -7.028149604797363]}, {"key": "yu2018semi", "year": "2018", "title": "Semi-supervised Hashing For Semi-paired Cross-view Retrieval", "abstract": "<p>Recently hashing techniques have gained importance in large-scale retrieval tasks because of their retrieval speed. Most of the existing cross-view frameworks assume that data are well paired. However the fully-paired multiview situation is not universal in real applications. The aim of the method proposed in this paper is to learn the hashing function for semi-paired cross-view retrieval tasks. To utilize the label information of partial data we propose a semi-supervised hashing learning framework which jointly performs feature extraction and classifier learning. The experimental results on two datasets show that our method outperforms several state-of-the-art methods in terms of retrieval accuracy.</p>\n", "tags": ["ARXIV", "Cross Modal", "Supervised"], "tsne_embedding": [-20.205482482910156, -13.183857917785645]}, {"key": "yu2019unsupervised", "year": "2019", "title": "Unsupervised Multi-modal Hashing For Cross-modal Retrieval", "abstract": "<p>With the advantage of low storage cost and high efficiency hashing learning has received much attention in the domain of Big Data. In this paper we propose a novel unsupervised hashing learning method to cope with this open problem to directly preserve the manifold structure by hashing. To address this problem both the semantic correlation in textual space and the locally geometric structure in the visual space are explored simultaneously in our framework. Besides the 2;1-norm constraint is imposed on the projection matrices to learn the discriminative hash function for each modality. Extensive experiments are performed to evaluate the proposed method on the three publicly available datasets and the experimental results show that our method can achieve superior performance over the state-of-the-art methods.</p>\n", "tags": ["ARXIV", "Cross Modal", "Unsupervised"], "tsne_embedding": [-5.978494167327881, -11.93227767944336]}, {"key": "yu2020self", "year": "2020", "title": "Self-supervised Asymmetric Deep Hashing With Margin-scalable Constraint", "abstract": "<p>Due to its effectivity and efficiency deep hashing approaches are widely used for large-scale visual search. However it is still challenging to produce compact and discriminative hash codes for images associated with multiple semantics for two main reasons 1) similarity constraints designed in most of the existing methods are based upon an oversimplified similarity assignment(i.e. 0 for instance pairs sharing no label 1 for instance pairs sharing at least 1 label) 2) the exploration in multi-semantic relevance are insufficient or even neglected in many of the existing methods. These problems significantly limit the discrimination of generated hash codes. In this paper we propose a novel self-supervised asymmetric deep hashing method with a margin-scalable constraint(SADH) approach to cope with these problems. SADH implements a self-supervised network to sufficiently preserve semantic information in a semantic feature dictionary and a semantic code dictionary for the semantics of the given dataset which efficiently and precisely guides a feature learning network to preserve multilabel semantic information using an asymmetric learning strategy. By further exploiting semantic dictionaries a new margin-scalable constraint is employed for both precise similarity searching and robust hash code generation. Extensive empirical research on four popular benchmarks validates the proposed method and shows it outperforms several state-of-the-art approaches.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [-5.756670951843262, 3.0206000804901123]}, {"key": "yu2021deep", "year": "2021", "title": "Deep Graph-neighbor Coherence Preserving Network for Unsupervised Cross-modal Hashing", "abstract": "<p>Unsupervised cross-modal hashing (UCMH) has become a hot topic recently. Current UCMH focuses on exploring data similarities. However, current UCMH methods calculate the similarity between two data, mainly relying on the two data\u2019s cross-modal features. These methods suffer from inaccurate similarity problems that result in a suboptimal retrieval Hamming space, because the cross-modal features between the data are not sufficient to describe the complex data relationships, such as situations where two data have different feature representations but share the inherent concepts. In this paper, we devise a deep graph-neighbor coherence preserving network (DGCPN). Specifically, DGCPN stems from graph models and explores graph-neighbor coherence by consolidating the information between data and their neighbors. DGCPN regulates comprehensive similarity preserving losses by exploiting three types of data similarities (i.e., the graph-neighbor coherence, the coexistent similarity, and the intra- and inter-modality consistency) and designs a half-real and half-binary optimization strategy to reduce the quantization errors during hashing. Essentially, DGCPN addresses the inaccurate similarity problem by exploring and exploiting the data\u2019s intrinsic relationships in a graph. We conduct extensive experiments on three public UCMH datasets. The experimental results demonstrate the superiority of DGCPN, e.g., by improving the mean average precision from 0.722 to 0.751 on MIRFlickr-25K using 64-bit hashing codes to retrieval texts from images. We will release the source code package and the trained model on https://github.com/Atmegal/DGCPN.</p>\n", "tags": ["AAAI", "Cross Modal", "Has Code", "Supervised"], "tsne_embedding": [0.039219845086336136, 25.0256404876709]}, {"key": "yu2022learning", "year": "2022", "title": "Learning To Hash Naturally Sorts", "abstract": "<p>Learning to hash pictures a list-wise sorting problem. Its testing metrics e.g. mean-average precision count on a sorted candidate list ordered by pair-wise code similarity. However scarcely does one train a deep hashing model with the sorted results end-to-end because of the non-differentiable nature of the sorting operation. This inconsistency in the objectives of training and test may lead to sub-optimal performance since the training loss often fails to reflect the actual retrieval metric. In this paper we tackle this problem by introducing Naturally-Sorted Hashing (NSH). We sort the Hamming distances of samples hash codes and accordingly gather their latent representations for self-supervised training. Thanks to the recent advances in differentiable sorting approximations the hash head receives gradients from the sorter so that the hash encoder can be optimized along with the training procedure. Additionally we describe a novel Sorted Noise-Contrastive Estimation (SortedNCE) loss that selectively picks positive and negative samples for contrastive learning which allows NSH to mine data semantic relations during training in an unsupervised manner. Our extensive experiments show the proposed NSH model significantly outperforms the existing unsupervised hashing methods on three benchmarked datasets.</p>\n", "tags": ["ARXIV", "Unsupervised"], "tsne_embedding": [1.870452642440796, -1.0170589685440063]}, {"key": "yu2022weighted", "year": "2022", "title": "Weighted Contrastive Hashing", "abstract": "<p>The development of unsupervised hashing is advanced by the recent popular contrastive learning paradigm. However previous contrastive learning-based works have been hampered by (1) insufficient data similarity mining based on global-only image representations and (2) the hash code semantic loss caused by the data augmentation. In this paper we propose a novel method namely Weighted Contrative Hashing (WCH) to take a step towards solving these two problems. We introduce a novel mutual attention module to alleviate the problem of information asymmetry in network features caused by the missing image structure during contrative augmentation. Furthermore we explore the fine-grained semantic relations between images i.e. we divide the images into multiple patches and calculate similarities between patches. The aggregated weighted similarities which reflect the deep image relations are distilled to facilitate the hash codes learning with a distillation loss so as to obtain better retrieval performance. Extensive experiments show that the proposed WCH significantly outperforms existing unsupervised hashing methods on three benchmark datasets.</p>\n", "tags": ["ARXIV", "Unsupervised"], "tsne_embedding": [-9.18864631652832, 10.165308952331543]}, {"key": "yu2023generating", "year": "2023", "title": "Generating Images With Multimodal Language Models", "abstract": "<p>We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities image retrieval novel image generation and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation our model is also capable of image retrieval from a prespecified dataset and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs and produce retrieved images generated images and generated text \u2013 outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.</p>\n", "tags": ["ARXIV", "Cross Modal", "Image Retrieval"], "tsne_embedding": [32.46397018432617, -2.151811122894287]}, {"key": "yu2023grounding", "year": "2023", "title": "Grounding Language Models To Images For Multimodal Inputs And Outputs", "abstract": "<p>We propose an efficient method to ground pretrained text-only language models to the visual domain enabling them to process arbitrarily interleaved image-and-text data and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining such as in-context learning and free-form text generation. We keep the language model frozen and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective general solution for leveraging pretrained language models in visually grounded settings.</p>\n", "tags": ["ARXIV", "Cross Modal", "Image Retrieval"], "tsne_embedding": [31.203969955444336, -2.3190395832061768]}, {"key": "yu2023hallucidoctor", "year": "2023", "title": "Hallucidoctor Mitigating Hallucinatory Toxicity In Visual Instruction Data", "abstract": "<p>Multi-modal Large Language Models (MLLMs) tuned on machine-generated instruction-following data have demonstrated remarkable performance in various multi-modal understanding and generation tasks. However the hallucinations inherent in machine-generated data which could lead to hallucinatory outputs in MLLMs remain under-explored. This work aims to investigate various hallucinations (i.e. object relation attribute hallucinations) and mitigate those hallucinatory toxicities in large-scale machine-generated visual instruction datasets. Drawing on the human ability to identify factual errors we present a novel hallucination detection and elimination framework HalluciDoctor based on the cross-checking paradigm. We use our framework to identify and eliminate hallucinations in the training data automatically. Interestingly HalluciDoctor also indicates that spurious correlations arising from long-tail object co-occurrences contribute to hallucinations. Based on that we execute counterfactual visual instruction expansion to balance data distribution thereby enhancing MLLMs resistance to hallucinations. Comprehensive experiments on hallucination evaluation benchmarks show that our method successfully mitigates 44.637; hallucinations relatively and maintains competitive performance compared to LLaVA. The data and code for this paper are publicly available. urlhttps://github.com/Yuqifan1117/HalluciDoctor}.</p>\n", "tags": ["ARXIV", "Has Code", "Supervised"], "tsne_embedding": [49.4983024597168, 6.966713905334473]}, {"key": "yu2023rlhf", "year": "2023", "title": "RLHF-V Towards Trustworthy Mllms Via Behavior Alignment From Fine-grained Correctional Human Feedback", "abstract": "<p>Multimodal Large Language Models (MLLMs) have recently demonstrated impressive capabilities in multimodal understanding reasoning and interaction. However existing MLLMs prevalently suffer from serious hallucination problems generating text that is not factually grounded in associated images. The problem makes existing MLLMs untrustworthy and thus impractical in real-world (especially high-stakes) applications. To address the challenge we present RLHF-V which enhances MLLM trustworthiness via behavior alignment from fine-grained correctional human feedback. Specifically RLHF-V collects human preference in the form of segment-level corrections on hallucinations and performs dense direct preference optimization over the human feedback. Comprehensive experiments on five benchmarks in both automatic and human evaluation show that RLHF-V can enable substantially more trustworthy MLLM behaviors with promising data and computation efficiency. Remarkably using 1.4k annotated data samples RLHF-V significantly reduces the hallucination rate of the base MLLM by 34.837; outperforming the concurrent LLaVA-RLHF trained on 10k annotated data. The final model achieves state-of-the-art performance in trustworthiness among open-source MLLMs and shows better robustness than GPT-4V in preventing hallucinations aroused from over-generalization. We open-source our code model and data at https://github.com/RLHF-V/RLHF-V.</p>\n", "tags": ["ARXIV", "Cross Modal", "Has Code"], "tsne_embedding": [48.34814453125, 5.5312275886535645]}, {"key": "yuan2018optimal", "year": "2018", "title": "Towards Optimal Deep Hashing via Policy Gradient", "abstract": "<p>In this paper, we propose a simple yet effective relaxation free method to learn more effective binary codes via policy gradient for\nscalable image search. While a variety of deep hashing methods have been\nproposed in recent years, most of them are confronted by the dilemma\nto obtain optimal binary codes in a truly end-to-end manner with nonsmooth sign activations. Unlike existing methods which usually employ a\ngeneral relaxation framework to adapt to the gradient-based algorithms,\nour approach formulates the non-smooth part of the hashing network\nas sampling with a stochastic policy, so that the retrieval performance\ndegradation caused by the relaxation can be avoided. Specifically, our\nmethod directly generates the binary codes and maximizes the expectation of rewards for similarity preservation, where the network can be\ntrained directly via policy gradient. Hence, the differentiation challenge\nfor discrete optimization can be naturally addressed, which leads to effective gradients and binary codes. Extensive experimental results on three\nbenchmark datasets validate the effectiveness of the proposed method.</p>\n", "tags": ["Deep Learning", "ECCV"], "tsne_embedding": [-6.032404899597168, 12.256111145019531]}, {"key": "yuan2019central", "year": "2019", "title": "Central Similarity Quantization For Efficient Image And Video Retrieval", "abstract": "<p>Existing data-dependent hashing methods usually learn hash functions from pairwise or triplet data relationships which only capture the data similarity locally and often suffer from low learning efficiency and low collision rate. In this work we propose a new emphglobal similarity metric termed as emphcentral similarity with which the hash codes of similar data pairs are encouraged to approach a common center and those for dissimilar pairs to converge to different centers to improve hash learning efficiency and retrieval accuracy. We principally formulate the computation of the proposed central similarity metric by introducing a new concept i.e. emphhash center that refers to a set of data points scattered in the Hamming space with a sufficient mutual distance between each other. We then provide an efficient method to construct well separated hash centers by leveraging the Hadamard matrix and Bernoulli distributions. Finally we propose the Central Similarity Quantization (CSQ) that optimizes the central similarity between data points w.r.t. their hash centers instead of optimizing the local similarity. CSQ is generic and applicable to both image and video hashing scenarios. Extensive experiments on large-scale image and video retrieval tasks demonstrate that CSQ can generate cohesive hash codes for similar data pairs and dispersed hash codes for dissimilar pairs achieving a noticeable boost in retrieval performance i.e. 337;-2037; in mAP over the previous state-of-the-arts. The code is at urlhttps://github.com/yuanli2333/Hadamard-Matrix-for-hashing}</p>\n", "tags": ["ARXIV", "Has Code", "Independent", "Quantisation", "Video Retrieval"], "tsne_embedding": [-15.445704460144043, 2.1715385913848877]}, {"key": "yuan2020central", "year": "2020", "title": "Central Similarity Hashing for Efficient Image and Video Retrieval", "abstract": "<p>Existing data-dependent hashing methods usually learn\nhash functions from the pairwise or triplet data relationships, which only capture the data similarity locally, and\noften suffer low learning efficiency and low collision rate.\nIn this work, we propose a new global similarity metric,\ntermed as central similarity, with which the hash codes for\nsimilar data pairs are encouraged to approach a common\ncenter and those for dissimilar pairs to converge to different centers, to improve hash learning efficiency and retrieval accuracy. We principally formulate the computation of the proposed central similarity metric by introducing a new concept, i.e. hash center that refers to a set\nof data points scattered in the Hamming space with sufficient mutual distance between each other. We then provide an efficient method to construct well separated hash\ncenters by leveraging the Hadamard matrix and Bernoulli\ndistributions. Finally, we propose the Central Similarity\nHashing (CSH) that optimizes the central similarity between data points w.r.t. their hash centers instead of optimizing the local similarity. The CSH is generic and applicable to both image and video hashing. Extensive experiments on large-scale image and video retrieval demonstrate CSH can generate cohesive hash codes for similar\ndata pairs and dispersed hash codes for dissimilar pairs,\nand achieve noticeable boost in retrieval performance, i.e.\n3%-20% in mAP over the previous state-of-the-art. The\ncodes are in: https://github.com/yuanli2333/\nHadamard-Matrix-for-hashing</p>\n", "tags": ["CVPR", "Deep Learning", "Has Code", "Video Retrieval"], "tsne_embedding": [-15.50051498413086, 2.1728906631469727]}, {"key": "yuan2020quant", "year": "2020", "title": "Central Similarity Quantization for Efficient Image and Video Retrieval", "abstract": "<p>Existing data-dependent hashing methods usually learn hash functions from pairwise or triplet data relationships, which only capture the data similarity locally, and often suffer from low learning efficiency and low collision rate. In this work, we propose a new global similarity metric, termed as central similarity, with which the hash codes of similar data pairs are encouraged to approach a common center and those for dissimilar pairs to converge to different centers, to improve hash learning efficiency and retrieval accuracy. We principally formulate the computation of the proposed central similarity metric by introducing a new concept, i.e., hash center that refers to a set of data points scattered in the Hamming space with a sufficient mutual distance between each other. We then provide an efficient method to construct well separated hash centers by leveraging the Hadamard matrix and Bernoulli distributions. Finally, we propose the Central Similarity Quantization (CSQ) that optimizes the central similarity between data points w.r.t. their hash centers instead of optimizing the local similarity. CSQ is generic and applicable to both image and video hashing scenarios. Extensive experiments on large-scale image and video retrieval tasks demonstrate that CSQ can generate cohesive hash codes for similar data pairs and dispersed hash codes for dissimilar pairs, achieving a noticeable boost in retrieval performance, i.e. 3%-20% in mAP over the previous state-of-the-arts.</p>\n", "tags": ["CVPR", "Deep Learning", "Has Code", "Image Retrieval", "Video Retrieval"], "tsne_embedding": [-15.496399879455566, 2.166224241256714]}, {"key": "yuan2023semantic", "year": "2023", "title": "Semantic-aware Adversarial Training For Reliable Deep Hashing Retrieval", "abstract": "<p>Deep hashing has been intensively studied and successfully applied in large-scale image retrieval systems due to its efficiency and effectiveness. Recent studies have recognized that the existence of adversarial examples poses a security threat to deep hashing models that is adversarial vulnerability. Notably it is challenging to efficiently distill reliable semantic representatives for deep hashing to guide adversarial learning and thereby it hinders the enhancement of adversarial robustness of deep hashing-based retrieval models. Moreover current researches on adversarial training for deep hashing are hard to be formalized into a unified minimax structure. In this paper we explore Semantic-Aware Adversarial Training (SAAT) for improving the adversarial robustness of deep hashing models. Specifically we conceive a discriminative mainstay features learning (DMFL) scheme to construct semantic representatives for guiding adversarial learning in deep hashing. Particularly our DMFL with the strict theoretical guarantee is adaptively optimized in a discriminative learning manner where both discriminative and semantic properties are jointly considered. Moreover adversarial examples are fabricated by maximizing the Hamming distance between the hash codes of adversarial samples and mainstay features the efficacy of which is validated in the adversarial attack trials. Further we for the first time formulate the formalized adversarial training of deep hashing into a unified minimax optimization under the guidance of the generated mainstay codes. Extensive experiments on benchmark datasets show superb attack performance against the state-of-the-art algorithms meanwhile the proposed adversarial training can effectively eliminate adversarial perturbations for trustworthy deep hashing-based retrieval. Our code is available at https://github.com/xandery-geek/SAAT.\u201d</p>\n", "tags": ["Has Code", "Image Retrieval", "Independent"], "tsne_embedding": [7.329407215118408, 3.7515976428985596]}, {"key": "yue2023llamarec", "year": "2023", "title": "Llamarec Two-stage Recommendation Using Large Language Models For Ranking", "abstract": "<p>Recently large language models (LLMs) have exhibited significant progress in language understanding and generation. By leveraging textual features customized LLMs are also applied for recommendation and demonstrate improvements across diverse recommendation scenarios. Yet the majority of existing methods perform training-free recommendation that heavily relies on pretrained knowledge (e.g. movie recommendation). In addition inference on LLMs is slow due to autoregressive generation rendering existing methods less effective for real-time recommendation. As such we propose a two-stage framework using large language models for ranking-based recommendation (LlamaRec). In particular we use small-scale sequential recommenders to retrieve candidates based on the user interaction history. Then both history and retrieved items are fed to the LLM in text via a carefully designed prompt template. Instead of generating next-item titles we adopt a verbalizer-based approach that transforms output logits into probability distributions over the candidate items. Therefore the proposed LlamaRec can efficiently rank items without generating long text. To validate the effectiveness of the proposed framework we compare against state-of-the-art baseline methods on benchmark datasets. Our experimental results demonstrate the performance of LlamaRec which consistently achieves superior performance in both recommendation performance and efficiency.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [43.97412872314453, -8.60660457611084]}, {"key": "yue2024less", "year": "2024", "title": "Less Is More Mitigating Multimodal Hallucination From An EOS Decision Perspective", "abstract": "<p>Large Multimodal Models (LMMs) often suffer from multimodal hallucinations wherein they may create content that is not present in the visual inputs. In this paper we explore a new angle of this issue overly detailed training data hinders the models ability to timely terminate generation leading to continued outputs beyond visual perception limits. By investigating how the model decides to terminate generation with EOS the special end-of-sentence token we find that the model assesses the completeness of the entire sequence by comparing the generated text with the image. This observation suggests that the model possesses an inherent potential of making proper EOS decisions based on its visual perception to avoid overly lengthy outputs. To take advantage of such potential we explore two methods to mitigate multimodal hallucinations a training objective that enables the model to reduce hallucinations by learning from regular instruction data and a data filtering strategy to prevent harmful training data from exacerbating model hallucinations. Both methods significantly improve the hallucination performance of LMMs without requiring any additional data or knowledge.</p>\n", "tags": ["ARXIV", "Cross Modal", "Supervised"], "tsne_embedding": [47.80959701538086, 6.453798294067383]}, {"key": "yun2024neurohash", "year": "2024", "title": "Neurohash A Hyperdimensional Neuro-symbolic Framework For Spatially-aware Image Hashing And Retrieval", "abstract": "<p>Customizable image retrieval from large datasets remains a critical challenge particularly when preserving spatial relationships within images. Traditional hashing methods primarily based on deep learning often fail to capture spatial information adequately and lack transparency. In this paper we introduce NeuroHash a novel neuro-symbolic framework leveraging Hyperdimensional Computing (HDC) to enable highly customizable spatially-aware image retrieval. NeuroHash combines pre-trained deep neural network models with HDC-based symbolic models allowing for flexible manipulation of hash values to support conditional image retrieval. Our method includes a self-supervised context-aware HDC encoder and novel loss terms for optimizing lower-dimensional bipolar hashing using multilinear hyperplanes. We evaluate NeuroHash on two benchmark datasets demonstrating superior performance compared to state-of-the-art hashing methods as measured by mAP@5K scores and our newly introduced metric mAP@5Kr which assesses spatial alignment. The results highlight NeuroHashs ability to achieve competitive performance while offering significant advantages in flexibility and customization paving the way for more advanced and versatile image retrieval systems.</p>\n", "tags": ["ARXIV", "Deep Learning", "Image Retrieval", "Supervised"], "tsne_embedding": [-7.307054042816162, 6.373920917510986]}, {"key": "zang2023contextual", "year": "2023", "title": "Contextual Object Detection With Multimodal Large Language Models", "abstract": "<p>Recent Multimodal Large Language Models (MLLMs) are remarkable in vision-language tasks such as image captioning and question answering but lack the essential perception ability i.e. object detection. In this work we address this limitation by introducing a novel research problem of contextual object detection \u2013 understanding visible objects within different human-AI interactive contexts. Three representative scenarios are investigated including the language cloze test visual captioning and question answering. Moreover we present ContextDET a unified multimodal model that is capable of end-to-end differentiable modeling of visual-language contexts so as to locate identify and associate visual objects with language inputs for human-AI interaction. Our ContextDET involves three key submodels (i) a visual encoder for extracting visual representations (ii) a pre-trained LLM for multimodal context decoding and (iii) a visual decoder for predicting bounding boxes given contextual object words. The new generate-then-detect framework enables us to detect object words within human vocabulary. Extensive experiments show the advantages of ContextDET on our proposed CODE benchmark open-vocabulary detection and referring image segmentation. Github https://github.com/yuhangzang/ContextDET.</p>\n", "tags": ["ARXIV", "Cross Modal", "Has Code"], "tsne_embedding": [39.494178771972656, 5.572587013244629]}, {"key": "zelikman2022star", "year": "2022", "title": "Star Bootstrapping Reasoning With Reasoning", "abstract": "<p>Generating step-by-step chain-of-thought rationales improves language model performance on complex reasoning tasks like mathematics or commonsense question-answering. However inducing language model rationale generation currently requires either constructing massive rationale datasets or sacrificing accuracy by using only few-shot inference. We propose a technique to iteratively leverage a small number of rationale examples and a large dataset without rationales to bootstrap the ability to perform successively more complex reasoning. This technique the Self-Taught Reasoner (STaR) relies on a simple loop generate rationales to answer many questions prompted with a few rationale examples; if the generated answers are wrong try again to generate a rationale given the correct answer; fine-tune on all the rationales that ultimately yielded correct answers; repeat. We show that STaR significantly improves performance on multiple datasets compared to a model fine-tuned to directly predict final answers and performs comparably to fine-tuning a 30(times) larger state-of-the-art language model on CommensenseQA. Thus STaR lets a model improve itself by learning from its own generated reasoning.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [29.766613006591797, 4.240925312042236]}, {"key": "zeng2019simultaneous", "year": "2019", "title": "Simultaneous Region Localization And Hash Coding For Fine-grained Image Retrieval", "abstract": "<p>Fine-grained image hashing is a challenging problem due to the difficulties of discriminative region localization and hash code generation. Most existing deep hashing approaches solve the two tasks independently. While these two tasks are correlated and can reinforce each other. In this paper we propose a deep fine-grained hashing to simultaneously localize the discriminative regions and generate the efficient binary codes. The proposed approach consists of a region localization module and a hash coding module. The region localization module aims to provide informative regions to the hash coding module. The hash coding module aims to generate effective binary codes and give feedback for learning better localizer. Moreover to better capture subtle differences multi-scale regions at different layers are learned without the need of bounding-box/part annotations. Extensive experiments are conducted on two public benchmark fine-grained datasets. The results demonstrate significant improvements in the performance of our method relative to other fine-grained hashing algorithms.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Independent"], "tsne_embedding": [-14.291507720947266, 11.177550315856934]}, {"key": "zeng2021pangu", "year": "2021", "title": "Pangu-(\u03b1) Large-scale Autoregressive Pretrained Chinese Language Models With Auto-parallel Computation", "abstract": "<p>Large-scale Pretrained Language Models (PLMs) have become the new paradigm for Natural Language Processing (NLP). PLMs with hundreds of billions parameters such as GPT-3 have demonstrated strong performances on natural language understanding and generation with textitfew-shot in-context learning. In this work we present our practice on training large-scale autoregressive language models named PanGu-(alpha) with up to 200 billion parameters. PanGu-(alpha) is developed under the MindSpore and trained on a cluster of 2048 Ascend 910 AI processors. The training parallelism strategy is implemented based on MindSpore Auto-parallel which composes five parallelism dimensions to scale the training task to 2048 processors efficiently including data parallelism op-level model parallelism pipeline model parallelism optimizer model parallelism and rematerialization. To enhance the generalization ability of PanGu-(alpha) we collect 1.1TB high-quality Chinese data from a wide range of domains to pretrain the model. We empirically test the generation ability of PanGu-(alpha) in various scenarios including text summarization question answering dialogue generation etc. Moreover we investigate the effect of model scales on the few-shot performances across a broad range of Chinese NLP tasks. The experimental results demonstrate the superior capabilities of PanGu-(alpha) in performing various tasks under few-shot or zero-shot settings.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [23.61773109436035, -7.843433856964111]}, {"key": "zeng2021phpq", "year": "2021", "title": "PHPQ Pyramid Hybrid Pooling Quantization For Efficient Fine-grained Image Retrieval", "abstract": "<p>Deep hashing approaches including deep quantization and deep binary hashing have become a common solution to large-scale image retrieval due to their high computation and storage efficiency. Most existing hashing methods cannot produce satisfactory results for fine-grained retrieval because they usually adopt the outputs of the last CNN layer to generate binary codes. Since deeper layers tend to summarize visual clues e.g. texture into abstract semantics e.g. dogs and cats the feature produced by the last CNN layer is less effective in capturing subtle but discriminative visual details that mostly exist in shallow layers. To improve fine-grained image hashing we propose Pyramid Hybrid Pooling Quantization (PHPQ). Specifically we propose a Pyramid Hybrid Pooling (PHP) module to capture and preserve fine-grained semantic information from multi-level features which emphasizes the subtle discrimination of different sub-categories. Besides we propose a learnable quantization module with a partial codebook attention mechanism which helps to optimize the most relevant codewords and improves the quantization. Comprehensive experiments on two widely-used public benchmarks i.e. CUB-200-2011 and Stanford Dogs demonstrate that PHPQ outperforms state-of-the-art methods.</p>\n", "tags": ["CNN", "Image Retrieval", "Quantisation"], "tsne_embedding": [-5.56281852722168, 3.9988396167755127]}, {"key": "zeng2022glm", "year": "2022", "title": "GLM-130B An Open Bilingual Pre-trained Model", "abstract": "<p>We introduce GLM-130B a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 (davinci) and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort we face numerous unexpected technical and engineering challenges particularly on loss spikes and divergence. In this paper we introduce the training process of GLM-130B including its design choices training strategies for both efficiency and stability and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B (davinci) on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B \u2013 the largest Chinese language model \u2013 across related benchmarks. Finally we leverage a unique scaling property of GLM-130B to reach INT4 quantization without post training with almost no performance loss making it the first among 100B-scale models and more importantly allowing its effective inference on 4(times)RTX 3090 (24G) or 8(times)RTX 2080 Ti (11G) GPUs the most affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code training logs related toolkit and lessons learned are open-sourced at urlhttps://github.com/THUDM/GLM-130B/}.</p>\n", "tags": ["ARXIV", "Has Code", "Quantisation"], "tsne_embedding": [22.02897834777832, -7.748745918273926]}, {"key": "zeng2022socratic", "year": "2022", "title": "Socratic Models Composing Zero-shot Multimodal Reasoning With Language", "abstract": "<p>Large pretrained (e.g. foundation) models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic they may only barely overlap. For example visual-language models (VLMs) are trained on Internet-scale image captions but large language models (LMs) are further trained on Internet-scale text with no images (e.g. spreadsheets SAT questions code). As a result these models store different forms of commonsense knowledge across different domains. In this work we show that this diversity is symbiotic and can be leveraged through Socratic Models (SMs) a modular framework in which multiple pretrained models may be composed zero-shot i.e. via multimodal-informed prompting to exchange information with each other and capture new multimodal capabilities without requiring finetuning. With minimal engineering SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval but also enable new applications such as (i) answering free-form questions about egocentric video (ii) engaging in multimodal assistive dialogue with people (e.g. for cooking recipes) by interfacing with external APIs and databases (e.g. web search) and (iii) robot perception and planning.</p>\n", "tags": ["ARXIV", "Cross Modal", "Text Retrieval"], "tsne_embedding": [15.564029693603516, -8.000850677490234]}, {"key": "zeng2023agenttuning", "year": "2023", "title": "Agenttuning Enabling Generalized Agent Abilities For Llms", "abstract": "<p>Open large language models (LLMs) with great performance in various tasks have significantly advanced the development of LLMs. However they are far inferior to commercial models such as ChatGPT and GPT-4 when acting as agents to tackle complex tasks in the real world. These agent tasks employ LLMs as the central controller responsible for planning memorization and tool utilization necessitating both fine-grained prompting methods and robust LLMs to achieve satisfactory performance. Though many prompting methods have been proposed to complete particular agent tasks there is lack of research focusing on improving the agent capabilities of LLMs themselves without compromising their general abilities. In this work we present AgentTuning a simple and general method to enhance the agent abilities of LLMs while maintaining their general LLM capabilities. We construct AgentInstruct a lightweight instruction-tuning dataset containing high-quality interaction trajectories. We employ a hybrid instruction-tuning strategy by combining AgentInstruct with open-source instructions from general domains. AgentTuning is used to instruction-tune the Llama 2 series resulting in AgentLM. Our evaluations show that AgentTuning enables LLMs agent capabilities without compromising general abilities. The AgentLM-70B is comparable to GPT-3.5-turbo on unseen agent tasks demonstrating generalized agent capabilities. We open source the AgentInstruct and AgentLM-7B 13B and 70B models at https://github.com/THUDM/AgentTuning, serving open and powerful alternatives to commercial LLMs for agent tasks.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [40.47372817993164, -4.925944805145264]}, {"key": "zeng2023cascading", "year": "2023", "title": "Cascading Hierarchical Networks With Multi-task Balanced Loss For Fine-grained Hashing", "abstract": "<p>With the explosive growth in the number of fine-grained images in the Internet era it has become a challenging problem to perform fast and efficient retrieval from large-scale fine-grained images. Among the many retrieval methods hashing methods are widely used due to their high efficiency and small storage space occupation. Fine-grained hashing is more challenging than traditional hashing problems due to the difficulties such as low inter-class variances and high intra-class variances caused by the characteristics of fine-grained images. To improve the retrieval accuracy of fine-grained hashing we propose a cascaded network to learn compact and highly semantic hash codes and introduce an attention-guided data augmentation method. We refer to this network as a cascaded hierarchical data augmentation network. We also propose a novel approach to coordinately balance the loss of multi-task learning. We do extensive experiments on some common fine-grained visual classification datasets. The experimental results demonstrate that our proposed method outperforms several state-of-art hashing methods and can effectively improve the accuracy of fine-grained retrieval. The source code is publicly available https://github.com/kaiba007/FG-CNET.\u201d</p>\n", "tags": ["ARXIV", "Has Code", "Supervised"], "tsne_embedding": [4.0795578956604, 8.818236351013184]}, {"key": "zeng2024federated", "year": "2024", "title": "Federated Recommendation Via Hybrid Retrieval Augmented Generation", "abstract": "<p>Federated Recommendation (FR) emerges as a novel paradigm that enables privacy-preserving recommendations. However traditional FR systems usually represent users/items with discrete identities (IDs) suffering from performance degradation due to the data sparsity and heterogeneity in FR. On the other hand Large Language Models (LLMs) as recommenders have proven effective across various recommendation scenarios. Yet LLM-based recommenders encounter challenges such as low inference efficiency and potential hallucination compromising their performance in real-world scenarios. To this end we propose GPT-FedRec a federated recommendation framework leveraging ChatGPT and a novel hybrid Retrieval Augmented Generation (RAG) mechanism. GPT-FedRec is a two-stage solution. The first stage is a hybrid retrieval process mining ID-based user patterns and text-based item features. Next the retrieved results are converted into text prompts and fed into GPT for re-ranking. Our proposed hybrid retrieval mechanism and LLM-based re-rank aims to extract generalized features from data and exploit pretrained knowledge within LLM overcoming data sparsity and heterogeneity in FR. In addition the RAG approach also prevents LLM hallucination improving the recommendation performance for real-world users. Experimental results on diverse benchmark datasets demonstrate the superior performance of GPT-FedRec against state-of-the-art baseline methods.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [41.91030502319336, -7.316471099853516]}, {"key": "zhai2023halle", "year": "2023", "title": "Halle-control Controlling Object Hallucination In Large Multimodal Models", "abstract": "<p>Current Large Multimodal Models (LMMs) achieve remarkable progress yet there remains significant uncertainty regarding their ability to accurately apprehend visual details that is in performing detailed captioning. To address this we introduce () a GPT-4 assisted evaluation method for detailed captioning. Interestingly while LMMs demonstrate minimal object existence hallucination in existing VQA benchmarks our proposed evaluation reveals continued susceptibility to such hallucinations. In this paper we make the first attempt to investigate such hallucination from different aspects including image resolution the language decoder size and instruction data amount quality granularity. Our findings underscore the unwarranted inference when the language description includes details at a finer object granularity than what the vision module can ground or verify thus inducing hallucination. To control such hallucinations we further attribute the reliability of captioning to contextual knowledge (involving only contextually grounded objects) and parametric knowledge (containing inferred objects by the model). Thus we introduce () a controllable LMM in terms of ()ucination in object ()xistence. HallE-Control can condition the captioning to shift between (i) exclusively depicting contextual knowledge for grounded objects and (ii) blending it with parametric knowledge to imagine inferred objects. Our method reduces hallucination by 4437; compared to LLaVA(_7B) and maintains the object coverage.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [50.38655090332031, 6.360889911651611]}, {"key": "zhan2020weakly", "year": "2020", "title": "Weakly-supervised Online Hashing", "abstract": "<p>With the rapid development of social websites recent years have witnessed an explosive growth of social images with user-provided tags which continuously arrive in a streaming fashion. Due to the fast query speed and low storage cost hashing-based methods for image search have attracted increasing attention. However existing hashing methods for social image retrieval are based on batch mode which violates the nature of social images i.e. social images are usually generated periodically or collected in a stream fashion. Although there exist many online image hashing methods they either adopt unsupervised learning which ignore the relevant tags or are designed in the supervised manner which needs high-quality labels. In this paper to overcome the above limitations we propose a new method named Weakly-supervised Online Hashing (WOH). In order to learn high-quality hash codes WOH exploits the weak supervision by considering the semantics of tags and removing the noise. Besides We develop a discrete online optimization algorithm for WOH which is efficient and scalable. Extensive experiments conducted on two real-world datasets demonstrate the superiority of WOH compared with several state-of-the-art hashing baselines.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Supervised", "Weakly Supervised"], "tsne_embedding": [1.37834632396698, 5.6876139640808105]}, {"key": "zhang2010self", "year": "2010", "title": "Self-taught Hashing For Fast Similarity Search", "abstract": "<p>The ability of fast similarity search at large scale is of great importance to many Information Retrieval (IR) applications. A promising way to accelerate similarity search is semantic hashing which designs compact binary codes for a large number of documents so that semantically similar documents are mapped to similar codes (within a short Hamming distance). Although some recently proposed techniques are able to generate high-quality codes for documents known in advance obtaining the codes for previously unseen documents remains to be a very challenging problem. In this paper we emphasise this issue and propose a novel Self-Taught Hashing (STH) approach to semantic hashing we first find the optimal (l)-bit binary codes for all documents in the given corpus via unsupervised learning and then train (l) classifiers via supervised learning to predict the (l)-bit code for any query document unseen before. Our experiments on three real-world text datasets show that the proposed approach using binarised Laplacian Eigenmap (LapEig) and linear Support Vector Machine (SVM) outperforms state-of-the-art techniques significantly.</p>\n", "tags": ["ARXIV", "Unsupervised"], "tsne_embedding": [11.789162635803223, -4.212964057922363]}, {"key": "zhang2011composite", "year": "2011", "title": "Composite Hashing with Multiple Information Sources", "abstract": "<p>Similarity search applications with a large amount of text\nand image data demands an efficient and effective solution.\nOne useful strategy is to represent the examples in databases\nas compact binary codes through semantic hashing, which\nhas attracted much attention due to its fast query/search\nspeed and drastically reduced storage requirement. All of\nthe current semantic hashing methods only deal with the\ncase when each example is represented by one type of features.\nHowever, examples are often described from several\ndifferent information sources in many real world applications.\nFor example, the characteristics of a webpage can be\nderived from both its content part and its associated links.\nTo address the problem of learning good hashing codes in\nthis scenario, we propose a novel research problem \u2013 Composite\nHashing with Multiple Information Sources (CHMIS).\nThe focus of the new research problem is to design an algorithm\nfor incorporating the features from different information\nsources into the binary hashing codes efficiently and\neffectively. In particular, we propose an algorithm CHMISAW\n(CHMIS with Adjusted Weights) for learning the codes.\nThe proposed algorithm integrates information from several\ndifferent sources into the binary hashing codes by adjusting\nthe weights on each individual source for maximizing\nthe coding performance, and enables fast conversion from\nquery examples to their binary hashing codes. Experimental\nresults on five different datasets demonstrate the superior\nperformance of the proposed method against several other\nstate-of-the-art semantic hashing techniques.</p>\n", "tags": [], "tsne_embedding": [-11.71210765838623, -1.014532446861267]}, {"key": "zhang2013binary", "year": "2013", "title": "Binary Code Ranking with Weighted Hamming Distance", "abstract": "<p>Binary hashing has been widely used for efficient similarity search due to its query and storage efficiency. In most\nexisting binary hashing methods, the high-dimensional data are embedded into Hamming space and the distance or\nsimilarity of two points are approximated by the Hamming\ndistance between their binary codes. The Hamming distance calculation is efficient, however, in practice, there are\noften lots of results sharing the same Hamming distance to\na query, which makes this distance measure ambiguous and\nposes a critical issue for similarity search where ranking is\nimportant. In this paper, we propose a weighted Hamming\ndistance ranking algorithm (WhRank) to rank the binary\ncodes of hashing methods. By assigning different bit-level\nweights to different hash bits, the returned binary codes\nare ranked at a finer-grained binary code level. We give\nan algorithm to learn the data-adaptive and query-sensitive\nweight for each hash bit. Evaluations on two large-scale\nimage data sets demonstrate the efficacy of our weighted\nHamming distance for binary code ranking.</p>\n", "tags": ["CVPR"], "tsne_embedding": [-14.646852493286133, 8.134095191955566]}, {"key": "zhang2014largescale", "year": "2014", "title": "Large-scale supervised multimodal hashing with semantic correlation maximization", "abstract": "<p>Due to its low storage cost and fast query speed, hashing\nhas been widely adopted for similarity search in multimedia\ndata. In particular, more and more attentions\nhave been payed to multimodal hashing for search in\nmultimedia data with multiple modalities, such as images\nwith tags. Typically, supervised information of semantic\nlabels is also available for the data points in\nmany real applications. Hence, many supervised multimodal\nhashing (SMH) methods have been proposed\nto utilize such semantic labels to further improve the\nsearch accuracy. However, the training time complexity\nof most existing SMH methods is too high, which\nmakes them unscalable to large-scale datasets. In this\npaper, a novel SMH method, called semantic correlation\nmaximization (SCM), is proposed to seamlessly integrate\nsemantic labels into the hashing learning procedure\nfor large-scale data modeling. Experimental results\non two real-world datasets show that SCM can signifi-\ncantly outperform the state-of-the-art SMH methods, in\nterms of both accuracy and scalability.</p>\n", "tags": ["AAAI", "Cross Modal", "Has Code", "Supervised"], "tsne_embedding": [-7.422305583953857, -4.145575523376465]}, {"key": "zhang2014latent", "year": "2014", "title": "Supervised Hashing with Latent Factor Models", "abstract": "<p>Due to its low storage cost and fast query speed, hashing\nhas been widely adopted for approximate nearest neighbor\nsearch in large-scale datasets. Traditional hashing methods\ntry to learn the hash codes in an unsupervised way where\nthe metric (Euclidean) structure of the training data is preserved.\nVery recently, supervised hashing methods, which\ntry to preserve the semantic structure constructed from the\nsemantic labels of the training points, have exhibited higher\naccuracy than unsupervised methods. In this paper, we\npropose a novel supervised hashing method, called latent\nfactor hashing (LFH), to learn similarity-preserving binary\ncodes based on latent factor models. An algorithm with\nconvergence guarantee is proposed to learn the parameters\nof LFH. Furthermore, a linear-time variant with stochastic\nlearning is proposed for training LFH on large-scale datasets.\nExperimental results on two large datasets with semantic\nlabels show that LFH can achieve superior accuracy than\nstate-of-the-art methods with comparable training time.</p>\n", "tags": ["SIGIR", "Supervised"], "tsne_embedding": [-1.9963949918746948, -6.233543872833252]}, {"key": "zhang2015bit", "year": "2015", "title": "Bit-scalable Deep Hashing With Regularized Similarity Learning For Image Retrieval And Person Re-identification", "abstract": "<p>Extracting informative image features and learning effective approximate hashing functions are two crucial steps in image retrieval . Conventional methods often study these two steps separately e.g. learning hash functions from a predefined hand-crafted feature space. Meanwhile the bit lengths of output hashing codes are preset in most previous methods neglecting the significance level of different bits and restricting their practical flexibility. To address these issues we propose a supervised learning framework to generate compact and bit-scalable hashing codes directly from raw images. We pose hashing learning as a problem of regularized similarity learning. Specifically we organize the training images into a batch of triplet samples each sample containing two images with the same label and one with a different label. With these triplet samples we maximize the margin between matched pairs and mismatched pairs in the Hamming space. In addition a regularization term is introduced to enforce the adjacency consistency i.e. images of similar appearances should have similar codes. The deep convolutional neural network is utilized to train the model in an end-to-end fashion where discriminative image features and hash functions are simultaneously optimized. Furthermore each bit of our hashing codes is unequally weighted so that we can manipulate the code lengths by truncating the insignificant bits. Our framework outperforms state-of-the-arts on public benchmarks of similar image search and also achieves promising results in the application of person re-identification in surveillance. It is also shown that the generated bit-scalable hashing codes well preserve the discriminative powers with shorter code lengths.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Supervised"], "tsne_embedding": [-7.448238849639893, 8.228069305419922]}, {"key": "zhang2015efficient", "year": "2015", "title": "Efficient Training Of Very Deep Neural Networks For Supervised Hashing", "abstract": "<p>In this paper we propose training very deep neural networks (DNNs) for supervised learning of hash codes. Existing methods in this context train relatively shallow networks limited by the issues arising in back propagation (e.e. vanishing gradients) as well as computational efficiency. We propose a novel and efficient training algorithm inspired by alternating direction method of multipliers (ADMM) that overcomes some of these limitations. Our method decomposes the training process into independent layer-wise local updates through auxiliary variables. Empirically we observe that our training algorithm always converges and its computational complexity is linearly proportional to the number of edges in the networks. Empirically we manage to train DNNs with 64 hidden layers and 1024 nodes per layer for supervised hashing in about 3 hours using a single GPU. Our proposed very deep supervised hashing (VDSH) method significantly outperforms the state-of-the-art on several benchmark datasets.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [6.351440906524658, -9.508962631225586]}, {"key": "zhang2016efficient", "year": "2016", "title": "Efficient Training of Very Deep Neural Networks for Supervised Hashing", "abstract": "<p>In this paper, we propose training very deep neural networks (DNNs) for supervised learning of hash codes. Existing methods in this context train relatively \u201cshallow\u201d networks limited by the issues arising in back propagation (e.e. vanishing gradients) as well as computational efficiency. We propose a novel and efficient training algorithm inspired by alternating direction method of multipliers (ADMM) that overcomes some of these limitations. Our method decomposes the training process into independent layer-wise local updates through auxiliary variables. Empirically we observe that our training algorithm always converges and its computational complexity is linearly proportional to the number of edges in the networks. Empirically we manage to train DNNs with 64 hidden layers and 1024 nodes per layer for supervised hashing in about 3 hours using a single GPU. Our proposed very deep supervised hashing (VDSH) method significantly outperforms the state-of-the-art on several benchmark datasets.</p>\n", "tags": ["CVPR", "Deep Learning", "Supervised"], "tsne_embedding": [6.318109035491943, -9.496781349182129]}, {"key": "zhang2016query", "year": "2016", "title": "Query-adaptive Image Retrieval By Deep Weighted Hashing", "abstract": "<p>Hashing methods have attracted much attention for large scale image retrieval. Some deep hashing methods have achieved promising results by taking advantage of the strong representation power of deep networks recently. However existing deep hashing methods treat all hash bits equally. On one hand a large number of images share the same distance to a query image due to the discrete Hamming distance which raises a critical issue of image retrieval where fine-grained rankings are very important. On the other hand different hash bits actually contribute to the image retrieval differently and treating them equally greatly affects the retrieval accuracy of image. To address the above two problems we propose the query-adaptive deep weighted hashing (QaDWH) approach which can perform fine-grained ranking for different queries by weighted Hamming distance. First a novel deep hashing network is proposed to learn the hash codes and corresponding class-wise weights jointly so that the learned weights can reflect the importance of different hash bits for different image classes. Second a query-adaptive image retrieval method is proposed which rapidly generates hash bit weights for different query images by fusing its semantic probability and the learned class-wise weights. Fine-grained image retrieval is then performed by the weighted Hamming distance which can provide more accurate ranking than the traditional Hamming distance. Experiments on four widely used datasets show that the proposed approach outperforms eight state-of-the-art hashing methods.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Independent"], "tsne_embedding": [-12.63121223449707, 9.1286039352417]}, {"key": "zhang2016scalable", "year": "2016", "title": "Scalable Discrete Supervised Hash Learning With Asymmetric Matrix Factorization", "abstract": "<p>Hashing method maps similar data to binary hashcodes with smaller hamming distance and it has received a broad attention due to its low storage cost and fast retrieval speed. However the existing limitations make the present algorithms difficult to deal with large-scale datasets (1) discrete constraints are involved in the learning of the hash function; (2) pairwise or triplet similarity is adopted to generate efficient hashcodes resulting both time and space complexity are greater than O(n^2). To address these issues we propose a novel discrete supervised hash learning framework which can be scalable to large-scale datasets. First the discrete learning procedure is decomposed into a binary classifier learning scheme and binary codes learning scheme which makes the learning procedure more efficient. Second we adopt the Asymmetric Low-rank Matrix Factorization and propose the Fast Clustering-based Batch Coordinate Descent method such that the time and space complexity is reduced to O(n). The proposed framework also provides a flexible paradigm to incorporate with arbitrary hash function including deep neural networks and kernel methods. Experiments on large-scale datasets demonstrate that the proposed method is superior or comparable with state-of-the-art hashing algorithms.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [-0.2278704047203064, -7.272711277008057]}, {"key": "zhang2016ssdh", "year": "2016", "title": "SSDH Semi-supervised Deep Hashing For Large Scale Image Retrieval", "abstract": "<p>Hashing methods have been widely used for efficient similarity retrieval on large scale image database. Traditional hashing methods learn hash functions to generate binary codes from hand-crafted features which achieve limited accuracy since the hand-crafted features cannot optimally represent the image content and preserve the semantic similarity. Recently several deep hashing methods have shown better performance because the deep architectures generate more discriminative feature representations. However these deep hashing methods are mainly designed for supervised scenarios which only exploit the semantic similarity information but ignore the underlying data structures. In this paper we propose the semi-supervised deep hashing (SSDH) approach to perform more effective hash function learning by simultaneously preserving semantic similarity and underlying data structures. The main contributions are as follows (1) We propose a semi-supervised loss to jointly minimize the empirical error on labeled data as well as the embedding error on both labeled and unlabeled data which can preserve the semantic similarity and capture the meaningful neighbors on the underlying data structures for effective hashing. (2) A semi-supervised deep hashing network is designed to extensively exploit both labeled and unlabeled data in which we propose an online graph construction method to benefit from the evolving deep features during training to better capture semantic neighbors. To the best of our knowledge the proposed deep network is the first deep hashing method that can perform hash code learning and feature learning simultaneously in a semi-supervised fashion. Experimental results on 5 widely-used datasets show that our proposed approach outperforms the state-of-the-art hashing methods.</p>\n", "tags": ["ARXIV", "Graph", "Image Retrieval", "Supervised"], "tsne_embedding": [-6.871776103973389, 1.5697230100631714]}, {"key": "zhang2017hashganattention", "year": "2017", "title": "Hashganattention-aware Deep Adversarial Hashing For Cross Modal Retrieval", "abstract": "<p>As the rapid growth of multi-modal data hashing methods for cross-modal retrieval have received considerable attention. Deep-networks-based cross-modal hashing methods are appealing as they can integrate feature learning and hash coding into end-to-end trainable frameworks. However it is still challenging to find content similarities between different modalities of data due to the heterogeneity gap. To further address this problem we propose an adversarial hashing network with attention mechanism to enhance the measurement of content similarities by selectively focusing on informative parts of multi-modal data. The proposed new adversarial network HashGAN consists of three building blocks 1) the feature learning module to obtain feature representations 2) the generative attention module to generate an attention mask which is used to obtain the attended (foreground) and the unattended (background) feature representations 3) the discriminative hash coding module to learn hash functions that preserve the similarities between different modalities. In our framework the generative module and the discriminative module are trained in an adversarial way the generator is learned to make the discriminator cannot preserve the similarities of multi-modal data w.r.t. the background feature representations while the discriminator aims to preserve the similarities of multi-modal data w.r.t. both the foreground and the background feature representations. Extensive evaluations on several benchmark datasets demonstrate that the proposed HashGAN brings substantial improvements over other state-of-the-art cross-modal hashing methods.</p>\n", "tags": ["ARXIV", "Cross Modal", "Independent"], "tsne_embedding": [6.01140832901001, 3.502673387527466]}, {"key": "zhang2017unsupervised", "year": "2017", "title": "Unsupervised Generative Adversarial Cross-modal Hashing", "abstract": "<p>Cross-modal hashing aims to map heterogeneous multimedia data into a common Hamming space which can realize fast and flexible retrieval across different modalities. Unsupervised cross-modal hashing is more flexible and applicable than supervised methods since no intensive labeling work is involved. However existing unsupervised methods learn hashing functions by preserving inter and intra correlations while ignoring the underlying manifold structure across different modalities which is extremely helpful to capture meaningful nearest neighbors of different modalities for cross-modal retrieval. To address the above problem in this paper we propose an Unsupervised Generative Adversarial Cross-modal Hashing approach (UGACH) which makes full use of GANs ability for unsupervised representation learning to exploit the underlying manifold structure of cross-modal data. The main contributions can be summarized as follows (1) We propose a generative adversarial network to model cross-modal hashing in an unsupervised fashion. In the proposed UGACH given a data of one modality the generative model tries to fit the distribution over the manifold structure and select informative data of another modality to challenge the discriminative model. The discriminative model learns to distinguish the generated data and the true positive data sampled from correlation graph to achieve better retrieval accuracy. These two models are trained in an adversarial way to improve each other and promote hashing function learning. (2) We propose a correlation graph based approach to capture the underlying manifold structure across different modalities so that data of different modalities but within the same manifold can have smaller Hamming distance and promote retrieval accuracy. Extensive experiments compared with 6 state-of-the-art methods verify the effectiveness of our proposed approach.</p>\n", "tags": ["ARXIV", "Cross Modal", "Graph", "Unsupervised"], "tsne_embedding": [-6.522157669067383, -0.9454507827758789]}, {"key": "zhang2018improved", "year": "2018", "title": "Improved Deep Hashing With Soft Pairwise Similarity For Multi-label Image Retrieval", "abstract": "<p>Hash coding has been widely used in the approximate nearest neighbor search for large-scale image retrieval. Recently many deep hashing methods have been proposed and shown largely improved performance over traditional feature-learning-based methods. Most of these methods examine the pairwise similarity on the semantic-level labels where the pairwise similarity is generally defined in a hard-assignment way. That is the pairwise similarity is 1 if they share no less than one class label and 0 if they do not share any. However such similarity definition cannot reflect the similarity ranking for pairwise images that hold multiple labels. In this paper a new deep hashing method is proposed for multi-label image retrieval by re-defining the pairwise similarity into an instance similarity where the instance similarity is quantified into a percentage based on the normalized semantic labels. Based on the instance similarity a weighted cross-entropy loss and a minimum mean square error loss are tailored for loss-function construction and are efficiently used for simultaneous feature learning and hash coding. Experiments on three popular datasets demonstrate that the proposed method outperforms the competing methods and achieves the state-of-the-art performance in multi-label image retrieval.</p>\n", "tags": ["ARXIV", "Image Retrieval"], "tsne_embedding": [-15.49254035949707, 6.923340320587158]}, {"key": "zhang2018semantic", "year": "2018", "title": "Semantic Cluster Unary Loss For Efficient Deep Hashing", "abstract": "<p>Hashing method maps similar data to binary hashcodes with smaller hamming distance which has received a broad attention due to its low storage cost and fast retrieval speed. With the rapid development of deep learning deep hashing methods have achieved promising results in efficient information retrieval. Most of the existing deep hashing methods adopt pairwise or triplet losses to deal with similarities underlying the data but the training is difficult and less efficient because (O(n^2)) data pairs and (O(n^3)) triplets are involved. To address these issues we propose a novel deep hashing algorithm with unary loss which can be trained very efficiently. We first of all introduce a Unary Upper Bound of the traditional triplet loss thus reducing the complexity to (O(n)) and bridging the classification-based unary loss and the triplet loss. Second we propose a novel Semantic Cluster Deep Hashing (SCDH) algorithm by introducing a modified Unary Upper Bound loss named Semantic Cluster Unary Loss (SCUL). The resultant hashcodes form several compact clusters which means hashcodes in the same cluster have similar semantic information. We also demonstrate that the proposed SCDH is easy to be extended to semi-supervised settings by incorporating the state-of-the-art semi-supervised learning algorithms. Experiments on large-scale datasets show that the proposed method is superior to state-of-the-art hashing algorithms.</p>\n", "tags": ["Deep Learning", "ICIP", "Supervised"], "tsne_embedding": [-1.3694571256637573, -7.419993877410889]}, {"key": "zhang2019collaborative", "year": "2019", "title": "Collaborative Quantization For Cross-modal Similarity Search", "abstract": "<p>Cross-modal similarity search is a problem about designing a search system supporting querying across content modalities e.g. using an image to search for texts or using a text to search for images. This paper presents a compact coding solution for efficient search with a focus on the quantization approach which has already shown the superior performance over the hashing solutions in the single-modal similarity search. We propose a cross-modal quantization approach which is among the early attempts to introduce quantization into cross-modal search. The major contribution lies in jointly learning the quantizers for both modalities through aligning the quantized representations for each pair of image and text belonging to a document. In addition our approach simultaneously learns the common space for both modalities in which quantization is conducted to enable efficient and effective search using the Euclidean distance computed in the common space with fast distance table lookup. Experimental results compared with several competitive algorithms over three benchmark datasets demonstrate that the proposed approach achieves the state-of-the-art performance.</p>\n", "tags": ["ARXIV", "Cross Modal", "Quantisation"], "tsne_embedding": [-12.820765495300293, 5.198355674743652]}, {"key": "zhang2019joint", "year": "2019", "title": "Joint Cluster Unary Loss For Efficient Cross-modal Hashing", "abstract": "<p>With the rapid growth of various types of multimodal data cross-modal deep hashing has received broad attention for solving cross-modal retrieval problems efficiently. Most cross-modal hashing methods follow the traditional supervised hashing framework in which the (O(n^2)) data pairs and (O(n^3)) data triplets are generated for training but the training procedure is less efficient because the complexity is high for large-scale dataset. To address these issues we propose a novel and efficient cross-modal hashing algorithm in which the unary loss is introduced. First of all We introduce the Cross-Modal Unary Loss (CMUL) with (O(n)) complexity to bridge the traditional triplet loss and classification-based unary loss. A more accurate bound of the triplet loss for structured multilabel data is also proposed in CMUL. Second we propose the novel Joint Cluster Cross-Modal Hashing (JCCH) algorithm for efficient hash learning in which the CMUL is involved. The resultant hashcodes form several clusters in which the hashcodes in the same cluster share similar semantic information and the heterogeneity gap on different modalities is diminished by sharing the clusters. The proposed algorithm is able to be applied to various types of data and experiments on large-scale datasets show that the proposed method is superior over or comparable with state-of-the-art cross-modal hashing methods and training with the proposed method is more efficient than others.</p>\n", "tags": ["ARXIV", "Cross Modal", "Supervised"], "tsne_embedding": [-1.9523075819015503, -7.607385635375977]}, {"key": "zhang2019pairwise", "year": "2019", "title": "Pairwise Teacher-student Network For Semi-supervised Hashing", "abstract": "<p>Hashing method maps similar high-dimensional data to binary hashcodes with smaller hamming distance and it has received broad attention due to its low storage cost and fast retrieval speed. Pairwise similarity is easily obtained and widely used for retrieval and most supervised hashing algorithms are carefully designed for the pairwise supervisions. As labeling all data pairs is difficult semi-supervised hashing is proposed which aims at learning efficient codes with limited labeled pairs and abundant unlabeled ones. Existing methods build graphs to capture the structure of dataset but they are not working well for complex data as the graph is built based on the data representations and determining the representations of complex data is difficult. In this paper we propose a novel teacher-student semi-supervised hashing framework in which the student is trained with the pairwise information produced by the teacher network. The network follows the smoothness assumption which achieves consistent distances for similar data pairs so that the retrieval results are similar for neighborhood queries. Experiments on large-scale datasets show that the proposed method reaches impressive gain over the supervised baselines and is superior to state-of-the-art semi-supervised hashing methods.</p>\n", "tags": ["ARXIV", "Graph", "Supervised"], "tsne_embedding": [4.932924747467041, -1.9630206823349]}, {"key": "zhang2019sadih", "year": "2019", "title": "SADIH Semantic-aware Discrete Hashing", "abstract": "<p>Due to its low storage cost and fast query speed hashing has been recognized to accomplish similarity search in large-scale multimedia retrieval applications. Particularly supervised hashing has recently received considerable research attention by leveraging the label information to preserve the pairwise similarities of data points in the Hamming space. However there still remain two crucial bottlenecks 1) the learning process of the full pairwise similarity preservation is computationally unaffordable and unscalable to deal with big data; 2) the available category information of data are not well-explored to learn discriminative hash functions. To overcome these challenges we propose a unified Semantic-Aware DIscrete Hashing (SADIH) framework which aims to directly embed the transformed semantic information into the asymmetric similarity approximation and discriminative hashing function learning. Specifically a semantic-aware latent embedding is introduced to asymmetrically preserve the full pairwise similarities while skillfully handle the cumbersome n times n pairwise similarity matrix. Meanwhile a semantic-aware autoencoder is developed to jointly preserve the data structures in the discriminative latent semantic space and perform data reconstruction. Moreover an efficient alternating optimization algorithm is proposed to solve the resulting discrete optimization problem. Extensive experimental results on multiple large-scale datasets demonstrate that our SADIH can clearly outperform the state-of-the-art baselines with the additional benefit of lower computational costs.</p>\n", "tags": ["ARXIV", "Supervised"], "tsne_embedding": [-7.800878524780273, 0.23795823752880096]}, {"key": "zhang2019semantic", "year": "2019", "title": "Semantic Hierarchy Preserving Deep Hashing For Large-scale Image Retrieval", "abstract": "<p>Deep hashing models have been proposed as an efficient method for large-scale similarity search. However most existing deep hashing methods only utilize fine-level labels for training while ignoring the natural semantic hierarchy structure. This paper presents an effective method that preserves the classwise similarity of full-level semantic hierarchy for large-scale image retrieval. Experiments on two benchmark datasets show that our method helps improve the fine-level retrieval performance. Moreover with the help of the semantic hierarchy it can produce significantly better binary codes for hierarchical retrieval which indicates its potential of providing more user-desired retrieval results.</p>\n", "tags": ["ARXIV", "Image Retrieval"], "tsne_embedding": [-4.531392574310303, 8.109593391418457]}, {"key": "zhang2020fast", "year": "2020", "title": "Fast Discrete Cross-Modal Hashing Based on Label Relaxation and Matrix Factorization", "abstract": "<p>In recent years, cross-media retrieval has drawn considerable attention due to the exponential growth of multimedia data. Many hashing approaches have been proposed for the cross-media search task. However, there are still open problems that warrant investigation. For example, most existing supervised hashing approaches employ a binary label matrix, which achieves small margins between wrong labels (0) and true labels (1). This may affect the retrieval performance by generating many false negatives and false positives. In addition, some methods adopt a relaxation scheme to solve the binary constraints, which may cause large quantization errors. There are also some discrete hashing methods that have been presented, but most of them are time-consuming. To conquer these problems, we present a label relaxation and discrete matrix factorization method (LRMF) for cross-modal retrieval. It offers a number of innovations. First of all, the proposed approach employs a novel label relaxation scheme to control the margins adaptively, which has the benefit of reducing the quantization error. Second, by virtue of the proposed discrete matrix factorization method designed to learn the binary codes, large quantization errors caused by relaxation can be avoided. The experimental results obtained on two widely-used databases demonstrate that LRMF outperforms state-of-the-art cross-media methods.</p>\n", "tags": ["Cross Modal", "ICPR", "Supervised"], "tsne_embedding": [-2.9176185131073, -10.092896461486816]}, {"key": "zhang2020hierarchical", "year": "2020", "title": "Hierarchical Deep Hashing for Fast Large Scale Image Retrieval", "abstract": "<p>Fast image retrieval is of great importance in many computer vision tasks and especially practical applications. Deep hashing, the state-of-the-art fast image retrieval scheme, introduces deep learning to learn the hash functions and generate binary hash codes, and outperforms the other image retrieval methods in terms of accuracy. However, all the existing deep hashing methods could only generate one level hash codes and require a linear traversal of all the hash codes to figure out the closest one when a new query arrives, which is very time-consuming and even intractable for large scale applications. In this work, we propose a Hierarchical Deep Hashing(HDHash) scheme to speed up the state-of-the-art deep hashing methods. More specifically, hierarchical deep hash codes of multiple levels can be generated and indexed with tree structures rather than linear ones, and pruning irrelevant branches can sharply decrease the retrieval time. To our best knowledge, this is the first work to introduce hierarchical indexed deep hashing for fast large scale image retrieval. Extensive experimental results on three benchmark datasets demonstrate that the proposed HDHash scheme achieves better or comparable accuracy with significantly improved efficiency and reduced memory as compared to state-of- the-art fast image retrieval schemes.</p>\n", "tags": ["Deep Learning", "ICPR", "Image Retrieval"], "tsne_embedding": [-3.6180214881896973, 1.490195870399475]}, {"key": "zhang2020survey", "year": "2020", "title": "A Survey On Deep Hashing For Image Retrieval", "abstract": "<p>Hashing has been widely used in approximate nearest search for large-scale database retrieval for its computation and storage efficiency. Deep hashing which devises convolutional neural network architecture to exploit and extract the semantic information or feature of images has received increasing attention recently. In this survey several deep supervised hashing methods for image retrieval are evaluated and I conclude three main different directions for deep supervised hashing methods. Several comments are made at the end. Moreover to break through the bottleneck of the existing hashing methods I propose a Shadow Recurrent Hashing(SRH) method as a try. Specifically I devise a CNN architecture to extract the semantic features of images and design a loss function to encourage similar images projected close. To this end I propose a concept shadow of the CNN output. During optimization process the CNN output and its shadow are guiding each other so as to achieve the optimal solution as much as possible. Several experiments on dataset CIFAR-10 show the satisfying performance of SRH.</p>\n", "tags": ["ARXIV", "CNN", "Image Retrieval", "Supervised", "Survey Paper"], "tsne_embedding": [1.7351033687591553, 15.54274845123291]}, {"key": "zhang2021cpm", "year": "2021", "title": "CPM-2 Large-scale Cost-effective Pre-trained Language Models", "abstract": "<p>In recent years the size of pre-trained language models (PLMs) has grown by leaps and bounds. However efficiency issues of these large-scale PLMs limit their utilization in real-world scenarios. We present a suite of cost-effective techniques for the use of PLMs to deal with the efficiency issues of pre-training fine-tuning and inference. (1) We introduce knowledge inheritance to accelerate the pre-training process by exploiting existing PLMs instead of training models from scratch. (2) We explore the best practice of prompt tuning with large-scale PLMs. Compared with conventional fine-tuning prompt tuning significantly reduces the number of task-specific parameters. (3) We implement a new inference toolkit namely InfMoE for using large-scale PLMs with limited computational resources. Based on our cost-effective pipeline we pre-train two models an encoder-decoder bilingual model with 11 billion parameters (CPM-2) and its corresponding MoE version with 198 billion parameters. In our experiments we compare CPM-2 with mT5 on downstream tasks. Experimental results show that CPM-2 has excellent general language intelligence. Moreover we validate the efficiency of InfMoE when conducting inference of large-scale models having tens of billions of parameters on a single GPU. All source code and model parameters are available at https://github.com/TsinghuaAI/CPM.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [19.4193115234375, -6.044798374176025]}, {"key": "zhang2021deep", "year": "2021", "title": "Deep Center-Based Dual-Constrained Hashing for Discriminative Face Image Retrieval", "abstract": "<p>With the advantages of low storage cost and extremely fast retrieval speed, deep hashing methods have attracted much attention for image retrieval recently. However, large-scale face image retrieval with significant intra-class variations is still challenging. Neither existing pairwise/triplet labels-based nor softmax classification loss-based deep hashing works can generate compact and discriminative binary codes. Considering these issues, we propose a center-based framework integrating end-to-end hashing learning and class centers learning simultaneously. The framework minimizes the intra-class variance by clustering intra-class samples into a learnable class center. To strengthen inter-class separability, it additionally imposes a novel regularization term to enlarge the Hamming distance between pairwise class centers. Moreover, a simple yet effective regression matrix is introduced to encourage intra-class samples to generate the same binary codes, which further enhances the hashing codes compactness. Experiments on four large-scale datasets show the proposed method outperforms state-of-the-art baselines under various code lengths and commonly-used evaluation metrics.</p>\n", "tags": ["Deep Learning", "Image Retrieval"], "tsne_embedding": [-3.127995491027832, 8.848260879516602]}, {"key": "zhang2021high", "year": "2021", "title": "High-order nonlocal Hashing for unsupervised cross-modal retrieval", "abstract": "<p>In light of the ability to enable efficient storage and fast query for big data, hashing techniques for cross-modal search have aroused extensive attention. Despite the great success achieved, unsupervised cross-modal hashing still suffers from lacking reliable similarity supervision and struggles with handling the heterogeneity issue between different modalities. To cope with these, in this paper, we devise a new deep hashing model, termed as High-order Nonlocal Hashing (HNH) to facilitate cross-modal retrieval with the following advantages. First, different from existing methods that mainly leverage low-level local-view similarity as the guidance for hashing learning, we propose a high-order affinity measure that considers the multi-modal neighbourhood structures from a nonlocal perspective, thereby comprehensively capturing the similarity relationships between data items. Second, a common representation is introduced to correlate different modalities. By enforcing the modal-specific descriptors and the common representation to be aligned with each other, the proposed HNH significantly bridges the modality gap and maintains the intra-consistency. Third, an effective affinity preserving objective function is delicately designed to generate high-quality binary codes. Extensive experiments evidence the superiority of the proposed HNH in unsupervised cross-modal retrieval tasks over the state-of-the-art baselines.</p>\n", "tags": ["Cross Modal", "Supervised", "WWW"], "tsne_embedding": [-13.13833236694336, -1.6513538360595703]}, {"key": "zhang2021improved", "year": "2021", "title": "Improved Deep Classwise Hashing With Centers Similarity Learning For Image Retrieval", "abstract": "<p>Deep supervised hashing for image retrieval has attracted researchers attention due to its high efficiency and superior retrieval performance. Most existing deep supervised hashing works which are based on pairwise/triplet labels suffer from the expensive computational cost and insufficient utilization of the semantics information. Recently deep classwise hashing introduced a classwise loss supervised by class labels information alternatively; however we find it still has its drawback. In this paper we propose an improved deep classwise hashing which enables hashing learning and class centers learning simultaneously. Specifically we design a two-step strategy on center similarity learning. It interacts with the classwise loss to attract the class center to concentrate on the intra-class samples while pushing other class centers as far as possible. The centers similarity learning contributes to generating more compact and discriminative hashing codes. We conduct experiments on three benchmark datasets. It shows that the proposed method effectively surpasses the original method and outperforms state-of-the-art baselines under various commonly-used evaluation metrics for image retrieval.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Supervised"], "tsne_embedding": [-3.5842690467834473, 8.833843231201172]}, {"key": "zhang2021instance", "year": "2021", "title": "Instance-weighted Central Similarity For Multi-label Image Retrieval", "abstract": "<p>Deep hashing has been widely applied to large-scale image retrieval by encoding high-dimensional data points into binary codes for efficient retrieval. Compared with pairwise/triplet similarity based hash learning central similarity based hashing can more efficiently capture the global data distribution. For multi-label image retrieval however previous methods only use multiple hash centers with equal weights to generate one centroid as the learning target which ignores the relationship between the weights of hash centers and the proportion of instance regions in the image. To address the above issue we propose a two-step alternative optimization approach Instance-weighted Central Similarity (ICS) to automatically learn the center weight corresponding to a hash code. Firstly we apply the maximum entropy regularizer to prevent one hash center from dominating the loss function and compute the center weights via projection gradient descent. Secondly we update neural network parameters by standard back-propagation with fixed center weights. More importantly the learned center weights can well reflect the proportion of foreground instances in the image. Our method achieves the state-of-the-art performance on the image retrieval benchmarks and especially improves the mAP by 1.637;-6.437; on the MS COCO dataset.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Supervised"], "tsne_embedding": [-1.6569392681121826, 12.732596397399902]}, {"key": "zhang2021joint", "year": "2021", "title": "Joint Learning Of Deep Retrieval Model And Product Quantization Based Embedding Index", "abstract": "<p>Embedding index that enables fast approximate nearest neighbor(ANN) search serves as an indispensable component for state-of-the-art deep retrieval systems. Traditional approaches often separating the two steps of embedding learning and index building incur additional indexing time and decayed retrieval accuracy. In this paper we propose a novel method called Poeem which stands for product quantization based embedding index jointly trained with deep retrieval model to unify the two separate steps within an end-to-end training by utilizing a few techniques including the gradient straight-through estimator warm start strategy optimal space decomposition and Givens rotation. Extensive experimental results show that the proposed method not only improves retrieval accuracy significantly but also reduces the indexing time to almost none. We have open sourced our approach for the sake of comparison and reproducibility.</p>\n", "tags": ["ARXIV", "Quantisation"], "tsne_embedding": [-17.908512115478516, -7.171558380126953]}, {"key": "zhang2021moon", "year": "2021", "title": "MOON Multi-hash Codes Joint Learning For Cross-media Retrieval", "abstract": "<p>In recent years cross-media hashing technique has attracted increasing attention for its high computation efficiency and low storage cost. However the existing approaches still have some limitations which need to be explored. 1) A fixed hash length (e.g. 16bits or 32bits) is predefined before learning the binary codes. Therefore these models need to be retrained when the hash length changes that consumes additional computation power reducing the scalability in practical applications. 2) Existing cross-modal approaches only explore the information in the original multimedia data to perform the hash learning without exploiting the semantic information contained in the learned hash codes. To this end we develop a novel Multiple hash cOdes jOint learNing method (MOON) for cross-media retrieval. Specifically the developed MOON synchronously learns the hash codes with multiple lengths in a unified framework. Besides to enhance the underlying discrimination we combine the clues from the multimodal data semantic labels and learned hash codes for hash learning. As far as we know the proposed MOON is the first work to simultaneously learn different length hash codes without retraining in cross-media retrieval. Experiments on several databases show that our MOON can achieve promising performance outperforming some recent competitive shallow and deep methods.</p>\n", "tags": ["ARXIV", "Cross Modal", "Independent"], "tsne_embedding": [-4.195439338684082, -6.816163063049316]}, {"key": "zhang2022active", "year": "2022", "title": "Active Example Selection For In-context Learning", "abstract": "<p>With a handful of demonstration examples large-scale language models show strong capability to perform various tasks by in-context learning from these examples without any fine-tuning. We demonstrate that in-context learning performance can be highly unstable across samples of examples indicating the idiosyncrasies of how language models acquire information. We formulate example selection for in-context learning as a sequential decision problem and propose a reinforcement learning algorithm for identifying generalizable policies to select demonstration examples. For GPT-2 our learned policies demonstrate strong abilities of generalizing to unseen tasks in training with a (5.8) improvement on average. Examples selected from our learned policies can even achieve a small improvement on GPT-3 Ada. However the improvement diminishes on larger GPT-3 models suggesting emerging capabilities of large language models.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [24.829648971557617, -2.9648663997650146]}, {"key": "zhang2022automatic", "year": "2022", "title": "Automatic Chain Of Thought Prompting In Large Language Models", "abstract": "<p>Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like Lets think step by step to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the Lets think step by step prompt to generate reasoning chains for demonstrations one by one i.e. lets think not just step by step but also one by one. However these generated chains often come with mistakes. To mitigate the effect of such mistakes we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3 Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [35.28687286376953, -4.594443321228027]}, {"key": "zhang2022hashing", "year": "2022", "title": "Hashing Learning With Hyper-class Representation", "abstract": "<p>Existing unsupervised hash learning is a kind of attribute-centered calculation. It may not accurately preserve the similarity between data. This leads to low down the performance of hash function learning. In this paper a hash algorithm is proposed with a hyper-class representation. It is a two-steps approach. The first step finds potential decision features and establish hyper-class. The second step constructs hash learning based on the hyper-class information in the first step so that the hash codes of the data within the hyper-class are as similar as possible as well as the hash codes of the data between the hyper-classes are as different as possible. To evaluate the efficiency a series of experiments are conducted on four public datasets. The experimental results show that the proposed hash algorithm is more efficient than the compared algorithms in terms of mean average precision (MAP) average precision (AP) and Hamming radius 2 (HAM2)</p>\n", "tags": ["ARXIV", "Unsupervised"], "tsne_embedding": [-9.029083251953125, -15.086360931396484]}, {"key": "zhang2022opt", "year": "2022", "title": "OPT Open Pre-trained Transformer Language Models", "abstract": "<p>Large language models which are often trained for hundreds of thousands of compute days have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost these models are difficult to replicate without significant capital. For the few that are available through APIs no access is granted to the full model weights making them difficult to study. We present Open Pre-trained Transformers (OPT) a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3 while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced along with code for experimenting with all of the released models.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [19.688751220703125, -13.367654800415039]}, {"key": "zhang2022supervised", "year": "2022", "title": "Supervised Deep Hashing For High-dimensional And Heterogeneous Case-based Reasoning", "abstract": "<p>Case-based Reasoning (CBR) on high-dimensional and heterogeneous data is a trending yet challenging and computationally expensive task in the real world. A promising approach is to obtain low-dimensional hash codes representing cases and perform a similarity retrieval of cases in Hamming space. However previous methods based on data-independent hashing rely on random projections or manual construction inapplicable to address specific data issues (e.g. high-dimensionality and heterogeneity) due to their insensitivity to data characteristics. To address these issues this work introduces a novel deep hashing network to learn similarity-preserving compact hash codes for efficient case retrieval and proposes a deep-hashing-enabled CBR model HeCBR. Specifically we introduce position embedding to represent heterogeneous features and utilize a multilinear interaction layer to obtain case embeddings which effectively filtrates zero-valued features to tackle high-dimensionality and sparsity and captures inter-feature couplings. Then we feed the case embeddings into fully-connected layers and subsequently a hash layer generates hash codes with a quantization regularizer to control the quantization loss during relaxation. To cater to incremental learning of CBR we further propose an adaptive learning strategy to update the hash function. Extensive experiments on public datasets show that HeCBR greatly reduces storage and significantly accelerates case retrieval. HeCBR achieves desirable performance compared with the state-of-the-art CBR methods and performs significantly better than hashing-based CBR methods in classification.</p>\n", "tags": ["ARXIV", "Cross Modal", "Quantisation", "Supervised"], "tsne_embedding": [-11.964545249938965, -2.3005423545837402]}, {"key": "zhang2023adalora", "year": "2023", "title": "Adalora Adaptive Budget Allocation For Parameter-efficient Fine-tuning", "abstract": "<p>Fine-tuning large pre-trained language models on downstream tasks has become an important paradigm in NLP. However common practice fine-tunes all of the parameters in a pre-trained model which becomes prohibitive when a large number of downstream tasks are present. Therefore many fine-tuning methods are proposed to learn incremental updates of pre-trained weights in a parameter efficient way e.g. low-rank increments. These methods often evenly distribute the budget of incremental updates across all pre-trained weight matrices and overlook the varying importance of different weight parameters. As a consequence the fine-tuning performance is suboptimal. To bridge this gap we propose AdaLoRA which adaptively allocates the parameter budget among weight matrices according to their importance score. In particular AdaLoRA parameterizes the incremental updates in the form of singular value decomposition. Such a novel approach allows us to effectively prune the singular values of unimportant updates which is essentially to reduce their parameter budget but circumvent intensive exact SVD computations. We conduct extensive experiments with several pre-trained models on natural language processing question answering and natural language generation to validate the effectiveness of AdaLoRA. Results demonstrate that AdaLoRA manifests notable improvement over baselines especially in the low budget settings. Our code is publicly available at https://github.com/QingruZhang/AdaLoRA .</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [19.20024299621582, -5.37814474105835]}, {"key": "zhang2023cumulative", "year": "2023", "title": "Cumulative Reasoning With Large Language Models", "abstract": "<p>Despite the recent advancements in language models (LMs) their ability to solve complex problems remains limited. This paper introduces Cumulative Reasoning (CR) a novel approach that utilizes LMs cumulatively and iteratively mirroring human thought processes for problem-solving. CR decomposes tasks into smaller manageable components and leverages previous propositions for effective composition significantly enhancing problem-solving capabilities. We demonstrate CRs superiority through several complex reasoning tasks it outperforms existing methods in logical inference tasks with up to a 9.337; improvement achieving 98.0437; accuracy on the curated FOLIO wiki dataset. In the Game of 24 it achieves 9837; accuracy marking a 2437; improvement over the prior state-of-the-art. Additionally CR sets new state-of-the-art on the MATH dataset achieving a 4.237; increase from previous methods and a 4337; relative improvement in the most challenging problems. By extending CR to incorporate a code environment without external aids like retrieval or web browsing we further harness the computational and logical reasoning capabilities of LMs achieving a remarkable 72.237; accuracy on the MATH dataset and outperforming the PAL/PoT method by 38.837;. Our work not only sets new state-of-the-art but also paves the way toward more sophisticated AI reasoning methods. The code is available at https://github.com/iiis-ai/cumulative-reasoning.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [24.50037384033203, 3.301990270614624]}, {"key": "zhang2023generative", "year": "2023", "title": "On Generative Agents In Recommendation", "abstract": "<p>Recommender systems are the cornerstone of todays information dissemination yet a disconnect between offline metrics and online performance greatly hinders their development. Addressing this challenge we envision a recommendation simulator capitalizing on recent breakthroughs in human-level intelligence exhibited by Large Language Models (LLMs). We propose Agent4Rec a user simulator in recommendation leveraging LLM-empowered generative agents equipped with user profile memory and actions modules specifically tailored for the recommender system. In particular these agents profile modules are initialized using real-world datasets (e.g. MovieLens Steam Amazon-Book) capturing users unique tastes and social traits; memory modules log both factual and emotional memories and are integrated with an emotion-driven reflection mechanism; action modules support a wide variety of behaviors spanning both taste-driven and emotion-driven actions. Each agent interacts with personalized recommender models in a page-by-page manner relying on a pre-implemented collaborative filtering-based recommendation algorithm. We delve into both the capabilities and limitations of Agent4Rec aiming to explore an essential research question To what extent can LLM-empowered generative agents faithfully simulate the behavior of real autonomous humans in recommender systems Extensive and multi-faceted evaluations of Agent4Rec highlight both the alignment and deviation between agents and user-personalized preferences. Beyond mere performance comparison we explore insightful experiments such as emulating the filter bubble effect and discovering the underlying causal relationships in recommendation tasks. Our codes are available at https://github.com/LehengTHU/Agent4Rec.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [43.467933654785156, -11.668968200683594]}, {"key": "zhang2023is", "year": "2023", "title": "Is Chatgpt Fair For Recommendation Evaluating Fairness In Large Language Model Recommendation", "abstract": "<p>The remarkable achievements of Large Language Models (LLMs) have led to the emergence of a novel recommendation paradigm \u2013 Recommendation via LLM (RecLLM). Nevertheless it is important to note that LLMs may contain social prejudices and therefore the fairness of recommendations made by RecLLM requires further investigation. To avoid the potential risks of RecLLM it is imperative to evaluate the fairness of RecLLM with respect to various sensitive attributes on the user side. Due to the differences between the RecLLM paradigm and the traditional recommendation paradigm it is problematic to directly use the fairness benchmark of traditional recommendation. To address the dilemma we propose a novel benchmark called Fairness of Recommendation via LLM (FaiRLLM). This benchmark comprises carefully crafted metrics and a dataset that accounts for eight sensitive attributes1 in two recommendation scenarios music and movies. By utilizing our FaiRLLM benchmark we conducted an evaluation of ChatGPT and discovered that it still exhibits unfairness to some sensitive attributes when generating recommendations. Our code and dataset can be found at https://github.com/jizhi-zhang/FaiRLLM.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [47.012107849121094, -9.618237495422363]}, {"key": "zhang2023multimodal", "year": "2023", "title": "Multimodal Chain-of-thought Reasoning In Language Models", "abstract": "<p>Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However existing CoT studies have primarily focused on the language modality. We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference. In this way answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With Multimodal-CoT our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark. Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https://github.com/amazon-science/mm-cot.</p>\n", "tags": ["ARXIV", "Cross Modal", "Has Code"], "tsne_embedding": [33.36405563354492, -4.671910285949707]}, {"key": "zhang2023prompt", "year": "2023", "title": "Prompt Highlighter Interactive Control For Multi-modal Llms", "abstract": "<p>This study targets a critical aspect of multi-modal LLMs (LLMsamp;VLMs) inference explicit controllable text generation. Multi-modal LLMs empower multi-modality understanding with the capability of semantic generation yet bring less explainability and heavier reliance on prompt contents due to their autoregressive generative nature. While manipulating prompt formats could improve outputs designing specific and precise prompts per task can be challenging and ineffective. To tackle this issue we introduce a novel inference method Prompt Highlighter which enables users to highlight specific prompt spans to interactively control the focus during generation. Motivated by the classifier-free diffusion guidance we form regular and unconditional context pairs based on highlighted tokens demonstrating that the autoregressive generation in models can be guided in a classifier-free way. Notably we find that during inference guiding the models with highlighted tokens through the attention weights leads to more desired outputs. Our approach is compatible with current LLMs and VLMs achieving impressive customized generation results without training. Experiments confirm its effectiveness in focusing on input contexts and generating reliable content. Without tuning on LLaVA-v1.5 our method secured 70.7 in the MMBench test and 1552.5 in MME-perception. The code is available at https://github.com/dvlab-research/Prompt-Highlighter/</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [34.05181884765625, -1.2063502073287964]}, {"key": "zhang2023recommendation", "year": "2023", "title": "Recommendation As Instruction Following A Large Language Model Empowered Recommendation Approach", "abstract": "<p>In the past decades recommender systems have attracted much attention in both research and industry communities and a large number of studies have been devoted to developing effective recommendation models. Basically speaking these models mainly learn the underlying user preference from historical behavior data and then estimate the user-item matching relationships for recommendations. Inspired by the recent progress on large language models (LLMs) we take a different approach to developing the recommendation models considering recommendation as instruction following by LLMs. The key idea is that the preferences or needs of a user can be expressed in natural language descriptions (called instructions) so that LLMs can understand and further execute the instruction for fulfilling the recommendation task. Instead of using public APIs of LLMs we instruction tune an open-source LLM (3B Flan-T5-XL) in order to better adapt LLMs to recommender systems. For this purpose we first design a general instruction format for describing the preference intention task form and context of a user in natural language. Then we manually design 39 instruction templates and automatically generate a large amount of user-personalized instruction data (252K instructions) with varying types of preferences and intentions. To demonstrate the effectiveness of our approach we instantiate the instruction templates into several widely-studied recommendation (or search) tasks and conduct extensive experiments on these tasks with real-world datasets. Experiment results show that the proposed approach can outperform several competitive baselines including the powerful GPT-3.5 on these evaluation tasks. Our approach sheds light on developing more user-friendly recommender systems in which users can freely communicate with the system and obtain more accurate recommendations via natural language instructions.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [40.71864700317383, -10.892578125]}, {"key": "zhang2023robust", "year": "2023", "title": "Robust Recommender System A Survey And Future Directions", "abstract": "<p>With the rapid growth of information recommender systems have become integral for providing personalized suggestions and overcoming information overload. However their practical deployment often encounters dirty data where noise or malicious information can lead to abnormal recommendations. Research on improving recommender systems robustness against such dirty data has thus gained significant attention. This survey provides a comprehensive review of recent work on recommender systems robustness. We first present a taxonomy to organize current techniques for withstanding malicious attacks and natural noise. We then explore state-of-the-art methods in each category including fraudster detection adversarial training certifiable robust training against malicious attacks and regularization purification self-supervised learning against natural noise. Additionally we summarize evaluation metrics and common datasets used to assess robustness. We discuss robustness across varying recommendation scenarios and its interplay with other properties like accuracy interpretability privacy and fairness. Finally we delve into open issues and future research directions in this emerging field. Our goal is to equip readers with a holistic understanding of robust recommender systems and spotlight pathways for future research and development.</p>\n", "tags": ["ARXIV", "Supervised", "Survey Paper"], "tsne_embedding": [47.92427444458008, -2.8809406757354736]}, {"key": "zhang2023speechgpt", "year": "2023", "title": "Speechgpt Empowering Large Language Models With Intrinsic Cross-modal Conversational Abilities", "abstract": "<p>Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However current speech-language models typically adopt the cascade paradigm preventing inter-modal knowledge transfer. In this paper we propose SpeechGPT a large language model with intrinsic cross-modal conversational abilities capable of perceiving and generating multi-model content. With discrete speech representations we first construct SpeechInstruct a large-scale cross-modal speech instruction dataset. Additionally we employ a three-stage training strategy that includes modality-adaptation pre-training cross-modal instruction fine-tuning and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow multi-modal human instructions and highlight the potential of handling multiple modalities with one model. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [37.15548324584961, -1.0748159885406494]}, {"key": "zhang2024debiasing", "year": "2024", "title": "Debiasing Multimodal Large Language Models", "abstract": "<p>In the realms of computer vision and natural language processing Large Vision-Language Models (LVLMs) have become indispensable tools proficient in generating textual descriptions based on visual inputs. Despite their advancements our investigation reveals a noteworthy bias in the generated content where the output is primarily influenced by the underlying Large Language Models (LLMs) prior rather than the input image. Our empirical experiments underscore the persistence of this bias as LVLMs often provide confident answers even in the absence of relevant images or given incongruent visual input. To rectify these biases and redirect the models focus toward vision information we introduce two simple training-free strategies. Firstly for tasks such as classification or multi-choice question-answering (QA) we propose a calibration step through affine transformation to adjust the output distribution. This Post-Hoc debias approach ensures uniform scores for each answer when the image is absent serving as an effective regularization technique to alleviate the influence of LLM priors. For more intricate open-ended generation tasks we extend this method to Debias sampling drawing inspirations from contrastive decoding methods. Furthermore our investigation sheds light on the instability of LVLMs across various decoding configurations. Through systematic exploration of different settings we significantly enhance performance surpassing reported results and raising concerns about the fairness of existing evaluations. Comprehensive experiments substantiate the effectiveness of our proposed strategies in mitigating biases. These strategies not only prove beneficial in minimizing hallucinations but also contribute to the generation of more helpful and precise illustrations.</p>\n", "tags": ["ARXIV", "Cross Modal", "Supervised"], "tsne_embedding": [45.86020278930664, 4.920780181884766]}, {"key": "zhang2024notellm", "year": "2024", "title": "Notellm A Retrievable Large Language Model For Note Recommendation", "abstract": "<p>People enjoy sharing notes including their experiences within online communities. Therefore recommending notes aligned with user interests has become a crucial task. Existing online methods only input notes into BERT-based models to generate note embeddings for assessing similarity. However they may underutilize some important cues e.g. hashtags or categories which represent the key concepts of notes. Indeed learning to generate hashtags/categories can potentially enhance note embeddings both of which compress key note information into limited content. Besides Large Language Models (LLMs) have significantly outperformed BERT in understanding natural languages. It is promising to introduce LLMs into note recommendation. In this paper we propose a novel unified framework called NoteLLM which leverages LLMs to address the item-to-item (I2I) note recommendation. Specifically we utilize Note Compression Prompt to compress a note into a single special token and further learn the potentially related notes embeddings via a contrastive learning approach. Moreover we use NoteLLM to summarize the note and generate the hashtag/category automatically through instruction tuning. Extensive validations on real scenarios demonstrate the effectiveness of our proposed method compared with the online baseline and show major improvements in the recommendation system of Xiaohongshu.</p>\n", "tags": ["ARXIV", "Deep Learning", "Self Supervised"], "tsne_embedding": [42.97820281982422, -16.252330780029297]}, {"key": "zhang2024spar", "year": "2024", "title": "SPAR Personalized Content-based Recommendation Via Long Engagement Attention", "abstract": "<p>Leveraging users long engagement histories is essential for personalized content recommendations. The success of pretrained language models (PLMs) in NLP has led to their use in encoding user histories and candidate items framing content recommendations as textual semantic matching tasks. However existing works still struggle with processing very long user historical text and insufficient user-item interaction. In this paper we introduce a content-based recommendation framework SPAR which effectively tackles the challenges of holistic user interest extraction from the long user engagement history. It achieves so by leveraging PLM poly-attention layers and attention sparsity mechanisms to encode users history in a session-based manner. The user and item side features are sufficiently fused for engagement prediction while maintaining standalone representations for both sides which is efficient for practical model deployment. Moreover we enhance user profiling by exploiting large language model (LLM) to extract global interests from user engagement history. Extensive experiments on two benchmark datasets demonstrate that our framework outperforms existing state-of-the-art (SoTA) methods.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [40.131202697753906, -14.284950256347656]}, {"key": "zhao2015deep", "year": "2015", "title": "Deep Semantic Ranking Based Hashing For Multi-label Image Retrieval", "abstract": "<p>With the rapid growth of web images hashing has received increasing interests in large scale image retrieval. Research efforts have been devoted to learning compact binary codes that preserve semantic similarity based on labels. However most of these hashing methods are designed to handle simple binary similarity. The complex multilevel semantic structure of images associated with multiple labels have not yet been well explored. Here we propose a deep semantic ranking based method for learning hash functions that preserve multilevel semantic similarity between multi-label images. In our approach deep convolutional neural network is incorporated into hash functions to jointly learn feature representations and mappings from them to hash codes which avoids the limitation of semantic representation power of hand-crafted features. Meanwhile a ranking list that encodes the multilevel similarity information is employed to guide the learning of such deep hash functions. An effective scheme based on surrogate loss is used to solve the intractable optimization problem of nonsmooth and multivariate ranking measures involved in the learning procedure. Experimental results show the superiority of our proposed approach over several state-of-the-art hashing methods in term of ranking evaluation metrics when tested on multi-label image datasets.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Supervised"], "tsne_embedding": [-4.271466255187988, 7.219137668609619]}, {"key": "zhao2021rescuing", "year": "2021", "title": "Rescuing Deep Hashing From Dead Bits Problem", "abstract": "<p>Deep hashing methods have shown great retrieval accuracy and efficiency in large-scale image retrieval. How to optimize discrete hash bits is always the focus in deep hashing methods. A common strategy in these methods is to adopt an activation function e.g. (operatornamesigmoid)((cdot)) or (operatornametanh)((cdot)) and minimize a quantization loss to approximate discrete values. However this paradigm may make more and more hash bits stuck into the wrong saturated area of the activation functions and never escaped. We call this problem Dead Bits Problem~(DBP). Besides the existing quantization loss will aggravate DBP as well. In this paper we propose a simple but effective gradient amplifier which acts before activation functions to alleviate DBP. Moreover we devise an error-aware quantization loss to further alleviate DBP. It avoids the negative effect of quantization loss based on the similarity between two images. The proposed gradient amplifier and error-aware quantization loss are compatible with a variety of deep hashing methods. Experimental results on three datasets demonstrate the efficiency of the proposed gradient amplifier and the error-aware quantization loss.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Quantisation"], "tsne_embedding": [-2.413949966430664, -1.3603390455245972]}, {"key": "zhao2023beyond", "year": "2023", "title": "Beyond Hallucinations Enhancing Lvlms Through Hallucination-aware Direct Preference Optimization", "abstract": "<p>Multimodal large language models have made significant advancements in recent years yet they still suffer from a common issue known as the hallucination problem in which the models generate textual descriptions that inaccurately depict or entirely fabricate content from associated images. This paper introduces a novel solution Hallucination-Aware Direct Preference Optimization (HA-DPO) which reframes the hallucination problem as a preference selection task. The model is trained to favor the non-hallucinating response when presented with two responses of the same image (one accurate and one hallucinatory). Furthermore this paper proposes an efficient pipeline for constructing positive~(non-hallucinatory) and negative~(hallucinatory) sample pairs ensuring a high-quality style-consistent dataset for robust preference learning. When applied to three mainstream multimodal models HA-DPO significantly reduced hallucination issues and amplified the models generalization capabilities. Notably the MiniGPT-4 model when enhanced with HA-DPO demonstrated a substantial improvement POPE accuracy rose from 51.1337; to 86.1337; (an absolute improvement of 3537;) and the MME score surged from 932.00 to 1326.46 (a relative improvement of 42.3237;). The codes models and datasets are made accessible at https://opendatalab.github.io/HA-DPO.</p>\n", "tags": ["ARXIV", "Cross Modal", "Has Code"], "tsne_embedding": [48.99488067626953, 4.819938659667969]}, {"key": "zhao2023evaluating", "year": "2023", "title": "On Evaluating Adversarial Robustness Of Large Vision-language Models", "abstract": "<p>Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented performance in response generation especially with visual inputs enabling more creative and adaptable interaction than large language models such as ChatGPT. Nonetheless multimodal generation exacerbates safety concerns since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g. vision). To this end we propose evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses. In particular we first craft targeted adversarial examples against pretrained models such as CLIP and BLIP and then transfer these adversarial examples to other VLMs such as MiniGPT-4 LLaVA UniDiffuser BLIP-2 and Img2Prompt. In addition we observe that black-box queries on these VLMs can further improve the effectiveness of targeted evasion resulting in a surprisingly high success rate for generating targeted responses. Our findings provide a quantitative understanding regarding the adversarial vulnerability of large VLMs and call for a more thorough examination of their potential security flaws before deployment in practice. Code is at https://github.com/yunqing-me/AttackVLM.</p>\n", "tags": ["ARXIV", "Cross Modal", "Has Code"], "tsne_embedding": [48.87948226928711, 1.808189868927002]}, {"key": "zhao2023expel", "year": "2023", "title": "Expel LLM Agents Are Experiential Learners", "abstract": "<p>The recent surge in research interest in applying large language models (LLMs) to decision-making tasks has flourished by leveraging the extensive world knowledge embedded in LLMs. While there is a growing demand to tailor LLMs for custom decision-making tasks finetuning them for specific tasks is resource-intensive and may diminish the models generalization capabilities. Moreover state-of-the-art language models like GPT-4 and Claude are primarily accessible through API calls with their parametric weights remaining proprietary and unavailable to the public. This scenario emphasizes the growing need for new methodologies that allow learning from agent experiences without requiring parametric updates. To address these problems we introduce the Experiential Learning (ExpeL) agent. Our agent autonomously gathers experiences and extracts knowledge using natural language from a collection of training tasks. At inference the agent recalls its extracted insights and past experiences to make informed decisions. Our empirical results highlight the robust learning efficacy of the ExpeL agent indicating a consistent enhancement in its performance as it accumulates experiences. We further explore the emerging capabilities and transfer learning potential of the ExpeL agent through qualitative observations and additional experiments.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [30.237756729125977, -8.350232124328613]}, {"key": "zhao2023recommender", "year": "2023", "title": "Recommender Systems In The Era Of Large Language Models (llms)", "abstract": "<p>With the prosperity of e-commerce and web applications Recommender Systems (RecSys) have become an important component of our daily life providing personalized suggestions that cater to user preferences. While Deep Neural Networks (DNNs) have made significant advancements in enhancing recommender systems by modeling user-item interactions and incorporating textual side information DNN-based methods still face limitations such as difficulties in understanding users interests and capturing textual side information inabilities in generalizing to various recommendation scenarios and reasoning on their predictions etc. Meanwhile the emergence of Large Language Models (LLMs) such as ChatGPT and GPT4 has revolutionized the fields of Natural Language Processing (NLP) and Artificial Intelligence (AI) due to their remarkable abilities in fundamental responsibilities of language understanding and generation as well as impressive generalization and reasoning capabilities. As a result recent studies have attempted to harness the power of LLMs to enhance recommender systems. Given the rapid evolution of this research direction in recommender systems there is a pressing need for a systematic overview that summarizes existing LLM-empowered recommender systems to provide researchers in relevant fields with an in-depth understanding. Therefore in this paper we conduct a comprehensive review of LLM-empowered recommender systems from various aspects including Pre-training Fine-tuning and Prompting. More specifically we first introduce representative methods to harness the power of LLMs (as a feature encoder) for learning representations of users and items. Then we review recent techniques of LLMs for enhancing recommender systems from three paradigms namely pre-training fine-tuning and prompting. Finally we comprehensively discuss future directions in this emerging field.</p>\n", "tags": ["ARXIV", "Supervised", "Survey Paper"], "tsne_embedding": [39.574737548828125, -12.435154914855957]}, {"key": "zhao2024llm", "year": "2024", "title": "Llm-based Federated Recommendation", "abstract": "<p>Large Language Models (LLMs) with their advanced contextual understanding abilities have demonstrated considerable potential in enhancing recommendation systems via fine-tuning methods. However fine-tuning requires users behavior data which poses considerable privacy risks due to the incorporation of sensitive user information. The unintended disclosure of such data could infringe upon data protection laws and give rise to ethical issues. To mitigate these privacy issues Federated Learning for Recommendation (Fed4Rec) has emerged as a promising approach. Nevertheless applying Fed4Rec to LLM-based recommendation presents two main challenges first an increase in the imbalance of performance across clients affecting the systems efficiency over time and second a high demand on clients computational and storage resources for local training and inference of LLMs. To address these challenges we introduce a Privacy-Preserving LLM-based Recommendation (PPLR) framework. The PPLR framework employs two primary strategies. First it implements a dynamic balance strategy which involves the design of dynamic parameter aggregation and adjustment of learning speed for different clients during the training phase to ensure relatively balanced performance across all clients. Second PPLR adopts a flexible storage strategy selectively retaining certain sensitive layers of the language model on the client side while offloading non-sensitive layers to the server. This approach aims to preserve user privacy while efficiently saving computational and storage resources. Experimental results demonstrate that PPLR not only achieves a balanced performance among clients but also enhances overall system performance in a manner that is both computationally and storage-efficient while effectively protecting user privacy.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [46.30531311035156, -12.83917236328125]}, {"key": "zhe2018deep", "year": "2018", "title": "Deep Class-wise Hashing Semantics-preserving Hashing Via Class-wise Loss", "abstract": "<p>Deep supervised hashing has emerged as an influential solution to large-scale semantic image retrieval problems in computer vision. In the light of recent progress convolutional neural network based hashing methods typically seek pair-wise or triplet labels to conduct the similarity preserving learning. However complex semantic concepts of visual contents are hard to capture by similar/dissimilar labels which limits the retrieval performance. Generally pair-wise or triplet losses not only suffer from expensive training costs but also lack in extracting sufficient semantic information. In this regard we propose a novel deep supervised hashing model to learn more compact class-level similarity preserving binary codes. Our deep learning based model is motivated by deep metric learning that directly takes semantic labels as supervised information in training and generates corresponding discriminant hashing code. Specifically a novel cubic constraint loss function based on Gaussian distribution is proposed which preserves semantic variations while penalizes the overlap part of different classes in the embedding space. To address the discrete optimization problem introduced by binary codes a two-step optimization strategy is proposed to provide efficient training and avoid the problem of gradient vanishing. Extensive experiments on four large-scale benchmark databases show that our model can achieve the state-of-the-art retrieval performance. Moreover when training samples are limited our method surpasses other supervised deep hashing methods with non-negligible margins.</p>\n", "tags": ["ARXIV", "Deep Learning", "Image Retrieval", "Supervised"], "tsne_embedding": [0.8951824903488159, 2.5572803020477295]}, {"key": "zhen2012co", "year": "2012", "title": "Co-regularized Hashing For Multimodal Data", "abstract": "<p>Hashing-based methods provide a very promising approach to large-scale similarity search. To obtain compact hash codes a recent trend seeks to learn the hash functions from data automatically. In this paper we study hash function learning in the context of multimodal data. We propose a novel multimodal hash function learning method called Co-Regularized Hashing (CRH) based on a boosted co-regularization framework. The hash functions for each bit of the hash codes are learned by solving DC (difference of convex functions) programs while the learning for multiple bits proceeds via a boosting procedure so that the bias introduced by the hash functions can be sequentially minimized. We empirically compare CRH with two state-of-the-art multimodal hash function learning methods on two publicly available data sets.</p>\n", "tags": ["Cross Modal", "Dataset", "Independent", "NEURIPS"], "tsne_embedding": [-11.275223731994629, -13.127567291259766]}, {"key": "zhen2012coregularised", "year": "2012", "title": "Co-Regularized Hashing for Multimodal Data", "abstract": "<p>Hashing-based methods provide a very promising approach to large-scale similarity\nsearch. To obtain compact hash codes, a recent trend seeks to learn the hash\nfunctions from data automatically. In this paper, we study hash function learning\nin the context of multimodal data. We propose a novel multimodal hash function\nlearning method, called Co-Regularized Hashing (CRH), based on a boosted coregularization\nframework. The hash functions for each bit of the hash codes are\nlearned by solving DC (difference of convex functions) programs, while the learning\nfor multiple bits proceeds via a boosting procedure so that the bias introduced\nby the hash functions can be sequentially minimized. We empirically compare\nCRH with two state-of-the-art multimodal hash function learning methods on two\npublicly available data sets.</p>\n", "tags": ["Cross Modal", "NEURIPS"], "tsne_embedding": [-11.280973434448242, -13.136953353881836]}, {"key": "zhen2015cross", "year": "2015", "title": "Cross-Modal Similarity Learning via Pairs, Preferences, and Active Supervision", "abstract": "<p>We present a probabilistic framework for learning pairwise similarities between objects belonging to different modalities, such as drugs and proteins, or text and\nimages. Our framework is based on learning a binary\ncode based representation for objects in each modality, and has the following key properties: (i) it can\nleverage both pairwise as well as easy-to-obtain relative\npreference based cross-modal constraints, (ii) the probabilistic framework naturally allows querying for the\nmost useful/informative constraints, facilitating an active learning setting (existing methods for cross-modal\nsimilarity learning do not have such a mechanism), and\n(iii) the binary code length is learned from the data. We\ndemonstrate the effectiveness of the proposed approach\non two problems that require computing pairwise similarities between cross-modal object pairs: cross-modal\nlink prediction in bipartite graphs, and hashing based\ncross-modal similarity search.</p>\n", "tags": ["AAAI", "Cross Modal"], "tsne_embedding": [-1.3876503705978394, 20.827592849731445]}, {"key": "zheng2020generative", "year": "2020", "title": "Generative Semantic Hashing Enhanced Via Boltzmann Machines", "abstract": "<p>Generative semantic hashing is a promising technique for large-scale information retrieval thanks to its fast retrieval speed and small memory footprint. For the tractability of training existing generative-hashing methods mostly assume a factorized form for the posterior distribution enforcing independence among the bits of hash codes. From the perspectives of both model representation and code space size independence is always not the best assumption. In this paper to introduce correlations among the bits of hash codes we propose to employ the distribution of Boltzmann machine as the variational posterior. To address the intractability issue of training we first develop an approximate method to reparameterize the distribution of a Boltzmann machine by augmenting it as a hierarchical concatenation of a Gaussian-like distribution and a Bernoulli distribution. Based on that an asymptotically-exact lower bound is further derived for the evidence lower bound (ELBO). With these novel techniques the entire model can be optimized efficiently. Extensive experimental results demonstrate that by effectively modeling correlations among different bits within a hash code our model can achieve significant performance gains.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [0.1986715942621231, -8.284455299377441]}, {"key": "zheng2023codegeex", "year": "2023", "title": "Codegeex A Pre-trained Model For Code Generation With Multilingual Benchmarking On Humaneval-x", "abstract": "<p>Large pre-trained code generation models such as OpenAI Codex can generate syntax- and function-correct code making the coding of programmers more productive and our pursuit of artificial general intelligence closer. In this paper we introduce CodeGeeX a multilingual model with 13 billion parameters for code generation. CodeGeeX is pre-trained on 850 billion tokens of 23 programming languages as of June 2022. Our extensive experiments suggest that CodeGeeX outperforms multilingual code models of similar scale for both the tasks of code generation and translation on HumanEval-X. Building upon HumanEval (Python only) we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++ Java JavaScript and Go. In addition we build CodeGeeX-based extensions on Visual Studio Code JetBrains and Cloud Studio generating 4.7 billion tokens for tens of thousands of active users per week. Our user study demonstrates that CodeGeeX can help to increase coding efficiency for 83.437; of its users. Finally CodeGeeX is publicly accessible and in Sep. 2022 we open-sourced its code model weights (the version of 850B tokens) API extensions and HumanEval-X at https://github.com/THUDM/CodeGeeX.</p>\n", "tags": ["ARXIV", "Has Code"], "tsne_embedding": [25.948667526245117, -12.75285816192627]}, {"key": "zheng2023ddcot", "year": "2023", "title": "Ddcot Duty-distinct Chain-of-thought Prompting For Multimodal Reasoning In Language Models", "abstract": "<p>A long-standing goal of AI systems is to perform complex multimodal reasoning like humans. Recently large language models (LLMs) have made remarkable strides in such multi-step reasoning on the language modality solely by leveraging the chain of thought (CoT) to mimic human thinking. However the transfer of these advancements to multimodal contexts introduces heightened challenges including but not limited to the impractical need for labor-intensive annotation and the limitations in terms of flexibility generalizability and explainability. To evoke CoT reasoning in multimodality this work first conducts an in-depth analysis of these challenges posed by multimodality and presents two key insights keeping critical thinking and letting everyone do their jobs in multimodal CoT reasoning. Furthermore this study proposes a novel DDCoT prompting that maintains a critical attitude through negative-space prompting and incorporates multimodality into reasoning by first dividing the reasoning responsibility of LLMs into reasoning and recognition and then integrating the visual recognition capability of visual models into the joint reasoning process. The rationales generated by DDCoT not only improve the reasoning abilities of both large and small language models in zero-shot prompting and fine-tuning learning significantly outperforming state-of-the-art methods but also exhibit impressive generalizability and explainability.</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [38.48031997680664, -3.4048702716827393]}, {"key": "zheng2023why", "year": "2023", "title": "Why Does Chatgpt Fall Short In Providing Truthful Answers", "abstract": "<p>Recent advancements in large language models such as ChatGPT have demonstrated significant potential to impact various aspects of human life. However ChatGPT still faces challenges in providing reliable and accurate answers to user questions. To better understand the models particular weaknesses in providing truthful answers we embark an in-depth exploration of open-domain question answering. Specifically we undertake a detailed examination of ChatGPTs failures categorized into comprehension factuality specificity and inference. We further pinpoint factuality as the most contributing failure and identify two critical abilities associated with factuality knowledge memorization and knowledge recall. Through experiments focusing on factuality we propose several potential enhancement strategies. Our findings suggest that augmenting the model with granular external knowledge and cues for knowledge recall can enhance the models factuality in answering questions.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [37.85960006713867, -6.737521171569824]}, {"key": "zhong2015deep", "year": "2015", "title": "A Deep Hashing Learning Network", "abstract": "<p>Hashing-based methods seek compact and efficient binary codes that preserve the neighborhood structure in the original data space. For most existing hashing methods an image is first encoded as a vector of hand-crafted visual feature followed by a hash projection and quantization step to get the compact binary vector. Most of the hand-crafted features just encode the low-level information of the input the feature may not preserve the semantic similarities of images pairs. Meanwhile the hashing function learning process is independent with the feature representation so the feature may not be optimal for the hashing projection. In this paper we propose a supervised hashing method based on a well designed deep convolutional neural network which tries to learn hashing code and compact representations of data simultaneously. The proposed model learn the binary codes by adding a compact sigmoid layer before the loss layer. Experiments on several image data sets show that the proposed model outperforms other state-of-the-art methods.</p>\n", "tags": ["ARXIV", "Quantisation", "Supervised"], "tsne_embedding": [-1.8189383745193481, 7.56020975112915]}, {"key": "zhong2015efficient", "year": "2015", "title": "Efficient Similarity Indexing And Searching In High Dimensions", "abstract": "<p>Efficient indexing and searching of high dimensional data has been an area of active research due to the growing exploitation of high dimensional data and the vulnerability of traditional search methods to the curse of dimensionality. This paper presents a new approach for fast and effective searching and indexing of high dimensional features using random partitions of the feature space. Experiments on both handwritten digits and 3-D shape descriptors have shown the proposed algorithm to be highly effective and efficient in indexing and searching real data sets of several hundred dimensions. We also compare its performance to that of the state-of-the-art locality sensitive hashing algorithm.</p>\n", "tags": ["ARXIV", "Independent"], "tsne_embedding": [-19.18485450744629, 24.66079330444336]}, {"key": "zhong2020compact", "year": "2020", "title": "Compact Deep Aggregation For Set Retrieval", "abstract": "<p>The objective of this work is to learn a compact embedding of a set of descriptors that is suitable for efficient retrieval and ranking whilst maintaining discriminability of the individual descriptors. We focus on a specific example of this general problem \u2013 that of retrieving images containing multiple faces from a large scale dataset of images. Here the set consists of the face descriptors in each image and given a query for multiple identities the goal is then to retrieve in order images which contain all the identities all but one (etc) To this end we make the following contributions first we propose a CNN architecture \u2013 (em) SetNet \u2013 to achieve the objective it learns face descriptors and their aggregation over a set to produce a compact fixed length descriptor designed for set retrieval and the score of an image is a count of the number of identities that match the query; second we show that this compact descriptor has minimal loss of discriminability up to two faces per image and degrades slowly after that \u2013 far exceeding a number of baselines; third we explore the speed vs. retrieval quality trade-off for set retrieval using this compact descriptor; and finally we collect and annotate a large dataset of images containing various number of celebrities which we use for evaluation and is publicly released.</p>\n", "tags": ["ARXIV", "CNN", "Text Retrieval"], "tsne_embedding": [0.6154800653457642, 16.39389419555664]}, {"key": "zhong2023memorybank", "year": "2023", "title": "Memorybank Enhancing Large Language Models With Long-term Memory", "abstract": "<p>Revolutionary advancements in Large Language Models have drastically reshaped our interactions with artificial intelligence systems. Despite this a notable hindrance remains-the deficiency of a long-term memory mechanism within these models. This shortfall becomes increasingly evident in situations demanding sustained interaction such as personal companion systems and psychological counseling. Therefore we propose MemoryBank a novel memory mechanism tailored for LLMs. MemoryBank enables the models to summon relevant memories continually evolve through continuous memory updates comprehend and adapt to a user personality by synthesizing information from past interactions. To mimic anthropomorphic behaviors and selectively preserve memory MemoryBank incorporates a memory updating mechanism inspired by the Ebbinghaus Forgetting Curve theory which permits the AI to forget and reinforce memory based on time elapsed and the relative significance of the memory thereby offering a human-like memory mechanism. MemoryBank is versatile in accommodating both closed-source models like ChatGPT and open-source models like ChatGLM. We exemplify application of MemoryBank through the creation of an LLM-based chatbot named SiliconFriend in a long-term AI Companion scenario. Further tuned with psychological dialogs SiliconFriend displays heightened empathy in its interactions. Experiment involves both qualitative analysis with real-world user dialogs and quantitative analysis with simulated dialogs. In the latter ChatGPT acts as users with diverse characteristics and generates long-term dialog contexts covering a wide array of topics. The results of our analysis reveal that SiliconFriend equipped with MemoryBank exhibits a strong capability for long-term companionship as it can provide emphatic response recall relevant memories and understand user personality.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [10.205707550048828, -12.75732707977295]}, {"key": "zhornyak2022hashencoding", "year": "2022", "title": "Hashencoding Autoencoding With Multiscale Coordinate Hashing", "abstract": "<p>We present HashEncoding a novel autoencoding architecture that leverages a non-parametric multiscale coordinate hash function to facilitate a per-pixel decoder without convolutions. By leveraging the space-folding behaviour of hashing functions HashEncoding allows for an inherently multiscale embedding space that remains much smaller than the original image. As a result the decoder requires very few parameters compared with decoders in traditional autoencoders approaching a non-parametric reconstruction of the original image and allowing for greater generalizability. Finally by allowing backpropagation directly to the coordinate space we show that HashEncoding can be exploited for geometric tasks such as optical flow.</p>\n", "tags": ["ARXIV", "Unsupervised"], "tsne_embedding": [-21.347475051879883, 9.371349334716797]}, {"key": "zhou2016transfer", "year": "2016", "title": "Transfer Hashing With Privileged Information", "abstract": "<p>Most existing learning to hash methods assume that there are sufficient data either labeled or unlabeled on the domain of interest (i.e. the target domain) for training. However this assumption cannot be satisfied in some real-world applications. To address this data sparsity issue in hashing inspired by transfer learning we propose a new framework named Transfer Hashing with Privileged Information (THPI). Specifically we extend the standard learning to hash method Iterative Quantization (ITQ) in a transfer learning manner namely ITQ+. In ITQ+ a new slack function is learned from auxiliary data to approximate the quantization error in ITQ. We developed an alternating optimization approach to solve the resultant optimization problem for ITQ+. We further extend ITQ+ to LapITQ+ by utilizing the geometry structure among the auxiliary data for learning more precise binary codes in the target domain. Extensive experiments on several benchmark datasets verify the effectiveness of our proposed approaches through comparisons with several state-of-the-art baselines.</p>\n", "tags": ["ARXIV", "Quantisation"], "tsne_embedding": [-0.2945711612701416, -0.9634293913841248]}, {"key": "zhou2017deep", "year": "2017", "title": "Deep Hashing With Triplet Quantization Loss", "abstract": "<p>With the explosive growth of image databases deep hashing which learns compact binary descriptors for images has become critical for fast image retrieval. Many existing deep hashing methods leverage quantization loss defined as distance between the features before and after quantization to reduce the error from binarizing features. While minimizing the quantization loss guarantees that quantization has minimal effect on retrieval accuracy it unfortunately significantly reduces the expressiveness of features even before the quantization. In this paper we show that the above definition of quantization loss is too restricted and in fact not necessary for maintaining high retrieval accuracy. We therefore propose a new form of quantization loss measured in triplets. The core idea of the triplet quantization loss is to learn discriminative real-valued descriptors which lead to minimal loss on retrieval accuracy after quantization. Extensive experiments on two widely used benchmark data sets of different scales CIFAR-10 and In-shop demonstrate that the proposed method outperforms the state-of-the-art deep hashing methods. Moreover we show that the compact binary descriptors obtained with triplet quantization loss lead to very small performance drop after quantization.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Quantisation"], "tsne_embedding": [-16.96198081970215, 11.268451690673828]}, {"key": "zhou2022least", "year": "2022", "title": "Least-to-most Prompting Enables Complex Reasoning In Large Language Models", "abstract": "<p>Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization we propose a novel prompting strategy least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation compositional generalization and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 9937; using just 14 exemplars compared to only 1637; accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15000 examples. We have included prompts for all the tasks in the Appendix.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [28.598299026489258, 2.1889851093292236]}, {"key": "zhou2023analyzing", "year": "2023", "title": "Analyzing And Mitigating Object Hallucination In Large Vision-language Models", "abstract": "<p>Large vision-language models (LVLMs) have shown remarkable abilities in understanding visual information with human languages. However LVLMs still suffer from object hallucination which is the problem of generating descriptions that include objects that do not actually exist in the images. This can negatively impact many vision-language tasks such as visual summarization and reasoning. To address this issue we propose a simple yet powerful algorithm LVLM Hallucination Revisor (LURE) to post-hoc rectify object hallucination in LVLMs by reconstructing less hallucinatory descriptions. LURE is grounded in a rigorous statistical analysis of the key factors underlying object hallucination including co-occurrence (the frequent appearance of certain objects alongside others in images) uncertainty (objects with higher uncertainty during LVLM decoding) and object position (hallucination often appears in the later part of the generated text). LURE can also be seamlessly integrated with any LVLMs. We evaluate LURE on six open-source LVLMs achieving a 2337; improvement in general object hallucination evaluation metrics over the previous best approach. In both GPT and human evaluations LURE consistently ranks at the top. Our data and code are available at https://github.com/YiyangZhou/LURE.</p>\n", "tags": ["ARXIV", "Cross Modal", "Has Code"], "tsne_embedding": [51.30080032348633, 6.427267074584961]}, {"key": "zhou2023lima", "year": "2023", "title": "LIMA Less Is More For Alignment", "abstract": "<p>Large language models are trained in two stages (1) unsupervised pretraining from raw text to learn general-purpose representations and (2) large scale instruction tuning and reinforcement learning to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1000 carefully curated prompts and responses without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance learning to follow specific response formats from only a handful of examples in the training data including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study responses from LIMA are either equivalent or strictly preferred to GPT-4 in 4337; of cases; this statistic is as high as 5837; when compared to Bard and 6537; versus DaVinci003 which was trained with human feedback. Taken together these results strongly suggest that almost all knowledge in large language models is learned during pretraining and only limited instruction tuning data is necessary to teach models to produce high quality output.</p>\n", "tags": ["ARXIV", "Unsupervised"], "tsne_embedding": [22.93146324157715, -2.1757915019989014]}, {"key": "zhu2013linear", "year": "2013", "title": "Linear cross-modal hashing for efficient multimedia search", "abstract": "<p>Most existing cross-modal hashing methods suffer from the scalability issue in the training phase. In this paper, we propose a novel \ncross-modal hashing approach with a linear time complexity to the training data size, to enable scalable indexing for multimedia \nsearch across multiple modals. Taking both the intra-similarity in each modal and the inter-similarity across different modals \ninto consideration, the proposed approach aims at effectively learning hash functions from large-scale training datasets. \nMore specifically, for each modal, we first partition the training data into $k$ clusters and then represent each training data \npoint with its distances to $k$ centroids of the clusters. Interestingly, such a k-dimensional data representation can reduce \nthe time complexity of the training phase from traditional O(n2) or higher to O(n), where $n$ is the training data size, leading to \npractical learning on large-scale datasets. We further prove that this new representation preserves the intra-similarity in each modal. \nTo preserve the inter-similarity among data points across different modals, we transform the derived data representations into a \ncommon binary subspace in which binary codes from all the modals are \u201cconsistent\u201d and comparable. The transformation simultaneously \noutputs the hash functions for all modals, which are used to convert unseen data into binary codes. Given a query of one modal, \nit is first mapped into the binary codes using the modal\u2019s hash functions, followed by matching the database binary codes of any other \nmodals. Experimental results on two benchmark datasets confirm the scalability and the effectiveness of the proposed approach in \ncomparison with the state of the art.</p>\n", "tags": ["Cross Modal", "MM"], "tsne_embedding": [-2.5552737712860107, -7.656593322753906]}, {"key": "zhu2016deep", "year": "2016", "title": "Deep Hashing Network for Efficient Similarity Retrieval", "abstract": "<p>Due to the storage and retrieval efficiency, hashing has been widely deployed to approximate nearest neighbor search for large-scale multimedia retrieval. Supervised hashing, which improves the quality of hash coding by exploiting the semantic similarity on data pairs, has received increasing attention recently. For most existing supervised hashing methods for image retrieval, an image is first represented as a vector of hand-crafted or machine-learned features, followed by another separate quantization step that generates binary codes.\nHowever, suboptimal hash coding may be produced, because the quantization error is not statistically minimized and the feature representation is not optimally compatible with the binary coding. In this paper, we propose a novel Deep Hashing Network (DHN) architecture for supervised hashing, in which we jointly learn good image representation tailored to hash coding and formally control the quantization error.\nThe DHN model constitutes four key components: (1) a sub-network with multiple convolution-pooling layers to capture image representations; (2) a fully-connected hashing layer to generate compact binary hash codes; (3) a pairwise cross-entropy loss layer for similarity-preserving learning; and (4) a pairwise quantization loss for controlling hashing quality. Extensive experiments on standard image retrieval datasets show the proposed DHN model yields substantial boosts over latest state-of-the-art hashing methods.</p>\n", "tags": ["AAAI", "Deep Learning", "Has Code", "Image Retrieval", "Quantisation", "Supervised"], "tsne_embedding": [-8.769329071044922, 8.354467391967773]}, {"key": "zhu2017discrete", "year": "2017", "title": "Discrete Multi-modal Hashing With Canonical Views For Robust Mobile Landmark Search", "abstract": "<p>Mobile landmark search (MLS) recently receives increasing attention for its great practical values. However it still remains unsolved due to two important challenges. One is high bandwidth consumption of query transmission and the other is the huge visual variations of query images sent from mobile devices. In this paper we propose a novel hashing scheme named as canonical view based discrete multi-modal hashing (CV-DMH) to handle these problems via a novel three-stage learning procedure. First a submodular function is designed to measure visual representativeness and redundancy of a view set. With it canonical views which capture key visual appearances of landmark with limited redundancy are efficiently discovered with an iterative mining strategy. Second multi-modal sparse coding is applied to transform visual features from multiple modalities into an intermediate representation. It can robustly and adaptively characterize visual contents of varied landmark images with certain canonical views. Finally compact binary codes are learned on intermediate representation within a tailored discrete binary embedding model which preserves visual relations of images measured with canonical views and removes the involved noises. In this part we develop a new augmented Lagrangian multiplier (ALM) based optimization method to directly solve the discrete binary codes. We can not only explicitly deal with the discrete constraint but also consider the bit-uncorrelated constraint and balance constraint together. Experiments on real world landmark datasets demonstrate the superior performance of CV-DMH over several state-of-the-art methods.</p>\n", "tags": ["ARXIV"], "tsne_embedding": [-4.670166969299316, 14.698373794555664]}, {"key": "zhu2017part", "year": "2017", "title": "Part-based Deep Hashing For Large-scale Person Re-identification", "abstract": "<p>Large-scale is a trend in person re-identification (re-id). It is important that real-time search be performed in a large gallery. While previous methods mostly focus on discriminative learning this paper makes the attempt in integrating deep learning and hashing into one framework to evaluate the efficiency and accuracy for large-scale person re-id. We integrate spatial information for discriminative visual representation by partitioning the pedestrian image into horizontal parts. Specifically Part-based Deep Hashing (PDH) is proposed in which batches of triplet samples are employed as the input of the deep hashing architecture. Each triplet sample contains two pedestrian images (or parts) with the same identity and one pedestrian image (or part) of the different identity. A triplet loss function is employed with a constraint that the Hamming distance of pedestrian images (or parts) with the same identity is smaller than ones with the different identity. In the experiment we show that the proposed Part-based Deep Hashing method yields very competitive re-id accuracy on the large-scale Market-1501 and Market-1501+500K datasets.</p>\n", "tags": ["ARXIV", "Deep Learning"], "tsne_embedding": [-18.230873107910156, 8.437119483947754]}, {"key": "zhu2019exploring", "year": "2019", "title": "Exploring Auxiliary Context Discrete Semantic Transfer Hashing For Scalable Image Retrieval", "abstract": "<p>Unsupervised hashing can desirably support scalable content-based image retrieval (SCBIR) for its appealing advantages of semantic label independence memory and search efficiency. However the learned hash codes are embedded with limited discriminative semantics due to the intrinsic limitation of image representation. To address the problem in this paper we propose a novel hashing approach dubbed as emphDiscrete Semantic Transfer Hashing (DSTH). The key idea is to emphdirectly augment the semantics of discrete image hash codes by exploring auxiliary contextual modalities. To this end a unified hashing framework is formulated to simultaneously preserve visual similarities of images and perform semantic transfer from contextual modalities. Further to guarantee direct semantic transfer and avoid information loss we explicitly impose the discrete constraint bit\u2013uncorrelation constraint and bit-balance constraint on hash codes. A novel and effective discrete optimization method based on augmented Lagrangian multiplier is developed to iteratively solve the optimization problem. The whole learning process has linear computation complexity and desirable scalability. Experiments on three benchmark datasets demonstrate the superiority of DSTH compared with several state-of-the-art approaches.</p>\n", "tags": ["ARXIV", "Image Retrieval", "Unsupervised"], "tsne_embedding": [-6.307614803314209, 13.124613761901855]}, {"key": "zhu2020dual", "year": "2020", "title": "Dual-level Semantic Transfer Deep Hashing For Efficient Social Image Retrieval", "abstract": "<p>Social network stores and disseminates a tremendous amount of user shared images. Deep hashing is an efficient indexing technique to support large-scale social image retrieval due to its deep representation capability fast retrieval speed and low storage cost. Particularly unsupervised deep hashing has well scalability as it does not require any manually labelled data for training. However owing to the lacking of label guidance existing methods suffer from severe semantic shortage when optimizing a large amount of deep neural network parameters. Differently in this paper we propose a Dual-level Semantic Transfer Deep Hashing (DSTDH) method to alleviate this problem with a unified deep hash learning framework. Our model targets at learning the semantically enhanced deep hash codes by specially exploiting the user-generated tags associated with the social images. Specifically we design a complementary dual-level semantic transfer mechanism to efficiently discover the potential semantics of tags and seamlessly transfer them into binary hash codes. On the one hand instance-level semantics are directly preserved into hash codes from the associated tags with adverse noise removing. Besides an image-concept hypergraph is constructed for indirectly transferring the latent high-order semantic correlations of images and tags into hash codes. Moreover the hash codes are obtained simultaneously with the deep representation learning by the discrete hash optimization strategy. Extensive experiments on two public social image retrieval datasets validate the superior performance of our method compared with state-of-the-art hashing methods. The source codes of our method can be obtained at https://github.com/research2020-1/DSTDH\u201d</p>\n", "tags": ["ARXIV", "Graph", "Has Code", "Image Retrieval", "Unsupervised"], "tsne_embedding": [1.476250410079956, 5.156923770904541]}, {"key": "zhu2022lower", "year": "2022", "title": "A Lower Bound Of Hash Codes Performance", "abstract": "<p>As a crucial approach for compact representation learning hashing has achieved great success in effectiveness and efficiency. Numerous heuristic Hamming space metric learning objectives are designed to obtain high-quality hash codes. Nevertheless a theoretical analysis of criteria for learning good hash codes remains largely unexploited. In this paper we prove that inter-class distinctiveness and intra-class compactness among hash codes determine the lower bound of hash codes performance. Promoting these two characteristics could lift the bound and improve hash learning. We then propose a surrogate model to fully exploit the above objective by estimating the posterior of hash codes and controlling it which results in a low-bias optimization. Extensive experiments reveal the effectiveness of the proposed method. By testing on a series of hash-models we obtain performance improvements among all of them with an up to (26.5) increase in mean Average Precision and an up to (20.5) increase in accuracy. Our code is publicly available at https://github.com/VL-Group/LBHash.</p>\n", "tags": ["Has Code", "Independent", "NEURIPS"], "tsne_embedding": [-9.084815979003906, -12.38037109375]}, {"key": "zhu2023central", "year": "2023", "title": "Central Similarity Multi-view Hashing For Multimedia Retrieval", "abstract": "<p>Hash representation learning of multi-view heterogeneous data is the key to improving the accuracy of multimedia retrieval. However existing methods utilize local similarity and fall short of deeply fusing the multi-view features resulting in poor retrieval accuracy. Current methods only use local similarity to train their model. These methods ignore global similarity. Furthermore most recent works fuse the multi-view features via a weighted sum or concatenation. We contend that these fusion methods are insufficient for capturing the interaction between various views. We present a novel Central Similarity Multi-View Hashing (CSMVH) method to address the mentioned problems. Central similarity learning is used for solving the local similarity problem which can utilize the global similarity between the hash center and samples. We present copious empirical data demonstrating the superiority of gate-based fusion over conventional approaches. On the MS COCO and NUS-WIDE the proposed CSMVH performs better than the state-of-the-art methods by a large margin (up to 11.4137; mean Average Precision (mAP) improvement).</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [-9.72905158996582, -6.334815979003906]}, {"key": "zhu2023chatgpt", "year": "2023", "title": "Chatgpt Asks BLIP-2 Answers Automatic Questioning Towards Enriched Visual Descriptions", "abstract": "<p>Asking insightful questions is crucial for acquiring knowledge and expanding our understanding of the world. However the importance of questioning has been largely overlooked in AI research where models have been primarily developed to answer questions. With the recent advancements of large language models (LLMs) like ChatGPT we discover their capability to ask high-quality questions when provided with a suitable prompt. This discovery presents a new opportunity to develop an automatic questioning system. In this paper we introduce ChatCaptioner a novel automatic-questioning method deployed in image captioning. Here ChatGPT is prompted to ask a series of informative questions about images to BLIP-2 a strong vision question-answering model. By keeping acquiring new visual information from BLIP-2s answers ChatCaptioner is able to generate more enriched image descriptions. We conduct human-subject evaluations on common image caption datasets such as COCO Conceptual Caption and WikiArt and compare ChatCaptioner with BLIP-2 as well as ground truth. Our results demonstrate that ChatCaptioners captions are significantly more informative receiving three times as many votes from human evaluators for providing the most image information. Besides ChatCaptioner identifies 5337; more objects within the image than BLIP-2 alone measured by WordNet synset matching. Code is available at https://github.com/Vision-CAIR/ChatCaptioner</p>\n", "tags": ["ARXIV", "Has Code", "Supervised"], "tsne_embedding": [40.8193473815918, 6.4820075035095215]}, {"key": "zhu2023clip", "year": "2023", "title": "CLIP Multi-modal Hashing A New Baseline CLIPMH", "abstract": "<p>The multi-modal hashing method is widely used in multimedia retrieval. It can fuse multi-source data to generate binary hash code. However the current multi-modal methods have the problem of low retrieval accuracy. The reason is that the individual backbone networks have limited feature expression capabilities and are not jointly pre-trained on large-scale unsupervised multi-modal data. To solve this problem we propose a new baseline CLIP Multi-modal Hashing (CLIPMH) method. It uses CLIP model to extract text and image features and then fuse to generate hash code. CLIP improves the expressiveness of each modal feature. In this way it can greatly improve the retrieval performance of multi-modal hashing methods. In comparison to state-of-the-art unsupervised and supervised multi-modal hashing methods experiments reveal that the proposed CLIPMH can significantly enhance performance (Maximum increase of 8.3837;). CLIP also has great advantages over the text and visual backbone networks commonly used before.</p>\n", "tags": ["ARXIV", "Unsupervised"], "tsne_embedding": [-8.734720230102539, -3.3582916259765625]}, {"key": "zhu2023collaborative", "year": "2023", "title": "Collaborative Large Language Model For Recommender Systems", "abstract": "<p>Recently there has been growing interest in developing the next-generation recommender systems (RSs) based on pretrained large language models (LLMs). However the semantic gap between natural language and recommendation tasks is still not well addressed leading to multiple issues such as spuriously correlated user/item descriptors ineffective language modeling on user/item data inefficient recommendations via auto-regression etc. In this paper we propose CLLM4Rec the first generative RS that tightly integrates the LLM paradigm and ID paradigm of RSs aiming to address the above challenges simultaneously. We first extend the vocabulary of pretrained LLMs with user/item ID tokens to faithfully model user/item collaborative and content semantics. Accordingly a novel soft+hard prompting strategy is proposed to effectively learn user/item collaborative/content token embeddings via language modeling on RS-specific corpora where each document is split into a prompt consisting of heterogeneous soft (user/item) tokens and hard (vocab) tokens and a main text consisting of homogeneous item tokens or vocab tokens to facilitate stable and effective language modeling. In addition a novel mutual regularization strategy is introduced to encourage CLLM4Rec to capture recommendation-related information from noisy user/item content. Finally we propose a novel recommendation-oriented finetuning strategy for CLLM4Rec where an item prediction head with multinomial likelihood is added to the pretrained CLLM4Rec backbone to predict hold-out items based on soft+hard prompts established from masked user-item interaction history where recommendations of multiple items can be generated efficiently without hallucination. Codes are released at https://github.com/yaochenzhu/llm4rec.</p>\n", "tags": ["ARXIV", "Cross Modal", "Has Code", "Supervised"], "tsne_embedding": [41.373497009277344, -7.931739807128906]}, {"key": "zhu2023deep", "year": "2023", "title": "Deep Metric Multi-view Hashing For Multimedia Retrieval", "abstract": "<p>Learning the hash representation of multi-view heterogeneous data is an important task in multimedia retrieval. However existing methods fail to effectively fuse the multi-view features and utilize the metric information provided by the dissimilar samples leading to limited retrieval precision. Current methods utilize weighted sum or concatenation to fuse the multi-view features. We argue that these fusion methods cannot capture the interaction among different views. Furthermore these methods ignored the information provided by the dissimilar samples. We propose a novel deep metric multi-view hashing (DMMVH) method to address the mentioned problems. Extensive empirical evidence is presented to show that gate-based fusion is better than typical methods. We introduce deep metric learning to the multi-view hashing problems which can utilize metric information of dissimilar samples. On the MIR-Flickr25K MS COCO and NUS-WIDE our method outperforms the current state-of-the-art methods by a large margin (up to 15.28 mean Average Precision (mAP) improvement).</p>\n", "tags": ["ARXIV", "Cross Modal"], "tsne_embedding": [-9.575963020324707, -6.455659866333008]}, {"key": "zhu2023minigpt", "year": "2023", "title": "Minigpt-4 Enhancing Vision-language Understanding With Advanced Large Language Models", "abstract": "<p>The recent GPT-4 has demonstrated extraordinary multi-modal abilities such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However the technical details behind GPT-4 continue to remain undisclosed. We believe that the enhanced multi-modal generation capabilities of GPT-4 stem from the utilization of sophisticated large language models (LLM). To examine this phenomenon we present MiniGPT-4 which aligns a frozen visual encoder with a frozen advanced LLM Vicuna using one projection layer. Our work for the first time uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4 such as detailed image description generation and website creation from hand-drawn drafts. Furthermore we also observe other emerging capabilities in MiniGPT-4 including writing stories and poems inspired by given images teaching users how to cook based on food photos and so on. In our experiment we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g. repetition and fragmentation). To address this problem we curate a detailed image description dataset in the second stage to finetune the model which consequently improves the models generation reliability and overall usability. Our code pre-trained model and collected dataset are available at https://minigpt-4.github.io/.</p>\n", "tags": ["ARXIV", "Cross Modal", "Has Code", "Independent"], "tsne_embedding": [35.07247543334961, -6.865583419799805]}, {"key": "zhu2024ibd", "year": "2024", "title": "IBD Alleviating Hallucinations In Large Vision-language Models Via Image-biased Decoding", "abstract": "<p>Despite achieving rapid developments and with widespread applications Large Vision-Language Models (LVLMs) confront a serious challenge of being prone to generating hallucinations. An over-reliance on linguistic priors has been identified as a key factor leading to these hallucinations. In this paper we propose to alleviate this problem by introducing a novel image-biased decoding (IBD) technique. Our method derives the next-token probability distribution by contrasting predictions from a conventional LVLM with those of an image-biased LVLM thereby amplifying the correct information highly correlated with image content while mitigating the hallucinatory errors caused by excessive dependence on text. We further conduct a comprehensive statistical analysis to validate the reliability of our method and design an adaptive adjustment strategy to achieve robust and flexible handling under varying conditions. Experimental results across multiple evaluation metrics verify that our method despite not requiring additional training data and only with a minimal increase in model parameters can significantly reduce hallucinations in LVLMs and enhance the truthfulness of the generated response.</p>\n", "tags": ["ARXIV", "Cross Modal", "Supervised"], "tsne_embedding": [49.066139221191406, 5.418230056762695]}, {"key": "zhuge2023mindstorms", "year": "2023", "title": "Mindstorms In Natural Language-based Societies Of Mind", "abstract": "<p>Both Minskys society of mind and Schmidhubers learning to think inspire diverse societies of large multimodal neural networks (NNs) that solve problems by interviewing each other in a mindstorm. Recent implementations of NN-based societies of minds consist of large language models (LLMs) and other NN-based experts communicating through a natural language interface. In doing so they overcome the limitations of single LLMs improving multimodal zero-shot reasoning. In these natural language-based societies of mind (NLSOMs) new agents \u2013 all communicating through the same universal symbolic language \u2013 are easily added in a modular fashion. To demonstrate the power of NLSOMs we assemble and experiment with several of them (having up to 129 members) leveraging mindstorms in them to solve some practical AI tasks visual question answering image captioning text-to-image synthesis 3D generation egocentric retrieval embodied AI and general language-based task solving. We view this as a starting point towards much larger NLSOMs with billions of agents-some of which may be humans. And with this emergence of great societies of heterogeneous minds many new research questions have suddenly become paramount to the future of artificial intelligence. What should be the social structure of an NLSOM What would be the (dis)advantages of having a monarchical rather than a democratic structure How can principles of NN economies be used to maximize the total reward of a reinforcement learning NLSOM In this work we identify discuss and try to answer some of these questions.</p>\n", "tags": ["ARXIV", "Cross Modal", "Supervised"], "tsne_embedding": [31.067729949951172, -13.159273147583008]}, {"key": "zieba2018bingan", "year": "2018", "title": "Bingan Learning Compact Binary Descriptors With A Regularized GAN", "abstract": "<p>In this paper we propose a novel regularization method for Generative Adversarial Networks that allows the model to learn discriminative yet compact binary representations of image patches (image descriptors). We exploit the dimensionality reduction that takes place in the intermediate layers of the discriminator network and train the binarized penultimate layers low-dimensional representation to mimic the distribution of the higher-dimensional preceding layers. To achieve this we introduce two loss terms that aim at (i) reducing the correlation between the dimensions of the binarized penultimate layers low-dimensional representation (i.e. maximizing joint entropy) and (ii) propagating the relations between the dimensions in the high-dimensional space to the low-dimensional space. We evaluate the resulting binary image descriptors on two challenging applications image matching and retrieval where they achieve state-of-the-art results.</p>\n", "tags": ["GAN", "NEURIPS", "Unsupervised"], "tsne_embedding": [-11.836069107055664, 15.79118537902832]}, {"key": "ziegler2012locally", "year": "2012", "title": "Locally Uniform Comparison Image Descriptor", "abstract": "<p>Keypoint matching between pairs of images using popular descriptors like SIFT or a faster variant called SURF is at the heart of many computer vision algorithms including recognition mosaicing and structure from motion. For real-time mobile applications very fast but less accurate descriptors like BRIEF and related methods use a random sampling of pairwise comparisons of pixel intensities in an image patch. Here we introduce Locally Uniform Comparison Image Descriptor (LUCID) a simple description method based on permutation distances between the ordering of intensities of RGB values between two patches. LUCID is computable in linear time with respect to patch size and does not require floating point computation. An analysis reveals an underlying issue that limits the potential of BRIEF and related approaches compared to LUCID. Experiments demonstrate that LUCID is faster than BRIEF and its accuracy is directly comparable to SURF while being more than an order of magnitude faster.</p>\n", "tags": ["Independent", "NEURIPS"], "tsne_embedding": [-23.090072631835938, 21.862638473510742]}, {"key": "zong2024safety", "year": "2024", "title": "Safety Fine-tuning At (almost) No Cost A Baseline For Vision Large Language Models", "abstract": "<p>Current vision large language models (VLLMs) exhibit remarkable capabilities yet are prone to generate harmful content and are vulnerable to even the simplest jailbreaking attacks. Our initial analysis finds that this is due to the presence of harmful data during vision-language instruction fine-tuning and that VLLM fine-tuning can cause forgetting of safety alignment previously learned by the underpinning LLM. To address this issue we first curate a vision-language safe instruction-following dataset VLGuard covering various harmful categories. Our experiments demonstrate that integrating this dataset into standard vision-language fine-tuning or utilizing it for post-hoc fine-tuning effectively safety aligns VLLMs. This alignment is achieved with minimal impact on or even enhancement of the models helpfulness. The versatility of our safety fine-tuning dataset makes it a valuable resource for safety-testing existing VLLMs training new models or safeguarding pre-trained VLLMs. Empirical results demonstrate that fine-tuned VLLMs effectively reject unsafe instructions and substantially reduce the success rates of several black-box adversarial attacks which approach zero in many cases. The code and dataset are available at https://github.com/ys-zong/VLGuard.</p>\n", "tags": ["ARXIV", "Cross Modal", "Has Code"], "tsne_embedding": [48.704524993896484, -0.8661203384399414]}, {"key": "zou2019transductive", "year": "2019", "title": "Transductive Zero-shot Hashing For Multilabel Image Retrieval", "abstract": "<p>Hash coding has been widely used in approximate nearest neighbor search for large-scale image retrieval. Given semantic annotations such as class labels and pairwise similarities of the training data hashing methods can learn and generate effective and compact binary codes. While some newly introduced images may contain undefined semantic labels which we call unseen images zeor-shot hashing techniques have been studied. However existing zeor-shot hashing methods focus on the retrieval of single-label images and cannot handle multi-label images. In this paper for the first time a novel transductive zero-shot hashing method is proposed for multi-label unseen image retrieval. In order to predict the labels of the unseen/target data a visual-semantic bridge is built via instance-concept coherence ranking on the seen/source data. Then pairwise similarity loss and focal quantization loss are constructed for training a hashing model using both the seen/source and unseen/target data. Extensive evaluations on three popular multi-label datasets demonstrate that the proposed hashing method achieves significantly better results than the competing methods.</p>\n", "tags": ["Image Retrieval", "Quantisation", "Supervised", "TNNLS"], "tsne_embedding": [-4.44771146774292, 6.45065450668335]}]