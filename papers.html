<!DOCTYPE html>
<html lang="en-us">

  <head>
<!-- Begin Web-Stat code v 7.0 -->
<span id="wts2185304"></span>
<script>var wts=document.createElement('script');wts.async=true;
wts.src='https://app.ardalio.com/log7.js';document.head.appendChild(wts);
wts.onload = function(){ wtslog7(2185304,4); };
</script><noscript><a href="https://www.web-stat.com">
<img src="https://app.ardalio.com/7/4/2185304.png" 
alt="Web-Stat web statistics"></a></noscript>
<!-- End Web-Stat code v 7.0 -->
  <!-- Hotjar Tracking Code for https://learning2hash.github.io/ -->
<script>
    (function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:1843243,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109544763-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-109544763-1');
</script>
<script>
    window.MathJax = {
      tex: {
        inlineMath: [["\\(","\\)"]],
        displayMath: [["\\[","\\]"]],
      },
      options: {
        processHtmlClass: "mathjax-content",
        processEscapes: true,
      }
    };
  </script>
  <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="keywords" content="machine learning, hashing, approximate nearest neighbour search, lsh, learning-to-hash">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Search all Publications on Machine Learning for Hashing | Awesome Learning to Hash</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Search all Publications on Machine Learning for Hashing" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A list of research papers for machine learning models for hashing." />
<meta property="og:description" content="A list of research papers for machine learning models for hashing." />
<link rel="canonical" href="https://learning2hash.github.io/papers.html" />
<meta property="og:url" content="https://learning2hash.github.io/papers.html" />
<meta property="og:site_name" content="Awesome Learning to Hash" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Search all Publications on Machine Learning for Hashing" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"A list of research papers for machine learning models for hashing.","headline":"Search all Publications on Machine Learning for Hashing","url":"https://learning2hash.github.io/papers.html"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link rel="search" href="/public/opensearchdescription.xml" 
      type="application/opensearchdescription+xml" 
      title="learning2hash" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
  integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
  crossorigin="anonymous"></script>
  
  <link rel="stylesheet" type="text/css" href="//cdn.datatables.net/1.10.16/css/jquery.dataTables.min.css">
  <script type="text/javascript" charset="utf8" src="//cdn.datatables.net/1.10.16/js/jquery.dataTables.min.js"></script>
</head>


  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <body class="theme-base-0c layout-reverse">

    <a href='/contributing.html' class='ribbon'>Add your paper to Learning2Hash</a>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Awesome Learning to Hash
        </a>
      </h1>
      <p class="lead">A Webpage dedicated to the latest research on Hash Function Learning. Maintained by <a href="http://sjmoran.github.io/">Sean Moran</a>.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="sidebar-item">
        <p style="font-size: 12px">
          Search related work 
          <input type='text' id='searchTarget' size="16"/> 
          <button onClick="search();">Go</button>
        </p>
      </div>
      <a class="sidebar-nav-item" href="/papers.html">All Papers</a>
      <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
      <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>
      <a class="sidebar-nav-item" href="/topic-viz.html">Topic-based Explorer</a>
      <a class="sidebar-nav-item" href="/tutorial.html">Tutorial</a>
      <a class="sidebar-nav-item" href="/resources.html">Resources, Courses &#38; Events</a>
      <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
    </nav>

    <div class="sidebar-item">
      <p style="font-size: 12px">
        Contact <a href="http://www.seanjmoran.com">Sean Moran</a> about this survey or website.
        <span style="font-size: 9px">
          Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
        </span>
      </p>
    </div>
  </div>
</div>

<script>
$("#searchTarget").keydown(function (e) {	
  if (e.keyCode == 13) {
    search();
  }
});

function search() {
  try {
    ga('send', 'event', 'search', 'search', $("#searchTarget").val());
  } finally {
    window.location = "/papers.html#" + $("#searchTarget").val();
  }
}
</script>


    <div class="content container">
      Search across all paper titles, abstracts, and authors by using the search field.
Please consider <a href="/contributing.html">contributing</a> by updating
the information of existing papers or adding new work.

<!-- Loading Indicator -->
<div id="loading">
  <p>Loading...</p>
</div>

<!-- Data Table -->
<table id="allPapers">
<thead>
  <tr>
    <th>Year</th>
    <th>Title</th>
    <th>Authors</th>
    <th>Venue</th>
    <th>Citations</th>
    <th>Abstract</th>
    <th>Tags</th>
  </tr>
  </thead>
  <tbody>
    
    
      
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/xu2025llama/">Llama Nemoretriever Colembed: Top-performing Text-image Retrieval Model</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Llama Nemoretriever Colembed: Top-performing Text-image Retrieval Model' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Llama Nemoretriever Colembed: Top-performing Text-image Retrieval Model' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>44</td>
    <td><p>Motivated by the growing demand for retrieval systems that operate across modalities, we introduce llama-nemoretriever-colembed, a unified text-image retrieval model that delivers state-of-the-art performance across multiple benchmarks. We release two model variants, 1B and 3B. The 3B model achieves state of the art performance, scoring NDCG@5 91.0 on ViDoRe V1 and 63.5 on ViDoRe V2, placing first on both leaderboards as of June 27, 2025.
  Our approach leverages the NVIDIA Eagle2 Vision-Language model (VLM), modifies its architecture by replacing causal attention with bidirectional attention, and integrates a ColBERT-style late interaction mechanism to enable fine-grained multimodal retrieval in a shared embedding space. While this mechanism delivers superior retrieval accuracy, it introduces trade-offs in storage and efficiency. We provide a comprehensive analysis of these trade-offs. Additionally, we adopt a two-stage training strategy to enhance the model’s retrieval capabilities.</p>
</td>
    <td>
      
        SIGIR 
      
        Text Retrieval 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/xing2025quari/">Quari: Query Adaptive Retrieval Improvement</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Quari: Query Adaptive Retrieval Improvement' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Quari: Query Adaptive Retrieval Improvement' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xing et al.</td> <!-- 🔧 You were missing this -->
    <td>Journal of Systems and Software</td>
    <td>15</td>
    <td><p>Massive-scale pretraining has made vision-language models increasingly popular for image-to-image and text-to-image retrieval across a broad collection of domains. However, these models do not perform well when used for challenging retrieval tasks, such as instance retrieval in very large-scale image collections. Recent work has shown that linear transformations of VLM features trained for instance retrieval can improve performance by emphasizing subspaces that relate to the domain of interest. In this paper, we explore a more extreme version of this specialization by learning to map a given query to a query-specific feature space transformation. Because this transformation is linear, it can be applied with minimal computational cost to millions of image embeddings, making it effective for large-scale retrieval or re-ranking. Results show that this method consistently outperforms state-of-the-art alternatives, including those that require many orders of magnitude more computation at query time.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/xiao2025mieb/">MIEB: Massive Image Embedding Benchmark</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=MIEB: Massive Image Embedding Benchmark' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=MIEB: Massive Image Embedding Benchmark' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xiao et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics</td>
    <td>134</td>
    <td><p>Image representations are often evaluated through disjointed, task-specific
protocols, leading to a fragmented understanding of model capabilities. For
instance, it is unclear whether an image embedding model adept at clustering
images is equally good at retrieving relevant images given a piece of text. We
introduce the Massive Image Embedding Benchmark (MIEB) to evaluate the
performance of image and image-text embedding models across the broadest
spectrum to date. MIEB spans 38 languages across 130 individual tasks, which we
group into 8 high-level categories. We benchmark 50 models across our
benchmark, finding that no single method dominates across all task categories.
We reveal hidden capabilities in advanced vision models such as their accurate
visual representation of texts, and their yet limited capabilities in
interleaved encodings and matching images and texts in the presence of
confounders. We also show that the performance of vision encoders on MIEB
correlates highly with their performance when used in multimodal large language
models. Our code, dataset, and leaderboard are publicly available at
https://github.com/embeddings-benchmark/mteb.</p>
</td>
    <td>
      
        EACL 
      
        TACL 
      
        NAACL 
      
        ACL 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/xiao2025locore/">LOCORE: Image Re-ranking With Long-context Sequence Modeling</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=LOCORE: Image Re-ranking With Long-context Sequence Modeling' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=LOCORE: Image Re-ranking With Long-context Sequence Modeling' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xiao et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the ACM International Conference on Image and Video Retrieval</td>
    <td>5</td>
    <td><p>We introduce LOCORE, Long-Context Re-ranker, a model that takes as input
local descriptors corresponding to an image query and a list of gallery images
and outputs similarity scores between the query and each gallery image. This
model is used for image retrieval, where typically a first ranking is performed
with an efficient similarity measure, and then a shortlist of top-ranked images
is re-ranked based on a more fine-grained similarity measure. Compared to
existing methods that perform pair-wise similarity estimation with local
descriptors or list-wise re-ranking with global descriptors, LOCORE is the
first method to perform list-wise re-ranking with local descriptors. To achieve
this, we leverage efficient long-context sequence models to effectively capture
the dependencies between query and gallery images at the local-descriptor
level. During testing, we process long shortlists with a sliding window
strategy that is tailored to overcome the context size limitations of sequence
models. Our approach achieves superior performance compared with other
re-rankers on established image retrieval benchmarks of landmarks (ROxf and
RPar), products (SOP), fashion items (In-Shop), and bird species (CUB-200)
while having comparable latency to the pair-wise local descriptor re-rankers.</p>
</td>
    <td>
      
        Hybrid ANN Methods 
      
        Video Retrieval 
      
        Re RANKING 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/wang2025ldir/">LDIR: Low-dimensional Dense And Interpretable Text Embeddings With Relative Representations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=LDIR: Low-dimensional Dense And Interpretable Text Embeddings With Relative Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=LDIR: Low-dimensional Dense And Interpretable Text Embeddings With Relative Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Yile, Shen Zhanyu, Huang Hui</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</td>
    <td>51</td>
    <td><p>Semantic text representation is a fundamental task in the field of natural language processing. Existing text embedding (e.g., SimCSE and LLM2Vec) have demonstrated excellent performance, but the values of each dimension are difficult to trace and interpret. Bag-of-words, as classic sparse interpretable embeddings, suffers from poor performance. Recently, Benara et al. (2024) propose interpretable text embeddings using large language models, which forms “0/1” embeddings based on responses to a series of questions. These interpretable text embeddings are typically high-dimensional (larger than 10,000). In this work, we propose Low-dimensional (lower than 500) Dense and Interpretable text embeddings with Relative representations (LDIR). The numerical values of its dimensions indicate semantic relatedness to different anchor texts through farthest point sampling, offering both semantic representation as well as a certain level of traceability and interpretability. We validate LDIR on multiple semantic textual similarity, retrieval, and clustering tasks. Extensive experimental results show that LDIR performs close to the black-box baseline models and outperforms the interpretable embeddings baselines with much fewer dimensions. Code is available at https://github.com/szu-tera/LDIR.</p>
</td>
    <td>
      
        EMNLP 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/wang2025uncertainty/">Uncertainty-aware Unsupervised Video Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Uncertainty-aware Unsupervised Video Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Uncertainty-aware Unsupervised Video Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>48</td>
    <td><p>Learning to hash has become popular for video retrieval due to its fast speed and low storage consumption. Previous efforts formulate video hashing as training a binary auto-encoder, for which noncontinuous latent representations are optimized by the biased straight-through (ST) back-propagation heuristic. We propose to formulate video hashing as learning a discrete variational auto-encoder with the factorized Bernoulli latent distribution, termed as Bernoulli variational auto-encoder (BerVAE). The corresponding evidence lower bound (ELBO) in our BerVAE implementation leads to closed-form gradient expression, which can be applied to achieve principled training along with some other unbiased gradient estimators. BerVAE enables uncertainty-aware video hashing by predicting the probability distribution of video hash code-words, thus providing reliable uncertainty quantification. Experiments on both simulated and real-world large-scale video data demonstrate that our BerVAE trained with unbiased gradient estimators can achieve the state-of-the-art retrieval performance. Furthermore, we show that quantified uncertainty is highly correlated to video retrieval performance, which can be leveraged to further improve the retrieval accuracy. Our code is available at https://github.com/wangyucheng1234/BerVAE</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/verma2025faster/">Faster And Space Efficient Indexing For Locality Sensitive Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Faster And Space Efficient Indexing For Locality Sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Faster And Space Efficient Indexing For Locality Sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Verma Bhisham Dev, Pratap Rameshwar</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 21st ACM international conference on Information and knowledge management</td>
    <td>52</td>
    <td><p>This work suggests faster and space-efficient index construction algorithms
for LSH for Euclidean distance (\textit{a.k.a.}~\ELSH) and cosine similarity
(\textit{a.k.a.}~\SRP). The index construction step of these LSHs relies on
grouping data points into several bins of hash tables based on their hashcode.
To generate an \(m\)-dimensional hashcode of the \(d\)-dimensional data point,
these LSHs first project the data point onto a \(d\)-dimensional random Gaussian
vector and then discretise the resulting inner product. The time and space
complexity of both \ELSH~and \SRP~for computing an \(m\)-sized hashcode of a
\(d\)-dimensional vector is \(O(md)\), which becomes impractical for large values
of \(m\) and \(d\). To overcome this problem, we propose two alternative LSH
hashcode generation algorithms both for Euclidean distance and cosine
similarity, namely, \CSELSH, \HCSELSH~and \CSSRP, \HCSSRP, respectively.
\CSELSH~and \CSSRP~are based on count sketch \cite{count_sketch} and
\HCSELSH~and \HCSSRP~utilize higher-order count sketch \cite{shi2019higher}.
These proposals significantly reduce the hashcode computation time from \(O(md)\)
to \(O(d)\). Additionally, both \CSELSH~and \CSSRP~reduce the space complexity
from \(O(md)\) to \(O(d)\); ~and \HCSELSH, \HCSSRP~ reduce the space complexity
from \(O(md)\) to \(O(N \sqrt[N]{d})\) respectively, where \(N\geq 1\) denotes the
size of the input/reshaped tensor. Our proposals are backed by strong
mathematical guarantees, and we validate their performance through simulations
on various real-world datasets.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Hashing Methods 
      
        CIKM 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/veneroso2025crisp/">CRISP: Clustering Multi-vector Representations For Denoising And Pruning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=CRISP: Clustering Multi-vector Representations For Denoising And Pruning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=CRISP: Clustering Multi-vector Representations For Denoising And Pruning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Veneroso et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Geoinformation and Cartography</td>
    <td>5</td>
    <td><p>Multi-vector models, such as ColBERT, are a significant advancement in neural information retrieval (IR), delivering state-of-the-art performance by representing queries and documents by multiple contextualized token-level embeddings. However, this increased representation size introduces considerable storage and computational overheads which have hindered widespread adoption in practice. A common approach to mitigate this overhead is to cluster the model’s frozen vectors, but this strategy’s effectiveness is fundamentally limited by the intrinsic clusterability of these embeddings. In this work, we introduce CRISP (Clustered Representations with Intrinsic Structure Pruning), a novel multi-vector training method which learns inherently clusterable representations directly within the end-to-end training process. By integrating clustering into the training phase rather than imposing it post-hoc, CRISP significantly outperforms post-hoc clustering at all representation sizes, as well as other token pruning methods. On the BEIR retrieval benchmarks, CRISP achieves a significant rate of ~3x reduction in the number of vectors while outperforming the original unpruned model. This indicates that learned clustering effectively denoises the model by filtering irrelevant information, thereby generating more robust multi-vector representations. With more aggressive clustering, CRISP achieves an 11x reduction in the number of vectors with only a \(3.6%\) quality loss.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/vongala2025compositional/">Compositional Image-text Matching And Retrieval By Grounding Entities</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Compositional Image-text Matching And Retrieval By Grounding Entities' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Compositional Image-text Matching And Retrieval By Grounding Entities' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Vongala Madhukar Reddy, Srivastava Saurabh, Košecká Jana</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>69</td>
    <td><p>Vision-language pretraining on large datasets of images-text pairs is one of
the main building blocks of current Vision-Language Models. While with
additional training, these models excel in various downstream tasks, including
visual question answering, image captioning, and visual commonsense reasoning.
However, a notable weakness of pretrained models like CLIP, is their inability
to perform entity grounding and compositional image and text
matching~\cite{Jiang2024ComCLIP, yang2023amc, Rajabi2023GroundedVSR,
learninglocalizeCVPR24}. In this work we propose a novel learning-free
zero-shot augmentation of CLIP embeddings that has favorable compositional
properties. We compute separate embeddings of sub-images of object entities and
relations that are localized by the state of the art open vocabulary detectors
and dynamically adjust the baseline global image embedding. % The final
embedding is obtained by computing a weighted combination of the sub-image
embeddings. The resulting embedding is then utilized for similarity computation
with text embedding, resulting in a average 1.5% improvement in image-text
matching accuracy on the Visual Genome and SVO Probes
datasets~\cite{krishna2017visualgenome, svo}. Notably, the enhanced embeddings
demonstrate superior retrieval performance, thus achieving significant gains on
the Flickr30K and MS-COCO retrieval benchmarks~\cite{flickr30ke, mscoco},
improving the state-of-the-art Recall@1 by 12% and 0.4%, respectively. Our
code is available at https://github.com/madhukarreddyvongala/GroundingCLIP.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/tavares2025multi/">Multi-label Cross-lingual Automatic Music Genre Classification From Lyrics With Sentence BERT</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multi-label Cross-lingual Automatic Music Genre Classification From Lyrics With Sentence BERT' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multi-label Cross-lingual Automatic Music Genre Classification From Lyrics With Sentence BERT' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tavares Tiago Fernandes, Ayres Fabio José</td> <!-- 🔧 You were missing this -->
    <td>2016 24th Signal Processing and Communication Application Conference (SIU)</td>
    <td>6</td>
    <td><p>Music genres are shaped by both the stylistic features of songs and the
cultural preferences of artists’ audiences. Automatic classification of music
genres using lyrics can be useful in several applications such as
recommendation systems, playlist creation, and library organization. We present
a multi-label, cross-lingual genre classification system based on multilingual
sentence embeddings generated by sBERT. Using a bilingual Portuguese-English
dataset with eight overlapping genres, we demonstrate the system’s ability to
train on lyrics in one language and predict genres in another. Our approach
outperforms the baseline approach of translating lyrics and using a
bag-of-words representation, improving the genrewise average F1-Score from 0.35
to 0.69. The classifier uses a one-vs-all architecture, enabling it to assign
multiple genre labels to a single lyric. Experimental results reveal that
dataset centralization notably improves cross-lingual performance. This
approach offers a scalable solution for genre classification across
underrepresented languages and cultural domains, advancing the capabilities of
music information retrieval systems.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/spaldingjamieson2025scalable/">Scalable K-means Clustering For Large K Via Seeded Approximate Nearest-neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Scalable K-means Clustering For Large K Via Seeded Approximate Nearest-neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Scalable K-means Clustering For Large K Via Seeded Approximate Nearest-neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Spalding-jamieson Jack, Robson Eliot Wong, Zheng da Wei</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>6</td>
    <td><p>For very large values of \(k\), we consider methods for fast \(k\)-means
clustering of massive datasets with \(10^7\sim10^9\) points in high-dimensions
(\(d\geq100\)). All current practical methods for this problem have runtimes at
least \(Ω(k^2)\). We find that initialization routines are not a bottleneck
for this case. Instead, it is critical to improve the speed of Lloyd’s
local-search algorithm, particularly the step that reassigns points to their
closest center. Attempting to improve this step naturally leads us to leverage
approximate nearest-neighbor search methods, although this alone is not enough
to be practical. Instead, we propose a family of problems we call “Seeded
Approximate Nearest-Neighbor Search”, for which we propose “Seeded
Search-Graph” methods as a solution.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/silavong2025deskew/">Deskew-lsh Based Code-to-code Recommendation Engine</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deskew-lsh Based Code-to-code Recommendation Engine' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deskew-lsh Based Code-to-code Recommendation Engine' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Silavong et al.</td> <!-- 🔧 You were missing this -->
    <td>Recommendation Systems in Software Engineering</td>
    <td>16</td>
    <td><p>Machine learning on source code (MLOnCode) is a popular research field that has been driven by the availability of large-scale code repositories and the development of powerful probabilistic and deep learning models for mining source code. Code-to-code recommendation is a task in MLOnCode that aims to recommend relevant, diverse and concise code snippets that usefully extend the code currently being written by a developer in their development environment (IDE). Code-to-code recommendation engines hold the promise of increasing developer productivity by reducing context switching from the IDE and increasing code-reuse. Existing code-to-code recommendation engines do not scale gracefully to large codebases, exhibiting a linear growth in query time as the code repository increases in size. In addition, existing code-to-code recommendation engines fail to account for the global statistics of code repositories in the ranking function, such as the distribution of code snippet lengths, leading to sub-optimal retrieval results. We address both of these weaknesses with <em>Senatus</em>, a new code-to-code recommendation engine. At the core of Senatus is <em>De-Skew</em> LSH a new locality sensitive hashing (LSH) algorithm that indexes the data for fast (sub-linear time) retrieval while also counteracting the skewness in the snippet length distribution using novel abstract syntax tree-based feature scoring and selection algorithms. We evaluate Senatus via automatic evaluation and with an expert developer user study and find the recommendations to be of higher quality than competing baselines, while achieving faster search. For example, on the CodeSearchNet dataset we show that Senatus improves performance by 6.7% F1 and query time 16x is faster compared to Facebook Aroma on the task of code-to-code recommendation.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/ren2025enhanced/">Enhanced Cross-modal 3D Retrieval Via Tri-modal Reconstruction</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Enhanced Cross-modal 3D Retrieval Via Tri-modal Reconstruction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Enhanced Cross-modal 3D Retrieval Via Tri-modal Reconstruction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ren Junlong, Wang Hao</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>54</td>
    <td><p>Cross-modal 3D retrieval is a critical yet challenging task, aiming to
achieve bi-directional retrieval between 3D and text modalities. Current
methods predominantly rely on a certain 3D representation (e.g., point cloud),
with few exploiting the 2D-3D consistency and complementary relationships,
which constrains their performance. To bridge this gap, we propose to adopt
multi-view images and point clouds to jointly represent 3D shapes, facilitating
tri-modal alignment (i.e., image, point, text) for enhanced cross-modal 3D
retrieval. Notably, we introduce tri-modal reconstruction to improve the
generalization ability of encoders. Given point features, we reconstruct image
features under the guidance of text features, and vice versa. With well-aligned
point cloud and multi-view image features, we aggregate them as multimodal
embeddings through fine-grained 2D-3D fusion to enhance geometric and semantic
understanding. Recognizing the significant noise in current datasets where many
3D shapes and texts share similar semantics, we employ hard negative
contrastive training to emphasize harder negatives with greater significance,
leading to robust discriminative embeddings. Extensive experiments on the
Text2Shape dataset demonstrate that our method significantly outperforms
previous state-of-the-art methods in both shape-to-text and text-to-shape
retrieval tasks by a substantial margin.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/reddy2025video/">Video-colbert: Contextualized Late Interaction For Text-to-video Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Video-colbert: Contextualized Late Interaction For Text-to-video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Video-colbert: Contextualized Late Interaction For Text-to-video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Reddy et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>7</td>
    <td><p>In this work, we tackle the problem of text-to-video retrieval (T2VR). Inspired by the success of late interaction techniques in text-document, text-image, and text-video retrieval, our approach, Video-ColBERT, introduces a simple and efficient mechanism for fine-grained similarity assessment between queries and videos. Video-ColBERT is built upon 3 main components: a fine-grained spatial and temporal token-wise interaction, query and visual expansions, and a dual sigmoid loss during training. We find that this interaction and training paradigm leads to strong individual, yet compatible, representations for encoding video content. These representations lead to increases in performance on common text-to-video retrieval benchmarks compared to other bi-encoder methods.</p>
</td>
    <td>
      
        SIGIR 
      
        Video Retrieval 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/pariente2025infinity/">Infinity Search: Approximate Vector Search With Projections On Q-metric Spaces</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Infinity Search: Approximate Vector Search With Projections On Q-metric Spaces' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Infinity Search: Approximate Vector Search With Projections On Q-metric Spaces' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Pariente et al.</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition Letters</td>
    <td>8</td>
    <td><p>Despite the ubiquity of vector search applications, prevailing search algorithms overlook the metric structure of vector embeddings, treating it as a constraint rather than exploiting its underlying properties. In this paper, we demonstrate that in \(q\)-metric spaces, metric trees can leverage a stronger version of the triangle inequality to reduce comparisons for exact search. Notably, as \(q\) approaches infinity, the search complexity becomes logarithmic. Therefore, we propose a novel projection method that embeds vector datasets with arbitrary dissimilarity measures into \(q\)-metric spaces while preserving the nearest neighbor. We propose to learn an approximation of this projection to efficiently transform query points to a space where euclidean distances satisfy the desired properties. Our experimental results with text and image vector embeddings show that learning \(q\)-metric approximations enables classic metric tree algorithms – which typically underperform with high-dimensional data – to achieve competitive performance against state-of-the-art search methods.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/mohoney2025quake/">Quake: Adaptive Indexing For Vector Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Quake: Adaptive Indexing For Vector Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Quake: Adaptive Indexing For Vector Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mohoney et al.</td> <!-- 🔧 You were missing this -->
    <td>ACM Transactions on Intelligent Systems and Technology</td>
    <td>21</td>
    <td><p>Vector search, the task of finding the k-nearest neighbors of a query vector against a database of high-dimensional vectors, underpins many machine learning applications, including retrieval-augmented generation, recommendation systems, and information retrieval. However, existing approximate nearest neighbor (ANN) methods perform poorly under dynamic and skewed workloads where data distributions evolve. We introduce Quake, an adaptive indexing system that maintains low latency and high recall in such environments. Quake employs a multi-level partitioning scheme that adjusts to updates and changing access patterns, guided by a cost model that predicts query latency based on partition sizes and access frequencies. Quake also dynamically sets query execution parameters to meet recall targets using a novel recall estimation model. Furthermore, Quake utilizes NUMA-aware intra-query parallelism for improved memory bandwidth utilization during search. To evaluate Quake, we prepare a Wikipedia vector search workload and develop a workload generator to create vector search workloads with configurable access patterns. Our evaluation shows that on dynamic workloads, Quake achieves query latency reductions of 1.5-38x and update latency reductions of 4.5-126x compared to state-of-the-art indexes such as SVS, DiskANN, HNSW, and SCANN.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/liu2025moboost/">Moboost: A Self-improvement Framework For Linear-based Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Moboost: A Self-improvement Framework For Linear-based Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Moboost: A Self-improvement Framework For Linear-based Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Knowledge and Data Engineering</td>
    <td>5</td>
    <td><p>The linear model is commonly utilized in hashing methods owing to its efficiency. To obtain better accuracy, linear-based hashing methods focus on designing a generalized linear objective function with different constraints or penalty terms that consider neighborhood information. In this study, we propose a novel generalized framework called Model Boost (MoBoost), which can achieve the self-improvement of the linear-based hashing. The proposed MoBoost is used to improve model parameter optimization for linear-based hashing methods without adding new constraints or penalty terms. In the proposed MoBoost, given a linear-based hashing method, we first execute the method several times to get several different hash codes for training samples, and then combine these different hash codes into one set utilizing one novel fusion strategy. Based on this set of hash codes, we learn some new parameters for the linear hash function that can significantly improve accuracy. The proposed MoBoost can be generally adopted in existing linear-based hashing methods, achieving more precise and stable performance compared to the original methods while imposing negligible added expenditure in terms of time and space. Extensive experiments are performed based on three benchmark datasets, and the results demonstrate the superior performance of the proposed framework.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Tools & Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/lin2025neighborretr/">Neighborretr: Balancing Hub Centrality In Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Neighborretr: Balancing Hub Centrality In Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Neighborretr: Balancing Hub Centrality In Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 25th ACM international conference on Multimedia</td>
    <td>592</td>
    <td><p>Cross-modal retrieval aims to bridge the semantic gap between different
modalities, such as visual and textual data, enabling accurate retrieval across
them. Despite significant advancements with models like CLIP that align
cross-modal representations, a persistent challenge remains: the hubness
problem, where a small subset of samples (hubs) dominate as nearest neighbors,
leading to biased representations and degraded retrieval accuracy. Existing
methods often mitigate hubness through post-hoc normalization techniques,
relying on prior data distributions that may not be practical in real-world
scenarios. In this paper, we directly mitigate hubness during training and
introduce NeighborRetr, a novel method that effectively balances the learning
of hubs and adaptively adjusts the relations of various kinds of neighbors. Our
approach not only mitigates the hubness problem but also enhances retrieval
performance, achieving state-of-the-art results on multiple cross-modal
retrieval benchmarks. Furthermore, NeighborRetr demonstrates robust
generalization to new domains with substantial distribution shifts,
highlighting its effectiveness in real-world applications. We make our code
publicly available at: https://github.com/zzezze/NeighborRetr .</p>
</td>
    <td>
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/li2025two/">Two Birds, One Stone: Jointly Learning Binary Code For Large-scale Face Image Retrieval And Attributes Prediction</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Two Birds, One Stone: Jointly Learning Binary Code For Large-scale Face Image Retrieval And Attributes Prediction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Two Birds, One Stone: Jointly Learning Binary Code For Large-scale Face Image Retrieval And Attributes Prediction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li et al.</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE International Conference on Computer Vision (ICCV)</td>
    <td>31</td>
    <td><p>We address the challenging large-scale content-based
face image retrieval problem, intended as searching images
based on the presence of specific subject, given one face
image of him/her. To this end, one natural demand is a supervised binary code learning method. While the learned
codes might be discriminating, people often have a further
expectation that whether some semantic message (e.g., visual attributes) can be read from the human-incomprehensible
codes. For this purpose, we propose a novel binary code
learning framework by jointly encoding identity discriminability and a number of facial attributes into unified binary code. In this way, the learned binary codes can be applied to not only fine-grained face image retrieval, but also
facial attributes prediction, which is the very innovation of
this work, just like killing two birds with one stone. To evaluate the effectiveness of the proposed method, extensive experiments are conducted on a new purified large-scale web
celebrity database, named CFW 60K, with abundant manual identity and attributes annotation, and experimental results exhibit the superiority of our method over state-of-the-art.</p>
</td>
    <td>
      
        SCALABILITY 
      
        Image Retrieval 
      
        Compact Codes 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/li2025clean/">Clean Image May Be Dangerous: Data Poisoning Attacks Against Deep Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Clean Image May Be Dangerous: Data Poisoning Attacks Against Deep Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Clean Image May Be Dangerous: Data Poisoning Attacks Against Deep Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>61</td>
    <td><p>Large-scale image retrieval using deep hashing has become increasingly
popular due to the exponential growth of image data and the remarkable feature
extraction capabilities of deep neural networks (DNNs). However, deep hashing
methods are vulnerable to malicious attacks, including adversarial and backdoor
attacks. It is worth noting that these attacks typically involve altering the
query images, which is not a practical concern in real-world scenarios. In this
paper, we point out that even clean query images can be dangerous, inducing
malicious target retrieval results, like undesired or illegal images. To the
best of our knowledge, we are the first to study data \textbf{p}oisoning
\textbf{a}ttacks against \textbf{d}eep \textbf{hash}ing
\textbf{(\textit{PADHASH})}. Specifically, we first train a surrogate model to
simulate the behavior of the target deep hashing model. Then, a strict gradient
matching strategy is proposed to generate the poisoned images. Extensive
experiments on different models, datasets, hash methods, and hash code lengths
demonstrate the effectiveness and generality of our attack method.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/krauthgamer2025power/">The Power Of Recursive Embeddings For \(\ell_p\) Metrics</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=The Power Of Recursive Embeddings For \(\ell_p\) Metrics' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=The Power Of Recursive Embeddings For \(\ell_p\) Metrics' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Krauthgamer Robert, Petruschka Nir, Sapir Shay</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the American Mathematical Society</td>
    <td>7</td>
    <td><p>Metric embedding is a powerful tool used extensively in mathematics and
computer science. We devise a new method of using metric embeddings
recursively, which turns out to be particularly effective in \(\ell_p\) spaces,
\(p&gt;2\), yielding state-of-the-art results for Lipschitz decomposition, for
Nearest Neighbor Search, and for embedding into \(ℓ₂\). In a nutshell, our
method composes metric embeddings by viewing them as reductions between
problems, and thereby obtains a new reduction that is substantially more
effective than the known reduction that employs a single embedding. We in fact
apply this method recursively, oftentimes using double recursion, which further
amplifies the gap from a single embedding.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/lei2025enhancing/">Enhancing Lexicon-based Text Embeddings With Large Language Models</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Enhancing Lexicon-based Text Embeddings With Large Language Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Enhancing Lexicon-based Text Embeddings With Large Language Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lei et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</td>
    <td>23</td>
    <td><p>Recent large language models (LLMs) have demonstrated exceptional performance
on general-purpose text embedding tasks. While dense embeddings have dominated
related research, we introduce the first Lexicon-based EmbeddiNgS (LENS)
leveraging LLMs that achieve competitive performance on these tasks. Regarding
the inherent tokenization redundancy issue and unidirectional attention
limitations in traditional causal LLMs, LENS consolidates the vocabulary space
through token embedding clustering, and investigates bidirectional attention
and various pooling strategies. Specifically, LENS simplifies lexicon matching
by assigning each dimension to a specific token cluster, where semantically
similar tokens are grouped together, and unlocking the full potential of LLMs
through bidirectional attention. Extensive experiments demonstrate that LENS
outperforms dense embeddings on the Massive Text Embedding Benchmark (MTEB),
delivering compact feature representations that match the sizes of dense
counterparts. Notably, combining LENSE with dense embeddings achieves
state-of-the-art performance on the retrieval subset of MTEB (i.e. BEIR).</p>
</td>
    <td>
      
        ACL 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/kasalick%C3%BD2025future/">The Future Is Sparse: Embedding Compression For Scalable Retrieval In Recommender Systems</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=The Future Is Sparse: Embedding Compression For Scalable Retrieval In Recommender Systems' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=The Future Is Sparse: Embedding Compression For Scalable Retrieval In Recommender Systems' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kasalický et al.</td> <!-- 🔧 You were missing this -->
    <td>ACM Computing Surveys</td>
    <td>9</td>
    <td><p>Industry-scale recommender systems face a core challenge: representing entities with high cardinality, such as users or items, using dense embeddings that must be accessible during both training and inference. However, as embedding sizes grow, memory constraints make storage and access increasingly difficult. We describe a lightweight, learnable embedding compression technique that projects dense embeddings into a high-dimensional, sparsely activated space. Designed for retrieval tasks, our method reduces memory requirements while preserving retrieval performance, enabling scalable deployment under strict resource constraints. Our results demonstrate that leveraging sparsity is a promising approach for improving the efficiency of large-scale recommenders. We release our code at https://github.com/recombee/CompresSAE.</p>
</td>
    <td>
      
        Large Scale Search 
      
        Survey Paper 
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/j%C3%A4%C3%A4saari2025vibe/">VIBE: Vector Index Benchmark For Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=VIBE: Vector Index Benchmark For Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=VIBE: Vector Index Benchmark For Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jääsaari et al.</td> <!-- 🔧 You were missing this -->
    <td>2023 International Workshop on Artificial Intelligence and Image Processing (IWAIIP)</td>
    <td>6</td>
    <td><p>Approximate nearest neighbor (ANN) search is a performance-critical component of many machine learning pipelines. Rigorous benchmarking is essential for evaluating the performance of vector indexes for ANN search. However, the datasets of the existing benchmarks are no longer representative of the current applications of ANN search. Hence, there is an urgent need for an up-to-date set of benchmarks. To this end, we introduce Vector Index Benchmark for Embeddings (VIBE), an open source project for benchmarking ANN algorithms. VIBE contains a pipeline for creating benchmark datasets using dense embedding models characteristic of modern applications, such as retrieval-augmented generation (RAG). To replicate real-world workloads, we also include out-of-distribution (OOD) datasets where the queries and the corpus are drawn from different distributions. We use VIBE to conduct a comprehensive evaluation of SOTA vector indexes, benchmarking 21 implementations on 12 in-distribution and 6 out-of-distribution datasets.</p>
</td>
    <td>
      
        Vector Indexing 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/mei2025temporal/">Temporal-aware Spiking Transformer Hashing Based On 3D-DWT</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Temporal-aware Spiking Transformer Hashing Based On 3D-DWT' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Temporal-aware Spiking Transformer Hashing Based On 3D-DWT' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mei et al.</td> <!-- 🔧 You were missing this -->
    <td>2015 International Conference on Green Computing and Internet of Things (ICGCIoT)</td>
    <td>8</td>
    <td><p>With the rapid growth of dynamic vision sensor (DVS) data, constructing a
low-energy, efficient data retrieval system has become an urgent task. Hash
learning is one of the most important retrieval technologies which can keep the
distance between hash codes consistent with the distance between DVS data. As
spiking neural networks (SNNs) can encode information through spikes, they
demonstrate great potential in promoting energy efficiency. Based on the binary
characteristics of SNNs, we first propose a novel supervised hashing method
named Spikinghash with a hierarchical lightweight structure. Spiking WaveMixer
(SWM) is deployed in shallow layers, utilizing a multilevel 3D discrete wavelet
transform (3D-DWT) to decouple spatiotemporal features into various
low-frequency and high frequency components, and then employing efficient
spectral feature fusion. SWM can effectively capture the temporal dependencies
and local spatial features. Spiking Self-Attention (SSA) is deployed in deeper
layers to further extract global spatiotemporal information. We also design a
hash layer utilizing binary characteristic of SNNs, which integrates
information over multiple time steps to generate final hash codes. Furthermore,
we propose a new dynamic soft similarity loss for SNNs, which utilizes membrane
potentials to construct a learnable similarity matrix as soft labels to fully
capture the similarity differences between classes and compensate information
loss in SNNs, thereby improving retrieval performance. Experiments on multiple
datasets demonstrate that Spikinghash can achieve state-of-the-art results with
low energy consumption and fewer parameters.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/jain2025simpledoc/">Simpledoc: Multi-modal Document Understanding With Dual-cue Page Retrieval And Iterative Refinement</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Simpledoc: Multi-modal Document Understanding With Dual-cue Page Retrieval And Iterative Refinement' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Simpledoc: Multi-modal Document Understanding With Dual-cue Page Retrieval And Iterative Refinement' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jain et al.</td> <!-- 🔧 You were missing this -->
    <td>2013 12th International Conference on Document Analysis and Recognition</td>
    <td>5</td>
    <td><p>Document Visual Question Answering (DocVQA) is a practical yet challenging task, which is to ask questions based on documents while referring to multiple pages and different modalities of information, e.g, images and tables. To handle multi-modality, recent methods follow a similar Retrieval Augmented Generation (RAG) pipeline, but utilize Visual Language Models (VLMs) based embedding model to embed and retrieve relevant pages as images, and generate answers with VLMs that can accept an image as input. In this paper, we introduce SimpleDoc, a lightweight yet powerful retrieval - augmented framework for DocVQA. It boosts evidence page gathering by first retrieving candidates through embedding similarity and then filtering and re-ranking these candidates based on page summaries. A single VLM-based reasoner agent repeatedly invokes this dual-cue retriever, iteratively pulling fresh pages into a working memory until the question is confidently answered. SimpleDoc outperforms previous baselines by 3.2% on average on 4 DocVQA datasets with much fewer pages retrieved. Our code is available at https://github.com/ag2ai/SimpleDoc.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/huang2025joint/">Joint Fusion And Encoding: Advancing Multimodal Retrieval From The Ground Up</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Joint Fusion And Encoding: Advancing Multimodal Retrieval From The Ground Up' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Joint Fusion And Encoding: Advancing Multimodal Retrieval From The Ground Up' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Huang et al.</td> <!-- 🔧 You were missing this -->
    <td>Remote Sensing</td>
    <td>43</td>
    <td><p>Information retrieval is indispensable for today’s Internet applications, yet
traditional semantic matching techniques often fall short in capturing the
fine-grained cross-modal interactions required for complex queries. Although
late-fusion two-tower architectures attempt to bridge this gap by independently
encoding visual and textual data before merging them at a high level, they
frequently overlook the subtle interplay essential for comprehensive
understanding. In this work, we rigorously assess these limitations and
introduce a unified retrieval framework that fuses visual and textual cues from
the ground up, enabling early cross-modal interactions for enhancing context
interpretation. Through a two-stage training process–comprising post-training
adaptation followed by instruction tuning–we adapt MLLMs as retrievers using a
simple one-tower architecture. Our approach outperforms conventional methods
across diverse retrieval scenarios, particularly when processing complex
multi-modal inputs. Notably, the joint fusion encoder yields greater
improvements on tasks that require modality fusion compared to those that do
not, underscoring the transformative potential of early integration strategies
and pointing toward a promising direction for contextually aware and effective
information retrieval.</p>
</td>
    <td>
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/huang2025digital/">Digital Collections Explorer: An Open-source, Multimodal Viewer For Searching Digital Collections</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Digital Collections Explorer: An Open-source, Multimodal Viewer For Searching Digital Collections' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Digital Collections Explorer: An Open-source, Multimodal Viewer For Searching Digital Collections' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Huang Ying-hsiang, Lee Benjamin Charles Germain</td> <!-- 🔧 You were missing this -->
    <td>International Journal of Digital Library Systems</td>
    <td>13</td>
    <td><p>We present Digital Collections Explorer, a web-based, open-source exploratory search platform that leverages CLIP (Contrastive Language-Image Pre-training) for enhanced visual discovery of digital collections. Our Digital Collections Explorer can be installed locally and configured to run on a visual collection of interest on disk in just a few steps. Building upon recent advances in multimodal search techniques, our interface enables natural language queries and reverse image searches over digital collections with visual features. This paper describes the system’s architecture, implementation, and application to various cultural heritage collections, demonstrating its potential for democratizing access to digital archives, especially those with impoverished metadata. We present case studies with maps, photographs, and PDFs extracted from web archives in order to demonstrate the flexibility of the Digital Collections Explorer, as well as its ease of use. We demonstrate that the Digital Collections Explorer scales to hundreds of thousands of images on a MacBook Pro with an M4 chip. Lastly, we host a public demo of Digital Collections Explorer.</p>
</td>
    <td>
      
        Tools & Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/gupta2025retreever/">Retreever: Tree-based Coarse-to-fine Representations For Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Retreever: Tree-based Coarse-to-fine Representations For Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Retreever: Tree-based Coarse-to-fine Representations For Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gupta et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings. International Conference on Image Processing</td>
    <td>6</td>
    <td><p>Document retrieval is a core component of question-answering systems, as it
enables conditioning answer generation on new and large-scale corpora. While
effective, the standard practice of encoding documents into high-dimensional
embeddings for similarity search entails large memory and compute footprints,
and also makes it hard to inspect the inner workings of the system. In this
paper, we propose a tree-based method for organizing and representing reference
documents at various granular levels, which offers the flexibility to balance
cost and utility, and eases the inspection of the corpus content and retrieval
operations. Our method, called ReTreever, jointly learns a routing function per
internal node of a binary tree such that query and reference documents are
assigned to similar tree branches, hence directly optimizing for retrieval
performance. Our evaluations show that ReTreever generally preserves full
representation accuracy. Its hierarchical structure further provides strong
coarse representations and enhances transparency by indirectly learning
meaningful semantic groupings. Among hierarchical retrieval methods, ReTreever
achieves the best retrieval accuracy at the lowest latency, proving that this
family of techniques can be viable in practical applications.</p>
</td>
    <td>
      
        Tree Based ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/guo2025gpu/">Gpu-accelerated Multi-relational Parallel Graph Retrieval For Web-scale Recommendations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Gpu-accelerated Multi-relational Parallel Graph Retrieval For Web-scale Recommendations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Gpu-accelerated Multi-relational Parallel Graph Retrieval For Web-scale Recommendations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Guo et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</td>
    <td>6</td>
    <td><p>Web recommendations provide personalized items from massive catalogs for
users, which rely heavily on retrieval stages to trade off the effectiveness
and efficiency of selecting a small relevant set from billion-scale candidates
in online digital platforms. As one of the largest Chinese search engine and
news feed providers, Baidu resorts to Deep Neural Network (DNN) and graph-based
Approximate Nearest Neighbor Search (ANNS) algorithms for accurate relevance
estimation and efficient search for relevant items. However, current retrieval
at Baidu fails in comprehensive user-item relational understanding due to
dissected interaction modeling, and performs inefficiently in large-scale
graph-based ANNS because of suboptimal traversal navigation and the GPU
computational bottleneck under high concurrency. To this end, we propose a
GPU-accelerated Multi-relational Parallel Graph Retrieval (GMP-GR) framework to
achieve effective yet efficient retrieval in web-scale recommendations. First,
we propose a multi-relational user-item relevance metric learning method that
unifies diverse user behaviors through multi-objective optimization and employs
a self-covariant loss to enhance pathfinding performance. Second, we develop a
hierarchical parallel graph-based ANNS to boost graph retrieval throughput,
which conducts breadth-depth-balanced searches on a large-scale item graph and
cost-effectively handles irregular neural computation via adaptive aggregation
on GPUs. In addition, we integrate system optimization strategies in the
deployment of GMP-GR in Baidu. Extensive experiments demonstrate the
superiority of GMP-GR in retrieval accuracy and efficiency. Deployed across
more than twenty applications at Baidu, GMP-GR serves hundreds of millions of
users with a throughput exceeding one hundred million requests per second.</p>
</td>
    <td>
      
        KDD 
      
        Large Scale Search 
      
        Recommender Systems 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/gritta2025dresd/">Dresd: Dense Retrieval For Speculative Decoding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Dresd: Dense Retrieval For Speculative Decoding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Dresd: Dense Retrieval For Speculative Decoding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gritta Milan, Xue Huiyin, Lampouras Gerasimos</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)</td>
    <td>6</td>
    <td><p>Speculative decoding (SD) accelerates Large Language Model (LLM) generation by using an efficient draft model to propose the next few tokens, which are verified by the LLM in a single forward call, reducing latency while preserving its outputs. We focus on retrieval-based SD where the draft model retrieves the next tokens from a non-parametric datastore. Sparse retrieval (REST), which operates on the surface form of strings, is currently the dominant paradigm due to its simplicity and scalability. However, its effectiveness is limited due to the usage of short contexts and exact string matching. Instead, we introduce Dense Retrieval for Speculative Decoding (DReSD), a novel framework that uses approximate nearest neighbour search with contextualised token embeddings to retrieve the most semantically relevant token sequences for SD. Extensive experiments show that DReSD achieves (on average) 87% higher acceptance rates, 65% longer accepted tokens and 19% faster generation speeds compared to sparse retrieval (REST).</p>
</td>
    <td>
      
        EACL 
      
        NAACL 
      
        ACL 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/garimella2025he/">HE-LRM: Encrypted Deep Learning Recommendation Models Using Fully Homomorphic Encryption</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=HE-LRM: Encrypted Deep Learning Recommendation Models Using Fully Homomorphic Encryption' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=HE-LRM: Encrypted Deep Learning Recommendation Models Using Fully Homomorphic Encryption' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Garimella et al.</td> <!-- 🔧 You were missing this -->
    <td>Soft Computing</td>
    <td>11</td>
    <td><p>Fully Homomorphic Encryption (FHE) is an encryption scheme that not only encrypts data but also allows for computations to be applied directly on the encrypted data. While computationally expensive, FHE can enable privacy-preserving neural inference in the client-server setting: a client encrypts their input with FHE and sends it to an untrusted server. The server then runs neural inference on the encrypted data and returns the encrypted results. The client decrypts the output locally, keeping both the input and result private from the server. Private inference has focused on networks with dense inputs such as image classification, and less attention has been given to networks with sparse features. Unlike dense inputs, sparse features require efficient encrypted lookup operations into large embedding tables, which present computational and memory constraints for FHE.
  In this paper, we explore the challenges and opportunities when applying FHE to Deep Learning Recommendation Models (DLRM) from both a compiler and systems perspective. DLRMs utilize conventional MLPs for dense features and embedding tables to map sparse, categorical features to dense vector representations. We develop novel methods for performing compressed embedding lookups in order to reduce FHE computational costs while keeping the underlying model performant. Our embedding lookup improves upon a state-of-the-art approach by \(77 \times\). Furthermore, we present an efficient multi-embedding packing strategy that enables us to perform a 44 million parameter embedding lookup under FHE. Finally, we integrate our solutions into the open-source Orion framework and present HE-LRM, an end-to-end encrypted DLRM. We evaluate HE-LRM on UCI (health prediction) and Criteo (click prediction), demonstrating that with the right compression and packing strategies, encrypted inference for recommendation systems is practical.</p>
</td>
    <td>
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/firtina2025enabling/">Enabling Fast, Accurate, And Efficient Real-time Genome Analysis Via New Algorithms And Techniques</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Enabling Fast, Accurate, And Efficient Real-time Genome Analysis Via New Algorithms And Techniques' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Enabling Fast, Accurate, And Efficient Real-time Genome Analysis Via New Algorithms And Techniques' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Firtina Can</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Computers</td>
    <td>8</td>
    <td><p>The advent of high-throughput sequencing technologies has revolutionized
genome analysis by enabling the rapid and cost-effective sequencing of large
genomes. Despite these advancements, the increasing complexity and volume of
genomic data present significant challenges related to accuracy, scalability,
and computational efficiency. These challenges are mainly due to various forms
of unwanted and unhandled variations in sequencing data, collectively referred
to as noise. In this dissertation, we address these challenges by providing a
deep understanding of different types of noise in genomic data and developing
techniques to mitigate the impact of noise on genome analysis.
  First, we introduce BLEND, a noise-tolerant hashing mechanism that quickly
identifies both exactly matching and highly similar sequences with arbitrary
differences using a single lookup of their hash values. Second, to enable
scalable and accurate analysis of noisy raw nanopore signals, we propose
RawHash, a novel mechanism that effectively reduces noise in raw nanopore
signals and enables accurate, real-time analysis by proposing the first
hash-based similarity search technique for raw nanopore signals. Third, we
extend the capabilities of RawHash with RawHash2, an improved mechanism that 1)
provides a better understanding of noise in raw nanopore signals to reduce it
more effectively and 2) improves the robustness of mapping decisions. Fourth,
we explore the broader implications and new applications of raw nanopore signal
analysis by introducing Rawsamble, the first mechanism for all-vs-all
overlapping of raw signals using hash-based search. Rawsamble enables the
construction of de novo assemblies directly from raw signals without
basecalling, which opens up new directions and uses for raw nanopore signal
analysis.</p>
</td>
    <td>
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/fan2025synergizing/">Synergizing Implicit And Explicit User Interests: A Multi-embedding Retrieval Framework At Pinterest</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Synergizing Implicit And Explicit User Interests: A Multi-embedding Retrieval Framework At Pinterest' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Synergizing Implicit And Explicit User Interests: A Multi-embedding Retrieval Framework At Pinterest' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fan et al.</td> <!-- 🔧 You were missing this -->
    <td>Social Cognitive and Affective Neuroscience</td>
    <td>356</td>
    <td><p>Industrial recommendation systems are typically composed of multiple stages, including retrieval, ranking, and blending. The retrieval stage plays a critical role in generating a high-recall set of candidate items that covers a wide range of diverse user interests. Effectively covering the diverse and long-tail user interests within this stage poses a significant challenge: traditional two-tower models struggle in this regard due to limited user-item feature interaction and often bias towards top use cases. To address these issues, we propose a novel multi-embedding retrieval framework designed to enhance user interest representation by generating multiple user embeddings conditioned on both implicit and explicit user interests. Implicit interests are captured from user history through a Differentiable Clustering Module (DCM), whereas explicit interests, such as topics that the user has followed, are modeled via Conditional Retrieval (CR). These methodologies represent a form of conditioned user representation learning that involves condition representation construction and associating the target item with the relevant conditions. Synergizing implicit and explicit user interests serves as a complementary approach to achieve more effective and comprehensive candidate retrieval as they benefit on different user segments and extract conditions from different but supplementary sources. Extensive experiments and A/B testing reveal significant improvements in user engagements and feed diversity metrics. Our proposed framework has been successfully deployed on Pinterest home feed.</p>
</td>
    <td>
      
        Tools & Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/faltings2025enhancing/">Enhancing Retrieval Systems With Inference-time Logical Reasoning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Enhancing Retrieval Systems With Inference-time Logical Reasoning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Enhancing Retrieval Systems With Inference-time Logical Reasoning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Faltings Felix, Wei Wei, Bao Yujia</td> <!-- 🔧 You were missing this -->
    <td>Behavioral and Brain Sciences</td>
    <td>31</td>
    <td><p>Traditional retrieval methods rely on transforming user queries into vector
representations and retrieving documents based on cosine similarity within an
embedding space. While efficient and scalable, this approach often fails to
handle complex queries involving logical constructs such as negations,
conjunctions, and disjunctions. In this paper, we propose a novel
inference-time logical reasoning framework that explicitly incorporates logical
reasoning into the retrieval process. Our method extracts logical reasoning
structures from natural language queries and then composes the individual
cosine similarity scores to formulate the final document scores. This approach
enables the retrieval process to handle complex logical reasoning without
compromising computational efficiency. Our results on both synthetic and
real-world benchmarks demonstrate that the proposed method consistently
outperforms traditional retrieval methods across different models and datasets,
significantly improving retrieval performance for complex queries.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/duan2025enhancing/">Enhancing Subsequent Video Retrieval Via Vision-language Models (vlms)</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Enhancing Subsequent Video Retrieval Via Vision-language Models (vlms)' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Enhancing Subsequent Video Retrieval Via Vision-language Models (vlms)' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Duan Yicheng, Huang Xi, Chen Duo</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>71</td>
    <td><p>The rapid growth of video content demands efficient and precise retrieval
systems. While vision-language models (VLMs) excel in representation learning,
they often struggle with adaptive, time-sensitive video retrieval. This paper
introduces a novel framework that combines vector similarity search with
graph-based data structures. By leveraging VLM embeddings for initial retrieval
and modeling contextual relationships among video segments, our approach
enables adaptive query refinement and improves retrieval accuracy. Experiments
demonstrate its precision, scalability, and robustness, offering an effective
solution for interactive video retrieval in dynamic environments.</p>
</td>
    <td>
      
        Video Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/deng2025large/">Large Vision-language Models For Knowledge-grounded Data Annotation Of Memes</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Large Vision-language Models For Knowledge-grounded Data Annotation Of Memes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Large Vision-language Models For Knowledge-grounded Data Annotation Of Memes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Deng Shiling, Belongie Serge, Christensen Peter Ebert</td> <!-- 🔧 You were missing this -->
    <td>2024 IEEE International Conference on Robotics and Automation (ICRA)</td>
    <td>5</td>
    <td><p>Memes have emerged as a powerful form of communication, integrating visual
and textual elements to convey humor, satire, and cultural messages. Existing
research has focused primarily on aspects such as emotion classification, meme
generation, propagation, interpretation, figurative language, and
sociolinguistics, but has often overlooked deeper meme comprehension and
meme-text retrieval. To address these gaps, this study introduces
ClassicMemes-50-templates (CM50), a large-scale dataset consisting of over
33,000 memes, centered around 50 popular meme templates. We also present an
automated knowledge-grounded annotation pipeline leveraging large
vision-language models to produce high-quality image captions, meme captions,
and literary device labels overcoming the labor intensive demands of manual
annotation. Additionally, we propose a meme-text retrieval CLIP model (mtrCLIP)
that utilizes cross-modal embedding to enhance meme analysis, significantly
improving retrieval performance. Our contributions include:(1) a novel dataset
for large-scale meme study, (2) a scalable meme annotation framework, and (3) a
fine-tuned CLIP for meme-text retrieval, all aimed at advancing the
understanding and analysis of memes at scale.</p>
</td>
    <td>
      
        ICRA 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/csizmadia2025distill/">Distill CLIP (DCLIP): Enhancing Image-text Retrieval Via Cross-modal Transformer Distillation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Distill CLIP (DCLIP): Enhancing Image-text Retrieval Via Cross-modal Transformer Distillation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Distill CLIP (DCLIP): Enhancing Image-text Retrieval Via Cross-modal Transformer Distillation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Csizmadia et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Geoscience and Remote Sensing</td>
    <td>36</td>
    <td><p>We present Distill CLIP (DCLIP), a fine-tuned variant of the CLIP model that enhances multimodal image-text retrieval while preserving the original model’s strong zero-shot classification capabilities. CLIP models are typically constrained by fixed image resolutions and limited context, which can hinder their effectiveness in retrieval tasks that require fine-grained cross-modal understanding. DCLIP addresses these challenges through a meta teacher-student distillation framework, where a cross-modal transformer teacher is fine-tuned to produce enriched embeddings via bidirectional cross-attention between YOLO-extracted image regions and corresponding textual spans. These semantically and spatially aligned global representations guide the training of a lightweight student model using a hybrid loss that combines contrastive learning and cosine similarity objectives. Despite being trained on only ~67,500 samples curated from MSCOCO, Flickr30k, and Conceptual Captions-just a fraction of CLIP’s original dataset-DCLIP significantly improves image-text retrieval metrics (Recall@K, MAP), while retaining approximately 94% of CLIP’s zero-shot classification performance. These results demonstrate that DCLIP effectively mitigates the trade-off between task specialization and generalization, offering a resource-efficient, domain-adaptive, and detail-sensitive solution for advanced vision-language tasks. Code available at https://anonymous.4open.science/r/DCLIP-B772/README.md.</p>
</td>
    <td>
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/bawa2025lsh/">LSH Forest: Self-tuning Indexes For Similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=LSH Forest: Self-tuning Indexes For Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=LSH Forest: Self-tuning Indexes For Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Bawa M., Condie, Ganesan</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>5</td>
    <td><p>We consider the problem of indexing high-dimensional data for answering (approximate) similarity-search queries. Similarity indexes prove to be important in a wide variety of settings: Web search
engines desire fast, parallel, main-memory-based indexes for similarity search on text data; database systems desire disk-based similarity indexes for high-dimensional data, including text and images;
peer-to-peer systems desire distributed similarity indexes with low
communication cost. We propose an indexing scheme called LSH
Forest which is applicable in all the above contexts. Our index uses the well-known technique of locality-sensitive hashing (LSH),
but improves upon previous designs by (a) eliminating the different data-dependent parameters for which LSH must be constantly hand-tuned, and (b) improving on LSH’s performance guarantees for skewed data distributions while retaining the same storage
and query overhead. We show how to construct this index in main
memory, on disk, in parallel systems, and in peer-to-peer systems.
We evaluate the design with experiments on multiple text corpora
and demonstrate both the self-tuning nature and the superior performance of LSH Forest.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/bach2025hierarchical/">Hierarchical Patch Compression For Colpali: Efficient Multi-vector Document Retrieval With Dynamic Pruning And Quantization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hierarchical Patch Compression For Colpali: Efficient Multi-vector Document Retrieval With Dynamic Pruning And Quantization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hierarchical Patch Compression For Colpali: Efficient Multi-vector Document Retrieval With Dynamic Pruning And Quantization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Bach Duong</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 9th annual international ACM SIGIR conference on Research and development in information retrieval  - SIGIR '86</td>
    <td>16</td>
    <td><p>Multi-vector document retrieval systems, such as ColPali, excel in fine-grained matching for complex queries but incur significant storage and computational costs due to their reliance on high-dimensional patch embeddings and late-interaction scoring. To address these challenges, we propose HPC-ColPali, a Hierarchical Patch Compression framework that enhances the efficiency of ColPali while preserving its retrieval accuracy. Our approach integrates three innovative techniques: (1) K-Means quantization, which compresses patch embeddings into 1-byte centroid indices, achieving up to 32\(\times\) storage reduction; (2) attention-guided dynamic pruning, utilizing Vision-Language Model attention weights to retain only the top-\(p%\) most salient patches, reducing late-interaction computation by up to 60% with less than 2% nDCG@10 loss; and (3) optional binary encoding of centroid indices into \(b\)-bit strings (\(b=\lceillog_2 K\rceil\)), enabling rapid Hamming distance-based similarity search for resource-constrained environments. Evaluated on the ViDoRe and SEC-Filings datasets, HPC-ColPali achieves 30–50% lower query latency under HNSW indexing while maintaining high retrieval precision. When integrated into a Retrieval-Augmented Generation pipeline for legal summarization, it reduces hallucination rates by 30% and halves end-to-end latency. These advancements establish HPC-ColPali as a scalable and efficient solution for multi-vector document retrieval across diverse applications. Code is available at https://github.com/DngBack/HPC-ColPali.</p>
</td>
    <td>
      
        SIGIR 
      
        Text Retrieval 
      
        Quantization 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/ananthakrishnan2025can/">Can Cross Encoders Produce Useful Sentence Embeddings?</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Can Cross Encoders Produce Useful Sentence Embeddings?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Can Cross Encoders Produce Useful Sentence Embeddings?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ananthakrishnan et al.</td> <!-- 🔧 You were missing this -->
    <td>Natural Language Engineering</td>
    <td>23</td>
    <td><p>Cross encoders (CEs) are trained with sentence pairs to detect relatedness.
As CEs require sentence pairs at inference, the prevailing view is that they
can only be used as re-rankers in information retrieval pipelines. Dual
encoders (DEs) are instead used to embed sentences, where sentence pairs are
encoded by two separate encoders with shared weights at training, and a loss
function that ensures the pair’s embeddings lie close in vector space if the
sentences are related. DEs however, require much larger datasets to train, and
are less accurate than CEs. We report a curious finding that embeddings from
earlier layers of CEs can in fact be used within an information retrieval
pipeline. We show how to exploit CEs to distill a lighter-weight DE, with a
5.15x speedup in inference time.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/acharya2025gloss/">Gloss: Generative Language Models With Semantic Search For Sequential Recommendation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Gloss: Generative Language Models With Semantic Search For Sequential Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Gloss: Generative Language Models With Semantic Search For Sequential Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Acharya Krishna, Petrov Aleksandr V., Ziani Juba</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 17th ACM Conference on Recommender Systems</td>
    <td>56</td>
    <td><p>We propose Generative Low-rank language model with Semantic Search (GLoSS), a generative recommendation framework that combines large language models with dense retrieval for sequential recommendation. Unlike prior methods such as GPT4Rec, which rely on lexical matching via BM25, GLoSS uses semantic search to retrieve relevant items beyond lexical matching. For query generation, we employ 4-bit quantized LlaMA-3 models fine-tuned with low-rank adaptation (LoRA), enabling efficient training and inference on modest hardware. We evaluate GLoSS on three real-world Amazon review datasets: Beauty, Toys, and Sports, and find that it achieves state-of-the-art performance. Compared to traditional ID-based baselines, GLoSS improves Recall@5 by 33.3%, 52.8%, and 15.2%, and NDCG@5 by 30.0%, 42.6%, and 16.1%, respectively. It also outperforms LLM-based recommenders such as P5, GPT4Rec, LlamaRec and E4SRec with Recall@5 gains of 4.3%, 22.8%, and 29.5%. Additionally, user segment evaluations show that GLoSS performs particularly well for cold-start users in the Amazon Toys and Sports datasets, and benefits from longer user histories in Amazon Beauty dataset, demonstrating robustness across different levels of interaction lengths.</p>
</td>
    <td>
      
        RecSys 
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/zhang2025discrete/">Discrete Scale-invariant Metric Learning For Efficient Collaborative Filtering</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Discrete Scale-invariant Metric Learning For Efficient Collaborative Filtering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Discrete Scale-invariant Metric Learning For Efficient Collaborative Filtering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang et al.</td> <!-- 🔧 You were missing this -->
    <td>Information Retrieval</td>
    <td>32</td>
    <td><p>Metric learning has attracted extensive interest for its ability to provide personalized recommendations based on the importance of observed user-item interactions. Current metric learning methods aim to push negative items away from the corresponding users and positive items by an absolute geometrical distance margin. However, items may come from imbalanced categories with different intra-class variations. Thus, the absolute distance margin may not be ideal for estimating the difference between user preferences over imbalanced items. To this end, we propose a new method, named discrete scale-invariant metric learning (DSIML), by adding binary constraints to users and items, which maps users and items into binary codes of a shared Hamming subspace to speed up the online recommendation. Specifically, we firstly propose a scale-invariant margin based on angles at the negative item points in the shared Hamming subspace. Then, we derive a scale-invariant triple hinge loss based on the margin. To capture more preference difference information, we integrate a pairwise ranking loss into the scale-invariant loss in the proposed model. Due to the difficulty of directly optimizing the mixed integer optimization problem formulated with \textit{log-sum-exp} functions, we seek to optimize its variational quadratic upper bound and learn hash codes with an alternating optimization strategy. Experiments on benchmark datasets clearly show that our proposed method is superior to competitive metric learning and hashing-based baselines for recommender systems. The implementation code is available at https://github.com/AnonyFeb/dsml.</p>
</td>
    <td>
      
        Text Retrieval 
      
        Recommender Systems 
      
        Distance Metric Learning 
      
        SIGIR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/yuan2025central/">Central Similarity Hashing For Efficient Image And Video Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Central Similarity Hashing For Efficient Image And Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Central Similarity Hashing For Efficient Image And Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yuan et al.</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>264</td>
    <td><p>Existing data-dependent hashing methods usually learn
hash functions from the pairwise or triplet data relationships, which only capture the data similarity locally, and
often suffer low learning efficiency and low collision rate.
In this work, we propose a new global similarity metric,
termed as central similarity, with which the hash codes for
similar data pairs are encouraged to approach a common
center and those for dissimilar pairs to converge to different centers, to improve hash learning efficiency and retrieval accuracy. We principally formulate the computation of the proposed central similarity metric by introducing a new concept, i.e. hash center that refers to a set
of data points scattered in the Hamming space with sufficient mutual distance between each other. We then provide an efficient method to construct well separated hash
centers by leveraging the Hadamard matrix and Bernoulli
distributions. Finally, we propose the Central Similarity
Hashing (CSH) that optimizes the central similarity between data points w.r.t. their hash centers instead of optimizing the local similarity. The CSH is generic and applicable to both image and video hashing. Extensive experiments on large-scale image and video retrieval demonstrate CSH can generate cohesive hash codes for similar
data pairs and dispersed hash codes for dissimilar pairs,
and achieve noticeable boost in retrieval performance, i.e.
3%-20% in mAP over the previous state-of-the-art. The
codes are in: https://github.com/yuanli2333/
Hadamard-Matrix-for-hashing</p>
</td>
    <td>
      
        Hashing Methods 
      
        Video Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/yin2025mstar/">MSTAR: Box-free Multi-query Scene Text Retrieval With Attention Recycling</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=MSTAR: Box-free Multi-query Scene Text Retrieval With Attention Recycling' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=MSTAR: Box-free Multi-query Scene Text Retrieval With Attention Recycling' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yin et al.</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition</td>
    <td>19</td>
    <td><p>Scene text retrieval has made significant progress with the assistance of accurate text localization. However, existing approaches typically require costly bounding box annotations for training. Besides, they mostly adopt a customized retrieval strategy but struggle to unify various types of queries to meet diverse retrieval needs. To address these issues, we introduce Muti-query Scene Text retrieval with Attention Recycling (MSTAR), a box-free approach for scene text retrieval. It incorporates progressive vision embedding to dynamically capture the multi-grained representation of texts and harmonizes free-style text queries with style-aware instructions. Additionally, a multi-instance matching module is integrated to enhance vision-language alignment. Furthermore, we build the Multi-Query Text Retrieval (MQTR) dataset, the first benchmark designed to evaluate the multi-query scene text retrieval capability of models, comprising four query types and 16k images. Extensive experiments demonstrate the superiority of our method across seven public datasets and the MQTR dataset. Notably, MSTAR marginally surpasses the previous state-of-the-art model by 6.4% in MAP on Total-Text while eliminating box annotation costs. Moreover, on the MQTR benchmark, MSTAR significantly outperforms the previous models by an average of 8.5%. The code and datasets are available at https://github.com/yingift/MSTAR.</p>
</td>
    <td>
      
        Text Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/yuan2025towards/">Towards Optimal Deep Hashing Via Policy Gradient</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Towards Optimal Deep Hashing Via Policy Gradient' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Towards Optimal Deep Hashing Via Policy Gradient' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yuan et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>27</td>
    <td><p>In this paper, we propose a simple yet effective relaxation free method to learn more effective binary codes via policy gradient for
scalable image search. While a variety of deep hashing methods have been
proposed in recent years, most of them are confronted by the dilemma
to obtain optimal binary codes in a truly end-to-end manner with nonsmooth sign activations. Unlike existing methods which usually employ a
general relaxation framework to adapt to the gradient-based algorithms,
our approach formulates the non-smooth part of the hashing network
as sampling with a stochastic policy, so that the retrieval performance
degradation caused by the relaxation can be avoided. Specifically, our
method directly generates the binary codes and maximizes the expectation of rewards for similarity preservation, where the network can be
trained directly via policy gradient. Hence, the differentiation challenge
for discrete optimization can be naturally addressed, which leads to effective gradients and binary codes. Extensive experimental results on three
benchmark datasets validate the effectiveness of the proposed method.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/zheng2025enhancing/">Enhancing Embedding Representation Stability In Recommendation Systems With Semantic ID</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Enhancing Embedding Representation Stability In Recommendation Systems With Semantic ID' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Enhancing Embedding Representation Stability In Recommendation Systems With Semantic ID' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zheng et al.</td> <!-- 🔧 You were missing this -->
    <td>Companion Proceedings of the ACM Web Conference 2024</td>
    <td>16</td>
    <td><p>The exponential growth of online content has posed significant challenges to
ID-based models in industrial recommendation systems, ranging from extremely
high cardinality and dynamically growing ID space, to highly skewed engagement
distributions, to prediction instability as a result of natural id life cycles
(e.g, the birth of new IDs and retirement of old IDs). To address these issues,
many systems rely on random hashing to handle the id space and control the
corresponding model parameters (i.e embedding table). However, this approach
introduces data pollution from multiple ids sharing the same embedding, leading
to degraded model performance and embedding representation instability.
  This paper examines these challenges and introduces Semantic ID prefix ngram,
a novel token parameterization technique that significantly improves the
performance of the original Semantic ID. Semantic ID prefix ngram creates
semantically meaningful collisions by hierarchically clustering items based on
their content embeddings, as opposed to random assignments. Through extensive
experimentation, we demonstrate that Semantic ID prefix ngram not only
addresses embedding instability but also significantly improves tail id
modeling, reduces overfitting, and mitigates representation shifts. We further
highlight the advantages of Semantic ID prefix ngram in attention-based models
that contextualize user histories, showing substantial performance
improvements. We also report our experience of integrating Semantic ID into
Meta production Ads Ranking system, leading to notable performance gains and
enhanced prediction stability in live deployments.</p>
</td>
    <td>
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/zhao2025note/">A Note On Efficient Privacy-preserving Similarity Search For Encrypted Vectors</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Note On Efficient Privacy-preserving Similarity Search For Encrypted Vectors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Note On Efficient Privacy-preserving Similarity Search For Encrypted Vectors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhao Dongfang</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Dependable and Secure Computing</td>
    <td>26</td>
    <td><p>Traditional approaches to vector similarity search over encrypted data rely
on fully homomorphic encryption (FHE) to enable computation without decryption.
However, the substantial computational overhead of FHE makes it impractical for
large-scale real-time applications. This work explores a more efficient
alternative: using additively homomorphic encryption (AHE) for
privacy-preserving similarity search. We consider scenarios where either the
query vector or the database vectors remain encrypted, a setting that
frequently arises in applications such as confidential recommender systems and
secure federated learning. While AHE only supports addition and scalar
multiplication, we show that it is sufficient to compute inner product
similarity–one of the most widely used similarity measures in vector
retrieval. Compared to FHE-based solutions, our approach significantly reduces
computational overhead by avoiding ciphertext-ciphertext multiplications and
bootstrapping, while still preserving correctness and privacy. We present an
efficient algorithm for encrypted similarity search under AHE and analyze its
error growth and security implications. Our method provides a scalable and
practical solution for privacy-preserving vector search in real-world machine
learning applications.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2025</td>
    <td>
      <a href="/publications/zou2025prompthash/">Prompthash: Affinity-prompted Collaborative Cross-modal Learning For Adaptive Hashing Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Prompthash: Affinity-prompted Collaborative Cross-modal Learning For Adaptive Hashing Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Prompthash: Affinity-prompted Collaborative Cross-modal Learning For Adaptive Hashing Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zou Qiang, Cheng Shuli, Chen Jiayi</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval</td>
    <td>7</td>
    <td><p>Cross-modal hashing is a promising approach for efficient data retrieval and
storage optimization. However, contemporary methods exhibit significant
limitations in semantic preservation, contextual integrity, and information
redundancy, which constrains retrieval efficacy. We present PromptHash, an
innovative framework leveraging affinity prompt-aware collaborative learning
for adaptive cross-modal hashing. We propose an end-to-end framework for
affinity-prompted collaborative hashing, with the following fundamental
technical contributions: (i) a text affinity prompt learning mechanism that
preserves contextual information while maintaining parameter efficiency, (ii)
an adaptive gated selection fusion architecture that synthesizes State Space
Model with Transformer network for precise cross-modal feature integration, and
(iii) a prompt affinity alignment strategy that bridges modal heterogeneity
through hierarchical contrastive learning. To the best of our knowledge, this
study presents the first investigation into affinity prompt awareness within
collaborative cross-modal adaptive hash learning, establishing a paradigm for
enhanced semantic consistency across modalities. Through comprehensive
evaluation on three benchmark multi-label datasets, PromptHash demonstrates
substantial performance improvements over existing approaches. Notably, on the
NUS-WIDE dataset, our method achieves significant gains of 18.22% and 18.65% in
image-to-text and text-to-image retrieval tasks, respectively. The code is
publicly available at https://github.com/ShiShuMo/PromptHash.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Multimodal Retrieval 
      
        Medical Retrieval 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/xu2024bi/">A Bi-metric Framework For Fast Similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Bi-metric Framework For Fast Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Bi-metric Framework For Fast Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xu Haike, Silwal Sandeep, Indyk Piotr</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>39</td>
    <td><p>We propose a new “bi-metric” framework for designing nearest neighbor data
structures. Our framework assumes two dissimilarity functions: a ground-truth
metric that is accurate but expensive to compute, and a proxy metric that is
cheaper but less accurate. In both theory and practice, we show how to
construct data structures using only the proxy metric such that the query
procedure achieves the accuracy of the expensive metric, while only using a
limited number of calls to both metrics. Our theoretical results instantiate
this framework for two popular nearest neighbor search algorithms: DiskANN and
Cover Tree. In both cases we show that, as long as the proxy metric used to
construct the data structure approximates the ground-truth metric up to a
bounded factor, our data structure achieves arbitrarily good approximation
guarantees with respect to the ground-truth metric. On the empirical side, we
apply the framework to the text retrieval problem with two dissimilarity
functions evaluated by ML models with vastly different computational costs. We
observe that for almost all data sets in the MTEB benchmark, our approach
achieves a considerably better accuracy-efficiency tradeoff than the
alternatives, such as re-ranking.</p>
</td>
    <td>
      
        Similarity Search 
      
        Tools & Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/xiao2024flair/">FLAIR: VLM With Fine-grained Language-informed Image Representations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=FLAIR: VLM With Fine-grained Language-informed Image Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=FLAIR: VLM With Fine-grained Language-informed Image Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xiao et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 International Conference on Document Analysis and Recognition Workshops (ICDARW)</td>
    <td>8</td>
    <td><p>CLIP has shown impressive results in aligning images and texts at scale.
However, its ability to capture detailed visual features remains limited
because CLIP matches images and texts at a global level. To address this issue,
we propose FLAIR, Fine-grained Language-informed Image Representations, an
approach that utilizes long and detailed image descriptions to learn localized
image embeddings. By sampling diverse sub-captions that describe fine-grained
details about an image, we train our vision-language model to produce not only
global embeddings but also text-specific image representations. Our model
introduces text-conditioned attention pooling on top of local image tokens to
produce fine-grained image representations that excel at retrieving detailed
image content. We achieve state-of-the-art performance on both, existing
multimodal retrieval benchmarks, as well as, our newly introduced fine-grained
retrieval task which evaluates vision-language models’ ability to retrieve
partial image content. Furthermore, our experiments demonstrate the
effectiveness of FLAIR trained on 30M image-text pairs in capturing
fine-grained visual information, including zero-shot semantic segmentation,
outperforming models trained on billions of pairs. Code is available at
https://github.com/ExplainableML/flair .</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/wu2024sign/">Sign-guided Bipartite Graph Hashing For Hamming Space Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Sign-guided Bipartite Graph Hashing For Hamming Space Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Sign-guided Bipartite Graph Hashing For Hamming Space Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu Xueyi</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the ACM Web Conference 2023</td>
    <td>10</td>
    <td><p>Bipartite graph hashing (BGH) is extensively used for Top-K search in Hamming
space at low storage and inference costs. Recent research adopts graph
convolutional hashing for BGH and has achieved the state-of-the-art
performance. However, the contributions of its various influencing factors to
hashing performance have not been explored in-depth, including the
same/different sign count between two binary embeddings during Hamming space
search (sign property), the contribution of sub-embeddings at each layer (model
property), the contribution of different node types in the bipartite graph
(node property), and the combination of augmentation methods. In this work, we
build a lightweight graph convolutional hashing model named LightGCH by mainly
removing the augmentation methods of the state-of-the-art model BGCH. By
analyzing the contributions of each layer and node type to performance, as well
as analyzing the Hamming similarity statistics at each layer, we find that the
actual neighbors in the bipartite graph tend to have low Hamming similarity at
the shallow layer, and all nodes tend to have high Hamming similarity at the
deep layers in LightGCH. To tackle these problems, we propose a novel
sign-guided framework SGBGH to make improvement, which uses sign-guided
negative sampling to improve the Hamming similarity of neighbors, and uses
sign-aware contrastive learning to help nodes learn more uniform
representations. Experimental results show that SGBGH outperforms BGCH and
LightGCH significantly in embedding quality.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/wong2024shotit/">Shotit: Compute-efficient Image-to-video Search Engine For The Cloud</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Shotit: Compute-efficient Image-to-video Search Engine For The Cloud' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Shotit: Compute-efficient Image-to-video Search Engine For The Cloud' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wong Leslie</td> <!-- 🔧 You were missing this -->
    <td>2014 IEEE International Conference on Image Processing (ICIP)</td>
    <td>15</td>
    <td><p>With the rapid growth of information technology, users are exposed to a
massive amount of data online, including image, music, and video. This has led
to strong needs to provide effective corresponsive search services such as
image, music, and video search services. Most of them are operated based on
keywords, namely using keywords to find related image, music, and video.
Additionally, there are image-to-image search services that enable users to
find similar images using one input image. Given that videos are essentially
composed of image frames, then similar videos can be searched by one input
image or screenshot. We want to target this scenario and provide an efficient
method and implementation in this paper.
  We present Shotit, a cloud-native image-to-video search engine that tailors
this search scenario in a compute-efficient approach. One main limitation faced
in this scenario is the scale of its dataset. A typical image-to-image search
engine only handles one-to-one relationships, colloquially, one image
corresponds to another single image. But image-to-video proliferates. Take a
24-min length video as an example, it will generate roughly 20,000 image
frames. As the number of videos grows, the scale of the dataset explodes
exponentially. In this case, a compute-efficient approach ought to be
considered, and the system design should cater to the cloud-native trend.
Choosing an emerging technology - vector database as its backbone, Shotit fits
these two metrics performantly. Experiments for two different datasets, a 50
thousand-scale Blender Open Movie dataset, and a 50 million-scale proprietary
TV genre dataset at a 4 Core 32GB RAM Intel Xeon Gold 6271C cloud machine with
object storage reveal the effectiveness of Shotit. A demo regarding the Blender
Open Movie dataset is illustrated within this paper.</p>
</td>
    <td>
      
        Video Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/tian2024fusionanns/">Fusionanns: An Efficient CPU/GPU Cooperative Processing Architecture For Billion-scale Approximate Nearest Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fusionanns: An Efficient CPU/GPU Cooperative Processing Architecture For Billion-scale Approximate Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fusionanns: An Efficient CPU/GPU Cooperative Processing Architecture For Billion-scale Approximate Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tian et al.</td> <!-- 🔧 You were missing this -->
    <td>2023 60th ACM/IEEE Design Automation Conference (DAC)</td>
    <td>7</td>
    <td><p>Approximate nearest neighbor search (ANNS) has emerged as a crucial component
of database and AI infrastructure. Ever-increasing vector datasets pose
significant challenges in terms of performance, cost, and accuracy for ANNS
services. None of modern ANNS systems can address these issues simultaneously.
We present FusionANNS, a high-throughput, low-latency, cost-efficient, and
high-accuracy ANNS system for billion-scale datasets using SSDs and only one
entry-level GPU. The key idea of FusionANNS lies in CPU/GPU collaborative
filtering and re-ranking mechanisms, which significantly reduce I/O operations
across CPUs, GPU, and SSDs to break through the I/O performance bottleneck.
Specifically, we propose three novel designs: (1) multi-tiered indexing to
avoid data swapping between CPUs and GPU, (2) heuristic re-ranking to eliminate
unnecessary I/Os and computations while guaranteeing high accuracy, and (3)
redundant-aware I/O deduplication to further improve I/O efficiency. We
implement FusionANNS and compare it with the state-of-the-art SSD-based ANNS
system – SPANN and GPU-accelerated in-memory ANNS system – RUMMY.
Experimental results show that FusionANNS achieves 1) 9.4-13.1X higher query
per second (QPS) and 5.7-8.8X higher cost efficiency compared with SPANN; 2)
and 2-4.9X higher QPS and 2.3-6.8X higher cost efficiency compared with RUMMY,
while guaranteeing low latency and high accuracy.</p>
</td>
    <td>
      
        Similarity Search 
      
        Large Scale Search 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/steck2024is/">Is Cosine-similarity Of Embeddings Really About Similarity?</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Is Cosine-similarity Of Embeddings Really About Similarity?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Is Cosine-similarity Of Embeddings Really About Similarity?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Steck Harald, Ekanadham Chaitanya, Kallus Nathan</td> <!-- 🔧 You were missing this -->
    <td>Companion Proceedings of the ACM Web Conference 2024</td>
    <td>27</td>
    <td><p>Cosine-similarity is the cosine of the angle between two vectors, or
equivalently the dot product between their normalizations. A popular
application is to quantify semantic similarity between high-dimensional objects
by applying cosine-similarity to a learned low-dimensional feature embedding.
This can work better but sometimes also worse than the unnormalized dot-product
between embedded vectors in practice. To gain insight into this empirical
observation, we study embeddings derived from regularized linear models, where
closed-form solutions facilitate analytical insights. We derive analytically
how cosine-similarity can yield arbitrary and therefore meaningless
`similarities.’ For some linear models the similarities are not even unique,
while for others they are implicitly controlled by the regularization. We
discuss implications beyond linear models: a combination of different
regularizations are employed when learning deep models; these have implicit and
unintended effects when taking cosine-similarities of the resulting embeddings,
rendering results opaque and possibly arbitrary. Based on these insights, we
caution against blindly using cosine-similarity and outline alternatives.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/tripathi2024honeybee/">Honeybee: A Scalable Modular Framework For Creating Multimodal Oncology Datasets With Foundational Embedding Models</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Honeybee: A Scalable Modular Framework For Creating Multimodal Oncology Datasets With Foundational Embedding Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Honeybee: A Scalable Modular Framework For Creating Multimodal Oncology Datasets With Foundational Embedding Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tripathi et al.</td> <!-- 🔧 You were missing this -->
    <td>Sensors</td>
    <td>9</td>
    <td><p>Developing accurate machine learning models for oncology requires
large-scale, high-quality multimodal datasets. However, creating such datasets
remains challenging due to the complexity and heterogeneity of medical data. To
address this challenge, we introduce HoneyBee, a scalable modular framework for
building multimodal oncology datasets that leverages foundation models to
generate representative embeddings. HoneyBee integrates various data
modalities, including clinical diagnostic and pathology imaging data, medical
notes, reports, records, and molecular data. It employs data preprocessing
techniques and foundation models to generate embeddings that capture the
essential features and relationships within the raw medical data. The generated
embeddings are stored in a structured format using Hugging Face datasets and
PyTorch dataloaders for accessibility. Vector databases enable efficient
querying and retrieval for machine learning applications. We demonstrate the
effectiveness of HoneyBee through experiments assessing the quality and
representativeness of these embeddings. The framework is designed to be
extensible to other medical domains and aims to accelerate oncology research by
providing high-quality, machine learning-ready datasets. HoneyBee is an ongoing
open-source effort, and the code, datasets, and models are available at the
project repository.</p>
</td>
    <td>
      
        Datasets 
      
        Tools & Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/schmidt2024evaluating/">Evaluating The Performance-deviation Of Itemknn In Recbole And Lenskit</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Evaluating The Performance-deviation Of Itemknn In Recbole And Lenskit' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Evaluating The Performance-deviation Of Itemknn In Recbole And Lenskit' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Schmidt Michael, Nitschke Jannik, Prinz Tim</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the fifth ACM conference on Recommender systems</td>
    <td>17</td>
    <td><p>This study examines the performance of item-based k-Nearest Neighbors
(ItemKNN) algorithms in the RecBole and LensKit recommender system libraries.
Using four data sets (Anime, Modcloth, ML-100K, and ML-1M), we assess each
library’s efficiency, accuracy, and scalability, focusing primarily on
normalized discounted cumulative gain (nDCG). Our results show that RecBole
outperforms LensKit on two of three metrics on the ML-100K data set: it
achieved an 18% higher nDCG, 14% higher precision, and 35% lower recall. To
ensure a fair comparison, we adjusted LensKit’s nDCG calculation to match
RecBole’s method. This alignment made the performance more comparable, with
LensKit achieving an nDCG of 0.2540 and RecBole 0.2674. Differences in
similarity matrix calculations were identified as the main cause of performance
deviations. After modifying LensKit to retain only the top K similar items,
both libraries showed nearly identical nDCG values across all data sets. For
instance, both achieved an nDCG of 0.2586 on the ML-1M data set with the same
random seed. Initially, LensKit’s original implementation only surpassed
RecBole in the ModCloth dataset.</p>
</td>
    <td>
      
        Recommender Systems 
      
        RecSys 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/saberi2024drew/">DREW : Towards Robust Data Provenance By Leveraging Error-controlled Watermarking</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=DREW : Towards Robust Data Provenance By Leveraging Error-controlled Watermarking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=DREW : Towards Robust Data Provenance By Leveraging Error-controlled Watermarking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Saberi et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE Symposium on Security and Privacy (SP)</td>
    <td>43</td>
    <td><p>Identifying the origin of data is crucial for data provenance, with
applications including data ownership protection, media forensics, and
detecting AI-generated content. A standard approach involves embedding-based
retrieval techniques that match query data with entries in a reference dataset.
However, this method is not robust against benign and malicious edits. To
address this, we propose Data Retrieval with Error-corrected codes and
Watermarking (DREW). DREW randomly clusters the reference dataset, injects
unique error-controlled watermark keys into each cluster, and uses these keys
at query time to identify the appropriate cluster for a given sample. After
locating the relevant cluster, embedding vector similarity retrieval is
performed within the cluster to find the most accurate matches. The integration
of error control codes (ECC) ensures reliable cluster assignments, enabling the
method to perform retrieval on the entire dataset in case the ECC algorithm
cannot detect the correct cluster with high confidence. This makes DREW
maintain baseline performance, while also providing opportunities for
performance improvements due to the increased likelihood of correctly matching
queries to their origin when performing retrieval on a smaller subset of the
dataset. Depending on the watermark technique used, DREW can provide
substantial improvements in retrieval accuracy (up to 40% for some datasets
and modification types) across multiple datasets and state-of-the-art embedding
models (e.g., DinoV2, CLIP), making our method a promising solution for secure
and reliable source identification. The code is available at
https://github.com/mehrdadsaberi/DREW</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/qiu2024hihpq/">Hihpq: Hierarchical Hyperbolic Product Quantization For Unsupervised Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hihpq: Hierarchical Hyperbolic Product Quantization For Unsupervised Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hihpq: Hierarchical Hyperbolic Product Quantization For Unsupervised Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Qiu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>5</td>
    <td><p>Existing unsupervised deep product quantization methods primarily aim for the
increased similarity between different views of the identical image, whereas
the delicate multi-level semantic similarities preserved between images are
overlooked. Moreover, these methods predominantly focus on the Euclidean space
for computational convenience, compromising their ability to map the
multi-level semantic relationships between images effectively. To mitigate
these shortcomings, we propose a novel unsupervised product quantization method
dubbed \textbf{Hi}erarchical \textbf{H}yperbolic \textbf{P}roduct
\textbf{Q}uantization (HiHPQ), which learns quantized representations by
incorporating hierarchical semantic similarity within hyperbolic geometry.
Specifically, we propose a hyperbolic product quantizer, where the hyperbolic
codebook attention mechanism and the quantized contrastive learning on the
hyperbolic product manifold are introduced to expedite quantization.
Furthermore, we propose a hierarchical semantics learning module, designed to
enhance the distinction between similar and non-matching images for a query by
utilizing the extracted hierarchical semantics as an additional training
supervision. Experiments on benchmarks show that our proposed method
outperforms state-of-the-art baselines.</p>
</td>
    <td>
      
        Quantization 
      
        Image Retrieval 
      
        Unsupervised 
      
        AAAI 
      
        SUPERVISED 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/qu2024automated/">Automated Similarity Metric Generation For Recommendation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Automated Similarity Metric Generation For Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Automated Similarity Metric Generation For Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Qu et al.</td> <!-- 🔧 You were missing this -->
    <td>International Journal of Electronic Commerce</td>
    <td>16</td>
    <td><p>The embedding-based architecture has become the dominant approach in modern
recommender systems, mapping users and items into a compact vector space. It
then employs predefined similarity metrics, such as the inner product, to
calculate similarity scores between user and item embeddings, thereby guiding
the recommendation of items that align closely with a user’s preferences. Given
the critical role of similarity metrics in recommender systems, existing
methods mainly employ handcrafted similarity metrics to capture the complex
characteristics of user-item interactions. Yet, handcrafted metrics may not
fully capture the diverse range of similarity patterns that can significantly
vary across different domains.
  To address this issue, we propose an Automated Similarity Metric Generation
method for recommendations, named AutoSMG, which can generate tailored
similarity metrics for various domains and datasets. Specifically, we first
construct a similarity metric space by sampling from a set of basic embedding
operators, which are then integrated into computational graphs to represent
metrics. We employ an evolutionary algorithm to search for the optimal metrics
within this metric space iteratively. To improve search efficiency, we utilize
an early stopping strategy and a surrogate model to approximate the performance
of candidate metrics instead of fully training models. Notably, our proposed
method is model-agnostic, which can seamlessly plugin into different
recommendation model architectures. The proposed method is validated on three
public recommendation datasets across various domains in the Top-K
recommendation task, and experimental results demonstrate that AutoSMG
outperforms both commonly used handcrafted metrics and those generated by other
search strategies.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/pham2024composing/">Composing Object Relations And Attributes For Image-text Matching</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Composing Object Relations And Attributes For Image-text Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Composing Object Relations And Attributes For Image-text Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Pham et al.</td> <!-- 🔧 You were missing this -->
    <td>2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>11</td>
    <td><p>We study the visual semantic embedding problem for image-text matching. Most
existing work utilizes a tailored cross-attention mechanism to perform local
alignment across the two image and text modalities. This is computationally
expensive, even though it is more powerful than the unimodal dual-encoder
approach. This work introduces a dual-encoder image-text matching model,
leveraging a scene graph to represent captions with nodes for objects and
attributes interconnected by relational edges. Utilizing a graph attention
network, our model efficiently encodes object-attribute and object-object
semantic relations, resulting in a robust and fast-performing system.
Representing caption as a scene graph offers the ability to utilize the strong
relational inductive bias of graph neural networks to learn object-attribute
and object-object relations effectively. To train the model, we propose losses
that align the image and caption both at the holistic level (image-caption) and
the local level (image-object entity), which we show is key to the success of
the model. Our model is termed Composition model for Object Relations and
Attributes, CORA. Experimental results on two prominent image-text retrieval
benchmarks, Flickr30K and MSCOCO, demonstrate that CORA outperforms existing
state-of-the-art computationally expensive cross-attention methods regarding
recall score while achieving fast computation speed of the dual encoder.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/paudel2024pixelmod/">PIXELMOD: Improving Soft Moderation Of Visual Misleading Information On Twitter</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=PIXELMOD: Improving Soft Moderation Of Visual Misleading Information On Twitter' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=PIXELMOD: Improving Soft Moderation Of Visual Misleading Information On Twitter' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Paudel et al.</td> <!-- 🔧 You were missing this -->
    <td>2023 IEEE Symposium on Security and Privacy (SP)</td>
    <td>5</td>
    <td><p>Images are a powerful and immediate vehicle to carry misleading or outright
false messages, yet identifying image-based misinformation at scale poses
unique challenges. In this paper, we present PIXELMOD, a system that leverages
perceptual hashes, vector databases, and optical character recognition (OCR) to
efficiently identify images that are candidates to receive soft moderation
labels on Twitter. We show that PIXELMOD outperforms existing image similarity
approaches when applied to soft moderation, with negligible performance
overhead. We then test PIXELMOD on a dataset of tweets surrounding the 2020 US
Presidential Election, and find that it is able to identify visually misleading
images that are candidates for soft moderation with 0.99% false detection and
2.06% false negatives.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/patel2024three/">Three Things To Know About Deep Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Three Things To Know About Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Three Things To Know About Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Patel Yash, Tolias Giorgos, Matas Jiri</td> <!-- 🔧 You were missing this -->
    <td>AMA Journal of Ethics</td>
    <td>39</td>
    <td><p>This paper addresses supervised deep metric learning for open-set image
retrieval, focusing on three key aspects: the loss function, mixup
regularization, and model initialization. In deep metric learning, optimizing
the retrieval evaluation metric, recall@k, via gradient descent is desirable
but challenging due to its non-differentiable nature. To overcome this, we
propose a differentiable surrogate loss that is computed on large batches,
nearly equivalent to the entire training set. This computationally intensive
process is made feasible through an implementation that bypasses the GPU memory
limitations. Additionally, we introduce an efficient mixup regularization
technique that operates on pairwise scalar similarities, effectively increasing
the batch size even further. The training process is further enhanced by
initializing the vision encoder using foundational models, which are
pre-trained on large-scale datasets. Through a systematic study of these
components, we demonstrate that their synergy enables large models to nearly
solve popular benchmarks.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/patel2024acorn/">ACORN: Performant And Predicate-agnostic Search Over Vector Embeddings And Structured Data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=ACORN: Performant And Predicate-agnostic Search Over Vector Embeddings And Structured Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=ACORN: Performant And Predicate-agnostic Search Over Vector Embeddings And Structured Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Patel et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the ACM on Management of Data</td>
    <td>13</td>
    <td><p>Applications increasingly leverage mixed-modality data, and must jointly
search over vector data, such as embedded images, text and video, as well as
structured data, such as attributes and keywords. Proposed methods for this
hybrid search setting either suffer from poor performance or support a severely
restricted set of search predicates (e.g., only small sets of equality
predicates), making them impractical for many applications. To address this, we
present ACORN, an approach for performant and predicate-agnostic hybrid search.
ACORN builds on Hierarchical Navigable Small Worlds (HNSW), a state-of-the-art
graph-based approximate nearest neighbor index, and can be implemented
efficiently by extending existing HNSW libraries. ACORN introduces the idea of
predicate subgraph traversal to emulate a theoretically ideal, but impractical,
hybrid search strategy. ACORN’s predicate-agnostic construction algorithm is
designed to enable this effective search strategy, while supporting a wide
array of predicate sets and query semantics. We systematically evaluate ACORN
on both prior benchmark datasets, with simple, low-cardinality predicate sets,
and complex multi-modal datasets not supported by prior methods. We show that
ACORN achieves state-of-the-art performance on all datasets, outperforming
prior methods with 2-1,000x higher throughput at a fixed recall.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/ootomo2024cagra/">CAGRA: Highly Parallel Graph Construction And Approximate Nearest Neighbor Search For Gpus</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=CAGRA: Highly Parallel Graph Construction And Approximate Nearest Neighbor Search For Gpus' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=CAGRA: Highly Parallel Graph Construction And Approximate Nearest Neighbor Search For Gpus' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ootomo et al.</td> <!-- 🔧 You were missing this -->
    <td>2024 IEEE 40th International Conference on Data Engineering (ICDE)</td>
    <td>7</td>
    <td><p>Approximate Nearest Neighbor Search (ANNS) plays a critical role in various
disciplines spanning data mining and artificial intelligence, from information
retrieval and computer vision to natural language processing and recommender
systems. Data volumes have soared in recent years and the computational cost of
an exhaustive exact nearest neighbor search is often prohibitive, necessitating
the adoption of approximate techniques. The balanced performance and recall of
graph-based approaches have more recently garnered significant attention in
ANNS algorithms, however, only a few studies have explored harnessing the power
of GPUs and multi-core processors despite the widespread use of massively
parallel and general-purpose computing. To bridge this gap, we introduce a
novel parallel computing hardware-based proximity graph and search algorithm.
By leveraging the high-performance capabilities of modern hardware, our
approach achieves remarkable efficiency gains. In particular, our method
surpasses existing CPU and GPU-based methods in constructing the proximity
graph, demonstrating higher throughput in both large- and small-batch searches
while maintaining compatible accuracy. In graph construction time, our method,
CAGRA, is 2.2~27x faster than HNSW, which is one of the CPU SOTA
implementations. In large-batch query throughput in the 90% to 95% recall
range, our method is 33~77x faster than HNSW, and is 3.8~8.8x faster than the
SOTA implementations for GPU. For a single query, our method is 3.4~53x faster
than HNSW at 95% recall.</p>
</td>
    <td>
      
        Similarity Search 
      
        Graph Based ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/nardini2024efficient/">Efficient Multi-vector Dense Retrieval Using Bit Vectors</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Multi-vector Dense Retrieval Using Bit Vectors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient Multi-vector Dense Retrieval Using Bit Vectors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Nardini Franco Maria, Rulli Cosimo, Venturini Rossano</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>10</td>
    <td><p>Dense retrieval techniques employ pre-trained large language models to build
a high-dimensional representation of queries and passages. These
representations compute the relevance of a passage w.r.t. to a query using
efficient similarity measures. In this line, multi-vector representations show
improved effectiveness at the expense of a one-order-of-magnitude increase in
memory footprint and query latency by encoding queries and documents on a
per-token level. Recently, PLAID has tackled these problems by introducing a
centroid-based term representation to reduce the memory impact of multi-vector
systems. By exploiting a centroid interaction mechanism, PLAID filters out
non-relevant documents, thus reducing the cost of the successive ranking
stages. This paper proposes ``Efficient Multi-Vector dense retrieval with Bit
vectors’’ (EMVB), a novel framework for efficient query processing in
multi-vector dense retrieval. First, EMVB employs a highly efficient
pre-filtering step of passages using optimized bit vectors. Second, the
computation of the centroid interaction happens column-wise, exploiting SIMD
instructions, thus reducing its latency. Third, EMVB leverages Product
Quantization (PQ) to reduce the memory footprint of storing vector
representations while jointly allowing for fast late interaction. Fourth, we
introduce a per-document term filtering method that further improves the
efficiency of the last step. Experiments on MS MARCO and LoTTE show that EMVB
is up to 2.8x faster while reducing the memory footprint by 1.8x with no loss
in retrieval accuracy compared to PLAID.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/miao2024locality/">Locality-sensitive Hashing-based Efficient Point Transformer With Applications In High-energy Physics</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Locality-sensitive Hashing-based Efficient Point Transformer With Applications In High-energy Physics' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Locality-sensitive Hashing-based Efficient Point Transformer With Applications In High-energy Physics' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Miao et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 21st ACM international conference on Information and knowledge management</td>
    <td>52</td>
    <td><p>This study introduces a novel transformer model optimized for large-scale
point cloud processing in scientific domains such as high-energy physics (HEP)
and astrophysics. Addressing the limitations of graph neural networks and
standard transformers, our model integrates local inductive bias and achieves
near-linear complexity with hardware-friendly regular operations. One
contribution of this work is the quantitative analysis of the error-complexity
tradeoff of various sparsification techniques for building efficient
transformers. Our findings highlight the superiority of using
locality-sensitive hashing (LSH), especially OR &amp; AND-construction LSH, in
kernel approximation for large-scale point cloud data with local inductive
bias. Based on this finding, we propose LSH-based Efficient Point Transformer
(HEPT), which combines E\(^2\)LSH with OR &amp; AND constructions and is built upon
regular computations. HEPT demonstrates remarkable performance on two critical
yet time-consuming HEP tasks, significantly outperforming existing GNNs and
transformers in accuracy and computational speed, marking a significant
advancement in geometric deep learning and large-scale scientific data
processing. Our code is available at https://github.com/Graph-COM/HEPT.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Hashing Methods 
      
        CIKM 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/mandarapu2024arkade/">Arkade: K-nearest Neighbor Search With Non-euclidean Distances Using GPU Ray Tracing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Arkade: K-nearest Neighbor Search With Non-euclidean Distances Using GPU Ray Tracing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Arkade: K-nearest Neighbor Search With Non-euclidean Distances Using GPU Ray Tracing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mandarapu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 38th ACM International Conference on Supercomputing</td>
    <td>5</td>
    <td><p>High-performance implementations of \(k\)-Nearest Neighbor Search (\(k\)NN) in
low dimensions use tree-based data structures. Tree algorithms are hard to
parallelize on GPUs due to their irregularity. However, newer Nvidia GPUs offer
hardware support for tree operations through ray-tracing cores. Recent works
have proposed using RT cores to implement \(k\)NN search, but they all have a
hardware-imposed constraint on the distance metric used in the search – the
Euclidean distance. We propose and implement two reductions to support \(k\)NN
for a broad range of distances other than the Euclidean distance: Arkade
Filter-Refine and Arkade Monotone Transformation, each of which allows
non-Euclidean distance-based nearest neighbor queries to be performed in terms
of the Euclidean distance. With our reductions, we observe that \(k\)NN search
time speedups range between \(1.6\)x-\(200\)x and \(1.3\)x-\(33.1\)x over various
state-of-the-art GPU shader core and RT core baselines, respectively. In
evaluation, we provide several insights on RT architectures’ ability to
efficiently build and traverse the tree by analyzing the \(k\)NN search time
trends.</p>
</td>
    <td>
      
        Similarity Search 
      
        Distance Metric Learning 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/manohar2024parlayann/">Parlayann: Scalable And Deterministic Parallel Graph-based Approximate Nearest Neighbor Search Algorithms</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Parlayann: Scalable And Deterministic Parallel Graph-based Approximate Nearest Neighbor Search Algorithms' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Parlayann: Scalable And Deterministic Parallel Graph-based Approximate Nearest Neighbor Search Algorithms' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Manohar et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 29th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming</td>
    <td>5</td>
    <td><p>Approximate nearest-neighbor search (ANNS) algorithms are a key part of the
modern deep learning stack due to enabling efficient similarity search over
high-dimensional vector space representations (i.e., embeddings) of data. Among
various ANNS algorithms, graph-based algorithms are known to achieve the best
throughput-recall tradeoffs. Despite the large scale of modern ANNS datasets,
existing parallel graph based implementations suffer from significant
challenges to scale to large datasets due to heavy use of locks and other
sequential bottlenecks, which 1) prevents them from efficiently scaling to a
large number of processors, and 2) results in nondeterminism that is
undesirable in certain applications.
  In this paper, we introduce ParlayANN, a library of deterministic and
parallel graph-based approximate nearest neighbor search algorithms, along with
a set of useful tools for developing such algorithms. In this library, we
develop novel parallel implementations for four state-of-the-art graph-based
ANNS algorithms that scale to billion-scale datasets. Our algorithms are
deterministic and achieve high scalability across a diverse set of challenging
datasets. In addition to the new algorithmic ideas, we also conduct a detailed
experimental study of our new algorithms as well as two existing non-graph
approaches. Our experimental results both validate the effectiveness of our new
techniques, and lead to a comprehensive comparison among ANNS algorithms on
large scale datasets with a list of interesting findings.</p>
</td>
    <td>
      
        Similarity Search 
      
        Graph Based ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/ma2024direction/">Direction-oriented Visual-semantic Embedding Model For Remote Sensing Image-text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Direction-oriented Visual-semantic Embedding Model For Remote Sensing Image-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Direction-oriented Visual-semantic Embedding Model For Remote Sensing Image-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ma Qing, Pan Jiancheng, Bai Cong</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Geoscience and Remote Sensing</td>
    <td>8</td>
    <td><p>Image-text retrieval has developed rapidly in recent years. However, it is
still a challenge in remote sensing due to visual-semantic imbalance, which
leads to incorrect matching of non-semantic visual and textual features. To
solve this problem, we propose a novel Direction-Oriented Visual-semantic
Embedding Model (DOVE) to mine the relationship between vision and language.
Our highlight is to conduct visual and textual representations in latent space,
directing them as close as possible to a redundancy-free regional visual
representation. Concretely, a Regional-Oriented Attention Module (ROAM)
adaptively adjusts the distance between the final visual and textual embeddings
in the latent semantic space, oriented by regional visual features. Meanwhile,
a lightweight Digging Text Genome Assistant (DTGA) is designed to expand the
range of tractable textual representation and enhance global word-level
semantic connections using less attention operations. Ultimately, we exploit a
global visual-semantic constraint to reduce single visual dependency and serve
as an external constraint for the final visual and textual representations. The
effectiveness and superiority of our method are verified by extensive
experiments including parameter evaluation, quantitative comparison, ablation
studies and visual analysis, on two benchmark datasets, RSICD and RSITMD.</p>
</td>
    <td>
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/luo2024learning/">Learning To Hash For Recommendation: A Survey</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning To Hash For Recommendation: A Survey' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning To Hash For Recommendation: A Survey' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Luo et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>705</td>
    <td><p>With the explosive growth of users and items, Recommender Systems (RS) are
facing unprecedented challenges on both retrieval efficiency and storage cost.
Fortunately, Learning to Hash (L2H) techniques have been shown as a promising
solution to address the two dilemmas, whose core idea is encoding
high-dimensional data into compact hash codes. To this end, L2H for RS (HashRec
for short) has recently received widespread attention to support large-scale
recommendations. In this survey, we present a comprehensive review of current
HashRec algorithms. Specifically, we first introduce the commonly used
two-tower models in the recall stage and identify two search strategies
frequently employed in L2H. Then, we categorize prior works into two-tier
taxonomy based on: (i) the type of loss function and (ii) the optimization
strategy. We also introduce some commonly used evaluation metrics to measure
the performance of HashRec algorithms. Finally, we shed light on the
limitations of the current research and outline the future research directions.
Furthermore, the summary of HashRec methods reviewed in this survey can be
found at
\href{https://github.com/Luo-Fangyuan/HashRec}{https://github.com/Luo-Fangyuan/HashRec}.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Survey Paper 
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/luo2024fine/">Fine-grained Embedding Dimension Optimization During Training For Recommender Systems</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fine-grained Embedding Dimension Optimization During Training For Recommender Systems' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fine-grained Embedding Dimension Optimization During Training For Recommender Systems' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Luo et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Web Conference 2021</td>
    <td>38</td>
    <td><p>Huge embedding tables in modern deep learning recommender models (DLRM)
require prohibitively large memory during training and inference. This paper
proposes FIITED, a system to automatically reduce the memory footprint via
FIne-grained In-Training Embedding Dimension pruning. By leveraging the key
insight that embedding vectors are not equally important, FIITED adaptively
adjusts the dimension of each individual embedding vector during model
training, assigning larger dimensions to more important embeddings while
adapting to dynamic changes in data. We prioritize embedding dimensions with
higher frequencies and gradients as more important. To enable efficient pruning
of embeddings and their dimensions during model training, we propose an
embedding storage system based on virtually-hashed physically-indexed hash
tables. Experiments on two industry models and months of realistic datasets
show that FIITED can reduce DLRM embedding size by more than 65% while
preserving model quality, outperforming state-of-the-art in-training embedding
pruning methods. On public datasets, FIITED can reduce the size of embedding
tables by 2.1x to 800x with negligible accuracy drop, while improving model
throughput.</p>
</td>
    <td>
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/lyou2024modality/">Modality-aware Representation Learning For Zero-shot Sketch-based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Modality-aware Representation Learning For Zero-shot Sketch-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Modality-aware Representation Learning For Zero-shot Sketch-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lyou et al.</td> <!-- 🔧 You were missing this -->
    <td>2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>5</td>
    <td><p>Zero-shot learning offers an efficient solution for a machine learning model
to treat unseen categories, avoiding exhaustive data collection. Zero-shot
Sketch-based Image Retrieval (ZS-SBIR) simulates real-world scenarios where it
is hard and costly to collect paired sketch-photo samples. We propose a novel
framework that indirectly aligns sketches and photos by contrasting them
through texts, removing the necessity of access to sketch-photo pairs. With an
explicit modality encoding learned from data, our approach disentangles
modality-agnostic semantics from modality-specific information, bridging the
modality gap and enabling effective cross-modal content retrieval within a
joint latent space. From comprehensive experiments, we verify the efficacy of
the proposed model on ZS-SBIR, and it can be also applied to generalized and
fine-grained settings.</p>
</td>
    <td>
      
        Image Retrieval 
      
        Few Shot & Zero Shot 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/long2024cfir/">CFIR: Fast And Effective Long-text To Image Retrieval For Large Corpora</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=CFIR: Fast And Effective Long-text To Image Retrieval For Large Corpora' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=CFIR: Fast And Effective Long-text To Image Retrieval For Large Corpora' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Long et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>5</td>
    <td><p>Text-to-image retrieval aims to find the relevant images based on a text
query, which is important in various use-cases, such as digital libraries,
e-commerce, and multimedia databases. Although Multimodal Large Language Models
(MLLMs) demonstrate state-of-the-art performance, they exhibit limitations in
handling large-scale, diverse, and ambiguous real-world needs of retrieval, due
to the computation cost and the injective embeddings they produce. This paper
presents a two-stage Coarse-to-Fine Index-shared Retrieval (CFIR) framework,
designed for fast and effective large-scale long-text to image retrieval. The
first stage, Entity-based Ranking (ER), adapts to long-text query ambiguity by
employing a multiple-queries-to-multiple-targets paradigm, facilitating
candidate filtering for the next stage. The second stage, Summary-based
Re-ranking (SR), refines these rankings using summarized queries. We also
propose a specialized Decoupling-BEiT-3 encoder, optimized for handling
ambiguous user needs and both stages, which also enhances computational
efficiency through vector-based similarity inference. Evaluation on the AToMiC
dataset reveals that CFIR surpasses existing MLLMs by up to 11.06% in
Recall@1000, while reducing training and retrieval times by 68.75% and 99.79%,
respectively. We will release our code to facilitate future research at
https://github.com/longkukuhi/CFIR.</p>
</td>
    <td>
      
        SIGIR 
      
        Text Retrieval 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/liu2024bi/">Bi-directional Training For Composed Image Retrieval Via Text Prompt Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Bi-directional Training For Composed Image Retrieval Via Text Prompt Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Bi-directional Training For Composed Image Retrieval Via Text Prompt Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>13</td>
    <td><p>Composed image retrieval searches for a target image based on a multi-modal
user query comprised of a reference image and modification text describing the
desired changes. Existing approaches to solving this challenging task learn a
mapping from the (reference image, modification text)-pair to an image
embedding that is then matched against a large image corpus. One area that has
not yet been explored is the reverse direction, which asks the question, what
reference image when modified as described by the text would produce the given
target image? In this work we propose a bi-directional training scheme that
leverages such reversed queries and can be applied to existing composed image
retrieval architectures with minimum changes, which improves the performance of
the model. To encode the bi-directional query we prepend a learnable token to
the modification text that designates the direction of the query and then
finetune the parameters of the text embedding module. We make no other changes
to the network architecture. Experiments on two standard datasets show that our
novel approach achieves improved performance over a baseline BLIP-based model
that itself already achieves competitive performance. Our code is released at
https://github.com/Cuberick-Orion/Bi-Blip4CIR.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/liu2024air/">Air-hloc: Adaptive Retrieved Images Selection For Efficient Visual Localisation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Air-hloc: Adaptive Retrieved Images Selection For Efficient Visual Localisation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Air-hloc: Adaptive Retrieved Images Selection For Efficient Visual Localisation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>17</td>
    <td><p>State-of-the-art hierarchical localisation pipelines (HLoc) employ image
retrieval (IR) to establish 2D-3D correspondences by selecting the top-\(k\) most
similar images from a reference database. While increasing \(k\) improves
localisation robustness, it also linearly increases computational cost and
runtime, creating a significant bottleneck. This paper investigates the
relationship between global and local descriptors, showing that greater
similarity between the global descriptors of query and database images
increases the proportion of feature matches. Low similarity queries
significantly benefit from increasing \(k\), while high similarity queries
rapidly experience diminishing returns. Building on these observations, we
propose an adaptive strategy that adjusts \(k\) based on the similarity between
the query’s global descriptor and those in the database, effectively mitigating
the feature-matching bottleneck. Our approach optimizes processing time without
sacrificing accuracy. Experiments on three indoor and outdoor datasets show
that AIR-HLoc reduces feature matching time by up to 30%, while preserving
state-of-the-art accuracy. The results demonstrate that AIR-HLoc facilitates a
latency-sensitive localisation system.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/li2024mixed/">Mixed-precision Embeddings For Large-scale Recommendation Models</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Mixed-precision Embeddings For Large-scale Recommendation Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Mixed-precision Embeddings For Large-scale Recommendation Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li et al.</td> <!-- 🔧 You were missing this -->
    <td>Fourteenth ACM Conference on Recommender Systems</td>
    <td>89</td>
    <td><p>Embedding techniques have become essential components of large databases in
the deep learning era. By encoding discrete entities, such as words, items, or
graph nodes, into continuous vector spaces, embeddings facilitate more
efficient storage, retrieval, and processing in large databases. Especially in
the domain of recommender systems, millions of categorical features are encoded
as unique embedding vectors, which facilitates the modeling of similarities and
interactions among features. However, numerous embedding vectors can result in
significant storage overhead. In this paper, we aim to compress the embedding
table through quantization techniques. Given that features vary in importance
levels, we seek to identify an appropriate precision for each feature to
balance model accuracy and memory usage. To this end, we propose a novel
embedding compression method, termed Mixed-Precision Embeddings (MPE).
Specifically, to reduce the size of the search space, we first group features
by frequency and then search precision for each feature group. MPE further
learns the probability distribution over precision levels for each feature
group, which can be used to identify the most suitable precision with a
specially designed sampling strategy. Extensive experiments on three public
datasets demonstrate that MPE significantly outperforms existing embedding
compression methods. Remarkably, MPE achieves about 200x compression on the
Criteo dataset without comprising the prediction accuracy.</p>
</td>
    <td>
      
        Recommender Systems 
      
        RecSys 
      
        Evaluation 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/lerner2024cross/">Cross-modal Retrieval For Knowledge-based Visual Question Answering</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-modal Retrieval For Knowledge-based Visual Question Answering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-modal Retrieval For Knowledge-based Visual Question Answering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lerner Paul List, Ferret Olivier List, Guinaudeau Camille</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>6</td>
    <td><p>Knowledge-based Visual Question Answering about Named Entities is a
challenging task that requires retrieving information from a multimodal
Knowledge Base. Named entities have diverse visual representations and are
therefore difficult to recognize. We argue that cross-modal retrieval may help
bridge the semantic gap between an entity and its depictions, and is foremost
complementary with mono-modal retrieval. We provide empirical evidence through
experiments with a multimodal dual encoder, namely CLIP, on the recent ViQuAE,
InfoSeek, and Encyclopedic-VQA datasets. Additionally, we study three different
strategies to fine-tune such a model: mono-modal, cross-modal, or joint
training. Our method, which combines mono-and cross-modal retrieval, is
competitive with billion-parameter models on the three datasets, while being
conceptually simpler and computationally cheaper.</p>
</td>
    <td>
      
        Graph Based ANN 
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/lee2024unified/">Unified Multimodal Interleaved Document Representation For Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unified Multimodal Interleaved Document Representation For Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unified Multimodal Interleaved Document Representation For Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lee et al.</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition</td>
    <td>40</td>
    <td><p>Information Retrieval (IR) methods aim to identify documents relevant to a
query, which have been widely applied in various natural language tasks.
However, existing approaches typically consider only the textual content within
documents, overlooking the fact that documents can contain multiple modalities,
including images and tables. Also, they often segment each long document into
multiple discrete passages for embedding, which prevents them from capturing
the overall document context and interactions between paragraphs. To address
these two challenges, we propose a method that holistically embeds documents
interleaved with multiple modalities by leveraging the capability of recent
vision-language models that enable the processing and integration of text,
images, and tables into a unified format and representation. Moreover, to
mitigate the information loss from segmenting documents into passages, instead
of representing and retrieving passages individually, we further merge the
representations of segmented passages into one single document representation,
while we additionally introduce a reranking strategy to decouple and identify
the relevant passage within the document if necessary. Then, through extensive
experiments on diverse IR scenarios considering both the textual and multimodal
queries, we show that our approach substantially outperforms relevant
baselines, thanks to the consideration of the multimodal information within
documents.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/kan2024object/">Object Retrieval For Visual Question Answering With Outside Knowledge</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Object Retrieval For Visual Question Answering With Outside Knowledge' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Object Retrieval For Visual Question Answering With Outside Knowledge' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kan et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>17</td>
    <td><p>Retrieval-augmented generation (RAG) with large language models (LLMs) plays a crucial role in question answering, as LLMs possess limited knowledge and are not updated with continuously growing information. Most recent work on RAG has focused primarily on text-based or large-image retrieval, which constrains the broader application of RAG models. We recognize that object-level retrieval is essential for addressing questions that extend beyond image content. To tackle this issue, we propose a task of object retrieval for visual question answering with outside knowledge (OR-OK-VQA), aimed to extend image-based content understanding in conjunction with LLMs. A key challenge in this task is retrieving diverse objects-related images that contribute to answering the questions. To enable accurate and robust general object retrieval, it is necessary to learn embeddings for local objects. This paper introduces a novel unsupervised deep feature embedding technique called multi-scale group collaborative embedding learning (MS-GCEL), developed to learn embeddings for long-tailed objects at different scales. Additionally, we establish an OK-VQA evaluation benchmark using images from the BelgaLogos, Visual Genome, and LVIS datasets. Prior to the OK-VQA evaluation, we construct a benchmark of challenges utilizing objects extracted from the COCO 2017 and VOC 2007 datasets to support the training and evaluation of general object retrieval models. Our evaluations on both general object retrieval and OK-VQA demonstrate the effectiveness of the proposed approach. The code and dataset will be publicly released for future research.</p>
</td>
    <td>
      
        SIGIR 
      
        Graph Based ANN 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/juvekar2024cos/">Cos-mix: Cosine Similarity And Distance Fusion For Improved Information Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cos-mix: Cosine Similarity And Distance Fusion For Improved Information Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cos-mix: Cosine Similarity And Distance Fusion For Improved Information Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Juvekar Kush, Purwar Anupam</td> <!-- 🔧 You were missing this -->
    <td>2012 7th International Conference on Computer Science &amp; Education (ICCSE)</td>
    <td>12</td>
    <td><p>This study proposes a novel hybrid retrieval strategy for Retrieval-Augmented
Generation (RAG) that integrates cosine similarity and cosine distance measures
to improve retrieval performance, particularly for sparse data. The traditional
cosine similarity measure is widely used to capture the similarity between
vectors in high-dimensional spaces. However, it has been shown that this
measure can yield arbitrary results in certain scenarios. To address this
limitation, we incorporate cosine distance measures to provide a complementary
perspective by quantifying the dissimilarity between vectors. Our approach is
experimented on proprietary data, unlike recent publications that have used
open-source datasets. The proposed method demonstrates enhanced retrieval
performance and provides a more comprehensive understanding of the semantic
relationships between documents or items. This hybrid strategy offers a
promising solution for efficiently and accurately retrieving relevant
information in knowledge-intensive applications, leveraging techniques such as
BM25 (sparse) retrieval , vector (Dense) retrieval, and cosine distance based
retrieval to facilitate efficient information retrieval.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/jush2024medical/">Medical Image Retrieval Using Pretrained Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Medical Image Retrieval Using Pretrained Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Medical Image Retrieval Using Pretrained Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jush et al.</td> <!-- 🔧 You were missing this -->
    <td>2024 IEEE International Symposium on Biomedical Imaging (ISBI)</td>
    <td>5</td>
    <td><p>A wide range of imaging techniques and data formats available for medical
images make accurate retrieval from image databases challenging.
  Efficient retrieval systems are crucial in advancing medical research,
enabling large-scale studies and innovative diagnostic tools. Thus, addressing
the challenges of medical image retrieval is essential for the continued
enhancement of healthcare and research.
  In this study, we evaluated the feasibility of employing four
state-of-the-art pretrained models for medical image retrieval at modality,
body region, and organ levels and compared the results of two similarity
indexing approaches. Since the employed networks take 2D images, we analyzed
the impacts of weighting and sampling strategies to incorporate 3D information
during retrieval of 3D volumes. We showed that medical image retrieval is
feasible using pretrained networks without any additional training or
fine-tuning steps. Using pretrained embeddings, we achieved a recall of 1 for
various tasks at modality, body region, and organ level.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/jedidi2024zero/">Zero-shot Dense Retrieval With Embeddings From Relevance Feedback</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Zero-shot Dense Retrieval With Embeddings From Relevance Feedback' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Zero-shot Dense Retrieval With Embeddings From Relevance Feedback' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jedidi et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</td>
    <td>77</td>
    <td><p>Building effective dense retrieval systems remains difficult when relevance
supervision is not available. Recent work has looked to overcome this challenge
by using a Large Language Model (LLM) to generate hypothetical documents that
can be used to find the closest real document. However, this approach relies
solely on the LLM to have domain-specific knowledge relevant to the query,
which may not be practical. Furthermore, generating hypothetical documents can
be inefficient as it requires the LLM to generate a large number of tokens for
each query. To address these challenges, we introduce Real Document Embeddings
from Relevance Feedback (ReDE-RF). Inspired by relevance feedback, ReDE-RF
proposes to re-frame hypothetical document generation as a relevance estimation
task, using an LLM to select which documents should be used for nearest
neighbor search. Through this re-framing, the LLM no longer needs
domain-specific knowledge but only needs to judge what is relevant.
Additionally, relevance estimation only requires the LLM to output a single
token, thereby improving search latency. Our experiments show that ReDE-RF
consistently surpasses state-of-the-art zero-shot dense retrieval methods
across a wide range of low-resource retrieval datasets while also making
significant improvements in latency per-query.</p>
</td>
    <td>
      
        Few Shot & Zero Shot 
      
        ACL 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/islam2024spatially/">Spatially Optimized Compact Deep Metric Learning Model For Similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Spatially Optimized Compact Deep Metric Learning Model For Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Spatially Optimized Compact Deep Metric Learning Model For Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Islam et al.</td> <!-- 🔧 You were missing this -->
    <td>Information Fusion</td>
    <td>61</td>
    <td><p>Spatial optimization is often overlooked in many computer vision tasks.
Filters should be able to recognize the features of an object regardless of
where it is in the image. Similarity search is a crucial task where spatial
features decide an important output. The capacity of convolution to capture
visual patterns across various locations is limited. In contrast to
convolution, the involution kernel is dynamically created at each pixel based
on the pixel value and parameters that have been learned. This study
demonstrates that utilizing a single layer of involution feature extractor
alongside a compact convolution model significantly enhances the performance of
similarity search. Additionally, we improve predictions by using the GELU
activation function rather than the ReLU. The negligible amount of weight
parameters in involution with a compact model with better performance makes the
model very useful in real-world implementations. Our proposed model is below 1
megabyte in size. We have experimented with our proposed methodology and other
models on CIFAR-10, FashionMNIST, and MNIST datasets. Our proposed method
outperforms across all three datasets.</p>
</td>
    <td>
      
        Similarity Search 
      
        Distance Metric Learning 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/iijima2024multimodal/">A Multimodal Approach For Cross-domain Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Multimodal Approach For Cross-domain Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Multimodal Approach For Cross-domain Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Iijima Lucas, Giakoumoglou Nikolaos, Stathaki Tania</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>32</td>
    <td><p>Cross-Domain Image Retrieval (CDIR) is a challenging task in computer vision,
aiming to match images across different visual domains such as sketches,
paintings, and photographs. Traditional approaches focus on visual image
features and rely heavily on supervised learning with labeled data and
cross-domain correspondences, which leads to an often struggle with the
significant domain gap. This paper introduces a novel unsupervised approach to
CDIR that incorporates textual context by leveraging pre-trained
vision-language models. Our method, dubbed as Caption-Matching (CM), uses
generated image captions as a domain-agnostic intermediate representation,
enabling effective cross-domain similarity computation without the need for
labeled data or fine-tuning. We evaluate our method on standard CDIR benchmark
datasets, demonstrating state-of-the-art performance in unsupervised settings
with improvements of 24.0% on Office-Home and 132.2% on DomainNet over previous
methods. We also demonstrate our method’s effectiveness on a dataset of
AI-generated images from Midjourney, showcasing its ability to handle complex,
multi-domain queries.</p>
</td>
    <td>
      
        Image Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/huang2024unsupervised/">Unsupervised Multilingual Dense Retrieval Via Generative Pseudo Labeling</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Multilingual Dense Retrieval Via Generative Pseudo Labeling' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Multilingual Dense Retrieval Via Generative Pseudo Labeling' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Huang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</td>
    <td>33</td>
    <td><p>Dense retrieval methods have demonstrated promising performance in
multilingual information retrieval, where queries and documents can be in
different languages. However, dense retrievers typically require a substantial
amount of paired data, which poses even greater challenges in multilingual
scenarios. This paper introduces UMR, an Unsupervised Multilingual dense
Retriever trained without any paired data. Our approach leverages the sequence
likelihood estimation capabilities of multilingual language models to acquire
pseudo labels for training dense retrievers. We propose a two-stage framework
which iteratively improves the performance of multilingual dense retrievers.
Experimental results on two benchmark datasets show that UMR outperforms
supervised baselines, showcasing the potential of training multilingual
retrievers without paired data, thereby enhancing their practicality. Our
source code, data, and models are publicly available at
https://github.com/MiuLab/UMR</p>
</td>
    <td>
      
        EACL 
      
        Unsupervised 
      
        NAACL 
      
        ACL 
      
        SUPERVISED 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/huang2024cross/">Cross-modal And Uni-modal Soft-label Alignment For Image-text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-modal And Uni-modal Soft-label Alignment For Image-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-modal And Uni-modal Soft-label Alignment For Image-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Huang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>9</td>
    <td><p>Current image-text retrieval methods have demonstrated impressive performance
in recent years. However, they still face two problems: the inter-modal
matching missing problem and the intra-modal semantic loss problem. These
problems can significantly affect the accuracy of image-text retrieval. To
address these challenges, we propose a novel method called Cross-modal and
Uni-modal Soft-label Alignment (CUSA). Our method leverages the power of
uni-modal pre-trained models to provide soft-label supervision signals for the
image-text retrieval model. Additionally, we introduce two alignment
techniques, Cross-modal Soft-label Alignment (CSA) and Uni-modal Soft-label
Alignment (USA), to overcome false negatives and enhance similarity recognition
between uni-modal samples. Our method is designed to be plug-and-play, meaning
it can be easily applied to existing image-text retrieval models without
changing their original architectures. Extensive experiments on various
image-text retrieval models and datasets, we demonstrate that our method can
consistently improve the performance of image-text retrieval and achieve new
state-of-the-art results. Furthermore, our method can also boost the uni-modal
retrieval performance of image-text retrieval models, enabling it to achieve
universal retrieval. The code and supplementary files can be found at
https://github.com/lerogo/aaai24_itr_cusa.</p>
</td>
    <td>
      
        Text Retrieval 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/ji2024hierarchical/">Hierarchical Matching And Reasoning For Multi-query Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hierarchical Matching And Reasoning For Multi-query Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hierarchical Matching And Reasoning For Multi-query Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ji et al.</td> <!-- 🔧 You were missing this -->
    <td>Neural Networks</td>
    <td>8</td>
    <td><p>As a promising field, Multi-Query Image Retrieval (MQIR) aims at searching
for the semantically relevant image given multiple region-specific text
queries. Existing works mainly focus on a single-level similarity between image
regions and text queries, which neglects the hierarchical guidance of
multi-level similarities and results in incomplete alignments. Besides, the
high-level semantic correlations that intrinsically connect different
region-query pairs are rarely considered. To address above limitations, we
propose a novel Hierarchical Matching and Reasoning Network (HMRN) for MQIR. It
disentangles MQIR into three hierarchical semantic representations, which is
responsible to capture fine-grained local details, contextual global scopes,
and high-level inherent correlations. HMRN comprises two modules: Scalar-based
Matching (SM) module and Vector-based Reasoning (VR) module. Specifically, the
SM module characterizes the multi-level alignment similarity, which consists of
a fine-grained local-level similarity and a context-aware global-level
similarity. Afterwards, the VR module is developed to excavate the potential
semantic correlations among multiple region-query pairs, which further explores
the high-level reasoning similarity. Finally, these three-level similarities
are aggregated into a joint similarity space to form the ultimate similarity.
Extensive experiments on the benchmark dataset demonstrate that our HMRN
substantially surpasses the current state-of-the-art methods. For instance,
compared with the existing best method Drill-down, the metric R@1 in the last
round is improved by 23.4%. Our source codes will be released at
https://github.com/LZH-053/HMRN.</p>
</td>
    <td>
      
        Image Retrieval 
      
        ICANN 
      
        Neural Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/hou2024bridging/">Bridging Language And Items For Retrieval And Recommendation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Bridging Language And Items For Retrieval And Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Bridging Language And Items For Retrieval And Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hou et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>11</td>
    <td><p>This paper introduces BLaIR, a series of pretrained sentence embedding models
specialized for recommendation scenarios. BLaIR is trained to learn
correlations between item metadata and potential natural language context,
which is useful for retrieving and recommending items. To pretrain BLaIR, we
collect Amazon Reviews 2023, a new dataset comprising over 570 million reviews
and 48 million items from 33 categories, significantly expanding beyond the
scope of previous versions. We evaluate the generalization ability of BLaIR
across multiple domains and tasks, including a new task named complex product
search, referring to retrieving relevant items given long, complex natural
language contexts. Leveraging large language models like ChatGPT, we
correspondingly construct a semi-synthetic evaluation set, Amazon-C4. Empirical
results on the new task, as well as conventional retrieval and recommendation
tasks, demonstrate that BLaIR exhibit strong text and item representation
capacity. Our datasets, code, and checkpoints are available at:
https://github.com/hyp1231/AmazonReviews2023.</p>
</td>
    <td>
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/hou2024semstamp/">Semstamp: A Semantic Watermark With Paraphrastic Robustness For Text Generation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semstamp: A Semantic Watermark With Paraphrastic Robustness For Text Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semstamp: A Semantic Watermark With Paraphrastic Robustness For Text Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hou et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)</td>
    <td>7</td>
    <td><p>Existing watermarking algorithms are vulnerable to paraphrase attacks because
of their token-level design. To address this issue, we propose SemStamp, a
robust sentence-level semantic watermarking algorithm based on
locality-sensitive hashing (LSH), which partitions the semantic space of
sentences. The algorithm encodes and LSH-hashes a candidate sentence generated
by an LLM, and conducts sentence-level rejection sampling until the sampled
sentence falls in watermarked partitions in the semantic embedding space. A
margin-based constraint is used to enhance its robustness. To show the
advantages of our algorithm, we propose a “bigram” paraphrase attack using the
paraphrase that has the fewest bigram overlaps with the original sentence. This
attack is shown to be effective against the existing token-level watermarking
method. Experimental results show that our novel semantic watermark algorithm
is not only more robust than the previous state-of-the-art method on both
common and bigram paraphrase attacks, but also is better at preserving the
quality of generation.</p>
</td>
    <td>
      
        Robustness 
      
        NAACL 
      
        ACL 
      
        EACL 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/he2024hybridhash/">Hybridhash: Hybrid Convolutional And Self-attention Deep Hashing For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hybridhash: Hybrid Convolutional And Self-attention Deep Hashing For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hybridhash: Hybrid Convolutional And Self-attention Deep Hashing For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>He Chao, Wei Hongxi</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2024 International Conference on Multimedia Retrieval</td>
    <td>5</td>
    <td><p>Deep image hashing aims to map input images into simple binary hash codes via
deep neural networks and thus enable effective large-scale image retrieval.
Recently, hybrid networks that combine convolution and Transformer have
achieved superior performance on various computer tasks and have attracted
extensive attention from researchers. Nevertheless, the potential benefits of
such hybrid networks in image retrieval still need to be verified. To this end,
we propose a hybrid convolutional and self-attention deep hashing method known
as HybridHash. Specifically, we propose a backbone network with stage-wise
architecture in which the block aggregation function is introduced to achieve
the effect of local self-attention and reduce the computational complexity. The
interaction module has been elaborately designed to promote the communication
of information between image blocks and to enhance the visual representations.
We have conducted comprehensive experiments on three widely used datasets:
CIFAR-10, NUS-WIDE and IMAGENET. The experimental results demonstrate that the
method proposed in this paper has superior performance with respect to
state-of-the-art deep hashing methods. Source code is available
https://github.com/shuaichaochao/HybridHash.</p>
</td>
    <td>
      
        Image Retrieval 
      
        Neural Hashing 
      
        Medical Retrieval 
      
        Multimodal Retrieval 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/g%C3%BCnther2024late/">Late Chunking: Contextual Chunk Embeddings Using Long-context Embedding Models</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Late Chunking: Contextual Chunk Embeddings Using Long-context Embedding Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Late Chunking: Contextual Chunk Embeddings Using Long-context Embedding Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Günther et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</td>
    <td>12</td>
    <td><p>Many use cases require retrieving smaller portions of text, and dense vector-based retrieval systems often perform better with shorter text segments, as the semantics are less likely to be over-compressed in the embeddings. Consequently, practitioners often split text documents into smaller chunks and encode them separately. However, chunk embeddings created in this way can lose contextual information from surrounding chunks, resulting in sub-optimal representations. In this paper, we introduce a novel method called late chunking, which leverages long context embedding models to first embed all tokens of the long text, with chunking applied after the transformer model and just before mean pooling - hence the term late in its naming. The resulting chunk embeddings capture the full contextual information, leading to superior results across various retrieval tasks. The method is generic enough to be applied to a wide range of long-context embedding models and works without additional training. To further increase the effectiveness of late chunking, we propose a dedicated fine-tuning approach for embedding models.</p>
</td>
    <td>
      
        ACL 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/gottesb%C3%BCren2024unleashing/">Unleashing Graph Partitioning For Large-scale Nearest Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unleashing Graph Partitioning For Large-scale Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unleashing Graph Partitioning For Large-scale Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gottesbüren et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Computers</td>
    <td>19</td>
    <td><p>We consider the fundamental problem of decomposing a large-scale approximate
nearest neighbor search (ANNS) problem into smaller sub-problems. The goal is
to partition the input points into neighborhood-preserving shards, so that the
nearest neighbors of any point are contained in only a few shards. When a query
arrives, a routing algorithm is used to identify the shards which should be
searched for its nearest neighbors. This approach forms the backbone of
distributed ANNS, where the dataset is so large that it must be split across
multiple machines.
  In this paper, we design simple and highly efficient routing methods, and
prove strong theoretical guarantees on their performance. A crucial
characteristic of our routing algorithms is that they are inherently modular,
and can be used with any partitioning method. This addresses a key drawback of
prior approaches, where the routing algorithms are inextricably linked to their
associated partitioning method. In particular, our new routing methods enable
the use of balanced graph partitioning, which is a high-quality partitioning
method without a naturally associated routing algorithm. Thus, we provide the
first methods for routing using balanced graph partitioning that are extremely
fast to train, admit low latency, and achieve high recall. We provide a
comprehensive evaluation of our full partitioning and routing pipeline on
billion-scale datasets, where it outperforms existing scalable partitioning
methods by significant margins, achieving up to 2.14x higher QPS at 90%
recall\(@10\) than the best competitor.</p>
</td>
    <td>
      
        Similarity Search 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/ge20243shnet/">3shnet: Boosting Image-sentence Retrieval Via Visual Semantic-spatial Self-highlighting</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=3shnet: Boosting Image-sentence Retrieval Via Visual Semantic-spatial Self-highlighting' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=3shnet: Boosting Image-sentence Retrieval Via Visual Semantic-spatial Self-highlighting' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ge et al.</td> <!-- 🔧 You were missing this -->
    <td>Information Processing &amp; Management</td>
    <td>12</td>
    <td><p>In this paper, we propose a novel visual Semantic-Spatial Self-Highlighting
Network (termed 3SHNet) for high-precision, high-efficiency and
high-generalization image-sentence retrieval. 3SHNet highlights the salient
identification of prominent objects and their spatial locations within the
visual modality, thus allowing the integration of visual semantics-spatial
interactions and maintaining independence between two modalities. This
integration effectively combines object regions with the corresponding semantic
and position layouts derived from segmentation to enhance the visual
representation. And the modality-independence guarantees efficiency and
generalization. Additionally, 3SHNet utilizes the structured contextual visual
scene information from segmentation to conduct the local (region-based) or
global (grid-based) guidance and achieve accurate hybrid-level retrieval.
Extensive experiments conducted on MS-COCO and Flickr30K benchmarks
substantiate the superior performances, inference efficiency and generalization
of the proposed 3SHNet when juxtaposed with contemporary state-of-the-art
methodologies. Specifically, on the larger MS-COCO 5K test set, we achieve
16.3%, 24.8%, and 18.3% improvements in terms of rSum score, respectively,
compared with the state-of-the-art methods using different image
representations, while maintaining optimal retrieval efficiency. Moreover, our
performance on cross-dataset generalization improves by 18.6%. Data and code
are available at https://github.com/XuriGe1995/3SHNet.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/gebre2024pfeed/">Pfeed: Generating Near Real-time Personalized Feeds Using Precomputed Embedding Similarities</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Pfeed: Generating Near Real-time Personalized Feeds Using Precomputed Embedding Similarities' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Pfeed: Generating Near Real-time Personalized Feeds Using Precomputed Embedding Similarities' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gebre et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 21st ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games</td>
    <td>30</td>
    <td><p>In personalized recommender systems, embeddings are often used to encode
customer actions and items, and retrieval is then performed in the embedding
space using approximate nearest neighbor search. However, this approach can
lead to two challenges: 1) user embeddings can restrict the diversity of
interests captured and 2) the need to keep them up-to-date requires an
expensive, real-time infrastructure. In this paper, we propose a method that
overcomes these challenges in a practical, industrial setting. The method
dynamically updates customer profiles and composes a feed every two minutes,
employing precomputed embeddings and their respective similarities. We tested
and deployed this method to personalise promotional items at Bol, one of the
largest e-commerce platforms of the Netherlands and Belgium. The method
enhanced customer engagement and experience, leading to a significant 4.9%
uplift in conversions.</p>
</td>
    <td>
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/garciamorato2024parametrizable/">A Parametrizable Algorithm For Distributed Approximate Similarity Search With Arbitrary Distances</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Parametrizable Algorithm For Distributed Approximate Similarity Search With Arbitrary Distances' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Parametrizable Algorithm For Distributed Approximate Similarity Search With Arbitrary Distances' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Garcia-morato et al.</td> <!-- 🔧 You were missing this -->
    <td>Advances in Database Systems</td>
    <td>37</td>
    <td><p>Recent studies have explored alternative distance measures for similarity
search in spaces with diverse topologies, emphasizing the importance of
selecting an appropriate distance function to improve the performance of
k-Nearest Neighbour search algorithms. However, a critical gap remains in
accommodating such diverse similarity measures, as most existing methods for
exact or approximate similarity search are explicitly designed for metric
spaces.
  To address this need, we propose PDASC (Parametrizable Distributed
Approximate Similarity Search with Clustering), a novel Approximate Nearest
Neighbour search algorithm. PDASC combines an innovative multilevel indexing
structure particularly adept at managing outliers, highly imbalanced datasets,
and sparse data distributions, with the flexibility to support arbitrary
distance functions achieved through the integration of clustering algorithms
that inherently accommodate them.
  Experimental results show that PDASC constitutes a reliable ANN search
method, suitable for operating in distributed data environments and for
handling datasets defined in different topologies, where the selection of the
most appropriate distance function is often non-trivial.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/gao2024rabitq/">Rabitq: Quantizing High-dimensional Vectors With A Theoretical Error Bound For Approximate Nearest Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Rabitq: Quantizing High-dimensional Vectors With A Theoretical Error Bound For Approximate Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Rabitq: Quantizing High-dimensional Vectors With A Theoretical Error Bound For Approximate Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gao Jianyang, Long Cheng</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the ACM on Management of Data</td>
    <td>15</td>
    <td><p>Searching for approximate nearest neighbors (ANN) in the high-dimensional
Euclidean space is a pivotal problem. Recently, with the help of fast
SIMD-based implementations, Product Quantization (PQ) and its variants can
often efficiently and accurately estimate the distances between the vectors and
have achieved great success in the in-memory ANN search. Despite their
empirical success, we note that these methods do not have a theoretical error
bound and are observed to fail disastrously on some real-world datasets.
Motivated by this, we propose a new randomized quantization method named
RaBitQ, which quantizes \(D\)-dimensional vectors into \(D\)-bit strings. RaBitQ
guarantees a sharp theoretical error bound and provides good empirical accuracy
at the same time. In addition, we introduce efficient implementations of
RaBitQ, supporting to estimate the distances with bitwise operations or
SIMD-based operations. Extensive experiments on real-world datasets confirm
that (1) our method outperforms PQ and its variants in terms of
accuracy-efficiency trade-off by a clear margin and (2) its empirical
performance is well-aligned with our theoretical analysis.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/faysse2024colpali/">Colpali: Efficient Document Retrieval With Vision Language Models</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Colpali: Efficient Document Retrieval With Vision Language Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Colpali: Efficient Document Retrieval With Vision Language Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Faysse et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval</td>
    <td>51</td>
    <td><p>Documents are visually rich structures that convey information through text,
but also figures, page layouts, tables, or even fonts. Since modern retrieval
systems mainly rely on the textual information they extract from document pages
to index documents -often through lengthy and brittle processes-, they struggle
to exploit key visual cues efficiently. This limits their capabilities in many
practical document retrieval applications such as Retrieval Augmented
Generation (RAG). To benchmark current systems on visually rich document
retrieval, we introduce the Visual Document Retrieval Benchmark ViDoRe,
composed of various page-level retrieval tasks spanning multiple domains,
languages, and practical settings. The inherent complexity and performance
shortcomings of modern systems motivate a new concept; doing document retrieval
by directly embedding the images of the document pages. We release ColPali, a
Vision Language Model trained to produce high-quality multi-vector embeddings
from images of document pages. Combined with a late interaction matching
mechanism, ColPali largely outperforms modern document retrieval pipelines
while being drastically simpler, faster and end-to-end trainable. We release
models, data, code and benchmarks under open licenses at https://hf.co/vidore.</p>
</td>
    <td>
      
        SIGIR 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/engels2024approximate/">Approximate Nearest Neighbor Search With Window Filters</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Approximate Nearest Neighbor Search With Window Filters' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Approximate Nearest Neighbor Search With Window Filters' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Engels et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the ACM Web Conference 2023</td>
    <td>27</td>
    <td><p>We define and investigate the problem of \(\textit{c-approximate window
search}\): approximate nearest neighbor search where each point in the dataset
has a numeric label, and the goal is to find nearest neighbors to queries
within arbitrary label ranges. Many semantic search problems, such as image and
document search with timestamp filters, or product search with cost filters,
are natural examples of this problem. We propose and theoretically analyze a
modular tree-based framework for transforming an index that solves the
traditional c-approximate nearest neighbor problem into a data structure that
solves window search. On standard nearest neighbor benchmark datasets equipped
with random label values, adversarially constructed embeddings, and image
search embeddings with real timestamps, we obtain up to a \(75\times\) speedup
over existing solutions at the same level of recall.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/douze2024faiss/">The Faiss Library</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=The Faiss Library' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=The Faiss Library' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Douze et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>17</td>
    <td><p>Vector databases typically manage large collections of embedding vectors.
Currently, AI applications are growing rapidly, and so is the number of
embeddings that need to be stored and indexed. The Faiss library is dedicated
to vector similarity search, a core functionality of vector databases. Faiss is
a toolkit of indexing methods and related primitives used to search, cluster,
compress and transform vectors. This paper describes the trade-off space of
vector search and the design principles of Faiss in terms of structure,
approach to optimization and interfacing. We benchmark key features of the
library and discuss a few selected applications to highlight its broad
applicability.</p>
</td>
    <td>
      
        Tools & Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/dordevic2024evidential/">Evidential Transformers For Improved Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Evidential Transformers For Improved Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Evidential Transformers For Improved Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dordevic Danilo, Kumar Suryansh</td> <!-- 🔧 You were missing this -->
    <td>2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>29</td>
    <td><p>We introduce the Evidential Transformer, an uncertainty-driven transformer
model for improved and robust image retrieval. In this paper, we make several
contributions to content-based image retrieval (CBIR). We incorporate
probabilistic methods into image retrieval, achieving robust and reliable
results, with evidential classification surpassing traditional training based
on multiclass classification as a baseline for deep metric learning.
Furthermore, we improve the state-of-the-art retrieval results on several
datasets by leveraging the Global Context Vision Transformer (GC ViT)
architecture. Our experimental results consistently demonstrate the reliability
of our approach, setting a new benchmark in CBIR in all test settings on the
Stanford Online Products (SOP) and CUB-200-2011 datasets.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/desai2024hashattention/">Hashattention: Semantic Sparsity For Faster Inference</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashattention: Semantic Sparsity For Faster Inference' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashattention: Semantic Sparsity For Faster Inference' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Desai et al.</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>129</td>
    <td><p>Leveraging long contexts is crucial for advanced AI systems, but attention computation poses a scalability challenge. While scaled dot-product attention (SDPA) exhibits token sparsity, i.e. only a few pivotal tokens significantly contribute to output, exploiting this sparsity remains challenging. Existing methods either suffer from quality degradation or require substantial additional resources. We show that identifying pivotal tokens is a Maximum Inner Product Search (MIPS) problem. However, existing MIPS solutions are not well-suited for SDPA, as they are not GPU-friendly and often underperform due to the separated query and key distributions. This paper introduces HashAttention, framing pivotal token identification as a recommendation problem. Given a query, HashAttention encodes keys and queries in Hamming space, capturing the required semantic similarity, using learned mapping functions. HashAttention efficiently identifies pivotal tokens for a given query using bitwise operations and computes attention using only these tokens, improving the overall attention efficiency. Trained on generic data, HashAttention reduces tokens used by up to \(16\times\) with minimal quality loss, requiring only 32 bits of auxiliary memory per token. Sparsity can be further improved to \(32\times\) through task-specific fine-tuning. On A100 GPU, at \(32\times\) sparsity, incorporating HashAttention reduces attention latency by up to \(4.3\times\) in GPT-FAST and \(2.54\times\) in FlashDecode, and achieves up to \(3.12\times\) higher throughput for GPT-FAST.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/chen2024ca/">Ca-jaccard: Camera-aware Jaccard Distance For Person Re-identification</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Ca-jaccard: Camera-aware Jaccard Distance For Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Ca-jaccard: Camera-aware Jaccard Distance For Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen et al.</td> <!-- 🔧 You were missing this -->
    <td>2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>6</td>
    <td><p>Person re-identification (re-ID) is a challenging task that aims to learn
discriminative features for person retrieval. In person re-ID, Jaccard distance
is a widely used distance metric, especially in re-ranking and clustering
scenarios. However, we discover that camera variation has a significant
negative impact on the reliability of Jaccard distance. In particular, Jaccard
distance calculates the distance based on the overlap of relevant neighbors.
Due to camera variation, intra-camera samples dominate the relevant neighbors,
which reduces the reliability of the neighbors by introducing intra-camera
negative samples and excluding inter-camera positive samples. To overcome this
problem, we propose a novel camera-aware Jaccard (CA-Jaccard) distance that
leverages camera information to enhance the reliability of Jaccard distance.
Specifically, we design camera-aware k-reciprocal nearest neighbors (CKRNNs) to
find k-reciprocal nearest neighbors on the intra-camera and inter-camera
ranking lists, which improves the reliability of relevant neighbors and
guarantees the contribution of inter-camera samples in the overlap. Moreover,
we propose a camera-aware local query expansion (CLQE) to mine reliable samples
in relevant neighbors by exploiting camera variation as a strong constraint and
assign these samples higher weights in overlap, further improving the
reliability. Our CA-Jaccard distance is simple yet effective and can serve as a
general distance metric for person re-ID methods with high reliability and low
computational cost. Extensive experiments demonstrate the effectiveness of our
method.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/cheng2024adapting/">Adapting Dual-encoder Vision-language Models For Paraphrased Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Adapting Dual-encoder Vision-language Models For Paraphrased Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Adapting Dual-encoder Vision-language Models For Paraphrased Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cheng et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</td>
    <td>9</td>
    <td><p>In the recent years, the dual-encoder vision-language models (\eg CLIP) have
achieved remarkable text-to-image retrieval performance. However, we discover
that these models usually results in very different retrievals for a pair of
paraphrased queries. Such behavior might render the retrieval system less
predictable and lead to user frustration. In this work, we consider the task of
paraphrased text-to-image retrieval where a model aims to return similar
results given a pair of paraphrased queries. To start with, we collect a
dataset of paraphrased image descriptions to facilitate quantitative evaluation
for this task. We then hypothesize that the undesired behavior of existing
dual-encoder model is due to their text towers which are trained on
image-sentence pairs and lack the ability to capture the semantic similarity
between paraphrased queries. To improve on this, we investigate multiple
strategies for training a dual-encoder model starting from a language model
pretrained on a large text corpus. Compared to public dual-encoder models such
as CLIP and OpenCLIP, the model trained with our best adaptation strategy
achieves a significantly higher ranking similarity for paraphrased queries
while maintaining similar zero-shot classification and retrieval accuracy.</p>
</td>
    <td>
      
        EMNLP 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/cao2024context/">Context Recovery And Knowledge Retrieval: A Novel Two-stream Framework For Video Anomaly Detection</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Context Recovery And Knowledge Retrieval: A Novel Two-stream Framework For Video Anomaly Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Context Recovery And Knowledge Retrieval: A Novel Two-stream Framework For Video Anomaly Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cao Congqi, Lu Yue, Zhang Yanning</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>14</td>
    <td><p>Video anomaly detection aims to find the events in a video that do not
conform to the expected behavior. The prevalent methods mainly detect anomalies
by snippet reconstruction or future frame prediction error. However, the error
is highly dependent on the local context of the current snippet and lacks the
understanding of normality. To address this issue, we propose to detect
anomalous events not only by the local context, but also according to the
consistency between the testing event and the knowledge about normality from
the training data. Concretely, we propose a novel two-stream framework based on
context recovery and knowledge retrieval, where the two streams can complement
each other. For the context recovery stream, we propose a spatiotemporal U-Net
which can fully utilize the motion information to predict the future frame.
Furthermore, we propose a maximum local error mechanism to alleviate the
problem of large recovery errors caused by complex foreground objects. For the
knowledge retrieval stream, we propose an improved learnable locality-sensitive
hashing, which optimizes hash functions via a Siamese network and a mutual
difference loss. The knowledge about normality is encoded and stored in hash
tables, and the distance between the testing event and the knowledge
representation is used to reveal the probability of anomaly. Finally, we fuse
the anomaly scores from the two streams to detect anomalies. Extensive
experiments demonstrate the effectiveness and complementarity of the two
streams, whereby the proposed two-stream framework achieves state-of-the-art
performance on four datasets.</p>
</td>
    <td>
      
        Tools & Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/bruch2024optimistic/">Optimistic Query Routing In Clustering-based Approximate Maximum Inner Product Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Optimistic Query Routing In Clustering-based Approximate Maximum Inner Product Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Optimistic Query Routing In Clustering-based Approximate Maximum Inner Product Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Bruch Sebastian, Krishnan Aditya, Nardini Franco Maria</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>6</td>
    <td><p>Clustering-based nearest neighbor search is an effective method in which
points are partitioned into geometric shards to form an index, with only a few
shards searched during query processing to find a set of top-\(k\) vectors. Even
though the search efficacy is heavily influenced by the algorithm that
identifies the shards to probe, it has received little attention in the
literature. This work bridges that gap by studying routing in clustering-based
maximum inner product search. We unpack existing routers and notice the
surprising contribution of optimism. We then take a page from the sequential
decision making literature and formalize that insight following the principle
of ``optimism in the face of uncertainty.’’ In particular, we present a
framework that incorporates the moments of the distribution of inner products
within each shard to estimate the maximum inner product. We then present an
instance of our algorithm that uses only the first two moments to reach the
same accuracy as state-of-the-art routers such as ScaNN by probing up to \(50%\)
fewer points on benchmark datasets. Our algorithm is also space-efficient: we
design a sketch of the second moment whose size is independent of the number of
points and requires \(\mathcal{O}(1)\) vectors per shard.</p>
</td>
    <td>
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/bruch2024efficient/">Efficient Inverted Indexes For Approximate Retrieval Over Learned Sparse Representations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Inverted Indexes For Approximate Retrieval Over Learned Sparse Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient Inverted Indexes For Approximate Retrieval Over Learned Sparse Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Bruch et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>6</td>
    <td><p>Learned sparse representations form an attractive class of contextual
embeddings for text retrieval. That is so because they are effective models of
relevance and are interpretable by design. Despite their apparent compatibility
with inverted indexes, however, retrieval over sparse embeddings remains
challenging. That is due to the distributional differences between learned
embeddings and term frequency-based lexical models of relevance such as BM25.
Recognizing this challenge, a great deal of research has gone into, among other
things, designing retrieval algorithms tailored to the properties of learned
sparse representations, including approximate retrieval systems. In fact, this
task featured prominently in the latest BigANN Challenge at NeurIPS 2023, where
approximate algorithms were evaluated on a large benchmark dataset by
throughput and recall. In this work, we propose a novel organization of the
inverted index that enables fast yet effective approximate retrieval over
learned sparse embeddings. Our approach organizes inverted lists into
geometrically-cohesive blocks, each equipped with a summary vector. During
query processing, we quickly determine if a block must be evaluated using the
summaries. As we show experimentally, single-threaded query processing using
our method, Seismic, reaches sub-millisecond per-query latency on various
sparse embeddings of the MS MARCO dataset while maintaining high recall. Our
results indicate that Seismic is one to two orders of magnitude faster than
state-of-the-art inverted index-based solutions and further outperforms the
winning (graph-based) submissions to the BigANN Challenge by a significant
margin.</p>
</td>
    <td>
      
        SIGIR 
      
        Text Retrieval 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/bhatnagar2024piecewise/">Piecewise-linear Manifolds For Deep Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Piecewise-linear Manifolds For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Piecewise-linear Manifolds For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Bhatnagar Shubhang, Ahuja Narendra</td> <!-- 🔧 You were missing this -->
    <td>Transactions of the American Mathematical Society</td>
    <td>7</td>
    <td><p>Unsupervised deep metric learning (UDML) focuses on learning a semantic
representation space using only unlabeled data. This challenging problem
requires accurately estimating the similarity between data points, which is
used to supervise a deep network. For this purpose, we propose to model the
high-dimensional data manifold using a piecewise-linear approximation, with
each low-dimensional linear piece approximating the data manifold in a small
neighborhood of a point. These neighborhoods are used to estimate similarity
between data points. We empirically show that this similarity estimate
correlates better with the ground truth than the similarity estimates of
current state-of-the-art techniques. We also show that proxies, commonly used
in supervised metric learning, can be used to model the piecewise-linear
manifold in an unsupervised setting, helping improve performance. Our method
outperforms existing unsupervised metric learning approaches on standard
zero-shot image retrieval benchmarks.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/aroraa2024smart/">Smart Multi-modal Search: Contextual Sparse And Dense Embedding Integration In Adobe Express</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Smart Multi-modal Search: Contextual Sparse And Dense Embedding Integration In Adobe Express' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Smart Multi-modal Search: Contextual Sparse And Dense Embedding Integration In Adobe Express' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Aroraa et al.</td> <!-- 🔧 You were missing this -->
    <td>Neurocomputing</td>
    <td>28</td>
    <td><p>As user content and queries become increasingly multi-modal, the need for
effective multi-modal search systems has grown. Traditional search systems
often rely on textual and metadata annotations for indexed images, while
multi-modal embeddings like CLIP enable direct search using text and image
embeddings. However, embedding-based approaches face challenges in integrating
contextual features such as user locale and recency. Building a scalable
multi-modal search system requires fine-tuning several components. This paper
presents a multi-modal search architecture and a series of AB tests that
optimize embeddings and multi-modal technologies in Adobe Express template
search. We address considerations such as embedding model selection, the roles
of embeddings in matching and ranking, and the balance between dense and sparse
embeddings. Our iterative approach demonstrates how utilizing sparse, dense,
and contextual features enhances short and long query search, significantly
reduces null rates (over 70%), and increases click-through rates (CTR). Our
findings provide insights into developing robust multi-modal search systems,
thereby enhancing relevance for complex queries.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/alibey2024boq/">Boq: A Place Is Worth A Bag Of Learnable Queries</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Boq: A Place Is Worth A Bag Of Learnable Queries' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Boq: A Place Is Worth A Bag Of Learnable Queries' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ali-bey Amar, Chaib-draa Brahim, Giguère Philippe</td> <!-- 🔧 You were missing this -->
    <td>2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>8</td>
    <td><p>In visual place recognition, accurately identifying and matching images of
locations under varying environmental conditions and viewpoints remains a
significant challenge. In this paper, we introduce a new technique, called
Bag-of-Queries (BoQ), which learns a set of global queries designed to capture
universal place-specific attributes. Unlike existing methods that employ
self-attention and generate the queries directly from the input features, BoQ
employs distinct learnable global queries, which probe the input features via
cross-attention, ensuring consistent information aggregation. In addition, our
technique provides an interpretable attention mechanism and integrates with
both CNN and Vision Transformer backbones. The performance of BoQ is
demonstrated through extensive experiments on 14 large-scale benchmarks. It
consistently outperforms current state-of-the-art techniques including NetVLAD,
MixVPR and EigenPlaces. Moreover, as a global retrieval technique (one-stage),
BoQ surpasses two-stage retrieval methods, such as Patch-NetVLAD, TransVPR and
R2Former, all while being orders of magnitude faster and more efficient. The
code and model weights are publicly available at
https://github.com/amaralibey/Bag-of-Queries.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/abdallah2024arabicaqa/">Arabicaqa: A Comprehensive Dataset For Arabic Question Answering</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Arabicaqa: A Comprehensive Dataset For Arabic Question Answering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Arabicaqa: A Comprehensive Dataset For Arabic Question Answering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Abdallah et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>7</td>
    <td><p>In this paper, we address the significant gap in Arabic natural language
processing (NLP) resources by introducing ArabicaQA, the first large-scale
dataset for machine reading comprehension and open-domain question answering in
Arabic. This comprehensive dataset, consisting of 89,095 answerable and 3,701
unanswerable questions created by crowdworkers to look similar to answerable
ones, along with additional labels of open-domain questions marks a crucial
advancement in Arabic NLP resources. We also present AraDPR, the first dense
passage retrieval model trained on the Arabic Wikipedia corpus, specifically
designed to tackle the unique challenges of Arabic text retrieval. Furthermore,
our study includes extensive benchmarking of large language models (LLMs) for
Arabic question answering, critically evaluating their performance in the
Arabic language context. In conclusion, ArabicaQA, AraDPR, and the benchmarking
of LLMs in Arabic question answering offer significant advancements in the
field of Arabic NLP. The dataset and code are publicly accessible for further
research https://github.com/DataScienceUIBK/ArabicaQA.</p>
</td>
    <td>
      
        Datasets 
      
        SIGIR 
      
        Graph Based ANN 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/zeighami2024nudge/">NUDGE: Lightweight Non-parametric Fine-tuning Of Embeddings For Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=NUDGE: Lightweight Non-parametric Fine-tuning Of Embeddings For Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=NUDGE: Lightweight Non-parametric Fine-tuning Of Embeddings For Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zeighami Sepanta, Wellmer Zac, Parameswaran Aditya</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>5</td>
    <td><p>\(k\)-Nearest Neighbor search on dense vector embeddings (\(k\)-NN retrieval)
from pre-trained embedding models is the predominant retrieval method for text
and images, as well as Retrieval-Augmented Generation (RAG) pipelines. In
practice, application developers often fine-tune the embeddings to improve
their accuracy on the dataset and query workload in hand. Existing approaches
either fine-tune the pre-trained model itself or, more efficiently, but at the
cost of accuracy, train adaptor models to transform the output of the
pre-trained model. We present NUDGE, a family of novel non-parametric embedding
fine-tuning approaches that are significantly more accurate and efficient than
both sets of existing approaches. NUDGE directly modifies the embeddings of
data records to maximize the accuracy of \(k\)-NN retrieval. We present a
thorough theoretical and experimental study of NUDGE’s non-parametric approach.
We show that even though the underlying problem is NP-Hard, constrained
variations can be solved efficiently. These constraints additionally ensure
that the changes to the embeddings are modest, avoiding large distortions to
the semantics learned during pre-training. In experiments across five
pre-trained models and nine standard text and image retrieval datasets, NUDGE
runs in minutes and often improves NDCG@10 by more than 10% over existing
fine-tuning methods. On average, NUDGE provides 3.3x and 4.3x higher increase
in accuracy and runs 200x and 3x faster, respectively, over fine-tuning the
pre-trained model and training adaptors.</p>
</td>
    <td>
      
        SIGIR 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/yun2024neurohash/">Neurohash: A Hyperdimensional Neuro-symbolic Framework For Spatially-aware Image Hashing And Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Neurohash: A Hyperdimensional Neuro-symbolic Framework For Spatially-aware Image Hashing And Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Neurohash: A Hyperdimensional Neuro-symbolic Framework For Spatially-aware Image Hashing And Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yun et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 5th ACM on International Conference on Multimedia Retrieval</td>
    <td>10</td>
    <td><p>Customizable image retrieval from large datasets remains a critical
challenge, particularly when preserving spatial relationships within images.
Traditional hashing methods, primarily based on deep learning, often fail to
capture spatial information adequately and lack transparency. In this paper, we
introduce NeuroHash, a novel neuro-symbolic framework leveraging
Hyperdimensional Computing (HDC) to enable highly customizable, spatially-aware
image retrieval. NeuroHash combines pre-trained deep neural network models with
HDC-based symbolic models, allowing for flexible manipulation of hash values to
support conditional image retrieval. Our method includes a self-supervised
context-aware HDC encoder and novel loss terms for optimizing lower-dimensional
bipolar hashing using multilinear hyperplanes. We evaluate NeuroHash on two
benchmark datasets, demonstrating superior performance compared to
state-of-the-art hashing methods, as measured by mAP@5K scores and our newly
introduced metric, mAP@5Kr, which assesses spatial alignment. The results
highlight NeuroHash’s ability to achieve competitive performance while offering
significant advantages in flexibility and customization, paving the way for
more advanced and versatile image retrieval systems.</p>
</td>
    <td>
      
        Image Retrieval 
      
        Tools & Libraries 
      
        Medical Retrieval 
      
        Multimodal Retrieval 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/ma2023cot/">Cot-mote: Exploring Contextual Masked Auto-encoder Pre-training With Mixture-of-textual-experts For Passage Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cot-mote: Exploring Contextual Masked Auto-encoder Pre-training With Mixture-of-textual-experts For Passage Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cot-mote: Exploring Contextual Masked Auto-encoder Pre-training With Mixture-of-textual-experts For Passage Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ma et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>12</td>
    <td><p>Passage retrieval aims to retrieve relevant passages from large collections
of the open-domain corpus. Contextual Masked Auto-Encoding has been proven
effective in representation bottleneck pre-training of a monolithic
dual-encoder for passage retrieval. Siamese or fully separated dual-encoders
are often adopted as basic retrieval architecture in the pre-training and
fine-tuning stages for encoding queries and passages into their latent
embedding spaces. However, simply sharing or separating the parameters of the
dual-encoder results in an imbalanced discrimination of the embedding spaces.
In this work, we propose to pre-train Contextual Masked Auto-Encoder with
Mixture-of-Textual-Experts (CoT-MoTE). Specifically, we incorporate
textual-specific experts for individually encoding the distinct properties of
queries and passages. Meanwhile, a shared self-attention layer is still kept
for unified attention modeling. Results on large-scale passage retrieval
benchmarks show steady improvement in retrieval performances. The quantitive
analysis also shows a more balanced discrimination of the latent embedding
spaces.</p>
</td>
    <td>
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/ma2023beat/">Beat: Bi-directional One-to-many Embedding Alignment For Text-based Person Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Beat: Bi-directional One-to-many Embedding Alignment For Text-based Person Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Beat: Bi-directional One-to-many Embedding Alignment For Text-based Person Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ma et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 31st ACM International Conference on Multimedia</td>
    <td>10</td>
    <td><p>Text-based person retrieval (TPR) is a challenging task that involves
retrieving a specific individual based on a textual description. Despite
considerable efforts to bridge the gap between vision and language, the
significant differences between these modalities continue to pose a challenge.
Previous methods have attempted to align text and image samples in a
modal-shared space, but they face uncertainties in optimization directions due
to the movable features of both modalities and the failure to account for
one-to-many relationships of image-text pairs in TPR datasets. To address this
issue, we propose an effective bi-directional one-to-many embedding paradigm
that offers a clear optimization direction for each sample, thus mitigating the
optimization problem. Additionally, this embedding scheme generates multiple
features for each sample without introducing trainable parameters, making it
easier to align with several positive samples. Based on this paradigm, we
propose a novel Bi-directional one-to-many Embedding Alignment (Beat) model to
address the TPR task. Our experimental results demonstrate that the proposed
Beat model achieves state-of-the-art performance on three popular TPR datasets,
including CUHK-PEDES (65.61 R@1), ICFG-PEDES (58.25 R@1), and RSTPReID (48.10
R@1). Furthermore, additional experiments on MS-COCO, CUB, and Flowers datasets
further demonstrate the potential of Beat to be applied to other image-text
retrieval tasks.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/ma2023harr/">HARR: Learning Discriminative And High-quality Hash Codes For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=HARR: Learning Discriminative And High-quality Hash Codes For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=HARR: Learning Discriminative And High-quality Hash Codes For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ma et al.</td> <!-- 🔧 You were missing this -->
    <td>ACM Transactions on Multimedia Computing, Communications, and Applications</td>
    <td>5</td>
    <td><p>This article studies deep unsupervised hashing, which has attracted increasing attention in large-scale image retrieval. The majority of recent approaches usually reconstruct semantic similarity information, which then guides the hash code learning. However, they still fail to achieve satisfactory performance in reality for two reasons. On the one hand, without accurate supervised information, these methods usually fail to produce independent and robust hash codes with semantics information well preserved, which may hinder effective image retrieval. On the other hand, due to discrete constraints, how to effectively optimize the hashing network in an end-to-end manner with small quantization errors remains a problem. To address these difficulties, we propose a novel unsupervised hashing method called HARR to learn discriminative and high-quality hash codes. To comprehensively explore semantic similarity structure, HARR adopts the Winner-Take-All hash to model the similarity structure. Then similarity-preserving hash codes are learned under the reliable guidance of the reconstructed similarity structure. Additionally, we improve the quality of hash codes by a bit correlation reduction module, which forces the cross-correlation matrix between a batch of hash codes under different augmentations to approach the identity matrix. In this way, the generated hash bits are expected to be invariant to disturbances with minimal redundancy, which can be further interpreted as an instantiation of the information bottleneck principle. Finally, for effective hashing network training, we minimize the cosine distances between real-value network outputs and their binary codes for small quantization errors. Extensive experiments demonstrate the effectiveness of our proposed HARR.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/ma2023anserini/">Anserini Gets Dense Retrieval: Integration Of Lucene's HNSW Indexes</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Anserini Gets Dense Retrieval: Integration Of Lucene's HNSW Indexes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Anserini Gets Dense Retrieval: Integration Of Lucene's HNSW Indexes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ma Xueguang, Teofili Tommaso, Lin Jimmy</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 32nd ACM International Conference on Information and Knowledge Management</td>
    <td>5</td>
    <td><p>Anserini is a Lucene-based toolkit for reproducible information retrieval
research in Java that has been gaining traction in the community. It provides
retrieval capabilities for both “traditional” bag-of-words retrieval models
such as BM25 as well as retrieval using learned sparse representations such as
SPLADE. With Pyserini, which provides a Python interface to Anserini, users
gain access to both sparse and dense retrieval models, as Pyserini implements
bindings to the Faiss vector search library alongside Lucene inverted indexes
in a uniform, consistent interface. Nevertheless, hybrid fusion techniques that
integrate sparse and dense retrieval models need to stitch together results
from two completely different “software stacks”, which creates unnecessary
complexities and inefficiencies. However, the introduction of HNSW indexes for
dense vector search in Lucene promises the integration of both dense and sparse
retrieval within a single software framework. We explore exactly this
integration in the context of Anserini. Experiments on the MS MARCO passage and
BEIR datasets show that our Anserini HNSW integration supports (reasonably)
effective and (reasonably) efficient approximate nearest neighbor search for
dense retrieval models, using only Lucene.</p>
</td>
    <td>
      
        Graph Based ANN 
      
        CIKM 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/luo2023lexlip/">Lexlip: Lexicon-bottlenecked Language-image Pre-training For Large-scale Image-text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Lexlip: Lexicon-bottlenecked Language-image Pre-training For Large-scale Image-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Lexlip: Lexicon-bottlenecked Language-image Pre-training For Large-scale Image-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Luo et al.</td> <!-- 🔧 You were missing this -->
    <td>2023 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>19</td>
    <td><p>Image-text retrieval (ITR) is a task to retrieve the relevant images/texts,
given the query from another modality. The conventional dense retrieval
paradigm relies on encoding images and texts into dense representations using
dual-stream encoders, however, it faces challenges with low retrieval speed in
large-scale retrieval scenarios. In this work, we propose the lexicon-weighting
paradigm, where sparse representations in vocabulary space are learned for
images and texts to take advantage of the bag-of-words models and efficient
inverted indexes, resulting in significantly reduced retrieval latency. A
crucial gap arises from the continuous nature of image data, and the
requirement for a sparse vocabulary space representation. To bridge this gap,
we introduce a novel pre-training framework, Lexicon-Bottlenecked
Language-Image Pre-Training (LexLIP), that learns importance-aware lexicon
representations. This framework features lexicon-bottlenecked modules between
the dual-stream encoders and weakened text decoders, allowing for constructing
continuous bag-of-words bottlenecks to learn lexicon-importance distributions.
Upon pre-training with same-scale data, our LexLIP achieves state-of-the-art
performance on two benchmark ITR datasets, MSCOCO and Flickr30k. Furthermore,
in large-scale retrieval scenarios, LexLIP outperforms CLIP with a 5.5 ~ 221.3X
faster retrieval speed and 13.2 ~ 48.8X less index storage memory.</p>
</td>
    <td>
      
        Text Retrieval 
      
        SCALABILITY 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/mckeown2023hamming/">Hamming Distributions Of Popular Perceptual Hashing Techniques</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hamming Distributions Of Popular Perceptual Hashing Techniques' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hamming Distributions Of Popular Perceptual Hashing Techniques' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mckeown Sean, Buchanan William J</td> <!-- 🔧 You were missing this -->
    <td>Forensic Science International: Digital Investigation</td>
    <td>9</td>
    <td><p>Content-based file matching has been widely deployed for decades, largely for
the detection of sources of copyright infringement, extremist materials, and
abusive sexual media. Perceptual hashes, such as Microsoft’s PhotoDNA, are one
automated mechanism for facilitating detection, allowing for machines to
approximately match visual features of an image or video in a robust manner.
However, there does not appear to be much public evaluation of such approaches,
particularly when it comes to how effective they are against content-preserving
modifications to media files. In this paper, we present a million-image scale
evaluation of several perceptual hashing archetypes for popular algorithms
(including Facebook’s PDQ, Apple’s Neuralhash, and the popular pHash library)
against seven image variants. The focal point is the distribution of Hamming
distance scores between both unrelated images and image variants to better
understand the problems faced by each approach.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/lu2023asymmetric/">Asymmetric Transfer Hashing With Adaptive Bipartite Graph Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Asymmetric Transfer Hashing With Adaptive Bipartite Graph Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Asymmetric Transfer Hashing With Adaptive Bipartite Graph Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lu et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Cybernetics</td>
    <td>8</td>
    <td><p>Thanks to the efficient retrieval speed and low storage consumption, learning
to hash has been widely used in visual retrieval tasks. However, existing
hashing methods assume that the query and retrieval samples lie in homogeneous
feature space within the same domain. As a result, they cannot be directly
applied to heterogeneous cross-domain retrieval. In this paper, we propose a
Generalized Image Transfer Retrieval (GITR) problem, which encounters two
crucial bottlenecks: 1) the query and retrieval samples may come from different
domains, leading to an inevitable {domain distribution gap}; 2) the features of
the two domains may be heterogeneous or misaligned, bringing up an additional
{feature gap}. To address the GITR problem, we propose an Asymmetric Transfer
Hashing (ATH) framework with its unsupervised/semi-supervised/supervised
realizations. Specifically, ATH characterizes the domain distribution gap by
the discrepancy between two asymmetric hash functions, and minimizes the
feature gap with the help of a novel adaptive bipartite graph constructed on
cross-domain data. By jointly optimizing asymmetric hash functions and the
bipartite graph, not only can knowledge transfer be achieved but information
loss caused by feature alignment can also be avoided. Meanwhile, to alleviate
negative transfer, the intrinsic geometrical structure of single-domain data is
preserved by involving a domain affinity graph. Extensive experiments on both
single-domain and cross-domain benchmarks under different GITR subtasks
indicate the superiority of our ATH method in comparison with the
state-of-the-art hashing methods.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/lu2023attributes/">Attributes Grouping And Mining Hashing For Fine-grained Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Attributes Grouping And Mining Hashing For Fine-grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Attributes Grouping And Mining Hashing For Fine-grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 31st ACM International Conference on Multimedia</td>
    <td>8</td>
    <td><p>In recent years, hashing methods have been popular in the large-scale media
search for low storage and strong representation capabilities. To describe
objects with similar overall appearance but subtle differences, more and more
studies focus on hashing-based fine-grained image retrieval. Existing hashing
networks usually generate both local and global features through attention
guidance on the same deep activation tensor, which limits the diversity of
feature representations. To handle this limitation, we substitute convolutional
descriptors for attention-guided features and propose an Attributes Grouping
and Mining Hashing (AGMH), which groups and embeds the category-specific visual
attributes in multiple descriptors to generate a comprehensive feature
representation for efficient fine-grained image retrieval. Specifically, an
Attention Dispersion Loss (ADL) is designed to force the descriptors to attend
to various local regions and capture diverse subtle details. Moreover, we
propose a Stepwise Interactive External Attention (SIEA) to mine critical
attributes in each descriptor and construct correlations between fine-grained
attributes and objects. The attention mechanism is dedicated to learning
discrete attributes, which will not cost additional computations in hash codes
generation. Finally, the compact binary codes are learned by preserving
pairwise similarities. Experimental results demonstrate that AGMH consistently
yields the best performance against state-of-the-art methods on fine-grained
benchmark datasets.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/liu2023efficient/">Efficient Token-guided Image-text Retrieval With Consistent Multimodal Contrastive Training</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Token-guided Image-text Retrieval With Consistent Multimodal Contrastive Training' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient Token-guided Image-text Retrieval With Consistent Multimodal Contrastive Training' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>19</td>
    <td><p>Image-text retrieval is a central problem for understanding the semantic
relationship between vision and language, and serves as the basis for various
visual and language tasks. Most previous works either simply learn
coarse-grained representations of the overall image and text, or elaborately
establish the correspondence between image regions or pixels and text words.
However, the close relations between coarse- and fine-grained representations
for each modality are important for image-text retrieval but almost neglected.
As a result, such previous works inevitably suffer from low retrieval accuracy
or heavy computational cost. In this work, we address image-text retrieval from
a novel perspective by combining coarse- and fine-grained representation
learning into a unified framework. This framework is consistent with human
cognition, as humans simultaneously pay attention to the entire sample and
regional elements to understand the semantic content. To this end, a
Token-Guided Dual Transformer (TGDT) architecture which consists of two
homogeneous branches for image and text modalities, respectively, is proposed
for image-text retrieval. The TGDT incorporates both coarse- and fine-grained
retrievals into a unified framework and beneficially leverages the advantages
of both retrieval approaches. A novel training objective called Consistent
Multimodal Contrastive (CMC) loss is proposed accordingly to ensure the intra-
and inter-modal semantic consistencies between images and texts in the common
embedding space. Equipped with a two-stage inference method based on the mixed
global and local cross-modal similarity, the proposed method achieves
state-of-the-art retrieval performances with extremely low inference time when
compared with representative recent approaches.</p>
</td>
    <td>
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/liu2023candidate/">Candidate Set Re-ranking For Composed Image Retrieval With Dual Multi-modal Encoder</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Candidate Set Re-ranking For Composed Image Retrieval With Dual Multi-modal Encoder' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Candidate Set Re-ranking For Composed Image Retrieval With Dual Multi-modal Encoder' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>8</td>
    <td><p>Composed image retrieval aims to find an image that best matches a given
multi-modal user query consisting of a reference image and text pair. Existing
methods commonly pre-compute image embeddings over the entire corpus and
compare these to a reference image embedding modified by the query text at test
time. Such a pipeline is very efficient at test time since fast vector
distances can be used to evaluate candidates, but modifying the reference image
embedding guided only by a short textual description can be difficult,
especially independent of potential candidates. An alternative approach is to
allow interactions between the query and every possible candidate, i.e.,
reference-text-candidate triplets, and pick the best from the entire set.
Though this approach is more discriminative, for large-scale datasets the
computational cost is prohibitive since pre-computation of candidate embeddings
is no longer possible. We propose to combine the merits of both schemes using a
two-stage model. Our first stage adopts the conventional vector distancing
metric and performs a fast pruning among candidates. Meanwhile, our second
stage employs a dual-encoder architecture, which effectively attends to the
input triplet of reference-text-candidate and re-ranks the candidates. Both
stages utilize a vision-and-language pre-trained network, which has proven
beneficial for various downstream tasks. Our method consistently outperforms
state-of-the-art approaches on standard benchmarks for the task. Our
implementation is available at
https://github.com/Cuberick-Orion/Candidate-Reranking-CIR.</p>
</td>
    <td>
      
        Hybrid ANN Methods 
      
        Image Retrieval 
      
        Re RANKING 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/liu2023instance/">Instance-variant Loss With Gaussian RBF Kernel For 3D Cross-modal Retriveal</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Instance-variant Loss With Gaussian RBF Kernel For 3D Cross-modal Retriveal' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Instance-variant Loss With Gaussian RBF Kernel For 3D Cross-modal Retriveal' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>54</td>
    <td><p>3D cross-modal retrieval is gaining attention in the multimedia community.
Central to this topic is learning a joint embedding space to represent data
from different modalities, such as images, 3D point clouds, and polygon meshes,
to extract modality-invariant and discriminative features. Hence, the
performance of cross-modal retrieval methods heavily depends on the
representational capacity of this embedding space. Existing methods treat all
instances equally, applying the same penalty strength to instances with varying
degrees of difficulty, ignoring the differences between instances. This can
result in ambiguous convergence or local optima, severely compromising the
separability of the feature space. To address this limitation, we propose an
Instance-Variant loss to assign different penalty strengths to different
instances, improving the space separability. Specifically, we assign different
penalty weights to instances positively related to their intra-class distance.
Simultaneously, we reduce the cross-modal discrepancy between features by
learning a shared weight vector for the same class data from different
modalities. By leveraging the Gaussian RBF kernel to evaluate sample
similarity, we further propose an Intra-Class loss function that minimizes the
intra-class distance among same-class instances. Extensive experiments on three
3D cross-modal datasets show that our proposed method surpasses recent
state-of-the-art approaches.</p>
</td>
    <td>
      
        Multimodal Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/li2023slim/">SLIM: Sparsified Late Interaction For Multi-vector Retrieval With Inverted Indexes</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=SLIM: Sparsified Late Interaction For Multi-vector Retrieval With Inverted Indexes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=SLIM: Sparsified Late Interaction For Multi-vector Retrieval With Inverted Indexes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>7</td>
    <td><p>This paper introduces Sparsified Late Interaction for Multi-vector (SLIM)
retrieval with inverted indexes. Multi-vector retrieval methods have
demonstrated their effectiveness on various retrieval datasets, and among them,
ColBERT is the most established method based on the late interaction of
contextualized token embeddings of pre-trained language models. However,
efficient ColBERT implementations require complex engineering and cannot take
advantage of off-the-shelf search libraries, impeding their practical use. To
address this issue, SLIM first maps each contextualized token vector to a
sparse, high-dimensional lexical space before performing late interaction
between these sparse token embeddings. We then introduce an efficient two-stage
retrieval architecture that includes inverted index retrieval followed by a
score refinement module to approximate the sparsified late interaction, which
is fully compatible with off-the-shelf lexical search libraries such as Lucene.
SLIM achieves competitive accuracy on MS MARCO Passages and BEIR compared to
ColBERT while being much smaller and faster on CPUs. To our knowledge, we are
the first to explore using sparse token representations for multi-vector
retrieval. Source code and data are integrated into the Pyserini IR toolkit.</p>
</td>
    <td>
      
        SIGIR 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/li2023constructing/">Constructing Tree-based Index For Efficient And Effective Dense Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Constructing Tree-based Index For Efficient And Effective Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Constructing Tree-based Index For Efficient And Effective Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>10</td>
    <td><p>Recent studies have shown that Dense Retrieval (DR) techniques can
significantly improve the performance of first-stage retrieval in IR systems.
Despite its empirical effectiveness, the application of DR is still limited. In
contrast to statistic retrieval models that rely on highly efficient inverted
index solutions, DR models build dense embeddings that are difficult to be
pre-processed with most existing search indexing systems. To avoid the
expensive cost of brute-force search, the Approximate Nearest Neighbor (ANN)
algorithm and corresponding indexes are widely applied to speed up the
inference process of DR models. Unfortunately, while ANN can improve the
efficiency of DR models, it usually comes with a significant price on retrieval
performance.
  To solve this issue, we propose JTR, which stands for Joint optimization of
TRee-based index and query encoding. Specifically, we design a new unified
contrastive learning loss to train tree-based index and query encoder in an
end-to-end manner. The tree-based negative sampling strategy is applied to make
the tree have the maximum heap property, which supports the effectiveness of
beam search well. Moreover, we treat the cluster assignment as an optimization
problem to update the tree-based index that allows overlapped clustering. We
evaluate JTR on numerous popular retrieval benchmarks. Experimental results
show that JTR achieves better retrieval performance while retaining high system
efficiency compared with widely-adopted baselines. It provides a potential
solution to balance efficiency and effectiveness in neural retrieval system
designs.</p>
</td>
    <td>
      
        SIGIR 
      
        Text Retrieval 
      
        Tree Based ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/li2023citadel/">CITADEL: Conditional Token Interaction Via Dynamic Lexical Routing For Efficient And Effective Multi-vector Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=CITADEL: Conditional Token Interaction Via Dynamic Lexical Routing For Efficient And Effective Multi-vector Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=CITADEL: Conditional Token Interaction Via Dynamic Lexical Routing For Efficient And Effective Multi-vector Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</td>
    <td>5</td>
    <td><p>Multi-vector retrieval methods combine the merits of sparse (e.g. BM25) and
dense (e.g. DPR) retrievers and have achieved state-of-the-art performance on
various retrieval tasks. These methods, however, are orders of magnitude slower
and need much more space to store their indices compared to their single-vector
counterparts. In this paper, we unify different multi-vector retrieval models
from a token routing viewpoint and propose conditional token interaction via
dynamic lexical routing, namely CITADEL, for efficient and effective
multi-vector retrieval. CITADEL learns to route different token vectors to the
predicted lexical ``keys’’ such that a query token vector only interacts with
document token vectors routed to the same key. This design significantly
reduces the computation cost while maintaining high accuracy. Notably, CITADEL
achieves the same or slightly better performance than the previous state of the
art, ColBERT-v2, on both in-domain (MS MARCO) and out-of-domain (BEIR)
evaluations, while being nearly 40 times faster. Code and data are available at
https://github.com/facebookresearch/dpr-scale.</p>
</td>
    <td>
      
        ACL 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/li2023differentially/">Differentially Private One Permutation Hashing And Bin-wise Consistent Weighted Sampling</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Differentially Private One Permutation Hashing And Bin-wise Consistent Weighted Sampling' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Differentially Private One Permutation Hashing And Bin-wise Consistent Weighted Sampling' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Xiaoyun, Li Ping</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</td>
    <td>36</td>
    <td><p>Minwise hashing (MinHash) is a standard algorithm widely used in the
industry, for large-scale search and learning applications with the binary
(0/1) Jaccard similarity. One common use of MinHash is for processing massive
n-gram text representations so that practitioners do not have to materialize
the original data (which would be prohibitive). Another popular use of MinHash
is for building hash tables to enable sub-linear time approximate near neighbor
(ANN) search. MinHash has also been used as a tool for building large-scale
machine learning systems. The standard implementation of MinHash requires
applying \(K\) random permutations. In comparison, the method of one permutation
hashing (OPH), is an efficient alternative of MinHash which splits the data
vectors into \(K\) bins and generates hash values within each bin. OPH is
substantially more efficient and also more convenient to use.
  In this paper, we combine the differential privacy (DP) with OPH (as well as
MinHash), to propose the DP-OPH framework with three variants: DP-OPH-fix,
DP-OPH-re and DP-OPH-rand, depending on which densification strategy is adopted
to deal with empty bins in OPH. A detailed roadmap to the algorithm design is
presented along with the privacy analysis. An analytical comparison of our
proposed DP-OPH methods with the DP minwise hashing (DP-MH) is provided to
justify the advantage of DP-OPH. Experiments on similarity search confirm the
merits of DP-OPH, and guide the choice of the proper variant in different
practical scenarios. Our technique is also extended to bin-wise consistent
weighted sampling (BCWS) to develop a new DP algorithm called DP-BCWS for
non-binary data. Experiments on classification tasks demonstrate that DP-BCWS
is able to achieve excellent utility at around \(\epsilon = 5\sim 10\), where
\(\epsilon\) is the standard parameter in the language of \((\epsilon,
\delta)\)-DP.</p>
</td>
    <td>
      
        KDD 
      
        Hashing Methods 
      
        Privacy & Security 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/li2023style/">The Style Transformer With Common Knowledge Optimization For Image-text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=The Style Transformer With Common Knowledge Optimization For Image-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=The Style Transformer With Common Knowledge Optimization For Image-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Signal Processing Letters</td>
    <td>5</td>
    <td><p>Image-text retrieval which associates different modalities has drawn broad
attention due to its excellent research value and broad real-world application.
However, most of the existing methods haven’t taken the high-level semantic
relationships (“style embedding”) and common knowledge from multi-modalities
into full consideration. To this end, we introduce a novel style transformer
network with common knowledge optimization (CKSTN) for image-text retrieval.
The main module is the common knowledge adaptor (CKA) with both the style
embedding extractor (SEE) and the common knowledge optimization (CKO) modules.
Specifically, the SEE uses the sequential update strategy to effectively
connect the features of different stages in SEE. The CKO module is introduced
to dynamically capture the latent concepts of common knowledge from different
modalities. Besides, to get generalized temporal common knowledge, we propose a
sequential update strategy to effectively integrate the features of different
layers in SEE with previous common feature units. CKSTN demonstrates the
superiorities of the state-of-the-art methods in image-text retrieval on MSCOCO
and Flickr30K datasets. Moreover, CKSTN is constructed based on the lightweight
transformer which is more convenient and practical for the application of real
scenes, due to the better performance and lower parameters.</p>
</td>
    <td>
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/leyvavallina2023data/">Data-efficient Large Scale Place Recognition With Graded Similarity Supervision</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Data-efficient Large Scale Place Recognition With Graded Similarity Supervision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Data-efficient Large Scale Place Recognition With Graded Similarity Supervision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Leyva-vallina Maria, Strisciuglio Nicola, Petkov Nicolai</td> <!-- 🔧 You were missing this -->
    <td>2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>23</td>
    <td><p>Visual place recognition (VPR) is a fundamental task of computer vision for
visual localization. Existing methods are trained using image pairs that either
depict the same place or not. Such a binary indication does not consider
continuous relations of similarity between images of the same place taken from
different positions, determined by the continuous nature of camera pose. The
binary similarity induces a noisy supervision signal into the training of VPR
methods, which stall in local minima and require expensive hard mining
algorithms to guarantee convergence. Motivated by the fact that two images of
the same place only partially share visual cues due to camera pose differences,
we deploy an automatic re-annotation strategy to re-label VPR datasets. We
compute graded similarity labels for image pairs based on available
localization metadata. Furthermore, we propose a new Generalized Contrastive
Loss (GCL) that uses graded similarity labels for training contrastive
networks. We demonstrate that the use of the new labels and GCL allow to
dispense from hard-pair mining, and to train image descriptors that perform
better in VPR by nearest neighbor search, obtaining superior or comparable
results than methods that require expensive hard-pair mining and re-ranking
techniques. Code and models available at:
https://github.com/marialeyvallina/generalized_contrastive_loss</p>
</td>
    <td>
      
        SCALABILITY 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/levi2023object/">Object-centric Open-vocabulary Image-retrieval With Aggregated Features</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Object-centric Open-vocabulary Image-retrieval With Aggregated Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Object-centric Open-vocabulary Image-retrieval With Aggregated Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Levi et al.</td> <!-- 🔧 You were missing this -->
    <td>Robotics: Science and Systems X</td>
    <td>40</td>
    <td><p>The task of open-vocabulary object-centric image retrieval involves the
retrieval of images containing a specified object of interest, delineated by an
open-set text query. As working on large image datasets becomes standard,
solving this task efficiently has gained significant practical importance.
Applications include targeted performance analysis of retrieved images using
ad-hoc queries and hard example mining during training. Recent advancements in
contrastive-based open vocabulary systems have yielded remarkable
breakthroughs, facilitating large-scale open vocabulary image retrieval.
However, these approaches use a single global embedding per image, thereby
constraining the system’s ability to retrieve images containing relatively
small object instances. Alternatively, incorporating local embeddings from
detection pipelines faces scalability challenges, making it unsuitable for
retrieval from large databases.
  In this work, we present a simple yet effective approach to object-centric
open-vocabulary image retrieval. Our approach aggregates dense embeddings
extracted from CLIP into a compact representation, essentially combining the
scalability of image retrieval pipelines with the object identification
capabilities of dense detection methods. We show the effectiveness of our
scheme to the task by achieving significantly better results than global
feature approaches on three datasets, increasing accuracy by up to 15 mAP
points. We further integrate our scheme into a large scale retrieval framework
and demonstrate our method’s advantages in terms of scalability and
interpretability.</p>
</td>
    <td>
      
        Image Retrieval 
      
        RSS 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/lin2023dense/">A Dense Representation Framework For Lexical And Semantic Matching</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Dense Representation Framework For Lexical And Semantic Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Dense Representation Framework For Lexical And Semantic Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin Sheng-chieh, Lin Jimmy</td> <!-- 🔧 You were missing this -->
    <td>ACM Transactions on Information Systems</td>
    <td>8</td>
    <td><p>Lexical and semantic matching capture different successful approaches to text
retrieval and the fusion of their results has proven to be more effective and
robust than either alone. Prior work performs hybrid retrieval by conducting
lexical and semantic matching using different systems (e.g., Lucene and Faiss,
respectively) and then fusing their model outputs. In contrast, our work
integrates lexical representations with dense semantic representations by
densifying high-dimensional lexical representations into what we call
low-dimensional dense lexical representations (DLRs). Our experiments show that
DLRs can effectively approximate the original lexical representations,
preserving effectiveness while improving query latency. Furthermore, we can
combine dense lexical and semantic representations to generate dense hybrid
representations (DHRs) that are more flexible and yield faster retrieval
compared to existing hybrid techniques. In addition, we explore it jointly
training lexical and semantic representations in a single model and empirically
show that the resulting DHRs are able to combine the advantages of the
individual components. Our best DHR model is competitive with state-of-the-art
single-vector and multi-vector dense retrievers in both in-domain and zero-shot
evaluation settings. Furthermore, our model is both faster and requires smaller
indexes, making our dense representation framework an attractive approach to
text retrieval. Our code is available at https://github.com/castorini/dhr.</p>
</td>
    <td>
      
        Tools & Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/lebailly2023cribo/">Cribo: Self-supervised Learning Via Cross-image Object-level Bootstrapping</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cribo: Self-supervised Learning Via Cross-image Object-level Bootstrapping' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cribo: Self-supervised Learning Via Cross-image Object-level Bootstrapping' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lebailly et al.</td> <!-- 🔧 You were missing this -->
    <td>Neurocomputing</td>
    <td>16</td>
    <td><p>Leveraging nearest neighbor retrieval for self-supervised representation
learning has proven beneficial with object-centric images. However, this
approach faces limitations when applied to scene-centric datasets, where
multiple objects within an image are only implicitly captured in the global
representation. Such global bootstrapping can lead to undesirable entanglement
of object representations. Furthermore, even object-centric datasets stand to
benefit from a finer-grained bootstrapping approach. In response to these
challenges, we introduce a novel Cross-Image Object-Level Bootstrapping method
tailored to enhance dense visual representation learning. By employing
object-level nearest neighbor bootstrapping throughout the training, CrIBo
emerges as a notably strong and adequate candidate for in-context learning,
leveraging nearest neighbor retrieval at test time. CrIBo shows
state-of-the-art performance on the latter task while being highly competitive
in more standard downstream segmentation tasks. Our code and pretrained models
are publicly available at https://github.com/tileb1/CrIBo.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Self SUPERVISED 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/kulkarni2023lexically/">Lexically-accelerated Dense Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Lexically-accelerated Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Lexically-accelerated Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kulkarni et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>13</td>
    <td><p>Retrieval approaches that score documents based on learned dense vectors
(i.e., dense retrieval) rather than lexical signals (i.e., conventional
retrieval) are increasingly popular. Their ability to identify related
documents that do not necessarily contain the same terms as those appearing in
the user’s query (thereby improving recall) is one of their key advantages.
However, to actually achieve these gains, dense retrieval approaches typically
require an exhaustive search over the document collection, making them
considerably more expensive at query-time than conventional lexical approaches.
Several techniques aim to reduce this computational overhead by approximating
the results of a full dense retriever. Although these approaches reasonably
approximate the top results, they suffer in terms of recall – one of the key
advantages of dense retrieval. We introduce ‘LADR’ (Lexically-Accelerated Dense
Retrieval), a simple-yet-effective approach that improves the efficiency of
existing dense retrieval models without compromising on retrieval
effectiveness. LADR uses lexical retrieval techniques to seed a dense retrieval
exploration that uses a document proximity graph. We explore two variants of
LADR: a proactive approach that expands the search space to the neighbors of
all seed documents, and an adaptive approach that selectively searches the
documents with the highest estimated relevance in an iterative fashion. Through
extensive experiments across a variety of dense retrieval models, we find that
LADR establishes a new dense retrieval effectiveness-efficiency Pareto frontier
among approximate k nearest neighbor techniques. Further, we find that when
tuned to take around 8ms per query in retrieval latency on our hardware, LADR
consistently achieves both precision and recall that are on par with an
exhaustive search on standard benchmarks.</p>
</td>
    <td>
      
        SIGIR 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/kim2023exposing/">Exposing And Mitigating Spurious Correlations For Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Exposing And Mitigating Spurious Correlations For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Exposing And Mitigating Spurious Correlations For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kim et al.</td> <!-- 🔧 You were missing this -->
    <td>2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>15</td>
    <td><p>Cross-modal retrieval methods are the preferred tool to search databases for
the text that best matches a query image and vice versa. However, image-text
retrieval models commonly learn to memorize spurious correlations in the
training data, such as frequent object co-occurrence, instead of looking at the
actual underlying reasons for the prediction in the image. For image-text
retrieval, this manifests in retrieved sentences that mention objects that are
not present in the query image. In this work, we introduce ODmAP@k, an object
decorrelation metric that measures a model’s robustness to spurious
correlations in the training data. We use automatic image and text
manipulations to control the presence of such object correlations in designated
test data. Additionally, our data synthesis technique is used to tackle model
biases due to spurious correlations of semantically unrelated objects in the
training data. We apply our proposed pipeline, which involves the finetuning of
image-text retrieval frameworks on carefully designed synthetic data, to three
state-of-the-art models for image-text retrieval. This results in significant
improvements for all three models, both in terms of the standard retrieval
performance and in terms of our object decorrelation metric. The code is
available at https://github.com/ExplainableML/Spurious_CM_Retrieval.</p>
</td>
    <td>
      
        Multimodal Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/kim2023improving/">Improving Cross-modal Retrieval With Set Of Diverse Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Improving Cross-modal Retrieval With Set Of Diverse Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Improving Cross-modal Retrieval With Set Of Diverse Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kim Dongwon, Kim Namyup, Kwak Suha</td> <!-- 🔧 You were missing this -->
    <td>2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>28</td>
    <td><p>Cross-modal retrieval across image and text modalities is a challenging task
due to its inherent ambiguity: An image often exhibits various situations, and
a caption can be coupled with diverse images. Set-based embedding has been
studied as a solution to this problem. It seeks to encode a sample into a set
of different embedding vectors that capture different semantics of the sample.
In this paper, we present a novel set-based embedding method, which is distinct
from previous work in two aspects. First, we present a new similarity function
called smooth-Chamfer similarity, which is designed to alleviate the side
effects of existing similarity functions for set-based embedding. Second, we
propose a novel set prediction module to produce a set of embedding vectors
that effectively captures diverse semantics of input by the slot attention
mechanism. Our method is evaluated on the COCO and Flickr30K datasets across
different visual backbones, where it outperforms existing methods including
ones that demand substantially larger computation at inference.</p>
</td>
    <td>
      
        Multimodal Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/kakarla2023zero/">Zero Shot Composed Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Zero Shot Composed Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Zero Shot Composed Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kakarla Santhosh, Venkata Gautama Shastry Bulusu</td> <!-- 🔧 You were missing this -->
    <td>2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>51</td>
    <td><p>Composed image retrieval (CIR) allows a user to locate a target image by applying a fine-grained textual edit (e.g., <code class="language-plaintext highlighter-rouge">turn the dress blue'' or</code>remove stripes’’) to a reference image. Zero-shot CIR, which embeds the image and the text with separate pretrained vision-language encoders, reaches only 20-25% Recall@10 on the FashionIQ benchmark. We improve this by fine-tuning BLIP-2 with a lightweight Q-Former that fuses visual and textual features into a single embedding, raising Recall@10 to 45.6% (shirt), 40.1% (dress), and 50.4% (top-tee) and increasing the average Recall@50 to 67.6%. We also examine Retrieval-DPO, which fine-tunes CLIP’s text encoder with a Direct Preference Optimization loss applied to FAISS-mined hard negatives. Despite extensive tuning of the scaling factor, index, and sampling strategy, Retrieval-DPO attains only 0.02% Recall@10 – far below zero-shot and prompt-tuned baselines – because it (i) lacks joint image-text fusion, (ii) uses a margin objective misaligned with top-\(K\) metrics, (iii) relies on low-quality negatives, and (iv) keeps the vision and Transformer layers frozen. Our results show that effective preference-based CIR requires genuine multimodal fusion, ranking-aware objectives, and carefully curated negatives.</p>
</td>
    <td>
      
        Image Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/tang2023renderers/">Renderers Are Good Zero-shot Representation Learners: Exploring Diffusion Latents For Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Renderers Are Good Zero-shot Representation Learners: Exploring Diffusion Latents For Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Renderers Are Good Zero-shot Representation Learners: Exploring Diffusion Latents For Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tang Michael, Shustin David</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</td>
    <td>7</td>
    <td><p>Can the latent spaces of modern generative neural rendering models serve as
representations for 3D-aware discriminative visual understanding tasks? We use
retrieval as a proxy for measuring the metric learning properties of the latent
spaces of Shap-E, including capturing view-independence and enabling the
aggregation of scene representations from the representations of individual
image views, and find that Shap-E representations outperform those of the
classical EfficientNet baseline representations zero-shot, and is still
competitive when both methods are trained using a contrative loss. These
findings give preliminary indication that 3D-based rendering and generative
models can yield useful representations for discriminative tasks in our
innately 3D-native world. Our code is available at
https://github.com/michaelwilliamtang/golden-retriever.</p>
</td>
    <td>
      
        Few Shot & Zero Shot 
      
        Distance Metric Learning 
      
        AAAI 
      
        IJCAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/tan2023unfolded/">Unfolded Self-reconstruction LSH: Towards Machine Unlearning In Approximate Nearest Neighbour Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unfolded Self-reconstruction LSH: Towards Machine Unlearning In Approximate Nearest Neighbour Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unfolded Self-reconstruction LSH: Towards Machine Unlearning In Approximate Nearest Neighbour Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tan et al.</td> <!-- 🔧 You were missing this -->
    <td>Electronics Letters</td>
    <td>5</td>
    <td><p>Approximate nearest neighbour (ANN) search is an essential component of
search engines, recommendation systems, etc. Many recent works focus on
learning-based data-distribution-dependent hashing and achieve good retrieval
performance. However, due to increasing demand for users’ privacy and security,
we often need to remove users’ data information from Machine Learning (ML)
models to satisfy specific privacy and security requirements. This need
requires the ANN search algorithm to support fast online data deletion and
insertion. Current learning-based hashing methods need retraining the hash
function, which is prohibitable due to the vast time-cost of large-scale data.
To address this problem, we propose a novel data-dependent hashing method named
unfolded self-reconstruction locality-sensitive hashing (USR-LSH). Our USR-LSH
unfolded the optimization update for instance-wise data reconstruction, which
is better for preserving data information than data-independent LSH. Moreover,
our USR-LSH supports fast online data deletion and insertion without
retraining. To the best of our knowledge, we are the first to address the
machine unlearning of retrieval problems. Empirically, we demonstrate that
USR-LSH outperforms the state-of-the-art data-distribution-independent LSH in
ANN tasks in terms of precision and recall. We also show that USR-LSH has
significantly faster data deletion and insertion time than learning-based
data-dependent hashing.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/tan2023multilingual/">Multilingual Representation Distillation With Contrastive Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multilingual Representation Distillation With Contrastive Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multilingual Representation Distillation With Contrastive Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tan et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics</td>
    <td>7</td>
    <td><p>Multilingual sentence representations from large models encode semantic
information from two or more languages and can be used for different
cross-lingual information retrieval and matching tasks. In this paper, we
integrate contrastive learning into multilingual representation distillation
and use it for quality estimation of parallel sentences (i.e., find
semantically similar sentences that can be used as translations of each other).
We validate our approach with multilingual similarity search and corpus
filtering tasks. Experiments across different low-resource languages show that
our method greatly outperforms previous sentence encoders such as LASER,
LASER3, and LaBSE.</p>
</td>
    <td>
      
        EACL 
      
        TACL 
      
        NAACL 
      
        ACL 
      
        Self SUPERVISED 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/song2023boosting/">Boosting Vision Transformers For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Boosting Vision Transformers For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Boosting Vision Transformers For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Song et al.</td> <!-- 🔧 You were missing this -->
    <td>2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>26</td>
    <td><p>Vision transformers have achieved remarkable progress in vision tasks such as
image classification and detection. However, in instance-level image retrieval,
transformers have not yet shown good performance compared to convolutional
networks. We propose a number of improvements that make transformers outperform
the state of the art for the first time. (1) We show that a hybrid architecture
is more effective than plain transformers, by a large margin. (2) We introduce
two branches collecting global (classification token) and local (patch tokens)
information, from which we form a global image representation. (3) In each
branch, we collect multi-layer features from the transformer encoder,
corresponding to skip connections across distant layers. (4) We enhance
locality of interactions at the deeper layers of the encoder, which is the
relative weakness of vision transformers. We train our model on all commonly
used training sets and, for the first time, we make fair comparisons separately
per training set. In all cases, we outperform previous models based on global
representation. Public code is available at
https://github.com/dealicious-inc/DToP.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/shen2023maru/">Maru: A Manga Retrieval And Understanding System Connecting Vision And Language</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Maru: A Manga Retrieval And Understanding System Connecting Vision And Language' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Maru: A Manga Retrieval And Understanding System Connecting Vision And Language' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shen Conghao Tom, Yao Violet, Liu Yixin</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>26</td>
    <td><p>Manga, a widely celebrated Japanese comic art form, is renowned for its
diverse narratives and distinct artistic styles. However, the inherently visual
and intricate structure of Manga, which comprises images housing multiple
panels, poses significant challenges for content retrieval. To address this, we
present MaRU (Manga Retrieval and Understanding), a multi-staged system that
connects vision and language to facilitate efficient search of both dialogues
and scenes within Manga frames. The architecture of MaRU integrates an object
detection model for identifying text and frame bounding boxes, a Vision
Encoder-Decoder model for text recognition, a text encoder for embedding text,
and a vision-text encoder that merges textual and visual information into a
unified embedding space for scene retrieval. Rigorous evaluations reveal that
MaRU excels in end-to-end dialogue retrieval and exhibits promising results for
scene retrieval.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/shao2023global/">Global Features Are All You Need For Image Retrieval And Reranking</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Global Features Are All You Need For Image Retrieval And Reranking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Global Features Are All You Need For Image Retrieval And Reranking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shao et al.</td> <!-- 🔧 You were missing this -->
    <td>2023 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>21</td>
    <td><p>Image retrieval systems conventionally use a two-stage paradigm, leveraging
global features for initial retrieval and local features for reranking.
However, the scalability of this method is often limited due to the significant
storage and computation cost incurred by local feature matching in the
reranking stage. In this paper, we present SuperGlobal, a novel approach that
exclusively employs global features for both stages, improving efficiency
without sacrificing accuracy. SuperGlobal introduces key enhancements to the
retrieval system, specifically focusing on the global feature extraction and
reranking processes. For extraction, we identify sub-optimal performance when
the widely-used ArcFace loss and Generalized Mean (GeM) pooling methods are
combined and propose several new modules to improve GeM pooling. In the
reranking stage, we introduce a novel method to update the global features of
the query and top-ranked images by only considering feature refinement with a
small set of images, thus being very compute and memory efficient. Our
experiments demonstrate substantial improvements compared to the state of the
art in standard benchmarks. Notably, on the Revisited Oxford+1M Hard dataset,
our single-stage results improve by 7.1%, while our two-stage gain reaches 3.7%
with a strong 64,865x speedup. Our two-stage system surpasses the current
single-stage state-of-the-art by 16.3%, offering a scalable, accurate
alternative for high-performing image retrieval systems with minimal time
overhead. Code: https://github.com/ShihaoShao-GH/SuperGlobal.</p>
</td>
    <td>
      
        Hybrid ANN Methods 
      
        Image Retrieval 
      
        Re RANKING 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/salemi2023symmetric/">A Symmetric Dual Encoding Dense Retrieval Framework For Knowledge-intensive Visual Question Answering</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Symmetric Dual Encoding Dense Retrieval Framework For Knowledge-intensive Visual Question Answering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Symmetric Dual Encoding Dense Retrieval Framework For Knowledge-intensive Visual Question Answering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Salemi Alireza, Pizzorno Juan Altmayer, Zamani Hamed</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>12</td>
    <td><p>Knowledge-Intensive Visual Question Answering (KI-VQA) refers to answering a
question about an image whose answer does not lie in the image. This paper
presents a new pipeline for KI-VQA tasks, consisting of a retriever and a
reader. First, we introduce DEDR, a symmetric dual encoding dense retrieval
framework in which documents and queries are encoded into a shared embedding
space using uni-modal (textual) and multi-modal encoders. We introduce an
iterative knowledge distillation approach that bridges the gap between the
representation spaces in these two encoders. Extensive evaluation on two
well-established KI-VQA datasets, i.e., OK-VQA and FVQA, suggests that DEDR
outperforms state-of-the-art baselines by 11.6% and 30.9% on OK-VQA and FVQA,
respectively. Utilizing the passages retrieved by DEDR, we further introduce
MM-FiD, an encoder-decoder multi-modal fusion-in-decoder model, for generating
a textual answer for KI-VQA tasks. MM-FiD encodes the question, the image, and
each retrieved passage separately and uses all passages jointly in its decoder.
Compared to competitive baselines in the literature, this approach leads to
5.5% and 8.5% improvements in terms of question answering accuracy on OK-VQA
and FVQA, respectively.</p>
</td>
    <td>
      
        SIGIR 
      
        Graph Based ANN 
      
        Text Retrieval 
      
        Tools & Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/sain2023exploiting/">Exploiting Unlabelled Photos For Stronger Fine-grained SBIR</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Exploiting Unlabelled Photos For Stronger Fine-grained SBIR' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Exploiting Unlabelled Photos For Stronger Fine-grained SBIR' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sain et al.</td> <!-- 🔧 You were missing this -->
    <td>2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>19</td>
    <td><p>This paper advances the fine-grained sketch-based image retrieval (FG-SBIR)
literature by putting forward a strong baseline that overshoots prior
state-of-the-arts by ~11%. This is not via complicated design though, but by
addressing two critical issues facing the community (i) the gold standard
triplet loss does not enforce holistic latent space geometry, and (ii) there
are never enough sketches to train a high accuracy model. For the former, we
propose a simple modification to the standard triplet loss, that explicitly
enforces separation amongst photos/sketch instances. For the latter, we put
forward a novel knowledge distillation module can leverage photo data for model
training. Both modules are then plugged into a novel plug-n-playable training
paradigm that allows for more stable training. More specifically, for (i) we
employ an intra-modal triplet loss amongst sketches to bring sketches of the
same instance closer from others, and one more amongst photos to push away
different photo instances while bringing closer a structurally augmented
version of the same photo (offering a gain of ~4-6%). To tackle (ii), we first
pre-train a teacher on the large set of unlabelled photos over the
aforementioned intra-modal photo triplet loss. Then we distill the contextual
similarity present amongst the instances in the teacher’s embedding space to
that in the student’s embedding space, by matching the distribution over
inter-feature distances of respective samples in both embedding spaces
(delivering a further gain of ~4-5%). Apart from outperforming prior arts
significantly, our model also yields satisfactory results on generalising to
new classes. Project page: https://aneeshan95.github.io/Sketch_PVT/</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/ribeiro2023sketch/">Sketch-an-anchor: Sub-epoch Fast Model Adaptation For Zero-shot Sketch-based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Sketch-an-anchor: Sub-epoch Fast Model Adaptation For Zero-shot Sketch-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Sketch-an-anchor: Sub-epoch Fast Model Adaptation For Zero-shot Sketch-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ribeiro Leo Sampaio Ferraz, Ponti Moacir Antonelli</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>16</td>
    <td><p>Sketch-an-Anchor is a novel method to train state-of-the-art Zero-shot
Sketch-based Image Retrieval (ZSSBIR) models in under an epoch. Most studies
break down the problem of ZSSBIR into two parts: domain alignment between
images and sketches, inherited from SBIR, and generalization to unseen data,
inherent to the zero-shot protocol. We argue one of these problems can be
considerably simplified and re-frame the ZSSBIR problem around the
already-stellar yet underexplored Zero-shot Image-based Retrieval performance
of off-the-shelf models. Our fast-converging model keeps the single-domain
performance while learning to extract similar representations from sketches. To
this end we introduce our Semantic Anchors – guiding embeddings learned from
word-based semantic spaces and features from off-the-shelf models – and
combine them with our novel Anchored Contrastive Loss. Empirical evidence shows
we can achieve state-of-the-art performance on all benchmark datasets while
training for 100x less iterations than other methods.</p>
</td>
    <td>
      
        Image Retrieval 
      
        Few Shot & Zero Shot 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/ravfogel2023description/">Description-based Text Similarity</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Description-based Text Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Description-based Text Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ravfogel et al.</td> <!-- 🔧 You were missing this -->
    <td>Sensors</td>
    <td>8</td>
    <td><p>Identifying texts with a given semantics is central for many information
seeking scenarios. Similarity search over vector embeddings appear to be
central to this ability, yet the similarity reflected in current text
embeddings is corpus-driven, and is inconsistent and sub-optimal for many use
cases. What, then, is a good notion of similarity for effective retrieval of
text?
  We identify the need to search for texts based on abstract descriptions of
their content, and the corresponding notion of <em>description based
similarity</em>. We demonstrate the inadequacy of current text embeddings and
propose an alternative model that significantly improves when used in standard
nearest neighbor search. The model is trained using positive and negative pairs
sourced through prompting a LLM, demonstrating how data from LLMs can be used
for creating new capabilities not immediately possible using the original
model.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/rajput2023recommender/">Recommender Systems With Generative Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Recommender Systems With Generative Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Recommender Systems With Generative Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Rajput et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>17</td>
    <td><p>Modern recommender systems perform large-scale retrieval by first embedding
queries and item candidates in the same unified space, followed by approximate
nearest neighbor search to select top candidates given a query embedding. In
this paper, we propose a novel generative retrieval approach, where the
retrieval model autoregressively decodes the identifiers of the target
candidates. To that end, we create semantically meaningful tuple of codewords
to serve as a Semantic ID for each item. Given Semantic IDs for items in a user
session, a Transformer-based sequence-to-sequence model is trained to predict
the Semantic ID of the next item that the user will interact with. To the best
of our knowledge, this is the first Semantic ID-based generative model for
recommendation tasks. We show that recommender systems trained with the
proposed paradigm significantly outperform the current SOTA models on various
datasets. In addition, we show that incorporating Semantic IDs into the
sequence-to-sequence model enhances its ability to generalize, as evidenced by
the improved retrieval performance observed for items with no prior interaction
history.</p>
</td>
    <td>
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/qu2023learnable/">Learnable Pillar-based Re-ranking For Image-text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learnable Pillar-based Re-ranking For Image-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learnable Pillar-based Re-ranking For Image-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Qu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>9</td>
    <td><p>Image-text retrieval aims to bridge the modality gap and retrieve cross-modal
content based on semantic similarities. Prior work usually focuses on the
pairwise relations (i.e., whether a data sample matches another) but ignores
the higher-order neighbor relations (i.e., a matching structure among multiple
data samples). Re-ranking, a popular post-processing practice, has revealed the
superiority of capturing neighbor relations in single-modality retrieval tasks.
However, it is ineffective to directly extend existing re-ranking algorithms to
image-text retrieval. In this paper, we analyze the reason from four
perspectives, i.e., generalization, flexibility, sparsity, and asymmetry, and
propose a novel learnable pillar-based re-ranking paradigm. Concretely, we
first select top-ranked intra- and inter-modal neighbors as pillars, and then
reconstruct data samples with the neighbor relations between them and the
pillars. In this way, each sample can be mapped into a multimodal pillar space
only using similarities, ensuring generalization. After that, we design a
neighbor-aware graph reasoning module to flexibly exploit the relations and
excavate the sparse positive items within a neighborhood. We also present a
structure alignment constraint to promote cross-modal collaboration and align
the asymmetric modalities. On top of various base backbones, we carry out
extensive experiments on two benchmark datasets, i.e., Flickr30K and MS-COCO,
demonstrating the effectiveness, superiority, generalization, and
transferability of our proposed re-ranking paradigm.</p>
</td>
    <td>
      
        SIGIR 
      
        Text Retrieval 
      
        Hybrid ANN Methods 
      
        Re RANKING 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/peng2023embedding/">Embedding-based Retrieval With LLM For Effective Agriculture Information Extracting From Unstructured Data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Embedding-based Retrieval With LLM For Effective Agriculture Information Extracting From Unstructured Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Embedding-based Retrieval With LLM For Effective Agriculture Information Extracting From Unstructured Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Peng et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>14</td>
    <td><p>Pest identification is a crucial aspect of pest control in agriculture.
However, most farmers are not capable of accurately identifying pests in the
field, and there is a limited number of structured data sources available for
rapid querying. In this work, we explored using domain-agnostic general
pre-trained large language model(LLM) to extract structured data from
agricultural documents with minimal or no human intervention. We propose a
methodology that involves text retrieval and filtering using embedding-based
retrieval, followed by LLM question-answering to automatically extract entities
and attributes from the documents, and transform them into structured data. In
comparison to existing methods, our approach achieves consistently better
accuracy in the benchmark while maintaining efficiency.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/rabbani2023large/">Large-scale Distributed Learning Via Private On-device Locality-sensitive Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Large-scale Distributed Learning Via Private On-device Locality-sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Large-scale Distributed Learning Via Private On-device Locality-sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Rabbani Tahseen, Bornstein Marco, Huang Furong</td> <!-- 🔧 You were missing this -->
    <td>2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</td>
    <td>7</td>
    <td><p>Locality-sensitive hashing (LSH) based frameworks have been used efficiently
to select weight vectors in a dense hidden layer with high cosine similarity to
an input, enabling dynamic pruning. While this type of scheme has been shown to
improve computational training efficiency, existing algorithms require repeated
randomized projection of the full layer weight, which is impractical for
computational- and memory-constrained devices. In a distributed setting,
deferring LSH analysis to a centralized host is (i) slow if the device cluster
is large and (ii) requires access to input data which is forbidden in a
federated context. Using a new family of hash functions, we develop one of the
first private, personalized, and memory-efficient on-device LSH frameworks. Our
framework enables privacy and personalization by allowing each device to
generate hash tables, without the help of a central host, using device-specific
hashing hyper-parameters (e.g. number of hash tables or hash length). Hash
tables are generated with a compressed set of the full weights, and can be
serially generated and discarded if the process is memory-intensive. This
allows devices to avoid maintaining (i) the fully-sized model and (ii) large
amounts of hash tables in local memory for LSH analysis. We prove several
statistical and sensitivity properties of our hash functions, and
experimentally demonstrate that our framework is competitive in training
large-scale recommender networks compared to other LSH frameworks which assume
unrestricted on-device capacity.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Hashing Methods 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/ouldnoughi2023clip/">CLIP-GCD: Simple Language Guided Generalized Category Discovery</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=CLIP-GCD: Simple Language Guided Generalized Category Discovery' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=CLIP-GCD: Simple Language Guided Generalized Category Discovery' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ouldnoughi Rabah, Kuo Chia-wen, Kira Zsolt</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>121</td>
    <td><p>Generalized Category Discovery (GCD) requires a model to both classify known
categories and cluster unknown categories in unlabeled data. Prior methods
leveraged self-supervised pre-training combined with supervised fine-tuning on
the labeled data, followed by simple clustering methods. In this paper, we
posit that such methods are still prone to poor performance on
out-of-distribution categories, and do not leverage a key ingredient: Semantic
relationships between object categories. We therefore propose to leverage
multi-modal (vision and language) models, in two complementary ways. First, we
establish a strong baseline by replacing uni-modal features with CLIP, inspired
by its zero-shot performance. Second, we propose a novel retrieval-based
mechanism that leverages CLIP’s aligned vision-language representations by
mining text descriptions from a text corpus for the labeled and unlabeled set.
We specifically use the alignment between CLIP’s visual encoding of the image
and textual encoding of the corpus to retrieve top-k relevant pieces of text
and incorporate their embeddings to perform joint image+text semi-supervised
clustering. We perform rigorous experimentation and ablations (including on
where to retrieve from, how much to retrieve, and how to combine information),
and validate our results on several datasets including out-of-distribution
domains, demonstrating state-of-art results.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/ning2023multi/">Multi-domain Recommendation With Embedding Disentangling And Domain Alignment</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multi-domain Recommendation With Embedding Disentangling And Domain Alignment' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multi-domain Recommendation With Embedding Disentangling And Domain Alignment' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ning et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 32nd ACM International Conference on Information and Knowledge Management</td>
    <td>11</td>
    <td><p>Multi-domain recommendation (MDR) aims to provide recommendations for
different domains (e.g., types of products) with overlapping users/items and is
common for platforms such as Amazon, Facebook, and LinkedIn that host multiple
services. Existing MDR models face two challenges: First, it is difficult to
disentangle knowledge that generalizes across domains (e.g., a user likes cheap
items) and knowledge specific to a single domain (e.g., a user likes blue
clothing but not blue cars). Second, they have limited ability to transfer
knowledge across domains with small overlaps. We propose a new MDR method named
EDDA with two key components, i.e., embedding disentangling recommender and
domain alignment, to tackle the two challenges respectively. In particular, the
embedding disentangling recommender separates both the model and embedding for
the inter-domain part and the intra-domain part, while most existing MDR
methods only focus on model-level disentangling. The domain alignment leverages
random walks from graph processing to identify similar user/item pairs from
different domains and encourages similar user/item pairs to have similar
embeddings, enhancing knowledge transfer. We compare EDDA with 12
state-of-the-art baselines on 3 real datasets. The results show that EDDA
consistently outperforms the baselines on all datasets and domains. All
datasets and codes are available at https://github.com/Stevenn9981/EDDA.</p>
</td>
    <td>
      
        CIKM 
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/ng2023unsupervised/">Unsupervised Hashing With Similarity Distribution Calibration</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Hashing With Similarity Distribution Calibration' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Hashing With Similarity Distribution Calibration' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ng et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>15</td>
    <td><p>Unsupervised hashing methods typically aim to preserve the similarity between
data points in a feature space by mapping them to binary hash codes. However,
these methods often overlook the fact that the similarity between data points
in the continuous feature space may not be preserved in the discrete hash code
space, due to the limited similarity range of hash codes. The similarity range
is bounded by the code length and can lead to a problem known as similarity
collapse. That is, the positive and negative pairs of data points become less
distinguishable from each other in the hash space. To alleviate this problem,
in this paper a novel Similarity Distribution Calibration (SDC) method is
introduced. SDC aligns the hash code similarity distribution towards a
calibration distribution (e.g., beta distribution) with sufficient spread
across the entire similarity range, thus alleviating the similarity collapse
problem. Extensive experiments show that our SDC outperforms significantly the
state-of-the-art alternatives on coarse category-level and instance-level image
retrieval. Code is available at https://github.com/kamwoh/sdc.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        SUPERVISED 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/nguyen2023generative/">Generative Retrieval As Dense Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Generative Retrieval As Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Generative Retrieval As Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Nguyen Thong, Yates Andrew</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>6</td>
    <td><p>Generative retrieval is a promising new neural retrieval paradigm that aims
to optimize the retrieval pipeline by performing both indexing and retrieval
with a single transformer model. However, this new paradigm faces challenges
with updating the index and scaling to large collections. In this paper, we
analyze two prominent variants of generative retrieval and show that they can
be conceptually viewed as bi-encoders for dense retrieval. Specifically, we
analytically demonstrate that the generative retrieval process can be
decomposed into dot products between query and document vectors, similar to
dense retrieval. This analysis leads us to propose a new variant of generative
retrieval, called Tied-Atomic, which addresses the updating and scaling issues
by incorporating techniques from dense retrieval. In experiments on two
datasets, NQ320k and the full MSMARCO, we confirm that this approach does not
reduce retrieval effectiveness while enabling the model to scale to large
collections.</p>
</td>
    <td>
      
        SIGIR 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/wei2023chain/">CHAIN: Exploring Global-local Spatio-temporal Information For Improved Self-supervised Video Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=CHAIN: Exploring Global-local Spatio-temporal Information For Improved Self-supervised Video Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=CHAIN: Exploring Global-local Spatio-temporal Information For Improved Self-supervised Video Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wei et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 31st ACM International Conference on Multimedia</td>
    <td>5</td>
    <td><p>Compressing videos into binary codes can improve retrieval speed and reduce
storage overhead. However, learning accurate hash codes for video retrieval can
be challenging due to high local redundancy and complex global dependencies
between video frames, especially in the absence of labels. Existing
self-supervised video hashing methods have been effective in designing
expressive temporal encoders, but have not fully utilized the temporal dynamics
and spatial appearance of videos due to less challenging and unreliable
learning tasks. To address these challenges, we begin by utilizing the
contrastive learning task to capture global spatio-temporal information of
videos for hashing. With the aid of our designed augmentation strategies, which
focus on spatial and temporal variations to create positive pairs, the learning
framework can generate hash codes that are invariant to motion, scale, and
viewpoint. Furthermore, we incorporate two collaborative learning tasks, i.e.,
frame order verification and scene change regularization, to capture local
spatio-temporal details within video frames, thereby enhancing the perception
of temporal structure and the modeling of spatio-temporal relationships. Our
proposed Contrastive Hashing with Global-Local Spatio-temporal Information
(CHAIN) outperforms state-of-the-art self-supervised video hashing methods on
four video benchmark datasets. Our codes will be released.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
        Self SUPERVISED 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/weng2023constant/">Constant Sequence Extension For Fast Search Using Weighted Hamming Distance</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Constant Sequence Extension For Fast Search Using Weighted Hamming Distance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Constant Sequence Extension For Fast Search Using Weighted Hamming Distance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Weng et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>8</td>
    <td><p>Representing visual data using compact binary codes is attracting increasing
attention as binary codes are used as direct indices into hash table(s) for
fast non-exhaustive search. Recent methods show that ranking binary codes using
weighted Hamming distance (WHD) rather than Hamming distance (HD) by generating
query-adaptive weights for each bit can better retrieve query-related items.
However, search using WHD is slower than that using HD. One main challenge is
that the complexity of extending a monotone increasing sequence using WHD to
probe buckets in hash table(s) for existing methods is at least proportional to
the square of the sequence length, while that using HD is proportional to the
sequence length. To overcome this challenge, we propose a novel fast
non-exhaustive search method using WHD. The key idea is to design a constant
sequence extension algorithm to perform each sequence extension in constant
computational complexity and the total complexity is proportional to the
sequence length, which is justified by theoretical analysis. Experimental
results show that our method is faster than other WHD-based search methods.
Also, compared with the HD-based non-exhaustive search method, our method has
comparable efficiency but retrieves more query-related items for the dataset of
up to one billion items.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/wang2023unified/">Unified Coarse-to-fine Alignment For Video-text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unified Coarse-to-fine Alignment For Video-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unified Coarse-to-fine Alignment For Video-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>2023 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>20</td>
    <td><p>The canonical approach to video-text retrieval leverages a coarse-grained or
fine-grained alignment between visual and textual information. However,
retrieving the correct video according to the text query is often challenging
as it requires the ability to reason about both high-level (scene) and
low-level (object) visual clues and how they relate to the text query. To this
end, we propose a Unified Coarse-to-fine Alignment model, dubbed UCoFiA.
Specifically, our model captures the cross-modal similarity information at
different granularity levels. To alleviate the effect of irrelevant visual
clues, we also apply an Interactive Similarity Aggregation module (ISA) to
consider the importance of different visual features while aggregating the
cross-modal similarity to obtain a similarity score for each granularity.
Finally, we apply the Sinkhorn-Knopp algorithm to normalize the similarities of
each level before summing them, alleviating over- and under-representation
issues at different levels. By jointly considering the crossmodal similarity of
different granularity, UCoFiA allows the effective unification of multi-grained
alignments. Empirically, UCoFiA outperforms previous state-of-the-art
CLIP-based methods on multiple video-text retrieval benchmarks, achieving 2.4%,
1.4% and 1.3% improvements in text-to-video retrieval R@1 on MSR-VTT,
Activity-Net, and DiDeMo, respectively. Our code is publicly available at
https://github.com/Ziyang412/UCoFiA.</p>
</td>
    <td>
      
        Text Retrieval 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/wei2023attribute/">Attribute-aware Deep Hashing With Self-consistency For Large-scale Fine-grained Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Attribute-aware Deep Hashing With Self-consistency For Large-scale Fine-grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Attribute-aware Deep Hashing With Self-consistency For Large-scale Fine-grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wei et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>16</td>
    <td><p>Our work focuses on tackling large-scale fine-grained image retrieval as
ranking the images depicting the concept of interests (i.e., the same
sub-category labels) highest based on the fine-grained details in the query. It
is desirable to alleviate the challenges of both fine-grained nature of small
inter-class variations with large intra-class variations and explosive growth
of fine-grained data for such a practical task. In this paper, we propose
attribute-aware hashing networks with self-consistency for generating
attribute-aware hash codes to not only make the retrieval process efficient,
but also establish explicit correspondences between hash codes and visual
attributes. Specifically, based on the captured visual representations by
attention, we develop an encoder-decoder structure network of a reconstruction
task to unsupervisedly distill high-level attribute-specific vectors from the
appearance-specific visual representations without attribute annotations. Our
models are also equipped with a feature decorrelation constraint upon these
attribute vectors to strengthen their representative abilities. Then, driven by
preserving original entities’ similarity, the required hash codes can be
generated from these attribute-specific vectors and thus become
attribute-aware. Furthermore, to combat simplicity bias in deep hashing, we
consider the model design from the perspective of the self-consistency
principle and propose to further enhance models’ self-consistency by equipping
an additional image reconstruction path. Comprehensive quantitative experiments
under diverse empirical settings on six fine-grained retrieval datasets and two
generic retrieval datasets show the superiority of our models over competing
methods.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        Image Retrieval 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/wang2023graph/">Graph-collaborated Auto-encoder Hashing For Multi-view Binary Clustering</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Graph-collaborated Auto-encoder Hashing For Multi-view Binary Clustering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Graph-collaborated Auto-encoder Hashing For Multi-view Binary Clustering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Neural Networks and Learning Systems</td>
    <td>56</td>
    <td><p>Unsupervised hashing methods have attracted widespread attention with the
explosive growth of large-scale data, which can greatly reduce storage and
computation by learning compact binary codes. Existing unsupervised hashing
methods attempt to exploit the valuable information from samples, which fails
to take the local geometric structure of unlabeled samples into consideration.
Moreover, hashing based on auto-encoders aims to minimize the reconstruction
loss between the input data and binary codes, which ignores the potential
consistency and complementarity of multiple sources data. To address the above
issues, we propose a hashing algorithm based on auto-encoders for multi-view
binary clustering, which dynamically learns affinity graphs with low-rank
constraints and adopts collaboratively learning between auto-encoders and
affinity graphs to learn a unified binary code, called Graph-Collaborated
Auto-Encoder Hashing for Multi-view Binary Clustering (GCAE). Specifically, we
propose a multi-view affinity graphs learning model with low-rank constraint,
which can mine the underlying geometric information from multi-view data. Then,
we design an encoder-decoder paradigm to collaborate the multiple affinity
graphs, which can learn a unified binary code effectively. Notably, we impose
the decorrelation and code balance constraints on binary codes to reduce the
quantization errors. Finally, we utilize an alternating iterative optimization
scheme to obtain the multi-view clustering results. Extensive experimental
results on \(5\) public datasets are provided to reveal the effectiveness of the
algorithm and its superior performance over other state-of-the-art
alternatives.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/wang2023contrastive/">Contrastive Masked Autoencoders For Self-supervised Video Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Contrastive Masked Autoencoders For Self-supervised Video Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Contrastive Masked Autoencoders For Self-supervised Video Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>17</td>
    <td><p>Self-Supervised Video Hashing (SSVH) models learn to generate short binary
representations for videos without ground-truth supervision, facilitating
large-scale video retrieval efficiency and attracting increasing research
attention. The success of SSVH lies in the understanding of video content and
the ability to capture the semantic relation among unlabeled videos. Typically,
state-of-the-art SSVH methods consider these two points in a two-stage training
pipeline, where they firstly train an auxiliary network by instance-wise
mask-and-predict tasks and secondly train a hashing model to preserve the
pseudo-neighborhood structure transferred from the auxiliary network. This
consecutive training strategy is inflexible and also unnecessary. In this
paper, we propose a simple yet effective one-stage SSVH method called ConMH,
which incorporates video semantic information and video similarity relationship
understanding in a single stage. To capture video semantic information for
better hashing learning, we adopt an encoder-decoder structure to reconstruct
the video from its temporal-masked frames. Particularly, we find that a higher
masking ratio helps video understanding. Besides, we fully exploit the
similarity relationship between videos by maximizing agreement between two
augmented views of a video, which contributes to more discriminative and robust
hash codes. Extensive experiments on three large-scale video datasets (i.e.,
FCVID, ActivityNet and YFCC) indicate that ConMH achieves state-of-the-art
results. Code is available at https://github.com/huangmozhi9527/ConMH.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
        AAAI 
      
        Self SUPERVISED 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/wang2023correspondence/">Correspondence-free Domain Alignment For Unsupervised Cross-domain Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Correspondence-free Domain Alignment For Unsupervised Cross-domain Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Correspondence-free Domain Alignment For Unsupervised Cross-domain Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>7</td>
    <td><p>Cross-domain image retrieval aims at retrieving images across different
domains to excavate cross-domain classificatory or correspondence
relationships. This paper studies a less-touched problem of cross-domain image
retrieval, i.e., unsupervised cross-domain image retrieval, considering the
following practical assumptions: (i) no correspondence relationship, and (ii)
no category annotations. It is challenging to align and bridge distinct domains
without cross-domain correspondence. To tackle the challenge, we present a
novel Correspondence-free Domain Alignment (CoDA) method to effectively
eliminate the cross-domain gap through In-domain Self-matching Supervision
(ISS) and Cross-domain Classifier Alignment (CCA). To be specific, ISS is
presented to encapsulate discriminative information into the latent common
space by elaborating a novel self-matching supervision mechanism. To alleviate
the cross-domain discrepancy, CCA is proposed to align distinct domain-specific
classifiers. Thanks to the ISS and CCA, our method could encode the
discrimination into the domain-invariant embedding space for unsupervised
cross-domain image retrieval. To verify the effectiveness of the proposed
method, extensive experiments are conducted on four benchmark datasets compared
with six state-of-the-art methods.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Image Retrieval 
      
        AAAI 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/wang2023note/">A Note On "efficient Task-specific Data Valuation For Nearest Neighbor Algorithms"</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Note On "efficient Task-specific Data Valuation For Nearest Neighbor Algorithms"' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Note On "efficient Task-specific Data Valuation For Nearest Neighbor Algorithms"' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Jiachen T., Jia Ruoxi</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the VLDB Endowment</td>
    <td>98</td>
    <td><p>Data valuation is a growing research field that studies the influence of
individual data points for machine learning (ML) models. Data Shapley, inspired
by cooperative game theory and economics, is an effective method for data
valuation. However, it is well-known that the Shapley value (SV) can be
computationally expensive. Fortunately, Jia et al. (2019) showed that for
K-Nearest Neighbors (KNN) models, the computation of Data Shapley is
surprisingly simple and efficient.
  In this note, we revisit the work of Jia et al. (2019) and propose a more
natural and interpretable utility function that better reflects the performance
of KNN models. We derive the corresponding calculation procedure for the Data
Shapley of KNN classifiers/regressors with the new utility functions. Our new
approach, dubbed soft-label KNN-SV, achieves the same time complexity as the
original method. We further provide an efficient approximation algorithm for
soft-label KNN-SV based on locality sensitive hashing (LSH). Our experimental
results demonstrate that Soft-label KNN-SV outperforms the original method on
most datasets in the task of mislabeled data detection, making it a better
baseline for future work on data valuation.</p>
</td>
    <td>
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/wang2023reliable/">Reliable And Efficient Evaluation Of Adversarial Robustness For Deep Hashing-based Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Reliable And Efficient Evaluation Of Adversarial Robustness For Deep Hashing-based Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Reliable And Efficient Evaluation Of Adversarial Robustness For Deep Hashing-based Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Information Forensics and Security</td>
    <td>21</td>
    <td><p>Deep hashing has been extensively applied to massive image retrieval due to
its efficiency and effectiveness. Recently, several adversarial attacks have
been presented to reveal the vulnerability of deep hashing models against
adversarial examples. However, existing attack methods suffer from degraded
performance or inefficiency because they underutilize the semantic relations
between original samples or spend a lot of time learning these relations with a
deep neural network. In this paper, we propose a novel Pharos-guided Attack,
dubbed PgA, to evaluate the adversarial robustness of deep hashing networks
reliably and efficiently. Specifically, we design pharos code to represent the
semantics of the benign image, which preserves the similarity to semantically
relevant samples and dissimilarity to irrelevant ones. It is proven that we can
quickly calculate the pharos code via a simple math formula. Accordingly, PgA
can directly conduct a reliable and efficient attack on deep hashing-based
retrieval by maximizing the similarity between the hash code of the adversarial
example and the pharos code. Extensive experiments on the benchmark datasets
verify that the proposed algorithm outperforms the prior state-of-the-arts in
both attack strength and speed.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        Robustness 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/xing2023multimorbidity/">Multimorbidity Content-based Medical Image Retrieval Using Proxies</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multimorbidity Content-based Medical Image Retrieval Using Proxies' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multimorbidity Content-based Medical Image Retrieval Using Proxies' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xing et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Access</td>
    <td>12</td>
    <td><p>Content-based medical image retrieval is an important diagnostic tool that
improves the explainability of computer-aided diagnosis systems and provides
decision making support to healthcare professionals. Medical imaging data, such
as radiology images, are often multimorbidity; a single sample may have more
than one pathology present. As such, image retrieval systems for the medical
domain must be designed for the multi-label scenario. In this paper, we propose
a novel multi-label metric learning method that can be used for both
classification and content-based image retrieval. In this way, our model is
able to support diagnosis by predicting the presence of diseases and provide
evidence for these predictions by returning samples with similar pathological
content to the user. In practice, the retrieved images may also be accompanied
by pathology reports, further assisting in the diagnostic process. Our method
leverages proxy feature vectors, enabling the efficient learning of a robust
feature space in which the distance between feature vectors can be used as a
measure of the similarity of those samples. Unlike existing proxy-based
methods, training samples are able to assign to multiple proxies that span
multiple class labels. This multi-label proxy assignment results in a feature
space that encodes the complex relationships between diseases present in
medical imaging data. Our method outperforms state-of-the-art image retrieval
systems and a set of baseline approaches. We demonstrate the efficacy of our
approach to both classification and content-based image retrieval on two
multimorbidity radiology datasets.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/wu2023pix2map/">Pix2map: Cross-modal Retrieval For Inferring Street Maps From Images</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Pix2map: Cross-modal Retrieval For Inferring Street Maps From Images' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Pix2map: Cross-modal Retrieval For Inferring Street Maps From Images' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu et al.</td> <!-- 🔧 You were missing this -->
    <td>2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>5</td>
    <td><p>Self-driving vehicles rely on urban street maps for autonomous navigation. In
this paper, we introduce Pix2Map, a method for inferring urban street map
topology directly from ego-view images, as needed to continually update and
expand existing maps. This is a challenging task, as we need to infer a complex
urban road topology directly from raw image data. The main insight of this
paper is that this problem can be posed as cross-modal retrieval by learning a
joint, cross-modal embedding space for images and existing maps, represented as
discrete graphs that encode the topological layout of the visual surroundings.
We conduct our experimental evaluation using the Argoverse dataset and show
that it is indeed possible to accurately retrieve street maps corresponding to
both seen and unseen roads solely from image data. Moreover, we show that our
retrieved maps can be used to update or expand existing maps and even show
proof-of-concept results for visual localization and image retrieval from
spatial graphs.</p>
</td>
    <td>
      
        Multimodal Retrieval 
      
        Evaluation 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/wu2023forb/">FORB: A Flat Object Retrieval Benchmark For Universal Image Embedding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=FORB: A Flat Object Retrieval Benchmark For Universal Image Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=FORB: A Flat Object Retrieval Benchmark For Universal Image Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>6</td>
    <td><p>Image retrieval is a fundamental task in computer vision. Despite recent
advances in this field, many techniques have been evaluated on a limited number
of domains, with a small number of instance categories. Notably, most existing
works only consider domains like 3D landmarks, making it difficult to
generalize the conclusions made by these works to other domains, e.g., logo and
other 2D flat objects. To bridge this gap, we introduce a new dataset for
benchmarking visual search methods on flat images with diverse patterns. Our
flat object retrieval benchmark (FORB) supplements the commonly adopted 3D
object domain, and more importantly, it serves as a testbed for assessing the
image embedding quality on out-of-distribution domains. In this benchmark we
investigate the retrieval accuracy of representative methods in terms of
candidate ranks, as well as matching score margin, a viewpoint which is largely
ignored by many works. Our experiments not only highlight the challenges and
rich heterogeneity of FORB, but also reveal the hidden properties of different
retrieval strategies. The proposed benchmark is a growing project and we expect
to expand in both quantity and variety of objects. The dataset and supporting
codes are available at https://github.com/pxiangwu/FORB/.</p>
</td>
    <td>
      
        Evaluation 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/wu2023distribution/">Distribution Aligned Feature Clustering For Zero-shot Sketch-based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Distribution Aligned Feature Clustering For Zero-shot Sketch-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Distribution Aligned Feature Clustering For Zero-shot Sketch-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu et al.</td> <!-- 🔧 You were missing this -->
    <td>Neural Processing Letters</td>
    <td>5</td>
    <td><p>Zero-Shot Sketch-Based Image Retrieval (ZS-SBIR) is a challenging cross-modal
retrieval task. In prior arts, the retrieval is conducted by sorting the
distance between the query sketch and each image in the gallery. However, the
domain gap and the zero-shot setting make neural networks hard to generalize.
This paper tackles the challenges from a new perspective: utilizing gallery
image features. We propose a Cluster-then-Retrieve (ClusterRetri) method that
performs clustering on the gallery images and uses the cluster centroids as
proxies for retrieval. Furthermore, a distribution alignment loss is proposed
to align the image and sketch features with a common Gaussian distribution,
reducing the domain gap. Despite its simplicity, our proposed method
outperforms the state-of-the-art methods by a large margin on popular datasets,
e.g., up to 31% and 39% relative improvement of mAP@all on the Sketchy and
TU-Berlin datasets.</p>
</td>
    <td>
      
        Image Retrieval 
      
        Few Shot & Zero Shot 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/tu2023unsupervised/">Unsupervised Hashing With Semantic Concept Mining</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Hashing With Semantic Concept Mining' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Hashing With Semantic Concept Mining' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the ACM on Management of Data</td>
    <td>6</td>
    <td><p>Recently, to improve the unsupervised image retrieval performance, plenty of
unsupervised hashing methods have been proposed by designing a semantic
similarity matrix, which is based on the similarities between image features
extracted by a pre-trained CNN model. However, most of these methods tend to
ignore high-level abstract semantic concepts contained in images. Intuitively,
concepts play an important role in calculating the similarity among images. In
real-world scenarios, each image is associated with some concepts, and the
similarity between two images will be larger if they share more identical
concepts. Inspired by the above intuition, in this work, we propose a novel
Unsupervised Hashing with Semantic Concept Mining, called UHSCM, which
leverages a VLP model to construct a high-quality similarity matrix.
Specifically, a set of randomly chosen concepts is first collected. Then, by
employing a vision-language pretraining (VLP) model with the prompt engineering
which has shown strong power in visual representation learning, the set of
concepts is denoised according to the training images. Next, the proposed
method UHSCM applies the VLP model with prompting again to mine the concept
distribution of each image and construct a high-quality semantic similarity
matrix based on the mined concept distributions. Finally, with the semantic
similarity matrix as guiding information, a novel hashing loss with a modified
contrastive loss based regularization item is proposed to optimize the hashing
network. Extensive experiments on three benchmark datasets show that the
proposed method outperforms the state-of-the-art baselines in the image
retrieval task.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        SUPERVISED 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/trivigno2023divide/">Divide&classify: Fine-grained Classification For City-wide Visual Place Recognition</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Divide&classify: Fine-grained Classification For City-wide Visual Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Divide&classify: Fine-grained Classification For City-wide Visual Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Trivigno et al.</td> <!-- 🔧 You were missing this -->
    <td>2023 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>7</td>
    <td><p>Visual Place recognition is commonly addressed as an image retrieval problem.
However, retrieval methods are impractical to scale to large datasets, densely
sampled from city-wide maps, since their dimension impact negatively on the
inference time. Using approximate nearest neighbour search for retrieval helps
to mitigate this issue, at the cost of a performance drop. In this paper we
investigate whether we can effectively approach this task as a classification
problem, thus bypassing the need for a similarity search. We find that existing
classification methods for coarse, planet-wide localization are not suitable
for the fine-grained and city-wide setting. This is largely due to how the
dataset is split into classes, because these methods are designed to handle a
sparse distribution of photos and as such do not consider the visual aliasing
problem across neighbouring classes that naturally arises in dense scenarios.
Thus, we propose a partitioning scheme that enables a fast and accurate
inference, preserving a simple learning procedure, and a novel inference
pipeline based on an ensemble of novel classifiers that uses the prototypes
learned via an angular margin loss. Our method, Divide&amp;Classify (D&amp;C), enjoys
the fast inference of classification solutions and an accuracy competitive with
retrieval methods on the fine-grained, city-wide setting. Moreover, we show
that D&amp;C can be paired with existing retrieval pipelines to speed up
computations by over 20 times while increasing their recall, leading to new
state-of-the-art results.</p>
</td>
    <td>
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/lin2023rafic/">RAFIC: Retrieval-augmented Few-shot Image Classification</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=RAFIC: Retrieval-augmented Few-shot Image Classification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=RAFIC: Retrieval-augmented Few-shot Image Classification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin Hangfei, Miao Li, Ziai Amir</td> <!-- 🔧 You were missing this -->
    <td>Findings of the Association for Computational Linguistics: EMNLP 2023</td>
    <td>5</td>
    <td><p>Few-shot image classification is the task of classifying unseen images to one
of N mutually exclusive classes, using only a small number of training examples
for each class. The limited availability of these examples (denoted as K)
presents a significant challenge to classification accuracy in some cases. To
address this, we have developed a method for augmenting the set of K with an
addition set of A retrieved images. We call this system Retrieval-Augmented
Few-shot Image Classification (RAFIC). Through a series of experiments, we
demonstrate that RAFIC markedly improves performance of few-shot image
classification across two challenging datasets. RAFIC consists of two main
components: (a) a retrieval component which uses CLIP, LAION-5B, and faiss, in
order to efficiently retrieve images similar to the supplied images, and (b)
retrieval meta-learning, which learns to judiciously utilize the retrieved
images. Code and data is available at github.com/amirziai/rafic.</p>
</td>
    <td>
      
        Few Shot & Zero Shot 
      
        ACL 
      
        EMNLP 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/jung2023few/">Few-shot Metric Learning: Online Adaptation Of Embedding For Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Few-shot Metric Learning: Online Adaptation Of Embedding For Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Few-shot Metric Learning: Online Adaptation Of Embedding For Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jung et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>5</td>
    <td><p>Metric learning aims to build a distance metric typically by learning an
effective embedding function that maps similar objects into nearby points in
its embedding space. Despite recent advances in deep metric learning, it
remains challenging for the learned metric to generalize to unseen classes with
a substantial domain gap. To tackle the issue, we explore a new problem of
few-shot metric learning that aims to adapt the embedding function to the
target domain with only a few annotated data. We introduce three few-shot
metric learning baselines and propose the Channel-Rectifier Meta-Learning
(CRML), which effectively adapts the metric space online by adjusting channels
of intermediate layers. Experimental analyses on miniImageNet, CUB-200-2011,
MPII, as well as a new dataset, miniDeepFashion, demonstrate that our method
consistently improves the learned metric by adapting it to target classes and
achieves a greater gain in image retrieval when the domain gap from the source
classes is larger.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
        Few Shot & Zero Shot 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/kobayashi2023sketch/">Sketch-based Medical Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Sketch-based Medical Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Sketch-based Medical Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kobayashi et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>6</td>
    <td><p>The amount of medical images stored in hospitals is increasing faster than
ever; however, utilizing the accumulated medical images has been limited. This
is because existing content-based medical image retrieval (CBMIR) systems
usually require example images to construct query vectors; nevertheless,
example images cannot always be prepared. Besides, there can be images with
rare characteristics that make it difficult to find similar example images,
which we call isolated samples. Here, we introduce a novel sketch-based medical
image retrieval (SBMIR) system that enables users to find images of interest
without example images. The key idea lies in feature decomposition of medical
images, whereby the entire feature of a medical image can be decomposed into
and reconstructed from normal and abnormal features. By extending this idea,
our SBMIR system provides an easy-to-use two-step graphical user interface:
users first select a template image to specify a normal feature and then draw a
semantic sketch of the disease on the template image to represent an abnormal
feature. Subsequently, it integrates the two kinds of input to construct a
query vector and retrieves reference images with the closest reference vectors.
Using two datasets, ten healthcare professionals with various clinical
backgrounds participated in the user test for evaluation. As a result, our
SBMIR system enabled users to overcome previous challenges, including image
retrieval based on fine-grained image characteristics, image retrieval without
example images, and image retrieval for isolated samples. Our SBMIR system
achieves flexible medical image retrieval on demand, thereby expanding the
utility of medical image databases.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/yang2023transformer/">Transformer-based Cross-modal Recipe Embeddings With Large Batch Training</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Transformer-based Cross-modal Recipe Embeddings With Large Batch Training' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Transformer-based Cross-modal Recipe Embeddings With Large Batch Training' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yang Jing, Chen Junwen, Yanai Keiji</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>6</td>
    <td><p>In this paper, we present a cross-modal recipe retrieval framework,
Transformer-based Network for Large Batch Training (TNLBT), which is inspired
by ACME~(Adversarial Cross-Modal Embedding) and H-T~(Hierarchical Transformer).
TNLBT aims to accomplish retrieval tasks while generating images from recipe
embeddings. We apply the Hierarchical Transformer-based recipe text encoder,
the Vision Transformer~(ViT)-based recipe image encoder, and an adversarial
network architecture to enable better cross-modal embedding learning for recipe
texts and images. In addition, we use self-supervised learning to exploit the
rich information in the recipe texts having no corresponding images. Since
contrastive learning could benefit from a larger batch size according to the
recent literature on self-supervised learning, we adopt a large batch size
during training and have validated its effectiveness. In the experiments, the
proposed framework significantly outperformed the current state-of-the-art
frameworks in both cross-modal recipe retrieval and image generation tasks on
the benchmark Recipe1M. This is the first work which confirmed the
effectiveness of large batch training on cross-modal recipe embeddings.</p>
</td>
    <td>
      
        Transformer Based ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/indyk2023worst/">Worst-case Performance Of Popular Approximate Nearest Neighbor Search Implementations: Guarantees And Limitations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Worst-case Performance Of Popular Approximate Nearest Neighbor Search Implementations: Guarantees And Limitations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Worst-case Performance Of Popular Approximate Nearest Neighbor Search Implementations: Guarantees And Limitations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Indyk Piotr, Xu Haike</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing</td>
    <td>34</td>
    <td><p>Graph-based approaches to nearest neighbor search are popular and powerful
tools for handling large datasets in practice, but they have limited
theoretical guarantees. We study the worst-case performance of recent
graph-based approximate nearest neighbor search algorithms, such as HNSW, NSG
and DiskANN. For DiskANN, we show that its “slow preprocessing” version
provably supports approximate nearest neighbor search query with constant
approximation ratio and poly-logarithmic query time, on data sets with bounded
“intrinsic” dimension. For the other data structure variants studied, including
DiskANN with “fast preprocessing”, HNSW and NSG, we present a family of
instances on which the empirical query time required to achieve a “reasonable”
accuracy is linear in instance size. For example, for DiskANN, we show that the
query procedure can take at least \(0.1 n\) steps on instances of size \(n\) before
it encounters any of the \(5\) nearest neighbors of the query.</p>
</td>
    <td>
      
        Similarity Search 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/hubenthal2023image/">Image-text Pre-training For Logo Recognition</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Image-text Pre-training For Logo Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Image-text Pre-training For Logo Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hubenthal Mark, Kumar Suren</td> <!-- 🔧 You were missing this -->
    <td>2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>5</td>
    <td><p>Open-set logo recognition is commonly solved by first detecting possible logo
regions and then matching the detected parts against an ever-evolving dataset
of cropped logo images. The matching model, a metric learning problem, is
especially challenging for logo recognition due to the mixture of text and
symbols in logos. We propose two novel contributions to improve the matching
model’s performance: (a) using image-text paired samples for pre-training, and
(b) an improved metric learning loss function. A standard paradigm of
fine-tuning ImageNet pre-trained models fails to discover the text sensitivity
necessary to solve the matching problem effectively. This work demonstrates the
importance of pre-training on image-text pairs, which significantly improves
the performance of a visual embedder trained for the logo retrieval task,
especially for more text-dominant classes. We construct a composite public logo
dataset combining LogoDet3K, OpenLogo, and FlickrLogos-47 deemed
OpenLogoDet3K47. We show that the same vision backbone pre-trained on
image-text data, when fine-tuned on OpenLogoDet3K47, achieves \(98.6%\)
recall@1, significantly improving performance over pre-training on Imagenet1K
(\(97.6%\)). We generalize the ProxyNCA++ loss function to propose ProxyNCAHN++
which incorporates class-specific hard negative images. The proposed method
sets new state-of-the-art on five public logo datasets considered, with a
\(3.5%\) zero-shot recall@1 improvement on LogoDet3K test, \(4%\) on OpenLogo,
\(6.5%\) on FlickrLogos-47, \(6.2%\) on Logos In The Wild, and \(0.6%\) on
BelgaLogo.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/huang2023lightweight/">Lightweight-yet-efficient: Revitalizing Ball-tree For Point-to-hyperplane Nearest Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Lightweight-yet-efficient: Revitalizing Ball-tree For Point-to-hyperplane Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Lightweight-yet-efficient: Revitalizing Ball-tree For Point-to-hyperplane Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Huang Qiang, Tung Anthony K. H.</td> <!-- 🔧 You were missing this -->
    <td>2023 IEEE 39th International Conference on Data Engineering (ICDE)</td>
    <td>5</td>
    <td><p>Finding the nearest neighbor to a hyperplane (or Point-to-Hyperplane Nearest
Neighbor Search, simply P2HNNS) is a new and challenging problem with
applications in many research domains. While existing state-of-the-art hashing
schemes (e.g., NH and FH) are able to achieve sublinear time complexity without
the assumption of the data being in a unit hypersphere, they require an
asymmetric transformation, which increases the data dimension from \(d\) to
\(Ω(d^2)\). This leads to considerable overhead for indexing and incurs
significant distortion errors.
  In this paper, we investigate a tree-based approach for solving P2HNNS using
the classical Ball-Tree index. Compared to hashing-based methods, tree-based
methods usually require roughly linear costs for construction, and they provide
different kinds of approximations with excellent flexibility. A simple
branch-and-bound algorithm with a novel lower bound is first developed on
Ball-Tree for performing P2HNNS. Then, a new tree structure named BC-Tree,
which maintains the Ball and Cone structures in the leaf nodes of Ball-Tree, is
described together with two effective strategies, i.e., point-level pruning and
collaborative inner product computing. BC-Tree inherits both the low
construction cost and lightweight property of Ball-Tree while providing a
similar or more efficient search. Experimental results over 16 real-world data
sets show that Ball-Tree and BC-Tree are around 1.1\(\sim\)10\(\times\) faster than
NH and FH, and they can reduce the index size and indexing time by about
1\(\sim\)3 orders of magnitudes on average. The code is available at
https://github.com/HuangQiang/BC-Tree.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/hemati2023learning/">Learning Binary And Sparse Permutation-invariant Representations For Fast And Memory Efficient Whole Slide Image Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Binary And Sparse Permutation-invariant Representations For Fast And Memory Efficient Whole Slide Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Binary And Sparse Permutation-invariant Representations For Fast And Memory Efficient Whole Slide Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hemati et al.</td> <!-- 🔧 You were missing this -->
    <td>Computers in Biology and Medicine</td>
    <td>9</td>
    <td><p>Learning suitable Whole slide images (WSIs) representations for efficient
retrieval systems is a non-trivial task. The WSI embeddings obtained from
current methods are in Euclidean space not ideal for efficient WSI retrieval.
Furthermore, most of the current methods require high GPU memory due to the
simultaneous processing of multiple sets of patches. To address these
challenges, we propose a novel framework for learning binary and sparse WSI
representations utilizing a deep generative modelling and the Fisher Vector. We
introduce new loss functions for learning sparse and binary
permutation-invariant WSI representations that employ instance-based training
achieving better memory efficiency. The learned WSI representations are
validated on The Cancer Genomic Atlas (TCGA) and Liver-Kidney-Stomach (LKS)
datasets. The proposed method outperforms Yottixel (a recent search engine for
histopathology images) both in terms of retrieval accuracy and speed. Further,
we achieve competitive performance against SOTA on the public benchmark LKS
dataset for WSI classification.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/hashimoto2023case/">Case-based Similar Image Retrieval For Weakly Annotated Large Histopathological Images Of Malignant Lymphoma Using Deep Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Case-based Similar Image Retrieval For Weakly Annotated Large Histopathological Images Of Malignant Lymphoma Using Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Case-based Similar Image Retrieval For Weakly Annotated Large Histopathological Images Of Malignant Lymphoma Using Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hashimoto et al.</td> <!-- 🔧 You were missing this -->
    <td>Medical Image Analysis</td>
    <td>18</td>
    <td><p>In the present study, we propose a novel case-based similar image retrieval
(SIR) method for hematoxylin and eosin (H&amp;E)-stained histopathological images
of malignant lymphoma. When a whole slide image (WSI) is used as an input
query, it is desirable to be able to retrieve similar cases by focusing on
image patches in pathologically important regions such as tumor cells. To
address this problem, we employ attention-based multiple instance learning,
which enables us to focus on tumor-specific regions when the similarity between
cases is computed. Moreover, we employ contrastive distance metric learning to
incorporate immunohistochemical (IHC) staining patterns as useful supervised
information for defining appropriate similarity between heterogeneous malignant
lymphoma cases. In the experiment with 249 malignant lymphoma patients, we
confirmed that the proposed method exhibited higher evaluation measures than
the baseline case-based SIR methods. Furthermore, the subjective evaluation by
pathologists revealed that our similarity measure using IHC staining patterns
is appropriate for representing the similarity of H&amp;E-stained tissue images for
malignant lymphoma.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/gupta2023medical/">Medical Image Retrieval Via Nearest Neighbor Search On Pre-trained Image Features</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Medical Image Retrieval Via Nearest Neighbor Search On Pre-trained Image Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Medical Image Retrieval Via Nearest Neighbor Search On Pre-trained Image Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gupta et al.</td> <!-- 🔧 You were missing this -->
    <td>Knowledge-Based Systems</td>
    <td>7</td>
    <td><p>Nearest neighbor search (NNS) aims to locate the points in high-dimensional
space that is closest to the query point. The brute-force approach for finding
the nearest neighbor becomes computationally infeasible when the number of
points is large. The NNS has multiple applications in medicine, such as
searching large medical imaging databases, disease classification, diagnosis,
etc. With a focus on medical imaging, this paper proposes DenseLinkSearch an
effective and efficient algorithm that searches and retrieves the relevant
images from heterogeneous sources of medical images. Towards this, given a
medical database, the proposed algorithm builds the index that consists of
pre-computed links of each point in the database. The search algorithm utilizes
the index to efficiently traverse the database in search of the nearest
neighbor. We extensively tested the proposed NNS approach and compared the
performance with state-of-the-art NNS approaches on benchmark datasets and our
created medical image datasets. The proposed approach outperformed the existing
approach in terms of retrieving accurate neighbors and retrieval speed. We also
explore the role of medical image feature representation in content-based
medical image retrieval tasks. We propose a Transformer-based feature
representation technique that outperformed the existing pre-trained Transformer
approach on CLEF 2011 medical image retrieval task. The source code of our
experiments are available at https://github.com/deepaknlp/DLS.</p>
</td>
    <td>
      
        Similarity Search 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/gupta2023caps/">CAPS: A Practical Partition Index For Filtered Similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=CAPS: A Practical Partition Index For Filtered Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=CAPS: A Practical Partition Index For Filtered Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gupta et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</td>
    <td>6</td>
    <td><p>With the surging popularity of approximate near-neighbor search (ANNS),
driven by advances in neural representation learning, the ability to serve
queries accompanied by a set of constraints has become an area of intense
interest. While the community has recently proposed several algorithms for
constrained ANNS, almost all of these methods focus on integration with
graph-based indexes, the predominant class of algorithms achieving
state-of-the-art performance in latency-recall tradeoffs. In this work, we take
a different approach and focus on developing a constrained ANNS algorithm via
space partitioning as opposed to graphs. To that end, we introduce Constrained
Approximate Partitioned Search (CAPS), an index for ANNS with filters via space
partitions that not only retains the benefits of a partition-based algorithm
but also outperforms state-of-the-art graph-based constrained search techniques
in recall-latency tradeoffs, with only 10% of the index size.</p>
</td>
    <td>
      
        SIGIR 
      
        Text Retrieval 
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/zhu2023clip/">CLIP Multi-modal Hashing: A New Baseline CLIPMH</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=CLIP Multi-modal Hashing: A New Baseline CLIPMH' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=CLIP Multi-modal Hashing: A New Baseline CLIPMH' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhu et al.</td> <!-- 🔧 You were missing this -->
    <td>Information Fusion</td>
    <td>37</td>
    <td><p>The multi-modal hashing method is widely used in multimedia retrieval. It can
fuse multi-source data to generate binary hash code. However, the current
multi-modal methods have the problem of low retrieval accuracy. The reason is
that the individual backbone networks have limited feature expression
capabilities and are not jointly pre-trained on large-scale unsupervised
multi-modal data. To solve this problem, we propose a new baseline CLIP
Multi-modal Hashing (CLIPMH) method. It uses CLIP model to extract text and
image features, and then fuse to generate hash code. CLIP improves the
expressiveness of each modal feature. In this way, it can greatly improve the
retrieval performance of multi-modal hashing methods. In comparison to
state-of-the-art unsupervised and supervised multi-modal hashing methods,
experiments reveal that the proposed CLIPMH can significantly enhance
performance (Maximum increase of 8.38%). CLIP also has great advantages over
the text and visual backbone networks commonly used before.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/zhang2023graph/">Graph Convolution Based Efficient Re-ranking For Visual Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Graph Convolution Based Efficient Re-ranking For Visual Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Graph Convolution Based Efficient Re-ranking For Visual Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>9</td>
    <td><p>Visual retrieval tasks such as image retrieval and person re-identification
(Re-ID) aim at effectively and thoroughly searching images with similar content
or the same identity. After obtaining retrieved examples, re-ranking is a
widely adopted post-processing step to reorder and improve the initial
retrieval results by making use of the contextual information from semantically
neighboring samples. Prevailing re-ranking approaches update distance metrics
and mostly rely on inefficient crosscheck set comparison operations while
computing expanded neighbors based distances. In this work, we present an
efficient re-ranking method which refines initial retrieval results by updating
features. Specifically, we reformulate re-ranking based on Graph Convolution
Networks (GCN) and propose a novel Graph Convolution based Re-ranking (GCR) for
visual retrieval tasks via feature propagation. To accelerate computation for
large-scale retrieval, a decentralized and synchronous feature propagation
algorithm which supports parallel or distributed computing is introduced. In
particular, the plain GCR is extended for cross-camera retrieval and an
improved feature propagation formulation is presented to leverage affinity
relationships across different cameras. It is also extended for video-based
retrieval, and Graph Convolution based Re-ranking for Video (GCRV) is proposed
by mathematically deriving a novel profile vector generation method for the
tracklet. Without bells and whistles, the proposed approaches achieve
state-of-the-art performances on seven benchmark datasets from three different
tasks, i.e., image retrieval, person Re-ID and video-based person Re-ID.</p>
</td>
    <td>
      
        Hybrid ANN Methods 
      
        Re RANKING 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/girdhar2023imagebind/">Imagebind: One Embedding Space To Bind Them All</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Imagebind: One Embedding Space To Bind Them All' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Imagebind: One Embedding Space To Bind Them All' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Girdhar et al.</td> <!-- 🔧 You were missing this -->
    <td>2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>332</td>
    <td><p>We present ImageBind, an approach to learn a joint embedding across six
different modalities - images, text, audio, depth, thermal, and IMU data. We
show that all combinations of paired data are not necessary to train such a
joint embedding, and only image-paired data is sufficient to bind the
modalities together. ImageBind can leverage recent large scale vision-language
models, and extends their zero-shot capabilities to new modalities just by
using their natural pairing with images. It enables novel emergent applications
‘out-of-the-box’ including cross-modal retrieval, composing modalities with
arithmetic, cross-modal detection and generation. The emergent capabilities
improve with the strength of the image encoder and we set a new
state-of-the-art on emergent zero-shot recognition tasks across modalities,
outperforming specialist supervised models. Finally, we show strong few-shot
recognition results outperforming prior work, and that ImageBind serves as a
new way to evaluate vision models for visual and non-visual tasks.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/ge2023cross/">Cross-modal Semantic Enhanced Interaction For Image-sentence Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-modal Semantic Enhanced Interaction For Image-sentence Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-modal Semantic Enhanced Interaction For Image-sentence Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ge et al.</td> <!-- 🔧 You were missing this -->
    <td>2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>27</td>
    <td><p>Image-sentence retrieval has attracted extensive research attention in
multimedia and computer vision due to its promising application. The key issue
lies in jointly learning the visual and textual representation to accurately
estimate their similarity. To this end, the mainstream schema adopts an
object-word based attention to calculate their relevance scores and refine
their interactive representations with the attention features, which, however,
neglects the context of the object representation on the inter-object
relationship that matches the predicates in sentences. In this paper, we
propose a Cross-modal Semantic Enhanced Interaction method, termed CMSEI for
image-sentence retrieval, which correlates the intra- and inter-modal semantics
between objects and words. In particular, we first design the intra-modal
spatial and semantic graphs based reasoning to enhance the semantic
representations of objects guided by the explicit relationships of the objects’
spatial positions and their scene graph. Then the visual and textual semantic
representations are refined jointly via the inter-modal interactive attention
and the cross-modal alignment. To correlate the context of objects with the
textual context, we further refine the visual semantic representation via the
cross-level object-sentence and word-image based interactive attention.
Experimental results on seven standard evaluation metrics show that the
proposed CMSEI outperforms the state-of-the-art and the alternative approaches
on MS-COCO and Flickr30K benchmarks.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/gao2023precise/">Precise Zero-shot Dense Retrieval Without Relevance Labels</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Precise Zero-shot Dense Retrieval Without Relevance Labels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Precise Zero-shot Dense Retrieval Without Relevance Labels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gao et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</td>
    <td>81</td>
    <td><p>While dense retrieval has been shown effective and efficient across tasks and
languages, it remains difficult to create effective fully zero-shot dense
retrieval systems when no relevance label is available. In this paper, we
recognize the difficulty of zero-shot learning and encoding relevance. Instead,
we propose to pivot through Hypothetical Document Embeddings~(HyDE). Given a
query, HyDE first zero-shot instructs an instruction-following language model
(e.g. InstructGPT) to generate a hypothetical document. The document captures
relevance patterns but is unreal and may contain false details. Then, an
unsupervised contrastively learned encoder~(e.g. Contriever) encodes the
document into an embedding vector. This vector identifies a neighborhood in the
corpus embedding space, where similar real documents are retrieved based on
vector similarity. This second step ground the generated document to the actual
corpus, with the encoder’s dense bottleneck filtering out the incorrect
details. Our experiments show that HyDE significantly outperforms the
state-of-the-art unsupervised dense retriever Contriever and shows strong
performance comparable to fine-tuned retrievers, across various tasks (e.g. web
search, QA, fact verification) and languages~(e.g. sw, ko, ja).</p>
</td>
    <td>
      
        Few Shot & Zero Shot 
      
        ACL 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/gao2023high/">High-dimensional Approximate Nearest Neighbor Search: With Reliable And Efficient Distance Comparison Operations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=High-dimensional Approximate Nearest Neighbor Search: With Reliable And Efficient Distance Comparison Operations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=High-dimensional Approximate Nearest Neighbor Search: With Reliable And Efficient Distance Comparison Operations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gao Jianyang, Long Cheng</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the ACM on Management of Data</td>
    <td>24</td>
    <td><p>Approximate K nearest neighbor (AKNN) search is a fundamental and challenging
problem. We observe that in high-dimensional space, the time consumption of
nearly all AKNN algorithms is dominated by that of the distance comparison
operations (DCOs). For each operation, it scans full dimensions of an object
and thus, runs in linear time wrt the dimensionality. To speed it up, we
propose a randomized algorithm named ADSampling which runs in logarithmic time
wrt to the dimensionality for the majority of DCOs and succeeds with high
probability. In addition, based on ADSampling we develop one general and two
algorithm-specific techniques as plugins to enhance existing AKNN algorithms.
Both theoretical and empirical studies confirm that: (1) our techniques
introduce nearly no accuracy loss and (2) they consistently improve the
efficiency.</p>
</td>
    <td>
      
        Similarity Search 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/galanopoulos2023are/">Are All Combinations Equal? Combining Textual And Visual Features With Multiple Space Learning For Text-based Video Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Are All Combinations Equal? Combining Textual And Visual Features With Multiple Space Learning For Text-based Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Are All Combinations Equal? Combining Textual And Visual Features With Multiple Space Learning For Text-based Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Galanopoulos Damianos, Mezaris Vasileios</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>11</td>
    <td><p>In this paper we tackle the cross-modal video retrieval problem and, more
specifically, we focus on text-to-video retrieval. We investigate how to
optimally combine multiple diverse textual and visual features into feature
pairs that lead to generating multiple joint feature spaces, which encode
text-video pairs into comparable representations. To learn these
representations our proposed network architecture is trained by following a
multiple space learning procedure. Moreover, at the retrieval stage, we
introduce additional softmax operations for revising the inferred query-video
similarities. Extensive experiments in several setups based on three
large-scale datasets (IACC.3, V3C1, and MSR-VTT) lead to conclusions on how to
best combine text-visual features and document the performance of the proposed
network. Source code is made publicly available at:
https://github.com/bmezaris/TextToVideoRetrieval-TtimesV</p>
</td>
    <td>
      
        Video Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/gan2023binary/">Binary Embedding-based Retrieval At Tencent</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Binary Embedding-based Retrieval At Tencent' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Binary Embedding-based Retrieval At Tencent' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gan et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</td>
    <td>7</td>
    <td><p>Large-scale embedding-based retrieval (EBR) is the cornerstone of
search-related industrial applications. Given a user query, the system of EBR
aims to identify relevant information from a large corpus of documents that may
be tens or hundreds of billions in size. The storage and computation turn out
to be expensive and inefficient with massive documents and high concurrent
queries, making it difficult to further scale up. To tackle the challenge, we
propose a binary embedding-based retrieval (BEBR) engine equipped with a
recurrent binarization algorithm that enables customized bits per dimension.
Specifically, we compress the full-precision query and document embeddings,
formulated as float vectors in general, into a composition of multiple binary
vectors using a lightweight transformation model with residual multilayer
perception (MLP) blocks. We can therefore tailor the number of bits for
different applications to trade off accuracy loss and cost savings.
Importantly, we enable task-agnostic efficient training of the binarization
model using a new embedding-to-embedding strategy. We also exploit the
compatible training of binary embeddings so that the BEBR engine can support
indexing among multiple embedding versions within a unified system. To further
realize efficient search, we propose Symmetric Distance Calculation (SDC) to
achieve lower response time than Hamming codes. We successfully employed the
introduced BEBR to Tencent products, including Sogou, Tencent Video, QQ World,
etc. The binarization algorithm can be seamlessly generalized to various tasks
with multiple modalities. Extensive experiments on offline benchmarks and
online A/B tests demonstrate the efficiency and effectiveness of our method,
significantly saving 30%~50% index costs with almost no loss of accuracy at the
system level.</p>
</td>
    <td>
      
        KDD 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/gao2023long/">Long-tail Cross Modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Long-tail Cross Modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Long-tail Cross Modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gao et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>5</td>
    <td><p>Existing Cross Modal Hashing (CMH) methods are mainly designed for balanced
data, while imbalanced data with long-tail distribution is more general in
real-world. Several long-tail hashing methods have been proposed but they can
not adapt for multi-modal data, due to the complex interplay between labels and
individuality and commonality information of multi-modal data. Furthermore, CMH
methods mostly mine the commonality of multi-modal data to learn hash codes,
which may override tail labels encoded by the individuality of respective
modalities. In this paper, we propose LtCMH (Long-tail CMH) to handle
imbalanced multi-modal data. LtCMH firstly adopts auto-encoders to mine the
individuality and commonality of different modalities by minimizing the
dependency between the individuality of respective modalities and by enhancing
the commonality of these modalities. Then it dynamically combines the
individuality and commonality with direct features extracted from respective
modalities to create meta features that enrich the representation of tail
labels, and binaries meta features to generate hash codes. LtCMH significantly
outperforms state-of-the-art baselines on long-tail datasets and holds a better
(or comparable) performance on datasets with balanced labels.</p>
</td>
    <td>
      
        Hashing Methods 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/ferraro2023contrastive/">Contrastive Learning For Cross-modal Artist Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Contrastive Learning For Cross-modal Artist Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Contrastive Learning For Cross-modal Artist Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ferraro et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE International Conference on Big Data (Big Data)</td>
    <td>5</td>
    <td><p>Music retrieval and recommendation applications often rely on content
features encoded as embeddings, which provide vector representations of items
in a music dataset. Numerous complementary embeddings can be derived from
processing items originally represented in several modalities, e.g., audio
signals, user interaction data, or editorial data. However, data of any given
modality might not be available for all items in any music dataset. In this
work, we propose a method based on contrastive learning to combine embeddings
from multiple modalities and explore the impact of the presence or absence of
embeddings from diverse modalities in an artist similarity task. Experiments on
two datasets suggest that our contrastive method outperforms single-modality
embeddings and baseline algorithms for combining modalities, both in terms of
artist retrieval accuracy and coverage. Improvements with respect to other
methods are particularly significant for less popular query artists. We
demonstrate our method successfully combines complementary information from
diverse modalities, and is more robust to missing modality data (i.e., it
better handles the retrieval of artists with different modality embeddings than
the query artist’s).</p>
</td>
    <td>
      
        Self SUPERVISED 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/zhu2023r/">\(r^{2}\)former: Unified \(r\)etrieval And \(r\)eranking Transformer For Place Recognition</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=\(r^{2}\)former: Unified \(r\)etrieval And \(r\)eranking Transformer For Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=\(r^{2}\)former: Unified \(r\)etrieval And \(r\)eranking Transformer For Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhu et al.</td> <!-- 🔧 You were missing this -->
    <td>2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>66</td>
    <td><p>Visual Place Recognition (VPR) estimates the location of query images by
matching them with images in a reference database. Conventional methods
generally adopt aggregated CNN features for global retrieval and RANSAC-based
geometric verification for reranking. However, RANSAC only employs geometric
information but ignores other possible information that could be useful for
reranking, e.g. local feature correlations, and attention values. In this
paper, we propose a unified place recognition framework that handles both
retrieval and reranking with a novel transformer model, named \(R^{2}\)Former.
The proposed reranking module takes feature correlation, attention value, and
xy coordinates into account, and learns to determine whether the image pair is
from the same location. The whole pipeline is end-to-end trainable and the
reranking module alone can also be adopted on other CNN or transformer
backbones as a generic component. Remarkably, \(R^{2}\)Former significantly
outperforms state-of-the-art methods on major VPR datasets with much less
inference time and memory consumption. It also achieves the state-of-the-art on
the hold-out MSLS challenge set and could serve as a simple yet strong solution
for real-world large-scale applications. Experiments also show vision
transformer tokens are comparable and sometimes better than CNN local features
on local matching. The code is released at
https://github.com/Jeff-Zilence/R2Former.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/dong2023region/">From Region To Patch: Attribute-aware Foreground-background Contrastive Learning For Fine-grained Fashion Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=From Region To Patch: Attribute-aware Foreground-background Contrastive Learning For Fine-grained Fashion Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=From Region To Patch: Attribute-aware Foreground-background Contrastive Learning For Fine-grained Fashion Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dong et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>9</td>
    <td><p>Attribute-specific fashion retrieval (ASFR) is a challenging information
retrieval task, which has attracted increasing attention in recent years.
Different from traditional fashion retrieval which mainly focuses on optimizing
holistic similarity, the ASFR task concentrates on attribute-specific
similarity, resulting in more fine-grained and interpretable retrieval results.
As the attribute-specific similarity typically corresponds to the specific
subtle regions of images, we propose a Region-to-Patch Framework (RPF) that
consists of a region-aware branch and a patch-aware branch to extract
fine-grained attribute-related visual features for precise retrieval in a
coarse-to-fine manner. In particular, the region-aware branch is first to be
utilized to locate the potential regions related to the semantic of the given
attribute. Then, considering that the located region is coarse and still
contains the background visual contents, the patch-aware branch is proposed to
capture patch-wise attribute-related details from the previous amplified
region. Such a hybrid architecture strikes a proper balance between region
localization and feature extraction. Besides, different from previous works
that solely focus on discriminating the attribute-relevant foreground visual
features, we argue that the attribute-irrelevant background features are also
crucial for distinguishing the detailed visual contexts in a contrastive
manner. Therefore, a novel E-InfoNCE loss based on the foreground and
background representations is further proposed to improve the discrimination of
attribute-specific representation. Extensive experiments on three datasets
demonstrate the effectiveness of our proposed framework, and also show a decent
generalization of our RPF on out-of-domain fashion images. Our source code is
available at https://github.com/HuiGuanLab/RPF.</p>
</td>
    <td>
      
        SIGIR 
      
        Text Retrieval 
      
        Self SUPERVISED 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/dong2023adapt/">Adapt And Align To Improve Zero-shot Sketch-based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Adapt And Align To Improve Zero-shot Sketch-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Adapt And Align To Improve Zero-shot Sketch-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dong et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>16</td>
    <td><p>Zero-shot sketch-based image retrieval (ZS-SBIR) is challenging due to the
cross-domain nature of sketches and photos, as well as the semantic gap between
seen and unseen image distributions. Previous methods fine-tune pre-trained
models with various side information and learning strategies to learn a compact
feature space that is shared between the sketch and photo domains and bridges
seen and unseen classes. However, these efforts are inadequate in adapting
domains and transferring knowledge from seen to unseen classes. In this paper,
we present an effective ``Adapt and Align’’ approach to address the key
challenges. Specifically, we insert simple and lightweight domain adapters to
learn new abstract concepts of the sketch domain and improve cross-domain
representation capabilities. Inspired by recent advances in image-text
foundation models (e.g., CLIP) on zero-shot scenarios, we explicitly align the
learned image embedding with a more semantic text embedding to achieve the
desired knowledge transfer from seen to unseen classes. Extensive experiments
on three benchmark datasets and two popular backbones demonstrate the
superiority of our method in terms of retrieval accuracy and flexibility.</p>
</td>
    <td>
      
        Image Retrieval 
      
        Few Shot & Zero Shot 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/dong2023seine/">SEINE: Segment-based Indexing For Neural Information Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=SEINE: Segment-based Indexing For Neural Information Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=SEINE: Segment-based Indexing For Neural Information Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dong Sibo, Goldstein Justin, Yang Grace Hui</td> <!-- 🔧 You were missing this -->
    <td>International Journal of Computer Science and Information Technology</td>
    <td>13</td>
    <td><p>Many early neural Information Retrieval (NeurIR) methods are re-rankers that
rely on a traditional first-stage retriever due to expensive query time
computations. Recently, representation-based retrievers have gained much
attention, which learns query representation and document representation
separately, making it possible to pre-compute document representations offline
and reduce the workload at query time. Both dense and sparse
representation-based retrievers have been explored. However, these methods
focus on finding the representation that best represents a text (aka metric
learning) and the actual retrieval function that is responsible for similarity
matching between query and document is kept at a minimum by using dot product.
One drawback is that unlike traditional term-level inverted index, the index
formed by these embeddings cannot be easily re-used by another retrieval
method. Another drawback is that keeping the interaction at minimum hurts
retrieval effectiveness. On the contrary, interaction-based retrievers are
known for their better retrieval effectiveness. In this paper, we propose a
novel SEgment-based Neural Indexing method, SEINE, which provides a general
indexing framework that can flexibly support a variety of interaction-based
neural retrieval methods. We emphasize on a careful decomposition of common
components in existing neural retrieval methods and propose to use
segment-level inverted index to store the atomic query-document interaction
values. Experiments on LETOR MQ2007 and MQ2008 datasets show that our indexing
method can accelerate multiple neural retrieval methods up to 28-times faster
without sacrificing much effectiveness.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/dai2023sketch/">Sketch Less Face Image Retrieval: A New Challenge</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Sketch Less Face Image Retrieval: A New Challenge' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Sketch Less Face Image Retrieval: A New Challenge' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dai et al.</td> <!-- 🔧 You were missing this -->
    <td>ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</td>
    <td>5</td>
    <td><p>In some specific scenarios, face sketch was used to identify a person.
However, drawing a complete face sketch often needs skills and takes time,
which hinder its widespread applicability in the practice. In this study, we
proposed a new task named sketch less face image retrieval (SLFIR), in which
the retrieval was carried out at each stroke and aim to retrieve the target
face photo using a partial sketch with as few strokes as possible (see Fig.1).
Firstly, we developed a method to generate the data of sketch with drawing
process, and opened such dataset; Secondly, we proposed a two-stage method as
the baseline for SLFIR that (1) A triplet network, was first adopt to learn the
joint embedding space shared between the complete sketch and its target face
photo; (2) Regarding the sketch drawing episode as a sequence, we designed a
LSTM module to optimize the representation of the incomplete face sketch.
Experiments indicate that the new framework can finish the retrieval using a
partial or pool drawing sketch.</p>
</td>
    <td>
      
        Image Retrieval 
      
        ICASSP 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/dadas2023opi/">OPI At Semeval 2023 Task 1: Image-text Embeddings And Multimodal Information Retrieval For Visual Word Sense Disambiguation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=OPI At Semeval 2023 Task 1: Image-text Embeddings And Multimodal Information Retrieval For Visual Word Sense Disambiguation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=OPI At Semeval 2023 Task 1: Image-text Embeddings And Multimodal Information Retrieval For Visual Word Sense Disambiguation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dadas Sławomir</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the The 17th International Workshop on Semantic Evaluation (SemEval-2023)</td>
    <td>5</td>
    <td><p>The goal of visual word sense disambiguation is to find the image that best
matches the provided description of the word’s meaning. It is a challenging
problem, requiring approaches that combine language and image understanding. In
this paper, we present our submission to SemEval 2023 visual word sense
disambiguation shared task. The proposed system integrates multimodal
embeddings, learning to rank methods, and knowledge-based approaches. We build
a classifier based on the CLIP model, whose results are enriched with
additional information retrieved from Wikipedia and lexical databases. Our
solution was ranked third in the multilingual task and won in the Persian
track, one of the three language subtasks.</p>
</td>
    <td>
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/chen2023supervised/">Supervised Auto-encoding Twin-bottleneck Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Auto-encoding Twin-bottleneck Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Supervised Auto-encoding Twin-bottleneck Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen Yuan, Marchand-maillet Stéphane</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>105</td>
    <td><p>Deep hashing has shown to be a complexity-efficient solution for the
Approximate Nearest Neighbor search problem in high dimensional space. Many
methods usually build the loss function from pairwise or triplet data points to
capture the local similarity structure. Other existing methods construct the
similarity graph and consider all points simultaneously. Auto-encoding
Twin-bottleneck Hashing is one such method that dynamically builds the graph.
Specifically, each input data is encoded into a binary code and a continuous
variable, or the so-called twin bottlenecks. The similarity graph is then
computed from these binary codes, which get updated consistently during the
training. In this work, we generalize the original model into a supervised deep
hashing network by incorporating the label information. In addition, we examine
the differences of codes structure between these two networks and consider the
class imbalance problem especially in multi-labeled datasets. Experiments on
three datasets yield statistically significant improvement against the original
model. Results are also comparable and competitive to other supervised methods.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/chen2023finger/">FINGER: Fast Inference For Graph-based Approximate Nearest Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=FINGER: Fast Inference For Graph-based Approximate Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=FINGER: Fast Inference For Graph-based Approximate Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the ACM Web Conference 2023</td>
    <td>13</td>
    <td><p>Approximate K-Nearest Neighbor Search (AKNNS) has now become ubiquitous in
modern applications, for example, as a fast search procedure with two tower
deep learning models. Graph-based methods for AKNNS in particular have received
great attention due to their superior performance. These methods rely on greedy
graph search to traverse the data points as embedding vectors in a database.
Under this greedy search scheme, we make a key observation: many distance
computations do not influence search updates so these computations can be
approximated without hurting performance. As a result, we propose FINGER, a
fast inference method to achieve efficient graph search. FINGER approximates
the distance function by estimating angles between neighboring residual vectors
with low-rank bases and distribution matching. The approximated distance can be
used to bypass unnecessary computations, which leads to faster searches.
Empirically, accelerating a popular graph-based method named HNSW by FINGER is
shown to outperform existing graph-based methods by 20%-60% across different
benchmark datasets.</p>
</td>
    <td>
      
        Similarity Search 
      
        Graph Based ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/chen2023bipartite/">Bipartite Graph Convolutional Hashing For Effective And Efficient Top-n Search In Hamming Space</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Bipartite Graph Convolutional Hashing For Effective And Efficient Top-n Search In Hamming Space' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Bipartite Graph Convolutional Hashing For Effective And Efficient Top-n Search In Hamming Space' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the ACM Web Conference 2023</td>
    <td>15</td>
    <td><p>Searching on bipartite graphs is basal and versatile to many real-world Web
applications, e.g., online recommendation, database retrieval, and
query-document searching. Given a query node, the conventional approaches rely
on the similarity matching with the vectorized node embeddings in the
continuous Euclidean space. To efficiently manage intensive similarity
computation, developing hashing techniques for graph structured data has
recently become an emerging research direction. Despite the retrieval
efficiency in Hamming space, prior work is however confronted with catastrophic
performance decay. In this work, we investigate the problem of hashing with
Graph Convolutional Network on bipartite graphs for effective Top-N search. We
propose an end-to-end Bipartite Graph Convolutional Hashing approach, namely
BGCH, which consists of three novel and effective modules: (1) adaptive graph
convolutional hashing, (2) latent feature dispersion, and (3) Fourier
serialized gradient estimation. Specifically, the former two modules achieve
the substantial retention of the structural information against the inevitable
information loss in hash encoding; the last module develops Fourier Series
decomposition to the hashing function in the frequency domain mainly for more
accurate gradient estimation. The extensive experiments on six real-world
datasets not only show the performance superiority over the competing
hashing-based counterparts, but also demonstrate the effectiveness of all
proposed model components contained therein.</p>
</td>
    <td>
      
        Similarity Search 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/chen2023multi/">Multi-level Visual Similarity Based Personalized Tourist Attraction Recommendation Using Geo-tagged Photos</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multi-level Visual Similarity Based Personalized Tourist Attraction Recommendation Using Geo-tagged Photos' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multi-level Visual Similarity Based Personalized Tourist Attraction Recommendation Using Geo-tagged Photos' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen et al.</td> <!-- 🔧 You were missing this -->
    <td>ACM Transactions on Knowledge Discovery from Data</td>
    <td>6</td>
    <td><p>Geo-tagged photo based tourist attraction recommendation can discover users’
travel preferences from their taken photos, so as to recommend suitable tourist
attractions to them. However, existing visual content based methods cannot
fully exploit the user and tourist attraction information of photos to extract
visual features, and do not differentiate the significances of different
photos. In this paper, we propose multi-level visual similarity based
personalized tourist attraction recommendation using geo-tagged photos (MEAL).
MEAL utilizes the visual contents of photos and interaction behavior data to
obtain the final embeddings of users and tourist attractions, which are then
used to predict the visit probabilities. Specifically, by crossing the user and
tourist attraction information of photos, we define four visual similarity
levels and introduce a corresponding quintuplet loss to embed the visual
contents of photos. In addition, to capture the significances of different
photos, we exploit the self-attention mechanism to obtain the visual
representations of users and tourist attractions. We conducted experiments on a
dataset crawled from Flickr, and the experimental results proved the advantage
of this method.</p>
</td>
    <td>
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/chen2023stair/">STAIR: Learning Sparse Text And Image Representation In Grounded Tokens</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=STAIR: Learning Sparse Text And Image Representation In Grounded Tokens' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=STAIR: Learning Sparse Text And Image Representation In Grounded Tokens' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</td>
    <td>8</td>
    <td><p>Image and text retrieval is one of the foundational tasks in the vision and
language domain with multiple real-world applications. State-of-the-art
approaches, e.g. CLIP, ALIGN, represent images and texts as dense embeddings
and calculate the similarity in the dense embedding space as the matching
score. On the other hand, sparse semantic features like bag-of-words models are
more interpretable, but believed to suffer from inferior accuracy than dense
representations. In this work, we show that it is possible to build a sparse
semantic representation that is as powerful as, or even better than, dense
presentations. We extend the CLIP model and build a sparse text and image
representation (STAIR), where the image and text are mapped to a sparse token
space. Each token in the space is a (sub-)word in the vocabulary, which is not
only interpretable but also easy to integrate with existing information
retrieval systems. STAIR model significantly outperforms a CLIP model with
+\(4.9%\) and +\(4.3%\) absolute Recall@1 improvement on COCO-5k
text\(\rightarrow\)image and image\(\rightarrow\)text retrieval respectively. It
also achieved better performance on both of ImageNet zero-shot and linear
probing compared to CLIP.</p>
</td>
    <td>
      
        EMNLP 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/zhang2023new/">A New Fine-grained Alignment Method For Image-text Matching</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A New Fine-grained Alignment Method For Image-text Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A New Fine-grained Alignment Method For Image-text Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Yang</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 19th International Conference on Content-based Multimedia Indexing</td>
    <td>23</td>
    <td><p>Image-text retrieval is a widely studied topic in the field of computer
vision due to the exponential growth of multimedia data, whose core concept is
to measure the similarity between images and text. However, most existing
retrieval methods heavily rely on cross-attention mechanisms for cross-modal
fine-grained alignment, which takes into account excessive irrelevant regions
and treats prominent and non-significant words equally, thereby limiting
retrieval accuracy. This paper aims to investigate an alignment approach that
reduces the involvement of non-significant fragments in images and text while
enhancing the alignment of prominent segments. For this purpose, we introduce
the Cross-Modal Prominent Fragments Enhancement Aligning Network(CPFEAN), which
achieves improved retrieval accuracy by diminishing the participation of
irrelevant regions during alignment and relatively increasing the alignment
similarity of prominent words. Additionally, we incorporate prior textual
information into image regions to reduce misalignment occurrences. In practice,
we first design a novel intra-modal fragments relationship reasoning method,
and subsequently employ our proposed alignment mechanism to compute the
similarity between images and text. Extensive quantitative comparative
experiments on MS-COCO and Flickr30K datasets demonstrate that our approach
outperforms state-of-the-art methods by about 5% to 10% in the rSum metric.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/zhang2023model/">Model-enhanced Vector Index</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Model-enhanced Vector Index' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Model-enhanced Vector Index' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang et al.</td> <!-- 🔧 You were missing this -->
    <td>Universe</td>
    <td>17</td>
    <td><p>Embedding-based retrieval methods construct vector indices to search for
document representations that are most similar to the query representations.
They are widely used in document retrieval due to low latency and decent recall
performance. Recent research indicates that deep retrieval solutions offer
better model quality, but are hindered by unacceptable serving latency and the
inability to support document updates. In this paper, we aim to enhance the
vector index with end-to-end deep generative models, leveraging the
differentiable advantages of deep retrieval models while maintaining desirable
serving efficiency. We propose Model-enhanced Vector Index (MEVI), a
differentiable model-enhanced index empowered by a twin-tower representation
model. MEVI leverages a Residual Quantization (RQ) codebook to bridge the
sequence-to-sequence deep retrieval and embedding-based models. To
substantially reduce the inference time, instead of decoding the unique
document ids in long sequential steps, we first generate some semantic virtual
cluster ids of candidate documents in a small number of steps, and then
leverage the well-adapted embedding vectors to further perform a fine-grained
search for the relevant documents in the candidate virtual clusters. We
empirically show that our model achieves better performance on the commonly
used academic benchmarks MSMARCO Passage and Natural Questions, with comparable
serving latency to dense retrieval solutions.</p>
</td>
    <td>
      
        Vector Indexing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/bruch2023approximate/">An Approximate Algorithm For Maximum Inner Product Search Over Streaming Sparse Vectors</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=An Approximate Algorithm For Maximum Inner Product Search Over Streaming Sparse Vectors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=An Approximate Algorithm For Maximum Inner Product Search Over Streaming Sparse Vectors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Bruch et al.</td> <!-- 🔧 You were missing this -->
    <td>ACM Transactions on Information Systems</td>
    <td>6</td>
    <td><p>Maximum Inner Product Search or top-k retrieval on sparse vectors is
well-understood in information retrieval, with a number of mature algorithms
that solve it exactly. However, all existing algorithms are tailored to text
and frequency-based similarity measures. To achieve optimal memory footprint
and query latency, they rely on the near stationarity of documents and on laws
governing natural languages. We consider, instead, a setup in which collections
are streaming – necessitating dynamic indexing – and where indexing and
retrieval must work with arbitrarily distributed real-valued vectors. As we
show, existing algorithms are no longer competitive in this setup, even against
naive solutions. We investigate this gap and present a novel approximate
solution, called Sinnamon, that can efficiently retrieve the top-k results for
sparse real valued vectors drawn from arbitrary distributions. Notably,
Sinnamon offers levers to trade-off memory consumption, latency, and accuracy,
making the algorithm suitable for constrained applications and systems. We give
theoretical results on the error introduced by the approximate nature of the
algorithm, and present an empirical evaluation of its performance on two
hardware platforms and synthetic and real-valued datasets. We conclude by
laying out concrete directions for future research on this general top-k
retrieval problem over sparse vectors.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/zhuang2023towards/">Towards Fast And Accurate Image-text Retrieval With Self-supervised Fine-grained Alignment</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Towards Fast And Accurate Image-text Retrieval With Self-supervised Fine-grained Alignment' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Towards Fast And Accurate Image-text Retrieval With Self-supervised Fine-grained Alignment' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhuang et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>6</td>
    <td><p>Image-text retrieval requires the system to bridge the heterogenous gap
between vision and language for accurate retrieval while keeping the network
lightweight-enough for efficient retrieval. Existing trade-off solutions mainly
study from the view of incorporating cross-modal interactions with the
independent-embedding framework or leveraging stronger pretrained encoders,
which still demand time-consuming similarity measurement or heavyweight model
structure in the retrieval stage. In this work, we propose an image-text
alignment module SelfAlign on top of the independent-embedding framework, which
improves the retrieval accuracy while maintains the retrieval efficiency
without extra supervision. SelfAlign contains two collaborative sub-modules
that force image-text alignment at both concept level and context level by
self-supervised contrastive learning. It does not require cross-modal embedding
interactions during training while maintaining independent image and text
encoders during retrieval. With comparable time cost, SelfAlign consistently
boosts the accuracy of state-of-the-art non-pretraining independent-embedding
models respectively by 9.1%, 4.2% and 6.6% in terms of R@sum score on
Flickr30K, MSCOCO 1K and MS-COCO 5K datasets. The retrieval accuracy also
outperforms most existing interactive-embedding models with orders of magnitude
decrease in retrieval time. The source code is available at:
https://github.com/Zjamie813/SelfAlign.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Text Retrieval 
      
        Self SUPERVISED 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/bin2023unifying/">Unifying Two-stream Encoders With Transformers For Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unifying Two-stream Encoders With Transformers For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unifying Two-stream Encoders With Transformers For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Bin et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 31st ACM International Conference on Multimedia</td>
    <td>16</td>
    <td><p>Most existing cross-modal retrieval methods employ two-stream encoders with
different architectures for images and texts, \textit{e.g.}, CNN for images and
RNN/Transformer for texts. Such discrepancy in architectures may induce
different semantic distribution spaces and limit the interactions between
images and texts, and further result in inferior alignment between images and
texts. To fill this research gap, inspired by recent advances of Transformers
in vision tasks, we propose to unify the encoder architectures with
Transformers for both modalities. Specifically, we design a cross-modal
retrieval framework purely based on two-stream Transformers, dubbed
\textbf{Hierarchical Alignment Transformers (HAT)}, which consists of an image
Transformer, a text Transformer, and a hierarchical alignment module. With such
identical architectures, the encoders could produce representations with more
similar characteristics for images and texts, and make the interactions and
alignments between them much easier. Besides, to leverage the rich semantics,
we devise a hierarchical alignment scheme to explore multi-level
correspondences of different layers between images and texts. To evaluate the
effectiveness of the proposed HAT, we conduct extensive experiments on two
benchmark datasets, MSCOCO and Flickr30K. Experimental results demonstrate that
HAT outperforms SOTA baselines by a large margin. Specifically, on two key
tasks, \textit{i.e.}, image-to-text and text-to-image retrieval, HAT achieves
7.6% and 16.7% relative score improvement of Recall@1 on MSCOCO, and 4.4%
and 11.6% on Flickr30k respectively. The code is available at
https://github.com/LuminosityX/HAT.</p>
</td>
    <td>
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/berton2023eigenplaces/">Eigenplaces: Training Viewpoint Robust Models For Visual Place Recognition</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Eigenplaces: Training Viewpoint Robust Models For Visual Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Eigenplaces: Training Viewpoint Robust Models For Visual Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Berton et al.</td> <!-- 🔧 You were missing this -->
    <td>2023 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>31</td>
    <td><p>Visual Place Recognition is a task that aims to predict the place of an image
(called query) based solely on its visual features. This is typically done
through image retrieval, where the query is matched to the most similar images
from a large database of geotagged photos, using learned global descriptors. A
major challenge in this task is recognizing places seen from different
viewpoints. To overcome this limitation, we propose a new method, called
EigenPlaces, to train our neural network on images from different point of
views, which embeds viewpoint robustness into the learned global descriptors.
The underlying idea is to cluster the training data so as to explicitly present
the model with different views of the same points of interest. The selection of
this points of interest is done without the need for extra supervision. We then
present experiments on the most comprehensive set of datasets in literature,
finding that EigenPlaces is able to outperform previous state of the art on the
majority of datasets, while requiring 60% less GPU memory for training and
using 50% smaller descriptors. The code and trained models for EigenPlaces are
available at {\small{https://github.com/gmberton/EigenPlaces}}, while
results with any other baseline can be computed with the codebase at
{\small{https://github.com/gmberton/auto_VPR}}.</p>
</td>
    <td>
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/baldrati2023composed/">Composed Image Retrieval Using Contrastive Learning And Task-oriented Clip-based Features</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Composed Image Retrieval Using Contrastive Learning And Task-oriented Clip-based Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Composed Image Retrieval Using Contrastive Learning And Task-oriented Clip-based Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Baldrati et al.</td> <!-- 🔧 You were missing this -->
    <td>ACM Transactions on Multimedia Computing, Communications, and Applications</td>
    <td>13</td>
    <td><p>Given a query composed of a reference image and a relative caption, the
Composed Image Retrieval goal is to retrieve images visually similar to the
reference one that integrates the modifications expressed by the caption. Given
that recent research has demonstrated the efficacy of large-scale vision and
language pre-trained (VLP) models in various tasks, we rely on features from
the OpenAI CLIP model to tackle the considered task. We initially perform a
task-oriented fine-tuning of both CLIP encoders using the element-wise sum of
visual and textual features. Then, in the second stage, we train a Combiner
network that learns to combine the image-text features integrating the bimodal
information and providing combined features used to perform the retrieval. We
use contrastive learning in both stages of training. Starting from the bare
CLIP features as a baseline, experimental results show that the task-oriented
fine-tuning and the carefully crafted Combiner network are highly effective and
outperform more complex state-of-the-art approaches on FashionIQ and CIRR, two
popular and challenging datasets for composed image retrieval. Code and
pre-trained models are available at https://github.com/ABaldrati/CLIP4Cir</p>
</td>
    <td>
      
        Self SUPERVISED 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/zhu2023coarse/">Coarse-to-fine: Learning Compact Discriminative Representation For Single-stage Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Coarse-to-fine: Learning Compact Discriminative Representation For Single-stage Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Coarse-to-fine: Learning Compact Discriminative Representation For Single-stage Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhu et al.</td> <!-- 🔧 You were missing this -->
    <td>2023 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>5</td>
    <td><p>Image retrieval targets to find images from a database that are visually
similar to the query image. Two-stage methods following retrieve-and-rerank
paradigm have achieved excellent performance, but their separate local and
global modules are inefficient to real-world applications. To better trade-off
retrieval efficiency and accuracy, some approaches fuse global and local
feature into a joint representation to perform single-stage image retrieval.
However, they are still challenging due to various situations to tackle,
\(e.g.\), background, occlusion and viewpoint. In this work, we design a
Coarse-to-Fine framework to learn Compact Discriminative representation (CFCD)
for end-to-end single-stage image retrieval-requiring only image-level labels.
Specifically, we first design a novel adaptive softmax-based loss which
dynamically tunes its scale and margin within each mini-batch and increases
them progressively to strengthen supervision during training and intra-class
compactness. Furthermore, we propose a mechanism which attentively selects
prominent local descriptors and infuse fine-grained semantic relations into the
global representation by a hard negative sampling strategy to optimize
inter-class distinctiveness at a global scale. Extensive experimental results
have demonstrated the effectiveness of our method, which achieves
state-of-the-art single-stage image retrieval performance on benchmarks such as
Revisited Oxford and Revisited Paris. Code is available at
https://github.com/bassyess/CFCD.</p>
</td>
    <td>
      
        Image Retrieval 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/an2023unicom/">Unicom: Universal And Compact Representation Learning For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unicom: Universal And Compact Representation Learning For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unicom: Universal And Compact Representation Learning For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>An et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>10</td>
    <td><p>Modern image retrieval methods typically rely on fine-tuning pre-trained
encoders to extract image-level descriptors. However, the most widely used
models are pre-trained on ImageNet-1K with limited classes. The pre-trained
feature representation is therefore not universal enough to generalize well to
the diverse open-world classes. In this paper, we first cluster the large-scale
LAION400M into one million pseudo classes based on the joint textual and visual
features extracted by the CLIP model. Due to the confusion of label
granularity, the automatically clustered dataset inevitably contains heavy
inter-class conflict. To alleviate such conflict, we randomly select partial
inter-class prototypes to construct the margin-based softmax loss. To further
enhance the low-dimensional feature representation, we randomly select partial
feature dimensions when calculating the similarities between embeddings and
class-wise prototypes. The dual random partial selections are with respect to
the class dimension and the feature dimension of the prototype matrix, making
the classification conflict-robust and the feature embedding compact. Our
method significantly outperforms state-of-the-art unsupervised and supervised
image retrieval approaches on multiple benchmarks. The code and pre-trained
models are released to facilitate future research
https://github.com/deepglint/unicom.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/aguerrebere2023similarity/">Similarity Search In The Blink Of An Eye With Compressed Indices</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Similarity Search In The Blink Of An Eye With Compressed Indices' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Similarity Search In The Blink Of An Eye With Compressed Indices' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Aguerrebere et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the VLDB Endowment</td>
    <td>15</td>
    <td><p>Nowadays, data is represented by vectors. Retrieving those vectors, among
millions and billions, that are similar to a given query is a ubiquitous
problem, known as similarity search, of relevance for a wide range of
applications. Graph-based indices are currently the best performing techniques
for billion-scale similarity search. However, their random-access memory
pattern presents challenges to realize their full potential. In this work, we
present new techniques and systems for creating faster and smaller graph-based
indices. To this end, we introduce a novel vector compression method,
Locally-adaptive Vector Quantization (LVQ), that uses per-vector scaling and
scalar quantization to improve search performance with fast similarity
computations and a reduced effective bandwidth, while decreasing memory
footprint and barely impacting accuracy. LVQ, when combined with a new
high-performance computing system for graph-based similarity search,
establishes the new state of the art in terms of performance and memory
footprint. For billions of vectors, LVQ outcompetes the second-best
alternatives: (1) in the low-memory regime, by up to 20.7x in throughput with
up to a 3x memory footprint reduction, and (2) in the high-throughput regime by
5.8x with 1.4x less memory.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/zhang2023orthonormal/">Orthonormal Product Quantization Network For Scalable Face Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Orthonormal Product Quantization Network For Scalable Face Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Orthonormal Product Quantization Network For Scalable Face Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Ming, Zhe Xuefei, Yan Hong</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition</td>
    <td>9</td>
    <td><p>Existing deep quantization methods provided an efficient solution for
large-scale image retrieval. However, the significant intra-class variations
like pose, illumination, and expressions in face images, still pose a challenge
for face image retrieval. In light of this, face image retrieval requires
sufficiently powerful learning metrics, which are absent in current deep
quantization works. Moreover, to tackle the growing unseen identities in the
query stage, face image retrieval drives more demands regarding model
generalization and system scalability than general image retrieval tasks. This
paper integrates product quantization with orthonormal constraints into an
end-to-end deep learning framework to effectively retrieve face images.
Specifically, a novel scheme that uses predefined orthonormal vectors as
codewords is proposed to enhance the quantization informativeness and reduce
codewords’ redundancy. A tailored loss function maximizes discriminability
among identities in each quantization subspace for both the quantized and
original features. An entropy-based regularization term is imposed to reduce
the quantization error. Experiments are conducted on four commonly-used face
datasets under both seen and unseen identities retrieval settings. Our method
outperforms all the compared deep hashing/quantization state-of-the-arts under
both settings. Results validate the effectiveness of the proposed orthonormal
codewords in improving models’ standard retrieval performance and
generalization ability. Combing with further experiments on two general image
datasets, it demonstrates the broad superiority of our method for scalable
image retrieval.</p>
</td>
    <td>
      
        Quantization 
      
        Image Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/zhang2023cafe/">CAFE: Towards Compact, Adaptive, And Fast Embedding For Large-scale Recommendation Models</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=CAFE: Towards Compact, Adaptive, And Fast Embedding For Large-scale Recommendation Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=CAFE: Towards Compact, Adaptive, And Fast Embedding For Large-scale Recommendation Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the ACM on Management of Data</td>
    <td>8</td>
    <td><p>Recently, the growing memory demands of embedding tables in Deep Learning
Recommendation Models (DLRMs) pose great challenges for model training and
deployment. Existing embedding compression solutions cannot simultaneously meet
three key design requirements: memory efficiency, low latency, and adaptability
to dynamic data distribution. This paper presents CAFE, a Compact, Adaptive,
and Fast Embedding compression framework that addresses the above requirements.
The design philosophy of CAFE is to dynamically allocate more memory resources
to important features (called hot features), and allocate less memory to
unimportant ones. In CAFE, we propose a fast and lightweight sketch data
structure, named HotSketch, to capture feature importance and report hot
features in real time. For each reported hot feature, we assign it a unique
embedding. For the non-hot features, we allow multiple features to share one
embedding by using hash embedding technique. Guided by our design philosophy,
we further propose a multi-level hash embedding framework to optimize the
embedding tables of non-hot features. We theoretically analyze the accuracy of
HotSketch, and analyze the model convergence against deviation. Extensive
experiments show that CAFE significantly outperforms existing embedding
compression methods, yielding 3.92% and 3.68% superior testing AUC on Criteo
Kaggle dataset and CriteoTB dataset at a compression ratio of 10000x. The
source codes of CAFE are available at GitHub.</p>
</td>
    <td>
      
        Recommender Systems 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/yuan2023semantic/">Semantic-aware Adversarial Training For Reliable Deep Hashing Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semantic-aware Adversarial Training For Reliable Deep Hashing Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semantic-aware Adversarial Training For Reliable Deep Hashing Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yuan et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Information Forensics and Security</td>
    <td>12</td>
    <td><p>Deep hashing has been intensively studied and successfully applied in
large-scale image retrieval systems due to its efficiency and effectiveness.
Recent studies have recognized that the existence of adversarial examples poses
a security threat to deep hashing models, that is, adversarial vulnerability.
Notably, it is challenging to efficiently distill reliable semantic
representatives for deep hashing to guide adversarial learning, and thereby it
hinders the enhancement of adversarial robustness of deep hashing-based
retrieval models. Moreover, current researches on adversarial training for deep
hashing are hard to be formalized into a unified minimax structure. In this
paper, we explore Semantic-Aware Adversarial Training (SAAT) for improving the
adversarial robustness of deep hashing models. Specifically, we conceive a
discriminative mainstay features learning (DMFL) scheme to construct semantic
representatives for guiding adversarial learning in deep hashing. Particularly,
our DMFL with the strict theoretical guarantee is adaptively optimized in a
discriminative learning manner, where both discriminative and semantic
properties are jointly considered. Moreover, adversarial examples are
fabricated by maximizing the Hamming distance between the hash codes of
adversarial samples and mainstay features, the efficacy of which is validated
in the adversarial attack trials. Further, we, for the first time, formulate
the formalized adversarial training of deep hashing into a unified minimax
optimization under the guidance of the generated mainstay codes. Extensive
experiments on benchmark datasets show superb attack performance against the
state-of-the-art algorithms, meanwhile, the proposed adversarial training can
effectively eliminate adversarial perturbations for trustworthy deep
hashing-based retrieval. Our code is available at
https://github.com/xandery-geek/SAAT.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/yu2023live/">Live Laparoscopic Video Retrieval With Compressed Uncertainty</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Live Laparoscopic Video Retrieval With Compressed Uncertainty' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Live Laparoscopic Video Retrieval With Compressed Uncertainty' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yu et al.</td> <!-- 🔧 You were missing this -->
    <td>Medical Image Analysis</td>
    <td>5</td>
    <td><p>Searching through large volumes of medical data to retrieve relevant
information is a challenging yet crucial task for clinical care. However the
primitive and most common approach to retrieval, involving text in the form of
keywords, is severely limited when dealing with complex media formats.
Content-based retrieval offers a way to overcome this limitation, by using rich
media as the query itself. Surgical video-to-video retrieval in particular is a
new and largely unexplored research problem with high clinical value,
especially in the real-time case: using real-time video hashing, search can be
achieved directly inside of the operating room. Indeed, the process of hashing
converts large data entries into compact binary arrays or hashes, enabling
large-scale search operations at a very fast rate. However, due to fluctuations
over the course of a video, not all bits in a given hash are equally reliable.
In this work, we propose a method capable of mitigating this uncertainty while
maintaining a light computational footprint. We present superior retrieval
results (3-4 % top 10 mean average precision) on a multi-task evaluation
protocol for surgery, using cholecystectomy phases, bypass phases, and coming
from an entirely new dataset introduced here, critical events across six
different surgery types. Success on this multi-task benchmark shows the
generalizability of our approach for surgical video retrieval.</p>
</td>
    <td>
      
        Video Retrieval 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/wu2022hqann/">HQANN: Efficient And Robust Similarity Search For Hybrid Queries With Structured And Unstructured Constraints</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=HQANN: Efficient And Robust Similarity Search For Hybrid Queries With Structured And Unstructured Constraints' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=HQANN: Efficient And Robust Similarity Search For Hybrid Queries With Structured And Unstructured Constraints' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</td>
    <td>8</td>
    <td><p>The in-memory approximate nearest neighbor search (ANNS) algorithms have
achieved great success for fast high-recall query processing, but are extremely
inefficient when handling hybrid queries with unstructured (i.e., feature
vectors) and structured (i.e., related attributes) constraints. In this paper,
we present HQANN, a simple yet highly efficient hybrid query processing
framework which can be easily embedded into existing proximity graph-based ANNS
algorithms. We guarantee both low latency and high recall by leveraging
navigation sense among attributes and fusing vector similarity search with
attribute filtering. Experimental results on both public and in-house datasets
demonstrate that HQANN is 10x faster than the state-of-the-art hybrid ANNS
solutions to reach the same recall quality and its performance is hardly
affected by the complexity of attributes. It can reach 99% recall@10 in just
around 50 microseconds On GLOVE-1.2M with thousands of attribute constraints.</p>
</td>
    <td>
      
        Similarity Search 
      
        CIKM 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/ma2022pre/">Pre-train A Discriminative Text Encoder For Dense Retrieval Via Contrastive Span Prediction</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Pre-train A Discriminative Text Encoder For Dense Retrieval Via Contrastive Span Prediction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Pre-train A Discriminative Text Encoder For Dense Retrieval Via Contrastive Span Prediction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ma et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>30</td>
    <td><p>Dense retrieval has shown promising results in many information retrieval
(IR) related tasks, whose foundation is high-quality text representation
learning for effective search. Some recent studies have shown that
autoencoder-based language models are able to boost the dense retrieval
performance using a weak decoder. However, we argue that 1) it is not
discriminative to decode all the input texts and, 2) even a weak decoder has
the bypass effect on the encoder. Therefore, in this work, we introduce a novel
contrastive span prediction task to pre-train the encoder alone, but still
retain the bottleneck ability of the autoencoder. % Therefore, in this work, we
propose to drop out the decoder and introduce a novel contrastive span
prediction task to pre-train the encoder alone. The key idea is to force the
encoder to generate the text representation close to its own random spans while
far away from others using a group-wise contrastive loss. In this way, we can
1) learn discriminative text representations efficiently with the group-wise
contrastive learning over spans and, 2) avoid the bypass effect of the decoder
thoroughly. Comprehensive experiments over publicly available retrieval
benchmark datasets show that our approach can outperform existing pre-training
methods for dense retrieval significantly.</p>
</td>
    <td>
      
        SIGIR 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/ma2022x/">X-CLIP: End-to-end Multi-grained Contrastive Learning For Video-text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=X-CLIP: End-to-end Multi-grained Contrastive Learning For Video-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=X-CLIP: End-to-end Multi-grained Contrastive Learning For Video-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ma et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 30th ACM International Conference on Multimedia</td>
    <td>147</td>
    <td><p>Video-text retrieval has been a crucial and fundamental task in multi-modal
research. The development of video-text retrieval has been considerably
promoted by large-scale multi-modal contrastive pre-training, which primarily
focuses on coarse-grained or fine-grained contrast. However, cross-grained
contrast, which is the contrast between coarse-grained representations and
fine-grained representations, has rarely been explored in prior research.
Compared with fine-grained or coarse-grained contrasts, cross-grained contrast
calculate the correlation between coarse-grained features and each fine-grained
feature, and is able to filter out the unnecessary fine-grained features guided
by the coarse-grained feature during similarity calculation, thus improving the
accuracy of retrieval. To this end, this paper presents a novel multi-grained
contrastive model, namely X-CLIP, for video-text retrieval. However, another
challenge lies in the similarity aggregation problem, which aims to aggregate
fine-grained and cross-grained similarity matrices to instance-level
similarity. To address this challenge, we propose the Attention Over Similarity
Matrix (AOSM) module to make the model focus on the contrast between essential
frames and words, thus lowering the impact of unnecessary frames and words on
retrieval results. With multi-grained contrast and the proposed AOSM module,
X-CLIP achieves outstanding performance on five widely-used video-text
retrieval datasets, including MSR-VTT (49.3 R@1), MSVD (50.4 R@1), LSMDC (26.1
R@1), DiDeMo (47.8 R@1) and ActivityNet (46.2 R@1). It outperforms the previous
state-of-theart by +6.3%, +6.6%, +11.1%, +6.7%, +3.8% relative improvements on
these benchmarks, demonstrating the superiority of multi-grained contrast and
AOSM.</p>
</td>
    <td>
      
        Self SUPERVISED 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/luo2022survey/">A Survey On Deep Hashing Methods</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Survey On Deep Hashing Methods' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Survey On Deep Hashing Methods' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Luo et al.</td> <!-- 🔧 You were missing this -->
    <td>ACM Transactions on Knowledge Discovery from Data</td>
    <td>113</td>
    <td><p>Nearest neighbor search aims to obtain the samples in the database with the
smallest distances from them to the queries, which is a basic task in a range
of fields, including computer vision and data mining. Hashing is one of the
most widely used methods for its computational and storage efficiency. With the
development of deep learning, deep hashing methods show more advantages than
traditional methods. In this survey, we detailedly investigate current deep
hashing algorithms including deep supervised hashing and deep unsupervised
hashing. Specifically, we categorize deep supervised hashing methods into
pairwise methods, ranking-based methods, pointwise methods as well as
quantization according to how measuring the similarities of the learned hash
codes. Moreover, deep unsupervised hashing is categorized into similarity
reconstruction-based methods, pseudo-label-based methods and prediction-free
self-supervised learning-based methods based on their semantic learning
manners. We also introduce three related important topics including
semi-supervised deep hashing, domain adaption deep hashing and multi-modal deep
hashing. Meanwhile, we present some commonly used public datasets and the
scheme to measure the performance of deep hashing algorithms. Finally, we
discuss some potential research directions in conclusion.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        Survey Paper 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/wu2022online/">Online Enhanced Semantic Hashing: Towards Effective And Efficient Retrieval For Streaming Multi-modal Data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Online Enhanced Semantic Hashing: Towards Effective And Efficient Retrieval For Streaming Multi-modal Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Online Enhanced Semantic Hashing: Towards Effective And Efficient Retrieval For Streaming Multi-modal Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>5</td>
    <td><p>With the vigorous development of multimedia equipment and applications,
efficient retrieval of large-scale multi-modal data has become a trendy
research topic. Thereinto, hashing has become a prevalent choice due to its
retrieval efficiency and low storage cost. Although multi-modal hashing has
drawn lots of attention in recent years, there still remain some problems. The
first point is that existing methods are mainly designed in batch mode and not
able to efficiently handle streaming multi-modal data. The second point is that
all existing online multi-modal hashing methods fail to effectively handle
unseen new classes which come continuously with streaming data chunks. In this
paper, we propose a new model, termed Online enhAnced SemantIc haShing (OASIS).
We design novel semantic-enhanced representation for data, which could help
handle the new coming classes, and thereby construct the enhanced semantic
objective function. An efficient and effective discrete online optimization
algorithm is further proposed for OASIS. Extensive experiments show that our
method can exceed the state-of-the-art models. For good reproducibility and
benefiting the community, our code and data are already available in
supplementary material and will be made publicly available.</p>
</td>
    <td>
      
        Similarity Search 
      
        Hashing Methods 
      
        Text Retrieval 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/wu2022learning/">Learning Deep Semantic Model For Code Search Using Codesearchnet Corpus</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Deep Semantic Model For Code Search Using Codesearchnet Corpus' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Deep Semantic Model For Code Search Using Codesearchnet Corpus' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu Chen, Yan Ming</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>6</td>
    <td><p>Semantic code search is the task of retrieving relevant code snippet given a
natural language query. Different from typical information retrieval tasks,
code search requires to bridge the semantic gap between the programming
language and natural language, for better describing intrinsic concepts and
semantics. Recently, deep neural network for code search has been a hot
research topic. Typical methods for neural code search first represent the code
snippet and query text as separate embeddings, and then use vector distance
(e.g. dot-product or cosine) to calculate the semantic similarity between them.
There exist many different ways for aggregating the variable length of code or
query tokens into a learnable embedding, including bi-encoder, cross-encoder,
and poly-encoder. The goal of the query encoder and code encoder is to produce
embeddings that are close with each other for a related pair of query and the
corresponding desired code snippet, in which the choice and design of encoder
is very significant.
  In this paper, we propose a novel deep semantic model which makes use of the
utilities of not only the multi-modal sources, but also feature extractors such
as self-attention, the aggregated vectors, combination of the intermediate
representations. We apply the proposed model to tackle the CodeSearchNet
challenge about semantic code search. We align cross-lingual embedding for
multi-modality learning with large batches and hard example mining, and combine
different learned representations for better enhancing the representation
learning. Our model is trained on CodeSearchNet corpus and evaluated on the
held-out data, the final model achieves 0.384 NDCG and won the first place in
this benchmark. Models and code are available at
https://github.com/overwindows/SemanticCodeSearch.git.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/lu2022learnable/">Learnable Locality-sensitive Hashing For Video Anomaly Detection</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learnable Locality-sensitive Hashing For Video Anomaly Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learnable Locality-sensitive Hashing For Video Anomaly Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lu Yue, Cao Congqi, Zhang Yanning</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Circuits and Systems for Video Technology</td>
    <td>23</td>
    <td><p>Video anomaly detection (VAD) mainly refers to identifying anomalous events
that have not occurred in the training set where only normal samples are
available. Existing works usually formulate VAD as a reconstruction or
prediction problem. However, the adaptability and scalability of these methods
are limited. In this paper, we propose a novel distance-based VAD method to
take advantage of all the available normal data efficiently and flexibly. In
our method, the smaller the distance between a testing sample and normal
samples, the higher the probability that the testing sample is normal.
Specifically, we propose to use locality-sensitive hashing (LSH) to map samples
whose similarity exceeds a certain threshold into the same bucket in advance.
In this manner, the complexity of near neighbor search is cut down
significantly. To make the samples that are semantically similar get closer and
samples not similar get further apart, we propose a novel learnable version of
LSH that embeds LSH into a neural network and optimizes the hash functions with
contrastive learning strategy. The proposed method is robust to data imbalance
and can handle the large intra-class variations in normal data flexibly.
Besides, it has a good ability of scalability. Extensive experiments
demonstrate the superiority of our method, which achieves new state-of-the-art
results on VAD benchmarks.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/long2022retrieval/">Retrieval Augmented Classification For Long-tail Visual Recognition</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Retrieval Augmented Classification For Long-tail Visual Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Retrieval Augmented Classification For Long-tail Visual Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Long et al.</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>40</td>
    <td><p>We introduce Retrieval Augmented Classification (RAC), a generic approach to
augmenting standard image classification pipelines with an explicit retrieval
module. RAC consists of a standard base image encoder fused with a parallel
retrieval branch that queries a non-parametric external memory of pre-encoded
images and associated text snippets. We apply RAC to the problem of long-tail
classification and demonstrate a significant improvement over previous
state-of-the-art on Places365-LT and iNaturalist-2018 (14.5% and 6.7%
respectively), despite using only the training datasets themselves as the
external information source. We demonstrate that RAC’s retrieval module,
without prompting, learns a high level of accuracy on tail classes. This, in
turn, frees the base encoder to focus on common classes, and improve its
performance thereon. RAC represents an alternative approach to utilizing large,
pretrained models without requiring fine-tuning, as well as a first step
towards more effectively making use of external memory within common computer
vision architectures.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/zheng2022introspective/">Introspective Deep Metric Learning For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Introspective Deep Metric Learning For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Introspective Deep Metric Learning For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zheng et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>10</td>
    <td><p>This paper proposes an introspective deep metric learning (IDML) framework
for uncertainty-aware comparisons of images. Conventional deep metric learning
methods produce confident semantic distances between images regardless of the
uncertainty level. However, we argue that a good similarity model should
consider the semantic discrepancies with caution to better deal with ambiguous
images for more robust training. To achieve this, we propose to represent an
image using not only a semantic embedding but also an accompanying uncertainty
embedding, which describes the semantic characteristics and ambiguity of an
image, respectively. We further propose an introspective similarity metric to
make similarity judgments between images considering both their semantic
differences and ambiguities. The proposed IDML framework improves the
performance of deep metric learning through uncertainty modeling and attains
state-of-the-art results on the widely used CUB-200-2011, Cars196, and Stanford
Online Products datasets for image retrieval and clustering. We further provide
an in-depth analysis of our framework to demonstrate the effectiveness and
reliability of IDML. Code is available at: https://github.com/wzzheng/IDML.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/liu2022hs/">HS-GCN: Hamming Spatial Graph Convolutional Networks For Recommendation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=HS-GCN: Hamming Spatial Graph Convolutional Networks For Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=HS-GCN: Hamming Spatial Graph Convolutional Networks For Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Knowledge and Data Engineering</td>
    <td>32</td>
    <td><p>An efficient solution to the large-scale recommender system is to represent
users and items as binary hash codes in the Hamming space. Towards this end,
existing methods tend to code users by modeling their Hamming similarities with
the items they historically interact with, which are termed as the first-order
similarities in this work. Despite their efficiency, these methods suffer from
the suboptimal representative capacity, since they forgo the correlation
established by connecting multiple first-order similarities, i.e., the relation
among the indirect instances, which could be defined as the high-order
similarity. To tackle this drawback, we propose to model both the first- and
the high-order similarities in the Hamming space through the user-item
bipartite graph. Therefore, we develop a novel learning to hash framework,
namely Hamming Spatial Graph Convolutional Networks (HS-GCN), which explicitly
models the Hamming similarity and embeds it into the codes of users and items.
Extensive experiments on three public benchmark datasets demonstrate that our
proposed model significantly outperforms several state-of-the-art hashing
models, and obtains performance comparable with the real-valued recommendation
models.</p>
</td>
    <td>
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/liu2022das/">DAS: Densely-anchored Sampling For Deep Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=DAS: Densely-anchored Sampling For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=DAS: Densely-anchored Sampling For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>13</td>
    <td><p>Deep Metric Learning (DML) serves to learn an embedding function to project
semantically similar data into nearby embedding space and plays a vital role in
many applications, such as image retrieval and face recognition. However, the
performance of DML methods often highly depends on sampling methods to choose
effective data from the embedding space in the training. In practice, the
embeddings in the embedding space are obtained by some deep models, where the
embedding space is often with barren area due to the absence of training
points, resulting in so called “missing embedding” issue. This issue may impair
the sample quality, which leads to degenerated DML performance. In this work,
we investigate how to alleviate the “missing embedding” issue to improve the
sampling quality and achieve effective DML. To this end, we propose a
Densely-Anchored Sampling (DAS) scheme that considers the embedding with
corresponding data point as “anchor” and exploits the anchor’s nearby embedding
space to densely produce embeddings without data points. Specifically, we
propose to exploit the embedding space around single anchor with Discriminative
Feature Scaling (DFS) and multiple anchors with Memorized Transformation
Shifting (MTS). In this way, by combing the embeddings with and without data
points, we are able to provide more embeddings to facilitate the sampling
process thus boosting the performance of DML. Our method is effortlessly
integrated into existing DML frameworks and improves them without bells and
whistles. Extensive experiments on three benchmark datasets demonstrate the
superiority of our method.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/liu2022neuromorphic/">Neuromorphic Computing For Content-based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Neuromorphic Computing For Content-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Neuromorphic Computing For Content-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>PLOS ONE</td>
    <td>15</td>
    <td><p>Neuromorphic computing mimics the neural activity of the brain through
emulating spiking neural networks. In numerous machine learning tasks,
neuromorphic chips are expected to provide superior solutions in terms of cost
and power efficiency. Here, we explore the application of Loihi, a neuromorphic
computing chip developed by Intel, for the computer vision task of image
retrieval. We evaluated the functionalities and the performance metrics that
are critical in content-based visual search and recommender systems using
deep-learning embeddings. Our results show that the neuromorphic solution is
about 2.5 times more energy-efficient compared with an ARM Cortex-A72 CPU and
12.5 times more energy-efficient compared with NVIDIA T4 GPU for inference by a
lightweight convolutional neural network without batching while maintaining the
same level of matching accuracy. The study validates the potential of
neuromorphic computing in low-power image retrieval, as a complementary
paradigm to the existing von Neumann architectures.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/wu2022self/">Self-supervised Consistent Quantization For Fully Unsupervised Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Self-supervised Consistent Quantization For Fully Unsupervised Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Self-supervised Consistent Quantization For Fully Unsupervised Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu Guile, Zhang Chao, Liwicki Stephan</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>51</td>
    <td><p>Unsupervised image retrieval aims to learn an efficient retrieval system
without expensive data annotations, but most existing methods rely heavily on
handcrafted feature descriptors or pre-trained feature extractors. To minimize
human supervision, recent advance proposes deep fully unsupervised image
retrieval aiming at training a deep model from scratch to jointly optimize
visual features and quantization codes. However, existing approach mainly
focuses on instance contrastive learning without considering underlying
semantic structure information, resulting in sub-optimal performance. In this
work, we propose a novel self-supervised consistent quantization approach to
deep fully unsupervised image retrieval, which consists of part consistent
quantization and global consistent quantization. In part consistent
quantization, we devise part neighbor semantic consistency learning with
codeword diversity regularization. This allows to discover underlying neighbor
structure information of sub-quantized representations as self-supervision. In
global consistent quantization, we employ contrastive learning for both
embedding and quantized representations and fuses these representations for
consistent contrastive regularization between instances. This can make up for
the loss of useful representation information during quantization and
regularize consistency between instances. With a unified learning objective of
part and global consistent quantization, our approach exploits richer
self-supervision cues to facilitate model learning. Extensive experiments on
three benchmark datasets show the superiority of our approach over the
state-of-the-art methods.</p>
</td>
    <td>
      
        Quantization 
      
        Self SUPERVISED 
      
        Image Retrieval 
      
        Unsupervised 
      
        SUPERVISED 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/lin2022deep/">Deep Unsupervised Hashing With Latent Semantic Components</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Unsupervised Hashing With Latent Semantic Components' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Unsupervised Hashing With Latent Semantic Components' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>17</td>
    <td><p>Deep unsupervised hashing has been appreciated in the regime of image
retrieval. However, most prior arts failed to detect the semantic components
and their relationships behind the images, which makes them lack discriminative
power. To make up the defect, we propose a novel Deep Semantic Components
Hashing (DSCH), which involves a common sense that an image normally contains a
bunch of semantic components with homology and co-occurrence relationships.
Based on this prior, DSCH regards the semantic components as latent variables
under the Expectation-Maximization framework and designs a two-step iterative
algorithm with the objective of maximum likelihood of training data. Firstly,
DSCH constructs a semantic component structure by uncovering the fine-grained
semantics components of images with a Gaussian Mixture Modal~(GMM), where an
image is represented as a mixture of multiple components, and the semantics
co-occurrence are exploited. Besides, coarse-grained semantics components, are
discovered by considering the homology relationships between fine-grained
components, and the hierarchy organization is then constructed. Secondly, DSCH
makes the images close to their semantic component centers at both fine-grained
and coarse-grained levels, and also makes the images share similar semantic
components close to each other. Extensive experiments on three benchmark
datasets demonstrate that the proposed hierarchical semantic components indeed
facilitate the hashing model to achieve superior performance.</p>
</td>
    <td>
      
        Unsupervised 
      
        Neural Hashing 
      
        AAAI 
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/zhao2022constrained/">Constrained Approximate Similarity Search On Proximity Graph</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Constrained Approximate Similarity Search On Proximity Graph' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Constrained Approximate Similarity Search On Proximity Graph' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhao Weijie, Tan Shulong, Li Ping</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</td>
    <td>19</td>
    <td><p>Search engines and recommendation systems are built to efficiently display
relevant information from those massive amounts of candidates. Typically a
three-stage mechanism is employed in those systems: (i) a small collection of
items are first retrieved by (e.g.,) approximate near neighbor search
algorithms; (ii) then a collection of constraints are applied on the retrieved
items; (iii) a fine-grained ranking neural network is employed to determine the
final recommendation. We observe a major defect of the original three-stage
pipeline: Although we only target to retrieve \(k\) vectors in the final
recommendation, we have to preset a sufficiently large \(s\) (\(s &gt; k\)) for each
query, and ``hope’’ the number of survived vectors after the filtering is not
smaller than \(k\). That is, at least \(k\) vectors in the \(s\) similar candidates
satisfy the query constraints.
  In this paper, we investigate this constrained similarity search problem and
attempt to merge the similarity search stage and the filtering stage into one
single search operation. We introduce AIRSHIP, a system that integrates a
user-defined function filtering into the similarity search framework. The
proposed system does not need to build extra indices nor require prior
knowledge of the query constraints. We propose three optimization strategies:
(1) starting point selection, (2) multi-direction search, and (3) biased
priority queue selection. Experimental evaluations on both synthetic and real
data confirm the effectiveness of the proposed AIRSHIP algorithm. We focus on
constrained graph-based approximate near neighbor (ANN) search in this study,
in part because graph-based ANN is known to achieve excellent performance. We
believe it is also possible to develop constrained hashing-based ANN or
constrained quantization-based ANN.</p>
</td>
    <td>
      
        KDD 
      
        Similarity Search 
      
        Graph Based ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/li2022adaptive/">Adaptive Structural Similarity Preserving For Unsupervised Cross Modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Adaptive Structural Similarity Preserving For Unsupervised Cross Modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Adaptive Structural Similarity Preserving For Unsupervised Cross Modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Liang, Zheng Baihua, Sun Weiwei</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 30th ACM International Conference on Multimedia</td>
    <td>22</td>
    <td><p>Cross-modal hashing is an important approach for multimodal data management
and application. Existing unsupervised cross-modal hashing algorithms mainly
rely on data features in pre-trained models to mine their similarity
relationships. However, their optimization objectives are based on the static
metric between the original uni-modal features, without further exploring data
correlations during the training. In addition, most of them mainly focus on
association mining and alignment among pairwise instances in continuous space
but ignore the latent structural correlations contained in the semantic hashing
space. In this paper, we propose an unsupervised hash learning framework,
namely Adaptive Structural Similarity Preservation Hashing (ASSPH), to solve
the above problems. Firstly, we propose an adaptive learning scheme, with
limited data and training batches, to enrich semantic correlations of unlabeled
instances during the training process and meanwhile to ensure a smooth
convergence of the training process. Secondly, we present an asymmetric
structural semantic representation learning scheme. We introduce structural
semantic metrics based on graph adjacency relations during the semantic
reconstruction and correlation mining stage and meanwhile align the structure
semantics in the hash space with an asymmetric binary optimization process.
Finally, we conduct extensive experiments to validate the enhancements of our
work in comparison with existing works.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/li2022dual/">Dual-stream Knowledge-preserving Hashing For Unsupervised Video Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Dual-stream Knowledge-preserving Hashing For Unsupervised Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Dual-stream Knowledge-preserving Hashing For Unsupervised Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>16</td>
    <td><p>Unsupervised video hashing usually optimizes binary codes by learning to
reconstruct input videos. Such reconstruction constraint spends much effort on
frame-level temporal context changes without focusing on video-level global
semantics that are more useful for retrieval. Hence, we address this problem by
decomposing video information into reconstruction-dependent and
semantic-dependent information, which disentangles the semantic extraction from
reconstruction constraint. Specifically, we first design a simple dual-stream
structure, including a temporal layer and a hash layer. Then, with the help of
semantic similarity knowledge obtained from self-supervision, the hash layer
learns to capture information for semantic retrieval, while the temporal layer
learns to capture the information for reconstruction. In this way, the model
naturally preserves the disentangled semantics into binary codes. Validated by
comprehensive experiments, our method consistently outperforms the
state-of-the-arts on three video benchmarks.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
        Video Retrieval 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/xin2022zero/">Zero-shot Dense Retrieval With Momentum Adversarial Domain Invariant Representations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Zero-shot Dense Retrieval With Momentum Adversarial Domain Invariant Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Zero-shot Dense Retrieval With Momentum Adversarial Domain Invariant Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xin et al.</td> <!-- 🔧 You were missing this -->
    <td>Findings of the Association for Computational Linguistics: ACL 2022</td>
    <td>17</td>
    <td><p>Dense retrieval (DR) methods conduct text retrieval by first encoding texts
in the embedding space and then matching them by nearest neighbor search. This
requires strong locality properties from the representation space, i.e, the
close allocations of each small group of relevant texts, which are hard to
generalize to domains without sufficient training data. In this paper, we aim
to improve the generalization ability of DR models from source training domains
with rich supervision signals to target domains without any relevant labels, in
the zero-shot setting. To achieve that, we propose Momentum adversarial Domain
Invariant Representation learning (MoDIR), which introduces a momentum method
in the DR training process to train a domain classifier distinguishing source
versus target, and then adversarially updates the DR encoder to learn domain
invariant representations. Our experiments show that MoDIR robustly outperforms
its baselines on 10+ ranking datasets from the BEIR benchmark in the zero-shot
setup, with more than 10% relative gains on datasets with enough sensitivity
for DR models’ evaluation. Source code of this paper will be released.</p>
</td>
    <td>
      
        Few Shot & Zero Shot 
      
        Robustness 
      
        ACL 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/leonhardt2022efficient/">Efficient Neural Ranking Using Forward Indexes</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Neural Ranking Using Forward Indexes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient Neural Ranking Using Forward Indexes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Leonhardt et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the ACM Web Conference 2022</td>
    <td>10</td>
    <td><p>Neural document ranking approaches, specifically transformer models, have
achieved impressive gains in ranking performance. However, query processing
using such over-parameterized models is both resource and time intensive. In
this paper, we propose the Fast-Forward index – a simple vector forward index
that facilitates ranking documents using interpolation of lexical and semantic
scores – as a replacement for contextual re-rankers and dense indexes based on
nearest neighbor search. Fast-Forward indexes rely on efficient sparse models
for retrieval and merely look up pre-computed dense transformer-based vector
representations of documents and passages in constant time for fast CPU-based
semantic similarity computation during query processing. We propose index
pruning and theoretically grounded early stopping techniques to improve the
query processing throughput. We conduct extensive large-scale experiments on
TREC-DL datasets and show improvements over hybrid indexes in performance and
query processing efficiency using only CPUs. Fast-Forward indexes can provide
superior ranking performance using interpolation due to the complementary
benefits of lexical and semantic similarities.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/lee2022correlation/">Correlation Verification For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Correlation Verification For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Correlation Verification For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lee et al.</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>59</td>
    <td><p>Geometric verification is considered a de facto solution for the re-ranking
task in image retrieval. In this study, we propose a novel image retrieval
re-ranking network named Correlation Verification Networks (CVNet). Our
proposed network, comprising deeply stacked 4D convolutional layers, gradually
compresses dense feature correlation into image similarity while learning
diverse geometric matching patterns from various image pairs. To enable
cross-scale matching, it builds feature pyramids and constructs cross-scale
feature correlations within a single inference, replacing costly multi-scale
inferences. In addition, we use curriculum learning with the hard negative
mining and Hide-and-Seek strategy to handle hard samples without losing
generality. Our proposed re-ranking network shows state-of-the-art performance
on several retrieval benchmarks with a significant margin (+12.6% in mAP on
ROxford-Hard+1M set) over state-of-the-art methods. The source code and models
are available online: https://github.com/sungonce/CVNet.</p>
</td>
    <td>
      
        Image Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/lei2022loopitr/">Loopitr: Combining Dual And Cross Encoder Architectures For Image-text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Loopitr: Combining Dual And Cross Encoder Architectures For Image-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Loopitr: Combining Dual And Cross Encoder Architectures For Image-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lei et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>6</td>
    <td><p>Dual encoders and cross encoders have been widely used for image-text
retrieval. Between the two, the dual encoder encodes the image and text
independently followed by a dot product, while the cross encoder jointly feeds
image and text as the input and performs dense multi-modal fusion. These two
architectures are typically modeled separately without interaction. In this
work, we propose LoopITR, which combines them in the same network for joint
learning. Specifically, we let the dual encoder provide hard negatives to the
cross encoder, and use the more discriminative cross encoder to distill its
predictions back to the dual encoder. Both steps are efficiently performed
together in the same model. Our work centers on empirical analyses of this
combined architecture, putting the main focus on the design of the distillation
objective. Our experimental results highlight the benefits of training the two
encoders in the same network, and demonstrate that distillation can be quite
effective with just a few hard negative examples. Experiments on two standard
datasets (Flickr30K and COCO) show our approach achieves state-of-the-art dual
encoder performance when compared with approaches using a similar amount of
data.</p>
</td>
    <td>
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/kordopatiszilos2022dns/">Dns: Distill-and-select For Efficient And Accurate Video Indexing And Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Dns: Distill-and-select For Efficient And Accurate Video Indexing And Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Dns: Distill-and-select For Efficient And Accurate Video Indexing And Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kordopatis-zilos et al.</td> <!-- 🔧 You were missing this -->
    <td>International Journal of Computer Vision</td>
    <td>27</td>
    <td><p>In this paper, we address the problem of high performance and computationally
efficient content-based video retrieval in large-scale datasets. Current
methods typically propose either: (i) fine-grained approaches employing
spatio-temporal representations and similarity calculations, achieving high
performance at a high computational cost or (ii) coarse-grained approaches
representing/indexing videos as global vectors, where the spatio-temporal
structure is lost, providing low performance but also having low computational
cost. In this work, we propose a Knowledge Distillation framework, called
Distill-and-Select (DnS), that starting from a well-performing fine-grained
Teacher Network learns: a) Student Networks at different retrieval performance
and computational efficiency trade-offs and b) a Selector Network that at test
time rapidly directs samples to the appropriate student to maintain both high
retrieval performance and high computational efficiency. We train several
students with different architectures and arrive at different trade-offs of
performance and efficiency, i.e., speed and storage requirements, including
fine-grained students that store/index videos using binary representations.
Importantly, the proposed scheme allows Knowledge Distillation in large,
unlabelled datasets – this leads to good students. We evaluate DnS on five
public datasets on three different video retrieval tasks and demonstrate a)
that our students achieve state-of-the-art performance in several cases and b)
that the DnS framework provides an excellent trade-off between retrieval
performance, computational speed, and storage space. In specific
configurations, the proposed method achieves similar mAP with the teacher but
is 20 times faster and requires 240 times less storage space. The collected
dataset and implementation are publicly available:
https://github.com/mever-team/distill-and-select.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/xiao2022progressively/">Progressively Optimized Bi-granular Document Representation For Scalable Embedding Based Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Progressively Optimized Bi-granular Document Representation For Scalable Embedding Based Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Progressively Optimized Bi-granular Document Representation For Scalable Embedding Based Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xiao et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the ACM Web Conference 2022</td>
    <td>13</td>
    <td><p>Ad-hoc search calls for the selection of appropriate answers from a
massive-scale corpus. Nowadays, the embedding-based retrieval (EBR) becomes a
promising solution, where deep learning based document representation and ANN
search techniques are allied to handle this task. However, a major challenge is
that the ANN index can be too large to fit into memory, given the considerable
size of answer corpus. In this work, we tackle this problem with Bi-Granular
Document Representation, where the lightweight sparse embeddings are indexed
and standby in memory for coarse-grained candidate search, and the heavyweight
dense embeddings are hosted in disk for fine-grained post verification. For the
best of retrieval accuracy, a Progressive Optimization framework is designed.
The sparse embeddings are learned ahead for high-quality search of candidates.
Conditioned on the candidate distribution induced by the sparse embeddings, the
dense embeddings are continuously learned to optimize the discrimination of
ground-truth from the shortlisted candidates. Besides, two techniques: the
contrastive quantization and the locality-centric sampling are introduced for
the learning of sparse and dense embeddings, which substantially contribute to
their performances. Thanks to the above features, our method effectively
handles massive-scale EBR with strong advantages in accuracy: with up to +4.3%
recall gain on million-scale corpus, and up to +17.5% recall gain on
billion-scale corpus. Besides, Our method is applied to a major sponsored
search platform with substantial gains on revenue (+1.95%), Recall (+1.01%) and
CTR (+0.49%). Our code is available at https://github.com/microsoft/BiDR.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/kim2022accelerating/">Accelerating Large-scale Graph-based Nearest Neighbor Search On A Computational Storage Platform</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Accelerating Large-scale Graph-based Nearest Neighbor Search On A Computational Storage Platform' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Accelerating Large-scale Graph-based Nearest Neighbor Search On A Computational Storage Platform' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kim et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Computers</td>
    <td>20</td>
    <td><p>K-nearest neighbor search is one of the fundamental tasks in various
applications and the hierarchical navigable small world (HNSW) has recently
drawn attention in large-scale cloud services, as it easily scales up the
database while offering fast search. On the other hand, a computational storage
device (CSD) that combines programmable logic and storage modules on a single
board becomes popular to address the data bandwidth bottleneck of modern
computing systems. In this paper, we propose a computational storage platform
that can accelerate a large-scale graph-based nearest neighbor search algorithm
based on SmartSSD CSD. To this end, we modify the algorithm more amenable on
the hardware and implement two types of accelerators using HLS- and RTL-based
methodology with various optimization methods. In addition, we scale up the
proposed platform to have 4 SmartSSDs and apply graph parallelism to boost the
system performance further. As a result, the proposed computational storage
platform achieves 75.59 query per second throughput for the SIFT1B dataset at
258.66W power dissipation, which is 12.83x and 17.91x faster and 10.43x and
24.33x more energy efficient than the conventional CPU-based and GPU-based
server platform, respectively. With multi-terabyte storage and custom
acceleration capability, we believe that the proposed computational storage
platform is a promising solution for cost-sensitive cloud datacenters.</p>
</td>
    <td>
      
        Similarity Search 
      
        Graph Based ANN 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/khaliq2022multires/">Multires-netvlad: Augmenting Place Recognition Training With Low-resolution Imagery</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multires-netvlad: Augmenting Place Recognition Training With Low-resolution Imagery' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multires-netvlad: Augmenting Place Recognition Training With Low-resolution Imagery' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Khaliq Ahmad, Milford Michael, Garg Sourav</td> <!-- 🔧 You were missing this -->
    <td>IEEE Robotics and Automation Letters</td>
    <td>37</td>
    <td><p>Visual Place Recognition (VPR) is a crucial component of 6-DoF localization,
visual SLAM and structure-from-motion pipelines, tasked to generate an initial
list of place match hypotheses by matching global place descriptors. However,
commonly-used CNN-based methods either process multiple image resolutions after
training or use a single resolution and limit multi-scale feature extraction to
the last convolutional layer during training. In this paper, we augment NetVLAD
representation learning with low-resolution image pyramid encoding which leads
to richer place representations. The resultant multi-resolution feature pyramid
can be conveniently aggregated through VLAD into a single compact
representation, avoiding the need for concatenation or summation of multiple
patches in recent multi-scale approaches. Furthermore, we show that the
underlying learnt feature tensor can be combined with existing multi-scale
approaches to improve their baseline performance. Evaluation on 15
viewpoint-varying and viewpoint-consistent benchmarking datasets confirm that
the proposed MultiRes-NetVLAD leads to state-of-the-art Recall@N performance
for global descriptor based retrieval, compared against 11 existing techniques.
Source code is publicly available at
https://github.com/Ahmedest61/MultiRes-NetVLAD.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/zhong2022evaluating/">Evaluating Token-level And Passage-level Dense Retrieval Models For Math Information Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Evaluating Token-level And Passage-level Dense Retrieval Models For Math Information Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Evaluating Token-level And Passage-level Dense Retrieval Models For Math Information Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhong et al.</td> <!-- 🔧 You were missing this -->
    <td>Findings of the Association for Computational Linguistics: EMNLP 2022</td>
    <td>12</td>
    <td><p>With the recent success of dense retrieval methods based on bi-encoders,
studies have applied this approach to various interesting downstream retrieval
tasks with good efficiency and in-domain effectiveness. Recently, we have also
seen the presence of dense retrieval models in Math Information Retrieval (MIR)
tasks, but the most effective systems remain classic retrieval methods that
consider hand-crafted structure features. In this work, we try to combine the
best of both worlds:\ a well-defined structure search method for effective
formula search and efficient bi-encoder dense retrieval models to capture
contextual similarities. Specifically, we have evaluated two representative
bi-encoder models for token-level and passage-level dense retrieval on recent
MIR tasks. Our results show that bi-encoder models are highly complementary to
existing structure search methods, and we are able to advance the
state-of-the-art on MIR datasets.</p>
</td>
    <td>
      
        ACL 
      
        EMNLP 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/jeong2022augmenting/">Augmenting Document Representations For Dense Retrieval With Interpolation And Perturbation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Augmenting Document Representations For Dense Retrieval With Interpolation And Perturbation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Augmenting Document Representations For Dense Retrieval With Interpolation And Perturbation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jeong et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</td>
    <td>7</td>
    <td><p>Dense retrieval models, which aim at retrieving the most relevant document
for an input query on a dense representation space, have gained considerable
attention for their remarkable success. Yet, dense models require a vast amount
of labeled training data for notable performance, whereas it is often
challenging to acquire query-document pairs annotated by humans. To tackle this
problem, we propose a simple but effective Document Augmentation for dense
Retrieval (DAR) framework, which augments the representations of documents with
their interpolation and perturbation. We validate the performance of DAR on
retrieval tasks with two benchmark datasets, showing that the proposed DAR
significantly outperforms relevant baselines on the dense retrieval of both the
labeled and unlabeled documents.</p>
</td>
    <td>
      
        ACL 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/jang2022deep/">Deep Hash Distillation For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Hash Distillation For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Hash Distillation For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jang et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>23</td>
    <td><p>In hash-based image retrieval systems, degraded or transformed inputs usually
generate different codes from the original, deteriorating the retrieval
accuracy. To mitigate this issue, data augmentation can be applied during
training. However, even if augmented samples of an image are similar in real
feature space, the quantization can scatter them far away in Hamming space.
This results in representation discrepancies that can impede training and
degrade performance. In this work, we propose a novel self-distilled hashing
scheme to minimize the discrepancy while exploiting the potential of augmented
data. By transferring the hash knowledge of the weakly-transformed samples to
the strong ones, we make the hash code insensitive to various transformations.
We also introduce hash proxy-based similarity learning and binary cross
entropy-based quantization loss to provide fine quality hash codes. Ultimately,
we construct a deep hashing framework that not only improves the existing deep
hashing approaches, but also achieves the state-of-the-art retrieval results.
Extensive experiments are conducted and confirm the effectiveness of our work.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/jaiswal2022ood/">Ood-diskann: Efficient And Scalable Graph ANNS For Out-of-distribution Queries</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Ood-diskann: Efficient And Scalable Graph ANNS For Out-of-distribution Queries' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Ood-diskann: Efficient And Scalable Graph ANNS For Out-of-distribution Queries' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jaiswal et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Knowledge and Data Engineering</td>
    <td>47</td>
    <td><p>State-of-the-art algorithms for Approximate Nearest Neighbor Search (ANNS)
such as DiskANN, FAISS-IVF, and HNSW build data dependent indices that offer
substantially better accuracy and search efficiency over data-agnostic indices
by overfitting to the index data distribution. When the query data is drawn
from a different distribution - e.g., when index represents image embeddings
and query represents textual embeddings - such algorithms lose much of this
performance advantage. On a variety of datasets, for a fixed recall target,
latency is worse by an order of magnitude or more for Out-Of-Distribution (OOD)
queries as compared to In-Distribution (ID) queries. The question we address in
this work is whether ANNS algorithms can be made efficient for OOD queries if
the index construction is given access to a small sample set of these queries.
We answer positively by presenting OOD-DiskANN, which uses a sparing sample (1%
of index set size) of OOD queries, and provides up to 40% improvement in mean
query latency over SoTA algorithms of a similar memory footprint. OOD-DiskANN
is scalable and has the efficiency of graph-based ANNS indices. Some of our
contributions can improve query efficiency for ID queries as well.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/jafari2022experimental/">Experimental Analysis Of Machine Learning Techniques For Finding Search Radius In Locality Sensitive Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Experimental Analysis Of Machine Learning Techniques For Finding Search Radius In Locality Sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Experimental Analysis Of Machine Learning Techniques For Finding Search Radius In Locality Sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jafari Omid, Nagarkar Parth</td> <!-- 🔧 You were missing this -->
    <td>International Journal of Fuzzy Logic and Intelligent Systems</td>
    <td>13</td>
    <td><p>Finding similar data in high-dimensional spaces is one of the important tasks
in multimedia applications. Approaches introduced to find exact searching
techniques often use tree-based index structures which are known to suffer from
the curse of the dimensionality problem that limits their performance.
Approximate searching techniques prefer performance over accuracy and they
return good enough results while achieving a better performance. Locality
Sensitive Hashing (LSH) is one of the most popular approximate nearest neighbor
search techniques for high-dimensional spaces. One of the most time-consuming
processes in LSH is to find the neighboring points in the projected spaces. An
improved LSH-based index structure, called radius-optimized Locality Sensitive
Hashing (roLSH) has been proposed to utilize Machine Learning and efficiently
find these neighboring points; thus, further improve the overall performance of
LSH. In this paper, we extend roLSH by experimentally studying the effect of
different types of famous Machine Learning techniques on overall performance.
We compare ten regression techniques on four real-world datasets and show that
Neural Network-based techniques are the best fit to be used in roLSH as their
accuracy and performance trade-off are the best compared to the other
techniques.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/hui2022efficient/">Efficient 3D Point Cloud Feature Learning For Large-scale Place Recognition</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient 3D Point Cloud Feature Learning For Large-scale Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient 3D Point Cloud Feature Learning For Large-scale Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hui et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>15</td>
    <td><p>Point cloud based retrieval for place recognition is still a challenging
problem due to drastic appearance and illumination changes of scenes in
changing environments. Existing deep learning based global descriptors for the
retrieval task usually consume a large amount of computation resources (e.g.,
memory), which may not be suitable for the cases of limited hardware resources.
In this paper, we develop an efficient point cloud learning network (EPC-Net)
to form a global descriptor for visual place recognition, which can obtain good
performance and reduce computation memory and inference time. First, we propose
a lightweight but effective neural network module, called ProxyConv, to
aggregate the local geometric features of point clouds. We leverage the spatial
adjacent matrix and proxy points to simplify the original edge convolution for
lower memory consumption. Then, we design a lightweight grouped VLAD network
(G-VLAD) to form global descriptors for retrieval. Compared with the original
VLAD network, we propose a grouped fully connected (GFC) layer to decompose the
high-dimensional vectors into a group of low-dimensional vectors, which can
reduce the number of parameters of the network and maintain the discrimination
of the feature vector. Finally, to further reduce the inference time, we
develop a simple version of EPC-Net, called EPC-Net-L, which consists of two
ProxyConv modules and one max pooling layer to aggregate global descriptors. By
distilling the knowledge from EPC-Net, EPC-Net-L can obtain discriminative
global descriptors for retrieval. Extensive experiments on the Oxford dataset
and three in-house datasets demonstrate that our proposed method can achieve
state-of-the-art performance with lower parameters, FLOPs, and runtime per
frame.</p>
</td>
    <td>
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/hu2022feature/">Feature Representation Learning For Unsupervised Cross-domain Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Feature Representation Learning For Unsupervised Cross-domain Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Feature Representation Learning For Unsupervised Cross-domain Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hu Conghui, Lee Gim Hee</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>12</td>
    <td><p>Current supervised cross-domain image retrieval methods can achieve excellent
performance. However, the cost of data collection and labeling imposes an
intractable barrier to practical deployment in real applications. In this
paper, we investigate the unsupervised cross-domain image retrieval task, where
class labels and pairing annotations are no longer a prerequisite for training.
This is an extremely challenging task because there is no supervision for both
in-domain feature representation learning and cross-domain alignment. We
address both challenges by introducing: 1) a new cluster-wise contrastive
learning mechanism to help extract class semantic-aware features, and 2) a
novel distance-of-distance loss to effectively measure and minimize the domain
discrepancy without any external supervision. Experiments on the Office-Home
and DomainNet datasets consistently show the superior image retrieval
accuracies of our framework over state-of-the-art approaches. Our source code
can be found at https://github.com/conghuihu/UCDIR.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Image Retrieval 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/hu2022badhash/">Badhash: Invisible Backdoor Attacks Against Deep Hashing With Clean Label</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Badhash: Invisible Backdoor Attacks Against Deep Hashing With Clean Label' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Badhash: Invisible Backdoor Attacks Against Deep Hashing With Clean Label' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 30th ACM International Conference on Multimedia</td>
    <td>26</td>
    <td><p>Due to its powerful feature learning capability and high efficiency, deep
hashing has achieved great success in large-scale image retrieval. Meanwhile,
extensive works have demonstrated that deep neural networks (DNNs) are
susceptible to adversarial examples, and exploring adversarial attack against
deep hashing has attracted many research efforts. Nevertheless, backdoor
attack, another famous threat to DNNs, has not been studied for deep hashing
yet. Although various backdoor attacks have been proposed in the field of image
classification, existing approaches failed to realize a truly imperceptive
backdoor attack that enjoys invisible triggers and clean label setting
simultaneously, and they also cannot meet the intrinsic demand of image
retrieval backdoor. In this paper, we propose BadHash, the first
generative-based imperceptible backdoor attack against deep hashing, which can
effectively generate invisible and input-specific poisoned images with clean
label. Specifically, we first propose a new conditional generative adversarial
network (cGAN) pipeline to effectively generate poisoned samples. For any given
benign image, it seeks to generate a natural-looking poisoned counterpart with
a unique invisible trigger. In order to improve the attack effectiveness, we
introduce a label-based contrastive learning network LabCLN to exploit the
semantic characteristics of different labels, which are subsequently used for
confusing and misleading the target model to learn the embedded trigger. We
finally explore the mechanism of backdoor attacks on image retrieval in the
hash space. Extensive experiments on multiple benchmark datasets verify that
BadHash can generate imperceptible poisoned samples with strong attack ability
and transferability over state-of-the-art deep hashing schemes.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/hu2022content/">Content-based Landmark Retrieval Combining Global And Local Features Using Siamese Neural Networks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Content-based Landmark Retrieval Combining Global And Local Features Using Siamese Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Content-based Landmark Retrieval Combining Global And Local Features Using Siamese Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hu et al.</td> <!-- 🔧 You were missing this -->
    <td>2016 2nd International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)</td>
    <td>14</td>
    <td><p>In this work, we present a method for landmark retrieval that utilizes global
and local features. A Siamese network is used for global feature extraction and
metric learning, which gives an initial ranking of the landmark search. We
utilize the extracted feature maps from the Siamese architecture as local
descriptors, the search results are then further refined using a cosine
similarity between local descriptors. We conduct a deeper analysis of the
Google Landmark Dataset, which is used for evaluation, and augment the dataset
to handle various intra-class variances. Furthermore, we conduct several
experiments to compare the effects of transfer learning and metric learning, as
well as experiments using other local descriptors. We show that a re-ranking
using local features can improve the search results. We believe that the
proposed local feature extraction using cosine similarity is a simple approach
that can be extended to many other retrieval tasks.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/hendriksen2022extending/">Extending CLIP For Category-to-image Retrieval In E-commerce</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Extending CLIP For Category-to-image Retrieval In E-commerce' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Extending CLIP For Category-to-image Retrieval In E-commerce' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hendriksen et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>14</td>
    <td><p>E-commerce provides rich multimodal data that is barely leveraged in
practice. One aspect of this data is a category tree that is being used in
search and recommendation. However, in practice, during a user’s session there
is often a mismatch between a textual and a visual representation of a given
category. Motivated by the problem, we introduce the task of category-to-image
retrieval in e-commerce and propose a model for the task, CLIP-ITA. The model
leverages information from multiple modalities (textual, visual, and attribute
modality) to create product representations. We explore how adding information
from multiple modalities (textual, visual, and attribute modality) impacts the
model’s performance. In particular, we observe that CLIP-ITA significantly
outperforms a comparable model that leverages only the visual modality and a
comparable model that leverages the visual and attribute modality.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/zhang2022lightfr/">Lightfr: Lightweight Federated Recommendation With Privacy-preserving Matrix Factorization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Lightfr: Lightweight Federated Recommendation With Privacy-preserving Matrix Factorization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Lightfr: Lightweight Federated Recommendation With Privacy-preserving Matrix Factorization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang et al.</td> <!-- 🔧 You were missing this -->
    <td>ACM Transactions on Information Systems</td>
    <td>43</td>
    <td><p>Federated recommender system (FRS), which enables many local devices to train
a shared model jointly without transmitting local raw data, has become a
prevalent recommendation paradigm with privacy-preserving advantages. However,
previous work on FRS performs similarity search via inner product in continuous
embedding space, which causes an efficiency bottleneck when the scale of items
is extremely large. We argue that such a scheme in federated settings ignores
the limited capacities in resource-constrained user devices (i.e., storage
space, computational overhead, and communication bandwidth), and makes it
harder to be deployed in large-scale recommender systems. Besides, it has been
shown that transmitting local gradients in real-valued form between server and
clients may leak users’ private information. To this end, we propose a
lightweight federated recommendation framework with privacy-preserving matrix
factorization, LightFR, that is able to generate high-quality binary codes by
exploiting learning to hash technique under federated settings, and thus enjoys
both fast online inference and economic memory consumption. Moreover, we devise
an efficient federated discrete optimization algorithm to collaboratively train
model parameters between the server and clients, which can effectively prevent
real-valued gradient attacks from malicious parties. Through extensive
experiments on four real-world datasets, we show that our LightFR model
outperforms several state-of-the-art FRS methods in terms of recommendation
accuracy, inference efficiency and data privacy.</p>
</td>
    <td>
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/zhu2022lower/">A Lower Bound Of Hash Codes' Performance</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Lower Bound Of Hash Codes' Performance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Lower Bound Of Hash Codes' Performance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of IEEE International Symposium on Information Theory</td>
    <td>9</td>
    <td><p>As a crucial approach for compact representation learning, hashing has
achieved great success in effectiveness and efficiency. Numerous heuristic
Hamming space metric learning objectives are designed to obtain high-quality
hash codes. Nevertheless, a theoretical analysis of criteria for learning good
hash codes remains largely unexploited. In this paper, we prove that
inter-class distinctiveness and intra-class compactness among hash codes
determine the lower bound of hash codes’ performance. Promoting these two
characteristics could lift the bound and improve hash learning. We then propose
a surrogate model to fully exploit the above objective by estimating the
posterior of hash codes and controlling it, which results in a low-bias
optimization. Extensive experiments reveal the effectiveness of the proposed
method. By testing on a series of hash-models, we obtain performance
improvements among all of them, with an up to \(26.5%\) increase in mean Average
Precision and an up to \(20.5%\) increase in accuracy. Our code is publicly
available at https://github.com/VL-Group/LBHash.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/gupta2022faircop/">Faircop: Facial Image Retrieval Using Contrastive Personalization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Faircop: Facial Image Retrieval Using Contrastive Personalization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Faircop: Facial Image Retrieval Using Contrastive Personalization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gupta et al.</td> <!-- 🔧 You were missing this -->
    <td>Digital Signal Processing</td>
    <td>18</td>
    <td><p>Retrieving facial images from attributes plays a vital role in various
systems such as face recognition and suspect identification. Compared to other
image retrieval tasks, facial image retrieval is more challenging due to the
high subjectivity involved in describing a person’s facial features. Existing
methods do so by comparing specific characteristics from the user’s mental
image against the suggested images via high-level supervision such as using
natural language. In contrast, we propose a method that uses a relatively
simpler form of binary supervision by utilizing the user’s feedback to label
images as either similar or dissimilar to the target image. Such supervision
enables us to exploit the contrastive learning paradigm for encapsulating each
user’s personalized notion of similarity. For this, we propose a novel loss
function optimized online via user feedback. We validate the efficacy of our
proposed approach using a carefully designed testbed to simulate user feedback
and a large-scale user study. Our experiments demonstrate that our method
iteratively improves personalization, leading to faster convergence and
enhanced recommendation relevance, thereby, improving user satisfaction. Our
proposed framework is also equipped with a user-friendly web interface with a
real-time experience for facial image retrieval.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/gupta2022zero/">Zero-shot Sketch Based Image Retrieval Using Graph Transformer</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Zero-shot Sketch Based Image Retrieval Using Graph Transformer' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Zero-shot Sketch Based Image Retrieval Using Graph Transformer' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gupta Sumrit, Chaudhuri Ushasi, Banerjee Biplab</td> <!-- 🔧 You were missing this -->
    <td>2022 26th International Conference on Pattern Recognition (ICPR)</td>
    <td>7</td>
    <td><p>The performance of a zero-shot sketch-based image retrieval (ZS-SBIR) task is
primarily affected by two challenges. The substantial domain gap between image
and sketch features needs to be bridged, while at the same time the side
information has to be chosen tactfully. Existing literature has shown that
varying the semantic side information greatly affects the performance of
ZS-SBIR. To this end, we propose a novel graph transformer based zero-shot
sketch-based image retrieval (GTZSR) framework for solving ZS-SBIR tasks which
uses a novel graph transformer to preserve the topology of the classes in the
semantic space and propagates the context-graph of the classes within the
embedding features of the visual space. To bridge the domain gap between the
visual features, we propose minimizing the Wasserstein distance between images
and sketches in a learned domain-shared space. We also propose a novel
compatibility loss that further aligns the two visual domains by bridging the
domain gap of one class with respect to the domain gap of all other classes in
the training set. Experimental results obtained on the extended Sketchy,
TU-Berlin, and QuickDraw datasets exhibit sharp improvements over the existing
state-of-the-art methods in both ZS-SBIR and generalized ZS-SBIR.</p>
</td>
    <td>
      
        Image Retrieval 
      
        Few Shot & Zero Shot 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/gui2022cross/">Cross-language Binary-source Code Matching With Intermediate Representations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-language Binary-source Code Matching With Intermediate Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-language Binary-source Code Matching With Intermediate Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gui et al.</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)</td>
    <td>21</td>
    <td><p>Binary-source code matching plays an important role in many security and
software engineering related tasks such as malware detection, reverse
engineering and vulnerability assessment. Currently, several approaches have
been proposed for binary-source code matching by jointly learning the
embeddings of binary code and source code in a common vector space. Despite
much effort, existing approaches target on matching the binary code and source
code written in a single programming language. However, in practice, software
applications are often written in different programming languages to cater for
different requirements and computing platforms. Matching binary and source code
across programming languages introduces additional challenges when maintaining
multi-language and multi-platform applications. To this end, this paper
formulates the problem of cross-language binary-source code matching, and
develops a new dataset for this new problem. We present a novel approach XLIR,
which is a Transformer-based neural network by learning the intermediate
representations for both binary and source code. To validate the effectiveness
of XLIR, comprehensive experiments are conducted on two tasks of cross-language
binary-source code matching, and cross-language source-source code matching, on
top of our curated dataset. Experimental results and analysis show that our
proposed XLIR with intermediate representations significantly outperforms other
state-of-the-art models in both of the two tasks.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/gu2022cross/">Cross-modal Image Retrieval With Deep Mutual Information Maximization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-modal Image Retrieval With Deep Mutual Information Maximization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-modal Image Retrieval With Deep Mutual Information Maximization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gu et al.</td> <!-- 🔧 You were missing this -->
    <td>Neurocomputing</td>
    <td>13</td>
    <td><p>In this paper, we study the cross-modal image retrieval, where the inputs
contain a source image plus some text that describes certain modifications to
this image and the desired image. Prior work usually uses a three-stage
strategy to tackle this task: 1) extract the features of the inputs; 2) fuse
the feature of the source image and its modified text to obtain fusion feature;
3) learn a similarity metric between the desired image and the source image +
modified text by using deep metric learning. Since classical image/text
encoders can learn the useful representation and common pair-based loss
functions of distance metric learning are enough for cross-modal retrieval,
people usually improve retrieval accuracy by designing new fusion networks.
However, these methods do not successfully handle the modality gap caused by
the inconsistent distribution and representation of the features of different
modalities, which greatly influences the feature fusion and similarity
learning. To alleviate this problem, we adopt the contrastive self-supervised
learning method Deep InforMax (DIM) to our approach to bridge this gap by
enhancing the dependence between the text, the image, and their fusion.
Specifically, our method narrows the modality gap between the text modality and
the image modality by maximizing mutual information between their not exactly
semantically identical representation. Moreover, we seek an effective common
subspace for the semantically same fusion feature and desired image’s feature
by utilizing Deep InforMax between the low-level layer of the image encoder and
the high-level layer of the fusion network. Extensive experiments on three
large-scale benchmark datasets show that we have bridged the modality gap
between different modalities and achieve state-of-the-art retrieval
performance.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/gu2022accelerating/">Accelerating Code Search With Deep Hashing And Code Classification</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Accelerating Code Search With Deep Hashing And Code Classification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Accelerating Code Search With Deep Hashing And Code Classification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</td>
    <td>9</td>
    <td><p>Code search is to search reusable code snippets from source code corpus based
on natural languages queries. Deep learning-based methods of code search have
shown promising results. However, previous methods focus on retrieval accuracy
but lacked attention to the efficiency of the retrieval process. We propose a
novel method CoSHC to accelerate code search with deep hashing and code
classification, aiming to perform an efficient code search without sacrificing
too much accuracy. To evaluate the effectiveness of CoSHC, we apply our method
to five code search models. Extensive experimental results indicate that
compared with previous code search baselines, CoSHC can save more than 90% of
retrieval time meanwhile preserving at least 99% of retrieval accuracy.</p>
</td>
    <td>
      
        ACL 
      
        Hashing Methods 
      
        Neural Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/gu2022local/">Local Citation Recommendation With Hierarchical-attention Text Encoder And Scibert-based Reranking</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Local Citation Recommendation With Hierarchical-attention Text Encoder And Scibert-based Reranking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Local Citation Recommendation With Hierarchical-attention Text Encoder And Scibert-based Reranking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gu Nianlong, Gao Yingqiang, Hahnloser Richard H. R.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>14</td>
    <td><p>The goal of local citation recommendation is to recommend a missing reference
from the local citation context and optionally also from the global context. To
balance the tradeoff between speed and accuracy of citation recommendation in
the context of a large-scale paper database, a viable approach is to first
prefetch a limited number of relevant documents using efficient ranking methods
and then to perform a fine-grained reranking using more sophisticated models.
In that vein, BM25 has been found to be a tough-to-beat approach to
prefetching, which is why recent work has focused mainly on the reranking step.
Even so, we explore prefetching with nearest neighbor search among text
embeddings constructed by a hierarchical attention network. When coupled with a
SciBERT reranker fine-tuned on local citation recommendation tasks, our
hierarchical Attention encoder (HAtten) achieves high prefetch recall for a
given number of candidates to be reranked. Consequently, our reranker requires
fewer prefetch candidates to rerank, yet still achieves state-of-the-art
performance on various local citation recommendation datasets such as ACL-200,
FullTextPeerRead, RefSeer, and arXiv.</p>
</td>
    <td>
      
        Hybrid ANN Methods 
      
        Recommender Systems 
      
        Re RANKING 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/groh2022ggnn/">GGNN: Graph-based GPU Nearest Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=GGNN: Graph-based GPU Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=GGNN: Graph-based GPU Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Groh et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Big Data</td>
    <td>33</td>
    <td><p>Approximate nearest neighbor (ANN) search in high dimensions is an integral
part of several computer vision systems and gains importance in deep learning
with explicit memory representations. Since PQT, FAISS, and SONG started to
leverage the massive parallelism offered by GPUs, GPU-based implementations are
a crucial resource for today’s state-of-the-art ANN methods. While most of
these methods allow for faster queries, less emphasis is devoted to
accelerating the construction of the underlying index structures. In this
paper, we propose a novel GPU-friendly search structure based on nearest
neighbor graphs and information propagation on graphs. Our method is designed
to take advantage of GPU architectures to accelerate the hierarchical
construction of the index structure and for performing the query. Empirical
evaluation shows that GGNN significantly surpasses the state-of-the-art CPU-
and GPU-based systems in terms of build-time, accuracy and search speed.</p>
</td>
    <td>
      
        Similarity Search 
      
        Graph Based ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/grosz2022minutiae/">Minutiae-guided Fingerprint Embeddings Via Vision Transformers</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Minutiae-guided Fingerprint Embeddings Via Vision Transformers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Minutiae-guided Fingerprint Embeddings Via Vision Transformers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Grosz et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>9</td>
    <td><p>Minutiae matching has long dominated the field of fingerprint recognition.
However, deep networks can be used to extract fixed-length embeddings from
fingerprints. To date, the few studies that have explored the use of CNN
architectures to extract such embeddings have shown extreme promise. Inspired
by these early works, we propose the first use of a Vision Transformer (ViT) to
learn a discriminative fixed-length fingerprint embedding. We further
demonstrate that by guiding the ViT to focus in on local, minutiae related
features, we can boost the recognition performance. Finally, we show that by
fusing embeddings learned by CNNs and ViTs we can reach near parity with a
commercial state-of-the-art (SOTA) matcher. In particular, we obtain a
TAR=94.23% @ FAR=0.1% on the NIST SD 302 public-domain dataset, compared to a
SOTA commercial matcher which obtains TAR=96.71% @ FAR=0.1%. Additionally, our
fixed-length embeddings can be matched orders of magnitude faster than the
commercial system (2.5 million matches/second compared to 50K matches/second).
We make our code and models publicly available to encourage further research on
this topic: https://github.com/tba.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/gong2022vit2hash/">Vit2hash: Unsupervised Information-preserving Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Vit2hash: Unsupervised Information-preserving Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Vit2hash: Unsupervised Information-preserving Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gong et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 24th ACM international conference on Multimedia</td>
    <td>10</td>
    <td><p>Unsupervised image hashing, which maps images into binary codes without
supervision, is a compressor with a high compression rate. Hence, how to
preserving meaningful information of the original data is a critical problem.
Inspired by the large-scale vision pre-training model, known as ViT, which has
shown significant progress for learning visual representations, in this paper,
we propose a simple information-preserving compressor to finetune the ViT model
for the target unsupervised hashing task. Specifically, from pixels to
continuous features, we first propose a feature-preserving module, using the
corrupted image as input to reconstruct the original feature from the
pre-trained ViT model and the complete image, so that the feature extractor can
focus on preserving the meaningful information of original data. Secondly, from
continuous features to hash codes, we propose a hashing-preserving module,
which aims to keep the semantic information from the pre-trained ViT model by
using the proposed Kullback-Leibler divergence loss. Besides, the quantization
loss and the similarity loss are added to minimize the quantization error. Our
method is very simple and achieves a significantly higher degree of MAP on
three benchmark image datasets.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/gong2022improving/">Improving Visual-semantic Embeddings By Learning Semantically-enhanced Hard Negatives For Cross-modal Information Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Improving Visual-semantic Embeddings By Learning Semantically-enhanced Hard Negatives For Cross-modal Information Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Improving Visual-semantic Embeddings By Learning Semantically-enhanced Hard Negatives For Cross-modal Information Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gong Yan, Cosma Georgina</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition</td>
    <td>13</td>
    <td><p>Visual Semantic Embedding (VSE) aims to extract the semantics of images and
their descriptions, and embed them into the same latent space for cross-modal
information retrieval. Most existing VSE networks are trained by adopting a
hard negatives loss function which learns an objective margin between the
similarity of relevant and irrelevant image-description embedding pairs.
However, the objective margin in the hard negatives loss function is set as a
fixed hyperparameter that ignores the semantic differences of the irrelevant
image-description pairs. To address the challenge of measuring the optimal
similarities between image-description pairs before obtaining the trained VSE
networks, this paper presents a novel approach that comprises two main parts:
(1) finds the underlying semantics of image descriptions; and (2) proposes a
novel semantically enhanced hard negatives loss function, where the learning
objective is dynamically determined based on the optimal similarity scores
between irrelevant image-description pairs. Extensive experiments were carried
out by integrating the proposed methods into five state-of-the-art VSE networks
that were applied to three benchmark datasets for cross-modal information
retrieval tasks. The results revealed that the proposed methods achieved the
best performance and can also be adopted by existing and future VSE networks.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/ghaemmaghami2022learning/">Learning To Collide: Recommendation System Model Compression With Learned Hash Functions</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning To Collide: Recommendation System Model Compression With Learned Hash Functions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning To Collide: Recommendation System Model Compression With Learned Hash Functions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ghaemmaghami et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the VLDB Endowment</td>
    <td>6</td>
    <td><p>A key characteristic of deep recommendation models is the immense memory
requirements of their embedding tables. These embedding tables can often reach
hundreds of gigabytes which increases hardware requirements and training cost.
A common technique to reduce model size is to hash all of the categorical
variable identifiers (ids) into a smaller space. This hashing reduces the
number of unique representations that must be stored in the embedding table;
thus decreasing its size. However, this approach introduces collisions between
semantically dissimilar ids that degrade model quality. We introduce an
alternative approach, Learned Hash Functions, which instead learns a new
mapping function that encourages collisions between semantically similar ids.
We derive this learned mapping from historical data and embedding access
patterns. We experiment with this technique on a production model and find that
a mapping informed by the combination of access frequency and a learned low
dimension embedding is the most effective. We demonstrate a small improvement
relative to the hashing trick and other collision related compression
techniques. This is ongoing work that explores the impact of categorical id
collisions on recommendation model quality and how those collisions may be
controlled to improve model performance.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/godi2022movingfashion/">Movingfashion: A Benchmark For The Video-to-shop Challenge</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Movingfashion: A Benchmark For The Video-to-shop Challenge' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Movingfashion: A Benchmark For The Video-to-shop Challenge' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Godi et al.</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>12</td>
    <td><p>Retrieving clothes which are worn in social media videos (Instagram, TikTok)
is the latest frontier of e-fashion, referred to as “video-to-shop” in the
computer vision literature. In this paper we present MovingFashion, the first
publicly available dataset to cope with this challenge. MovingFashion is
composed of 14855 social videos, each one of them associated to e-commerce
“shop” images where the corresponding clothing items are clearly portrayed. In
addition, we present a network for retrieving the shop images in this scenario,
dubbed SEAM Match-RCNN. The model is trained by image-to-video domain
adaptation, allowing to use video sequences where only their association with a
shop image is given, eliminating the need of millions of annotated bounding
boxes. SEAM Match-RCNN builds an embedding, where an attention-based weighted
sum of few frames (10) of a social video is enough to individuate the correct
product within the first 5 retrieved items in a 14K+ shop element gallery with
an accuracy of 80%. This provides the best performance on MovingFashion,
comparing exhaustively against the related state-of-the-art approaches and
alternative baselines.</p>
</td>
    <td>
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/gao2022clusterea/">Clusterea: Scalable Entity Alignment With Stochastic Training And Normalized Mini-batch Similarities</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Clusterea: Scalable Entity Alignment With Stochastic Training And Normalized Mini-batch Similarities' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Clusterea: Scalable Entity Alignment With Stochastic Training And Normalized Mini-batch Similarities' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gao et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</td>
    <td>29</td>
    <td><p>Entity alignment (EA) aims at finding equivalent entities in different
knowledge graphs (KGs). Embedding-based approaches have dominated the EA task
in recent years. Those methods face problems that come from the geometric
properties of embedding vectors, including hubness and isolation. To solve
these geometric problems, many normalization approaches have been adopted for
EA. However, the increasing scale of KGs renders it hard for EA models to adopt
the normalization processes, thus limiting their usage in real-world
applications. To tackle this challenge, we present ClusterEA, a general
framework that is capable of scaling up EA models and enhancing their results
by leveraging normalization methods on mini-batches with a high entity
equivalent rate. ClusterEA contains three components to align entities between
large-scale KGs, including stochastic training, ClusterSampler, and
SparseFusion. It first trains a large-scale Siamese GNN for EA in a stochastic
fashion to produce entity embeddings. Based on the embeddings, a novel
ClusterSampler strategy is proposed for sampling highly overlapped
mini-batches. Finally, ClusterEA incorporates SparseFusion, which normalizes
local and global similarity and then fuses all similarity matrices to obtain
the final similarity matrix. Extensive experiments with real-life datasets on
EA benchmarks offer insight into the proposed framework, and suggest that it is
capable of outperforming the state-of-the-art scalable EA framework by up to 8
times in terms of Hits@1.</p>
</td>
    <td>
      
        KDD 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/zhang2022hashing/">Hashing Learning With Hyper-class Representation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing Learning With Hyper-class Representation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing Learning With Hyper-class Representation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Shichao, Li Jiaye</td> <!-- 🔧 You were missing this -->
    <td>Neurocomputing</td>
    <td>26</td>
    <td><p>Existing unsupervised hash learning is a kind of attribute-centered
calculation. It may not accurately preserve the similarity between data. This
leads to low down the performance of hash function learning. In this paper, a
hash algorithm is proposed with a hyper-class representation. It is a two-steps
approach. The first step finds potential decision features and establish
hyper-class. The second step constructs hash learning based on the hyper-class
information in the first step, so that the hash codes of the data within the
hyper-class are as similar as possible, as well as the hash codes of the data
between the hyper-classes are as different as possible. To evaluate the
efficiency, a series of experiments are conducted on four public datasets. The
experimental results show that the proposed hash algorithm is more efficient
than the compared algorithms, in terms of mean average precision (MAP), average
precision (AP) and Hamming radius 2 (HAM2)</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/zhang2022discriminative/">Discriminative Supervised Subspace Learning For Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Discriminative Supervised Subspace Learning For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Discriminative Supervised Subspace Learning For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>56</td>
    <td><p>Nowadays the measure between heterogeneous data is still an open problem for
cross-modal retrieval. The core of cross-modal retrieval is how to measure the
similarity between different types of data. Many approaches have been developed
to solve the problem. As one of the mainstream, approaches based on subspace
learning pay attention to learning a common subspace where the similarity among
multi-modal data can be measured directly. However, many of the existing
approaches only focus on learning a latent subspace. They ignore the full use
of discriminative information so that the semantically structural information
is not well preserved. Therefore satisfactory results can not be achieved as
expected. We in this paper propose a discriminative supervised subspace
learning for cross-modal retrieval(DS2L), to make full use of discriminative
information and better preserve the semantically structural information.
Specifically, we first construct a shared semantic graph to preserve the
semantic structure within each modality. Subsequently, the Hilbert-Schmidt
Independence Criterion(HSIC) is introduced to preserve the consistence between
feature-similarity and semantic-similarity of samples. Thirdly, we introduce a
similarity preservation term, thus our model can compensate for the
shortcomings of insufficient use of discriminative data and better preserve the
semantically structural information within each modality. The experimental
results obtained on three well-known benchmark datasets demonstrate the
effectiveness and competitiveness of the proposed method against the compared
classic subspace learning approaches.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/zhao2022feature/">A Feature Consistency Driven Attention Erasing Network For Fine-grained Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Feature Consistency Driven Attention Erasing Network For Fine-grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Feature Consistency Driven Attention Erasing Network For Fine-grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhao et al.</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition</td>
    <td>20</td>
    <td><p>Large-scale fine-grained image retrieval has two main problems. First, low
dimensional feature embedding can fasten the retrieval process but bring
accuracy reduce due to overlooking the feature of significant attention regions
of images in fine-grained datasets. Second, fine-grained images lead to the
same category query hash codes mapping into the different cluster in database
hash latent space. To handle these two issues, we propose a feature consistency
driven attention erasing network (FCAENet) for fine-grained image retrieval.
For the first issue, we propose an adaptive augmentation module in FCAENet,
which is selective region erasing module (SREM). SREM makes the network more
robust on subtle differences of fine-grained task by adaptively covering some
regions of raw images. The feature extractor and hash layer can learn more
representative hash code for fine-grained images by SREM. With regard to the
second issue, we fully exploit the pair-wise similarity information and add the
enhancing space relation loss (ESRL) in FCAENet to make the vulnerable relation
stabler between the query hash code and database hash code. We conduct
extensive experiments on five fine-grained benchmark datasets (CUB2011,
Aircraft, NABirds, VegFru, Food101) for 12bits, 24bits, 32bits, 48bits hash
code. The results show that FCAENet achieves the state-of-the-art (SOTA)
fine-grained retrieval performance compared with other methods.</p>
</td>
    <td>
      
        Image Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/fan2022constructing/">Constructing Phrase-level Semantic Labels To Form Multi-grained Supervision For Image-text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Constructing Phrase-level Semantic Labels To Form Multi-grained Supervision For Image-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Constructing Phrase-level Semantic Labels To Form Multi-grained Supervision For Image-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fan et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2022 International Conference on Multimedia Retrieval</td>
    <td>10</td>
    <td><p>Existing research for image text retrieval mainly relies on sentence-level
supervision to distinguish matched and mismatched sentences for a query image.
However, semantic mismatch between an image and sentences usually happens in
finer grain, i.e., phrase level. In this paper, we explore to introduce
additional phrase-level supervision for the better identification of mismatched
units in the text. In practice, multi-grained semantic labels are automatically
constructed for a query image in both sentence-level and phrase-level. We
construct text scene graphs for the matched sentences and extract entities and
triples as the phrase-level labels. In order to integrate both supervision of
sentence-level and phrase-level, we propose Semantic Structure Aware Multimodal
Transformer (SSAMT) for multi-modal representation learning. Inside the SSAMT,
we utilize different kinds of attention mechanisms to enforce interactions of
multi-grain semantic units in both sides of vision and language. For the
training, we propose multi-scale matching losses from both global and local
perspectives, and penalize mismatched phrases. Experimental results on MS-COCO
and Flickr30K show the effectiveness of our approach compared to some
state-of-the-art models.</p>
</td>
    <td>
      
        Multimodal Retrieval 
      
        Text Retrieval 
      
        Medical Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/engelsma2022hers/">HERS: Homomorphically Encrypted Representation Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=HERS: Homomorphically Encrypted Representation Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=HERS: Homomorphically Encrypted Representation Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Engelsma Joshua J., Jain Anil K., Boddeti Vishnu Naresh</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Biometrics, Behavior, and Identity Science</td>
    <td>34</td>
    <td><p>We present a method to search for a probe (or query) image representation
against a large gallery in the encrypted domain. We require that the probe and
gallery images be represented in terms of a fixed-length representation, which
is typical for representations obtained from learned networks. Our encryption
scheme is agnostic to how the fixed-length representation is obtained and can
therefore be applied to any fixed-length representation in any application
domain. Our method, dubbed HERS (Homomorphically Encrypted Representation
Search), operates by (i) compressing the representation towards its estimated
intrinsic dimensionality with minimal loss of accuracy (ii) encrypting the
compressed representation using the proposed fully homomorphic encryption
scheme, and (iii) efficiently searching against a gallery of encrypted
representations directly in the encrypted domain, without decrypting them.
Numerical results on large galleries of face, fingerprint, and object datasets
such as ImageNet show that, for the first time, accurate and fast image search
within the encrypted domain is feasible at scale (500 seconds; \(275\times\)
speed up over state-of-the-art for encrypted search against a gallery of 100
million). Code is available at
https://github.com/human-analysis/hers-encrypted-image-search</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/fahim2022unsupervised/">Unsupervised Space Partitioning For Nearest Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Space Partitioning For Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Space Partitioning For Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fahim Abrar, Ali Mohammed Eunus, Cheema Muhammad Aamir</td> <!-- 🔧 You were missing this -->
    <td>2021 11th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)</td>
    <td>8</td>
    <td><p>Approximate Nearest Neighbor Search (ANNS) in high dimensional spaces is
crucial for many real-life applications (e.g., e-commerce, web, multimedia,
etc.) dealing with an abundance of data. This paper proposes an end-to-end
learning framework that couples the partitioning (one critical step of ANNS)
and learning-to-search steps using a custom loss function. A key advantage of
our proposed solution is that it does not require any expensive pre-processing
of the dataset, which is one of the critical limitations of the
state-of-the-art approach. We achieve the above edge by formulating a
multi-objective custom loss function that does not need ground truth labels to
quantify the quality of a given data-space partition, making it entirely
unsupervised. We also propose an ensembling technique by adding varying input
weights to the loss function to train an ensemble of models to enhance the
search quality. On several standard benchmarks for ANNS, we show that our
method beats the state-of-the-art space partitioning method and the ubiquitous
K-means clustering method while using fewer parameters and shorter offline
training times. We also show that incorporating our space-partitioning strategy
into state-of-the-art ANNS techniques such as ScaNN can improve their
performance significantly. Finally, we present our unsupervised partitioning
approach as a promising alternative to many widely used clustering methods,
such as K-means clustering and DBSCAN.</p>
</td>
    <td>
      
        Similarity Search 
      
        SUPERVISED 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/dubey2022vision/">Vision Transformer Hashing For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Vision Transformer Hashing For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Vision Transformer Hashing For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dubey Shiv Ram, Singh Satish Kumar, Chu Wei-ta</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE International Conference on Multimedia and Expo (ICME)</td>
    <td>47</td>
    <td><p>Deep learning has shown a tremendous growth in hashing techniques for image
retrieval. Recently, Transformer has emerged as a new architecture by utilizing
self-attention without convolution. Transformer is also extended to Vision
Transformer (ViT) for the visual recognition with a promising performance on
ImageNet. In this paper, we propose a Vision Transformer based Hashing (VTS)
for image retrieval. We utilize the pre-trained ViT on ImageNet as the backbone
network and add the hashing head. The proposed VTS model is fine tuned for
hashing under six different image retrieval frameworks, including Deep
Supervised Hashing (DSH), HashNet, GreedyHash, Improved Deep Hashing Network
(IDHN), Deep Polarized Network (DPN) and Central Similarity Quantization (CSQ)
with their objective functions. We perform the extensive experiments on
CIFAR10, ImageNet, NUS-Wide, and COCO datasets. The proposed VTS based image
retrieval outperforms the recent state-of-the-art hashing techniques with a
great margin. We also find the proposed VTS model as the backbone network is
better than the existing networks, such as AlexNet and ResNet. The code is
released at https://github.com/shivram1987/VisionTransformerHashing.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/zhou2022moving/">Moving Towards Centers: Re-ranking With Attention And Memory For Re-identification</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Moving Towards Centers: Re-ranking With Attention And Memory For Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Moving Towards Centers: Re-ranking With Attention And Memory For Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhou Yunhao, Wang Yi, Chau Lap-pui</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>8</td>
    <td><p>Re-ranking utilizes contextual information to optimize the initial ranking
list of person or vehicle re-identification (re-ID), which boosts the retrieval
performance at post-processing steps. This paper proposes a re-ranking network
to predict the correlations between the probe and top-ranked neighbor samples.
Specifically, all the feature embeddings of query and gallery images are
expanded and enhanced by a linear combination of their neighbors, with the
correlation prediction serving as discriminative combination weights. The
combination process is equivalent to moving independent embeddings toward the
identity centers, improving cluster compactness. For correlation prediction, we
first aggregate the contextual information for probe’s k-nearest neighbors via
the Transformer encoder. Then, we distill and refine the probe-related features
into the Contextual Memory cell via attention mechanism. Like humans that
retrieve images by not only considering probe images but also memorizing the
retrieved ones, the Contextual Memory produces multi-view descriptions for each
instance. Finally, the neighbors are reconstructed with features fetched from
the Contextual Memory, and a binary classifier predicts their correlations with
the probe. Experiments on six widely-used person and vehicle re-ID benchmarks
demonstrate the effectiveness of the proposed method. Especially, our method
surpasses the state-of-the-art re-ranking approaches on large-scale datasets by
a significant margin, i.e., with an average 4.83% CMC@1 and 14.83% mAP
improvements on VERI-Wild, MSMT17, and VehicleID datasets.</p>
</td>
    <td>
      
        Hybrid ANN Methods 
      
        Re RANKING 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/zhang2022vldeformer/">Vldeformer: Vision-language Decomposed Transformer For Fast Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Vldeformer: Vision-language Decomposed Transformer For Fast Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Vldeformer: Vision-language Decomposed Transformer For Fast Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang et al.</td> <!-- 🔧 You were missing this -->
    <td>Knowledge-Based Systems</td>
    <td>18</td>
    <td><p>Cross-model retrieval has emerged as one of the most important upgrades for
text-only search engines (SE). Recently, with powerful representation for
pairwise text-image inputs via early interaction, the accuracy of
vision-language (VL) transformers has outperformed existing methods for
text-image retrieval. However, when the same paradigm is used for inference,
the efficiency of the VL transformers is still too low to be applied in a real
cross-modal SE. Inspired by the mechanism of human learning and using
cross-modal knowledge, this paper presents a novel Vision-Language Decomposed
Transformer (VLDeformer), which greatly increases the efficiency of VL
transformers while maintaining their outstanding accuracy. By the proposed
method, the cross-model retrieval is separated into two stages: the VL
transformer learning stage, and the VL decomposition stage. The latter stage
plays the role of single modal indexing, which is to some extent like the term
indexing of a text SE. The model learns cross-modal knowledge from
early-interaction pre-training and is then decomposed into an individual
encoder. The decomposition requires only small target datasets for supervision
and achieves both \(1000+\) times acceleration and less than \(0.6\)% average
recall drop. VLDeformer also outperforms state-of-the-art visual-semantic
embedding methods on COCO and Flickr30k.</p>
</td>
    <td>
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/doan2022coophash/">Coophash: Cooperative Learning Of Multipurpose Descriptor And Contrastive Pair Generator Via Variational MCMC Teaching For Supervised Image Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Coophash: Cooperative Learning Of Multipurpose Descriptor And Contrastive Pair Generator Via Variational MCMC Teaching For Supervised Image Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Coophash: Cooperative Learning Of Multipurpose Descriptor And Contrastive Pair Generator Via Variational MCMC Teaching For Supervised Image Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Doan et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>397</td>
    <td><p>Leveraging supervised information can lead to superior retrieval performance
in the image hashing domain but the performance degrades significantly without
enough labeled data. One effective solution to boost performance is to employ
generative models, such as Generative Adversarial Networks (GANs), to generate
synthetic data in an image hashing model. However, GAN-based methods are
difficult to train, which prevents the hashing approaches from jointly training
the generative models and the hash functions. This limitation results in
sub-optimal retrieval performance. To overcome this limitation, we propose a
novel framework, the generative cooperative hashing network, which is based on
energy-based cooperative learning. This framework jointly learns a powerful
generative representation of the data and a robust hash function via two
components: a top-down contrastive pair generator that synthesizes contrastive
images and a bottom-up multipurpose descriptor that simultaneously represents
the images from multiple perspectives, including probability density, hash
code, latent code, and category. The two components are jointly learned via a
novel likelihood-based cooperative learning scheme. We conduct experiments on
several real-world datasets and show that the proposed method outperforms the
competing hashing supervised methods, achieving up to 10% relative improvement
over the current state-of-the-art supervised hashing methods, and exhibits a
significantly better performance in out-of-distribution retrieval.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
        Image Retrieval 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/doan2022one/">One Loss For Quantization: Deep Hashing With Discrete Wasserstein Distributional Matching</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=One Loss For Quantization: Deep Hashing With Discrete Wasserstein Distributional Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=One Loss For Quantization: Deep Hashing With Discrete Wasserstein Distributional Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Doan Khoa D., Yang Peng, Li Ping</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>37</td>
    <td><p>Image hashing is a principled approximate nearest neighbor approach to find
similar items to a query in a large collection of images. Hashing aims to learn
a binary-output function that maps an image to a binary vector. For optimal
retrieval performance, producing balanced hash codes with low-quantization
error to bridge the gap between the learning stage’s continuous relaxation and
the inference stage’s discrete quantization is important. However, in the
existing deep supervised hashing methods, coding balance and low-quantization
error are difficult to achieve and involve several losses. We argue that this
is because the existing quantization approaches in these methods are
heuristically constructed and not effective to achieve these objectives. This
paper considers an alternative approach to learning the quantization
constraints. The task of learning balanced codes with low quantization error is
re-formulated as matching the learned distribution of the continuous codes to a
pre-defined discrete, uniform distribution. This is equivalent to minimizing
the distance between two distributions. We then propose a computationally
efficient distributional distance by leveraging the discrete property of the
hash functions. This distributional distance is a valid distance and enjoys
lower time and sample complexities. The proposed single-loss quantization
objective can be integrated into any existing supervised hashing method to
improve code balance and quantization error. Experiments confirm that the
proposed approach substantially improves the performance of several
representative hashing~methods.</p>
</td>
    <td>
      
        Quantization 
      
        Hashing Methods 
      
        Neural Hashing 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/deng2022insclr/">Insclr: Improving Instance Retrieval With Self-supervision</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Insclr: Improving Instance Retrieval With Self-supervision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Insclr: Improving Instance Retrieval With Self-supervision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Deng et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>11</td>
    <td><p>This work aims at improving instance retrieval with self-supervision. We find
that fine-tuning using the recently developed self-supervised (SSL) learning
methods, such as SimCLR and MoCo, fails to improve the performance of instance
retrieval. In this work, we identify that the learnt representations for
instance retrieval should be invariant to large variations in viewpoint and
background etc., whereas self-augmented positives applied by the current SSL
methods can not provide strong enough signals for learning robust
instance-level representations. To overcome this problem, we propose InsCLR, a
new SSL method that builds on the \textit{instance-level} contrast, to learn
the intra-class invariance by dynamically mining meaningful pseudo positive
samples from both mini-batches and a memory bank during training. Extensive
experiments demonstrate that InsCLR achieves similar or even better performance
than the state-of-the-art SSL methods on instance retrieval. Code is available
at https://github.com/zeludeng/insclr.</p>
</td>
    <td>
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/delmas2022artemis/">ARTEMIS: Attention-based Retrieval With Text-explicit Matching And Implicit Similarity</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=ARTEMIS: Attention-based Retrieval With Text-explicit Matching And Implicit Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=ARTEMIS: Attention-based Retrieval With Text-explicit Matching And Implicit Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Delmas et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>26</td>
    <td><p>An intuitive way to search for images is to use queries composed of an
example image and a complementary text. While the first provides rich and
implicit context for the search, the latter explicitly calls for new traits, or
specifies how some elements of the example image should be changed to retrieve
the desired target image. Current approaches typically combine the features of
each of the two elements of the query into a single representation, which can
then be compared to the ones of the potential target images. Our work aims at
shedding new light on the task by looking at it through the prism of two
familiar and related frameworks: text-to-image and image-to-image retrieval.
Taking inspiration from them, we exploit the specific relation of each query
element with the targeted image and derive light-weight attention mechanisms
which enable to mediate between the two complementary modalities. We validate
our approach on several retrieval benchmarks, querying with images and their
associated free-form text modifiers. Our method obtains state-of-the-art
results without resorting to side information, multi-level features, heavy
pre-training nor large architectures as in previous works.</p>
</td>
    <td>
      
        Transformer Based ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/dai2022multi/">Multi-granularity Association Learning Framework For On-the-fly Fine-grained Sketch-based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multi-granularity Association Learning Framework For On-the-fly Fine-grained Sketch-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multi-granularity Association Learning Framework For On-the-fly Fine-grained Sketch-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dai et al.</td> <!-- 🔧 You were missing this -->
    <td>Knowledge-Based Systems</td>
    <td>12</td>
    <td><p>Fine-grained sketch-based image retrieval (FG-SBIR) addresses the problem of
retrieving a particular photo in a given query sketch. However, its widespread
applicability is limited by the fact that it is difficult to draw a complete
sketch for most people, and the drawing process often takes time. In this
study, we aim to retrieve the target photo with the least number of strokes
possible (incomplete sketch), named on-the-fly FG-SBIR (Bhunia et al. 2020),
which starts retrieving at each stroke as soon as the drawing begins. We
consider that there is a significant correlation among these incomplete
sketches in the sketch drawing episode of each photo. To learn more efficient
joint embedding space shared between the photo and its incomplete sketches, we
propose a multi-granularity association learning framework that further
optimizes the embedding space of all incomplete sketches. Specifically, based
on the integrity of the sketch, we can divide a complete sketch episode into
several stages, each of which corresponds to a simple linear mapping layer.
Moreover, our framework guides the vector space representation of the current
sketch to approximate that of its later sketches to realize the retrieval
performance of the sketch with fewer strokes to approach that of the sketch
with more strokes. In the experiments, we proposed more realistic challenges,
and our method achieved superior early retrieval efficiency over the
state-of-the-art methods and alternative baselines on two publicly available
fine-grained sketch retrieval datasets.</p>
</td>
    <td>
      
        Tools & Libraries 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/elezi2022group/">The Group Loss++: A Deeper Look Into Group Loss For Deep Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=The Group Loss++: A Deeper Look Into Group Loss For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=The Group Loss++: A Deeper Look Into Group Loss For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Elezi et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>13</td>
    <td><p>Deep metric learning has yielded impressive results in tasks such as
clustering and image retrieval by leveraging neural networks to obtain highly
discriminative feature embeddings, which can be used to group samples into
different classes. Much research has been devoted to the design of smart loss
functions or data mining strategies for training such networks. Most methods
consider only pairs or triplets of samples within a mini-batch to compute the
loss function, which is commonly based on the distance between embeddings. We
propose Group Loss, a loss function based on a differentiable label-propagation
method that enforces embedding similarity across all samples of a group while
promoting, at the same time, low-density regions amongst data points belonging
to different groups. Guided by the smoothness assumption that “similar objects
should belong to the same group”, the proposed loss trains the neural network
for a classification task, enforcing a consistent labelling amongst samples
within a class. We design a set of inference strategies tailored towards our
algorithm, named Group Loss++ that further improve the results of our model. We
show state-of-the-art results on clustering and image retrieval on four
retrieval datasets, and present competitive results on two person
re-identification datasets, providing a unified framework for retrieval and
re-identification.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/couairon2022embedding/">Embedding Arithmetic Of Multimodal Queries For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Embedding Arithmetic Of Multimodal Queries For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Embedding Arithmetic Of Multimodal Queries For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Couairon et al.</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>13</td>
    <td><p>Latent text representations exhibit geometric regularities, such as the
famous analogy: queen is to king what woman is to man. Such structured semantic
relations were not demonstrated on image representations. Recent works aiming
at bridging this semantic gap embed images and text into a multimodal space,
enabling the transfer of text-defined transformations to the image modality. We
introduce the SIMAT dataset to evaluate the task of Image Retrieval with
Multimodal queries. SIMAT contains 6k images and 18k textual transformation
queries that aim at either replacing scene elements or changing pairwise
relationships between scene elements. The goal is to retrieve an image
consistent with the (source image, text transformation) query. We use an
image/text matching oracle (OSCAR) to assess whether the image transformation
is successful. The SIMAT dataset will be publicly available. We use SIMAT to
evaluate the geometric properties of multimodal embedding spaces trained with
an image/text matching objective, like CLIP. We show that vanilla CLIP
embeddings are not very well suited to transform images with delta vectors, but
that a simple finetuning on the COCO dataset can bring dramatic improvements.
We also study whether it is beneficial to leverage pretrained universal
sentence encoders (FastText, LASER and LaBSE).</p>
</td>
    <td>
      
        Image Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/cohen2022this/">"this Is My Unicorn, Fluffy": Personalizing Frozen Vision-language Representations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q="this Is My Unicorn, Fluffy": Personalizing Frozen Vision-language Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q="this Is My Unicorn, Fluffy": Personalizing Frozen Vision-language Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cohen et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>31</td>
    <td><p>Large Vision &amp; Language models pretrained on web-scale data provide
representations that are invaluable for numerous V&amp;L problems. However, it is
unclear how they can be used for reasoning about user-specific visual concepts
in unstructured language. This problem arises in multiple domains, from
personalized image retrieval to personalized interaction with smart devices. We
introduce a new learning setup called Personalized Vision &amp; Language (PerVL)
with two new benchmark datasets for retrieving and segmenting user-specific
“personalized” concepts “in the wild”. In PerVL, one should learn personalized
concepts (1) independently of the downstream task (2) allowing a pretrained
model to reason about them with free language, and (3) does not require
personalized negative examples. We propose an architecture for solving PerVL
that operates by extending the input vocabulary of a pretrained model with new
word embeddings for the new personalized concepts. The model can then reason
about them by simply using them in a sentence. We demonstrate that our approach
learns personalized visual concepts from a few examples and can effectively
apply them in image retrieval and semantic segmentation using rich textual
queries.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/yu2022learning/">Learning To Hash Naturally Sorts</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning To Hash Naturally Sorts' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning To Hash Naturally Sorts' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence</td>
    <td>11</td>
    <td><p>Learning to hash pictures a list-wise sorting problem. Its testing metrics,
e.g., mean-average precision, count on a sorted candidate list ordered by
pair-wise code similarity. However, scarcely does one train a deep hashing
model with the sorted results end-to-end because of the non-differentiable
nature of the sorting operation. This inconsistency in the objectives of
training and test may lead to sub-optimal performance since the training loss
often fails to reflect the actual retrieval metric. In this paper, we tackle
this problem by introducing Naturally-Sorted Hashing (NSH). We sort the Hamming
distances of samples’ hash codes and accordingly gather their latent
representations for self-supervised training. Thanks to the recent advances in
differentiable sorting approximations, the hash head receives gradients from
the sorter so that the hash encoder can be optimized along with the training
procedure. Additionally, we describe a novel Sorted Noise-Contrastive
Estimation (SortedNCE) loss that selectively picks positive and negative
samples for contrastive learning, which allows NSH to mine data semantic
relations during training in an unsupervised manner. Our extensive experiments
show the proposed NSH model significantly outperforms the existing unsupervised
hashing methods on three benchmarked datasets.</p>
</td>
    <td>
      
        Hashing Methods 
      
        AAAI 
      
        IJCAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/chen2022learning/">Learning Binarized Graph Representations With Multi-faceted Quantization Reinforcement For Top-k Recommendation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Binarized Graph Representations With Multi-faceted Quantization Reinforcement For Top-k Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Binarized Graph Representations With Multi-faceted Quantization Reinforcement For Top-k Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</td>
    <td>24</td>
    <td><p>Learning vectorized embeddings is at the core of various recommender systems
for user-item matching. To perform efficient online inference, representation
quantization, aiming to embed the latent features by a compact sequence of
discrete numbers, recently shows the promising potentiality in optimizing both
memory and computation overheads. However, existing work merely focuses on
numerical quantization whilst ignoring the concomitant information loss issue,
which, consequently, leads to conspicuous performance degradation. In this
paper, we propose a novel quantization framework to learn Binarized Graph
Representations for Top-K Recommendation (BiGeaR). BiGeaR introduces
multi-faceted quantization reinforcement at the pre-, mid-, and post-stage of
binarized representation learning, which substantially retains the
representation informativeness against embedding binarization. In addition to
saving the memory footprint, BiGeaR further develops solid online inference
acceleration with bitwise operations, providing alternative flexibility for the
realistic deployment. The empirical results over five large real-world
benchmarks show that BiGeaR achieves about 22%~40% performance improvement over
the state-of-the-art quantization-based recommender system, and recovers about
95%~102% of the performance capability of the best full-precision counterpart
with over 8x time and space reduction.</p>
</td>
    <td>
      
        KDD 
      
        Quantization 
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/chen2022fuzzy/">Fuzzy Logic Based Logical Query Answering On Knowledge Graphs</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fuzzy Logic Based Logical Query Answering On Knowledge Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fuzzy Logic Based Logical Query Answering On Knowledge Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen Xuelu, Hu Ziniu, Sun Yizhou</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>40</td>
    <td><p>Answering complex First-Order Logical (FOL) queries on large-scale incomplete
knowledge graphs (KGs) is an important yet challenging task. Recent advances
embed logical queries and KG entities in the same space and conduct query
answering via dense similarity search. However, most logical operators designed
in previous studies do not satisfy the axiomatic system of classical logic,
limiting their performance. Moreover, these logical operators are parameterized
and thus require many complex FOL queries as training data, which are often
arduous to collect or even inaccessible in most real-world KGs. We thus present
FuzzQE, a fuzzy logic based logical query embedding framework for answering FOL
queries over KGs. FuzzQE follows fuzzy logic to define logical operators in a
principled and learning-free manner, where only entity and relation embeddings
require learning. FuzzQE can further benefit from labeled complex logical
queries for training. Extensive experiments on two benchmark datasets
demonstrate that FuzzQE provides significantly better performance in answering
FOL queries compared to state-of-the-art methods. In addition, FuzzQE trained
with only KG link prediction can achieve comparable performance to those
trained with extra complex query data.</p>
</td>
    <td>
      
        Graph Based ANN 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/chen2022deep/">Deep Learning For Instance Retrieval: A Survey</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Learning For Instance Retrieval: A Survey' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Learning For Instance Retrieval: A Survey' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>122</td>
    <td><p>In recent years a vast amount of visual content has been generated and shared
from many fields, such as social media platforms, medical imaging, and
robotics. This abundance of content creation and sharing has introduced new
challenges, particularly that of searching databases for similar
content-Content Based Image Retrieval (CBIR)-a long-established research area
in which improved efficiency and accuracy are needed for real-time retrieval.
Artificial intelligence has made progress in CBIR and has significantly
facilitated the process of instance search. In this survey we review recent
instance retrieval works that are developed based on deep learning algorithms
and techniques, with the survey organized by deep network architecture types,
deep features, feature embedding and aggregation methods, and network
fine-tuning strategies. Our survey considers a wide variety of recent methods,
whereby we identify milestone work, reveal connections among various methods
and present the commonly used benchmarks, evaluation results, common
challenges, and propose promising future directions.</p>
</td>
    <td>
      
        Survey Paper 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/chen2022intra/">Intra-modal Constraint Loss For Image-text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Intra-modal Constraint Loss For Image-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Intra-modal Constraint Loss For Image-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen et al.</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE International Conference on Image Processing (ICIP)</td>
    <td>6</td>
    <td><p>Cross-modal retrieval has drawn much attention in both computer vision and
natural language processing domains. With the development of convolutional and
recurrent neural networks, the bottleneck of retrieval across image-text
modalities is no longer the extraction of image and text features but an
efficient loss function learning in embedding space. Many loss functions try to
closer pairwise features from heterogeneous modalities. This paper proposes a
method for learning joint embedding of images and texts using an intra-modal
constraint loss function to reduce the violation of negative pairs from the
same homogeneous modality. Experimental results show that our approach
outperforms state-of-the-art bi-directional image-text retrieval methods on
Flickr30K and Microsoft COCO datasets. Our code is publicly available:
https://github.com/CanonChen/IMC.</p>
</td>
    <td>
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/chen2022transhash/">Transhash: Transformer-based Hamming Hashing For Efficient Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Transhash: Transformer-based Hamming Hashing For Efficient Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Transhash: Transformer-based Hamming Hashing For Efficient Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2022 International Conference on Multimedia Retrieval</td>
    <td>36</td>
    <td><p>Deep hamming hashing has gained growing popularity in approximate nearest
neighbour search for large-scale image retrieval. Until now, the deep hashing
for the image retrieval community has been dominated by convolutional neural
network architectures, e.g. \texttt{Resnet}\cite{he2016deep}. In this paper,
inspired by the recent advancements of vision transformers, we present
\textbf{Transhash}, a pure transformer-based framework for deep hashing
learning. Concretely, our framework is composed of two major modules: (1) Based
on \textit{Vision Transformer} (ViT), we design a siamese vision transformer
backbone for image feature extraction. To learn fine-grained features, we
innovate a dual-stream feature learning on top of the transformer to learn
discriminative global and local features. (2) Besides, we adopt a Bayesian
learning scheme with a dynamically constructed similarity matrix to learn
compact binary hash codes. The entire framework is jointly trained in an
end-to-end manner.~To the best of our knowledge, this is the first work to
tackle deep hashing learning problems without convolutional neural networks
(\textit{CNNs}). We perform comprehensive experiments on three widely-studied
datasets: \textbf{CIFAR-10}, \textbf{NUSWIDE} and \textbf{IMAGENET}. The
experiments have evidenced our superiority against the existing
state-of-the-art deep hashing methods. Specifically, we achieve 8.2%, 2.6%,
12.7% performance gains in terms of average \textit{mAP} for different hash
bit lengths on three public datasets, respectively.</p>
</td>
    <td>
      
        Image Retrieval 
      
        Medical Retrieval 
      
        Multimodal Retrieval 
      
        Transformer Based ANN 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/chen2022approximate/">Approximate Nearest Neighbor Search Under Neural Similarity Metric For Large-scale Recommendation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Approximate Nearest Neighbor Search Under Neural Similarity Metric For Large-scale Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Approximate Nearest Neighbor Search Under Neural Similarity Metric For Large-scale Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</td>
    <td>12</td>
    <td><p>Model-based methods for recommender systems have been studied extensively for
years. Modern recommender systems usually resort to 1) representation learning
models which define user-item preference as the distance between their
embedding representations, and 2) embedding-based Approximate Nearest Neighbor
(ANN) search to tackle the efficiency problem introduced by large-scale corpus.
While providing efficient retrieval, the embedding-based retrieval pattern also
limits the model capacity since the form of user-item preference measure is
restricted to the distance between their embedding representations. However,
for other more precise user-item preference measures, e.g., preference scores
directly derived from a deep neural network, they are computationally
intractable because of the lack of an efficient retrieval method, and an
exhaustive search for all user-item pairs is impractical. In this paper, we
propose a novel method to extend ANN search to arbitrary matching functions,
e.g., a deep neural network. Our main idea is to perform a greedy walk with a
matching function in a similarity graph constructed from all items. To solve
the problem that the similarity measures of graph construction and user-item
matching function are heterogeneous, we propose a pluggable adversarial
training task to ensure the graph search with arbitrary matching function can
achieve fairly high precision. Experimental results in both open source and
industry datasets demonstrate the effectiveness of our method. The proposed
method has been fully deployed in the Taobao display advertising platform and
brings a considerable advertising revenue increase. We also summarize our
detailed experiences in deployment in this paper.</p>
</td>
    <td>
      
        Recommender Systems 
      
        Distance Metric Learning 
      
        CIKM 
      
        Similarity Search 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/yeh2022embedding/">Embedding Compression With Hashing For Efficient Representation Learning In Large-scale Graph</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Embedding Compression With Hashing For Efficient Representation Learning In Large-scale Graph' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Embedding Compression With Hashing For Efficient Representation Learning In Large-scale Graph' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yeh et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</td>
    <td>13</td>
    <td><p>Graph neural networks (GNNs) are deep learning models designed specifically
for graph data, and they typically rely on node features as the input to the
first layer. When applying such a type of network on the graph without node
features, one can extract simple graph-based node features (e.g., number of
degrees) or learn the input node representations (i.e., embeddings) when
training the network. While the latter approach, which trains node embeddings,
more likely leads to better performance, the number of parameters associated
with the embeddings grows linearly with the number of nodes. It is therefore
impractical to train the input node embeddings together with GNNs within
graphics processing unit (GPU) memory in an end-to-end fashion when dealing
with industrial-scale graph data. Inspired by the embedding compression methods
developed for natural language processing (NLP) tasks, we develop a node
embedding compression method where each node is compactly represented with a
bit vector instead of a floating-point vector. The parameters utilized in the
compression method can be trained together with GNNs. We show that the proposed
node embedding compression method achieves superior performance compared to the
alternatives.</p>
</td>
    <td>
      
        KDD 
      
        Hashing Methods 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/zerveas2022coder/">CODER: An Efficient Framework For Improving Retrieval Through Contextual Document Embedding Reranking</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=CODER: An Efficient Framework For Improving Retrieval Through Contextual Document Embedding Reranking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=CODER: An Efficient Framework For Improving Retrieval Through Contextual Document Embedding Reranking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zerveas et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</td>
    <td>6</td>
    <td><p>Contrastive learning has been the dominant approach to training dense
retrieval models. In this work, we investigate the impact of ranking context -
an often overlooked aspect of learning dense retrieval models. In particular,
we examine the effect of its constituent parts: jointly scoring a large number
of negatives per query, using retrieved (query-specific) instead of random
negatives, and a fully list-wise loss. To incorporate these factors into
training, we introduce Contextual Document Embedding Reranking (CODER), a
highly efficient retrieval framework. When reranking, it incurs only a
negligible computational overhead on top of a first-stage method at run time
(delay per query in the order of milliseconds), allowing it to be easily
combined with any state-of-the-art dual encoder method. After fine-tuning
through CODER, which is a lightweight and fast process, models can also be used
as stand-alone retrievers. Evaluating CODER in a large set of experiments on
the MS~MARCO and TripClick collections, we show that the contextual reranking
of precomputed document embeddings leads to a significant improvement in
retrieval performance. This improvement becomes even more pronounced when more
relevance information per query is available, shown in the TripClick
collection, where we establish new state-of-the-art results by a large margin.</p>
</td>
    <td>
      
        Hybrid ANN Methods 
      
        Tools & Libraries 
      
        EMNLP 
      
        Re RANKING 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/bhunia2022adaptive/">Adaptive Fine-grained Sketch-based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Adaptive Fine-grained Sketch-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Adaptive Fine-grained Sketch-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Bhunia et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>19</td>
    <td><p>The recent focus on Fine-Grained Sketch-Based Image Retrieval (FG-SBIR) has
shifted towards generalising a model to new categories without any training
data from them. In real-world applications, however, a trained FG-SBIR model is
often applied to both new categories and different human sketchers, i.e.,
different drawing styles. Although this complicates the generalisation problem,
fortunately, a handful of examples are typically available, enabling the model
to adapt to the new category/style. In this paper, we offer a novel perspective
– instead of asking for a model that generalises, we advocate for one that
quickly adapts, with just very few samples during testing (in a few-shot
manner). To solve this new problem, we introduce a novel model-agnostic
meta-learning (MAML) based framework with several key modifications: (1) As a
retrieval task with a margin-based contrastive loss, we simplify the MAML
training in the inner loop to make it more stable and tractable. (2) The margin
in our contrastive loss is also meta-learned with the rest of the model. (3)
Three additional regularisation losses are introduced in the outer loop, to
make the meta-learned FG-SBIR model more effective for category/style
adaptation. Extensive experiments on public datasets suggest a large gain over
generalisation and zero-shot based approaches, and a few strong few-shot
baselines.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/bogolin2022cross/">Cross Modal Retrieval With Querybank Normalisation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross Modal Retrieval With Querybank Normalisation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross Modal Retrieval With Querybank Normalisation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Bogolin et al.</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>58</td>
    <td><p>Profiting from large-scale training datasets, advances in neural architecture
design and efficient inference, joint embeddings have become the dominant
approach for tackling cross-modal retrieval. In this work we first show that,
despite their effectiveness, state-of-the-art joint embeddings suffer
significantly from the longstanding “hubness problem” in which a small number
of gallery embeddings form the nearest neighbours of many queries. Drawing
inspiration from the NLP literature, we formulate a simple but effective
framework called Querybank Normalisation (QB-Norm) that re-normalises query
similarities to account for hubs in the embedding space. QB-Norm improves
retrieval performance without requiring retraining. Differently from prior
work, we show that QB-Norm works effectively without concurrent access to any
test set queries. Within the QB-Norm framework, we also propose a novel
similarity normalisation method, the Dynamic Inverted Softmax, that is
significantly more robust than existing approaches. We showcase QB-Norm across
a range of cross modal retrieval models and benchmarks where it consistently
enhances strong baselines beyond the state of the art. Code is available at
https://vladbogo.github.io/QB-Norm/.</p>
</td>
    <td>
      
        Multimodal Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/bao2022mmfl/">Mmfl-net: Multi-scale And Multi-granularity Feature Learning For Cross-domain Fashion Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Mmfl-net: Multi-scale And Multi-granularity Feature Learning For Cross-domain Fashion Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Mmfl-net: Multi-scale And Multi-granularity Feature Learning For Cross-domain Fashion Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Bao et al.</td> <!-- 🔧 You were missing this -->
    <td>Multimedia Tools and Applications</td>
    <td>5</td>
    <td><p>Instance-level image retrieval in fashion is a challenging issue owing to its
increasing importance in real-scenario visual fashion search. Cross-domain
fashion retrieval aims to match the unconstrained customer images as queries
for photographs provided by retailers; however, it is a difficult task due to a
wide range of consumer-to-shop (C2S) domain discrepancies and also considering
that clothing image is vulnerable to various non-rigid deformations. To this
end, we propose a novel multi-scale and multi-granularity feature learning
network (MMFL-Net), which can jointly learn global-local aggregation feature
representations of clothing images in a unified framework, aiming to train a
cross-domain model for C2S fashion visual similarity. First, a new
semantic-spatial feature fusion part is designed to bridge the semantic-spatial
gap by applying top-down and bottom-up bidirectional multi-scale feature
fusion. Next, a multi-branch deep network architecture is introduced to capture
global salient, part-informed, and local detailed information, and extracting
robust and discrimination feature embedding by integrating the similarity
learning of coarse-to-fine embedding with the multiple granularities. Finally,
the improved trihard loss, center loss, and multi-task classification loss are
adopted for our MMFL-Net, which can jointly optimize intra-class and
inter-class distance and thus explicitly improve intra-class compactness and
inter-class discriminability between its visual representations for feature
learning. Furthermore, our proposed model also combines the multi-task
attribute recognition and classification module with multi-label semantic
attributes and product ID labels. Experimental results demonstrate that our
proposed MMFL-Net achieves significant improvement over the state-of-the-art
methods on the two datasets, DeepFashion-C2S and Street2Shop.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/memmesheimer2022skeleton/">Skeleton-dml: Deep Metric Learning For Skeleton-based One-shot Action Recognition</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Skeleton-dml: Deep Metric Learning For Skeleton-based One-shot Action Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Skeleton-dml: Deep Metric Learning For Skeleton-based One-shot Action Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Memmesheimer et al.</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>28</td>
    <td><p>One-shot action recognition allows the recognition of human-performed actions
with only a single training example. This can influence human-robot-interaction
positively by enabling the robot to react to previously unseen behaviour. We
formulate the one-shot action recognition problem as a deep metric learning
problem and propose a novel image-based skeleton representation that performs
well in a metric learning setting. Therefore, we train a model that projects
the image representations into an embedding space. In embedding space the
similar actions have a low euclidean distance while dissimilar actions have a
higher distance. The one-shot action recognition problem becomes a
nearest-neighbor search in a set of activity reference samples. We evaluate the
performance of our proposed representation against a variety of other
skeleton-based image representations. In addition, we present an ablation study
that shows the influence of different embedding vector sizes, losses and
augmentation. Our approach lifts the state-of-the-art by 3.3% for the one-shot
action recognition protocol on the NTU RGB+D 120 dataset under a comparable
training setup. With additional augmentation our result improved over 7.7%.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/alibey2022gsv/">Gsv-cities: Toward Appropriate Supervised Visual Place Recognition</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Gsv-cities: Toward Appropriate Supervised Visual Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Gsv-cities: Toward Appropriate Supervised Visual Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ali-bey Amar, Chaib-draa Brahim, Giguère Philippe</td> <!-- 🔧 You were missing this -->
    <td>Neurocomputing</td>
    <td>55</td>
    <td><p>This paper aims to investigate representation learning for large scale visual
place recognition, which consists of determining the location depicted in a
query image by referring to a database of reference images. This is a
challenging task due to the large-scale environmental changes that can occur
over time (i.e., weather, illumination, season, traffic, occlusion). Progress
is currently challenged by the lack of large databases with accurate ground
truth. To address this challenge, we introduce GSV-Cities, a new image dataset
providing the widest geographic coverage to date with highly accurate ground
truth, covering more than 40 cities across all continents over a 14-year
period. We subsequently explore the full potential of recent advances in deep
metric learning to train networks specifically for place recognition, and
evaluate how different loss functions influence performance. In addition, we
show that performance of existing methods substantially improves when trained
on GSV-Cities. Finally, we introduce a new fully convolutional aggregation
layer that outperforms existing techniques, including GeM, NetVLAD and
CosPlace, and establish a new state-of-the-art on large-scale benchmarks, such
as Pittsburgh, Mapillary-SLS, SPED and Nordland. The dataset and code are
available for research purposes at https://github.com/amaralibey/gsv-cities.</p>
</td>
    <td>
      
        SUPERVISED 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/an2022fast/">Fast And Incremental Loop Closure Detection With Deep Features And Proximity Graphs</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast And Incremental Loop Closure Detection With Deep Features And Proximity Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast And Incremental Loop Closure Detection With Deep Features And Proximity Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>An et al.</td> <!-- 🔧 You were missing this -->
    <td>Journal of Field Robotics</td>
    <td>44</td>
    <td><p>In recent years, the robotics community has extensively examined methods
concerning the place recognition task within the scope of simultaneous
localization and mapping applications.This article proposes an appearance-based
loop closure detection pipeline named ``FILD++” (Fast and Incremental Loop
closure Detection).First, the system is fed by consecutive images and, via
passing them twice through a single convolutional neural network, global and
local deep features are extracted.Subsequently, a hierarchical navigable
small-world graph incrementally constructs a visual database representing the
robot’s traversed path based on the computed global features.Finally, a query
image, grabbed each time step, is set to retrieve similar locations on the
traversed route.An image-to-image pairing follows, which exploits local
features to evaluate the spatial information. Thus, in the proposed article, we
propose a single network for global and local feature extraction in contrast to
our previous work (FILD), while an exhaustive search for the verification
process is adopted over the generated deep local features avoiding the
utilization of hash codes. Exhaustive experiments on eleven publicly available
datasets exhibit the system’s high performance (achieving the highest recall
score on eight of them) and low execution times (22.05 ms on average in New
College, which is the largest one containing 52480 images) compared to other
state-of-the-art approaches.</p>
</td>
    <td>
      
        Graph Based ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/aksoy2022satellite/">Satellite Image Search In Agoraeo</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Satellite Image Search In Agoraeo' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Satellite Image Search In Agoraeo' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Aksoy et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the VLDB Endowment</td>
    <td>8</td>
    <td><p>The growing operational capability of global Earth Observation (EO) creates
new opportunities for data-driven approaches to understand and protect our
planet. However, the current use of EO archives is very restricted due to the
huge archive sizes and the limited exploration capabilities provided by EO
platforms. To address this limitation, we have recently proposed MiLaN, a
content-based image retrieval approach for fast similarity search in satellite
image archives. MiLaN is a deep hashing network based on metric learning that
encodes high-dimensional image features into compact binary hash codes. We use
these codes as keys in a hash table to enable real-time nearest neighbor search
and highly accurate retrieval. In this demonstration, we showcase the
efficiency of MiLaN by integrating it with EarthQube, a browser and search
engine within AgoraEO. EarthQube supports interactive visual exploration and
Query-by-Example over satellite image repositories. Demo visitors will interact
with EarthQube playing the role of different users that search images in a
large-scale remote sensing archive by their semantic content and apply other
filters.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/jia2022fast/">Fast Online Hashing With Multi-label Projection</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Online Hashing With Multi-label Projection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Online Hashing With Multi-label Projection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jia et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>6</td>
    <td><p>Hashing has been widely researched to solve the large-scale approximate
nearest neighbor search problem owing to its time and storage superiority. In
recent years, a number of online hashing methods have emerged, which can update
the hash functions to adapt to the new stream data and realize dynamic
retrieval. However, existing online hashing methods are required to update the
whole database with the latest hash functions when a query arrives, which leads
to low retrieval efficiency with the continuous increase of the stream data. On
the other hand, these methods ignore the supervision relationship among the
examples, especially in the multi-label case. In this paper, we propose a novel
Fast Online Hashing (FOH) method which only updates the binary codes of a small
part of the database. To be specific, we first build a query pool in which the
nearest neighbors of each central point are recorded. When a new query arrives,
only the binary codes of the corresponding potential neighbors are updated. In
addition, we create a similarity matrix which takes the multi-label supervision
information into account and bring in the multi-label projection loss to
further preserve the similarity among the multi-label data. The experimental
results on two common benchmarks show that the proposed FOH can achieve
dramatic superiority on query time up to 6.28 seconds less than
state-of-the-art baselines with competitive retrieval accuracy.</p>
</td>
    <td>
      
        Hashing Methods 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/yu2022self/">Self-supervised Asymmetric Deep Hashing With Margin-scalable Constraint</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Self-supervised Asymmetric Deep Hashing With Margin-scalable Constraint' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Self-supervised Asymmetric Deep Hashing With Margin-scalable Constraint' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yu et al.</td> <!-- 🔧 You were missing this -->
    <td>Neurocomputing</td>
    <td>5</td>
    <td><p>Due to its effectivity and efficiency, deep hashing approaches are widely
used for large-scale visual search. However, it is still challenging to produce
compact and discriminative hash codes for images associated with multiple
semantics for two main reasons, 1) similarity constraints designed in most of
the existing methods are based upon an oversimplified similarity
assignment(i.e., 0 for instance pairs sharing no label, 1 for instance pairs
sharing at least 1 label), 2) the exploration in multi-semantic relevance are
insufficient or even neglected in many of the existing methods. These problems
significantly limit the discrimination of generated hash codes. In this paper,
we propose a novel self-supervised asymmetric deep hashing method with a
margin-scalable constraint(SADH) approach to cope with these problems. SADH
implements a self-supervised network to sufficiently preserve semantic
information in a semantic feature dictionary and a semantic code dictionary for
the semantics of the given dataset, which efficiently and precisely guides a
feature learning network to preserve multilabel semantic information using an
asymmetric learning strategy. By further exploiting semantic dictionaries, a
new margin-scalable constraint is employed for both precise similarity
searching and robust hash code generation. Extensive empirical research on four
popular benchmarks validates the proposed method and shows it outperforms
several state-of-the-art approaches.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        SUPERVISED 
      
        Self SUPERVISED 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/tian2022learned/">A Learned Index For Exact Similarity Search In Metric Spaces</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Learned Index For Exact Similarity Search In Metric Spaces' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Learned Index For Exact Similarity Search In Metric Spaces' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tian et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Knowledge and Data Engineering</td>
    <td>12</td>
    <td><p>Indexing is an effective way to support efficient query processing in large
databases. Recently the concept of learned index, which replaces or complements
traditional index structures with machine learning models, has been actively
explored to reduce storage and search costs. However, accurate and efficient
similarity query processing in high-dimensional metric spaces remains to be an
open challenge. In this paper, we propose a novel indexing approach called LIMS
that uses data clustering, pivot-based data transformation techniques and
learned indexes to support efficient similarity query processing in metric
spaces. In LIMS, the underlying data is partitioned into clusters such that
each cluster follows a relatively uniform data distribution. Data
redistribution is achieved by utilizing a small number of pivots for each
cluster. Similar data are mapped into compact regions and the mapped values are
totally ordinal. Machine learning models are developed to approximate the
position of each data record on disk. Efficient algorithms are designed for
processing range queries and nearest neighbor queries based on LIMS, and for
index maintenance with dynamic updates. Extensive experiments on real-world and
synthetic datasets demonstrate the superiority of LIMS compared with
traditional indexes and state-of-the-art learned indexes.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/tian2022semi/">Semi-supervised Multimodal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semi-supervised Multimodal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semi-supervised Multimodal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tian et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>8</td>
    <td><p>Retrieving nearest neighbors across correlated data in multiple modalities,
such as image-text pairs on Facebook and video-tag pairs on YouTube, has become
a challenging task due to the huge amount of data. Multimodal hashing methods
that embed data into binary codes can boost the retrieving speed and reduce
storage requirement. As unsupervised multimodal hashing methods are usually
inferior to supervised ones, while the supervised ones requires too much
manually labeled data, the proposed method in this paper utilizes a part of
labels to design a semi-supervised multimodal hashing method. It first computes
the transformation matrices for data matrices and label matrix. Then, with
these transformation matrices, fuzzy logic is introduced to estimate a label
matrix for unlabeled data. Finally, it uses the estimated label matrix to learn
hashing functions for data in each modality to generate a unified binary code
matrix. Experiments show that the proposed semi-supervised method with 50%
labels can get a medium performance among the compared supervised ones and
achieve an approximate performance to the best supervised method with 90%
labels. With only 10% labels, the proposed method can still compete with the
worst compared supervised one.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/thakur2022injecting/">Injecting Domain Adaptation With Learning-to-hash For Effective And Efficient Zero-shot Dense Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Injecting Domain Adaptation With Learning-to-hash For Effective And Efficient Zero-shot Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Injecting Domain Adaptation With Learning-to-hash For Effective And Efficient Zero-shot Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Thakur Nandan, Reimers Nils, Lin Jimmy</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>6</td>
    <td><p>Dense retrieval overcome the lexical gap and has shown great success in
ad-hoc information retrieval (IR). Despite their success, dense retrievers are
expensive to serve across practical use cases. For use cases requiring to
search from millions of documents, the dense index becomes bulky and requires
high memory usage for storing the index. More recently, learning-to-hash (LTH)
techniques, for e.g., BPR and JPQ, produce binary document vectors, thereby
reducing the memory requirement to efficiently store the dense index. LTH
techniques are supervised and finetune the retriever using a ranking loss. They
outperform their counterparts, i.e., traditional out-of-the-box vector
compression techniques such as PCA or PQ. A missing piece from prior work is
that existing techniques have been evaluated only in-domain, i.e., on a single
dataset such as MS MARCO. In our work, we evaluate LTH and vector compression
techniques for improving the downstream zero-shot retrieval accuracy of the
TAS-B dense retriever while maintaining efficiency at inference. Our results
demonstrate that, unlike prior work, LTH strategies when applied naively can
underperform the zero-shot TAS-B dense retriever on average by up to 14%
nDCG@10 on the BEIR benchmark. To solve this limitation, in our work, we
propose an easy yet effective solution of injecting domain adaptation with
existing supervised LTH techniques. We experiment with two well-known
unsupervised domain adaptation techniques: GenQ and GPL. Our domain adaptation
injection technique can improve the downstream zero-shot retrieval
effectiveness for both BPR and JPQ variants of the TAS-B model by on average
11.5% and 8.2% nDCG@10 while both maintaining 32\(\times\) memory efficiency and
14\(\times\) and 2\(\times\) speedup respectively in CPU retrieval latency on BEIR.
All our code, models, and data are publicly available at
https://github.com/thakur-nandan/income.</p>
</td>
    <td>
      
        Few Shot & Zero Shot 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/xu2022hyp/">Hyp\(^2\) Loss: Beyond Hypersphere Metric Space For Multi-label Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hyp\(^2\) Loss: Beyond Hypersphere Metric Space For Multi-label Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hyp\(^2\) Loss: Beyond Hypersphere Metric Space For Multi-label Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 30th ACM International Conference on Multimedia</td>
    <td>15</td>
    <td><p>Image retrieval has become an increasingly appealing technique with broad
multimedia application prospects, where deep hashing serves as the dominant
branch towards low storage and efficient retrieval. In this paper, we carried
out in-depth investigations on metric learning in deep hashing for establishing
a powerful metric space in multi-label scenarios, where the pair loss suffers
high computational overhead and converge difficulty, while the proxy loss is
theoretically incapable of expressing the profound label dependencies and
exhibits conflicts in the constructed hypersphere space. To address the
problems, we propose a novel metric learning framework with Hybrid Proxy-Pair
Loss (HyP\(^2\) Loss) that constructs an expressive metric space with efficient
training complexity w.r.t. the whole dataset. The proposed HyP\(^2\) Loss focuses
on optimizing the hypersphere space by learnable proxies and excavating
data-to-data correlations of irrelevant pairs, which integrates sufficient data
correspondence of pair-based methods and high-efficiency of proxy-based
methods. Extensive experiments on four standard multi-label benchmarks justify
the proposed method outperforms the state-of-the-art, is robust among different
hash bits and achieves significant performance gains with a faster, more stable
convergence speed. Our code is available at
https://github.com/JerryXu0129/HyP2-Loss.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/sun2022deep/">Deep Normalized Cross-modal Hashing With Bi-direction Relation Reasoning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Normalized Cross-modal Hashing With Bi-direction Relation Reasoning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Normalized Cross-modal Hashing With Bi-direction Relation Reasoning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sun et al.</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>26</td>
    <td><p>Due to the continuous growth of large-scale multi-modal data and increasing requirements for retrieval speed, deep cross-modal hashing has gained increasing attention recently. Most of existing studies take a similarity matrix as supervision to optimize their models, and the inner product between continuous surrogates of hash codes is utilized to depict the similarity in the Hamming space. However, all of them merely consider the relevant information to build the similarity matrix, ignoring the contribution of the irrelevant one, i.e., the categories that samples do not belong to. Therefore, they cannot effectively alleviate the effect of dissimilar samples. Moreover, due to the modality distribution difference, directly utilizing continuous surrogates of hash codes to calculate similarity may induce suboptimal retrieval performance. To tackle these issues, in this paper, we propose a novel deep normalized cross-modal hashing scheme with bi-direction relation reasoning, named Bi_NCMH. Specifically, we build the multi-level semantic similarity matrix by considering bi-direction relation, i.e., consistent and inconsistent relation. It hence can holistically characterize relations among instances. Besides, we execute feature normalization on continuous surrogates of hash codes to eliminate the deviation caused by modality gap, which further reduces the negative impact of binarization on retrieval performance. Extensive experiments on two cross-modal benchmark datasets demonstrate the superiority of our model over several state-of-the-art baselines.</p>
</td>
    <td>
      
        Hashing Methods 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/su2022global/">Global Learnable Attention For Single Image Super-resolution</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Global Learnable Attention For Single Image Super-resolution' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Global Learnable Attention For Single Image Super-resolution' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Su et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>33</td>
    <td><p>Self-similarity is valuable to the exploration of non-local textures in
single image super-resolution (SISR). Researchers usually assume that the
importance of non-local textures is positively related to their similarity
scores. In this paper, we surprisingly found that when repairing severely
damaged query textures, some non-local textures with low-similarity which are
closer to the target can provide more accurate and richer details than the
high-similarity ones. In these cases, low-similarity does not mean inferior but
is usually caused by different scales or orientations. Utilizing this finding,
we proposed a Global Learnable Attention (GLA) to adaptively modify similarity
scores of non-local textures during training instead of only using a fixed
similarity scoring function such as the dot product. The proposed GLA can
explore non-local textures with low-similarity but more accurate details to
repair severely damaged textures. Furthermore, we propose to adopt Super-Bit
Locality-Sensitive Hashing (SB-LSH) as a preprocessing method for our GLA. With
the SB-LSH, the computational complexity of our GLA is reduced from quadratic
to asymptotic linear with respect to the image size. In addition, the proposed
GLA can be integrated into existing deep SISR models as an efficient general
building block. Based on the GLA, we constructed a Deep Learnable Similarity
Network (DLSN), which achieves state-of-the-art performance for SISR tasks of
different degradation types (e.g. blur and noise). Our code and a pre-trained
DLSN have been uploaded to GitHub{\dag} for validation.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/zhao2022progressive/">Progressive Learning For Image Retrieval With Hybrid-modality Queries</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Progressive Learning For Image Retrieval With Hybrid-modality Queries' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Progressive Learning For Image Retrieval With Hybrid-modality Queries' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhao Yida, Song Yuqing, Jin Qin</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>21</td>
    <td><p>Image retrieval with hybrid-modality queries, also known as composing text
and image for image retrieval (CTI-IR), is a retrieval task where the search
intention is expressed in a more complex query format, involving both vision
and text modalities. For example, a target product image is searched using a
reference product image along with text about changing certain attributes of
the reference image as the query. It is a more challenging image retrieval task
that requires both semantic space learning and cross-modal fusion. Previous
approaches that attempt to deal with both aspects achieve unsatisfactory
performance. In this paper, we decompose the CTI-IR task into a three-stage
learning problem to progressively learn the complex knowledge for image
retrieval with hybrid-modality queries. We first leverage the semantic
embedding space for open-domain image-text retrieval, and then transfer the
learned knowledge to the fashion-domain with fashion-related pre-training
tasks. Finally, we enhance the pre-trained model from single-query to
hybrid-modality query for the CTI-IR task. Furthermore, as the contribution of
individual modality in the hybrid-modality query varies for different retrieval
scenarios, we propose a self-supervised adaptive weighting strategy to
dynamically determine the importance of image and text in the hybrid-modality
query for better retrieval. Extensive experiments show that our proposed model
significantly outperforms state-of-the-art methods in the mean of Recall@K by
24.9% and 9.5% on the Fashion-IQ and Shoes benchmark datasets respectively.</p>
</td>
    <td>
      
        SIGIR 
      
        Text Retrieval 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/shukor2022transformer/">Transformer Decoders With Multimodal Regularization For Cross-modal Food Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Transformer Decoders With Multimodal Regularization For Cross-modal Food Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Transformer Decoders With Multimodal Regularization For Cross-modal Food Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shukor et al.</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>22</td>
    <td><p>Cross-modal image-recipe retrieval has gained significant attention in recent
years. Most work focuses on improving cross-modal embeddings using unimodal
encoders, that allow for efficient retrieval in large-scale databases, leaving
aside cross-attention between modalities which is more computationally
expensive. We propose a new retrieval framework, T-Food (Transformer Decoders
with MultiModal Regularization for Cross-Modal Food Retrieval) that exploits
the interaction between modalities in a novel regularization scheme, while
using only unimodal encoders at test time for efficient retrieval. We also
capture the intra-dependencies between recipe entities with a dedicated recipe
encoder, and propose new variants of triplet losses with dynamic margins that
adapt to the difficulty of the task. Finally, we leverage the power of the
recent Vision and Language Pretraining (VLP) models such as CLIP for the image
encoder. Our approach outperforms existing approaches by a large margin on the
Recipe1M dataset. Specifically, we achieve absolute improvements of 8.1 % (72.6
R@1) and +10.9 % (44.6 R@1) on the 1k and 10k test sets respectively. The code
is available here:https://github.com/mshukor/TFood</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/shvetsova2022everything/">Everything At Once -- Multi-modal Fusion Transformer For Video Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Everything At Once -- Multi-modal Fusion Transformer For Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Everything At Once -- Multi-modal Fusion Transformer For Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shvetsova et al.</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>98</td>
    <td><p>Multi-modal learning from video data has seen increased attention recently as
it allows to train semantically meaningful embeddings without human annotation
enabling tasks like zero-shot retrieval and classification. In this work, we
present a multi-modal, modality agnostic fusion transformer approach that
learns to exchange information between multiple modalities, such as video,
audio, and text, and integrate them into a joined multi-modal representation to
obtain an embedding that aggregates multi-modal temporal information. We
propose to train the system with a combinatorial loss on everything at once,
single modalities as well as pairs of modalities, explicitly leaving out any
add-ons such as position or modality encoding. At test time, the resulting
model can process and fuse any number of input modalities. Moreover, the
implicit properties of the transformer allow to process inputs of different
lengths. To evaluate the proposed approach, we train the model on the large
scale HowTo100M dataset and evaluate the resulting embedding space on four
challenging benchmark datasets obtaining state-of-the-art results in zero-shot
video retrieval and zero-shot video action localization.</p>
</td>
    <td>
      
        Video Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/shi2022learning/">Learning Similarity Preserving Binary Codes For Recommender Systems</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Similarity Preserving Binary Codes For Recommender Systems' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Similarity Preserving Binary Codes For Recommender Systems' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shi Yang, Chung Young-joo</td> <!-- 🔧 You were missing this -->
    <td>ACM Transactions on Multimedia Computing, Communications, and Applications</td>
    <td>19</td>
    <td><p>Hashing-based Recommender Systems (RSs) are widely studied to provide
scalable services. The existing methods for the systems combine three modules
to achieve efficiency: feature extraction, interaction modeling, and
binarization. In this paper, we study an unexplored module combination for the
hashing-based recommender systems, namely Compact Cross-Similarity Recommender
(CCSR). Inspired by cross-modal retrieval, CCSR utilizes Maximum a Posteriori
similarity instead of matrix factorization and rating reconstruction to model
interactions between users and items. We conducted experiments on MovieLens1M,
Amazon product review, Ichiba purchase dataset and confirmed CCSR outperformed
the existing matrix factorization-based methods. On the Movielens1M dataset,
the absolute performance improvements are up to 15.69% in NDCG and 4.29% in
Recall. In addition, we extensively studied three binarization modules: \(sign\),
scaled tanh, and sign-scaled tanh. The result demonstrated that although
differentiable scaled tanh is popular in recent discrete feature learning
literature, a huge performance drop occurs when outputs of scaled \(tanh\) are
forced to be binary.</p>
</td>
    <td>
      
        Recommender Systems 
      
        Compact Codes 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/sheynin2022knn/">Knn-diffusion: Image Generation Via Large-scale Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Knn-diffusion: Image Generation Via Large-scale Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Knn-diffusion: Image Generation Via Large-scale Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sheynin et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>26</td>
    <td><p>Recent text-to-image models have achieved impressive results. However, since
they require large-scale datasets of text-image pairs, it is impractical to
train them on new domains where data is scarce or not labeled. In this work, we
propose using large-scale retrieval methods, in particular, efficient
k-Nearest-Neighbors (kNN), which offers novel capabilities: (1) training a
substantially small and efficient text-to-image diffusion model without any
text, (2) generating out-of-distribution images by simply swapping the
retrieval database at inference time, and (3) performing text-driven local
semantic manipulations while preserving object identity. To demonstrate the
robustness of our method, we apply our kNN approach on two state-of-the-art
diffusion backbones, and show results on several different datasets. As
evaluated by human studies and automatic metrics, our method achieves
state-of-the-art results compared to existing approaches that train
text-to-image generation models using images only (without paired text data)</p>
</td>
    <td>
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/shen2022semicon/">SEMICON: A Learning-to-hash Solution For Large-scale Fine-grained Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=SEMICON: A Learning-to-hash Solution For Large-scale Fine-grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=SEMICON: A Learning-to-hash Solution For Large-scale Fine-grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shen et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>13</td>
    <td><p>In this paper, we propose Suppression-Enhancing Mask based attention and
Interactive Channel transformatiON (SEMICON) to learn binary hash codes for
dealing with large-scale fine-grained image retrieval tasks. In SEMICON, we
first develop a suppression-enhancing mask (SEM) based attention to dynamically
localize discriminative image regions. More importantly, different from
existing attention mechanism simply erasing previous discriminative regions,
our SEM is developed to restrain such regions and then discover other
complementary regions by considering the relation between activated regions in
a stage-by-stage fashion. In each stage, the interactive channel transformation
(ICON) module is afterwards designed to exploit correlations across channels of
attended activation tensors. Since channels could generally correspond to the
parts of fine-grained objects, the part correlation can be also modeled
accordingly, which further improves fine-grained retrieval accuracy. Moreover,
to be computational economy, ICON is realized by an efficient two-step process.
Finally, the hash learning of our SEMICON consists of both global- and
local-level branches for better representing fine-grained objects and then
generating binary hash codes explicitly corresponding to multiple levels.
Experiments on five benchmark fine-grained datasets show our superiority over
competing methods.</p>
</td>
    <td>
      
        Image Retrieval 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/shi2022information/">Information-theoretic Hashing For Zero-shot Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Information-theoretic Hashing For Zero-shot Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Information-theoretic Hashing For Zero-shot Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shi et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE International Conference on Data Mining (ICDM)</td>
    <td>11</td>
    <td><p>Zero-shot cross-modal retrieval (ZS-CMR) deals with the retrieval problem
among heterogenous data from unseen classes. Typically, to guarantee
generalization, the pre-defined class embeddings from natural language
processing (NLP) models are used to build a common space. In this paper,
instead of using an extra NLP model to define a common space beforehand, we
consider a totally different way to construct (or learn) a common hamming space
from an information-theoretic perspective. We term our model the
Information-Theoretic Hashing (ITH), which is composed of two cascading
modules: an Adaptive Information Aggregation (AIA) module; and a Semantic
Preserving Encoding (SPE) module. Specifically, our AIA module takes the
inspiration from the Principle of Relevant Information (PRI) to construct a
common space that adaptively aggregates the intrinsic semantics of different
modalities of data and filters out redundant or irrelevant information. On the
other hand, our SPE module further generates the hashing codes of different
modalities by preserving the similarity of intrinsic semantics with the
element-wise Kullback-Leibler (KL) divergence. A total correlation
regularization term is also imposed to reduce the redundancy amongst different
dimensions of hash codes. Sufficient experiments on three benchmark datasets
demonstrate the superiority of the proposed ITH in ZS-CMR. Source code is
available in the supplementary material.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Multimodal Retrieval 
      
        Few Shot & Zero Shot 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/song2022asymmetric/">Asymmetric Hash Code Learning For Remote Sensing Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Asymmetric Hash Code Learning For Remote Sensing Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Asymmetric Hash Code Learning For Remote Sensing Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Song et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Geoscience and Remote Sensing</td>
    <td>44</td>
    <td><p>Remote sensing image retrieval (RSIR), aiming at searching for a set of
similar items to a given query image, is a very important task in remote
sensing applications. Deep hashing learning as the current mainstream method
has achieved satisfactory retrieval performance. On one hand, various deep
neural networks are used to extract semantic features of remote sensing images.
On the other hand, the hashing techniques are subsequently adopted to map the
high-dimensional deep features to the low-dimensional binary codes. This kind
of methods attempts to learn one hash function for both the query and database
samples in a symmetric way. However, with the number of database samples
increasing, it is typically time-consuming to generate the hash codes of
large-scale database images. In this paper, we propose a novel deep hashing
method, named asymmetric hash code learning (AHCL), for RSIR. The proposed AHCL
generates the hash codes of query and database images in an asymmetric way. In
more detail, the hash codes of query images are obtained by binarizing the
output of the network, while the hash codes of database images are directly
learned by solving the designed objective function. In addition, we combine the
semantic information of each image and the similarity information of pairs of
images as supervised information to train a deep hashing network, which
improves the representation ability of deep features and hash codes. The
experimental results on three public datasets demonstrate that the proposed
method outperforms symmetric methods in terms of retrieval accuracy and
efficiency. The source code is available at
https://github.com/weiweisong415/Demo AHCL for TGRS2022.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/xu2022hhf/">HHF: Hashing-guided Hinge Function For Deep Hashing Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=HHF: Hashing-guided Hinge Function For Deep Hashing Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=HHF: Hashing-guided Hinge Function For Deep Hashing Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xu et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>16</td>
    <td><p>Deep hashing has shown promising performance in large-scale image retrieval.
However, latent codes extracted by Deep Neural Networks (DNNs) will inevitably
lose semantic information during the binarization process, which damages the
retrieval accuracy and makes it challenging. Although many existing approaches
perform regularization to alleviate quantization errors, we figure out an
incompatible conflict between metric learning and quantization learning. The
metric loss penalizes the inter-class distances to push different classes
unconstrained far away. Worse still, it tends to map the latent code deviate
from ideal binarization point and generate severe ambiguity in the binarization
process. Based on the minimum distance of the binary linear code, we creatively
propose Hashing-guided Hinge Function (HHF) to avoid such conflict. In detail,
the carefully-designed inflection point, which relies on the hash bit length
and category numbers, is explicitly adopted to balance the metric term and
quantization term. Such a modification prevents the network from falling into
local metric optimal minima in deep hashing. Extensive experiments in CIFAR-10,
CIFAR-100, ImageNet, and MS-COCO show that HHF consistently outperforms
existing techniques, and is robust and flexible to transplant into other
methods. Code is available at https://github.com/JerryXu0129/HHF.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/sangkloy2022sketch/">A Sketch Is Worth A Thousand Words: Image Retrieval With Text And Sketch</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Sketch Is Worth A Thousand Words: Image Retrieval With Text And Sketch' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Sketch Is Worth A Thousand Words: Image Retrieval With Text And Sketch' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sangkloy et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>24</td>
    <td><p>We address the problem of retrieving images with both a sketch and a text
query. We present TASK-former (Text And SKetch transformer), an end-to-end
trainable model for image retrieval using a text description and a sketch as
input. We argue that both input modalities complement each other in a manner
that cannot be achieved easily by either one alone. TASK-former follows the
late-fusion dual-encoder approach, similar to CLIP, which allows efficient and
scalable retrieval since the retrieval set can be indexed independently of the
queries. We empirically demonstrate that using an input sketch (even a poorly
drawn one) in addition to text considerably increases retrieval recall compared
to traditional text-based image retrieval. To evaluate our approach, we collect
5,000 hand-drawn sketches for images in the test set of the COCO dataset. The
collected sketches are available a https://janesjanes.github.io/tsbir/.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/santhanam2022colbertv2/">Colbertv2: Effective And Efficient Retrieval Via Lightweight Late Interaction</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Colbertv2: Effective And Efficient Retrieval Via Lightweight Late Interaction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Colbertv2: Effective And Efficient Retrieval Via Lightweight Late Interaction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Santhanam et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</td>
    <td>163</td>
    <td><p>Neural information retrieval (IR) has greatly advanced search and other
knowledge-intensive language tasks. While many neural IR methods encode queries
and documents into single-vector representations, late interaction models
produce multi-vector representations at the granularity of each token and
decompose relevance modeling into scalable token-level computations. This
decomposition has been shown to make late interaction more effective, but it
inflates the space footprint of these models by an order of magnitude. In this
work, we introduce ColBERTv2, a retriever that couples an aggressive residual
compression mechanism with a denoised supervision strategy to simultaneously
improve the quality and space footprint of late interaction. We evaluate
ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art
quality within and outside the training domain while reducing the space
footprint of late interaction models by 6–10\(\times\).</p>
</td>
    <td>
      
        Similarity Search 
      
        EACL 
      
        NAACL 
      
        ACL 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/schall2022gpr1200/">GPR1200: A Benchmark For General-purpose Content-based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=GPR1200: A Benchmark For General-purpose Content-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=GPR1200: A Benchmark For General-purpose Content-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Schall et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>18</td>
    <td><p>Even though it has extensively been shown that retrieval specific training of
deep neural networks is beneficial for nearest neighbor image search quality,
most of these models are trained and tested in the domain of landmarks images.
However, some applications use images from various other domains and therefore
need a network with good generalization properties - a general-purpose CBIR
model. To the best of our knowledge, no testing protocol has so far been
introduced to benchmark models with respect to general image retrieval quality.
After analyzing popular image retrieval test sets we decided to manually curate
GPR1200, an easy to use and accessible but challenging benchmark dataset with a
broad range of image categories. This benchmark is subsequently used to
evaluate various pretrained models of different architectures on their
generalization qualities. We show that large-scale pretraining significantly
improves retrieval performance and present experiments on how to further
increase these properties by appropriate fine-tuning. With these promising
results, we hope to increase interest in the research topic of general-purpose
CBIR.</p>
</td>
    <td>
      
        Evaluation 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/ruta2022stylebabel/">Stylebabel: Artistic Style Tagging And Captioning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Stylebabel: Artistic Style Tagging And Captioning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Stylebabel: Artistic Style Tagging And Captioning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ruta et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>6</td>
    <td><p>We present StyleBabel, a unique open access dataset of natural language
captions and free-form tags describing the artistic style of over 135K digital
artworks, collected via a novel participatory method from experts studying at
specialist art and design schools. StyleBabel was collected via an iterative
method, inspired by `Grounded Theory’: a qualitative approach that enables
annotation while co-evolving a shared language for fine-grained artistic style
attribute description. We demonstrate several downstream tasks for StyleBabel,
adapting the recent ALADIN architecture for fine-grained style similarity, to
train cross-modal embeddings for: 1) free-form tag generation; 2) natural
language description of artistic style; 3) fine-grained text search of style.
To do so, we extend ALADIN with recent advances in Visual Transformer (ViT) and
cross-modal representation learning, achieving a state of the art accuracy in
fine-grained style retrieval.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/sami2022benchmarking/">Benchmarking Human Face Similarity Using Identical Twins</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Benchmarking Human Face Similarity Using Identical Twins' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Benchmarking Human Face Similarity Using Identical Twins' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sami et al.</td> <!-- 🔧 You were missing this -->
    <td>IET Biometrics</td>
    <td>5</td>
    <td><p>The problem of distinguishing identical twins and non-twin look-alikes in
automated facial recognition (FR) applications has become increasingly
important with the widespread adoption of facial biometrics. Due to the high
facial similarity of both identical twins and look-alikes, these face pairs
represent the hardest cases presented to facial recognition tools. This work
presents an application of one of the largest twin datasets compiled to date to
address two FR challenges: 1) determining a baseline measure of facial
similarity between identical twins and 2) applying this similarity measure to
determine the impact of doppelgangers, or look-alikes, on FR performance for
large face datasets. The facial similarity measure is determined via a deep
convolutional neural network. This network is trained on a tailored
verification task designed to encourage the network to group together highly
similar face pairs in the embedding space and achieves a test AUC of 0.9799.
The proposed network provides a quantitative similarity score for any two given
faces and has been applied to large-scale face datasets to identify similar
face pairs. An additional analysis which correlates the comparison score
returned by a facial recognition tool and the similarity score returned by the
proposed network has also been performed.</p>
</td>
    <td>
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/ruan2022tricolo/">Tricolo: Trimodal Contrastive Loss For Text To Shape Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Tricolo: Trimodal Contrastive Loss For Text To Shape Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Tricolo: Trimodal Contrastive Loss For Text To Shape Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ruan et al.</td> <!-- 🔧 You were missing this -->
    <td>2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>5</td>
    <td><p>Text-to-shape retrieval is an increasingly relevant problem with the growth
of 3D shape data. Recent work on contrastive losses for learning joint
embeddings over multimodal data has been successful at tasks such as retrieval
and classification. Thus far, work on joint representation learning for 3D
shapes and text has focused on improving embeddings through modeling of complex
attention between representations, or multi-task learning. We propose a
trimodal learning scheme over text, multi-view images and 3D shape voxels, and
show that with large batch contrastive learning we achieve good performance on
text-to-shape retrieval without complex attention mechanisms or losses. Our
experiments serve as a foundation for follow-up work on building trimodal
embeddings for text-image-shape.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/qu2022single/">Single-shot Embedding Dimension Search In Recommender System</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Single-shot Embedding Dimension Search In Recommender System' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Single-shot Embedding Dimension Search In Recommender System' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Qu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>14</td>
    <td><p>As a crucial component of most modern deep recommender systems, feature
embedding maps high-dimensional sparse user/item features into low-dimensional
dense embeddings. However, these embeddings are usually assigned a unified
dimension, which suffers from the following issues: (1) high memory usage and
computation cost. (2) sub-optimal performance due to inferior dimension
assignments. In order to alleviate the above issues, some works focus on
automated embedding dimension search by formulating it as hyper-parameter
optimization or embedding pruning problems. However, they either require
well-designed search space for hyperparameters or need time-consuming
optimization procedures. In this paper, we propose a Single-Shot Embedding
Dimension Search method, called SSEDS, which can efficiently assign dimensions
for each feature field via a single-shot embedding pruning operation while
maintaining the recommendation accuracy of the model. Specifically, it
introduces a criterion for identifying the importance of each embedding
dimension for each feature field. As a result, SSEDS could automatically obtain
mixed-dimensional embeddings by explicitly reducing redundant embedding
dimensions based on the corresponding dimension importance ranking and the
predefined parameter budget. Furthermore, the proposed SSEDS is model-agnostic,
meaning that it could be integrated into different base recommendation models.
The extensive offline experiments are conducted on two widely used public
datasets for CTR prediction tasks, and the results demonstrate that SSEDS can
still achieve strong recommendation performance even if it has reduced 90%
parameters. Moreover, SSEDS has also been deployed on the WeChat Subscription
platform for practical recommendation services. The 7-day online A/B test
results show that SSEDS can significantly improve the performance of the online
recommendation model.</p>
</td>
    <td>
      
        SIGIR 
      
        Text Retrieval 
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/qiu2022pre/">Pre-training Tasks For User Intent Detection And Embedding Retrieval In E-commerce Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Pre-training Tasks For User Intent Detection And Embedding Retrieval In E-commerce Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Pre-training Tasks For User Intent Detection And Embedding Retrieval In E-commerce Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Qiu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</td>
    <td>13</td>
    <td><p>BERT-style models pre-trained on the general corpus (e.g., Wikipedia) and
fine-tuned on specific task corpus, have recently emerged as breakthrough
techniques in many NLP tasks: question answering, text classification, sequence
labeling and so on. However, this technique may not always work, especially for
two scenarios: a corpus that contains very different text from the general
corpus Wikipedia, or a task that learns embedding spacial distribution for a
specific purpose (e.g., approximate nearest neighbor search). In this paper, to
tackle the above two scenarios that we have encountered in an industrial
e-commerce search system, we propose customized and novel pre-training tasks
for two critical modules: user intent detection and semantic embedding
retrieval. The customized pre-trained models after fine-tuning, being less than
10% of BERT-base’s size in order to be feasible for cost-efficient CPU serving,
significantly improve the other baseline models: 1) no pre-training model and
2) fine-tuned model from the official pre-trained BERT using general corpus, on
both offline datasets and online system. We have open sourced our datasets for
the sake of reproducibility and future works.</p>
</td>
    <td>
      
        CIKM 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/xu2022laprador/">Laprador: Unsupervised Pretrained Dense Retriever For Zero-shot Text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Laprador: Unsupervised Pretrained Dense Retriever For Zero-shot Text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Laprador: Unsupervised Pretrained Dense Retriever For Zero-shot Text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xu et al.</td> <!-- 🔧 You were missing this -->
    <td>Findings of the Association for Computational Linguistics: ACL 2022</td>
    <td>22</td>
    <td><p>In this paper, we propose LaPraDoR, a pretrained dual-tower dense retriever
that does not require any supervised data for training. Specifically, we first
present Iterative Contrastive Learning (ICoL) that iteratively trains the query
and document encoders with a cache mechanism. ICoL not only enlarges the number
of negative instances but also keeps representations of cached examples in the
same hidden space. We then propose Lexicon-Enhanced Dense Retrieval (LEDR) as a
simple yet effective way to enhance dense retrieval with lexical matching. We
evaluate LaPraDoR on the recently proposed BEIR benchmark, including 18
datasets of 9 zero-shot text retrieval tasks. Experimental results show that
LaPraDoR achieves state-of-the-art performance compared with supervised dense
retrieval models, and further analysis reveals the effectiveness of our
training strategy and objectives. Compared to re-ranking, our lexicon-enhanced
approach can be run in milliseconds (22.5x faster) while achieving superior
performance.</p>
</td>
    <td>
      
        Few Shot & Zero Shot 
      
        ACL 
      
        Unsupervised 
      
        SUPERVISED 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/pishdad2022uncertainty/">Uncertainty-based Cross-modal Retrieval With Probabilistic Representations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Uncertainty-based Cross-modal Retrieval With Probabilistic Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Uncertainty-based Cross-modal Retrieval With Probabilistic Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Pishdad et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>161</td>
    <td><p>Probabilistic embeddings have proven useful for capturing polysemous word
meanings, as well as ambiguity in image matching. In this paper, we study the
advantages of probabilistic embeddings in a cross-modal setting (i.e., text and
images), and propose a simple approach that replaces the standard vector point
embeddings in extant image-text matching models with probabilistic
distributions that are parametrically learned. Our guiding hypothesis is that
the uncertainty encoded in the probabilistic embeddings captures the
cross-modal ambiguity in the input instances, and that it is through capturing
this uncertainty that the probabilistic models can perform better at downstream
tasks, such as image-to-text or text-to-image retrieval. Through extensive
experiments on standard and new benchmarks, we show a consistent advantage for
probabilistic representations in cross-modal retrieval, and validate the
ability of our embeddings to capture uncertainty.</p>
</td>
    <td>
      
        Multimodal Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/zhan2022learning/">Learning Discrete Representations Via Constrained Clustering For Effective And Efficient Dense Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Discrete Representations Via Constrained Clustering For Effective And Efficient Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Discrete Representations Via Constrained Clustering For Effective And Efficient Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhan et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining</td>
    <td>32</td>
    <td><p>Dense Retrieval (DR) has achieved state-of-the-art first-stage ranking
effectiveness. However, the efficiency of most existing DR models is limited by
the large memory cost of storing dense vectors and the time-consuming nearest
neighbor search (NNS) in vector space. Therefore, we present RepCONC, a novel
retrieval model that learns discrete Representations via CONstrained
Clustering. RepCONC jointly trains dual-encoders and the Product Quantization
(PQ) method to learn discrete document representations and enables fast
approximate NNS with compact indexes. It models quantization as a constrained
clustering process, which requires the document embeddings to be uniformly
clustered around the quantization centroids and supports end-to-end
optimization of the quantization method and dual-encoders. We theoretically
demonstrate the importance of the uniform clustering constraint in RepCONC and
derive an efficient approximate solution for constrained clustering by reducing
it to an instance of the optimal transport problem. Besides constrained
clustering, RepCONC further adopts a vector-based inverted file system (IVF) to
support highly efficient vector search on CPUs. Extensive experiments on two
popular ad-hoc retrieval benchmarks show that RepCONC achieves better ranking
effectiveness than competitive vector quantization baselines under different
compression ratio settings. It also substantially outperforms a wide range of
existing retrieval models in terms of retrieval effectiveness, memory
efficiency, and time efficiency.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/patel2022recall/">Recall@k Surrogate Loss With Large Batches And Similarity Mixup</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Recall@k Surrogate Loss With Large Batches And Similarity Mixup' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Recall@k Surrogate Loss With Large Batches And Similarity Mixup' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Patel Yash, Tolias Giorgos, Matas Jiri</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>28</td>
    <td><p>This work focuses on learning deep visual representation models for retrieval
by exploring the interplay between a new loss function, the batch size, and a
new regularization approach. Direct optimization, by gradient descent, of an
evaluation metric, is not possible when it is non-differentiable, which is the
case for recall in retrieval. A differentiable surrogate loss for the recall is
proposed in this work. Using an implementation that sidesteps the hardware
constraints of the GPU memory, the method trains with a very large batch size,
which is essential for metrics computed on the entire retrieval database. It is
assisted by an efficient mixup regularization approach that operates on
pairwise scalar similarities and virtually increases the batch size further.
The suggested method achieves state-of-the-art performance in several image
retrieval benchmarks when used for deep metric learning. For instance-level
recognition, the method outperforms similar approaches that train using an
approximation of average precision.</p>
</td>
    <td>
      
        Evaluation 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/partalidou2022improving/">Improving Zero-shot Entity Retrieval Through Effective Dense Representations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Improving Zero-shot Entity Retrieval Through Effective Dense Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Improving Zero-shot Entity Retrieval Through Effective Dense Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Partalidou Eleni, Christou Despina, Tsoumakas Grigorios</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 12th Hellenic Conference on Artificial Intelligence</td>
    <td>5</td>
    <td><p>Entity Linking (EL) seeks to align entity mentions in text to entries in a
knowledge-base and is usually comprised of two phases: candidate generation and
candidate ranking. While most methods focus on the latter, it is the candidate
generation phase that sets an upper bound to both time and accuracy performance
of the overall EL system. This work’s contribution is a significant improvement
in candidate generation which thus raises the performance threshold for EL, by
generating candidates that include the gold entity in the least candidate set
(top-K). We propose a simple approach that efficiently embeds mention-entity
pairs in dense space through a BERT-based bi-encoder. Specifically, we extend
(Wu et al., 2020) by introducing a new pooling function and incorporating
entity type side-information. We achieve a new state-of-the-art 84.28% accuracy
on top-50 candidates on the Zeshel dataset, compared to the previous 82.06% on
the top-64 of (Wu et al., 2020). We report the results from extensive
experimentation using our proposed model on both seen and unseen entity
datasets. Our results suggest that our method could be a useful complement to
existing EL approaches.</p>
</td>
    <td>
      
        Few Shot & Zero Shot 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/ortega2022unconventional/">Unconventional Application Of K-means For Distributed Approximate Similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unconventional Application Of K-means For Distributed Approximate Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unconventional Application Of K-means For Distributed Approximate Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ortega et al.</td> <!-- 🔧 You were missing this -->
    <td>Information Sciences</td>
    <td>5</td>
    <td><p>Similarity search based on a distance function in metric spaces is a
fundamental problem for many applications. Queries for similar objects lead to
the well-known machine learning task of nearest-neighbours identification. Many
data indexing strategies, collectively known as Metric Access Methods (MAM),
have been proposed to speed up queries for similar elements in this context.
Moreover, since exact approaches to solve similarity queries can be complex and
time-consuming, alternative options have appeared to reduce query execution
time, such as returning approximate results or resorting to distributed
computing platforms. In this paper, we introduce MASK (Multilevel Approximate
Similarity search with \(k\)-means), an unconventional application of the
\(k\)-means algorithm as the foundation of a multilevel index structure for
approximate similarity search, suitable for metric spaces. We show that
inherent properties of \(k\)-means, like representing high-density data areas
with fewer prototypes, can be leveraged for this purpose. An implementation of
this new indexing method is evaluated, using a synthetic dataset and a
real-world dataset in a high-dimensional and high-sparsity space. Results are
promising and underpin the applicability of this novel indexing method in
multiple domains.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/neculai2022probabilistic/">Probabilistic Compositional Embeddings For Multimodal Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Probabilistic Compositional Embeddings For Multimodal Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Probabilistic Compositional Embeddings For Multimodal Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Neculai Andrei, Chen Yanbei, Akata Zeynep</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>21</td>
    <td><p>Existing works in image retrieval often consider retrieving images with one
or two query inputs, which do not generalize to multiple queries. In this work,
we investigate a more challenging scenario for composing multiple multimodal
queries in image retrieval. Given an arbitrary number of query images and (or)
texts, our goal is to retrieve target images containing the semantic concepts
specified in multiple multimodal queries. To learn an informative embedding
that can flexibly encode the semantics of various queries, we propose a novel
multimodal probabilistic composer (MPC). Specifically, we model input images
and texts as probabilistic embeddings, which can be further composed by a
probabilistic composition rule to facilitate image retrieval with multiple
multimodal queries. We propose a new benchmark based on the MS-COCO dataset and
evaluate our model on various setups that compose multiple images and (or) text
queries for multimodal image retrieval. Without bells and whistles, we show
that our probabilistic model formulation significantly outperforms existing
related methods on multimodal image retrieval while generalizing well to query
with different amounts of inputs given in arbitrary visual and (or) textual
modalities. Code is available here: https://github.com/andreineculai/MPC.</p>
</td>
    <td>
      
        Image Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/neelakantan2022text/">Text And Code Embeddings By Contrastive Pre-training</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Text And Code Embeddings By Contrastive Pre-training' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Text And Code Embeddings By Contrastive Pre-training' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Neelakantan et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>110</td>
    <td><p>Text embeddings are useful features in many applications such as semantic
search and computing text similarity. Previous work typically trains models
customized for different use cases, varying in dataset choice, training
objective and model architecture. In this work, we show that contrastive
pre-training on unsupervised data at scale leads to high quality vector
representations of text and code. The same unsupervised text embeddings that
achieve new state-of-the-art results in linear-probe classification also
display impressive semantic search capabilities and sometimes even perform
competitively with fine-tuned models. On linear-probe classification accuracy
averaging over 7 tasks, our best unsupervised model achieves a relative
improvement of 4% and 1.8% over previous best unsupervised and supervised text
embedding models respectively. The same text embeddings when evaluated on
large-scale semantic search attains a relative improvement of 23.4%, 14.7%, and
10.6% over previous best unsupervised methods on MSMARCO, Natural Questions and
TriviaQA benchmarks, respectively. Similarly to text embeddings, we train code
embedding models on (text, code) pairs, obtaining a 20.8% relative improvement
over prior best work on code search.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/mikriukov2022deep/">Deep Unsupervised Contrastive Hashing For Large-scale Cross-modal Text-image Retrieval In Remote Sensing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Unsupervised Contrastive Hashing For Large-scale Cross-modal Text-image Retrieval In Remote Sensing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Unsupervised Contrastive Hashing For Large-scale Cross-modal Text-image Retrieval In Remote Sensing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mikriukov Georgii, Ravanbakhsh Mahdyar, Demir Begüm</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>19</td>
    <td><p>Due to the availability of large-scale multi-modal data (e.g., satellite
images acquired by different sensors, text sentences, etc) archives, the
development of cross-modal retrieval systems that can search and retrieve
semantically relevant data across different modalities based on a query in any
modality has attracted great attention in RS. In this paper, we focus our
attention on cross-modal text-image retrieval, where queries from one modality
(e.g., text) can be matched to archive entries from another (e.g., image). Most
of the existing cross-modal text-image retrieval systems require a high number
of labeled training samples and also do not allow fast and memory-efficient
retrieval due to their intrinsic characteristics. These issues limit the
applicability of the existing cross-modal retrieval systems for large-scale
applications in RS. To address this problem, in this paper we introduce a novel
deep unsupervised cross-modal contrastive hashing (DUCH) method for RS
text-image retrieval. The proposed DUCH is made up of two main modules: 1)
feature extraction module (which extracts deep representations of the
text-image modalities); and 2) hashing module (which learns to generate
cross-modal binary hash codes from the extracted representations). Within the
hashing module, we introduce a novel multi-objective loss function including:
i) contrastive objectives that enable similarity preservation in both intra-
and inter-modal similarities; ii) an adversarial objective that is enforced
across two modalities for cross-modal representation consistency; iii)
binarization objectives for generating representative hash codes. Experimental
results show that the proposed DUCH outperforms state-of-the-art unsupervised
cross-modal hashing methods on two multi-modal (image and text) benchmark
archives in RS. Our code is publicly available at
https://git.tu-berlin.de/rsim/duch.</p>
</td>
    <td>
      
        Image Retrieval 
      
        Unsupervised 
      
        Neural Hashing 
      
        SUPERVISED 
      
        Hashing Methods 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/mikriukov2022unsupervised/">An Unsupervised Cross-modal Hashing Method Robust To Noisy Training Image-text Correspondences In Remote Sensing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=An Unsupervised Cross-modal Hashing Method Robust To Noisy Training Image-text Correspondences In Remote Sensing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=An Unsupervised Cross-modal Hashing Method Robust To Noisy Training Image-text Correspondences In Remote Sensing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mikriukov Georgii, Ravanbakhsh Mahdyar, Demir Begüm</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE International Conference on Image Processing (ICIP)</td>
    <td>5</td>
    <td><p>The development of accurate and scalable cross-modal image-text retrieval
methods, where queries from one modality (e.g., text) can be matched to archive
entries from another (e.g., remote sensing image) has attracted great attention
in remote sensing (RS). Most of the existing methods assume that a reliable
multi-modal training set with accurately matched text-image pairs is existing.
However, this assumption may not always hold since the multi-modal training
sets may include noisy pairs (i.e., textual descriptions/captions associated to
training images can be noisy), distorting the learning process of the retrieval
methods. To address this problem, we propose a novel unsupervised cross-modal
hashing method robust to the noisy image-text correspondences (CHNR). CHNR
consists of three modules: 1) feature extraction module, which extracts feature
representations of image-text pairs; 2) noise detection module, which detects
potential noisy correspondences; and 3) hashing module that generates
cross-modal binary hash codes. The proposed CHNR includes two training phases:
i) meta-learning phase that uses a small portion of clean (i.e., reliable) data
to train the noise detection module in an adversarial fashion; and ii) the main
training phase for which the trained noise detection module is used to identify
noisy correspondences while the hashing module is trained on the noisy
multi-modal training set. Experimental results show that the proposed CHNR
outperforms state-of-the-art methods. Our code is publicly available at
https://git.tu-berlin.de/rsim/chnr</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/messina2022aladin/">ALADIN: Distilling Fine-grained Alignment Scores For Efficient Image-text Matching And Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=ALADIN: Distilling Fine-grained Alignment Scores For Efficient Image-text Matching And Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=ALADIN: Distilling Fine-grained Alignment Scores For Efficient Image-text Matching And Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Messina et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 19th International Conference on Content-based Multimedia Indexing</td>
    <td>20</td>
    <td><p>Image-text matching is gaining a leading role among tasks involving the joint
understanding of vision and language. In literature, this task is often used as
a pre-training objective to forge architectures able to jointly deal with
images and texts. Nonetheless, it has a direct downstream application:
cross-modal retrieval, which consists in finding images related to a given
query text or vice-versa. Solving this task is of critical importance in
cross-modal search engines. Many recent methods proposed effective solutions to
the image-text matching problem, mostly using recent large vision-language (VL)
Transformer networks. However, these models are often computationally
expensive, especially at inference time. This prevents their adoption in
large-scale cross-modal retrieval scenarios, where results should be provided
to the user almost instantaneously. In this paper, we propose to fill in the
gap between effectiveness and efficiency by proposing an ALign And DIstill
Network (ALADIN). ALADIN first produces high-effective scores by aligning at
fine-grained level images and texts. Then, it learns a shared embedding space -
where an efficient kNN search can be performed - by distilling the relevance
scores obtained from the fine-grained alignments. We obtained remarkable
results on MS-COCO, showing that our method can compete with state-of-the-art
VL Transformers while being almost 90 times faster. The code for reproducing
our results is available at https://github.com/mesnico/ALADIN.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/weinzaepfel2022learning/">Learning Super-features For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Super-features For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Super-features For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Weinzaepfel et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>11</td>
    <td><p>Methods that combine local and global features have recently shown excellent
performance on multiple challenging deep image retrieval benchmarks, but their
use of local features raises at least two issues. First, these local features
simply boil down to the localized map activations of a neural network, and
hence can be extremely redundant. Second, they are typically trained with a
global loss that only acts on top of an aggregation of local features; by
contrast, testing is based on local feature matching, which creates a
discrepancy between training and testing. In this paper, we propose a novel
architecture for deep image retrieval, based solely on mid-level features that
we call Super-features. These Super-features are constructed by an iterative
attention module and constitute an ordered set in which each element focuses on
a localized and discriminant image pattern. For training, they require only
image labels. A contrastive loss operates directly at the level of
Super-features and focuses on those that match across images. A second
complementary loss encourages diversity. Experiments on common landmark
retrieval benchmarks validate that Super-features substantially outperform
state-of-the-art methods when using the same number of features, and only
require a significantly smaller memory footprint to match their performance.
Code and models are available at: https://github.com/naver/FIRe.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/wei2022accurate/">Accurate Instance-level CAD Model Retrieval In A Large-scale Database</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Accurate Instance-level CAD Model Retrieval In A Large-scale Database' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Accurate Instance-level CAD Model Retrieval In A Large-scale Database' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wei et al.</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</td>
    <td>5</td>
    <td><p>We present a new solution to the fine-grained retrieval of clean CAD models
from a large-scale database in order to recover detailed object shape
geometries for RGBD scans. Unlike previous work simply indexing into a
moderately small database using an object shape descriptor and accepting the
top retrieval result, we argue that in the case of a large-scale database a
more accurate model may be found within a neighborhood of the descriptor. More
importantly, we propose that the distinctiveness deficiency of shape
descriptors at the instance level can be compensated by a geometry-based
re-ranking of its neighborhood. Our approach first leverages the discriminative
power of learned representations to distinguish between different categories of
models and then uses a novel robust point set distance metric to re-rank the
CAD neighborhood, enabling fine-grained retrieval in a large shape database.
Evaluation on a real-world dataset shows that our geometry-based re-ranking is
a conceptually simple but highly effective method that can lead to a
significant improvement in retrieval accuracy compared to the state-of-the-art.</p>
</td>
    <td>
      
        IROS 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/wei2022hyperbolic/">Hyperbolic Hierarchical Contrastive Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hyperbolic Hierarchical Contrastive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hyperbolic Hierarchical Contrastive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wei et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence</td>
    <td>19</td>
    <td><p>Hierarchical semantic structures, naturally existing in real-world datasets,
can assist in capturing the latent distribution of data to learn robust hash
codes for retrieval systems. Although hierarchical semantic structures can be
simply expressed by integrating semantically relevant data into a high-level
taxon with coarser-grained semantics, the construction, embedding, and
exploitation of the structures remain tricky for unsupervised hash learning. To
tackle these problems, we propose a novel unsupervised hashing method named
Hyperbolic Hierarchical Contrastive Hashing (HHCH). We propose to embed
continuous hash codes into hyperbolic space for accurate semantic expression
since embedding hierarchies in hyperbolic space generates less distortion than
in hyper-sphere space and Euclidean space. In addition, we extend the K-Means
algorithm to hyperbolic space and perform the proposed hierarchical hyperbolic
K-Means algorithm to construct hierarchical semantic structures adaptively. To
exploit the hierarchical semantic structures in hyperbolic space, we designed
the hierarchical contrastive learning algorithm, including hierarchical
instance-wise and hierarchical prototype-wise contrastive learning. Extensive
experiments on four benchmark datasets demonstrate that the proposed method
outperforms the state-of-the-art unsupervised hashing methods. Codes will be
released.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        AAAI 
      
        IJCAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/wang2022text/">Text Embeddings By Weakly-supervised Contrastive Pre-training</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Text Embeddings By Weakly-supervised Contrastive Pre-training' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Text Embeddings By Weakly-supervised Contrastive Pre-training' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>74</td>
    <td><p>This paper presents E5, a family of state-of-the-art text embeddings that
transfer well to a wide range of tasks. The model is trained in a contrastive
manner with weak supervision signals from our curated large-scale text pair
dataset (called CCPairs). E5 can be readily used as a general-purpose embedding
model for any tasks requiring a single-vector representation of texts such as
retrieval, clustering, and classification, achieving strong performance in both
zero-shot and fine-tuned settings. We conduct extensive evaluations on 56
datasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the
first model that outperforms the strong BM25 baseline on the BEIR retrieval
benchmark without using any labeled data. When fine-tuned, E5 obtains the best
results on the MTEB benchmark, beating existing embedding models with 40x more
parameters.</p>
</td>
    <td>
      
        SUPERVISED 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/xuan2022dissecting/">Dissecting The Impact Of Different Loss Functions With Gradient Surgery</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Dissecting The Impact Of Different Loss Functions With Gradient Surgery' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Dissecting The Impact Of Different Loss Functions With Gradient Surgery' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xuan Hong, Pless Robert</td> <!-- 🔧 You were missing this -->
    <td>Journal of Property Research</td>
    <td>27</td>
    <td><p>Pair-wise loss is an approach to metric learning that learns a semantic
embedding by optimizing a loss function that encourages images from the same
semantic class to be mapped closer than images from different classes. The
literature reports a large and growing set of variations of the pair-wise loss
strategies. Here we decompose the gradient of these loss functions into
components that relate to how they push the relative feature positions of the
anchor-positive and anchor-negative pairs. This decomposition allows the
unification of a large collection of current pair-wise loss functions.
Additionally, explicitly constructing pair-wise gradient updates to separate
out these effects gives insights into which have the biggest impact, and leads
to a simple algorithm that beats the state of the art for image retrieval on
the CAR, CUB and Stanford Online products datasets.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/wang2022hybrid/">Hybrid Contrastive Quantization For Efficient Cross-view Video Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hybrid Contrastive Quantization For Efficient Cross-view Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hybrid Contrastive Quantization For Efficient Cross-view Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the ACM Web Conference 2022</td>
    <td>5</td>
    <td><p>With the recent boom of video-based social platforms (e.g., YouTube and
TikTok), video retrieval using sentence queries has become an important demand
and attracts increasing research attention. Despite the decent performance,
existing text-video retrieval models in vision and language communities are
impractical for large-scale Web search because they adopt brute-force search
based on high-dimensional embeddings. To improve efficiency, Web search engines
widely apply vector compression libraries (e.g., FAISS) to post-process the
learned embeddings. Unfortunately, separate compression from feature encoding
degrades the robustness of representations and incurs performance decay. To
pursue a better balance between performance and efficiency, we propose the
first quantized representation learning method for cross-view video retrieval,
namely Hybrid Contrastive Quantization (HCQ). Specifically, HCQ learns both
coarse-grained and fine-grained quantizations with transformers, which provide
complementary understandings for texts and videos and preserve comprehensive
semantic information. By performing Asymmetric-Quantized Contrastive Learning
(AQ-CL) across views, HCQ aligns texts and videos at coarse-grained and
multiple fine-grained levels. This hybrid-grained learning strategy serves as
strong supervision on the cross-view video quantization model, where
contrastive learning at different levels can be mutually promoted. Extensive
experiments on three Web video benchmark datasets demonstrate that HCQ achieves
competitive performance with state-of-the-art non-compressed retrieval methods
while showing high efficiency in storage and computation. Code and
configurations are available at https://github.com/gimpong/WWW22-HCQ.</p>
</td>
    <td>
      
        Quantization 
      
        Video Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/wang2022english/">English Contrastive Learning Can Learn Universal Cross-lingual Sentence Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=English Contrastive Learning Can Learn Universal Cross-lingual Sentence Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=English Contrastive Learning Can Learn Universal Cross-lingual Sentence Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Yau-shian, Wu Ashley, Neubig Graham</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</td>
    <td>9</td>
    <td><p>Universal cross-lingual sentence embeddings map semantically similar
cross-lingual sentences into a shared embedding space. Aligning cross-lingual
sentence embeddings usually requires supervised cross-lingual parallel
sentences. In this work, we propose mSimCSE, which extends SimCSE to
multilingual settings and reveal that contrastive learning on English data can
surprisingly learn high-quality universal cross-lingual sentence embeddings
without any parallel data. In unsupervised and weakly supervised settings,
mSimCSE significantly improves previous sentence embedding methods on
cross-lingual retrieval and multilingual STS tasks. The performance of
unsupervised mSimCSE is comparable to fully supervised methods in retrieving
low-resource languages and multilingual STS. The performance can be further
enhanced when cross-lingual NLI data is available. Our code is publicly
available at https://github.com/yaushian/mSimCSE.</p>
</td>
    <td>
      
        Self SUPERVISED 
      
        EMNLP 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/wang2022navigable/">Navigable Proximity Graph-driven Native Hybrid Queries With Structured And Unstructured Constraints</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Navigable Proximity Graph-driven Native Hybrid Queries With Structured And Unstructured Constraints' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Navigable Proximity Graph-driven Native Hybrid Queries With Structured And Unstructured Constraints' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</td>
    <td>7</td>
    <td><p>As research interest surges, vector similarity search is applied in multiple
fields, including data mining, computer vision, and information retrieval.
{Given a set of objects (e.g., a set of images) and a query object, we can
easily transform each object into a feature vector and apply the vector
similarity search to retrieve the most similar objects. However, the original
vector similarity search cannot well support \textit{hybrid queries}, where
users not only input unstructured query constraint (i.e., the feature vector of
query object) but also structured query constraint (i.e., the desired
attributes of interest). Hybrid query processing aims at identifying these
objects with similar feature vectors to query object and satisfying the given
attribute constraints. Recent efforts have attempted to answer a hybrid query
by performing attribute filtering and vector similarity search separately and
then merging the results later, which limits efficiency and accuracy because
they are not purpose-built for hybrid queries.} In this paper, we propose a
native hybrid query (NHQ) framework based on proximity graph (PG), which
provides the specialized \textit{composite index and joint pruning} modules for
hybrid queries. We easily deploy existing various PGs on this framework to
process hybrid queries efficiently. Moreover, we present two novel navigable
PGs (NPGs) with optimized edge selection and routing strategies, which obtain
better overall performance than existing PGs. After that, we deploy the
proposed NPGs in NHQ to form two hybrid query methods, which significantly
outperform the state-of-the-art competitors on all experimental datasets
(10\(\times\) faster under the same \textit{Recall}), including eight public and
one in-house real-world datasets. Our code and datasets have been released at
https://github.com/AshenOn3/NHQ.</p>
</td>
    <td>
      
        Graph Based ANN 
      
        CIKM 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/wang2022coder/">CODER: Coupled Diversity-sensitive Momentum Contrastive Learning For Image-text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=CODER: Coupled Diversity-sensitive Momentum Contrastive Learning For Image-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=CODER: Coupled Diversity-sensitive Momentum Contrastive Learning For Image-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>17</td>
    <td><p>Image-Text Retrieval (ITR) is challenging in bridging visual and lingual
modalities. Contrastive learning has been adopted by most prior arts. Except
for limited amount of negative image-text pairs, the capability of constrastive
learning is restricted by manually weighting negative pairs as well as
unawareness of external knowledge. In this paper, we propose our novel Coupled
Diversity-Sensitive Momentum Constrastive Learning (CODER) for improving
cross-modal representation. Firstly, a novel diversity-sensitive contrastive
learning (DCL) architecture is invented. We introduce dynamic dictionaries for
both modalities to enlarge the scale of image-text pairs, and
diversity-sensitiveness is achieved by adaptive negative pair weighting.
Furthermore, two branches are designed in CODER. One learns instance-level
embeddings from image/text, and it also generates pseudo online clustering
labels for its input image/text based on their embeddings. Meanwhile, the other
branch learns to query from commonsense knowledge graph to form concept-level
descriptors for both modalities. Afterwards, both branches leverage DCL to
align the cross-modal embedding spaces while an extra pseudo clustering label
prediction loss is utilized to promote concept-level representation learning
for the second branch. Extensive experiments conducted on two popular
benchmarks, i.e. MSCOCO and Flicker30K, validate CODER remarkably outperforms
the state-of-the-art approaches.</p>
</td>
    <td>
      
        Self SUPERVISED 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/wang2022binary/">Binary Representation Via Jointly Personalized Sparse Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Binary Representation Via Jointly Personalized Sparse Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Binary Representation Via Jointly Personalized Sparse Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>ACM Transactions on Multimedia Computing, Communications, and Applications</td>
    <td>18</td>
    <td><p>Unsupervised hashing has attracted much attention for binary representation
learning due to the requirement of economical storage and efficiency of binary
codes. It aims to encode high-dimensional features in the Hamming space with
similarity preservation between instances. However, most existing methods learn
hash functions in manifold-based approaches. Those methods capture the local
geometric structures (i.e., pairwise relationships) of data, and lack
satisfactory performance in dealing with real-world scenarios that produce
similar features (e.g. color and shape) with different semantic information. To
address this challenge, in this work, we propose an effective unsupervised
method, namely Jointly Personalized Sparse Hashing (JPSH), for binary
representation learning. To be specific, firstly, we propose a novel
personalized hashing module, i.e., Personalized Sparse Hashing (PSH). Different
personalized subspaces are constructed to reflect category-specific attributes
for different clusters, adaptively mapping instances within the same cluster to
the same Hamming space. In addition, we deploy sparse constraints for different
personalized subspaces to select important features. We also collect the
strengths of the other clusters to build the PSH module with avoiding
over-fitting. Then, to simultaneously preserve semantic and pairwise
similarities in our JPSH, we incorporate the PSH and manifold-based hash
learning into the seamless formulation. As such, JPSH not only distinguishes
the instances from different clusters, but also preserves local neighborhood
structures within the cluster. Finally, an alternating optimization algorithm
is adopted to iteratively capture analytical solutions of the JPSH model.
Extensive experiments on four benchmark datasets verify that the JPSH
outperforms several hashing algorithms on the similarity search task.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/wang2022cgat/">Cgat: Center-guided Adversarial Training For Deep Hashing-based Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cgat: Center-guided Adversarial Training For Deep Hashing-based Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cgat: Center-guided Adversarial Training For Deep Hashing-based Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Xunguang, Lin Yiqun, Li Xiaomeng</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the ACM Web Conference 2023</td>
    <td>5</td>
    <td><p>Deep hashing has been extensively utilized in massive image retrieval because
of its efficiency and effectiveness. However, deep hashing models are
vulnerable to adversarial examples, making it essential to develop adversarial
defense methods for image retrieval. Existing solutions achieved limited
defense performance because of using weak adversarial samples for training and
lacking discriminative optimization objectives to learn robust features. In
this paper, we present a min-max based Center-guided Adversarial Training,
namely CgAT, to improve the robustness of deep hashing networks through worst
adversarial examples. Specifically, we first formulate the center code as a
semantically-discriminative representative of the input image content, which
preserves the semantic similarity with positive samples and dissimilarity with
negative examples. We prove that a mathematical formula can calculate the
center code immediately. After obtaining the center codes in each optimization
iteration of the deep hashing network, they are adopted to guide the
adversarial training process. On the one hand, CgAT generates the worst
adversarial examples as augmented data by maximizing the Hamming distance
between the hash codes of the adversarial examples and the center codes. On the
other hand, CgAT learns to mitigate the effects of adversarial samples by
minimizing the Hamming distance to the center codes. Extensive experiments on
the benchmark datasets demonstrate the effectiveness of our adversarial
training algorithm in defending against adversarial attacks for deep
hashing-based retrieval. Compared with the current state-of-the-art defense
method, we significantly improve the defense performance by an average of
18.61%, 12.35%, and 11.56% on FLICKR-25K, NUS-WIDE, and MS-COCO,
respectively. The code is available at https://github.com/xunguangwang/CgAT.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/wang2022cross/">Cross-lingual Cross-modal Retrieval With Noise-robust Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-lingual Cross-modal Retrieval With Noise-robust Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-lingual Cross-modal Retrieval With Noise-robust Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 30th ACM International Conference on Multimedia</td>
    <td>18</td>
    <td><p>Despite the recent developments in the field of cross-modal retrieval, there
has been less research focusing on low-resource languages due to the lack of
manually annotated datasets. In this paper, we propose a noise-robust
cross-lingual cross-modal retrieval method for low-resource languages. To this
end, we use Machine Translation (MT) to construct pseudo-parallel sentence
pairs for low-resource languages. However, as MT is not perfect, it tends to
introduce noise during translation, rendering textual embeddings corrupted and
thereby compromising the retrieval performance. To alleviate this, we introduce
a multi-view self-distillation method to learn noise-robust target-language
representations, which employs a cross-attention module to generate soft
pseudo-targets to provide direct supervision from the similarity-based view and
feature-based view. Besides, inspired by the back-translation in unsupervised
MT, we minimize the semantic discrepancies between origin sentences and
back-translated sentences to further improve the noise robustness of the
textual encoder. Extensive experiments are conducted on three video-text and
image-text cross-modal retrieval benchmarks across different languages, and the
results demonstrate that our method significantly improves the overall
performance without using extra human-labeled data. In addition, equipped with
a pre-trained visual encoder from a recent vision-and-language pre-training
framework, i.e., CLIP, our model achieves a significant performance gain,
showing that our method is compatible with popular pre-training models. Code
and data are available at https://github.com/HuiGuanLab/nrccr.</p>
</td>
    <td>
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/wang2022neural/">A Neural Corpus Indexer For Document Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Neural Corpus Indexer For Document Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Neural Corpus Indexer For Document Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>43</td>
    <td><p>Current state-of-the-art document retrieval solutions mainly follow an
index-retrieve paradigm, where the index is hard to be directly optimized for
the final retrieval target. In this paper, we aim to show that an end-to-end
deep neural network unifying training and indexing stages can significantly
improve the recall performance of traditional methods. To this end, we propose
Neural Corpus Indexer (NCI), a sequence-to-sequence network that generates
relevant document identifiers directly for a designated query. To optimize the
recall performance of NCI, we invent a prefix-aware weight-adaptive decoder
architecture, and leverage tailored techniques including query generation,
semantic document identifiers, and consistency-based regularization. Empirical
studies demonstrated the superiority of NCI on two commonly used academic
benchmarks, achieving +21.4% and +16.8% relative enhancement for Recall@1 on
NQ320k dataset and R-Precision on TriviaQA dataset, respectively, compared to
the best baseline method.</p>
</td>
    <td>
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/xue2022cross/">Cross-scale Context Extracted Hashing For Fine-grained Image Binary Encoding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-scale Context Extracted Hashing For Fine-grained Image Binary Encoding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-scale Context Extracted Hashing For Fine-grained Image Binary Encoding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xue et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>Deep hashing has been widely applied to large-scale image retrieval tasks
owing to efficient computation and low storage cost by encoding
high-dimensional image data into binary codes. Since binary codes do not
contain as much information as float features, the essence of binary encoding
is preserving the main context to guarantee retrieval quality. However, the
existing hashing methods have great limitations on suppressing redundant
background information and accurately encoding from Euclidean space to Hamming
space by a simple sign function. In order to solve these problems, a
Cross-Scale Context Extracted Hashing Network (CSCE-Net) is proposed in this
paper. Firstly, we design a two-branch framework to capture fine-grained local
information while maintaining high-level global semantic information. Besides,
Attention guided Information Extraction module (AIE) is introduced between two
branches, which suppresses areas of low context information cooperated with
global sliding windows. Unlike previous methods, our CSCE-Net learns a
content-related Dynamic Sign Function (DSF) to replace the original simple sign
function. Therefore, the proposed CSCE-Net is context-sensitive and able to
perform well on accurate image binary encoding. We further demonstrate that our
CSCE-Net is superior to the existing hashing methods, which improves retrieval
performance on standard benchmarks.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/maji2021cbir/">CBIR Using Features Derived By Deep Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=CBIR Using Features Derived By Deep Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=CBIR Using Features Derived By Deep Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Maji Subhadip, Bose Smarajit</td> <!-- 🔧 You were missing this -->
    <td>ACM/IMS Transactions on Data Science</td>
    <td>10</td>
    <td><p>In a Content Based Image Retrieval (CBIR) System, the task is to retrieve
similar images from a large database given a query image. The usual procedure
is to extract some useful features from the query image, and retrieve images
which have similar set of features. For this purpose, a suitable similarity
measure is chosen, and images with high similarity scores are retrieved.
Naturally the choice of these features play a very important role in the
success of this system, and high level features are required to reduce the
semantic gap.
  In this paper, we propose to use features derived from pre-trained network
models from a deep-learning convolution network trained for a large image
classification problem. This approach appears to produce vastly superior
results for a variety of databases, and it outperforms many contemporary CBIR
systems. We analyse the retrieval time of the method, and also propose a
pre-clustering of the database based on the above-mentioned features which
yields comparable results in a much shorter time in most of the cases.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/maheshwari2021scene/">Scene Graph Embeddings Using Relative Similarity Supervision</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Scene Graph Embeddings Using Relative Similarity Supervision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Scene Graph Embeddings Using Relative Similarity Supervision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Maheshwari Paridhi, Chaudhry Ritwick, Vinay Vishwa</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>10</td>
    <td><p>Scene graphs are a powerful structured representation of the underlying
content of images, and embeddings derived from them have been shown to be
useful in multiple downstream tasks. In this work, we employ a graph
convolutional network to exploit structure in scene graphs and produce image
embeddings useful for semantic image retrieval. Different from
classification-centric supervision traditionally available for learning image
representations, we address the task of learning from relative similarity
labels in a ranking context. Rooted within the contrastive learning paradigm,
we propose a novel loss function that operates on pairs of similar and
dissimilar images and imposes relative ordering between them in embedding
space. We demonstrate that this Ranking loss, coupled with an intuitive triple
sampling strategy, leads to robust representations that outperform well-known
contrastive losses on the retrieval task. In addition, we provide qualitative
evidence of how retrieved results that utilize structured scene information
capture the global context of the scene, different from visual similarity
search.</p>
</td>
    <td>
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/malali2021learning/">Learning To Embed Semantic Similarity For Joint Image-text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning To Embed Semantic Similarity For Joint Image-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning To Embed Semantic Similarity For Joint Image-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Malali Noam, Keller Yosi</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>8</td>
    <td><p>We present a deep learning approach for learning the joint semantic
embeddings of images and captions in a Euclidean space, such that the semantic
similarity is approximated by the L2 distances in the embedding space. For
that, we introduce a metric learning scheme that utilizes multitask learning to
learn the embedding of identical semantic concepts using a center loss. By
introducing a differentiable quantization scheme into the end-to-end trainable
network, we derive a semantic embedding of semantically similar concepts in
Euclidean space. We also propose a novel metric learning formulation using an
adaptive margin hinge loss, that is refined during the training phase. The
proposed scheme was applied to the MS-COCO, Flicke30K and Flickr8K datasets,
and was shown to compare favorably with contemporary state-of-the-art
approaches.</p>
</td>
    <td>
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/mafla2021stacmr/">Stacmr: Scene-text Aware Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Stacmr: Scene-text Aware Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Stacmr: Scene-text Aware Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mafla et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>25</td>
    <td><p>Recent models for cross-modal retrieval have benefited from an increasingly
rich understanding of visual scenes, afforded by scene graphs and object
interactions to mention a few. This has resulted in an improved matching
between the visual representation of an image and the textual representation of
its caption. Yet, current visual representations overlook a key aspect: the
text appearing in images, which may contain crucial information for retrieval.
In this paper, we first propose a new dataset that allows exploration of
cross-modal retrieval where images contain scene-text instances. Then, armed
with this dataset, we describe several approaches which leverage scene text,
including a better scene-text aware cross-modal retrieval method which uses
specialized representations for text from the captions and text from the visual
scene, and reconcile them in a common embedding space. Extensive experiments
confirm that cross-modal retrieval approaches benefit from scene text and
highlight interesting research questions worth exploring further. Dataset and
code are available at http://europe.naverlabs.com/stacmr</p>
</td>
    <td>
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/macdonald2021approximate/">On Approximate Nearest Neighbour Selection For Multi-stage Dense Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=On Approximate Nearest Neighbour Selection For Multi-stage Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=On Approximate Nearest Neighbour Selection For Multi-stage Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Macdonald Craig, Tonellotto Nicola</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</td>
    <td>14</td>
    <td><p>Dense retrieval, which describes the use of contextualised language models
such as BERT to identify documents from a collection by leveraging approximate
nearest neighbour (ANN) techniques, has been increasing in popularity. Two
families of approaches have emerged, depending on whether documents and queries
are represented by single or multiple embeddings. ColBERT, the exemplar of the
latter, uses an ANN index and approximate scores to identify a set of candidate
documents for each query embedding, which are then re-ranked using accurate
document representations. In this manner, a large number of documents can be
retrieved for each query, hindering the efficiency of the approach. In this
work, we investigate the use of ANN scores for ranking the candidate documents,
in order to decrease the number of candidate documents being fully scored.
Experiments conducted on the MSMARCO passage ranking corpus demonstrate that,
by cutting of the candidate set by using the approximate scores to only 200
documents, we can still obtain an effective ranking without statistically
significant differences in effectiveness, and resulting in a 2x speedup in
efficiency.</p>
</td>
    <td>
      
        Similarity Search 
      
        CIKM 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/zhan2021weakly/">Weakly-supervised Online Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Weakly-supervised Online Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Weakly-supervised Online Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhan et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE International Conference on Multimedia and Expo (ICME)</td>
    <td>8</td>
    <td><p>With the rapid development of social websites, recent years have witnessed an
explosive growth of social images with user-provided tags which continuously
arrive in a streaming fashion. Due to the fast query speed and low storage
cost, hashing-based methods for image search have attracted increasing
attention. However, existing hashing methods for social image retrieval are
based on batch mode which violates the nature of social images, i.e., social
images are usually generated periodically or collected in a stream fashion.
Although there exist many online image hashing methods, they either adopt
unsupervised learning which ignore the relevant tags, or are designed in the
supervised manner which needs high-quality labels. In this paper, to overcome
the above limitations, we propose a new method named Weakly-supervised Online
Hashing (WOH). In order to learn high-quality hash codes, WOH exploits the weak
supervision by considering the semantics of tags and removing the noise.
Besides, We develop a discrete online optimization algorithm for WOH, which is
efficient and scalable. Extensive experiments conducted on two real-world
datasets demonstrate the superiority of WOH compared with several
state-of-the-art hashing baselines.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/touvron2021grafit/">Grafit: Learning Fine-grained Image Representations With Coarse Labels</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Grafit: Learning Fine-grained Image Representations With Coarse Labels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Grafit: Learning Fine-grained Image Representations With Coarse Labels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Touvron et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>57</td>
    <td><p>This paper tackles the problem of learning a finer representation than the
one provided by training labels. This enables fine-grained category retrieval
of images in a collection annotated with coarse labels only.
  Our network is learned with a nearest-neighbor classifier objective, and an
instance loss inspired by self-supervised learning. By jointly leveraging the
coarse labels and the underlying fine-grained latent space, it significantly
improves the accuracy of category-level retrieval methods.
  Our strategy outperforms all competing methods for retrieving or classifying
images at a finer granularity than that available at train time. It also
improves the accuracy for transfer learning tasks to fine-grained datasets,
thereby establishing the new state of the art on five public benchmarks, like
iNaturalist-2018.</p>
</td>
    <td>
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/silavong2021senatus/">Senatus -- A Fast And Accurate Code-to-code Recommendation Engine</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Senatus -- A Fast And Accurate Code-to-code Recommendation Engine' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Senatus -- A Fast And Accurate Code-to-code Recommendation Engine' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Silavong et al.</td> <!-- 🔧 You were missing this -->
    <td>13th AIAA/CEAS Aeroacoustics Conference  (28th AIAA Aeroacoustics Conference)</td>
    <td>6</td>
    <td><p>Machine learning on source code (MLOnCode) is a popular research field that
has been driven by the availability of large-scale code repositories and the
development of powerful probabilistic and deep learning models for mining
source code. Code-to-code recommendation is a task in MLOnCode that aims to
recommend relevant, diverse and concise code snippets that usefully extend the
code currently being written by a developer in their development environment
(IDE). Code-to-code recommendation engines hold the promise of increasing
developer productivity by reducing context switching from the IDE and
increasing code-reuse. Existing code-to-code recommendation engines do not
scale gracefully to large codebases, exhibiting a linear growth in query time
as the code repository increases in size. In addition, existing code-to-code
recommendation engines fail to account for the global statistics of code
repositories in the ranking function, such as the distribution of code snippet
lengths, leading to sub-optimal retrieval results. We address both of these
weaknesses with <em>Senatus</em>, a new code-to-code recommendation engine. At
the core of Senatus is <em>De-Skew</em> LSH a new locality sensitive hashing
(LSH) algorithm that indexes the data for fast (sub-linear time) retrieval
while also counteracting the skewness in the snippet length distribution using
novel abstract syntax tree-based feature scoring and selection algorithms. We
evaluate Senatus and find the recommendations to be of higher quality than
competing baselines, while achieving faster search. For example on the
CodeSearchNet dataset Senatus improves performance by 31.21% F1 and
147.9<em>x</em> faster query time compared to Facebook Aroma. Senatus also
outperforms standard MinHash LSH by 29.2% F1 and 51.02<em>x</em> faster query
time.</p>
</td>
    <td>
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/ma2021rank/">Rank-consistency Deep Hashing For Scalable Multi-label Image Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Rank-consistency Deep Hashing For Scalable Multi-label Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Rank-consistency Deep Hashing For Scalable Multi-label Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ma Cheng, Lu Jiwen, Zhou Jie</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>14</td>
    <td><p>As hashing becomes an increasingly appealing technique for large-scale image
retrieval, multi-label hashing is also attracting more attention for the
ability to exploit multi-level semantic contents. In this paper, we propose a
novel deep hashing method for scalable multi-label image search. Unlike
existing approaches with conventional objectives such as contrast and triplet
losses, we employ a rank list, rather than pairs or triplets, to provide
sufficient global supervision information for all the samples. Specifically, a
new rank-consistency objective is applied to align the similarity orders from
two spaces, the original space and the hamming space. A powerful loss function
is designed to penalize the samples whose semantic similarity and hamming
distance are mismatched in two spaces. Besides, a multi-label softmax
cross-entropy loss is presented to enhance the discriminative power with a
concise formulation of the derivative function. In order to manipulate the
neighborhood structure of the samples with different labels, we design a
multi-label clustering loss to cluster the hashing vectors of the samples with
the same labels by reducing the distances between the samples and their
multiple corresponding class centers. The state-of-the-art experimental results
achieved on three public multi-label datasets, MIRFLICKR-25K, IAPRTC12 and
NUS-WIDE, demonstrate the effectiveness of the proposed method.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/zaratiana2021contrastive/">Contrastive String Representation Learning Using Synthetic Data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Contrastive String Representation Learning Using Synthetic Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Contrastive String Representation Learning Using Synthetic Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zaratiana Urchade</td> <!-- 🔧 You were missing this -->
    <td>International Journal of Automation and Computing</td>
    <td>6</td>
    <td><p>String representation Learning (SRL) is an important task in the field of
Natural Language Processing, but it remains under-explored. The goal of SRL is
to learn dense and low-dimensional vectors (or embeddings) for encoding
character sequences. The learned representation from this task can be used in
many downstream application tasks such as string similarity matching or lexical
normalization. In this paper, we propose a new method for to train a SRL model
by only using synthetic data. Our approach makes use of Contrastive Learning in
order to maximize similarity between related strings while minimizing it for
unrelated strings. We demonstrate the effectiveness of our approach by
evaluating the learned representation on the task of string similarity
matching. Codes, data and pretrained models will be made publicly available.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/luo2021cimon/">CIMON: Towards High-quality Hash Codes</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=CIMON: Towards High-quality Hash Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=CIMON: Towards High-quality Hash Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Luo et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence</td>
    <td>26</td>
    <td><p>Recently, hashing is widely used in approximate nearest neighbor search for
its storage and computational efficiency. Most of the unsupervised hashing
methods learn to map images into semantic similarity-preserving hash codes by
constructing local semantic similarity structure from the pre-trained model as
the guiding information, i.e., treating each point pair similar if their
distance is small in feature space. However, due to the inefficient
representation ability of the pre-trained model, many false positives and
negatives in local semantic similarity will be introduced and lead to error
propagation during the hash code learning. Moreover, few of the methods
consider the robustness of models, which will cause instability of hash codes
to disturbance. In this paper, we propose a new method named
{\textbf{C}}omprehensive s{\textbf{I}}milarity {\textbf{M}}ining and
c{\textbf{O}}nsistency lear{\textbf{N}}ing (CIMON). First, we use global
refinement and similarity statistical distribution to obtain reliable and
smooth guidance. Second, both semantic and contrastive consistency learning are
introduced to derive both disturb-invariant and discriminative hash codes.
Extensive experiments on several benchmark datasets show that the proposed
method outperforms a wide range of state-of-the-art methods in both retrieval
performance and robustness.</p>
</td>
    <td>
      
        Hashing Methods 
      
        AAAI 
      
        IJCAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/tahmasebzadeh2021geowine/">Geowine: Geolocation Based Wiki, Image,news And Event Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Geowine: Geolocation Based Wiki, Image,news And Event Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Geowine: Geolocation Based Wiki, Image,news And Event Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tahmasebzadeh et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>7</td>
    <td><p>In the context of social media, geolocation inference on news or events has
become a very important task. In this paper, we present the GeoWINE
(Geolocation-based Wiki-Image-News-Event retrieval) demonstrator, an effective
modular system for multimodal retrieval which expects only a single image as
input. The GeoWINE system consists of five modules in order to retrieve related
information from various sources. The first module is a state-of-the-art model
for geolocation estimation of images. The second module performs a
geospatial-based query for entity retrieval using the Wikidata knowledge graph.
The third module exploits four different image embedding representations, which
are used to retrieve most similar entities compared to the input image. The
embeddings are derived from the tasks of geolocation estimation, place
recognition, ImageNet-based image classification, and their combination. The
last two modules perform news and event retrieval from EventRegistry and the
Open Event Knowledge Graph (OEKG). GeoWINE provides an intuitive interface for
end-users and is insightful for experts for reconfiguration to individual
setups. The GeoWINE achieves promising results in entity label prediction for
images on Google Landmarks dataset. The demonstrator is publicly available at
http://cleopatra.ijs.si/geowine/.</p>
</td>
    <td>
      
        SIGIR 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/torres2021compact/">Compact And Effective Representations For Sketch-based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Compact And Effective Representations For Sketch-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Compact And Effective Representations For Sketch-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Torres Pablo, Saavedra Jose M.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>12</td>
    <td><p>Sketch-based image retrieval (SBIR) has undergone an increasing interest in
the community of computer vision bringing high impact in real applications. For
instance, SBIR brings an increased benefit to eCommerce search engines because
it allows users to formulate a query just by drawing what they need to buy.
However, current methods showing high precision in retrieval work in a high
dimensional space, which negatively affects aspects like memory consumption and
time processing. Although some authors have also proposed compact
representations, these drastically degrade the performance in a low dimension.
Therefore in this work, we present different results of evaluating methods for
producing compact embeddings in the context of sketch-based image retrieval.
Our main interest is in strategies aiming to keep the local structure of the
original space. The recent unsupervised local-topology preserving dimension
reduction method UMAP fits our requirements and shows outstanding performance,
improving even the precision achieved by SOTA methods. We evaluate six methods
in two different datasets. We use Flickr15K and eCommerce datasets; the latter
is another contribution of this work. We show that UMAP allows us to have
feature vectors of 16 bytes improving precision by more than 35%.</p>
</td>
    <td>
      
        Image Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/lu2021domain/">Domain-aware SE Network For Sketch-based Image Retrieval With Multiplicative Euclidean Margin Softmax</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Domain-aware SE Network For Sketch-based Image Retrieval With Multiplicative Euclidean Margin Softmax' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Domain-aware SE Network For Sketch-based Image Retrieval With Multiplicative Euclidean Margin Softmax' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 29th ACM International Conference on Multimedia</td>
    <td>12</td>
    <td><p>This paper proposes a novel approach for Sketch-Based Image Retrieval (SBIR),
for which the key is to bridge the gap between sketches and photos in terms of
the data representation. Inspired by channel-wise attention explored in recent
years, we present a Domain-Aware Squeeze-and-Excitation (DASE) network, which
seamlessly incorporates the prior knowledge of sample sketch or photo into SE
module and make the SE module capable of emphasizing appropriate channels
according to domain signal. Accordingly, the proposed network can switch its
mode to achieve a better domain feature with lower intra-class discrepancy.
Moreover, while previous works simply focus on minimizing intra-class distance
and maximizing inter-class distance, we introduce a loss function, named
Multiplicative Euclidean Margin Softmax (MEMS), which introduces multiplicative
Euclidean margin into feature space and ensure that the maximum intra-class
distance is smaller than the minimum inter-class distance. This facilitates
learning a highly discriminative feature space and ensures a more accurate
image retrieval result. Extensive experiments are conducted on two widely used
SBIR benchmark datasets. Our approach achieves better results on both datasets,
surpassing the state-of-the-art methods by a large margin.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/lu2021less/">Less Is More: Pre-train A Strong Text Encoder For Dense Retrieval Using A Weak Decoder</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Less Is More: Pre-train A Strong Text Encoder For Dense Retrieval Using A Weak Decoder' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Less Is More: Pre-train A Strong Text Encoder For Dense Retrieval Using A Weak Decoder' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</td>
    <td>24</td>
    <td><p>Dense retrieval requires high-quality text sequence embeddings to support
effective search in the representation space. Autoencoder-based language models
are appealing in dense retrieval as they train the encoder to output
high-quality embedding that can reconstruct the input texts. However, in this
paper, we provide theoretical analyses and show empirically that an autoencoder
language model with a low reconstruction loss may not provide good sequence
representations because the decoder may take shortcuts by exploiting language
patterns. To address this, we propose a new self-learning method that
pre-trains the autoencoder using a \textit{weak} decoder, with restricted
capacity and attention flexibility to push the encoder to provide better text
representations. Our experiments on web search, news recommendation, and open
domain question answering show that our pre-trained model significantly boosts
the effectiveness and few-shot ability of dense retrieval models. Our code is
available at https://github.com/microsoft/SEED-Encoder/.</p>
</td>
    <td>
      
        EMNLP 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/medini2021solar/">SOLAR: Sparse Orthogonal Learned And Random Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=SOLAR: Sparse Orthogonal Learned And Random Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=SOLAR: Sparse Orthogonal Learned And Random Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Medini Tharun, Chen Beidi, Shrivastava Anshumali</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>6</td>
    <td><p>Dense embedding models are commonly deployed in commercial search engines,
wherein all the document vectors are pre-computed, and near-neighbor search
(NNS) is performed with the query vector to find relevant documents. However,
the bottleneck of indexing a large number of dense vectors and performing an
NNS hurts the query time and accuracy of these models. In this paper, we argue
that high-dimensional and ultra-sparse embedding is a significantly superior
alternative to dense low-dimensional embedding for both query efficiency and
accuracy. Extreme sparsity eliminates the need for NNS by replacing them with
simple lookups, while its high dimensionality ensures that the embeddings are
informative even when sparse. However, learning extremely high dimensional
embeddings leads to blow up in the model size. To make the training feasible,
we propose a partitioning algorithm that learns such high dimensional
embeddings across multiple GPUs without any communication. This is facilitated
by our novel asymmetric mixture of Sparse, Orthogonal, Learned and Random
(SOLAR) Embeddings. The label vectors are random, sparse, and near-orthogonal
by design, while the query vectors are learned and sparse. We theoretically
prove that our way of one-sided learning is equivalent to learning both query
and label embeddings. With these unique properties, we can successfully train
500K dimensional SOLAR embeddings for the tasks of searching through 1.6M books
and multi-label classification on the three largest public datasets. We achieve
superior precision and recall compared to the respective state-of-the-art
baselines for each of the tasks with up to 10 times faster speed.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/wu2021hashing/">Hashing-accelerated Graph Neural Networks For Link Prediction</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing-accelerated Graph Neural Networks For Link Prediction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing-accelerated Graph Neural Networks For Link Prediction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Web Conference 2021</td>
    <td>30</td>
    <td><p>Networks are ubiquitous in the real world. Link prediction, as one of the key
problems for network-structured data, aims to predict whether there exists a
link between two nodes. The traditional approaches are based on the explicit
similarity computation between the compact node representation by embedding
each node into a low-dimensional space. In order to efficiently handle the
intensive similarity computation in link prediction, the hashing technique has
been successfully used to produce the node representation in the Hamming space.
However, the hashing-based link prediction algorithms face accuracy loss from
the randomized hashing techniques or inefficiency from the learning to hash
techniques in the embedding process. Currently, the Graph Neural Network (GNN)
framework has been widely applied to the graph-related tasks in an end-to-end
manner, but it commonly requires substantial computational resources and memory
costs due to massive parameter learning, which makes the GNN-based algorithms
impractical without the help of a powerful workhorse. In this paper, we propose
a simple and effective model called #GNN, which balances the trade-off between
accuracy and efficiency. #GNN is able to efficiently acquire node
representation in the Hamming space for link prediction by exploiting the
randomized hashing technique to implement message passing and capture
high-order proximity in the GNN framework. Furthermore, we characterize the
discriminative power of #GNN in probability. The extensive experimental results
demonstrate that the proposed #GNN algorithm achieves accuracy comparable to
the learning-based algorithms and outperforms the randomized algorithm, while
running significantly faster than the learning-based algorithms. Also, the
proposed algorithm shows excellent scalability on a large-scale network with
the limited resources.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/liu2021image/">Image Retrieval On Real-life Images With Pre-trained Vision-and-language Models</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Image Retrieval On Real-life Images With Pre-trained Vision-and-language Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Image Retrieval On Real-life Images With Pre-trained Vision-and-language Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>108</td>
    <td><p>We extend the task of composed image retrieval, where an input query consists
of an image and short textual description of how to modify the image. Existing
methods have only been applied to non-complex images within narrow domains,
such as fashion products, thereby limiting the scope of study on in-depth
visual reasoning in rich image and language contexts. To address this issue, we
collect the Compose Image Retrieval on Real-life images (CIRR) dataset, which
consists of over 36,000 pairs of crowd-sourced, open-domain images with
human-generated modifying text. To extend current methods to the open-domain,
we propose CIRPLANT, a transformer based model that leverages rich pre-trained
vision-and-language (V&amp;L) knowledge for modifying visual features conditioned
on natural language. Retrieval is then done by nearest neighbor lookup on the
modified features. We demonstrate that with a relatively simple architecture,
CIRPLANT outperforms existing methods on open-domain images, while matching
state-of-the-art accuracy on the existing narrow datasets, such as fashion.
Together with the release of CIRR, we believe this work will inspire further
research on composed image retrieval.</p>
</td>
    <td>
      
        Image Retrieval 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/liu2021hit/">Hit: Hierarchical Transformer With Momentum Contrast For Video-text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hit: Hierarchical Transformer With Momentum Contrast For Video-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hit: Hierarchical Transformer With Momentum Contrast For Video-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>134</td>
    <td><p>Video-Text Retrieval has been a hot research topic with the growth of
multimedia data on the internet. Transformer for video-text learning has
attracted increasing attention due to its promising performance. However,
existing cross-modal transformer approaches typically suffer from two major
limitations: 1) Exploitation of the transformer architecture where different
layers have different feature characteristics is limited; 2) End-to-end
training mechanism limits negative sample interactions in a mini-batch. In this
paper, we propose a novel approach named Hierarchical Transformer (HiT) for
video-text retrieval. HiT performs Hierarchical Cross-modal Contrastive
Matching in both feature-level and semantic-level, achieving multi-view and
comprehensive retrieval results. Moreover, inspired by MoCo, we propose
Momentum Cross-modal Contrast for cross-modal learning to enable large-scale
negative sample interactions on-the-fly, which contributes to the generation of
more precise and discriminative representations. Experimental results on the
three major Video-Text Retrieval benchmark datasets demonstrate the advantages
of our method.</p>
</td>
    <td>
      
        Text Retrieval 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/yan2021binary/">Binary Code Based Hash Embedding For Web-scale Applications</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Binary Code Based Hash Embedding For Web-scale Applications' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Binary Code Based Hash Embedding For Web-scale Applications' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yan et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</td>
    <td>10</td>
    <td><p>Nowadays, deep learning models are widely adopted in web-scale applications
such as recommender systems, and online advertising. In these applications,
embedding learning of categorical features is crucial to the success of deep
learning models. In these models, a standard method is that each categorical
feature value is assigned a unique embedding vector which can be learned and
optimized. Although this method can well capture the characteristics of the
categorical features and promise good performance, it can incur a huge memory
cost to store the embedding table, especially for those web-scale applications.
Such a huge memory cost significantly holds back the effectiveness and
usability of EDRMs. In this paper, we propose a binary code based hash
embedding method which allows the size of the embedding table to be reduced in
arbitrary scale without compromising too much performance. Experimental
evaluation results show that one can still achieve 99% performance even if the
embedding table size is reduced 1000\(\times\) smaller than the original one with
our proposed method.</p>
</td>
    <td>
      
        Large Scale Search 
      
        CIKM 
      
        Compact Codes 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/liu2021densernet/">Densernet: Weakly Supervised Visual Localization Using Multi-scale Feature Aggregation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Densernet: Weakly Supervised Visual Localization Using Multi-scale Feature Aggregation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Densernet: Weakly Supervised Visual Localization Using Multi-scale Feature Aggregation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>119</td>
    <td><p>In this work, we introduce a Denser Feature Network (DenserNet) for visual
localization. Our work provides three principal contributions. First, we
develop a convolutional neural network (CNN) architecture which aggregates
feature maps at different semantic levels for image representations. Using
denser feature maps, our method can produce more keypoint features and increase
image retrieval accuracy. Second, our model is trained end-to-end without
pixel-level annotation other than positive and negative GPS-tagged image pairs.
We use a weakly supervised triplet ranking loss to learn discriminative
features and encourage keypoint feature repeatability for image representation.
Finally, our method is computationally efficient as our architecture has shared
features and parameters during computation. Our method can perform accurate
large-scale localization under challenging conditions while remaining the
computational constraint. Extensive experiment results indicate that our method
sets a new state-of-the-art on four challenging large-scale localization
benchmarks and three image retrieval benchmarks.</p>
</td>
    <td>
      
        SUPERVISED 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/ling2021deep/">Deep Graph Matching And Searching For Semantic Code Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Graph Matching And Searching For Semantic Code Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Graph Matching And Searching For Semantic Code Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ling et al.</td> <!-- 🔧 You were missing this -->
    <td>ACM Transactions on Knowledge Discovery from Data</td>
    <td>41</td>
    <td><p>Code retrieval is to find the code snippet from a large corpus of source code
repositories that highly matches the query of natural language description.
Recent work mainly uses natural language processing techniques to process both
query texts (i.e., human natural language) and code snippets (i.e., machine
programming language), however neglecting the deep structured features of query
texts and source codes, both of which contain rich semantic information. In
this paper, we propose an end-to-end deep graph matching and searching (DGMS)
model based on graph neural networks for the task of semantic code retrieval.
To this end, we first represent both natural language query texts and
programming language code snippets with the unified graph-structured data, and
then use the proposed graph matching and searching model to retrieve the best
matching code snippet. In particular, DGMS not only captures more structural
information for individual query texts or code snippets but also learns the
fine-grained similarity between them by cross-attention based semantic matching
operations. We evaluate the proposed DGMS model on two public code retrieval
datasets with two representative programming languages (i.e., Java and Python).
Experiment results demonstrate that DGMS significantly outperforms
state-of-the-art baseline models by a large margin on both datasets. Moreover,
our extensive ablation studies systematically investigate and illustrate the
impact of each part of DGMS.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/liu2021fddh/">FDDH: Fast Discriminative Discrete Hashing For Large-scale Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=FDDH: Fast Discriminative Discrete Hashing For Large-scale Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=FDDH: Fast Discriminative Discrete Hashing For Large-scale Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Xin, Wang Xingzhi, Cheung Yiu-ming</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Neural Networks and Learning Systems</td>
    <td>49</td>
    <td><p>Cross-modal hashing, favored for its effectiveness and efficiency, has
received wide attention to facilitating efficient retrieval across different
modalities. Nevertheless, most existing methods do not sufficiently exploit the
discriminative power of semantic information when learning the hash codes,
while often involving time-consuming training procedure for handling the
large-scale dataset. To tackle these issues, we formulate the learning of
similarity-preserving hash codes in terms of orthogonally rotating the semantic
data so as to minimize the quantization loss of mapping such data to hamming
space, and propose an efficient Fast Discriminative Discrete Hashing (FDDH)
approach for large-scale cross-modal retrieval. More specifically, FDDH
introduces an orthogonal basis to regress the targeted hash codes of training
examples to their corresponding semantic labels, and utilizes “-dragging
technique to provide provable large semantic margins. Accordingly, the
discriminative power of semantic information can be explicitly captured and
maximized. Moreover, an orthogonal transformation scheme is further proposed to
map the nonlinear embedding data into the semantic subspace, which can well
guarantee the semantic consistency between the data feature and its semantic
representation. Consequently, an efficient closed form solution is derived for
discriminative hash code learning, which is very computationally efficient. In
addition, an effective and stable online learning strategy is presented for
optimizing modality-specific projection functions, featuring adaptivity to
different training sizes and streaming data. The proposed FDDH approach
theoretically approximates the bi-Lipschitz continuity, runs sufficiently fast,
and also significantly improves the retrieval performance over the
state-of-the-art methods. The source code is released at:
https://github.com/starxliu/FDDH.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Multimodal Retrieval 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/wei2021net/">A-net: Learning Attribute-aware Hash Codes For Large-scale Fine-grained Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A-net: Learning Attribute-aware Hash Codes For Large-scale Fine-grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A-net: Learning Attribute-aware Hash Codes For Large-scale Fine-grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wei et al.</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>7</td>
    <td><p>Our work focuses on tackling large-scale fine-grained image retrieval as ranking the images depicting the concept of interests (i.e., the same sub-category labels) highest based on the fine-grained details in the query. It is desirable to alleviate the challenges of both fine-grained nature of small inter-class variations with large intra-class variations and explosive growth of fine-grained data for such a practical task. In this paper, we propose an Attribute-Aware hashing Network (A-Net) for generating attribute-aware hash codes to not only make the retrieval process efficient, but also establish explicit correspondences between hash codes and visual attributes. Specifically, based on the captured visual representations by attention, we develop an encoder-decoder structure network of a reconstruction task to unsupervisedly distill high-level attribute-specific vectors from the appearance-specific visual representations without attribute annotations. A-Net is also equipped with a feature decorrelation constraint upon these attribute vectors to enhance their representation abilities. Finally, the required hash codes are generated by the attribute vectors driven by preserving original similarities. Qualitative experiments on five benchmark fine-grained datasets show our superiority over competing methods. More importantly, quantitative results demonstrate the obtained hash codes can strongly correspond to certain kinds of crucial properties of fine-grained objects.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/lin2021deep/">Deep Self-adaptive Hashing For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Self-adaptive Hashing For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Self-adaptive Hashing For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</td>
    <td>8</td>
    <td><p>Hashing technology has been widely used in image retrieval due to its
computational and storage efficiency. Recently, deep unsupervised hashing
methods have attracted increasing attention due to the high cost of human
annotations in the real world and the superiority of deep learning technology.
However, most deep unsupervised hashing methods usually pre-compute a
similarity matrix to model the pairwise relationship in the pre-trained feature
space. Then this similarity matrix would be used to guide hash learning, in
which most of the data pairs are treated equivalently. The above process is
confronted with the following defects: 1) The pre-computed similarity matrix is
inalterable and disconnected from the hash learning process, which cannot
explore the underlying semantic information. 2) The informative data pairs may
be buried by the large number of less-informative data pairs. To solve the
aforementioned problems, we propose a Deep Self-Adaptive Hashing (DSAH) model
to adaptively capture the semantic information with two special designs:
Adaptive Neighbor Discovery (AND) and Pairwise Information Content (PIC).
Firstly, we adopt the AND to initially construct a neighborhood-based
similarity matrix, and then refine this initial similarity matrix with a novel
update strategy to further investigate the semantic structure behind the
learned representation. Secondly, we measure the priorities of data pairs with
PIC and assign adaptive weights to them, which is relies on the assumption that
more dissimilar data pairs contain more discriminative information for hash
learning. Extensive experiments on several datasets demonstrate that the above
two technologies facilitate the deep hashing model to achieve superior
performance.</p>
</td>
    <td>
      
        Hashing Methods 
      
        CIKM 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/lifshitz2021gram/">Gram Barcodes For Histopathology Tissue Texture Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Gram Barcodes For Histopathology Tissue Texture Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Gram Barcodes For Histopathology Tissue Texture Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lifshitz Shalev, Riasatian Abtin, Tizhoosh H. R.</td> <!-- 🔧 You were missing this -->
    <td>2018 International Joint Conference on Neural Networks (IJCNN)</td>
    <td>13</td>
    <td><p>Recent advances in digital pathology have led to the need for Histopathology
Image Retrieval (HIR) systems that search through databases of biopsy images to
find similar cases to a given query image. These HIR systems allow pathologists
to effortlessly and efficiently access thousands of previously diagnosed cases
in order to exploit the knowledge in the corresponding pathology reports. Since
HIR systems may have to deal with millions of gigapixel images, the extraction
of compact and expressive image features must be available to allow for
efficient and accurate retrieval. In this paper, we propose the application of
Gram barcodes as image features for HIR systems. Unlike most feature generation
schemes, Gram barcodes are based on high-order statistics that describe tissue
texture by summarizing the correlations between different feature maps in
layers of convolutional neural networks. We run HIR experiments on three public
datasets using a pre-trained VGG19 network for Gram barcode generation and
showcase highly competitive results.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/lian2021quotient/">Quotient Space-based Keyword Retrieval In Sponsored Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Quotient Space-based Keyword Retrieval In Sponsored Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Quotient Space-based Keyword Retrieval In Sponsored Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lian et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 12th ACM conference on Electronic commerce</td>
    <td>8</td>
    <td><p>Synonymous keyword retrieval has become an important problem for sponsored
search ever since major search engines relax the exact match product’s matching
requirement to a synonymous level. Since the synonymous relations between
queries and keywords are quite scarce, the traditional information retrieval
framework is inefficient in this scenario. In this paper, we propose a novel
quotient space-based retrieval framework to address this problem. Considering
the synonymy among keywords as a mathematical equivalence relation, we can
compress the synonymous keywords into one representative, and the corresponding
quotient space would greatly reduce the size of the keyword repository. Then an
embedding-based retrieval is directly conducted between queries and the keyword
representatives. To mitigate the semantic gap of the quotient space-based
retrieval, a single semantic siamese model is utilized to detect both the
keyword–keyword and query-keyword synonymous relations. The experiments show
that with our quotient space-based retrieval method, the synonymous keyword
retrieving performance can be greatly improved in terms of memory cost and
recall efficiency. This method has been successfully implemented in Baidu’s
online sponsored search system and has yielded a significant improvement in
revenue.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/liang2021dynamic/">Dynamic Sampling For Deep Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Dynamic Sampling For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Dynamic Sampling For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liang Chang-hui, Zhao Wan-lei, Chen Run-qing</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition Letters</td>
    <td>5</td>
    <td><p>Deep metric learning maps visually similar images onto nearby locations and
visually dissimilar images apart from each other in an embedding manifold. The
learning process is mainly based on the supplied image negative and positive
training pairs. In this paper, a dynamic sampling strategy is proposed to
organize the training pairs in an easy-to-hard order to feed into the network.
It allows the network to learn general boundaries between categories from the
easy training pairs at its early stages and finalize the details of the model
mainly relying on the hard training samples in the later. Compared to the
existing training sample mining approaches, the hard samples are mined with
little harm to the learned general model. This dynamic sampling strategy is
formularized as two simple terms that are compatible with various loss
functions. Consistent performance boost is observed when it is integrated with
several popular loss functions on fashion search, fine-grained classification,
and person re-identification tasks.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/zheng2021deep/">Deep Relational Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Relational Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Relational Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zheng et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>36</td>
    <td><p>This paper presents a deep relational metric learning (DRML) framework for
image clustering and retrieval. Most existing deep metric learning methods
learn an embedding space with a general objective of increasing interclass
distances and decreasing intraclass distances. However, the conventional losses
of metric learning usually suppress intraclass variations which might be
helpful to identify samples of unseen classes. To address this problem, we
propose to adaptively learn an ensemble of features that characterizes an image
from different aspects to model both interclass and intraclass distributions.
We further employ a relational module to capture the correlations among each
feature in the ensemble and construct a graph to represent an image. We then
perform relational inference on the graph to integrate the ensemble and obtain
a relation-aware embedding to measure the similarities. Extensive experiments
on the widely-used CUB-200-2011, Cars196, and Stanford Online Products datasets
demonstrate that our framework improves existing deep metric learning methods
and achieves very competitive results.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/li2021task/">Task-adaptive Asymmetric Deep Cross-modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Task-adaptive Asymmetric Deep Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Task-adaptive Asymmetric Deep Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li et al.</td> <!-- 🔧 You were missing this -->
    <td>Knowledge-Based Systems</td>
    <td>17</td>
    <td><p>Supervised cross-modal hashing aims to embed the semantic correlations of
heterogeneous modality data into the binary hash codes with discriminative
semantic labels. Because of its advantages on retrieval and storage efficiency,
it is widely used for solving efficient cross-modal retrieval. However,
existing researches equally handle the different tasks of cross-modal
retrieval, and simply learn the same couple of hash functions in a symmetric
way for them. Under such circumstance, the uniqueness of different cross-modal
retrieval tasks are ignored and sub-optimal performance may be brought.
Motivated by this, we present a Task-adaptive Asymmetric Deep Cross-modal
Hashing (TA-ADCMH) method in this paper. It can learn task-adaptive hash
functions for two sub-retrieval tasks via simultaneous modality representation
and asymmetric hash learning. Unlike previous cross-modal hashing approaches,
our learning framework jointly optimizes semantic preserving that transforms
deep features of multimedia data into binary hash codes, and the semantic
regression which directly regresses query modality representation to explicit
label. With our model, the binary codes can effectively preserve semantic
correlations across different modalities, meanwhile, adaptively capture the
query semantics. The superiority of TA-ADCMH is proved on two standard datasets
from many aspects.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/li2021self/">Self-supervised Video Hashing Via Bidirectional Transformers</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Self-supervised Video Hashing Via Bidirectional Transformers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Self-supervised Video Hashing Via Bidirectional Transformers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>38</td>
    <td><p>Most existing unsupervised video hashing methods are built on unidirectional models with less reliable training objectives, which underuse the correlations among frames and the similarity structure between videos. To enable efficient scalable video retrieval, we propose a self-supervised video Hashing method based on Bidirectional Transformers (BTH). Based on the encoder-decoder structure of transformers, we design a visual cloze task to fully exploit the bidirectional correlations between frames. To unveil the similarity structure between unlabeled video data, we further develop a similarity reconstruction task by establishing reliable and effective similarity connections in the video space. Furthermore, we develop a cluster assignment task to exploit the structural statistics of the whole dataset such that more discriminative binary codes can be learned. Extensive experiments implemented on three public benchmark datasets, FCVID, ActivityNet and YFCC, demonstrate the superiority of our proposed approach.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
        Self SUPERVISED 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/li2021extra/">EXTRA: Explanation Ranking Datasets For Explainable Recommendation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=EXTRA: Explanation Ranking Datasets For Explainable Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=EXTRA: Explanation Ranking Datasets For Explainable Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Lei, Zhang Yongfeng, Chen Li</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>29</td>
    <td><p>Recently, research on explainable recommender systems has drawn much
attention from both academia and industry, resulting in a variety of
explainable models. As a consequence, their evaluation approaches vary from
model to model, which makes it quite difficult to compare the explainability of
different models. To achieve a standard way of evaluating recommendation
explanations, we provide three benchmark datasets for EXplanaTion RAnking
(denoted as EXTRA), on which explainability can be measured by ranking-oriented
metrics. Constructing such datasets, however, poses great challenges. First,
user-item-explanation triplet interactions are rare in existing recommender
systems, so how to find alternatives becomes a challenge. Our solution is to
identify nearly identical sentences from user reviews. This idea then leads to
the second challenge, i.e., how to efficiently categorize the sentences in a
dataset into different groups, since it has quadratic runtime complexity to
estimate the similarity between any two sentences. To mitigate this issue, we
provide a more efficient method based on Locality Sensitive Hashing (LSH) that
can detect near-duplicates in sub-linear time for a given query. Moreover, we
make our code publicly available to allow researchers in the community to
create their own datasets.</p>
</td>
    <td>
      
        Datasets 
      
        SIGIR 
      
        Text Retrieval 
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/li2021c/">C-minhash: Practically Reducing Two Permutations To Just One</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=C-minhash: Practically Reducing Two Permutations To Just One' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=C-minhash: Practically Reducing Two Permutations To Just One' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Xiaoyun, Li Ping</td> <!-- 🔧 You were missing this -->
    <td>Journal of Combinatorial Theory, Series A</td>
    <td>5</td>
    <td><p>Traditional minwise hashing (MinHash) requires applying \(K\) independent
permutations to estimate the Jaccard similarity in massive binary (0/1) data,
where \(K\) can be (e.g.,) 1024 or even larger, depending on applications. The
recent work on C-MinHash (Li and Li, 2021) has shown, with rigorous proofs,
that only two permutations are needed. An initial permutation is applied to
break whatever structures which might exist in the data, and a second
permutation is re-used \(K\) times to produce \(K\) hashes, via a circulant
shifting fashion. (Li and Li, 2021) has proved that, perhaps surprisingly, even
though the \(K\) hashes are correlated, the estimation variance is strictly
smaller than the variance of the traditional MinHash.
  It has been demonstrated in (Li and Li, 2021) that the initial permutation in
C-MinHash is indeed necessary. For the ease of theoretical analysis, they have
used two independent permutations. In this paper, we show that one can actually
simply use one permutation. That is, one single permutation is used for both
the initial pre-processing step to break the structures in the data and the
circulant hashing step to generate \(K\) hashes. Although the theoretical
analysis becomes very complicated, we are able to explicitly write down the
expression for the expectation of the estimator. The new estimator is no longer
unbiased but the bias is extremely small and has essentially no impact on the
estimation accuracy (mean square errors). An extensive set of experiments are
provided to verify our claim for using just one permutation.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/li2021deep/">Deep Unsupervised Image Hashing By Maximizing Bit Entropy</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Unsupervised Image Hashing By Maximizing Bit Entropy' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Unsupervised Image Hashing By Maximizing Bit Entropy' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Yunqiang, van Gemert Jan</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>75</td>
    <td><p>Unsupervised hashing is important for indexing huge image or video
collections without having expensive annotations available. Hashing aims to
learn short binary codes for compact storage and efficient semantic retrieval.
We propose an unsupervised deep hashing layer called Bi-half Net that maximizes
entropy of the binary codes. Entropy is maximal when both possible values of
the bit are uniformly (half-half) distributed. To maximize bit entropy, we do
not add a term to the loss function as this is difficult to optimize and tune.
Instead, we design a new parameter-free network layer to explicitly force
continuous image features to approximate the optimal half-half bit
distribution. This layer is shown to minimize a penalized term of the
Wasserstein distance between the learned continuous image features and the
optimal half-half bit distribution. Experimental results on the image datasets
Flickr25k, Nus-wide, Cifar-10, Mscoco, Mnist and the video datasets Ucf-101 and
Hmdb-51 show that our approach leads to compact codes and compares favorably to
the current state-of-the-art.</p>
</td>
    <td>
      
        Image Retrieval 
      
        Unsupervised 
      
        AAAI 
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/li2021more/">More Robust Dense Retrieval With Contrastive Dual Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=More Robust Dense Retrieval With Contrastive Dual Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=More Robust Dense Retrieval With Contrastive Dual Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2021 ACM SIGIR International Conference on Theory of Information Retrieval</td>
    <td>23</td>
    <td><p>Dense retrieval conducts text retrieval in the embedding space and has shown
many advantages compared to sparse retrieval. Existing dense retrievers
optimize representations of queries and documents with contrastive training and
map them to the embedding space. The embedding space is optimized by aligning
the matched query-document pairs and pushing the negative documents away from
the query. However, in such training paradigm, the queries are only optimized
to align to the documents and are coarsely positioned, leading to an
anisotropic query embedding space. In this paper, we analyze the embedding
space distributions and propose an effective training paradigm, Contrastive
Dual Learning for Approximate Nearest Neighbor (DANCE) to learn fine-grained
query representations for dense retrieval. DANCE incorporates an additional
dual training object of query retrieval, inspired by the classic information
retrieval training axiom, query likelihood. With contrastive learning, the dual
training object of DANCE learns more tailored representations for queries and
documents to keep the embedding space smooth and uniform, thriving on the
ranking performance of DANCE on the MS MARCO document retrieval task. Different
from ANCE that only optimized with the document retrieval task, DANCE
concentrates the query embeddings closer to document representations while
making the document distribution more discriminative. Such concentrated query
embedding distribution assigns more uniform negative sampling probabilities to
queries and helps to sufficiently optimize query representations in the query
retrieval task. Our codes are released at https://github.com/thunlp/DANCE.</p>
</td>
    <td>
      
        SIGIR 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/levi2021rethinking/">Rethinking Preventing Class-collapsing In Metric Learning With Margin-based Losses</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Rethinking Preventing Class-collapsing In Metric Learning With Margin-based Losses' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Rethinking Preventing Class-collapsing In Metric Learning With Margin-based Losses' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Levi et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>15</td>
    <td><p>Metric learning seeks perceptual embeddings where visually similar instances
are close and dissimilar instances are apart, but learned representations can
be sub-optimal when the distribution of intra-class samples is diverse and
distinct sub-clusters are present. Although theoretically with optimal
assumptions, margin-based losses such as the triplet loss and margin loss have
a diverse family of solutions. We theoretically prove and empirically show that
under reasonable noise assumptions, margin-based losses tend to project all
samples of a class with various modes onto a single point in the embedding
space, resulting in a class collapse that usually renders the space ill-sorted
for classification or retrieval. To address this problem, we propose a simple
modification to the embedding losses such that each sample selects its nearest
same-class counterpart in a batch as the positive element in the tuple. This
allows for the presence of multiple sub-clusters within each class. The
adaptation can be integrated into a wide range of metric learning losses. The
proposed sampling method demonstrates clear benefits on various fine-grained
image retrieval datasets over a variety of existing losses; qualitative
retrieval results show that samples with similar visual patterns are indeed
closer in the embedding space.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/xie2021learning/">Learning TFIDF Enhanced Joint Embedding For Recipe-image Cross-modal Retrieval Service</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning TFIDF Enhanced Joint Embedding For Recipe-image Cross-modal Retrieval Service' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning TFIDF Enhanced Joint Embedding For Recipe-image Cross-modal Retrieval Service' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xie et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Services Computing</td>
    <td>24</td>
    <td><p>It is widely acknowledged that learning joint embeddings of recipes with
images is challenging due to the diverse composition and deformation of
ingredients in cooking procedures. We present a Multi-modal Semantics enhanced
Joint Embedding approach (MSJE) for learning a common feature space between the
two modalities (text and image), with the ultimate goal of providing
high-performance cross-modal retrieval services. Our MSJE approach has three
unique features. First, we extract the TFIDF feature from the title,
ingredients and cooking instructions of recipes. By determining the
significance of word sequences through combining LSTM learned features with
their TFIDF features, we encode a recipe into a TFIDF weighted vector for
capturing significant key terms and how such key terms are used in the
corresponding cooking instructions. Second, we combine the recipe TFIDF feature
with the recipe sequence feature extracted through two-stage LSTM networks,
which is effective in capturing the unique relationship between a recipe and
its associated image(s). Third, we further incorporate TFIDF enhanced category
semantics to improve the mapping of image modality and to regulate the
similarity loss function during the iterative learning of cross-modal joint
embedding. Experiments on the benchmark dataset Recipe1M show the proposed
approach outperforms the state-of-the-art approaches.</p>
</td>
    <td>
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/lassance2021composite/">Composite Code Sparse Autoencoders For First Stage Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Composite Code Sparse Autoencoders For First Stage Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Composite Code Sparse Autoencoders For First Stage Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lassance Carlos, Formal Thibault, Clinchant Stephane</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>5</td>
    <td><p>We propose a Composite Code Sparse Autoencoder (CCSA) approach for
Approximate Nearest Neighbor (ANN) search of document representations based on
Siamese-BERT models. In Information Retrieval (IR), the ranking pipeline is
generally decomposed in two stages: the first stage focus on retrieving a
candidate set from the whole collection. The second stage re-ranks the
candidate set by relying on more complex models. Recently, Siamese-BERT models
have been used as first stage ranker to replace or complement the traditional
bag-of-word models. However, indexing and searching a large document collection
require efficient similarity search on dense vectors and this is why ANN
techniques come into play. Since composite codes are naturally sparse, we first
show how CCSA can learn efficient parallel inverted index thanks to an
uniformity regularizer. Second, CCSA can be used as a binary quantization
method and we propose to combine it with the recent graph based ANN techniques.
Our experiments on MSMARCO dataset reveal that CCSA outperforms IVF with
product quantization. Furthermore, CCSA binary quantization is beneficial for
the index size, and memory usage for the graph-based HNSW method, while
maintaining a good level of recall and MRR. Third, we compare with recent
supervised quantization methods for image retrieval and find that CCSA is able
to outperform them.</p>
</td>
    <td>
      
        SIGIR 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/krishna2021evaluating/">Evaluating Contrastive Models For Instance-based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Evaluating Contrastive Models For Instance-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Evaluating Contrastive Models For Instance-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Krishna Tarun, Mcguinness Kevin, O'connor Noel</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2021 International Conference on Multimedia Retrieval</td>
    <td>23</td>
    <td><p>In this work, we evaluate contrastive models for the task of image retrieval.
We hypothesise that models that are learned to encode semantic similarity among
instances via discriminative learning should perform well on the task of image
retrieval, where relevancy is defined in terms of instances of the same object.
Through our extensive evaluation, we find that representations from models
trained using contrastive methods perform on-par with (and outperforms) a
pre-trained supervised baseline trained on the ImageNet labels in retrieval
tasks under various configurations. This is remarkable given that the
contrastive models require no explicit supervision. Thus, we conclude that
these models can be used to bootstrap base models to build more robust image
retrieval engines.</p>
</td>
    <td>
      
        Evaluation 
      
        Multimodal Retrieval 
      
        Image Retrieval 
      
        Medical Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/lakshman2021embracing/">Embracing Structure In Data For Billion-scale Semantic Product Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Embracing Structure In Data For Billion-scale Semantic Product Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Embracing Structure In Data For Billion-scale Semantic Product Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lakshman et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</td>
    <td>56</td>
    <td><p>We present principled approaches to train and deploy dyadic neural embedding
models at the billion scale, focusing our investigation on the application of
semantic product search. When training a dyadic model, one seeks to embed two
different types of entities (e.g., queries and documents or users and movies)
in a common vector space such that pairs with high relevance are positioned
nearby. During inference, given an embedding of one type (e.g., a query or a
user), one seeks to retrieve the entities of the other type (e.g., documents or
movies, respectively) that are highly relevant. In this work, we show that
exploiting the natural structure of real-world datasets helps address both
challenges efficiently. Specifically, we model dyadic data as a bipartite graph
with edges between pairs with positive associations. We then propose to
partition this network into semantically coherent clusters and thus reduce our
search space by focusing on a small subset of these partitions for a given
input. During training, this technique enables us to efficiently mine hard
negative examples while, at inference, we can quickly find the nearest
neighbors for a given embedding. We provide offline experimental results that
demonstrate the efficacy of our techniques for both training and inference on a
billion-scale Amazon.com product search dataset.</p>
</td>
    <td>
      
        KDD 
      
        Large Scale Search 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/kosti%C4%872021multi/">Multi-modal Retrieval Of Tables And Texts Using Tri-encoder Models</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multi-modal Retrieval Of Tables And Texts Using Tri-encoder Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multi-modal Retrieval Of Tables And Texts Using Tri-encoder Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kostić Bogdan, Risch Julian, Möller Timo</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 3rd Workshop on Machine Reading for Question Answering</td>
    <td>12</td>
    <td><p>Open-domain extractive question answering works well on textual data by first
retrieving candidate texts and then extracting the answer from those
candidates. However, some questions cannot be answered by text alone but
require information stored in tables. In this paper, we present an approach for
retrieving both texts and tables relevant to a question by jointly encoding
texts, tables and questions into a single vector space. To this end, we create
a new multi-modal dataset based on text and table datasets from related work
and compare the retrieval performance of different encoding schemata. We find
that dense vector embeddings of transformer models outperform sparse embeddings
on four out of six evaluation datasets. Comparing different dense embedding
models, tri-encoders with one encoder for each question, text and table,
increase retrieval performance compared to bi-encoders with one encoder for the
question and one for both text and tables. We release the newly created
multi-modal dataset to the community so that it can be used for training and
evaluation.</p>
</td>
    <td>
      
        Graph Based ANN 
      
        Graph-based ANN 
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/kordopatiszilos2021leveraging/">Leveraging Efficientnet And Contrastive Learning For Accurate Global-scale Location Estimation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Leveraging Efficientnet And Contrastive Learning For Accurate Global-scale Location Estimation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Leveraging Efficientnet And Contrastive Learning For Accurate Global-scale Location Estimation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kordopatis-zilos et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2021 International Conference on Multimedia Retrieval</td>
    <td>14</td>
    <td><p>In this paper, we address the problem of global-scale image geolocation,
proposing a mixed classification-retrieval scheme. Unlike other methods that
strictly tackle the problem as a classification or retrieval task, we combine
the two practices in a unified solution leveraging the advantages of each
approach with two different modules. The first leverages the EfficientNet
architecture to assign images to a specific geographic cell in a robust way.
The second introduces a new residual architecture that is trained with
contrastive learning to map input images to an embedding space that minimizes
the pairwise geodesic distance of same-location images. For the final location
estimation, the two modules are combined with a search-within-cell scheme,
where the locations of most similar images from the predicted geographic cell
are aggregated based on a spatial clustering scheme. Our approach demonstrates
very competitive performance on four public datasets, achieving new
state-of-the-art performance in fine granularity scales, i.e., 15.0% at 1km
range on Im2GPS3k.</p>
</td>
    <td>
      
        Self SUPERVISED 
      
        Multimodal Retrieval 
      
        Medical Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/vanblokland2021partial/">Partial 3D Object Retrieval Using Local Binary QUICCI Descriptors And Dissimilarity Tree Indexing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Partial 3D Object Retrieval Using Local Binary QUICCI Descriptors And Dissimilarity Tree Indexing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Partial 3D Object Retrieval Using Local Binary QUICCI Descriptors And Dissimilarity Tree Indexing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>van Blokland Bart Iver, Theoharis Theoharis</td> <!-- 🔧 You were missing this -->
    <td>Computers &amp; Graphics</td>
    <td>6</td>
    <td><p>A complete pipeline is presented for accurate and efficient partial 3D object
retrieval based on Quick Intersection Count Change Image (QUICCI) binary local
descriptors and a novel indexing tree. It is shown how a modification to the
QUICCI query descriptor makes it ideal for partial retrieval. An indexing
structure called Dissimilarity Tree is proposed which can significantly
accelerate searching the large space of local descriptors; this is applicable
to QUICCI and other binary descriptors. The index exploits the distribution of
bits within descriptors for efficient retrieval. The retrieval pipeline is
tested on the artificial part of SHREC’16 dataset with near-ideal retrieval
results.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/ko2021low/">Low-precision Quantization For Efficient Nearest Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Low-precision Quantization For Efficient Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Low-precision Quantization For Efficient Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ko et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>1863</td>
    <td><p>Fast k-Nearest Neighbor search over real-valued vector spaces (KNN) is an
important algorithmic task for information retrieval and recommendation
systems. We present a method for using reduced precision to represent vectors
through quantized integer values, enabling both a reduction in the memory
overhead of indexing these vectors and faster distance computations at query
time. While most traditional quantization techniques focus on minimizing the
reconstruction error between a point and its uncompressed counterpart, we focus
instead on preserving the behavior of the underlying distance metric.
Furthermore, our quantization approach is applied at the implementation level
and can be combined with existing KNN algorithms. Our experiments on both open
source and proprietary datasets across multiple popular KNN frameworks validate
that quantized distance metrics can reduce memory by 60% and improve query
throughput by 30%, while incurring only a 2% reduction in recall.</p>
</td>
    <td>
      
        Quantization 
      
        Evaluation 
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/kim2021swamp/">Swamp: Swapped Assignment Of Multi-modal Pairs For Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Swamp: Swapped Assignment Of Multi-modal Pairs For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Swamp: Swapped Assignment Of Multi-modal Pairs For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kim Minyoung</td> <!-- 🔧 You were missing this -->
    <td>Findings of the Association for Computational Linguistics: EMNLP 2021</td>
    <td>14</td>
    <td><p>We tackle the cross-modal retrieval problem, where learning is only
supervised by relevant multi-modal pairs in the data. Although the contrastive
learning is the most popular approach for this task, it makes potentially wrong
assumption that the instances in different pairs are automatically irrelevant.
To address the issue, we propose a novel loss function that is based on
self-labeling of the unknown semantic classes. Specifically, we aim to predict
class labels of the data instances in each modality, and assign those labels to
the corresponding instances in the other modality (i.e., swapping the pseudo
labels). With these swapped labels, we learn the data embedding for each
modality using the supervised cross-entropy loss. This way, cross-modal
instances from different pairs that are semantically related can be aligned to
each other by the class predictor. We tested our approach on several real-world
cross-modal retrieval problems, including text-based video retrieval,
sketch-based image retrieval, and image-text retrieval. For all these tasks our
method achieves significant performance improvement over the contrastive
learning.</p>
</td>
    <td>
      
        ACL 
      
        EMNLP 
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/kim2021multi/">Multi-level Distance Regularization For Deep Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multi-level Distance Regularization For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multi-level Distance Regularization For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kim Yonghyun, Park Wonpyo</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>11</td>
    <td><p>We propose a novel distance-based regularization method for deep metric
learning called Multi-level Distance Regularization (MDR). MDR explicitly
disturbs a learning procedure by regularizing pairwise distances between
embedding vectors into multiple levels that represents a degree of similarity
between a pair. In the training stage, the model is trained with both MDR and
an existing loss function of deep metric learning, simultaneously; the two
losses interfere with the objective of each other, and it makes the learning
process difficult. Moreover, MDR prevents some examples from being ignored or
overly influenced in the learning process. These allow the parameters of the
embedding network to be settle on a local optima with better generalization.
Without bells and whistles, MDR with simple Triplet loss achieves
the-state-of-the-art performance in various benchmark datasets: CUB-200-2011,
Cars-196, Stanford Online Products, and In-Shop Clothes Retrieval. We
extensively perform ablation studies on its behaviors to show the effectiveness
of MDR. By easily adopting our MDR, the previous approaches can be improved in
performance and generalization ability.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/xiong2021approximate/">Approximate Nearest Neighbor Negative Contrastive Learning For Dense Text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Approximate Nearest Neighbor Negative Contrastive Learning For Dense Text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Approximate Nearest Neighbor Negative Contrastive Learning For Dense Text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xiong et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>356</td>
    <td><p>Conducting text retrieval in a dense learned representation space has many
intriguing advantages over sparse retrieval. Yet the effectiveness of dense
retrieval (DR) often requires combination with sparse retrieval. In this paper,
we identify that the main bottleneck is in the training mechanisms, where the
negative instances used in training are not representative of the irrelevant
documents in testing. This paper presents Approximate nearest neighbor Negative
Contrastive Estimation (ANCE), a training mechanism that constructs negatives
from an Approximate Nearest Neighbor (ANN) index of the corpus, which is
parallelly updated with the learning process to select more realistic negative
training instances. This fundamentally resolves the discrepancy between the
data distribution used in the training and testing of DR. In our experiments,
ANCE boosts the BERT-Siamese DR model to outperform all competitive dense and
sparse retrieval baselines. It nearly matches the accuracy of
sparse-retrieval-and-BERT-reranking using dot-product in the ANCE-learned
representation space and provides almost 100x speed-up.</p>
</td>
    <td>
      
        Similarity Search 
      
        Text Retrieval 
      
        Self SUPERVISED 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/kobayashi2021decomposing/">Decomposing Normal And Abnormal Features Of Medical Images For Content-based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Decomposing Normal And Abnormal Features Of Medical Images For Content-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Decomposing Normal And Abnormal Features Of Medical Images For Content-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kobayashi et al.</td> <!-- 🔧 You were missing this -->
    <td>Medical Image Analysis</td>
    <td>31</td>
    <td><p>Medical images can be decomposed into normal and abnormal features, which is
considered as the compositionality. Based on this idea, we propose an
encoder-decoder network to decompose a medical image into two discrete latent
codes: a normal anatomy code and an abnormal anatomy code. Using these latent
codes, we demonstrate a similarity retrieval by focusing on either normal or
abnormal features of medical images.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/xiao2021neural/">Neural Pathsim For Inductive Similarity Search In Heterogeneous Information Networks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Neural Pathsim For Inductive Similarity Search In Heterogeneous Information Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Neural Pathsim For Inductive Similarity Search In Heterogeneous Information Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xiao et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</td>
    <td>7</td>
    <td><p>PathSim is a widely used meta-path-based similarity in heterogeneous
information networks. Numerous applications rely on the computation of PathSim,
including similarity search and clustering. Computing PathSim scores on large
graphs is computationally challenging due to its high time and storage
complexity. In this paper, we propose to transform the problem of approximating
the ground truth PathSim scores into a learning problem. We design an
encoder-decoder based framework, NeuPath, where the algorithmic structure of
PathSim is considered. Specifically, the encoder module identifies Top T
optimized path instances, which can approximate the ground truth PathSim, and
maps each path instance to an embedding vector. The decoder transforms each
embedding vector into a scalar respectively, which identifies the similarity
score. We perform extensive experiments on two real-world datasets in different
domains, ACM and IMDB. Our results demonstrate that NeuPath performs better
than state-of-the-art baselines in the PathSim approximation task and
similarity search task.</p>
</td>
    <td>
      
        Similarity Search 
      
        CIKM 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/kang2021learning/">Learning To Embed Categorical Features Without Embedding Tables For Recommendation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning To Embed Categorical Features Without Embedding Tables For Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning To Embed Categorical Features Without Embedding Tables For Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</td>
    <td>34</td>
    <td><p>Embedding learning of categorical features (e.g. user/item IDs) is at the
core of various recommendation models including matrix factorization and neural
collaborative filtering. The standard approach creates an embedding table where
each row represents a dedicated embedding vector for every unique feature
value. However, this method fails to efficiently handle high-cardinality
features and unseen feature values (e.g. new video ID) that are prevalent in
real-world recommendation systems. In this paper, we propose an alternative
embedding framework Deep Hash Embedding (DHE), replacing embedding tables by a
deep embedding network to compute embeddings on the fly. DHE first encodes the
feature value to a unique identifier vector with multiple hashing functions and
transformations, and then applies a DNN to convert the identifier vector to an
embedding. The encoding module is deterministic, non-learnable, and free of
storage, while the embedding network is updated during the training time to
learn embedding generation. Empirical results show that DHE achieves comparable
AUC against the standard one-hot full embedding, with smaller model sizes. Our
work sheds light on the design of DNN-based alternative embedding schemes for
categorical features without using embedding table lookup.</p>
</td>
    <td>
      
        KDD 
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/jin2021unsupervised/">Unsupervised Discrete Hashing With Affinity Similarity</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Discrete Hashing With Affinity Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Discrete Hashing With Affinity Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jin et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>14</td>
    <td><p>In recent years, supervised hashing has been validated to greatly boost the performance of image retrieval. However, the label-hungry property requires massive label collection, making it intractable in practical scenarios. To liberate the model training procedure from laborious manual annotations, some unsupervised methods are proposed. However, the following two factors make unsupervised algorithms inferior to their supervised counterparts: (1) Without manually-defined labels, it is difficult to capture the semantic information across data, which is of crucial importance to guide robust binary code learning. (2) The widely adopted relaxation on binary constraints results in quantization error accumulation in the optimization procedure. To address the above-mentioned problems, in this paper, we propose a novel Unsupervised Discrete Hashing method (UDH). Specifically, to capture the semantic information, we propose a balanced graph-based semantic loss which explores the affinity priors in the original feature space. Then, we propose a novel self-supervised loss, termed orthogonal consistent loss, which can leverage semantic loss of instance and impose independence of codes. Moreover, by integrating the discrete optimization into the proposed unsupervised framework, the binary constraints are consistently preserved, alleviating the influence of quantization errors. Extensive experiments demonstrate that UDH outperforms state-of-the-art unsupervised methods for image retrieval.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/jose2021optimized/">Optimized Feature Space Learning For Generating Efficient Binary Codes For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Optimized Feature Space Learning For Generating Efficient Binary Codes For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Optimized Feature Space Learning For Generating Efficient Binary Codes For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jose et al.</td> <!-- 🔧 You were missing this -->
    <td>Signal Processing: Image Communication</td>
    <td>7</td>
    <td><p>In this paper we propose an approach for learning low dimensional optimized
feature space with minimum intra-class variance and maximum inter-class
variance. We address the problem of high-dimensionality of feature vectors
extracted from neural networks by taking care of the global statistics of
feature space. Classical approach of Linear Discriminant Analysis (LDA) is
generally used for generating an optimized low dimensional feature space for
single-labeled images. Since, image retrieval involves both multi-labeled and
single-labeled images, we utilize the equivalence between LDA and Canonical
Correlation Analysis (CCA) to generate an optimized feature space for
single-labeled images and use CCA to generate an optimized feature space for
multi-labeled images. Our approach correlates the projections of feature
vectors with label vectors in our CCA based network architecture. The neural
network minimize a loss function which maximizes the correlation coefficients.
We binarize our generated feature vectors with the popular Iterative
Quantization (ITQ) approach and also propose an ensemble network to generate
binary codes of desired bit length for image retrieval. Our measurement of mean
average precision shows competitive results on other state-of-the-art
single-labeled and multi-labeled image retrieval datasets.</p>
</td>
    <td>
      
        Image Retrieval 
      
        Compact Codes 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/tonellotto2021query/">Query Embedding Pruning For Dense Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Query Embedding Pruning For Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Query Embedding Pruning For Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tonellotto Nicola, Macdonald Craig</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</td>
    <td>16</td>
    <td><p>Recent advances in dense retrieval techniques have offered the promise of
being able not just to re-rank documents using contextualised language models
such as BERT, but also to use such models to identify documents from the
collection in the first place. However, when using dense retrieval approaches
that use multiple embedded representations for each query, a large number of
documents can be retrieved for each query, hindering the efficiency of the
method. Hence, this work is the first to consider efficiency improvements in
the context of a dense retrieval approach (namely ColBERT), by pruning query
term embeddings that are estimated not to be useful for retrieving relevant
documents. Our proposed query embeddings pruning reduces the cost of the dense
retrieval operation, as well as reducing the number of documents that are
retrieved and hence require to be fully scored. Experiments conducted on the
MSMARCO passage ranking corpus demonstrate that, when reducing the number of
query embeddings used from 32 to 3 based on the collection frequency of the
corresponding tokens, query embedding pruning results in no statistically
significant differences in effectiveness, while reducing the number of
documents retrieved by 70%. In terms of mean response time for the end-to-end
to end system, this results in a 2.65x speedup.</p>
</td>
    <td>
      
        CIKM 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/jeong2021asmr/">ASMR: Learning Attribute-based Person Search With Adaptive Semantic Margin Regularizer</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=ASMR: Learning Attribute-based Person Search With Adaptive Semantic Margin Regularizer' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=ASMR: Learning Attribute-based Person Search With Adaptive Semantic Margin Regularizer' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jeong Boseung, Park Jicheol, Kwak Suha</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>17</td>
    <td><p>Attribute-based person search is the task of finding person images that are
best matched with a set of text attributes given as query. The main challenge
of this task is the large modality gap between attributes and images. To reduce
the gap, we present a new loss for learning cross-modal embeddings in the
context of attribute-based person search. We regard a set of attributes as a
category of people sharing the same traits. In a joint embedding space of the
two modalities, our loss pulls images close to their person categories for
modality alignment. More importantly, it pushes apart a pair of person
categories by a margin determined adaptively by their semantic distance, where
the distance metric is learned end-to-end so that the loss considers importance
of each attribute when relating person categories. Our loss guided by the
adaptive semantic margin leads to more discriminative and semantically
well-arranged distributions of person images. As a consequence, it enables a
simple embedding model to achieve state-of-the-art records on public benchmarks
without bells and whistles.</p>
</td>
    <td>
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/jang2021ultra/">Ultra-high Dimensional Sparse Representations With Binarization For Efficient Text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Ultra-high Dimensional Sparse Representations With Binarization For Efficient Text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Ultra-high Dimensional Sparse Representations With Binarization For Efficient Text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</td>
    <td>9</td>
    <td><p>The semantic matching capabilities of neural information retrieval can
ameliorate synonymy and polysemy problems of symbolic approaches. However,
neural models’ dense representations are more suitable for re-ranking, due to
their inefficiency. Sparse representations, either in symbolic or latent form,
are more efficient with an inverted index. Taking the merits of the sparse and
dense representations, we propose an ultra-high dimensional (UHD)
representation scheme equipped with directly controllable sparsity. UHD’s large
capacity and minimal noise and interference among the dimensions allow for
binarized representations, which are highly efficient for storage and search.
Also proposed is a bucketing method, where the embeddings from multiple layers
of BERT are selected/merged to represent diverse linguistic aspects. We test
our models with MS MARCO and TREC CAR, showing that our models outperforms
other sparse models</p>
</td>
    <td>
      
        EMNLP 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/jang2021similarity/">Similarity Guided Deep Face Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Similarity Guided Deep Face Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Similarity Guided Deep Face Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jang Young Kyun, Cho Nam Ik</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2020 International Conference on Multimedia Retrieval</td>
    <td>8</td>
    <td><p>Face image retrieval, which searches for images of the same identity from the
query input face image, is drawing more attention as the size of the image
database increases rapidly. In order to conduct fast and accurate retrieval, a
compact hash code-based methods have been proposed, and recently, deep face
image hashing methods with supervised classification training have shown
outstanding performance. However, classification-based scheme has a
disadvantage in that it cannot reveal complex similarities between face images
into the hash code learning. In this paper, we attempt to improve the face
image retrieval quality by proposing a Similarity Guided Hashing (SGH) method,
which gently considers self and pairwise-similarity simultaneously. SGH employs
various data augmentations designed to explore elaborate similarities between
face images, solving both intra and inter identity-wise difficulties. Extensive
experimental results on the protocols with existing benchmarks and an
additionally proposed large scale higher resolution face image dataset
demonstrate that our SGH delivers state-of-the-art retrieval performance.</p>
</td>
    <td>
      
        Multimodal Retrieval 
      
        Image Retrieval 
      
        Medical Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/zhang2021improved/">Improved Deep Classwise Hashing With Centers Similarity Learning For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Improved Deep Classwise Hashing With Centers Similarity Learning For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Improved Deep Classwise Hashing With Centers Similarity Learning For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Ming, Yan Hong</td> <!-- 🔧 You were missing this -->
    <td>2020 25th International Conference on Pattern Recognition (ICPR)</td>
    <td>8</td>
    <td><p>Deep supervised hashing for image retrieval has attracted researchers’
attention due to its high efficiency and superior retrieval performance. Most
existing deep supervised hashing works, which are based on pairwise/triplet
labels, suffer from the expensive computational cost and insufficient
utilization of the semantics information. Recently, deep classwise hashing
introduced a classwise loss supervised by class labels information
alternatively; however, we find it still has its drawback. In this paper, we
propose an improved deep classwise hashing, which enables hashing learning and
class centers learning simultaneously. Specifically, we design a two-step
strategy on center similarity learning. It interacts with the classwise loss to
attract the class center to concentrate on the intra-class samples while
pushing other class centers as far as possible. The centers similarity learning
contributes to generating more compact and discriminative hashing codes. We
conduct experiments on three benchmark datasets. It shows that the proposed
method effectively surpasses the original method and outperforms
state-of-the-art baselines under various commonly-used evaluation metrics for
image retrieval.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/jang2021self/">Self-supervised Product Quantization For Deep Unsupervised Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Self-supervised Product Quantization For Deep Unsupervised Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Self-supervised Product Quantization For Deep Unsupervised Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jang Young Kyun, Cho Nam Ik</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>52</td>
    <td><p>Supervised deep learning-based hash and vector quantization are enabling fast
and large-scale image retrieval systems. By fully exploiting label annotations,
they are achieving outstanding retrieval performances compared to the
conventional methods. However, it is painstaking to assign labels precisely for
a vast amount of training data, and also, the annotation process is
error-prone. To tackle these issues, we propose the first deep unsupervised
image retrieval method dubbed Self-supervised Product Quantization (SPQ)
network, which is label-free and trained in a self-supervised manner. We design
a Cross Quantized Contrastive learning strategy that jointly learns codewords
and deep visual descriptors by comparing individually transformed images
(views). Our method analyzes the image contents to extract descriptive
features, allowing us to understand image representations for accurate
retrieval. By conducting extensive experiments on benchmarks, we demonstrate
that the proposed method yields state-of-the-art results even without
supervised pretraining.</p>
</td>
    <td>
      
        Quantization 
      
        Self SUPERVISED 
      
        Image Retrieval 
      
        Unsupervised 
      
        SUPERVISED 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/zhang2021deep/">Deep Center-based Dual-constrained Hashing For Discriminative Face Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Center-based Dual-constrained Hashing For Discriminative Face Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Center-based Dual-constrained Hashing For Discriminative Face Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Ming, Zhe, Yan</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition</td>
    <td>21</td>
    <td><p>With the advantages of low storage cost and extremely fast retrieval speed, deep hashing methods have attracted much attention for image retrieval recently. However, large-scale face image retrieval with significant intra-class variations is still challenging. Neither existing pairwise/triplet labels-based nor softmax classification loss-based deep hashing works can generate compact and discriminative binary codes. Considering these issues, we propose a center-based framework integrating end-to-end hashing learning and class centers learning simultaneously. The framework minimizes the intra-class variance by clustering intra-class samples into a learnable class center. To strengthen inter-class separability, it additionally imposes a novel regularization term to enlarge the Hamming distance between pairwise class centers. Moreover, a simple yet effective regression matrix is introduced to encourage intra-class samples to generate the same binary codes, which further enhances the hashing codes compactness. Experiments on four large-scale datasets show the proposed method outperforms state-of-the-art baselines under various code lengths and commonly-used evaluation metrics.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/wang2021towards/">Towards A Model For LSH</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Towards A Model For LSH' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Towards A Model For LSH' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Li</td> <!-- 🔧 You were missing this -->
    <td>Infection and Immunity</td>
    <td>19</td>
    <td><p>As data volumes continue to grow, clustering and outlier detection algorithms
are becoming increasingly time-consuming. Classical index structures for
neighbor search are no longer sustainable due to the “curse of dimensionality”.
Instead, approximated index structures offer a good opportunity to
significantly accelerate the neighbor search for clustering and outlier
detection and to have the lowest possible error rate in the results of the
algorithms. Locality-sensitive hashing is one of those. We indicate directions
to model the properties of LSH.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/zhang2021given/">Given Users Recommendations Based On Reviews On Yelp</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Given Users Recommendations Based On Reviews On Yelp' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Given Users Recommendations Based On Reviews On Yelp' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang et al.</td> <!-- 🔧 You were missing this -->
    <td>Journal of Medical Internet Research</td>
    <td>23</td>
    <td><p>In our project, we focus on NLP-based hybrid recommendation systems. Our data
is from Yelp Data. For our hybrid recommendation system, we have two major
components: the first part is to embed the reviews with the Bert model and
word2vec model; the second part is the implementation of an item-based
collaborative filtering algorithm to compute the similarity of each review
under different categories of restaurants. In the end, with the help of
similarity scores, we are able to recommend users the most matched restaurant
based on their recorded reviews. The coding work is split into several parts:
selecting samples and data cleaning, processing, embedding, computing
similarity, and computing prediction and error. Due to the size of the data,
each part will generate one or more JSON files as the milestone to reduce the
pressure on memory and the communication between each part.</p>
</td>
    <td>
      
        Survey Paper 
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/zhang2021fast/">Fast Discrete Cross-modal Hashing Based On Label Relaxation And Matrix Factorization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Discrete Cross-modal Hashing Based On Label Relaxation And Matrix Factorization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Discrete Cross-modal Hashing Based On Label Relaxation And Matrix Factorization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang et al.</td> <!-- 🔧 You were missing this -->
    <td>2020 25th International Conference on Pattern Recognition (ICPR)</td>
    <td>9</td>
    <td><p>In recent years, cross-media retrieval has drawn considerable attention due to the exponential growth of multimedia data. Many hashing approaches have been proposed for the cross-media search task. However, there are still open problems that warrant investigation. For example, most existing supervised hashing approaches employ a binary label matrix, which achieves small margins between wrong labels (0) and true labels (1). This may affect the retrieval performance by generating many false negatives and false positives. In addition, some methods adopt a relaxation scheme to solve the binary constraints, which may cause large quantization errors. There are also some discrete hashing methods that have been presented, but most of them are time-consuming. To conquer these problems, we present a label relaxation and discrete matrix factorization method (LRMF) for cross-modal retrieval. It offers a number of innovations. First of all, the proposed approach employs a novel label relaxation scheme to control the margins adaptively, which has the benefit of reducing the quantization error. Second, by virtue of the proposed discrete matrix factorization method designed to learn the binary codes, large quantization errors caused by relaxation can be avoided. The experimental results obtained on two widely-used databases demonstrate that LRMF outperforms state-of-the-art cross-media methods.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/zhang2021joint/">Joint Learning Of Deep Retrieval Model And Product Quantization Based Embedding Index</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Joint Learning Of Deep Retrieval Model And Product Quantization Based Embedding Index' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Joint Learning Of Deep Retrieval Model And Product Quantization Based Embedding Index' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>6</td>
    <td><p>Embedding index that enables fast approximate nearest neighbor(ANN) search,
serves as an indispensable component for state-of-the-art deep retrieval
systems. Traditional approaches, often separating the two steps of embedding
learning and index building, incur additional indexing time and decayed
retrieval accuracy. In this paper, we propose a novel method called Poeem,
which stands for product quantization based embedding index jointly trained
with deep retrieval model, to unify the two separate steps within an end-to-end
training, by utilizing a few techniques including the gradient straight-through
estimator, warm start strategy, optimal space decomposition and Givens
rotation. Extensive experimental results show that the proposed method not only
improves retrieval accuracy significantly but also reduces the indexing time to
almost none. We have open sourced our approach for the sake of comparison and
reproducibility.</p>
</td>
    <td>
      
        SIGIR 
      
        Text Retrieval 
      
        Quantization 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/zhang2021leveraging/">Leveraging Local And Global Descriptors In Parallel To Search Correspondences For Visual Localization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Leveraging Local And Global Descriptors In Parallel To Search Correspondences For Visual Localization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Leveraging Local And Global Descriptors In Parallel To Search Correspondences For Visual Localization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Pengju, Wu Yihong, Liu Bingxi</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition</td>
    <td>13</td>
    <td><p>Visual localization to compute 6DoF camera pose from a given image has wide
applications such as in robotics, virtual reality, augmented reality, etc. Two
kinds of descriptors are important for the visual localization. One is global
descriptors that extract the whole feature from each image. The other is local
descriptors that extract the local feature from each image patch usually
enclosing a key point. More and more methods of the visual localization have
two stages: at first to perform image retrieval by global descriptors and then
from the retrieval feedback to make 2D-3D point correspondences by local
descriptors. The two stages are in serial for most of the methods. This simple
combination has not achieved superiority of fusing local and global
descriptors. The 3D points obtained from the retrieval feedback are as the
nearest neighbor candidates of the 2D image points only by global descriptors.
Each of the 2D image points is also called a query local feature when
performing the 2D-3D point correspondences. In this paper, we propose a novel
parallel search framework, which leverages advantages of both local and global
descriptors to get nearest neighbor candidates of a query local feature.
Specifically, besides using deep learning based global descriptors, we also
utilize local descriptors to construct random tree structures for obtaining
nearest neighbor candidates of the query local feature. We propose a new
probabilistic model and a new deep learning based local descriptor when
constructing the random trees. A weighted Hamming regularization term to keep
discriminativeness after binarization is given in the loss function for the
proposed local descriptor. The loss function co-trains both real and binary
descriptors of which the results are integrated into the random trees.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/zhang2021high/">High-order Nonlocal Hashing For Unsupervised Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=High-order Nonlocal Hashing For Unsupervised Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=High-order Nonlocal Hashing For Unsupervised Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang et al.</td> <!-- 🔧 You were missing this -->
    <td>World Wide Web</td>
    <td>60</td>
    <td><p>In light of the ability to enable efficient storage and fast query for big data, hashing techniques for cross-modal search have aroused extensive attention. Despite the great success achieved, unsupervised cross-modal hashing still suffers from lacking reliable similarity supervision and struggles with handling the heterogeneity issue between different modalities. To cope with these, in this paper, we devise a new deep hashing model, termed as High-order Nonlocal Hashing (HNH) to facilitate cross-modal retrieval with the following advantages. First, different from existing methods that mainly leverage low-level local-view similarity as the guidance for hashing learning, we propose a high-order affinity measure that considers the multi-modal neighbourhood structures from a nonlocal perspective, thereby comprehensively capturing the similarity relationships between data items. Second, a common representation is introduced to correlate different modalities. By enforcing the modal-specific descriptors and the common representation to be aligned with each other, the proposed HNH significantly bridges the modality gap and maintains the intra-consistency. Third, an effective affinity preserving objective function is delicately designed to generate high-quality binary codes. Extensive experiments evidence the superiority of the proposed HNH in unsupervised cross-modal retrieval tasks over the state-of-the-art baselines.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
        Multimodal Retrieval 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/hoe2021one/">One Loss For All: Deep Hashing With A Single Cosine Similarity Based Learning Objective</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=One Loss For All: Deep Hashing With A Single Cosine Similarity Based Learning Objective' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=One Loss For All: Deep Hashing With A Single Cosine Similarity Based Learning Objective' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hoe et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>46</td>
    <td><p>A deep hashing model typically has two main learning objectives: to make the
learned binary hash codes discriminative and to minimize a quantization error.
With further constraints such as bit balance and code orthogonality, it is not
uncommon for existing models to employ a large number (&gt;4) of losses. This
leads to difficulties in model training and subsequently impedes their
effectiveness. In this work, we propose a novel deep hashing model with only a
single learning objective. Specifically, we show that maximizing the cosine
similarity between the continuous codes and their corresponding binary
orthogonal codes can ensure both hash code discriminativeness and quantization
error minimization. Further, with this learning objective, code balancing can
be achieved by simply using a Batch Normalization (BN) layer and multi-label
classification is also straightforward with label smoothing. The result is an
one-loss deep hashing model that removes all the hassles of tuning the weights
of various losses. Importantly, extensive experiments show that our model is
highly effective, outperforming the state-of-the-art multi-loss hashing models
on three large-scale instance retrieval benchmarks, often by significant
margins. Code is available at https://github.com/kamwoh/orthohash</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        Distance Metric Learning 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/hoang2021multi/">Multi-modal Mutual Information Maximization: A Novel Approach For Unsupervised Deep Cross-modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multi-modal Mutual Information Maximization: A Novel Approach For Unsupervised Deep Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multi-modal Mutual Information Maximization: A Novel Approach For Unsupervised Deep Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hoang et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Neural Networks and Learning Systems</td>
    <td>27</td>
    <td><p>In this paper, we adopt the maximizing mutual information (MI) approach to
tackle the problem of unsupervised learning of binary hash codes for efficient
cross-modal retrieval. We proposed a novel method, dubbed Cross-Modal Info-Max
Hashing (CMIMH). First, to learn informative representations that can preserve
both intra- and inter-modal similarities, we leverage the recent advances in
estimating variational lower-bound of MI to maximize the MI between the binary
representations and input features and between binary representations of
different modalities. By jointly maximizing these MIs under the assumption that
the binary representations are modelled by multivariate Bernoulli
distributions, we can learn binary representations, which can preserve both
intra- and inter-modal similarities, effectively in a mini-batch manner with
gradient descent. Furthermore, we find out that trying to minimize the modality
gap by learning similar binary representations for the same instance from
different modalities could result in less informative representations. Hence,
balancing between reducing the modality gap and losing modality-private
information is important for the cross-modal retrieval tasks. Quantitative
evaluations on standard benchmark datasets demonstrate that the proposed method
consistently outperforms other state-of-the-art cross-modal retrieval methods.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/ufer2021object/">Object Retrieval And Localization In Large Art Collections Using Deep Multi-style Feature Fusion And Iterative Voting</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Object Retrieval And Localization In Large Art Collections Using Deep Multi-style Feature Fusion And Iterative Voting' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Object Retrieval And Localization In Large Art Collections Using Deep Multi-style Feature Fusion And Iterative Voting' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ufer Nikolai, Lang Sabine, Ommer Björn</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>8</td>
    <td><p>The search for specific objects or motifs is essential to art history as both
assist in decoding the meaning of artworks. Digitization has produced large art
collections, but manual methods prove to be insufficient to analyze them. In
the following, we introduce an algorithm that allows users to search for image
regions containing specific motifs or objects and find similar regions in an
extensive dataset, helping art historians to analyze large digitized art
collections. Computer vision has presented efficient methods for visual
instance retrieval across photographs. However, applied to art collections,
they reveal severe deficiencies because of diverse motifs and massive domain
shifts induced by differences in techniques, materials, and styles. In this
paper, we present a multi-style feature fusion approach that successfully
reduces the domain gap and improves retrieval results without labelled data or
curated image collections. Our region-based voting with GPU-accelerated
approximate nearest-neighbour search allows us to find and localize even small
motifs within an extensive dataset in a few seconds. We obtain state-of-the-art
results on the Brueghel dataset and demonstrate its generalization to
inhomogeneous collections with a large number of distractors.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/yang2021dolg/">DOLG: Single-stage Image Retrieval With Deep Orthogonal Fusion Of Local And Global Features</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=DOLG: Single-stage Image Retrieval With Deep Orthogonal Fusion Of Local And Global Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=DOLG: Single-stage Image Retrieval With Deep Orthogonal Fusion Of Local And Global Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yang et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>110</td>
    <td><p>Image Retrieval is a fundamental task of obtaining images similar to the
query one from a database. A common image retrieval practice is to firstly
retrieve candidate images via similarity search using global image features and
then re-rank the candidates by leveraging their local features. Previous
learning-based studies mainly focus on either global or local image
representation learning to tackle the retrieval task. In this paper, we abandon
the two-stage paradigm and seek to design an effective single-stage solution by
integrating local and global information inside images into compact image
representations. Specifically, we propose a Deep Orthogonal Local and Global
(DOLG) information fusion framework for end-to-end image retrieval. It
attentively extracts representative local information with multi-atrous
convolutions and self-attention at first. Components orthogonal to the global
image representation are then extracted from the local information. At last,
the orthogonal components are concatenated with the global representation as a
complementary, and then aggregation is performed to generate the final
representation. The whole framework is end-to-end differentiable and can be
trained with image-level labels. Extensive experimental results validate the
effectiveness of our solution and show that our model achieves state-of-the-art
image retrieval performances on Revisited Oxford and Paris datasets.</p>
</td>
    <td>
      
        Image Retrieval 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/he2021self/">Self-supervised Video Retrieval Transformer Network</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Self-supervised Video Retrieval Transformer Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Self-supervised Video Retrieval Transformer Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>He et al.</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>72</td>
    <td><p>Content-based video retrieval aims to find videos from a large video database
that are similar to or even near-duplicate of a given query video. Video
representation and similarity search algorithms are crucial to any video
retrieval system. To derive effective video representation, most video
retrieval systems require a large amount of manually annotated data for
training, making it costly inefficient. In addition, most retrieval systems are
based on frame-level features for video similarity searching, making it
expensive both storage wise and search wise. We propose a novel video retrieval
system, termed SVRTN, that effectively addresses the above shortcomings. It
first applies self-supervised training to effectively learn video
representation from unlabeled data to avoid the expensive cost of manual
annotation. Then, it exploits transformer structure to aggregate frame-level
features into clip-level to reduce both storage space and search complexity. It
can learn the complementary and discriminative information from the
interactions among clip frames, as well as acquire the frame permutation and
missing invariant ability to support more flexible retrieval manners.
Comprehensive experiments on two challenging video retrieval datasets, namely
FIVR-200K and SVD, verify the effectiveness of our proposed SVRTN method, which
achieves the best performance of video retrieval on accuracy and efficiency.</p>
</td>
    <td>
      
        Self SUPERVISED 
      
        CVPR 
      
        Video Retrieval 
      
        SUPERVISED 
      
        Transformer Based ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/he2021unsupervised/">Unsupervised Domain-adaptive Hash For Networks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Domain-adaptive Hash For Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Domain-adaptive Hash For Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>He et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of The Web Conference 2020</td>
    <td>98</td>
    <td><p>Abundant real-world data can be naturally represented by large-scale
networks, which demands efficient and effective learning algorithms. At the
same time, labels may only be available for some networks, which demands these
algorithms to be able to adapt to unlabeled networks. Domain-adaptive hash
learning has enjoyed considerable success in the computer vision community in
many practical tasks due to its lower cost in both retrieval time and storage
footprint. However, it has not been applied to multiple-domain networks. In
this work, we bridge this gap by developing an unsupervised domain-adaptive
hash learning method for networks, dubbed UDAH. Specifically, we develop four
{task-specific yet correlated} components: (1) network structure preservation
via a hard groupwise contrastive loss, (2) relaxation-free supervised hashing,
(3) cross-domain intersected discriminators, and (4) semantic center alignment.
We conduct a wide range of experiments to evaluate the effectiveness and
efficiency of our method on a range of tasks including link prediction, node
classification, and neighbor recommendation. Our evaluation results demonstrate
that our model achieves better performance than the state-of-the-art
conventional discrete embedding methods over all the tasks.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/hansen2021representation/">Representation Learning For Efficient And Effective Similarity Search And Recommendation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Representation Learning For Efficient And Effective Similarity Search And Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Representation Learning For Efficient And Effective Similarity Search And Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hansen Casper</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>16</td>
    <td><p>How data is represented and operationalized is critical for building
computational solutions that are both effective and efficient. A common
approach is to represent data objects as binary vectors, denoted \textit{hash
codes}, which require little storage and enable efficient similarity search
through direct indexing into a hash table or through similarity computations in
an appropriate space. Due to the limited expressibility of hash codes, compared
to real-valued representations, a core open challenge is how to generate hash
codes that well capture semantic content or latent properties using a small
number of bits, while ensuring that the hash codes are distributed in a way
that does not reduce their search efficiency. State of the art methods use
representation learning for generating such hash codes, focusing on neural
autoencoder architectures where semantics are encoded into the hash codes by
learning to reconstruct the original inputs of the hash codes. This thesis
addresses the above challenge and makes a number of contributions to
representation learning that (i) improve effectiveness of hash codes through
more expressive representations and a more effective similarity measure than
the current state of the art, namely the Hamming distance, and (ii) improve
efficiency of hash codes by learning representations that are especially suited
to the choice of search method. The contributions are empirically validated on
several tasks related to similarity search and recommendation.</p>
</td>
    <td>
      
        SIGIR 
      
        Text Retrieval 
      
        Recommender Systems 
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/hansen2021unsupervised/">Unsupervised Multi-index Semantic Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Multi-index Semantic Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Multi-index Semantic Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hansen et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Web Conference 2021</td>
    <td>8</td>
    <td><p>Semantic hashing represents documents as compact binary vectors (hash codes)
and allows both efficient and effective similarity search in large-scale
information retrieval. The state of the art has primarily focused on learning
hash codes that improve similarity search effectiveness, while assuming a
brute-force linear scan strategy for searching over all the hash codes, even
though much faster alternatives exist. One such alternative is multi-index
hashing, an approach that constructs a smaller candidate set to search over,
which depending on the distribution of the hash codes can lead to sub-linear
search time. In this work, we propose Multi-Index Semantic Hashing (MISH), an
unsupervised hashing model that learns hash codes that are both effective and
highly efficient by being optimized for multi-index hashing. We derive novel
training objectives, which enable to learn hash codes that reduce the candidate
sets produced by multi-index hashing, while being end-to-end trainable. In
fact, our proposed training objectives are model agnostic, i.e., not tied to
how the hash codes are generated specifically in MISH, and are straight-forward
to include in existing and future semantic hashing models. We experimentally
compare MISH to state-of-the-art semantic hashing baselines in the task of
document similarity search. We find that even though multi-index hashing also
improves the efficiency of the baselines compared to a linear scan, they are
still upwards of 33% slower than MISH, while MISH is still able to obtain
state-of-the-art effectiveness.</p>
</td>
    <td>
      
        Text Retrieval 
      
        Unsupervised 
      
        Vector Indexing 
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/zhang2021mr/">Mr. Tydi: A Multi-lingual Benchmark For Dense Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Mr. Tydi: A Multi-lingual Benchmark For Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Mr. Tydi: A Multi-lingual Benchmark For Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 1st Workshop on Multilingual Representation Learning</td>
    <td>24</td>
    <td><p>We present Mr. TyDi, a multi-lingual benchmark dataset for mono-lingual
retrieval in eleven typologically diverse languages, designed to evaluate
ranking with learned dense representations. The goal of this resource is to
spur research in dense retrieval techniques in non-English languages, motivated
by recent observations that existing techniques for representation learning
perform poorly when applied to out-of-distribution data. As a starting point,
we provide zero-shot baselines for this new dataset based on a multi-lingual
adaptation of DPR that we call “mDPR”. Experiments show that although the
effectiveness of mDPR is much lower than BM25, dense representations
nevertheless appear to provide valuable relevance signals, improving BM25
results in sparse-dense hybrids. In addition to analyses of our results, we
also discuss future challenges and present a research agenda in multi-lingual
dense retrieval. Mr. TyDi can be downloaded at
https://github.com/castorini/mr.tydi.</p>
</td>
    <td>
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/gupta2021irli/">IRLI: Iterative Re-partitioning For Learning To Index</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=IRLI: Iterative Re-partitioning For Learning To Index' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=IRLI: Iterative Re-partitioning For Learning To Index' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gupta et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</td>
    <td>6</td>
    <td><p>Neural models have transformed the fundamental information retrieval problem
of mapping a query to a giant set of items. However, the need for efficient and
low latency inference forces the community to reconsider efficient approximate
near-neighbor search in the item space. To this end, learning to index is
gaining much interest in recent times. Methods have to trade between obtaining
high accuracy while maintaining load balance and scalability in distributed
settings. We propose a novel approach called IRLI (pronounced `early’), which
iteratively partitions the items by learning the relevant buckets directly from
the query-item relevance data. Furthermore, IRLI employs a superior
power-of-\(k\)-choices based load balancing strategy. We mathematically show that
IRLI retrieves the correct item with high probability under very natural
assumptions and provides superior load balancing. IRLI surpasses the best
baseline’s precision on multi-label classification while being \(5x\) faster on
inference. For near-neighbor search tasks, the same method outperforms the
state-of-the-art Learned Hashing approach NeuralLSH by requiring only ~
{1/6}^th of the candidates for the same recall. IRLI is both data and model
parallel, making it ideal for distributed GPU implementation. We demonstrate
this advantage by indexing 100 million dense vectors and surpassing the popular
FAISS library by &gt;10% on recall.</p>
</td>
    <td>
      
        KDD 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/guo2021deep/">Deep Kernel Supervised Hashing For Node Classification In Structural Networks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Kernel Supervised Hashing For Node Classification In Structural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Kernel Supervised Hashing For Node Classification In Structural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Guo et al.</td> <!-- 🔧 You were missing this -->
    <td>Information Sciences</td>
    <td>5</td>
    <td><p>Node classification in structural networks has been proven to be useful in
many real world applications. With the development of network embedding, the
performance of node classification has been greatly improved. However, nearly
all the existing network embedding based methods are hard to capture the actual
category features of a node because of the linearly inseparable problem in
low-dimensional space; meanwhile they cannot incorporate simultaneously network
structure information and node label information into network embedding. To
address the above problems, in this paper, we propose a novel Deep Kernel
Supervised Hashing (DKSH) method to learn the hashing representations of nodes
for node classification. Specifically, a deep multiple kernel learning is first
proposed to map nodes into suitable Hilbert space to deal with linearly
inseparable problem. Then, instead of only considering structural similarity
between two nodes, a novel similarity matrix is designed to merge both network
structure information and node label information. Supervised by the similarity
matrix, the learned hashing representations of nodes simultaneously preserve
the two kinds of information well from the learned Hilbert space. Extensive
experiments show that the proposed method significantly outperforms the
state-of-the-art baselines over three real world benchmark datasets.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        SUPERVISED 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/hajiaghayi2021unbiased/">Unbiased Sentence Encoder For Large-scale Multi-lingual Search Engines</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unbiased Sentence Encoder For Large-scale Multi-lingual Search Engines' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unbiased Sentence Encoder For Large-scale Multi-lingual Search Engines' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hajiaghayi Mahdi, Hajiaghayi Monir, Bolin Mark</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)</td>
    <td>35</td>
    <td><p>In this paper, we present a multi-lingual sentence encoder that can be used
in search engines as a query and document encoder. This embedding enables a
semantic similarity score between queries and documents that can be an
important feature in document ranking and relevancy. To train such a customized
sentence encoder, it is beneficial to leverage users search data in the form of
query-document clicked pairs however, we must avoid relying too much on search
click data as it is biased and does not cover many unseen cases. The search
data is heavily skewed towards short queries and for long queries is small and
often noisy. The goal is to design a universal multi-lingual encoder that works
for all cases and covers both short and long queries. We select a number of
public NLI datasets in different languages and translation data and together
with user search data we train a language model using a multi-task approach. A
challenge is that these datasets are not homogeneous in terms of content, size
and the balance ratio. While the public NLI datasets are usually two-sentence
based with the same portion of positive and negative pairs, the user search
data can contain multi-sentence documents and only positive pairs. We show how
multi-task training enables us to leverage all these datasets and exploit
knowledge sharing across these tasks.</p>
</td>
    <td>
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/gu2021multimodal/">Multimodal Representation For Neural Code Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multimodal Representation For Neural Code Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multimodal Representation For Neural Code Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gu Jian, Chen Zimin, Monperrus Martin</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE International Conference on Software Maintenance and Evolution (ICSME)</td>
    <td>32</td>
    <td><p>Semantic code search is about finding semantically relevant code snippets for
a given natural language query. In the state-of-the-art approaches, the
semantic similarity between code and query is quantified as the distance of
their representation in the shared vector space. In this paper, to improve the
vector space, we introduce tree-serialization methods on a simplified form of
AST and build the multimodal representation for the code data. We conduct
extensive experiments using a single corpus that is large-scale and
multi-language: CodeSearchNet. Our results show that both our tree-serialized
representations and multimodal learning model improve the performance of code
search. Last, we define intuitive quantification metrics oriented to the
completeness of semantic and syntactic information of the code data, to help
understand the experimental findings.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/tseytlin2021hotel/">Hotel Recognition Via Latent Image Embedding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hotel Recognition Via Latent Image Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hotel Recognition Via Latent Image Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tseytlin Boris, Makarov Ilya</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>7</td>
    <td><p>We approach the problem of hotel recognition with deep metric learning. We
overview the existing approaches and propose a modification to Contrastive loss
called Contrastive-Triplet loss. We construct a robust pipeline for
benchmarking metric learning models and perform experiments on Hotels-50K and
CUB200 datasets. Contrastive-Triplet loss is shown to achieve better retrieval
on Hotels-50k. We open-source our code.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/gkelios2021investigating/">Investigating The Vision Transformer Model For Image Retrieval Tasks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Investigating The Vision Transformer Model For Image Retrieval Tasks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Investigating The Vision Transformer Model For Image Retrieval Tasks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gkelios Socratis, Boutalis Yiannis, Chatzichristofis Savvas A.</td> <!-- 🔧 You were missing this -->
    <td>2021 17th International Conference on Distributed Computing in Sensor Systems (DCOSS)</td>
    <td>30</td>
    <td><p>This paper introduces a plug-and-play descriptor that can be effectively
adopted for image retrieval tasks without prior initialization or preparation.
The description method utilizes the recently proposed Vision Transformer
network while it does not require any training data to adjust parameters. In
image retrieval tasks, the use of Handcrafted global and local descriptors has
been very successfully replaced, over the last years, by the Convolutional
Neural Networks (CNN)-based methods. However, the experimental evaluation
conducted in this paper on several benchmarking datasets against 36
state-of-the-art descriptors from the literature demonstrates that a neural
network that contains no convolutional layer, such as Vision Transformer, can
shape a global descriptor and achieve competitive results. As fine-tuning is
not required, the presented methodology’s low complexity encourages adoption of
the architecture as an image retrieval baseline model, replacing the
traditional and well adopted CNN-based approaches and inaugurating a new era in
image retrieval approaches.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/geigle2021retrieve/">Retrieve Fast, Rerank Smart: Cooperative And Joint Approaches For Improved Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Retrieve Fast, Rerank Smart: Cooperative And Joint Approaches For Improved Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Retrieve Fast, Rerank Smart: Cooperative And Joint Approaches For Improved Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Geigle et al.</td> <!-- 🔧 You were missing this -->
    <td>Transactions of the Association for Computational Linguistics</td>
    <td>23</td>
    <td><p>Current state-of-the-art approaches to cross-modal retrieval process text and
visual input jointly, relying on Transformer-based architectures with
cross-attention mechanisms that attend over all words and objects in an image.
While offering unmatched retrieval performance, such models: 1) are typically
pretrained from scratch and thus less scalable, 2) suffer from huge retrieval
latency and inefficiency issues, which makes them impractical in realistic
applications. To address these crucial gaps towards both improved and efficient
cross-modal retrieval, we propose a novel fine-tuning framework that turns any
pretrained text-image multi-modal model into an efficient retrieval model. The
framework is based on a cooperative retrieve-and-rerank approach which
combines: 1) twin networks (i.e., a bi-encoder) to separately encode all items
of a corpus, enabling efficient initial retrieval, and 2) a cross-encoder
component for a more nuanced (i.e., smarter) ranking of the retrieved small set
of items. We also propose to jointly fine-tune the two components with shared
weights, yielding a more parameter-efficient model. Our experiments on a series
of standard cross-modal retrieval benchmarks in monolingual, multilingual, and
zero-shot setups, demonstrate improved accuracy and huge efficiency benefits
over the state-of-the-art cross-encoders.</p>
</td>
    <td>
      
        EACL 
      
        TACL 
      
        NAACL 
      
        ACL 
      
        Re RANKING 
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/yang2021deep/">Deep Attention-guided Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Attention-guided Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Attention-guided Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yang et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Circuits and Systems for Video Technology</td>
    <td>29</td>
    <td><p>With the rapid growth of multimedia data (e.g., image, audio and video etc.)
on the web, learning-based hashing techniques such as Deep Supervised Hashing
(DSH) have proven to be very efficient for large-scale multimedia search. The
recent successes seen in Learning-based hashing methods are largely due to the
success of deep learning-based hashing methods. However, there are some
limitations to previous learning-based hashing methods (e.g., the learned hash
codes containing repetitive and highly correlated information). In this paper,
we propose a novel learning-based hashing method, named Deep Attention-guided
Hashing (DAgH). DAgH is implemented using two stream frameworks. The core idea
is to use guided hash codes which are generated by the hashing network of the
first stream framework (called first hashing network) to guide the training of
the hashing network of the second stream framework (called second hashing
network). Specifically, in the first network, it leverages an attention network
and hashing network to generate the attention-guided hash codes from the
original images. The loss function we propose contains two components: the
semantic loss and the attention loss. The attention loss is used to punish the
attention network to obtain the salient region from pairs of images; in the
second network, these attention-guided hash codes are used to guide the
training of the second hashing network (i.e., these codes are treated as
supervised labels to train the second network). By doing this, DAgH can make
full use of the most critical information contained in images to guide the
second hashing network in order to learn efficient hash codes in a true
end-to-end fashion. Results from our experiments demonstrate that DAgH can
generate high quality hash codes and it outperforms current state-of-the-art
methods on three benchmark datasets, CIFAR-10, NUS-WIDE, and ImageNet.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/ge2021structured/">Structured Multi-modal Feature Embedding And Alignment For Image-sentence Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Structured Multi-modal Feature Embedding And Alignment For Image-sentence Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Structured Multi-modal Feature Embedding And Alignment For Image-sentence Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ge et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 29th ACM International Conference on Multimedia</td>
    <td>43</td>
    <td><p>The current state-of-the-art image-sentence retrieval methods implicitly
align the visual-textual fragments, like regions in images and words in
sentences, and adopt attention modules to highlight the relevance of
cross-modal semantic correspondences. However, the retrieval performance
remains unsatisfactory due to a lack of consistent representation in both
semantics and structural spaces. In this work, we propose to address the above
issue from two aspects: (i) constructing intrinsic structure (along with
relations) among the fragments of respective modalities, e.g., “dog \(\to\) play
\(\to\) ball” in semantic structure for an image, and (ii) seeking explicit
inter-modal structural and semantic correspondence between the visual and
textual modalities. In this paper, we propose a novel Structured Multi-modal
Feature Embedding and Alignment (SMFEA) model for image-sentence retrieval. In
order to jointly and explicitly learn the visual-textual embedding and the
cross-modal alignment, SMFEA creates a novel multi-modal structured module with
a shared context-aware referral tree. In particular, the relations of the
visual and textual fragments are modeled by constructing Visual Context-aware
Structured Tree encoder (VCS-Tree) and Textual Context-aware Structured Tree
encoder (TCS-Tree) with shared labels, from which visual and textual features
can be jointly learned and optimized. We utilize the multi-modal tree structure
to explicitly align the heterogeneous image-sentence data by maximizing the
semantic and structural similarity between corresponding inter-modal tree
nodes. Extensive experiments on Microsoft COCO and Flickr30K benchmarks
demonstrate the superiority of the proposed model in comparison to the
state-of-the-art methods.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/ganea2021incremental/">Incremental Few-shot Instance Segmentation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Incremental Few-shot Instance Segmentation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Incremental Few-shot Instance Segmentation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ganea Dan Andrei, Boom Bas, Poppe Ronald</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>55</td>
    <td><p>Few-shot instance segmentation methods are promising when labeled training
data for novel classes is scarce. However, current approaches do not facilitate
flexible addition of novel classes. They also require that examples of each
class are provided at train and test time, which is memory intensive. In this
paper, we address these limitations by presenting the first incremental
approach to few-shot instance segmentation: iMTFA. We learn discriminative
embeddings for object instances that are merged into class representatives.
Storing embedding vectors rather than images effectively solves the memory
overhead problem. We match these class embeddings at the RoI-level using cosine
similarity. This allows us to add new classes without the need for further
training or access to previous training data. In a series of experiments, we
consistently outperform the current state-of-the-art. Moreover, the reduced
memory requirements allow us to evaluate, for the first time, few-shot instance
segmentation performance on all classes in COCO jointly.</p>
</td>
    <td>
      
        Few Shot & Zero Shot 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/gadd2021unsupervised/">Unsupervised Place Recognition With Deep Embedding Learning Over Radar Videos</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Place Recognition With Deep Embedding Learning Over Radar Videos' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Place Recognition With Deep Embedding Learning Over Radar Videos' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gadd Matthew, de Martini Daniele, Newman Paul</td> <!-- 🔧 You were missing this -->
    <td>2021 20th International Conference on Advanced Robotics (ICAR)</td>
    <td>15</td>
    <td><p>We learn, in an unsupervised way, an embedding from sequences of radar images
that is suitable for solving place recognition problem using complex radar
data. We experiment on 280 km of data and show performance exceeding
state-of-the-art supervised approaches, localising correctly 98.38% of the time
when using just the nearest database candidate.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/fuentes2021sketch/">Sketch-qnet: A Quadruplet Convnet For Color Sketch-based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Sketch-qnet: A Quadruplet Convnet For Color Sketch-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Sketch-qnet: A Quadruplet Convnet For Color Sketch-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fuentes Anibal, Saavedra Jose M.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>14</td>
    <td><p>Architectures based on siamese networks with triplet loss have shown
outstanding performance on the image-based similarity search problem. This
approach attempts to discriminate between positive (relevant) and negative
(irrelevant) items. However, it undergoes a critical weakness. Given a query,
it cannot discriminate weakly relevant items, for instance, items of the same
type but different color or texture as the given query, which could be a
serious limitation for many real-world search applications. Therefore, in this
work, we present a quadruplet-based architecture that overcomes the
aforementioned weakness. Moreover, we present an instance of this quadruplet
network, which we call Sketch-QNet, to deal with the color sketch-based image
retrieval (CSBIR) problem, achieving new state-of-the-art results.</p>
</td>
    <td>
      
        Image Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/fu2021deep/">Deep Momentum Uncertainty Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Momentum Uncertainty Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Momentum Uncertainty Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fu et al.</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition</td>
    <td>15</td>
    <td><p>Combinatorial optimization (CO) has been a hot research topic because of its
theoretic and practical importance. As a classic CO problem, deep hashing aims
to find an optimal code for each data from finite discrete possibilities, while
the discrete nature brings a big challenge to the optimization process.
Previous methods usually mitigate this challenge by binary approximation,
substituting binary codes for real-values via activation functions or
regularizations. However, such approximation leads to uncertainty between
real-values and binary ones, degrading retrieval performance. In this paper, we
propose a novel Deep Momentum Uncertainty Hashing (DMUH). It explicitly
estimates the uncertainty during training and leverages the uncertainty
information to guide the approximation process. Specifically, we model
bit-level uncertainty via measuring the discrepancy between the output of a
hashing network and that of a momentum-updated network. The discrepancy of each
bit indicates the uncertainty of the hashing network to the approximate output
of that bit. Meanwhile, the mean discrepancy of all bits in a hashing code can
be regarded as image-level uncertainty. It embodies the uncertainty of the
hashing network to the corresponding input image. The hashing bit and image
with higher uncertainty are paid more attention during optimization. To the
best of our knowledge, this is the first work to study the uncertainty in
hashing bits. Extensive experiments are conducted on four datasets to verify
the superiority of our method, including CIFAR-10, NUS-WIDE, MS-COCO, and a
million-scale dataset Clothing1M. Our method achieves the best performance on
all of the datasets and surpasses existing state-of-the-art methods by a large
margin.</p>
</td>
    <td>
      
        Hashing Methods 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/fitzgerald2021moleman/">MOLEMAN: Mention-only Linking Of Entities With A Mention Annotation Network</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=MOLEMAN: Mention-only Linking Of Entities With A Mention Annotation Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=MOLEMAN: Mention-only Linking Of Entities With A Mention Annotation Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fitzgerald et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</td>
    <td>8</td>
    <td><p>We present an instance-based nearest neighbor approach to entity linking. In
contrast to most prior entity retrieval systems which represent each entity
with a single vector, we build a contextualized mention-encoder that learns to
place similar mentions of the same entity closer in vector space than mentions
of different entities. This approach allows all mentions of an entity to serve
as “class prototypes” as inference involves retrieving from the full set of
labeled entity mentions in the training set and applying the nearest mention
neighbor’s entity label. Our model is trained on a large multilingual corpus of
mention pairs derived from Wikipedia hyperlinks, and performs nearest neighbor
inference on an index of 700 million mentions. It is simpler to train, gives
more interpretable predictions, and outperforms all other systems on two
multilingual entity linking benchmarks.</p>
</td>
    <td>
      
        ACL 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/fernandes2021locality/">Locality Sensitive Hashing With Extended Differential Privacy</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Locality Sensitive Hashing With Extended Differential Privacy' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Locality Sensitive Hashing With Extended Differential Privacy' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fernandes Natasha, Kawamoto Yusuke, Murakami Takao</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>7</td>
    <td><p>Extended differential privacy, a generalization of standard differential
privacy (DP) using a general metric, has been widely studied to provide
rigorous privacy guarantees while keeping high utility. However, existing works
on extended DP are limited to few metrics, such as the Euclidean metric.
Consequently, they have only a small number of applications, such as
location-based services and document processing. In this paper, we propose a
couple of mechanisms providing extended DP with a different metric: angular
distance (or cosine distance). Our mechanisms are based on locality sensitive
hashing (LSH), which can be applied to the angular distance and work well for
personal data in a high-dimensional space. We theoretically analyze the privacy
properties of our mechanisms, and prove extended DP for input data by taking
into account that LSH preserves the original metric only approximately. We
apply our mechanisms to friend matching based on high-dimensional personal data
with angular distance in the local model, and evaluate our mechanisms using two
real datasets. We show that LDP requires a very large privacy budget and that
RAPPOR does not work in this application. Then we show that our mechanisms
enable friend matching with high utility and rigorous privacy guarantees based
on extended DP.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Hashing Methods 
      
        Privacy & Security 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/fang2021combating/">Combating Ambiguity For Hash-code Learning In Medical Instance Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Combating Ambiguity For Hash-code Learning In Medical Instance Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Combating Ambiguity For Hash-code Learning In Medical Instance Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fang et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Journal of Biomedical and Health Informatics</td>
    <td>10</td>
    <td><p>When encountering a dubious diagnostic case, medical instance retrieval can
help radiologists make evidence-based diagnoses by finding images containing
instances similar to a query case from a large image database. The similarity
between the query case and retrieved similar cases is determined by visual
features extracted from pathologically abnormal regions. However, the
manifestation of these regions often lacks specificity, i.e., different
diseases can have the same manifestation, and different manifestations may
occur at different stages of the same disease. To combat the manifestation
ambiguity in medical instance retrieval, we propose a novel deep framework
called Y-Net, encoding images into compact hash-codes generated from
convolutional features by feature aggregation. Y-Net can learn highly
discriminative convolutional features by unifying the pixel-wise segmentation
loss and classification loss. The segmentation loss allows exploring subtle
spatial differences for good spatial-discriminability while the classification
loss utilizes class-aware semantic information for good semantic-separability.
As a result, Y-Net can enhance the visual features in pathologically abnormal
regions and suppress the disturbing of the background during model training,
which could effectively embed discriminative features into the hash-codes in
the retrieval stage. Extensive experiments on two medical image datasets
demonstrate that Y-Net can alleviate the ambiguity of pathologically abnormal
regions and its retrieval performance outperforms the state-of-the-art method
by an average of 9.27% on the returned list of 10.</p>
</td>
    <td>
      
        Alt 
      
        ALT 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/fang2021deep/">Deep Triplet Hashing Network For Case-based Medical Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Triplet Hashing Network For Case-based Medical Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Triplet Hashing Network For Case-based Medical Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fang Jiansheng, Fu Huazhu, Liu Jiang</td> <!-- 🔧 You were missing this -->
    <td>Medical Image Analysis</td>
    <td>50</td>
    <td><p>Deep hashing methods have been shown to be the most efficient approximate
nearest neighbor search techniques for large-scale image retrieval. However,
existing deep hashing methods have a poor small-sample ranking performance for
case-based medical image retrieval. The top-ranked images in the returned query
results may be as a different class than the query image. This ranking problem
is caused by classification, regions of interest (ROI), and small-sample
information loss in the hashing space. To address the ranking problem, we
propose an end-to-end framework, called Attention-based Triplet Hashing (ATH)
network, to learn low-dimensional hash codes that preserve the classification,
ROI, and small-sample information. We embed a spatial-attention module into the
network structure of our ATH to focus on ROI information. The spatial-attention
module aggregates the spatial information of feature maps by utilizing
max-pooling, element-wise maximum, and element-wise mean operations jointly
along the channel axis. The triplet cross-entropy loss can help to map the
classification information of images and similarity between images into the
hash codes. Extensive experiments on two case-based medical datasets
demonstrate that our proposed ATH can further improve the retrieval performance
compared to the state-of-the-art deep hashing methods and boost the ranking
performance for small samples. Compared to the other loss methods, the triplet
cross-entropy loss can enhance the classification performance and hash
code-discriminability</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/esser2021faster/">A Faster Algorithm For Finding Closest Pairs In Hamming Metric</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Faster Algorithm For Finding Closest Pairs In Hamming Metric' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Faster Algorithm For Finding Closest Pairs In Hamming Metric' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Esser Andre, Kübler Robert, Zweydinger Floyd</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 1st Workshop on New Trends in Similarity Search</td>
    <td>7</td>
    <td><p>We study the Closest Pair Problem in Hamming metric, which asks to find the
pair with the smallest Hamming distance in a collection of binary vectors. We
give a new randomized algorithm for the problem on uniformly random input
outperforming previous approaches whenever the dimension of input points is
small compared to the dataset size. For moderate to large dimensions, our
algorithm matches the time complexity of the previously best-known locality
sensitive hashing based algorithms. Technically our algorithm follows similar
design principles as Dubiner (IEEE Trans. Inf. Theory 2010) and May-Ozerov
(Eurocrypt 2015). Besides improving the time complexity in the aforementioned
areas, we significantly simplify the analysis of these previous works. We give
a modular analysis, which allows us to investigate the performance of the
algorithm also on non-uniform input distributions. Furthermore, we give a proof
of concept implementation of our algorithm which performs well in comparison to
a quadratic search baseline. This is the first step towards answering an open
question raised by May and Ozerov regarding the practicability of algorithms
following these design principles.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/ertl2021setsketch/">Setsketch: Filling The Gap Between Minhash And Hyperloglog</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Setsketch: Filling The Gap Between Minhash And Hyperloglog' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Setsketch: Filling The Gap Between Minhash And Hyperloglog' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ertl Otmar</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the VLDB Endowment</td>
    <td>17</td>
    <td><p>MinHash and HyperLogLog are sketching algorithms that have become
indispensable for set summaries in big data applications. While HyperLogLog
allows counting different elements with very little space, MinHash is suitable
for the fast comparison of sets as it allows estimating the Jaccard similarity
and other joint quantities. This work presents a new data structure called
SetSketch that is able to continuously fill the gap between both use cases. Its
commutative and idempotent insert operation and its mergeable state make it
suitable for distributed environments. Fast, robust, and easy-to-implement
estimators for cardinality and joint quantities, as well as the ability to use
SetSketch for similarity search, enable versatile applications. The presented
joint estimator can also be applied to other data structures such as MinHash,
HyperLogLog, or HyperMinHash, where it even performs better than the
corresponding state-of-the-art estimators in many cases.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/engels2021practical/">Practical Near Neighbor Search Via Group Testing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Practical Near Neighbor Search Via Group Testing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Practical Near Neighbor Search Via Group Testing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Engels Joshua, Coleman Benjamin, Shrivastava Anshumali</td> <!-- 🔧 You were missing this -->
    <td>ACM SIGMOD Record</td>
    <td>12</td>
    <td><p>We present a new algorithm for the approximate near neighbor problem that
combines classical ideas from group testing with locality-sensitive hashing
(LSH). We reduce the near neighbor search problem to a group testing problem by
designating neighbors as “positives,” non-neighbors as “negatives,” and
approximate membership queries as group tests. We instantiate this framework
using distance-sensitive Bloom Filters to Identify Near-Neighbor Groups
(FLINNG). We prove that FLINNG has sub-linear query time and show that our
algorithm comes with a variety of practical advantages. For example, FLINNG can
be constructed in a single pass through the data, consists entirely of
efficient integer operations, and does not require any distance computations.
We conduct large-scale experiments on high-dimensional search tasks such as
genome search, URL similarity search, and embedding search over the massive
YFCC100M dataset. In our comparison with leading algorithms such as HNSW and
FAISS, we find that FLINNG can provide up to a 10x query speedup with
substantially smaller indexing time and memory.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/elnouby2021training/">Training Vision Transformers For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Training Vision Transformers For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Training Vision Transformers For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>El-nouby et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>120</td>
    <td><p>Transformers have shown outstanding results for natural language
understanding and, more recently, for image classification. We here extend this
work and propose a transformer-based approach for image retrieval: we adopt
vision transformers for generating image descriptors and train the resulting
model with a metric learning objective, which combines a contrastive loss with
a differential entropy regularizer. Our results show consistent and significant
improvements of transformers over convolution-based approaches. In particular,
our method outperforms the state of the art on several public benchmarks for
category-level retrieval, namely Stanford Online Product, In-Shop and CUB-200.
Furthermore, our experiments on ROxford and RParis also show that, in
comparable settings, transformers are competitive for particular object
retrieval, especially in the regime of short vector representations and
low-resolution images.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/dwibedi2021little/">With A Little Help From My Friends: Nearest-neighbor Contrastive Learning Of Visual Representations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=With A Little Help From My Friends: Nearest-neighbor Contrastive Learning Of Visual Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=With A Little Help From My Friends: Nearest-neighbor Contrastive Learning Of Visual Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dwibedi et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>293</td>
    <td><p>Self-supervised learning algorithms based on instance discrimination train
encoders to be invariant to pre-defined transformations of the same instance.
While most methods treat different views of the same image as positives for a
contrastive loss, we are interested in using positives from other instances in
the dataset. Our method, Nearest-Neighbor Contrastive Learning of visual
Representations (NNCLR), samples the nearest neighbors from the dataset in the
latent space, and treats them as positives. This provides more semantic
variations than pre-defined transformations.
  We find that using the nearest-neighbor as positive in contrastive losses
improves performance significantly on ImageNet classification, from 71.7% to
75.6%, outperforming previous state-of-the-art methods. On semi-supervised
learning benchmarks we improve performance significantly when only 1% ImageNet
labels are available, from 53.8% to 56.5%. On transfer learning benchmarks our
method outperforms state-of-the-art methods (including supervised learning with
ImageNet) on 8 out of 12 downstream datasets. Furthermore, we demonstrate
empirically that our method is less reliant on complex data augmentations. We
see a relative reduction of only 2.1% ImageNet Top-1 accuracy when we train
using only random crops.</p>
</td>
    <td>
      
        Self SUPERVISED 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/dutta2021when/">When Hashing Met Matching: Efficient Spatio-temporal Search For Ridesharing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=When Hashing Met Matching: Efficient Spatio-temporal Search For Ridesharing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=When Hashing Met Matching: Efficient Spatio-temporal Search For Ridesharing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dutta Chinmoy</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>5</td>
    <td><p>Carpooling, or sharing a ride with other passengers, holds immense potential
for urban transportation. Ridesharing platforms enable such sharing of rides
using real-time data. Finding ride matches in real-time at urban scale is a
difficult combinatorial optimization task and mostly heuristic approaches are
applied. In this work, we mathematically model the problem as that of finding
near-neighbors and devise a novel efficient spatio-temporal search algorithm
based on the theory of locality sensitive hashing for Maximum Inner Product
Search (MIPS). The proposed algorithm can find \(k\) near-optimal potential
matches for every ride from a pool of \(n\) rides in time \(O(n^{1 + \rho} (k +
log n) log k)\) and space \(O(n^{1 + \rho} log k)\) for a small \(\rho &lt; 1\). Our
algorithm can be extended in several useful and interesting ways increasing its
practical appeal. Experiments with large NY yellow taxi trip datasets show that
our algorithm consistently outperforms state-of-the-art heuristic methods
thereby proving its practical applicability.</p>
</td>
    <td>
      
        Hashing Methods 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/dzabraev2021mdmmt/">MDMMT: Multidomain Multimodal Transformer For Video Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=MDMMT: Multidomain Multimodal Transformer For Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=MDMMT: Multidomain Multimodal Transformer For Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dzabraev et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>105</td>
    <td><p>We present a new state-of-the-art on the text to video retrieval task on
MSRVTT and LSMDC benchmarks where our model outperforms all previous solutions
by a large margin. Moreover, state-of-the-art results are achieved with a
single model on two datasets without finetuning. This multidomain
generalisation is achieved by a proper combination of different video caption
datasets. We show that training on different datasets can improve test results
of each other. Additionally we check intersection between many popular datasets
and found that MSRVTT has a significant overlap between the test and the train
parts, and the same situation is observed for ActivityNet.</p>
</td>
    <td>
      
        Video Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/duan2021slade/">SLADE: A Self-training Framework For Distance Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=SLADE: A Self-training Framework For Distance Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=SLADE: A Self-training Framework For Distance Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Duan et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>8</td>
    <td><p>Most existing distance metric learning approaches use fully labeled data to
learn the sample similarities in an embedding space. We present a self-training
framework, SLADE, to improve retrieval performance by leveraging additional
unlabeled data. We first train a teacher model on the labeled data and use it
to generate pseudo labels for the unlabeled data. We then train a student model
on both labels and pseudo labels to generate final feature embeddings. We use
self-supervised representation learning to initialize the teacher model. To
better deal with noisy pseudo labels generated by the teacher network, we
design a new feature basis learning component for the student network, which
learns basis functions of feature representations for unlabeled data. The
learned basis vectors better measure the pairwise similarity and are used to
select high-confident samples for training the student network. We evaluate our
method on standard retrieval benchmarks: CUB-200, Cars-196 and In-shop.
Experimental results demonstrate that our approach significantly improves the
performance over the state-of-the-art methods.</p>
</td>
    <td>
      
        Tools & Libraries 
      
        Distance Metric Learning 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/dubey2021decade/">A Decade Survey Of Content Based Image Retrieval Using Deep Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Decade Survey Of Content Based Image Retrieval Using Deep Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Decade Survey Of Content Based Image Retrieval Using Deep Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dubey Shiv Ram</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Circuits and Systems for Video Technology</td>
    <td>242</td>
    <td><p>The content based image retrieval aims to find the similar images from a
large scale dataset against a query image. Generally, the similarity between
the representative features of the query image and dataset images is used to
rank the images for retrieval. In early days, various hand designed feature
descriptors have been investigated based on the visual cues such as color,
texture, shape, etc. that represent the images. However, the deep learning has
emerged as a dominating alternative of hand-designed feature engineering from a
decade. It learns the features automatically from the data. This paper presents
a comprehensive survey of deep learning based developments in the past decade
for content based image retrieval. The categorization of existing
state-of-the-art methods from different perspectives is also performed for
greater understanding of the progress. The taxonomy used in this survey covers
different supervision, different networks, different descriptor type and
different retrieval type. A performance analysis is also performed using the
state-of-the-art methods. The insights are also presented for the benefit of
the researchers to observe the progress and to make the best choices. The
survey presented in this paper will help in further research progress in image
retrieval using deep learning.</p>
</td>
    <td>
      
        Survey Paper 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/dong2021fine/">Fine-grained Fashion Similarity Prediction By Attribute-specific Embedding Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fine-grained Fashion Similarity Prediction By Attribute-specific Embedding Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fine-grained Fashion Similarity Prediction By Attribute-specific Embedding Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dong et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>32</td>
    <td><p>This paper strives to predict fine-grained fashion similarity. In this
similarity paradigm, one should pay more attention to the similarity in terms
of a specific design/attribute between fashion items. For example, whether the
collar designs of the two clothes are similar. It has potential value in many
fashion related applications, such as fashion copyright protection. To this
end, we propose an Attribute-Specific Embedding Network (ASEN) to jointly learn
multiple attribute-specific embeddings, thus measure the fine-grained
similarity in the corresponding space. The proposed ASEN is comprised of a
global branch and a local branch. The global branch takes the whole image as
input to extract features from a global perspective, while the local branch
takes as input the zoomed-in region-of-interest (RoI) w.r.t. the specified
attribute thus able to extract more fine-grained features. As the global branch
and the local branch extract the features from different perspectives, they are
complementary to each other. Additionally, in each branch, two attention
modules, i.e., Attribute-aware Spatial Attention and Attribute-aware Channel
Attention, are integrated to make ASEN be able to locate the related regions
and capture the essential patterns under the guidance of the specified
attribute, thus make the learned attribute-specific embeddings better reflect
the fine-grained similarity. Extensive experiments on three fashion-related
datasets, i.e., FashionAI, DARN, and DeepFashion, show the effectiveness of
ASEN for fine-grained fashion similarity prediction and its potential for
fashion reranking. Code and data are available at
https://github.com/maryeon/asenpp .</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/wang2021asymmetric/">Asymmetric Correlation Quantization Hashing For Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Asymmetric Correlation Quantization Hashing For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Asymmetric Correlation Quantization Hashing For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Lu, Yang Jie</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>21</td>
    <td><p>Due to the superiority in similarity computation and database storage for
large-scale multiple modalities data, cross-modal hashing methods have
attracted extensive attention in similarity retrieval across the heterogeneous
modalities. However, there are still some limitations to be further taken into
account: (1) most current CMH methods transform real-valued data points into
discrete compact binary codes under the binary constraints, limiting the
capability of representation for original data on account of abundant loss of
information and producing suboptimal hash codes; (2) the discrete binary
constraint learning model is hard to solve, where the retrieval performance may
greatly reduce by relaxing the binary constraints for large quantization error;
(3) handling the learning problem of CMH in a symmetric framework, leading to
difficult and complex optimization objective. To address above challenges, in
this paper, a novel Asymmetric Correlation Quantization Hashing (ACQH) method
is proposed. Specifically, ACQH learns the projection matrixs of heterogeneous
modalities data points for transforming query into a low-dimensional
real-valued vector in latent semantic space and constructs the stacked
compositional quantization embedding in a coarse-to-fine manner for indicating
database points by a series of learnt real-valued codeword in the codebook with
the help of pointwise label information regression simultaneously. Besides, the
unified hash codes across modalities can be directly obtained by the discrete
iterative optimization framework devised in the paper. Comprehensive
experiments on diverse three benchmark datasets have shown the effectiveness
and rationality of ACQH.</p>
</td>
    <td>
      
        Quantization 
      
        Hashing Methods 
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/yu2021deep/">Deep Graph-neighbor Coherence Preserving Network For Unsupervised Cross-modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Graph-neighbor Coherence Preserving Network For Unsupervised Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Graph-neighbor Coherence Preserving Network For Unsupervised Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>136</td>
    <td><p>Unsupervised cross-modal hashing (UCMH) has become a hot topic recently. Current UCMH focuses on exploring data similarities. However, current UCMH methods calculate the similarity between two data, mainly relying on the two data’s cross-modal features. These methods suffer from inaccurate similarity problems that result in a suboptimal retrieval Hamming space, because the cross-modal features between the data are not sufficient to describe the complex data relationships, such as situations where two data have different feature representations but share the inherent concepts. In this paper, we devise a deep graph-neighbor coherence preserving network (DGCPN). Specifically, DGCPN stems from graph models and explores graph-neighbor coherence by consolidating the information between data and their neighbors. DGCPN regulates comprehensive similarity preserving losses by exploiting three types of data similarities (i.e., the graph-neighbor coherence, the coexistent similarity, and the intra- and inter-modality consistency) and designs a half-real and half-binary optimization strategy to reduce the quantization errors during hashing. Essentially, DGCPN addresses the inaccurate similarity problem by exploring and exploiting the data’s intrinsic relationships in a graph. We conduct extensive experiments on three public UCMH datasets. The experimental results demonstrate the superiority of DGCPN, e.g., by improving the mean average precision from 0.722 to 0.751 on MIRFlickr-25K using 64-bit hashing codes to retrieval texts from images. We will release the source code package and the trained model on https://github.com/Atmegal/DGCPN.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
        AAAI 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/dash2021open/">Open Knowledge Graphs Canonicalization Using Variational Autoencoders</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Open Knowledge Graphs Canonicalization Using Variational Autoencoders' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Open Knowledge Graphs Canonicalization Using Variational Autoencoders' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dash et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</td>
    <td>12</td>
    <td><p>Noun phrases and Relation phrases in open knowledge graphs are not
canonicalized, leading to an explosion of redundant and ambiguous
subject-relation-object triples. Existing approaches to solve this problem take
a two-step approach. First, they generate embedding representations for both
noun and relation phrases, then a clustering algorithm is used to group them
using the embeddings as features. In this work, we propose Canonicalizing Using
Variational Autoencoders (CUVA), a joint model to learn both embeddings and
cluster assignments in an end-to-end approach, which leads to a better vector
representation for the noun and relation phrases. Our evaluation over multiple
benchmarks shows that CUVA outperforms the existing state-of-the-art
approaches. Moreover, we introduce CanonicNell, a novel dataset to evaluate
entity canonicalization systems.</p>
</td>
    <td>
      
        EMNLP 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/zolfaghari2021crossclr/">Crossclr: Cross-modal Contrastive Learning For Multi-modal Video Representations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Crossclr: Cross-modal Contrastive Learning For Multi-modal Video Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Crossclr: Cross-modal Contrastive Learning For Multi-modal Video Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zolfaghari et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>95</td>
    <td><p>Contrastive learning allows us to flexibly define powerful losses by
contrasting positive pairs from sets of negative samples. Recently, the
principle has also been used to learn cross-modal embeddings for video and
text, yet without exploiting its full potential. In particular, previous losses
do not take the intra-modality similarities into account, which leads to
inefficient embeddings, as the same content is mapped to multiple points in the
embedding space. With CrossCLR, we present a contrastive loss that fixes this
issue. Moreover, we define sets of highly related samples in terms of their
input embeddings and exclude them from the negative samples to avoid issues
with false negatives. We show that these principles consistently improve the
quality of the learned embeddings. The joint embeddings learned with CrossCLR
extend the state of the art in video-text retrieval on Youcook2 and LSMDC
datasets and in video captioning on Youcook2 dataset by a large margin. We also
demonstrate the generality of the concept by learning improved joint embeddings
for other pairs of modalities.</p>
</td>
    <td>
      
        Self SUPERVISED 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/dong2021using/">Using Text To Teach Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Using Text To Teach Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Using Text To Teach Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dong et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>5</td>
    <td><p>Image retrieval relies heavily on the quality of the data modeling and the
distance measurement in the feature space. Building on the concept of image
manifold, we first propose to represent the feature space of images, learned
via neural networks, as a graph. Neighborhoods in the feature space are now
defined by the geodesic distance between images, represented as graph vertices
or manifold samples. When limited images are available, this manifold is
sparsely sampled, making the geodesic computation and the corresponding
retrieval harder. To address this, we augment the manifold samples with
geometrically aligned text, thereby using a plethora of sentences to teach us
about images. In addition to extensive results on standard datasets
illustrating the power of text to help in image retrieval, a new public dataset
based on CLEVR is introduced to quantify the semantic similarity between visual
data and text data. The experimental results show that the joint embedding
manifold is a robust representation, allowing it to be a better basis to
perform image retrieval given only an image and a textual instruction on the
desired modifications over the image</p>
</td>
    <td>
      
        Image Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/%C3%B1anculef2021self/">Self-supervised Bernoulli Autoencoders For Semi-supervised Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Self-supervised Bernoulli Autoencoders For Semi-supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Self-supervised Bernoulli Autoencoders For Semi-supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ñanculef et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>5</td>
    <td><p>Semantic hashing is an emerging technique for large-scale similarity search
based on representing high-dimensional data using similarity-preserving binary
codes used for efficient indexing and search. It has recently been shown that
variational autoencoders, with Bernoulli latent representations parametrized by
neural nets, can be successfully trained to learn such codes in supervised and
unsupervised scenarios, improving on more traditional methods thanks to their
ability to handle the binary constraints architecturally. However, the scenario
where labels are scarce has not been studied yet.
  This paper investigates the robustness of hashing methods based on
variational autoencoders to the lack of supervision, focusing on two
semi-supervised approaches currently in use. The first augments the variational
autoencoder’s training objective to jointly model the distribution over the
data and the class labels. The second approach exploits the annotations to
define an additional pairwise loss that enforces consistency between the
similarity in the code (Hamming) space and the similarity in the label space.
Our experiments show that both methods can significantly increase the hash
codes’ quality. The pairwise approach can exhibit an advantage when the number
of labelled points is large. However, we found that this method degrades
quickly and loses its advantage when labelled samples decrease. To circumvent
this problem, we propose a novel supervision method in which the model uses its
label distribution predictions to implement the pairwise objective. Compared to
the best baseline, this procedure yields similar performance in fully
supervised settings but improves the results significantly when labelled data
is scarce. Our code is made publicly available at
https://github.com/amacaluso/SSB-VAE.</p>
</td>
    <td>
      
        Self SUPERVISED 
      
        Unsupervised 
      
        Neural Hashing 
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/wang2021cross/">Cross-batch Negative Sampling For Training Two-tower Recommenders</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-batch Negative Sampling For Training Two-tower Recommenders' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-batch Negative Sampling For Training Two-tower Recommenders' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Jinpeng, Zhu Jieming, He Xiuqiang</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>28</td>
    <td><p>The two-tower architecture has been widely applied for learning item and user
representations, which is important for large-scale recommender systems. Many
two-tower models are trained using various in-batch negative sampling
strategies, where the effects of such strategies inherently rely on the size of
mini-batches. However, training two-tower models with a large batch size is
inefficient, as it demands a large volume of memory for item and user contents
and consumes a lot of time for feature encoding. Interestingly, we find that
neural encoders can output relatively stable features for the same input after
warming up in the training process. Based on such facts, we propose a simple
yet effective sampling strategy called Cross-Batch Negative Sampling (CBNS),
which takes advantage of the encoded item embeddings from recent mini-batches
to boost the model training. Both theoretical analysis and empirical
evaluations demonstrate the effectiveness and the efficiency of CBNS.</p>
</td>
    <td>
      
        SIGIR 
      
        Text Retrieval 
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/croitoru2021teachtext/">TEACHTEXT: Crossmodal Generalized Distillation For Text-video Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=TEACHTEXT: Crossmodal Generalized Distillation For Text-video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=TEACHTEXT: Crossmodal Generalized Distillation For Text-video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Croitoru et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>108</td>
    <td><p>In recent years, considerable progress on the task of text-video retrieval
has been achieved by leveraging large-scale pretraining on visual and audio
datasets to construct powerful video encoders. By contrast, despite the natural
symmetry, the design of effective algorithms for exploiting large-scale
language pretraining remains under-explored. In this work, we are the first to
investigate the design of such algorithms and propose a novel generalized
distillation method, TeachText, which leverages complementary cues from
multiple text encoders to provide an enhanced supervisory signal to the
retrieval model. Moreover, we extend our method to video side modalities and
show that we can effectively reduce the number of used modalities at test time
without compromising performance. Our approach advances the state of the art on
several video retrieval benchmarks by a significant margin and adds no
computational overhead at test time. Last but not least, we show an effective
application of our method for eliminating noise from retrieval datasets. Code
and data can be found at https://www.robots.ox.ac.uk/~vgg/research/teachtext/.</p>
</td>
    <td>
      
        Video Retrieval 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/wang2021prototype/">Prototype-supervised Adversarial Network For Targeted Attack Of Deep Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Prototype-supervised Adversarial Network For Targeted Attack Of Deep Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Prototype-supervised Adversarial Network For Targeted Attack Of Deep Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>41</td>
    <td><p>Due to its powerful capability of representation learning and high-efficiency
computation, deep hashing has made significant progress in large-scale image
retrieval. However, deep hashing networks are vulnerable to adversarial
examples, which is a practical secure problem but seldom studied in
hashing-based retrieval field. In this paper, we propose a novel
prototype-supervised adversarial network (ProS-GAN), which formulates a
flexible generative architecture for efficient and effective targeted hashing
attack. To the best of our knowledge, this is the first generation-based method
to attack deep hashing networks. Generally, our proposed framework consists of
three parts, i.e., a PrototypeNet, a generator, and a discriminator.
Specifically, the designed PrototypeNet embeds the target label into the
semantic representation and learns the prototype code as the category-level
representative of the target label. Moreover, the semantic representation and
the original image are jointly fed into the generator for a flexible targeted
attack. Particularly, the prototype code is adopted to supervise the generator
to construct the targeted adversarial example by minimizing the Hamming
distance between the hash code of the adversarial example and the prototype
code. Furthermore, the generator is against the discriminator to simultaneously
encourage the adversarial examples visually realistic and the semantic
representation informative. Extensive experiments verify that the proposed
framework can efficiently produce adversarial examples with better targeted
attack performance and transferability over state-of-the-art targeted attack
methods of deep hashing. The related codes could be available at
https://github.com/xunguangwang/ProS-GAN .</p>
</td>
    <td>
      
        Robustness 
      
        CVPR 
      
        Neural Hashing 
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/coleman2021graph/">Graph Reordering For Cache-efficient Near Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Graph Reordering For Cache-efficient Near Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Graph Reordering For Cache-efficient Near Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Coleman et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>Graph search is one of the most successful algorithmic trends in near
neighbor search. Several of the most popular and empirically successful
algorithms are, at their core, a simple walk along a pruned near neighbor
graph. Such algorithms consistently perform at the top of industrial speed
benchmarks for applications such as embedding search. However, graph traversal
applications often suffer from poor memory access patterns, and near neighbor
search is no exception to this rule. Our measurements show that popular search
indices such as the hierarchical navigable small-world graph (HNSW) can have
poor cache miss performance. To address this problem, we apply graph reordering
algorithms to near neighbor graphs. Graph reordering is a memory layout
optimization that groups commonly-accessed nodes together in memory. We present
exhaustive experiments applying several reordering algorithms to a leading
graph-based near neighbor method based on the HNSW index. We find that
reordering improves the query time by up to 40%, and we demonstrate that the
time needed to reorder the graph is negligible compared to the time required to
construct the index.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/chun2021probabilistic/">Probabilistic Embeddings For Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Probabilistic Embeddings For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Probabilistic Embeddings For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chun et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>169</td>
    <td><p>Cross-modal retrieval methods build a common representation space for samples
from multiple modalities, typically from the vision and the language domains.
For images and their captions, the multiplicity of the correspondences makes
the task particularly challenging. Given an image (respectively a caption),
there are multiple captions (respectively images) that equally make sense. In
this paper, we argue that deterministic functions are not sufficiently powerful
to capture such one-to-many correspondences. Instead, we propose to use
Probabilistic Cross-Modal Embedding (PCME), where samples from the different
modalities are represented as probabilistic distributions in the common
embedding space. Since common benchmarks such as COCO suffer from
non-exhaustive annotations for cross-modal matches, we propose to additionally
evaluate retrieval on the CUB dataset, a smaller yet clean database where all
possible image-caption pairs are annotated. We extensively ablate PCME and
demonstrate that it not only improves the retrieval performance over its
deterministic counterpart but also provides uncertainty estimates that render
the embeddings more interpretable. Code is available at
https://github.com/naver-ai/pcme</p>
</td>
    <td>
      
        Multimodal Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/yu2021improving/">Improving Query Representations For Dense Retrieval With Pseudo Relevance Feedback</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Improving Query Representations For Dense Retrieval With Pseudo Relevance Feedback' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Improving Query Representations For Dense Retrieval With Pseudo Relevance Feedback' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yu Hongchien, Xiong Chenyan, Callan Jamie</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</td>
    <td>44</td>
    <td><p>Dense retrieval systems conduct first-stage retrieval using embedded
representations and simple similarity metrics to match a query to documents.
Its effectiveness depends on encoded embeddings to capture the semantics of
queries and documents, a challenging task due to the shortness and ambiguity of
search queries. This paper proposes ANCE-PRF, a new query encoder that uses
pseudo relevance feedback (PRF) to improve query representations for dense
retrieval. ANCE-PRF uses a BERT encoder that consumes the query and the top
retrieved documents from a dense retrieval model, ANCE, and it learns to
produce better query embeddings directly from relevance labels. It also keeps
the document index unchanged to reduce overhead. ANCE-PRF significantly
outperforms ANCE and other recent dense retrieval systems on several datasets.
Analysis shows that the PRF encoder effectively captures the relevant and
complementary information from PRF documents, while ignoring the noise with its
learned attention mechanism.</p>
</td>
    <td>
      
        CIKM 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/wang2021domain/">Domain-smoothing Network For Zero-shot Sketch-based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Domain-smoothing Network For Zero-shot Sketch-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Domain-smoothing Network For Zero-shot Sketch-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence</td>
    <td>28</td>
    <td><p>Zero-Shot Sketch-Based Image Retrieval (ZS-SBIR) is a novel cross-modal
retrieval task, where abstract sketches are used as queries to retrieve natural
images under zero-shot scenario. Most existing methods regard ZS-SBIR as a
traditional classification problem and employ a cross-entropy or triplet-based
loss to achieve retrieval, which neglect the problems of the domain gap between
sketches and natural images and the large intra-class diversity in sketches.
Toward this end, we propose a novel Domain-Smoothing Network (DSN) for ZS-SBIR.
Specifically, a cross-modal contrastive method is proposed to learn generalized
representations to smooth the domain gap by mining relations with additional
augmented samples. Furthermore, a category-specific memory bank with sketch
features is explored to reduce intra-class diversity in the sketch domain.
Extensive experiments demonstrate that our approach notably outperforms the
state-of-the-art methods in both Sketchy and TU-Berlin datasets. Our source
code is publicly available at https://github.com/haowang1992/DSN.</p>
</td>
    <td>
      
        Few Shot & Zero Shot 
      
        Image Retrieval 
      
        AAAI 
      
        IJCAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/cheng2021cnn/">CNN Retrieval Based Unsupervised Metric Learning For Near-duplicated Video Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=CNN Retrieval Based Unsupervised Metric Learning For Near-duplicated Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=CNN Retrieval Based Unsupervised Metric Learning For Near-duplicated Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cheng Hao, Wang Ping, Qi Chun</td> <!-- 🔧 You were missing this -->
    <td>2020 25th International Conference on Pattern Recognition (ICPR)</td>
    <td>16</td>
    <td><p>As important data carriers, the drastically increasing number of multimedia
videos often brings many duplicate and near-duplicate videos in the top results
of search. Near-duplicate video retrieval (NDVR) can cluster and filter out the
redundant contents. In this paper, the proposed NDVR approach extracts the
frame-level video representation based on convolutional neural network (CNN)
features from fully-connected layer and aggregated intermediate convolutional
layers. Unsupervised metric learning is used for similarity measurement and
feature matching. An efficient re-ranking algorithm combined with k-nearest
neighborhood fuses the retrieval results from two levels of features and
further improves the retrieval performance. Extensive experiments on the widely
used CC_WEB_VIDEO dataset shows that the proposed approach exhibits superior
performance over the state-of-the-art.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Video Retrieval 
      
        Distance Metric Learning 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/yuan2021multimodal/">Multimodal Contrastive Training For Visual Representation Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multimodal Contrastive Training For Visual Representation Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multimodal Contrastive Training For Visual Representation Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yuan et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>125</td>
    <td><p>We develop an approach to learning visual representations that embraces
multimodal data, driven by a combination of intra- and inter-modal similarity
preservation objectives. Unlike existing visual pre-training methods, which
solve a proxy prediction task in a single domain, our method exploits intrinsic
data properties within each modality and semantic information from cross-modal
correlation simultaneously, hence improving the quality of learned visual
representations. By including multimodal training in a unified framework with
different types of contrastive losses, our method can learn more powerful and
generic visual features. We first train our model on COCO and evaluate the
learned visual representations on various downstream tasks including image
classification, object detection, and instance segmentation. For example, the
visual representations pre-trained on COCO by our method achieve
state-of-the-art top-1 validation accuracy of \(55.3%\) on ImageNet
classification, under the common transfer protocol. We also evaluate our method
on the large-scale Stock images dataset and show its effectiveness on
multi-label image tagging, and cross-modal retrieval tasks.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/yuan2021exploring/">Exploring A Fine-grained Multiscale Method For Cross-modal Remote Sensing Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Exploring A Fine-grained Multiscale Method For Cross-modal Remote Sensing Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Exploring A Fine-grained Multiscale Method For Cross-modal Remote Sensing Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yuan et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Geoscience and Remote Sensing</td>
    <td>110</td>
    <td><p>Remote sensing (RS) cross-modal text-image retrieval has attracted extensive
attention for its advantages of flexible input and efficient query. However,
traditional methods ignore the characteristics of multi-scale and redundant
targets in RS image, leading to the degradation of retrieval accuracy. To cope
with the problem of multi-scale scarcity and target redundancy in RS multimodal
retrieval task, we come up with a novel asymmetric multimodal feature matching
network (AMFMN). Our model adapts to multi-scale feature inputs, favors
multi-source retrieval methods, and can dynamically filter redundant features.
AMFMN employs the multi-scale visual self-attention (MVSA) module to extract
the salient features of RS image and utilizes visual features to guide the text
representation. Furthermore, to alleviate the positive samples ambiguity caused
by the strong intraclass similarity in RS image, we propose a triplet loss
function with dynamic variable margin based on prior similarity of sample
pairs. Finally, unlike the traditional RS image-text dataset with coarse text
and higher intraclass similarity, we construct a fine-grained and more
challenging Remote sensing Image-Text Match dataset (RSITMD), which supports RS
image retrieval through keywords and sentence separately and jointly.
Experiments on four RS text-image datasets demonstrate that the proposed model
can achieve state-of-the-art performance in cross-modal RS text-image retrieval
task.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/wang2021scene/">Scene Text Retrieval Via Joint Text Detection And Similarity Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Scene Text Retrieval Via Joint Text Detection And Similarity Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Scene Text Retrieval Via Joint Text Detection And Similarity Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>41</td>
    <td><p>Scene text retrieval aims to localize and search all text instances from an
image gallery, which are the same or similar to a given query text. Such a task
is usually realized by matching a query text to the recognized words, outputted
by an end-to-end scene text spotter. In this paper, we address this problem by
directly learning a cross-modal similarity between a query text and each text
instance from natural images. Specifically, we establish an end-to-end
trainable network, jointly optimizing the procedures of scene text detection
and cross-modal similarity learning. In this way, scene text retrieval can be
simply performed by ranking the detected text instances with the learned
similarity. Experiments on three benchmark datasets demonstrate our method
consistently outperforms the state-of-the-art scene text spotting/retrieval
approaches. In particular, the proposed framework of joint detection and
similarity learning achieves significantly better performance than separated
methods. Code is available at: https://github.com/lanfeng4659/STR-TDSL.</p>
</td>
    <td>
      
        Text Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/yu2021unsupervised/">Unsupervised Multi-modal Hashing For Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Multi-modal Hashing For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Multi-modal Hashing For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yu Jun, Wu Xiao-jun</td> <!-- 🔧 You were missing this -->
    <td>Cognitive Computation</td>
    <td>9</td>
    <td><p>With the advantage of low storage cost and high efficiency, hashing learning
has received much attention in the domain of Big Data. In this paper, we
propose a novel unsupervised hashing learning method to cope with this open
problem to directly preserve the manifold structure by hashing. To address this
problem, both the semantic correlation in textual space and the locally
geometric structure in the visual space are explored simultaneously in our
framework. Besides, the `2;1-norm constraint is imposed on the projection
matrices to learn the discriminative hash function for each modality. Extensive
experiments are performed to evaluate the proposed method on the three publicly
available datasets and the experimental results show that our method can
achieve superior performance over the state-of-the-art methods.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
        Multimodal Retrieval 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/vasudeva2021loop/">Loop: Looking For Optimal Hard Negative Embeddings For Deep Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Loop: Looking For Optimal Hard Negative Embeddings For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Loop: Looking For Optimal Hard Negative Embeddings For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Vasudeva et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>12</td>
    <td><p>Deep metric learning has been effectively used to learn distance metrics for
different visual tasks like image retrieval, clustering, etc. In order to aid
the training process, existing methods either use a hard mining strategy to
extract the most informative samples or seek to generate hard synthetics using
an additional network. Such approaches face different challenges and can lead
to biased embeddings in the former case, and (i) harder optimization (ii)
slower training speed (iii) higher model complexity in the latter case. In
order to overcome these challenges, we propose a novel approach that looks for
optimal hard negatives (LoOp) in the embedding space, taking full advantage of
each tuple by calculating the minimum distance between a pair of positives and
a pair of negatives. Unlike mining-based methods, our approach considers the
entire space between pairs of embeddings to calculate the optimal hard
negatives. Extensive experiments combining our approach and representative
metric learning losses reveal a significant boost in performance on three
benchmark datasets.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/wang2021pseudo/">Pseudo-relevance Feedback For Multiple Representation Dense Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Pseudo-relevance Feedback For Multiple Representation Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Pseudo-relevance Feedback For Multiple Representation Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2021 ACM SIGIR International Conference on Theory of Information Retrieval</td>
    <td>54</td>
    <td><p>Pseudo-relevance feedback mechanisms, from Rocchio to the relevance models,
have shown the usefulness of expanding and reweighting the users’ initial
queries using information occurring in an initial set of retrieved documents,
known as the pseudo-relevant set. Recently, dense retrieval – through the use
of neural contextual language models such as BERT for analysing the documents’
and queries’ contents and computing their relevance scores – has shown a
promising performance on several information retrieval tasks still relying on
the traditional inverted index for identifying documents relevant to a query.
Two different dense retrieval families have emerged: the use of single embedded
representations for each passage and query (e.g. using BERT’s [CLS] token), or
via multiple representations (e.g. using an embedding for each token of the
query and document). In this work, we conduct the first study into the
potential for multiple representation dense retrieval to be enhanced using
pseudo-relevance feedback. In particular, based on the pseudo-relevant set of
documents identified using a first-pass dense retrieval, we extract
representative feedback embeddings (using KMeans clustering) – while ensuring
that these embeddings discriminate among passages (based on IDF) – which are
then added to the query representation. These additional feedback embeddings
are shown to both enhance the effectiveness of a reranking as well as an
additional dense retrieval operation. Indeed, experiments on the MSMARCO
passage ranking dataset show that MAP can be improved by upto 26% on the TREC
2019 query set and 10% on the TREC 2020 query set by the application of our
proposed ColBERT-PRF method on a ColBERT dense retrieval approach.</p>
</td>
    <td>
      
        SIGIR 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/chen2021efficient/">Efficient Object Embedding For Spliced Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Object Embedding For Spliced Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient Object Embedding For Spliced Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>12</td>
    <td><p>Detecting spliced images is one of the emerging challenges in computer
vision. Unlike prior methods that focus on detecting low-level artifacts
generated during the manipulation process, we use an image retrieval approach
to tackle this problem. When given a spliced query image, our goal is to
retrieve the original image from a database of authentic images. To achieve
this goal, we propose representing an image by its constituent objects based on
the intuition that the finest granularity of manipulations is oftentimes at the
object-level. We introduce a framework, object embeddings for spliced image
retrieval (OE-SIR), that utilizes modern object detectors to localize object
regions. Each region is then embedded and collectively used to represent the
image. Further, we propose a student-teacher training paradigm for learning
discriminative embeddings within object regions to avoid expensive multiple
forward passes. Detailed analysis of the efficacy of different feature
embedding models is also provided in this study. Extensive experimental results
show that the OE-SIR achieves state-of-the-art performance in spliced image
retrieval.</p>
</td>
    <td>
      
        Image Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/chen2021augnet/">Augnet: End-to-end Unsupervised Visual Representation Learning With Image Augmentation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Augnet: End-to-end Unsupervised Visual Representation Learning With Image Augmentation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Augnet: End-to-end Unsupervised Visual Representation Learning With Image Augmentation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>Most of the achievements in artificial intelligence so far were accomplished
by supervised learning which requires numerous annotated training data and thus
costs innumerable manpower for labeling. Unsupervised learning is one of the
effective solutions to overcome such difficulties. In our work, we propose
AugNet, a new deep learning training paradigm to learn image features from a
collection of unlabeled pictures. We develop a method to construct the
similarities between pictures as distance metrics in the embedding space by
leveraging the inter-correlation between augmented versions of samples. Our
experiments demonstrate that the method is able to represent the image in low
dimensional space and performs competitively in downstream tasks such as image
classification and image similarity comparison. Specifically, we achieved over
60% and 27% accuracy on the STL10 and CIFAR100 datasets with unsupervised
clustering, respectively. Moreover, unlike many deep-learning-based image
retrieval algorithms, our approach does not require access to external
annotated datasets to train the feature extractor, but still shows comparable
or even better feature representation ability and easy-to-use characteristics.
In our evaluations, the method outperforms all the state-of-the-art image
retrieval algorithms on some out-of-domain image datasets. The code for the
model implementation is available at
https://github.com/chenmingxiang110/AugNet.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/chen2021long/">Long-tail Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Long-tail Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Long-tail Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>9</td>
    <td><p>Hashing, which represents data items as compact binary codes, has
been becoming a more and more popular technique, e.g., for large-scale image retrieval, owing to its super fast search speed as well
as its extremely economical memory consumption. However, existing hashing methods all try to learn binary codes from artificially
balanced datasets which are not commonly available in real-world
scenarios. In this paper, we propose Long-Tail Hashing Network
(LTHNet), a novel two-stage deep hashing approach that addresses
the problem of learning to hash for more realistic datasets where
the data labels roughly exhibit a long-tail distribution. Specifically,
the first stage is to learn relaxed embeddings of the given dataset
with its long-tail characteristic taken into account via an end-to-end deep neural network; the second stage is to binarize those
obtained embeddings. A critical part of LTHNet is its extended dynamic meta-embedding module which can adaptively realize visual
knowledge transfer between head and tail classes, and thus enrich
image representations for hashing. Our experiments have shown
that LTHNet achieves dramatic performance improvements over all
state-of-the-art competitors on long-tail datasets, with no or little
sacrifice on balanced datasets. Further analyses reveal that while to
our surprise directly manipulating class weights in the loss function
has little effect, the extended dynamic meta-embedding module, the
usage of cross-entropy loss instead of square loss, and the relatively
small batch-size for training all contribute to LTHNet’s success.</p>
</td>
    <td>
      
        SIGIR 
      
        Hashing Methods 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/chen2021multimodal/">Multimodal Clustering Networks For Self-supervised Learning From Unlabeled Videos</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multimodal Clustering Networks For Self-supervised Learning From Unlabeled Videos' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multimodal Clustering Networks For Self-supervised Learning From Unlabeled Videos' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>52</td>
    <td><p>Multimodal self-supervised learning is getting more and more attention as it
allows not only to train large networks without human supervision but also to
search and retrieve data across various modalities. In this context, this paper
proposes a self-supervised training framework that learns a common multimodal
embedding space that, in addition to sharing representations across different
modalities, enforces a grouping of semantically similar instances. To this end,
we extend the concept of instance-level contrastive learning with a multimodal
clustering step in the training pipeline to capture semantic similarities
across modalities. The resulting embedding space enables retrieval of samples
across all modalities, even from unseen datasets and different domains. To
evaluate our approach, we train our model on the HowTo100M dataset and evaluate
its zero-shot retrieval capabilities in two challenging domains, namely
text-to-video retrieval, and temporal action localization, showing
state-of-the-art results on four different datasets.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Self SUPERVISED 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/wang2021distilling/">Distilling Knowledge By Mimicking Features</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Distilling Knowledge By Mimicking Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Distilling Knowledge By Mimicking Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Guo-hua, Ge Yifan, Wu Jianxin</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>20</td>
    <td><p>Knowledge distillation (KD) is a popular method to train efficient networks
(“student”) with the help of high-capacity networks (“teacher”). Traditional
methods use the teacher’s soft logits as extra supervision to train the student
network. In this paper, we argue that it is more advantageous to make the
student mimic the teacher’s features in the penultimate layer. Not only the
student can directly learn more effective information from the teacher feature,
feature mimicking can also be applied for teachers trained without a softmax
layer. Experiments show that it can achieve higher accuracy than traditional
KD. To further facilitate feature mimicking, we decompose a feature vector into
the magnitude and the direction. We argue that the teacher should give more
freedom to the student feature’s magnitude, and let the student pay more
attention on mimicking the feature direction. To meet this requirement, we
propose a loss term based on locality-sensitive hashing (LSH). With the help of
this new loss, our method indeed mimics feature directions more accurately,
relaxes constraints on feature magnitudes, and achieves state-of-the-art
distillation accuracy. We provide theoretical analyses of how LSH facilitates
feature direction mimicking, and further extend feature mimicking to
multi-label recognition and object detection.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/wang2021meta/">Meta Cross-modal Hashing On Long-tailed Data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Meta Cross-modal Hashing On Long-tailed Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Meta Cross-modal Hashing On Long-tailed Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>7</td>
    <td><p>Due to the advantage of reducing storage while speeding up query time on big
heterogeneous data, cross-modal hashing has been extensively studied for
approximate nearest neighbor search of multi-modal data. Most hashing methods
assume that training data is class-balanced.However, in practice, real world
data often have a long-tailed distribution. In this paper, we introduce a
meta-learning based cross-modal hashing method (MetaCMH) to handle long-tailed
data. Due to the lack of training samples in the tail classes, MetaCMH first
learns direct features from data in different modalities, and then introduces
an associative memory module to learn the memory features of samples of the
tail classes. It then combines the direct and memory features to obtain meta
features for each sample. For samples of the head classes of the long tail
distribution, the weight of the direct features is larger, because there are
enough training data to learn them well; while for rare classes, the weight of
the memory features is larger. Finally, MetaCMH uses a likelihood loss function
to preserve the similarity in different modalities and learns hash functions in
an end-to-end fashion. Experiments on long-tailed datasets show that MetaCMH
performs significantly better than state-of-the-art methods, especially on the
tail classes.</p>
</td>
    <td>
      
        Hashing Methods 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/changpinyo2021telling/">Telling The What While Pointing To The Where: Multimodal Queries For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Telling The What While Pointing To The Where: Multimodal Queries For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Telling The What While Pointing To The Where: Multimodal Queries For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Changpinyo et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>19</td>
    <td><p>Most existing image retrieval systems use text queries as a way for the user
to express what they are looking for. However, fine-grained image retrieval
often requires the ability to also express where in the image the content they
are looking for is. The text modality can only cumbersomely express such
localization preferences, whereas pointing is a more natural fit. In this
paper, we propose an image retrieval setup with a new form of multimodal
queries, where the user simultaneously uses both spoken natural language (the
what) and mouse traces over an empty canvas (the where) to express the
characteristics of the desired target image. We then describe simple
modifications to an existing image retrieval model, enabling it to operate in
this setup. Qualitative and quantitative experiments show that our model
effectively takes this spatial guidance into account, and provides
significantly more accurate retrieval results compared to text-only equivalent
systems.</p>
</td>
    <td>
      
        Image Retrieval 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/bianchi2021query2prod2vec/">Query2prod2vec Grounded Word Embeddings For Ecommerce</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Query2prod2vec Grounded Word Embeddings For Ecommerce' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Query2prod2vec Grounded Word Embeddings For Ecommerce' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Bianchi Federico, Tagliabue Jacopo, Yu Bingqing</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers</td>
    <td>11</td>
    <td><p>We present Query2Prod2Vec, a model that grounds lexical representations for
product search in product embeddings: in our model, meaning is a mapping
between words and a latent space of products in a digital shop. We leverage
shopping sessions to learn the underlying space and use merchandising
annotations to build lexical analogies for evaluation: our experiments show
that our model is more accurate than known techniques from the NLP and IR
literature. Finally, we stress the importance of data efficiency for product
search outside of retail giants, and highlight how Query2Prod2Vec fits with
practical constraints faced by most practitioners.</p>
</td>
    <td>
      
        EACL 
      
        NAACL 
      
        ACL 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/bianchi2021contrastive/">Contrastive Language-image Pre-training For The Italian Language</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Contrastive Language-image Pre-training For The Italian Language' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Contrastive Language-image Pre-training For The Italian Language' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Bianchi et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>19</td>
    <td><p>CLIP (Contrastive Language-Image Pre-training) is a very recent multi-modal
model that jointly learns representations of images and texts. The model is
trained on a massive amount of English data and shows impressive performance on
zero-shot classification tasks. Training the same model on a different language
is not trivial, since data in other languages might be not enough and the model
needs high-quality translations of the texts to guarantee a good performance.
In this paper, we present the first CLIP model for the Italian Language
(CLIP-Italian), trained on more than 1.4 million image-text pairs. Results show
that CLIP-Italian outperforms the multilingual CLIP model on the tasks of image
retrieval and zero-shot classification.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/bain2021condensed/">Condensed Movies: Story Based Retrieval With Contextual Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Condensed Movies: Story Based Retrieval With Contextual Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Condensed Movies: Story Based Retrieval With Contextual Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Bain et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>68</td>
    <td><p>Our objective in this work is long range understanding of the narrative
structure of movies. Instead of considering the entire movie, we propose to
learn from the `key scenes’ of the movie, providing a condensed look at the
full storyline. To this end, we make the following three contributions: (i) We
create the Condensed Movies Dataset (CMD) consisting of the key scenes from
over 3K movies: each key scene is accompanied by a high level semantic
description of the scene, character face-tracks, and metadata about the movie.
The dataset is scalable, obtained automatically from YouTube, and is freely
available for anybody to download and use. It is also an order of magnitude
larger than existing movie datasets in the number of movies; (ii) We provide a
deep network baseline for text-to-video retrieval on our dataset, combining
character, speech and visual cues into a single video embedding; and finally
(iii) We demonstrate how the addition of context from other video clips
improves retrieval performance.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/andoni2021average/">From Average Embeddings To Nearest Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=From Average Embeddings To Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=From Average Embeddings To Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Andoni Alexandr, Cheikhi David</td> <!-- 🔧 You were missing this -->
    <td>Journal of Mathematical Imaging and Vision</td>
    <td>6</td>
    <td><p>In this note, we show that one can use average embeddings, introduced
recently in [Naor’20, arXiv:1905.01280], to obtain efficient algorithms for
approximate nearest neighbor search. In particular, a metric \(X\) embeds into
\(ℓ₂\) on average, with distortion \(D\), if, for any distribution \(\mu\) on
\(X\), the embedding is \(D\) Lipschitz and the (square of) distance does not
decrease on average (wrt \(\mu\)). In particular existence of such an embedding
(assuming it is efficient) implies a \(O(D^3)\) approximate nearest neighbor
search under \(X\). This can be seen as a strengthening of the classic
(bi-Lipschitz) embedding approach to nearest neighbor search, and is another
application of data-dependent hashing paradigm.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/amrouche2021hashing/">Hashing And Metric Learning For Charged Particle Tracking</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing And Metric Learning For Charged Particle Tracking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing And Metric Learning For Charged Particle Tracking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Amrouche et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>We propose a novel approach to charged particle tracking at high intensity
particle colliders based on Approximate Nearest Neighbors search. With hundreds
of thousands of measurements per collision to be reconstructed e.g. at the High
Luminosity Large Hadron Collider, the currently employed combinatorial track
finding approaches become inadequate. Here, we use hashing techniques to
separate measurements into buckets of 20-50 hits and increase their purity
using metric learning. Two different approaches are studied to further resolve
tracks inside buckets: Local Fisher Discriminant Analysis and Neural Networks
for triplet similarity learning. We demonstrate the proposed approach on
simulated collisions and show significant speed improvement with bucket
tracking efficiency of 96% and a fake rate of 8% on unseen particle events.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Distance Metric Learning 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/agarwal2021dynamic/">Dynamic Enumeration Of Similarity Joins</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Dynamic Enumeration Of Similarity Joins' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Dynamic Enumeration Of Similarity Joins' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Agarwal et al.</td> <!-- 🔧 You were missing this -->
    <td>2008 IEEE 24th International Conference on Data Engineering</td>
    <td>10</td>
    <td><p>This paper considers enumerating answers to similarity-join queries under
dynamic updates: Given two sets of \(n\) points \(A,B\) in \(\mathbb{R}^d\), a metric
\(\phi(\cdot)\), and a distance threshold \(r &gt; 0\), report all pairs of points
\((a, b) \in A \times B\) with \(\phi(a,b) \le r\). Our goal is to store \(A,B\) into
a dynamic data structure that, whenever asked, can enumerate all result pairs
with worst-case delay guarantee, i.e., the time between enumerating two
consecutive pairs is bounded. Furthermore, the data structure can be
efficiently updated when a point is inserted into or deleted from \(A\) or \(B\).
  We propose several efficient data structures for answering similarity-join
queries in low dimension. For exact enumeration of similarity join, we present
near-linear-size data structures for \(\ell_1, \ell_\infty\) metrics with
\(log^{O(1)} n\) update time and delay. We show that such a data structure is
not feasible for the \(ℓ₂\) metric for \(d \ge 4\). For approximate enumeration
of similarity join, where the distance threshold is a soft constraint, we
obtain a unified linear-size data structure for \(\ell_p\) metric, with
\(log^{O(1)} n\) delay and update time. In high dimensions, we present an
efficient data structure with worst-case delay-guarantee using locality
sensitive hashing (LSH).</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/abouelnaga2021distillpose/">Distillpose: Lightweight Camera Localization Using Auxiliary Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Distillpose: Lightweight Camera Localization Using Auxiliary Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Distillpose: Lightweight Camera Localization Using Auxiliary Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Abouelnaga Yehya, Bui Mai, Ilic Slobodan</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</td>
    <td>7</td>
    <td><p>We propose a lightweight retrieval-based pipeline to predict 6DOF camera
poses from RGB images. Our pipeline uses a convolutional neural network (CNN)
to encode a query image as a feature vector. A nearest neighbor lookup finds
the pose-wise nearest database image. A siamese convolutional neural network
regresses the relative pose from the nearest neighboring database image to the
query image. The relative pose is then applied to the nearest neighboring
absolute pose to obtain the query image’s final absolute pose prediction. Our
model is a distilled version of NN-Net that reduces its parameters by 98.87%,
information retrieval feature vector size by 87.5%, and inference time by
89.18% without a significant decrease in localization accuracy.</p>
</td>
    <td>
      
        IROS 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/agrawal2021tag/">Tag Embedding Based Personalized Point Of Interest Recommendation System</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Tag Embedding Based Personalized Point Of Interest Recommendation System' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Tag Embedding Based Personalized Point Of Interest Recommendation System' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Agrawal Suraj, Roy Dwaipayan, Mitra Mandar</td> <!-- 🔧 You were missing this -->
    <td>Information Processing &amp; Management</td>
    <td>21</td>
    <td><p>Personalized Point of Interest recommendation is very helpful for satisfying
users’ needs at new places. In this article, we propose a tag embedding based
method for Personalized Recommendation of Point Of Interest. We model the
relationship between tags corresponding to Point Of Interest. The model
provides representative embedding corresponds to a tag in a way that related
tags will be closer. We model Point of Interest-based on tag embedding and also
model the users (user profile) based on the Point Of Interest rated by them.
finally, we rank the user’s candidate Point Of Interest based on cosine
similarity between user’s embedding and Point of Interest’s embedding. Further,
we find the parameters required to model user by discrete optimizing over
different measures (like ndcg@5, MRR, …). We also analyze the result while
considering the same parameters for all users and individual parameters for
each user. Along with it we also analyze the effect on the result while
changing the dataset to model the relationship between tags. Our method also
minimizes the privacy leak issue. We used TREC Contextual Suggestion 2016 Phase
2 dataset and have significant improvement over all the measures on the state
of the art method. It improves ndcg@5 by 12.8%, p@5 by 4.3%, and MRR by 7.8%,
which shows the effectiveness of the method.</p>
</td>
    <td>
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/zhang2021moon/">MOON: Multi-hash Codes Joint Learning For Cross-media Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=MOON: Multi-hash Codes Joint Learning For Cross-media Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=MOON: Multi-hash Codes Joint Learning For Cross-media Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang et al.</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition Letters</td>
    <td>10</td>
    <td><p>In recent years, cross-media hashing technique has attracted increasing
attention for its high computation efficiency and low storage cost. However,
the existing approaches still have some limitations, which need to be explored.
1) A fixed hash length (e.g., 16bits or 32bits) is predefined before learning
the binary codes. Therefore, these models need to be retrained when the hash
length changes, that consumes additional computation power, reducing the
scalability in practical applications. 2) Existing cross-modal approaches only
explore the information in the original multimedia data to perform the hash
learning, without exploiting the semantic information contained in the learned
hash codes. To this end, we develop a novel Multiple hash cOdes jOint learNing
method (MOON) for cross-media retrieval. Specifically, the developed MOON
synchronously learns the hash codes with multiple lengths in a unified
framework. Besides, to enhance the underlying discrimination, we combine the
clues from the multimodal data, semantic labels and learned hash codes for hash
learning. As far as we know, the proposed MOON is the first work to
simultaneously learn different length hash codes without retraining in
cross-media retrieval. Experiments on several databases show that our MOON can
achieve promising performance, outperforming some recent competitive shallow
and deep methods.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Multimodal Retrieval 
      
        Medical Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/wang2021comprehensive/">A Comprehensive Survey And Experimental Comparison Of Graph-based Approximate Nearest Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Comprehensive Survey And Experimental Comparison Of Graph-based Approximate Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Comprehensive Survey And Experimental Comparison Of Graph-based Approximate Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the VLDB Endowment</td>
    <td>119</td>
    <td><p>Approximate nearest neighbor search (ANNS) constitutes an important operation
in a multitude of applications, including recommendation systems, information
retrieval, and pattern recognition. In the past decade, graph-based ANNS
algorithms have been the leading paradigm in this domain, with dozens of
graph-based ANNS algorithms proposed. Such algorithms aim to provide effective,
efficient solutions for retrieving the nearest neighbors for a given query.
Nevertheless, these efforts focus on developing and optimizing algorithms with
different approaches, so there is a real need for a comprehensive survey about
the approaches’ relative performance, strengths, and pitfalls. Thus here we
provide a thorough comparative analysis and experimental evaluation of 13
representative graph-based ANNS algorithms via a new taxonomy and fine-grained
pipeline. We compared each algorithm in a uniform test environment on eight
real-world datasets and 12 synthetic datasets with varying sizes and
characteristics. Our study yields novel discoveries, offerings several useful
principles to improve algorithms, thus designing an optimized method that
outperforms the state-of-the-art algorithms. This effort also helped us
pinpoint algorithms’ working portions, along with rule-of-thumb recommendations
about promising research directions and suitable algorithms for practitioners
in different fields.</p>
</td>
    <td>
      
        Similarity Search 
      
        Graph Based ANN 
      
        Survey Paper 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/scribano2021all/">All You Can Embed: Natural Language Based Vehicle Retrieval With Spatio-temporal Transformers</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=All You Can Embed: Natural Language Based Vehicle Retrieval With Spatio-temporal Transformers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=All You Can Embed: Natural Language Based Vehicle Retrieval With Spatio-temporal Transformers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Scribano et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>7</td>
    <td><p>Combining Natural Language with Vision represents a unique and interesting
challenge in the domain of Artificial Intelligence. The AI City Challenge Track
5 for Natural Language-Based Vehicle Retrieval focuses on the problem of
combining visual and textual information, applied to a smart-city use case. In
this paper, we present All You Can Embed (AYCE), a modular solution to
correlate single-vehicle tracking sequences with natural language. The main
building blocks of the proposed architecture are (i) BERT to provide an
embedding of the textual descriptions, (ii) a convolutional backbone along with
a Transformer model to embed the visual information. For the training of the
retrieval model, a variation of the Triplet Margin Loss is proposed to learn a
distance measure between the visual and language embeddings. The code is
publicly available at https://github.com/cscribano/AYCE_2021.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/schuhmann2021laion/">LAION-400M: Open Dataset Of Clip-filtered 400 Million Image-text Pairs</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=LAION-400M: Open Dataset Of Clip-filtered 400 Million Image-text Pairs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=LAION-400M: Open Dataset Of Clip-filtered 400 Million Image-text Pairs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Schuhmann et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>312</td>
    <td><p>Multi-modal language-vision models trained on hundreds of millions of
image-text pairs (e.g. CLIP, DALL-E) gained a recent surge, showing remarkable
capability to perform zero- or few-shot learning and transfer even in absence
of per-sample labels on target image data. Despite this trend, to date there
has been no publicly available datasets of sufficient scale for training such
models from scratch. To address this issue, in a community effort we build and
release for public LAION-400M, a dataset with CLIP-filtered 400 million
image-text pairs, their CLIP embeddings and kNN indices that allow efficient
similarity search.</p>
</td>
    <td>
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/schubert2021triangle/">A Triangle Inequality For Cosine Similarity</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Triangle Inequality For Cosine Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Triangle Inequality For Cosine Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Schubert Erich</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>19</td>
    <td><p>Similarity search is a fundamental problem for many data analysis techniques.
Many efficient search techniques rely on the triangle inequality of metrics,
which allows pruning parts of the search space based on transitive bounds on
distances. Recently, Cosine similarity has become a popular alternative choice
to the standard Euclidean metric, in particular in the context of textual data
and neural network embeddings. Unfortunately, Cosine similarity is not metric
and does not satisfy the standard triangle inequality. Instead, many search
techniques for Cosine rely on approximation techniques such as locality
sensitive hashing. In this paper, we derive a triangle inequality for Cosine
similarity that is suitable for efficient similarity search with many standard
search structures (such as the VP-tree, Cover-tree, and M-tree); show that this
bound is tight and discuss fast approximations for it. We hope that this spurs
new research on accelerating exact similarity search for cosine similarity, and
possible other similarity measures beyond the existing work for distance
metrics.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/seidenschwarz2021learning/">Learning Intra-batch Connections For Deep Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Intra-batch Connections For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Intra-batch Connections For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Seidenschwarz Jenny, Elezi Ismail, Leal-taixé Laura</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>23</td>
    <td><p>The goal of metric learning is to learn a function that maps samples to a
lower-dimensional space where similar samples lie closer than dissimilar ones.
Particularly, deep metric learning utilizes neural networks to learn such a
mapping. Most approaches rely on losses that only take the relations between
pairs or triplets of samples into account, which either belong to the same
class or two different classes. However, these methods do not explore the
embedding space in its entirety. To this end, we propose an approach based on
message passing networks that takes all the relations in a mini-batch into
account. We refine embedding vectors by exchanging messages among all samples
in a given batch allowing the training process to be aware of its overall
structure. Since not all samples are equally important to predict a decision
boundary, we use an attention mechanism during message passing to allow samples
to weigh the importance of each neighbor accordingly. We achieve
state-of-the-art results on clustering and image retrieval on the CUB-200-2011,
Cars196, Stanford Online Products, and In-Shop Clothes datasets. To facilitate
further research, we make available the code and the models at
https://github.com/dvl-tum/intra_batch_connections.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/schubert2021graph/">Graph-based Non-linear Least Squares Optimization For Visual Place Recognition In Changing Environments</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Graph-based Non-linear Least Squares Optimization For Visual Place Recognition In Changing Environments' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Graph-based Non-linear Least Squares Optimization For Visual Place Recognition In Changing Environments' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Schubert Stefan, Neubert Peer, Protzel Peter</td> <!-- 🔧 You were missing this -->
    <td>IEEE Robotics and Automation Letters</td>
    <td>9</td>
    <td><p>Visual place recognition is an important subproblem of mobile robot
localization. Since it is a special case of image retrieval, the basic source
of information is the pairwise similarity of image descriptors. However, the
embedding of the image retrieval problem in this robotic task provides
additional structure that can be exploited, e.g. spatio-temporal consistency.
Several algorithms exist to exploit this structure, e.g., sequence processing
approaches or descriptor standardization approaches for changing environments.
In this paper, we propose a graph-based framework to systematically exploit
different types of additional structure and information. The graphical model is
used to formulate a non-linear least squares problem that can be optimized with
standard tools. Beyond sequences and standardization, we propose the usage of
intra-set similarities within the database and/or the query image set as
additional source of information. If available, our approach also allows to
seamlessly integrate additional knowledge about poses of database images. We
evaluate the system on a variety of standard place recognition datasets and
demonstrate performance improvements for a large number of different
configurations including different sources of information, different types of
constraints, and online or offline place recognition setups.</p>
</td>
    <td>
      
        Graph Based ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/sanakoyeu2021improving/">Improving Deep Metric Learning By Divide And Conquer</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Improving Deep Metric Learning By Divide And Conquer' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Improving Deep Metric Learning By Divide And Conquer' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sanakoyeu et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>16</td>
    <td><p>Deep metric learning (DML) is a cornerstone of many computer vision
applications. It aims at learning a mapping from the input domain to an
embedding space, where semantically similar objects are located nearby and
dissimilar objects far from another. The target similarity on the training data
is defined by user in form of ground-truth class labels. However, while the
embedding space learns to mimic the user-provided similarity on the training
data, it should also generalize to novel categories not seen during training.
Besides user-provided groundtruth training labels, a lot of additional visual
factors (such as viewpoint changes or shape peculiarities) exist and imply
different notions of similarity between objects, affecting the generalization
on the images unseen during training. However, existing approaches usually
directly learn a single embedding space on all available training data,
struggling to encode all different types of relationships, and do not
generalize well. We propose to build a more expressive representation by
jointly splitting the embedding space and the data hierarchically into smaller
sub-parts. We successively focus on smaller subsets of the training data,
reducing its variance and learning a different embedding subspace for each data
subset. Moreover, the subspaces are learned jointly to cover not only the
intricacies, but the breadth of the data as well. Only after that, we build the
final embedding from the subspaces in the conquering stage. The proposed
algorithm acts as a transparent wrapper that can be placed around arbitrary
existing DML methods. Our approach significantly improves upon the
state-of-the-art on image retrieval, clustering, and re-identification tasks
evaluated using CUB200-2011, CARS196, Stanford Online Products, In-shop
Clothes, and PKU VehicleID datasets.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/salvador2021revamping/">Revamping Cross-modal Recipe Retrieval With Hierarchical Transformers And Self-supervised Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Revamping Cross-modal Recipe Retrieval With Hierarchical Transformers And Self-supervised Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Revamping Cross-modal Recipe Retrieval With Hierarchical Transformers And Self-supervised Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Salvador et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>61</td>
    <td><p>Cross-modal recipe retrieval has recently gained substantial attention due to
the importance of food in people’s lives, as well as the availability of vast
amounts of digital cooking recipes and food images to train machine learning
models. In this work, we revisit existing approaches for cross-modal recipe
retrieval and propose a simplified end-to-end model based on well established
and high performing encoders for text and images. We introduce a hierarchical
recipe Transformer which attentively encodes individual recipe components
(titles, ingredients and instructions). Further, we propose a self-supervised
loss function computed on top of pairs of individual recipe components, which
is able to leverage semantic relationships within recipes, and enables training
using both image-recipe and recipe-only samples. We conduct a thorough analysis
and ablation studies to validate our design choices. As a result, our proposed
method achieves state-of-the-art performance in the cross-modal recipe
retrieval task on the Recipe1M dataset. We make code and models publicly
available.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Self SUPERVISED 
      
        Transformer Based ANN 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/yan2021hierarchical/">Hierarchical Attention Fusion For Geo-localization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hierarchical Attention Fusion For Geo-localization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hierarchical Attention Fusion For Geo-localization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yan et al.</td> <!-- 🔧 You were missing this -->
    <td>ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</td>
    <td>32</td>
    <td><p>Geo-localization is a critical task in computer vision. In this work, we cast
the geo-localization as a 2D image retrieval task. Current state-of-the-art
methods for 2D geo-localization are not robust to locate a scene with drastic
scale variations because they only exploit features from one semantic level for
image representations. To address this limitation, we introduce a hierarchical
attention fusion network using multi-scale features for geo-localization. We
extract the hierarchical feature maps from a convolutional neural network (CNN)
and organically fuse the extracted features for image representations. Our
training is self-supervised using adaptive weights to control the attention of
feature emphasis from each hierarchical level. Evaluation results on the image
retrieval and the large-scale geo-localization benchmarks indicate that our
method outperforms the existing state-of-the-art methods. Code is available
here: https://github.com/YanLiqi/HAF.</p>
</td>
    <td>
      
        ICASSP 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/ren2021pair/">PAIR: Leveraging Passage-centric Similarity Relation For Improving Dense Passage Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=PAIR: Leveraging Passage-centric Similarity Relation For Improving Dense Passage Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=PAIR: Leveraging Passage-centric Similarity Relation For Improving Dense Passage Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ren et al.</td> <!-- 🔧 You were missing this -->
    <td>Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</td>
    <td>22</td>
    <td><p>Recently, dense passage retrieval has become a mainstream approach to finding
relevant information in various natural language processing tasks. A number of
studies have been devoted to improving the widely adopted dual-encoder
architecture. However, most of the previous studies only consider query-centric
similarity relation when learning the dual-encoder retriever. In order to
capture more comprehensive similarity relations, we propose a novel approach
that leverages both query-centric and PAssage-centric sImilarity Relations
(called PAIR) for dense passage retrieval. To implement our approach, we make
three major technical contributions by introducing formal formulations of the
two kinds of similarity relations, generating high-quality pseudo labeled data
via knowledge distillation, and designing an effective two-stage training
procedure that incorporates passage-centric similarity relation constraint.
Extensive experiments show that our approach significantly outperforms previous
state-of-the-art models on both MSMARCO and Natural Questions datasets.</p>
</td>
    <td>
      
        ACL 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/ruta2021aladin/">ALADIN: All Layer Adaptive Instance Normalization For Fine-grained Style Similarity</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=ALADIN: All Layer Adaptive Instance Normalization For Fine-grained Style Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=ALADIN: All Layer Adaptive Instance Normalization For Fine-grained Style Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ruta et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>28</td>
    <td><p>We present ALADIN (All Layer AdaIN); a novel architecture for searching
images based on the similarity of their artistic style. Representation learning
is critical to visual search, where distance in the learned search embedding
reflects image similarity. Learning an embedding that discriminates
fine-grained variations in style is hard, due to the difficulty of defining and
labelling style. ALADIN takes a weakly supervised approach to learning a
representation for fine-grained style similarity of digital artworks,
leveraging BAM-FG, a novel large-scale dataset of user generated content
groupings gathered from the web. ALADIN sets a new state of the art accuracy
for style-based visual search over both coarse labelled style data (BAM) and
BAM-FG; a new 2.62 million image dataset of 310,000 fine-grained style
groupings also contributed by this work.</p>
</td>
    <td>
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/ravi2021buy/">Buy Me That Look: An Approach For Recommending Similar Fashion Products</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Buy Me That Look: An Approach For Recommending Similar Fashion Products' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Buy Me That Look: An Approach For Recommending Similar Fashion Products' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ravi et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE 4th International Conference on Multimedia Information Processing and Retrieval (MIPR)</td>
    <td>6</td>
    <td><p>Have you ever looked at an Instagram model, or a model in a fashion
e-commerce web-page, and thought \textit{“Wish I could get a list of fashion
items similar to the ones worn by the model!”}. This is what we address in this
paper, where we propose a novel computer vision based technique called
\textbf{ShopLook} to address the challenging problem of recommending similar
fashion products. The proposed method has been evaluated at Myntra
(www.myntra.com), a leading online fashion e-commerce platform. In particular,
given a user query and the corresponding Product Display Page (PDP) against the
query, the goal of our method is to recommend similar fashion products
corresponding to the entire set of fashion articles worn by a model in the PDP
full-shot image (the one showing the entire model from head to toe). The
novelty and strength of our method lies in its capability to recommend similar
articles for all the fashion items worn by the model, in addition to the
primary article corresponding to the query. This is not only important to
promote cross-sells for boosting revenue, but also for improving customer
experience and engagement. In addition, our approach is also capable of
recommending similar products for User Generated Content (UGC), eg., fashion
article images uploaded by users. Formally, our proposed method consists of the
following components (in the same order): i) Human keypoint detection, ii) Pose
classification, iii) Article localisation and object detection, along with
active learning feedback, and iv) Triplet network based image embedding model.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/radhakrishnan2021deep/">Deep Metric Learning For Ground Images</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Metric Learning For Ground Images' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Metric Learning For Ground Images' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Radhakrishnan et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>50</td>
    <td><p>Ground texture based localization methods are potential prospects for
low-cost, high-accuracy self-localization solutions for robots. These methods
estimate the pose of a given query image, i.e. the current observation of the
ground from a downward-facing camera, in respect to a set of reference images
whose poses are known in the application area. In this work, we deal with the
initial localization task, in which we have no prior knowledge about the
current robot positioning. In this situation, the localization method would
have to consider all available reference images. However, in order to reduce
computational effort and the risk of receiving a wrong result, we would like to
consider only those reference images that are actually overlapping with the
query image. For this purpose, we propose a deep metric learning approach that
retrieves the most similar reference images to the query image. In contrast to
existing approaches to image retrieval for ground images, our approach achieves
significantly better recall performance and improves the localization
performance of a state-of-the-art ground texture based localization method.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/ribeiro2021scene/">Scene Designer: A Unified Model For Scene Search And Synthesis From Sketch</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Scene Designer: A Unified Model For Scene Search And Synthesis From Sketch' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Scene Designer: A Unified Model For Scene Search And Synthesis From Sketch' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ribeiro et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)</td>
    <td>6</td>
    <td><p>Scene Designer is a novel method for searching and generating images using
free-hand sketches of scene compositions; i.e. drawings that describe both the
appearance and relative positions of objects. Our core contribution is a single
unified model to learn both a cross-modal search embedding for matching
sketched compositions to images, and an object embedding for layout synthesis.
We show that a graph neural network (GNN) followed by Transformer under our
novel contrastive learning setting is required to allow learning correlations
between object type, appearance and arrangement, driving a mask generation
module that synthesises coherent scene layouts, whilst also delivering state of
the art sketch based visual search of scenes.</p>
</td>
    <td>
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/portilloquintero2021straightforward/">A Straightforward Framework For Video Retrieval Using CLIP</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Straightforward Framework For Video Retrieval Using CLIP' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Straightforward Framework For Video Retrieval Using CLIP' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Portillo-quintero Jesús Andrés, Ortiz-bayliss José Carlos, Terashima-marín Hugo</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>76</td>
    <td><p>Video Retrieval is a challenging task where a text query is matched to a
video or vice versa. Most of the existing approaches for addressing such a
problem rely on annotations made by the users. Although simple, this approach
is not always feasible in practice. In this work, we explore the application of
the language-image model, CLIP, to obtain video representations without the
need for said annotations. This model was explicitly trained to learn a common
space where images and text can be compared. Using various techniques described
in this document, we extended its application to videos, obtaining
state-of-the-art results on the MSR-VTT and MSVD benchmarks.</p>
</td>
    <td>
      
        Video Retrieval 
      
        Tools & Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/qiu2021unsupervised/">Unsupervised Hashing With Contrastive Information Bottleneck</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Hashing With Contrastive Information Bottleneck' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Hashing With Contrastive Information Bottleneck' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Qiu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence</td>
    <td>51</td>
    <td><p>Many unsupervised hashing methods are implicitly established on the idea of
reconstructing the input data, which basically encourages the hashing codes to
retain as much information of original data as possible. However, this
requirement may force the models spending lots of their effort on
reconstructing the unuseful background information, while ignoring to preserve
the discriminative semantic information that is more important for the hashing
task. To tackle this problem, inspired by the recent success of contrastive
learning in learning continuous representations, we propose to adapt this
framework to learn binary hashing codes. Specifically, we first propose to
modify the objective function to meet the specific requirement of hashing and
then introduce a probabilistic binary representation layer into the model to
facilitate end-to-end training of the entire model. We further prove the strong
connection between the proposed contrastive-learning-based hashing method and
the mutual information, and show that the proposed model can be considered
under the broader framework of the information bottleneck (IB). Under this
perspective, a more general hashing model is naturally obtained. Extensive
experimental results on three benchmark image datasets demonstrate that the
proposed hashing method significantly outperforms existing baselines.</p>
</td>
    <td>
      
        IJCAI 
      
        Unsupervised 
      
        Neural Hashing 
      
        AAAI 
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/yamada2021efficient/">Efficient Passage Retrieval With Hashing For Open-domain Question Answering</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Passage Retrieval With Hashing For Open-domain Question Answering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient Passage Retrieval With Hashing For Open-domain Question Answering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yamada Ikuya, Asai Akari, Hajishirzi Hannaneh</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</td>
    <td>46</td>
    <td><p>Most state-of-the-art open-domain question answering systems use a neural
retrieval model to encode passages into continuous vectors and extract them
from a knowledge source. However, such retrieval models often require large
memory to run because of the massive size of their passage index. In this
paper, we introduce Binary Passage Retriever (BPR), a memory-efficient neural
retrieval model that integrates a learning-to-hash technique into the
state-of-the-art Dense Passage Retriever (DPR) to represent the passage index
using compact binary codes rather than continuous vectors. BPR is trained with
a multi-task objective over two tasks: efficient candidate generation based on
binary codes and accurate reranking based on continuous vectors. Compared with
DPR, BPR substantially reduces the memory cost from 65GB to 2GB without a loss
of accuracy on two standard open-domain question answering benchmarks: Natural
Questions and TriviaQA. Our code and trained models are available at
https://github.com/studio-ousia/bpr.</p>
</td>
    <td>
      
        Graph Based ANN 
      
        ACL 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/pham2021chef/">CHEF: Cross-modal Hierarchical Embeddings For Food Domain Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=CHEF: Cross-modal Hierarchical Embeddings For Food Domain Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=CHEF: Cross-modal Hierarchical Embeddings For Food Domain Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Pham et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>13</td>
    <td><p>Despite the abundance of multi-modal data, such as image-text pairs, there
has been little effort in understanding the individual entities and their
different roles in the construction of these data instances. In this work, we
endeavour to discover the entities and their corresponding importance in
cooking recipes automaticall} as a visual-linguistic association problem. More
specifically, we introduce a novel cross-modal learning framework to jointly
model the latent representations of images and text in the food image-recipe
association and retrieval tasks. This model allows one to discover complex
functional and hierarchical relationships between images and text, and among
textual parts of a recipe including title, ingredients and cooking
instructions. Our experiments show that by making use of efficient
tree-structured Long Short-Term Memory as the text encoder in our computational
cross-modal retrieval framework, we are not only able to identify the main
ingredients and cooking actions in the recipe descriptions without explicit
supervision, but we can also learn more meaningful feature representations of
food recipes, appropriate for challenging cross-modal retrieval and recipe
adaption tasks.</p>
</td>
    <td>
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/wieczorek2021unreasonable/">On The Unreasonable Effectiveness Of Centroids In Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=On The Unreasonable Effectiveness Of Centroids In Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=On The Unreasonable Effectiveness Of Centroids In Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wieczorek Mikolaj, Rychalska Barbara, Dabrowski Jacek</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>84</td>
    <td><p>Image retrieval task consists of finding similar images to a query image from
a set of gallery (database) images. Such systems are used in various
applications e.g. person re-identification (ReID) or visual product search.
Despite active development of retrieval models it still remains a challenging
task mainly due to large intra-class variance caused by changes in view angle,
lighting, background clutter or occlusion, while inter-class variance may be
relatively low. A large portion of current research focuses on creating more
robust features and modifying objective functions, usually based on Triplet
Loss. Some works experiment with using centroid/proxy representation of a class
to alleviate problems with computing speed and hard samples mining used with
Triplet Loss. However, these approaches are used for training alone and
discarded during the retrieval stage. In this paper we propose to use the mean
centroid representation both during training and retrieval. Such an aggregated
representation is more robust to outliers and assures more stable features. As
each class is represented by a single embedding - the class centroid - both
retrieval time and storage requirements are reduced significantly. Aggregating
multiple embeddings results in a significant reduction of the search space due
to lowering the number of candidate target vectors, which makes the method
especially suitable for production deployments. Comprehensive experiments
conducted on two ReID and Fashion Retrieval datasets demonstrate effectiveness
of our method, which outperforms the current state-of-the-art. We propose
centroid training and retrieval as a viable method for both Fashion Retrieval
and ReID applications.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/xu2021videoclip/">Videoclip: Contrastive Pre-training For Zero-shot Video-text Understanding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Videoclip: Contrastive Pre-training For Zero-shot Video-text Understanding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Videoclip: Contrastive Pre-training For Zero-shot Video-text Understanding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</td>
    <td>266</td>
    <td><p>We present VideoCLIP, a contrastive approach to pre-train a unified model for
zero-shot video and text understanding, without using any labels on downstream
tasks. VideoCLIP trains a transformer for video and text by contrasting
temporally overlapping positive video-text pairs with hard negatives from
nearest neighbor retrieval. Our experiments on a diverse series of downstream
tasks, including sequence-level text-video retrieval, VideoQA, token-level
action localization, and action segmentation reveal state-of-the-art
performance, surpassing prior work, and in some cases even outperforming
supervised approaches. Code is made available at
https://github.com/pytorch/fairseq/tree/main/examples/MMPT.</p>
</td>
    <td>
      
        EMNLP 
      
        Few Shot & Zero Shot 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/won2021multimodal/">Multimodal Metric Learning For Tag-based Music Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multimodal Metric Learning For Tag-based Music Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multimodal Metric Learning For Tag-based Music Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Won et al.</td> <!-- 🔧 You were missing this -->
    <td>ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</td>
    <td>28</td>
    <td><p>Tag-based music retrieval is crucial to browse large-scale music libraries
efficiently. Hence, automatic music tagging has been actively explored, mostly
as a classification task, which has an inherent limitation: a fixed vocabulary.
On the other hand, metric learning enables flexible vocabularies by using
pretrained word embeddings as side information. Also, metric learning has
already proven its suitability for cross-modal retrieval tasks in other domains
(e.g., text-to-image) by jointly learning a multimodal embedding space. In this
paper, we investigate three ideas to successfully introduce multimodal metric
learning for tag-based music retrieval: elaborate triplet sampling, acoustic
and cultural music information, and domain-specific word embeddings. Our
experimental results show that the proposed ideas enhance the retrieval system
quantitatively, and qualitatively. Furthermore, we release the MSD500, a subset
of the Million Song Dataset (MSD) containing 500 cleaned tags, 7 manually
annotated tag categories, and user taste profiles.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
        ICASSP 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/zeng2021phpq/">PHPQ: Pyramid Hybrid Pooling Quantization For Efficient Fine-grained Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=PHPQ: Pyramid Hybrid Pooling Quantization For Efficient Fine-grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=PHPQ: Pyramid Hybrid Pooling Quantization For Efficient Fine-grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zeng et al.</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition Letters</td>
    <td>9</td>
    <td><p>Deep hashing approaches, including deep quantization and deep binary hashing,
have become a common solution to large-scale image retrieval due to their high
computation and storage efficiency. Most existing hashing methods cannot
produce satisfactory results for fine-grained retrieval, because they usually
adopt the outputs of the last CNN layer to generate binary codes. Since deeper
layers tend to summarize visual clues, e.g., texture, into abstract semantics,
e.g., dogs and cats, the feature produced by the last CNN layer is less
effective in capturing subtle but discriminative visual details that mostly
exist in shallow layers. To improve fine-grained image hashing, we propose
Pyramid Hybrid Pooling Quantization (PHPQ). Specifically, we propose a Pyramid
Hybrid Pooling (PHP) module to capture and preserve fine-grained semantic
information from multi-level features, which emphasizes the subtle
discrimination of different sub-categories. Besides, we propose a learnable
quantization module with a partial codebook attention mechanism, which helps to
optimize the most relevant codewords and improves the quantization.
Comprehensive experiments on two widely-used public benchmarks, i.e.,
CUB-200-2011 and Stanford Dogs, demonstrate that PHPQ outperforms
state-of-the-art methods.</p>
</td>
    <td>
      
        Quantization 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/nguyen2021oscar/">Oscar-net: Object-centric Scene Graph Attention For Image Attribution</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Oscar-net: Object-centric Scene Graph Attention For Image Attribution' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Oscar-net: Object-centric Scene Graph Attention For Image Attribution' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Nguyen et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>14</td>
    <td><p>Images tell powerful stories but cannot always be trusted. Matching images
back to trusted sources (attribution) enables users to make a more informed
judgment of the images they encounter online. We propose a robust image hashing
algorithm to perform such matching. Our hash is sensitive to manipulation of
subtle, salient visual details that can substantially change the story told by
an image. Yet the hash is invariant to benign transformations (changes in
quality, codecs, sizes, shapes, etc.) experienced by images during online
redistribution. Our key contribution is OSCAR-Net (Object-centric Scene Graph
Attention for Image Attribution Network); a robust image hashing model inspired
by recent successes of Transformers in the visual domain. OSCAR-Net constructs
a scene graph representation that attends to fine-grained changes of every
object’s visual appearance and their spatial relationships. The network is
trained via contrastive learning on a dataset of original and manipulated
images yielding a state of the art image hash for content fingerprinting that
scales to millions of images.</p>
</td>
    <td>
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/nguyen2021deep/">A Deep Local And Global Scene-graph Matching For Image-text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Deep Local And Global Scene-graph Matching For Image-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Deep Local And Global Scene-graph Matching For Image-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Nguyen Manh-duy, Nguyen Binh T., Gurrin Cathal</td> <!-- 🔧 You were missing this -->
    <td>Frontiers in Artificial Intelligence and Applications</td>
    <td>24</td>
    <td><p>Conventional approaches to image-text retrieval mainly focus on indexing
visual objects appearing in pictures but ignore the interactions between these
objects. Such objects occurrences and interactions are equivalently useful and
important in this field as they are usually mentioned in the text. Scene graph
presentation is a suitable method for the image-text matching challenge and
obtained good results due to its ability to capture the inter-relationship
information. Both images and text are represented in scene graph levels and
formulate the retrieval challenge as a scene graph matching challenge. In this
paper, we introduce the Local and Global Scene Graph Matching (LGSGM) model
that enhances the state-of-the-art method by integrating an extra graph
convolution network to capture the general information of a graph.
Specifically, for a pair of scene graphs of an image and its caption, two
separate models are used to learn the features of each graph’s nodes and edges.
Then a Siamese-structure graph convolution model is employed to embed graphs
into vector forms. We finally combine the graph-level and the vector-level to
calculate the similarity of this image-text pair. The empirical experiments
show that our enhancement with the combination of levels can improve the
performance of the baseline method by increasing the recall by more than 10% on
the Flickr30k dataset.</p>
</td>
    <td>
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/wang2021weakly/">Weakly Supervised Deep Hyperspherical Quantization For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Weakly Supervised Deep Hyperspherical Quantization For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Weakly Supervised Deep Hyperspherical Quantization For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>9</td>
    <td><p>Deep quantization methods have shown high efficiency on large-scale image
retrieval. However, current models heavily rely on ground-truth information,
hindering the application of quantization in label-hungry scenarios. A more
realistic demand is to learn from inexhaustible uploaded images that are
associated with informal tags provided by amateur users. Though such sketchy
tags do not obviously reveal the labels, they actually contain useful semantic
information for supervising deep quantization. To this end, we propose
Weakly-Supervised Deep Hyperspherical Quantization (WSDHQ), which is the first
work to learn deep quantization from weakly tagged images. Specifically, 1) we
use word embeddings to represent the tags and enhance their semantic
information based on a tag correlation graph. 2) To better preserve semantic
information in quantization codes and reduce quantization error, we jointly
learn semantics-preserving embeddings and supervised quantizer on hypersphere
by employing a well-designed fusion layer and tailor-made loss functions.
Extensive experiments show that WSDHQ can achieve state-of-art performance on
weakly-supervised compact coding. Code is available at
https://github.com/gimpong/AAAI21-WSDHQ.</p>
</td>
    <td>
      
        Quantization 
      
        SUPERVISED 
      
        Image Retrieval 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/moskvyak2021keypoint/">Keypoint-aligned Embeddings For Image Retrieval And Re-identification</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Keypoint-aligned Embeddings For Image Retrieval And Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Keypoint-aligned Embeddings For Image Retrieval And Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Moskvyak et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>22</td>
    <td><p>Learning embeddings that are invariant to the pose of the object is crucial
in visual image retrieval and re-identification. The existing approaches for
person, vehicle, or animal re-identification tasks suffer from high intra-class
variance due to deformable shapes and different camera viewpoints. To overcome
this limitation, we propose to align the image embedding with a predefined
order of the keypoints. The proposed keypoint aligned embeddings model
(KAE-Net) learns part-level features via multi-task learning which is guided by
keypoint locations. More specifically, KAE-Net extracts channels from a feature
map activated by a specific keypoint through learning the auxiliary task of
heatmap reconstruction for this keypoint. The KAE-Net is compact, generic and
conceptually simple. It achieves state of the art performance on the benchmark
datasets of CUB-200-2011, Cars196 and VeRi-776 for retrieval and
re-identification tasks.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/morris2021contextual/">Contextual Document Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Contextual Document Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Contextual Document Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Morris John X., Rush Alexander M.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</td>
    <td>190</td>
    <td><p>Dense document embeddings are central to neural retrieval. The dominant
paradigm is to train and construct embeddings by running encoders directly on
individual documents. In this work, we argue that these embeddings, while
effective, are implicitly out-of-context for targeted use cases of retrieval,
and that a contextualized document embedding should take into account both the
document and neighboring documents in context - analogous to contextualized
word embeddings. We propose two complementary methods for contextualized
document embeddings: first, an alternative contrastive learning objective that
explicitly incorporates the document neighbors into the intra-batch contextual
loss; second, a new contextual architecture that explicitly encodes neighbor
document information into the encoded representation. Results show that both
methods achieve better performance than biencoders in several settings, with
differences especially pronounced out-of-domain. We achieve state-of-the-art
results on the MTEB benchmark with no hard negative mining, score distillation,
dataset-specific instructions, intra-GPU example-sharing, or extremely large
batch sizes. Our method can be applied to improve performance on any
contrastive learning dataset and any biencoder.</p>
</td>
    <td>
      
        ACL 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/miech2021thinking/">Thinking Fast And Slow: Efficient Text-to-visual Retrieval With Transformers</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Thinking Fast And Slow: Efficient Text-to-visual Retrieval With Transformers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Thinking Fast And Slow: Efficient Text-to-visual Retrieval With Transformers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Miech et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>111</td>
    <td><p>Our objective is language-based search of large-scale image and video
datasets. For this task, the approach that consists of independently mapping
text and vision to a joint embedding space, a.k.a. dual encoders, is attractive
as retrieval scales and is efficient for billions of images using approximate
nearest neighbour search. An alternative approach of using vision-text
transformers with cross-attention gives considerable improvements in accuracy
over the joint embeddings, but is often inapplicable in practice for
large-scale retrieval given the cost of the cross-attention mechanisms required
for each sample at test time. This work combines the best of both worlds. We
make the following three contributions. First, we equip transformer-based
models with a new fine-grained cross-attention architecture, providing
significant improvements in retrieval accuracy whilst preserving scalability.
Second, we introduce a generic approach for combining a Fast dual encoder model
with our Slow but accurate transformer-based model via distillation and
re-ranking. Finally, we validate our approach on the Flickr30K image dataset
where we show an increase in inference speed by several orders of magnitude
while having results competitive to the state of the art. We also extend our
method to the video domain, improving the state of the art on the VATEX
dataset.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/messina2021towards/">Towards Efficient Cross-modal Visual Textual Retrieval Using Transformer-encoder Deep Features</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Towards Efficient Cross-modal Visual Textual Retrieval Using Transformer-encoder Deep Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Towards Efficient Cross-modal Visual Textual Retrieval Using Transformer-encoder Deep Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Messina et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 International Conference on Content-Based Multimedia Indexing (CBMI)</td>
    <td>7</td>
    <td><p>Cross-modal retrieval is an important functionality in modern search engines,
as it increases the user experience by allowing queries and retrieved objects
to pertain to different modalities. In this paper, we focus on the
image-sentence retrieval task, where the objective is to efficiently find
relevant images for a given sentence (image-retrieval) or the relevant
sentences for a given image (sentence-retrieval). Computer vision literature
reports the best results on the image-sentence matching task using deep neural
networks equipped with attention and self-attention mechanisms. They evaluate
the matching performance on the retrieval task by performing sequential scans
of the whole dataset. This method does not scale well with an increasing amount
of images or captions. In this work, we explore different preprocessing
techniques to produce sparsified deep multi-modal features extracting them from
state-of-the-art deep-learning architectures for image-text matching. Our main
objective is to lay down the paths for efficient indexing of complex
multi-modal descriptions. We use the recently introduced TERN architecture as
an image-sentence features extractor. It is designed for producing fixed-size
1024-d vectors describing whole images and sentences, as well as
variable-length sets of 1024-d vectors describing the various building
components of the two modalities (image regions and sentence words
respectively). All these vectors are enforced by the TERN design to lie into
the same common space. Our experiments show interesting preliminary results on
the explored methods and suggest further experimentation in this important
research direction.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/messina2021fine/">Fine-grained Visual Textual Alignment For Cross-modal Retrieval Using Transformer Encoders</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fine-grained Visual Textual Alignment For Cross-modal Retrieval Using Transformer Encoders' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fine-grained Visual Textual Alignment For Cross-modal Retrieval Using Transformer Encoders' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Messina et al.</td> <!-- 🔧 You were missing this -->
    <td>ACM Transactions on Multimedia Computing, Communications, and Applications</td>
    <td>116</td>
    <td><p>Despite the evolution of deep-learning-based visual-textual processing
systems, precise multi-modal matching remains a challenging task. In this work,
we tackle the task of cross-modal retrieval through image-sentence matching
based on word-region alignments, using supervision only at the global
image-sentence level. Specifically, we present a novel approach called
Transformer Encoder Reasoning and Alignment Network (TERAN). TERAN enforces a
fine-grained match between the underlying components of images and sentences,
i.e., image regions and words, respectively, in order to preserve the
informative richness of both modalities. TERAN obtains state-of-the-art results
on the image retrieval task on both MS-COCO and Flickr30k datasets. Moreover,
on MS-COCO, it also outperforms current approaches on the sentence retrieval
task.
  Focusing on scalable cross-modal information retrieval, TERAN is designed to
keep the visual and textual data pipelines well separated. Cross-attention
links invalidate any chance to separately extract visual and textual features
needed for the online search and the offline indexing steps in large-scale
retrieval systems. In this respect, TERAN merges the information from the two
domains only during the final alignment phase, immediately before the loss
computation. We argue that the fine-grained alignments produced by TERAN pave
the way towards the research for effective and efficient methods for
large-scale cross-modal information retrieval. We compare the effectiveness of
our approach against relevant state-of-the-art methods. On the MS-COCO 1K test
set, we obtain an improvement of 5.7% and 3.5% respectively on the image and
the sentence retrieval tasks on the Recall@1 metric. The code used for the
experiments is publicly available on GitHub at
https://github.com/mesnico/TERAN.</p>
</td>
    <td>
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/messina2021transformer/">Transformer Reasoning Network For Image-text Matching And Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Transformer Reasoning Network For Image-text Matching And Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Transformer Reasoning Network For Image-text Matching And Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Messina et al.</td> <!-- 🔧 You were missing this -->
    <td>2020 25th International Conference on Pattern Recognition (ICPR)</td>
    <td>56</td>
    <td><p>Image-text matching is an interesting and fascinating task in modern AI
research. Despite the evolution of deep-learning-based image and text
processing systems, multi-modal matching remains a challenging problem. In this
work, we consider the problem of accurate image-text matching for the task of
multi-modal large-scale information retrieval. State-of-the-art results in
image-text matching are achieved by inter-playing image and text features from
the two different processing pipelines, usually using mutual attention
mechanisms. However, this invalidates any chance to extract separate visual and
textual features needed for later indexing steps in large-scale retrieval
systems. In this regard, we introduce the Transformer Encoder Reasoning Network
(TERN), an architecture built upon one of the modern relationship-aware
self-attentive architectures, the Transformer Encoder (TE). This architecture
is able to separately reason on the two different modalities and to enforce a
final common abstract concept space by sharing the weights of the deeper
transformer layers. Thanks to this design, the implemented network is able to
produce compact and very rich visual and textual features available for the
successive indexing step. Experiments are conducted on the MS-COCO dataset, and
we evaluate the results using a discounted cumulative gain metric with
relevance computed exploiting caption similarities, in order to assess possibly
non-exact but relevant search results. We demonstrate that on this metric we
are able to achieve state-of-the-art results in the image retrieval task. Our
code is freely available at https://github.com/mesnico/TERN.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/zhao2021embedding/">Embedding In Recommender Systems: A Survey</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Embedding In Recommender Systems: A Survey' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Embedding In Recommender Systems: A Survey' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhao et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>8</td>
    <td><p>Recommender systems have become an essential component of many online
platforms, providing personalized recommendations to users. A crucial aspect is
embedding techniques that coverts the high-dimensional discrete features, such
as user and item IDs, into low-dimensional continuous vectors and can enhance
the recommendation performance. Applying embedding techniques captures complex
entity relationships and has spurred substantial research. In this survey, we
provide an overview of the recent literature on embedding techniques in
recommender systems. This survey covers embedding methods like collaborative
filtering, self-supervised learning, and graph-based techniques. Collaborative
filtering generates embeddings capturing user-item preferences, excelling in
sparse data. Self-supervised methods leverage contrastive or generative
learning for various tasks. Graph-based techniques like node2vec exploit
complex relationships in network-rich environments. Addressing the scalability
challenges inherent to embedding methods, our survey delves into innovative
directions within the field of recommendation systems. These directions aim to
enhance performance and reduce computational complexity, paving the way for
improved recommender systems. Among these innovative approaches, we will
introduce Auto Machine Learning (AutoML), hash techniques, and quantization
techniques in this survey. We discuss various architectures and techniques and
highlight the challenges and future directions in these aspects. This survey
aims to provide a comprehensive overview of the state-of-the-art in this
rapidly evolving field and serve as a useful resource for researchers and
practitioners working in the area of recommender systems.</p>
</td>
    <td>
      
        Survey Paper 
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/sun2021lightningdot/">Lightningdot: Pre-training Visual-semantic Embeddings For Real-time Image-text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Lightningdot: Pre-training Visual-semantic Embeddings For Real-time Image-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Lightningdot: Pre-training Visual-semantic Embeddings For Real-time Image-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sun et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</td>
    <td>73</td>
    <td><p>Multimodal pre-training has propelled great advancement in
vision-and-language research. These large-scale pre-trained models, although
successful, fatefully suffer from slow inference speed due to enormous
computation cost mainly from cross-modal attention in Transformer architecture.
When applied to real-life applications, such latency and computation demand
severely deter the practical use of pre-trained models. In this paper, we study
Image-text retrieval (ITR), the most mature scenario of V+L application, which
has been widely studied even prior to the emergence of recent pre-trained
models. We propose a simple yet highly effective approach, LightningDOT that
accelerates the inference time of ITR by thousands of times, without
sacrificing accuracy. LightningDOT removes the time-consuming cross-modal
attention by pre-training on three novel learning objectives, extracting
feature indexes offline, and employing instant dot-product matching with
further re-ranking, which significantly speeds up retrieval process. In fact,
LightningDOT achieves new state of the art across multiple ITR benchmarks such
as Flickr30k, COCO and Multi30K, outperforming existing pre-trained models that
consume 1000x magnitude of computational hours. Code and pre-training
checkpoints are available at https://github.com/intersun/LightningDOT.</p>
</td>
    <td>
      
        EACL 
      
        NAACL 
      
        ACL 
      
        Efficiency 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/sun2021real/">Real-time Human Action Recognition Using Locally Aggregated Kinematic-guided Skeletonlet And Supervised Hashing-by-analysis Model</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Real-time Human Action Recognition Using Locally Aggregated Kinematic-guided Skeletonlet And Supervised Hashing-by-analysis Model' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Real-time Human Action Recognition Using Locally Aggregated Kinematic-guided Skeletonlet And Supervised Hashing-by-analysis Model' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sun et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Cybernetics</td>
    <td>15</td>
    <td><p>3D action recognition is referred to as the classification of action
sequences which consist of 3D skeleton joints. While many research work are
devoted to 3D action recognition, it mainly suffers from three problems: highly
complicated articulation, a great amount of noise, and a low implementation
efficiency. To tackle all these problems, we propose a real-time 3D action
recognition framework by integrating the locally aggregated kinematic-guided
skeletonlet (LAKS) with a supervised hashing-by-analysis (SHA) model. We first
define the skeletonlet as a few combinations of joint offsets grouped in terms
of kinematic principle, and then represent an action sequence using LAKS, which
consists of a denoising phase and a locally aggregating phase. The denoising
phase detects the noisy action data and adjust it by replacing all the features
within it with the features of the corresponding previous frame, while the
locally aggregating phase sums the difference between an offset feature of the
skeletonlet and its cluster center together over all the offset features of the
sequence. Finally, the SHA model which combines sparse representation with a
hashing model, aiming at promoting the recognition accuracy while maintaining a
high efficiency. Experimental results on MSRAction3D, UTKinectAction3D and
Florence3DAction datasets demonstrate that the proposed method outperforms
state-of-the-art methods in both recognition accuracy and implementation
efficiency.</p>
</td>
    <td>
      
        Unsupervised 
      
        Neural Hashing 
      
        SUPERVISED 
      
        Hashing Methods 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/sumbul2021deep/">Deep Learning For Image Search And Retrieval In Large Remote Sensing Archives</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Learning For Image Search And Retrieval In Large Remote Sensing Archives' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Learning For Image Search And Retrieval In Large Remote Sensing Archives' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sumbul Gencer, Kang Jian, Demir Begüm</td> <!-- 🔧 You were missing this -->
    <td>Deep Learning for the Earth Sciences</td>
    <td>24</td>
    <td><p>This chapter presents recent advances in content based image search and
retrieval (CBIR) systems in remote sensing (RS) for fast and accurate
information discovery from massive data archives. Initially, we analyze the
limitations of the traditional CBIR systems that rely on the hand-crafted RS
image descriptors. Then, we focus our attention on the advances in RS CBIR
systems for which deep learning (DL) models are at the forefront. In
particular, we present the theoretical properties of the most recent DL based
CBIR systems for the characterization of the complex semantic content of RS
images. After discussing their strengths and limitations, we present the deep
hashing based CBIR systems that have high time-efficient search capability
within huge data archives. Finally, the most promising research directions in
RS CBIR are discussed.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/zhan2021jointly/">Jointly Optimizing Query Encoder And Product Quantization To Improve Retrieval Performance</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Jointly Optimizing Query Encoder And Product Quantization To Improve Retrieval Performance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Jointly Optimizing Query Encoder And Product Quantization To Improve Retrieval Performance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhan et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</td>
    <td>58</td>
    <td><p>Recently, Information Retrieval community has witnessed fast-paced advances
in Dense Retrieval (DR), which performs first-stage retrieval with
embedding-based search. Despite the impressive ranking performance, previous
studies usually adopt brute-force search to acquire candidates, which is
prohibitive in practical Web search scenarios due to its tremendous memory
usage and time cost. To overcome these problems, vector compression methods
have been adopted in many practical embedding-based retrieval applications. One
of the most popular methods is Product Quantization (PQ). However, although
existing vector compression methods including PQ can help improve the
efficiency of DR, they incur severely decayed retrieval performance due to the
separation between encoding and compression. To tackle this problem, we present
JPQ, which stands for Joint optimization of query encoding and Product
Quantization. It trains the query encoder and PQ index jointly in an end-to-end
manner based on three optimization strategies, namely ranking-oriented loss, PQ
centroid optimization, and end-to-end negative sampling. We evaluate JPQ on two
publicly available retrieval benchmarks. Experimental results show that JPQ
significantly outperforms popular vector compression methods. Compared with
previous DR models that use brute-force search, JPQ almost matches the best
retrieval performance with 30x compression on index size. The compressed index
further brings 10x speedup on CPU and 2x speedup on GPU in query latency.</p>
</td>
    <td>
      
        Quantization 
      
        Evaluation 
      
        CIKM 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/stein2021self/">Self-supervised Similarity Search For Large Scientific Datasets</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Self-supervised Similarity Search For Large Scientific Datasets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Self-supervised Similarity Search For Large Scientific Datasets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Stein et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>13</td>
    <td><p>We present the use of self-supervised learning to explore and exploit large
unlabeled datasets. Focusing on 42 million galaxy images from the latest data
release of the Dark Energy Spectroscopic Instrument (DESI) Legacy Imaging
Surveys, we first train a self-supervised model to distill low-dimensional
representations that are robust to symmetries, uncertainties, and noise in each
image. We then use the representations to construct and publicly release an
interactive semantic similarity search tool. We demonstrate how our tool can be
used to rapidly discover rare objects given only a single example, increase the
speed of crowd-sourcing campaigns, and construct and improve training sets for
supervised applications. While we focus on images from sky surveys, the
technique is straightforward to apply to any scientific dataset of any
dimensionality. The similarity search web app can be found at
https://github.com/georgestein/galaxy_search</p>
</td>
    <td>
      
        Datasets 
      
        Similarity Search 
      
        SUPERVISED 
      
        Self SUPERVISED 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/staszewski2021new/">A New Approach To Descriptors Generation For Image Retrieval By Analyzing Activations Of Deep Neural Network Layers</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A New Approach To Descriptors Generation For Image Retrieval By Analyzing Activations Of Deep Neural Network Layers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A New Approach To Descriptors Generation For Image Retrieval By Analyzing Activations Of Deep Neural Network Layers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Staszewski et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Neural Networks and Learning Systems</td>
    <td>22</td>
    <td><p>In this paper, we consider the problem of descriptors construction for the
task of content-based image retrieval using deep neural networks. The idea of
neural codes, based on fully connected layers activations, is extended by
incorporating the information contained in convolutional layers. It is known
that the total number of neurons in the convolutional part of the network is
large and the majority of them have little influence on the final
classification decision. Therefore, in the paper we propose a novel algorithm
that allows us to extract the most significant neuron activations and utilize
this information to construct effective descriptors. The descriptors consisting
of values taken from both the fully connected and convolutional layers
perfectly represent the whole image content. The images retrieved using these
descriptors match semantically very well to the query image, and also they are
similar in other secondary image characteristics, like background, textures or
color distribution. These features of the proposed descriptors are verified
experimentally based on the IMAGENET1M dataset using the VGG16 neural network.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/singh2021freshdiskann/">Freshdiskann: A Fast And Accurate Graph-based ANN Index For Streaming Similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Freshdiskann: A Fast And Accurate Graph-based ANN Index For Streaming Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Freshdiskann: A Fast And Accurate Graph-based ANN Index For Streaming Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Singh et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>10</td>
    <td><p>Approximate nearest neighbor search (ANNS) is a fundamental building block in
information retrieval with graph-based indices being the current
state-of-the-art and widely used in the industry. Recent advances in
graph-based indices have made it possible to index and search billion-point
datasets with high recall and millisecond-level latency on a single commodity
machine with an SSD.
  However, existing graph algorithms for ANNS support only static indices that
cannot reflect real-time changes to the corpus required by many key real-world
scenarios (e.g. index of sentences in documents, email, or a news index). To
overcome this drawback, the current industry practice for manifesting updates
into such indices is to periodically re-build these indices, which can be
prohibitively expensive.
  In this paper, we present the first graph-based ANNS index that reflects
corpus updates into the index in real-time without compromising on search
performance. Using update rules for this index, we design FreshDiskANN, a
system that can index over a billion points on a workstation with an SSD and
limited memory, and support thousands of concurrent real-time inserts, deletes
and searches per second each, while retaining \(&gt;95%\) 5-recall@5. This
represents a 5-10x reduction in the cost of maintaining freshness in indices
when compared to existing methods.</p>
</td>
    <td>
      
        Vector Indexing 
      
        Graph Based ANN 
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/song2021deep/">Deep Robust Multilevel Semantic Cross-modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Robust Multilevel Semantic Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Robust Multilevel Semantic Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Song Ge, Zhao Jun, Tan Xiaoyang</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition</td>
    <td>21</td>
    <td><p>Hashing based cross-modal retrieval has recently made significant progress.
But straightforward embedding data from different modalities into a joint
Hamming space will inevitably produce false codes due to the intrinsic modality
discrepancy and noises. We present a novel Robust Multilevel Semantic Hashing
(RMSH) for more accurate cross-modal retrieval. It seeks to preserve
fine-grained similarity among data with rich semantics, while explicitly
require distances between dissimilar points to be larger than a specific value
for strong robustness. For this, we give an effective bound of this value based
on the information coding-theoretic analysis, and the above goals are embodied
into a margin-adaptive triplet loss. Furthermore, we introduce pseudo-codes via
fusing multiple hash codes to explore seldom-seen semantics, alleviating the
sparsity problem of similarity information. Experiments on three benchmarks
show the validity of the derived bounds, and our method achieves
state-of-the-art performance.</p>
</td>
    <td>
      
        Hashing Methods 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/wei2021pp/">Pp-shitu: A Practical Lightweight Image Recognition System</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Pp-shitu: A Practical Lightweight Image Recognition System' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Pp-shitu: A Practical Lightweight Image Recognition System' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wei et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>6</td>
    <td><p>In recent years, image recognition applications have developed rapidly. A
large number of studies and techniques have emerged in different fields, such
as face recognition, pedestrian and vehicle re-identification, landmark
retrieval, and product recognition. In this paper, we propose a practical
lightweight image recognition system, named PP-ShiTu, consisting of the
following 3 modules, mainbody detection, feature extraction and vector search.
We introduce popular strategies including metric learning, deep hash, knowledge
distillation and model quantization to improve accuracy and inference speed.
With strategies above, PP-ShiTu works well in different scenarios with a set of
models trained on a mixed dataset. Experiments on different datasets and
benchmarks show that the system is widely effective in different domains of
image recognition. All the above mentioned models are open-sourced and the code
is available in the GitHub repository PaddleClas on PaddlePaddle.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/tchayekondi2021new/">A New Hashing Based Nearest Neighbors Selection Technique For Big Datasets</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A New Hashing Based Nearest Neighbors Selection Technique For Big Datasets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A New Hashing Based Nearest Neighbors Selection Technique For Big Datasets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tchaye-kondi Jude, Zhai Yanlong, Zhu Liehuang</td> <!-- 🔧 You were missing this -->
    <td>Computer Science &amp; Information Technology (CS &amp; IT)</td>
    <td>5</td>
    <td><p>KNN has the reputation to be the word simplest but efficient supervised
learning algorithm used for either classification or regression. KNN prediction
efficiency highly depends on the size of its training data but when this
training data grows KNN suffers from slowness in making decisions since it
needs to search nearest neighbors within the entire dataset at each decision
making. This paper proposes a new technique that enables the selection of
nearest neighbors directly in the neighborhood of a given observation. The
proposed approach consists of dividing the data space into subcells of a
virtual grid built on top of data space. The mapping between the data points
and subcells is performed using hashing. When it comes to select the nearest
neighbors of a given observation, we firstly identify the cell the observation
belongs by using hashing, and then we look for nearest neighbors from that
central cell and cells around it layer by layer. From our experiment
performance analysis on publicly available datasets, our algorithm outperforms
the original KNN in time efficiency with a prediction quality as good as that
of KNN it also offers competitive performance with solutions like KDtree</p>
</td>
    <td>
      
        Datasets 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/tang2021when/">When Similarity Digest Meets Vector Management System: A Survey On Similarity Hash Function</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=When Similarity Digest Meets Vector Management System: A Survey On Similarity Hash Function' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=When Similarity Digest Meets Vector Management System: A Survey On Similarity Hash Function' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the VLDB Endowment</td>
    <td>95</td>
    <td><p>The booming vector manage system calls for feasible similarity hash function
as a front-end to perform similarity analysis. In this paper, we make a
systematical survey on the existent well-known similarity hash functions to
tease out the satisfied ones. We conclude that the similarity hash function
MinHash and Nilsimsa can be directly marshaled into the pipeline of similarity
analysis using vector manage system. After that, we make a brief and empirical
discussion on the performance, drawbacks of the these functions and highlight
MinHash, the variant of SimHash and feature hashing are the best for vector
management system for large-scale similarity analysis.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Survey Paper 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/tan2021instance/">Instance-level Image Retrieval Using Reranking Transformers</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Instance-level Image Retrieval Using Reranking Transformers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Instance-level Image Retrieval Using Reranking Transformers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tan Fuwen, Yuan Jiangbo, Ordonez Vicente</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>71</td>
    <td><p>Instance-level image retrieval is the task of searching in a large database
for images that match an object in a query image. To address this task, systems
usually rely on a retrieval step that uses global image descriptors, and a
subsequent step that performs domain-specific refinements or reranking by
leveraging operations such as geometric verification based on local features.
In this work, we propose Reranking Transformers (RRTs) as a general model to
incorporate both local and global features to rerank the matching images in a
supervised fashion and thus replace the relatively expensive process of
geometric verification. RRTs are lightweight and can be easily parallelized so
that reranking a set of top matching results can be performed in a single
forward-pass. We perform extensive experiments on the Revisited Oxford and
Paris datasets, and the Google Landmarks v2 dataset, showing that RRTs
outperform previous reranking approaches while using much fewer local
descriptors. Moreover, we demonstrate that, unlike existing approaches, RRTs
can be optimized jointly with the feature extractor, which can lead to feature
representations tailored to downstream tasks and further accuracy improvements.
The code and trained models are publicly available at
https://github.com/uvavision/RerankingTransformer.</p>
</td>
    <td>
      
        Hybrid ANN Methods 
      
        Image Retrieval 
      
        Re RANKING 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/tang2021improving/">Improving Document Representations By Generating Pseudo Query Embeddings For Dense Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Improving Document Representations By Generating Pseudo Query Embeddings For Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Improving Document Representations By Generating Pseudo Query Embeddings For Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</td>
    <td>23</td>
    <td><p>Recently, the retrieval models based on dense representations have been
gradually applied in the first stage of the document retrieval tasks, showing
better performance than traditional sparse vector space models. To obtain high
efficiency, the basic structure of these models is Bi-encoder in most cases.
However, this simple structure may cause serious information loss during the
encoding of documents since the queries are agnostic. To address this problem,
we design a method to mimic the queries on each of the documents by an
iterative clustering process and represent the documents by multiple pseudo
queries (i.e., the cluster centroids). To boost the retrieval process using
approximate nearest neighbor search library, we also optimize the matching
function with a two-step score calculation procedure. Experimental results on
several popular ranking and QA datasets show that our model can achieve
state-of-the-art results.</p>
</td>
    <td>
      
        ACL 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/jang2020generalized/">Generalized Product Quantization Network For Semi-supervised Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Generalized Product Quantization Network For Semi-supervised Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Generalized Product Quantization Network For Semi-supervised Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jang Young Kyun, Cho Nam Ik</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>41</td>
    <td><p>Image retrieval methods that employ hashing or vector quantization have
achieved great success by taking advantage of deep learning. However, these
approaches do not meet expectations unless expensive label information is
sufficient. To resolve this issue, we propose the first quantization-based
semi-supervised image retrieval scheme: Generalized Product Quantization (GPQ)
network. We design a novel metric learning strategy that preserves semantic
similarity between labeled data, and employ entropy regularization term to
fully exploit inherent potentials of unlabeled data. Our solution increases the
generalization capacity of the quantization network, which allows overcoming
previous limitations in the retrieval community. Extensive experimental results
demonstrate that GPQ yields state-of-the-art performance on large-scale real
image benchmark datasets.</p>
</td>
    <td>
      
        Quantization 
      
        Image Retrieval 
      
        SUPERVISED 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/ji2020attribute/">Attribute-guided Network For Cross-modal Zero-shot Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Attribute-guided Network For Cross-modal Zero-shot Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Attribute-guided Network For Cross-modal Zero-shot Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ji et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Neural Networks and Learning Systems</td>
    <td>83</td>
    <td><p>Zero-Shot Hashing aims at learning a hashing model that is trained only by
instances from seen categories but can generate well to those of unseen
categories. Typically, it is achieved by utilizing a semantic embedding space
to transfer knowledge from seen domain to unseen domain. Existing efforts
mainly focus on single-modal retrieval task, especially Image-Based Image
Retrieval (IBIR). However, as a highlighted research topic in the field of
hashing, cross-modal retrieval is more common in real world applications. To
address the Cross-Modal Zero-Shot Hashing (CMZSH) retrieval task, we propose a
novel Attribute-Guided Network (AgNet), which can perform not only IBIR, but
also Text-Based Image Retrieval (TBIR). In particular, AgNet aligns different
modal data into a semantically rich attribute space, which bridges the gap
caused by modality heterogeneity and zero-shot setting. We also design an
effective strategy that exploits the attribute to guide the generation of hash
codes for image and text within the same network. Extensive experimental
results on three benchmark datasets (AwA, SUN, and ImageNet) demonstrate the
superiority of AgNet on both cross-modal and single-modal zero-shot image
retrieval tasks.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Few Shot & Zero Shot 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/zhang2020learning/">Learning To Represent Image And Text With Denotation Graph</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning To Represent Image And Text With Denotation Graph' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning To Represent Image And Text With Denotation Graph' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</td>
    <td>23</td>
    <td><p>Learning to fuse vision and language information and representing them is an
important research problem with many applications. Recent progresses have
leveraged the ideas of pre-training (from language modeling) and attention
layers in Transformers to learn representation from datasets containing images
aligned with linguistic expressions that describe the images. In this paper, we
propose learning representations from a set of implied, visually grounded
expressions between image and text, automatically mined from those datasets. In
particular, we use denotation graphs to represent how specific concepts (such
as sentences describing images) can be linked to abstract and generic concepts
(such as short phrases) that are also visually grounded. This type of
generic-to-specific relations can be discovered using linguistic analysis
tools. We propose methods to incorporate such relations into learning
representation. We show that state-of-the-art multimodal learning models can be
further improved by leveraging automatically harvested structural relations.
The representations lead to stronger empirical results on downstream tasks of
cross-modal image retrieval, referring expression, and compositional
attribute-object recognition. Both our codes and the extracted denotation
graphs on the Flickr30K and the COCO datasets are publically available on
https://sha-lab.github.io/DG.</p>
</td>
    <td>
      
        EMNLP 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/hu2020pyretri/">Pyretri: A Pytorch-based Library For Unsupervised Image Retrieval By Deep Convolutional Neural Networks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Pyretri: A Pytorch-based Library For Unsupervised Image Retrieval By Deep Convolutional Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Pyretri: A Pytorch-based Library For Unsupervised Image Retrieval By Deep Convolutional Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 28th ACM International Conference on Multimedia</td>
    <td>18</td>
    <td><p>Despite significant progress of applying deep learning methods to the field
of content-based image retrieval, there has not been a software library that
covers these methods in a unified manner. In order to fill this gap, we
introduce PyRetri, an open source library for deep learning based unsupervised
image retrieval. The library encapsulates the retrieval process in several
stages and provides functionality that covers various prominent methods for
each stage. The idea underlying its design is to provide a unified platform for
deep learning based image retrieval research, with high usability and
extensibility. To the best of our knowledge, this is the first open-source
library for unsupervised image retrieval by deep learning.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Tools & Libraries 
      
        Image Retrieval 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/yang2020learning/">Learning Shared Semantic Space With Correlation Alignment For Cross-modal Event Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Shared Semantic Space With Correlation Alignment For Cross-modal Event Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Shared Semantic Space With Correlation Alignment For Cross-modal Event Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yang et al.</td> <!-- 🔧 You were missing this -->
    <td>ACM Transactions on Multimedia Computing, Communications, and Applications</td>
    <td>28</td>
    <td><p>In this paper, we propose to learn shared semantic space with correlation
alignment (\({S}^{3}CA\)) for multimodal data representations, which aligns
nonlinear correlations of multimodal data distributions in deep neural networks
designed for heterogeneous data. In the context of cross-modal (event)
retrieval, we design a neural network with convolutional layers and
fully-connected layers to extract features for images, including images on
Flickr-like social media. Simultaneously, we exploit a fully-connected neural
network to extract semantic features for texts, including news articles from
news media. In particular, nonlinear correlations of layer activations in the
two neural networks are aligned with correlation alignment during the joint
training of the networks. Furthermore, we project the multimodal data into a
shared semantic space for cross-modal (event) retrieval, where the distances
between heterogeneous data samples can be measured directly. In addition, we
contribute a Wiki-Flickr Event dataset, where the multimodal data samples are
not describing each other in pairs like the existing paired datasets, but all
of them are describing semantic events. Extensive experiments conducted on both
paired and unpaired datasets manifest the effectiveness of \({S}^{3}CA\),
outperforming the state-of-the-art methods.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/hu2020creating/">Creating Something From Nothing: Unsupervised Knowledge Distillation For Cross-modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Creating Something From Nothing: Unsupervised Knowledge Distillation For Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Creating Something From Nothing: Unsupervised Knowledge Distillation For Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hu et al.</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>117</td>
    <td><p>In recent years, cross-modal hashing (CMH) has attracted increasing
attentions, mainly because its potential ability of mapping contents from
different modalities, especially in vision and language, into the same space,
so that it becomes efficient in cross-modal data retrieval. There are two main
frameworks for CMH, differing from each other in whether semantic supervision
is required. Compared to the unsupervised methods, the supervised methods often
enjoy more accurate results, but require much heavier labors in data
annotation. In this paper, we propose a novel approach that enables guiding a
supervised method using outputs produced by an unsupervised method.
Specifically, we make use of teacher-student optimization for propagating
knowledge. Experiments are performed on two popular CMH benchmarks, i.e., the
MIRFlickr and NUS-WIDE datasets. Our approach outperforms all existing
unsupervised methods by a large margin.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
        Unsupervised 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/hu2020dasgil/">DASGIL: Domain Adaptation For Semantic And Geometric-aware Image-based Localization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=DASGIL: Domain Adaptation For Semantic And Geometric-aware Image-based Localization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=DASGIL: Domain Adaptation For Semantic And Geometric-aware Image-based Localization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hu et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>48</td>
    <td><p>Long-Term visual localization under changing environments is a challenging
problem in autonomous driving and mobile robotics due to season, illumination
variance, etc. Image retrieval for localization is an efficient and effective
solution to the problem. In this paper, we propose a novel multi-task
architecture to fuse the geometric and semantic information into the
multi-scale latent embedding representation for visual place recognition. To
use the high-quality ground truths without any human effort, the effective
multi-scale feature discriminator is proposed for adversarial training to
achieve the domain adaptation from synthetic virtual KITTI dataset to
real-world KITTI dataset. The proposed approach is validated on the Extended
CMU-Seasons dataset and Oxford RobotCar dataset through a series of crucial
comparison experiments, where our performance outperforms state-of-the-art
baselines for retrieval-based localization and large-scale place recognition
under the challenging environment.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/zhu2020ocor/">Ocor: An Overlapping-aware Code Retriever</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Ocor: An Overlapping-aware Code Retriever' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Ocor: An Overlapping-aware Code Retriever' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhu et al.</td> <!-- 🔧 You were missing this -->
    <td>ASE 2020 35th IEEE/ACM International Conference on Automated Software Engineering Proceedings</td>
    <td>9</td>
    <td><p>Code retrieval helps developers reuse the code snippet in the open-source
projects. Given a natural language description, code retrieval aims to search
for the most relevant code among a set of code. Existing state-of-the-art
approaches apply neural networks to code retrieval. However, these approaches
still fail to capture an important feature: overlaps. The overlaps between
different names used by different people indicate that two different names may
be potentially related (e.g., “message” and “msg”), and the overlaps between
identifiers in code and words in natural language descriptions indicate that
the code snippet and the description may potentially be related. To address
these problems, we propose a novel neural architecture named OCoR, where we
introduce two specifically-designed components to capture overlaps: the first
embeds identifiers by character to capture the overlaps between identifiers,
and the second introduces a novel overlap matrix to represent the degrees of
overlaps between each natural language word and each identifier.
  The evaluation was conducted on two established datasets. The experimental
results show that OCoR significantly outperforms the existing state-of-the-art
approaches and achieves 13.1% to 22.3% improvements. Moreover, we also
conducted several in-depth experiments to help understand the performance of
different components in OCoR.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/lin2020fast/">Fast Class-wise Updating For Online Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Class-wise Updating For Online Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Class-wise Updating For Online Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>22</td>
    <td><p>Online image hashing has received increasing research attention recently,
which processes large-scale data in a streaming fashion to update the hash
functions on-the-fly. To this end, most existing works exploit this problem
under a supervised setting, i.e., using class labels to boost the hashing
performance, which suffers from the defects in both adaptivity and efficiency:
First, large amounts of training batches are required to learn up-to-date hash
functions, which leads to poor online adaptivity. Second, the training is
time-consuming, which contradicts with the core need of online learning. In
this paper, a novel supervised online hashing scheme, termed Fast Class-wise
Updating for Online Hashing (FCOH), is proposed to address the above two
challenges by introducing a novel and efficient inner product operation. To
achieve fast online adaptivity, a class-wise updating method is developed to
decompose the binary code learning and alternatively renew the hash functions
in a class-wise fashion, which well addresses the burden on large amounts of
training batches. Quantitatively, such a decomposition further leads to at
least 75% storage saving. To further achieve online efficiency, we propose a
semi-relaxation optimization, which accelerates the online training by treating
different binary constraints independently. Without additional constraints and
variables, the time complexity is significantly reduced. Such a scheme is also
quantitatively shown to well preserve past information during updating hashing
functions. We have quantitatively demonstrated that the collective effort of
class-wise updating and semi-relaxation optimization provides a superior
performance comparing to various state-of-the-art methods, which is verified
through extensive experiments on three widely-used datasets.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/lin2020fashion/">Fashion Outfit Complementary Item Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fashion Outfit Complementary Item Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fashion Outfit Complementary Item Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin Yen-liang, Tran Son, Davis Larry S.</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>81</td>
    <td><p>Complementary fashion item recommendation is critical for fashion outfit
completion. Existing methods mainly focus on outfit compatibility prediction
but not in a retrieval setting. We propose a new framework for outfit
complementary item retrieval. Specifically, a category-based subspace attention
network is presented, which is a scalable approach for learning the subspace
attentions. In addition, we introduce an outfit ranking loss that better models
the item relationships of an entire outfit. We evaluate our method on the
outfit compatibility, FITB and new retrieval tasks. Experimental results
demonstrate that our approach outperforms state-of-the-art methods in both
compatibility prediction and complementary item retrieval</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/lin2020hadamard/">Hadamard Matrix Guided Online Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hadamard Matrix Guided Online Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hadamard Matrix Guided Online Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin et al.</td> <!-- 🔧 You were missing this -->
    <td>International Journal of Computer Vision</td>
    <td>41</td>
    <td><p>Online image hashing has attracted increasing research attention recently,
which receives large-scale data in a streaming manner to update the hash
functions on-the-fly. Its key challenge lies in the difficulty of balancing the
learning timeliness and model accuracy. To this end, most works follow a
supervised setting, i.e., using class labels to boost the hashing performance,
which defects in two aspects: First, strong constraints, e.g., orthogonal or
similarity preserving, are used, which however are typically relaxed and lead
to large accuracy drop. Second, large amounts of training batches are required
to learn the up-to-date hash functions, which largely increase the learning
complexity. To handle the above challenges, a novel supervised online hashing
scheme termed Hadamard Matrix Guided Online Hashing (HMOH) is proposed in this
paper. Our key innovation lies in introducing Hadamard matrix, which is an
orthogonal binary matrix built via Sylvester method. In particular, to release
the need of strong constraints, we regard each column of Hadamard matrix as the
target code for each class label, which by nature satisfies several desired
properties of hashing codes. To accelerate the online training, LSH is first
adopted to align the lengths of target code and to-be-learned binary code. We
then treat the learning of hash functions as a set of binary classification
problems to fit the assigned target code. Finally, extensive experiments
demonstrate the superior accuracy and efficiency of the proposed method over
various state-of-the-art methods. Codes are available at
https://github.com/lmbxmu/mycode.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/liang2020embedding/">Embedding-based Zero-shot Retrieval Through Query Generation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Embedding-based Zero-shot Retrieval Through Query Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Embedding-based Zero-shot Retrieval Through Query Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liang et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>23</td>
    <td><p>Passage retrieval addresses the problem of locating relevant passages,
usually from a large corpus, given a query. In practice, lexical term-matching
algorithms like BM25 are popular choices for retrieval owing to their
efficiency. However, term-based matching algorithms often miss relevant
passages that have no lexical overlap with the query and cannot be finetuned to
downstream datasets. In this work, we consider the embedding-based two-tower
architecture as our neural retrieval model. Since labeled data can be scarce
and because neural retrieval models require vast amounts of data to train, we
propose a novel method for generating synthetic training data for retrieval.
Our system produces remarkable results, significantly outperforming BM25 on 5
out of 6 datasets tested, by an average of 2.45 points for Recall@1. In some
cases, our model trained on synthetic data can even outperform the same model
trained on real data</p>
</td>
    <td>
      
        Few Shot & Zero Shot 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/xiong2020answering/">Answering Complex Open-domain Questions With Multi-hop Dense Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Answering Complex Open-domain Questions With Multi-hop Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Answering Complex Open-domain Questions With Multi-hop Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xiong et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>97</td>
    <td><p>We propose a simple and efficient multi-hop dense retrieval approach for
answering complex open-domain questions, which achieves state-of-the-art
performance on two multi-hop datasets, HotpotQA and multi-evidence FEVER.
Contrary to previous work, our method does not require access to any
corpus-specific information, such as inter-document hyperlinks or
human-annotated entity markers, and can be applied to any unstructured text
corpus. Our system also yields a much better efficiency-accuracy trade-off,
matching the best published accuracy on HotpotQA while being 10 times faster at
inference time.</p>
</td>
    <td>
      
        Graph Based ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/roy2020metric/">Metric-learning Based Deep Hashing Network For Content Based Retrieval Of Remote Sensing Images</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Metric-learning Based Deep Hashing Network For Content Based Retrieval Of Remote Sensing Images' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Metric-learning Based Deep Hashing Network For Content Based Retrieval Of Remote Sensing Images' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Roy et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Geoscience and Remote Sensing Letters</td>
    <td>75</td>
    <td><p>Hashing methods have been recently found very effective in retrieval of
remote sensing (RS) images due to their computational efficiency and fast
search speed. The traditional hashing methods in RS usually exploit
hand-crafted features to learn hash functions to obtain binary codes, which can
be insufficient to optimally represent the information content of RS images. To
overcome this problem, in this paper we introduce a metric-learning based
hashing network, which learns: 1) a semantic-based metric space for effective
feature representation; and 2) compact binary hash codes for fast archive
search. Our network considers an interplay of multiple loss functions that
allows to jointly learn a metric based semantic space facilitating similar
images to be clustered together in that target space and at the same time
producing compact final activations that lose negligible information when
binarized. Experiments carried out on two benchmark RS archives point out that
the proposed network significantly improves the retrieval performance under the
same retrieval time when compared to the state-of-the-art hashing methods in
RS.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        Distance Metric Learning 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/li2020sea/">SEA: Sentence Encoder Assembly For Video Retrieval By Textual Queries</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=SEA: Sentence Encoder Assembly For Video Retrieval By Textual Queries' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=SEA: Sentence Encoder Assembly For Video Retrieval By Textual Queries' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>51</td>
    <td><p>Retrieving unlabeled videos by textual queries, known as Ad-hoc Video Search
(AVS), is a core theme in multimedia data management and retrieval. The success
of AVS counts on cross-modal representation learning that encodes both query
sentences and videos into common spaces for semantic similarity computation.
Inspired by the initial success of previously few works in combining multiple
sentence encoders, this paper takes a step forward by developing a new and
general method for effectively exploiting diverse sentence encoders. The
novelty of the proposed method, which we term Sentence Encoder Assembly (SEA),
is two-fold. First, different from prior art that use only a single common
space, SEA supports text-video matching in multiple encoder-specific common
spaces. Such a property prevents the matching from being dominated by a
specific encoder that produces an encoding vector much longer than other
encoders. Second, in order to explore complementarities among the individual
common spaces, we propose multi-space multi-loss learning. As extensive
experiments on four benchmarks (MSR-VTT, TRECVID AVS 2016-2019, TGIF and MSVD)
show, SEA surpasses the state-of-the-art. In addition, SEA is extremely ease to
implement. All this makes SEA an appealing solution for AVS and promising for
continuously advancing the task by harvesting new sentence encoders.</p>
</td>
    <td>
      
        Video Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/vaccaro2020image/">Image Retrieval Using Multi-scale CNN Features Pooling</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Image Retrieval Using Multi-scale CNN Features Pooling' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Image Retrieval Using Multi-scale CNN Features Pooling' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Vaccaro et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2020 International Conference on Multimedia Retrieval</td>
    <td>16</td>
    <td><p>In this paper, we address the problem of image retrieval by learning images
representation based on the activations of a Convolutional Neural Network. We
present an end-to-end trainable network architecture that exploits a novel
multi-scale local pooling based on NetVLAD and a triplet mining procedure based
on samples difficulty to obtain an effective image representation. Extensive
experiments show that our approach is able to reach state-of-the-art results on
three standard datasets.</p>
</td>
    <td>
      
        Multimodal Retrieval 
      
        Image Retrieval 
      
        Medical Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/xu2020learning/">On Learning Semantic Representations For Million-scale Free-hand Sketches</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=On Learning Semantic Representations For Million-scale Free-hand Sketches' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=On Learning Semantic Representations For Million-scale Free-hand Sketches' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xu et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Circuits and Systems for Video Technology</td>
    <td>10</td>
    <td><p>In this paper, we study learning semantic representations for million-scale
free-hand sketches. This is highly challenging due to the domain-unique traits
of sketches, e.g., diverse, sparse, abstract, noisy. We propose a dual-branch
CNNRNN network architecture to represent sketches, which simultaneously encodes
both the static and temporal patterns of sketch strokes. Based on this
architecture, we further explore learning the sketch-oriented semantic
representations in two challenging yet practical settings, i.e., hashing
retrieval and zero-shot recognition on million-scale sketches. Specifically, we
use our dual-branch architecture as a universal representation framework to
design two sketch-specific deep models: (i) We propose a deep hashing model for
sketch retrieval, where a novel hashing loss is specifically designed to
accommodate both the abstract and messy traits of sketches. (ii) We propose a
deep embedding model for sketch zero-shot recognition, via collecting a
large-scale edge-map dataset and proposing to extract a set of semantic vectors
from edge-maps as the semantic knowledge for sketch zero-shot domain alignment.
Both deep models are evaluated by comprehensive experiments on million-scale
sketches and outperform the state-of-the-art competitors.</p>
</td>
    <td>
      
        Large Scale Search 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/li2020discriminative/">Discriminative Multi-view Privileged Information Learning For Image Re-ranking</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Discriminative Multi-view Privileged Information Learning For Image Re-ranking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Discriminative Multi-view Privileged Information Learning For Image Re-ranking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>10</td>
    <td><p>Conventional multi-view re-ranking methods usually perform asymmetrical
matching between the region of interest (ROI) in the query image and the whole
target image for similarity computation. Due to the inconsistency in the visual
appearance, this practice tends to degrade the retrieval accuracy particularly
when the image ROI, which is usually interpreted as the image objectness,
accounts for a smaller region in the image. Since Privileged Information (PI),
which can be viewed as the image prior, enables well characterizing the image
objectness, we are aiming at leveraging PI for further improving the
performance of the multi-view re-ranking accuracy in this paper. Towards this
end, we propose a discriminative multi-view re-ranking approach in which both
the original global image visual contents and the local auxiliary PI features
are simultaneously integrated into a unified training framework for generating
the latent subspaces with sufficient discriminating power. For the on-the-fly
re-ranking, since the multi-view PI features are unavailable, we only project
the original multi-view image representations onto the latent subspace, and
thus the re-ranking can be achieved by computing and sorting the distances from
the multi-view embeddings to the separating hyperplane. Extensive experimental
evaluations on the two public benchmarks Oxford5k and Paris6k reveal our
approach provides further performance boost for accurate image re-ranking,
whilst the comparative study demonstrates the advantage of our method against
other multi-view re-ranking methods.</p>
</td>
    <td>
      
        Hybrid ANN Methods 
      
        Re RANKING 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/li2020hamming/">Hamming OCR: A Locality Sensitive Hashing Neural Network For Scene Text Recognition</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hamming OCR: A Locality Sensitive Hashing Neural Network For Scene Text Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hamming OCR: A Locality Sensitive Hashing Neural Network For Scene Text Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>Recently, inspired by Transformer, self-attention-based scene text
recognition approaches have achieved outstanding performance. However, we find
that the size of model expands rapidly with the lexicon increasing.
Specifically, the number of parameters for softmax classification layer and
output embedding layer are proportional to the vocabulary size. It hinders the
development of a lightweight text recognition model especially applied for
Chinese and multiple languages. Thus, we propose a lightweight scene text
recognition model named Hamming OCR. In this model, a novel Hamming classifier,
which adopts locality sensitive hashing (LSH) algorithm to encode each
character, is proposed to replace the softmax regression and the generated LSH
code is directly employed to replace the output embedding. We also present a
simplified transformer decoder to reduce the number of parameters by removing
the feed-forward network and using cross-layer parameter sharing technique.
Compared with traditional methods, the number of parameters in both
classification and embedding layers is independent on the size of vocabulary,
which significantly reduces the storage requirement without loss of accuracy.
Experimental results on several datasets, including four public benchmaks and a
Chinese text dataset synthesized by SynthText with more than 20,000 characters,
shows that Hamming OCR achieves competitive results.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/zhang2020collaborative/">Collaborative Generative Hashing For Marketing And Fast Cold-start Recommendation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Collaborative Generative Hashing For Marketing And Fast Cold-start Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Collaborative Generative Hashing For Marketing And Fast Cold-start Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Yan, Tsang Ivor W., Duan Lixin</td> <!-- 🔧 You were missing this -->
    <td>IEEE Intelligent Systems</td>
    <td>6</td>
    <td><p>Cold-start has being a critical issue in recommender systems with the
explosion of data in e-commerce. Most existing studies proposed to alleviate
the cold-start problem are also known as hybrid recommender systems that learn
representations of users and items by combining user-item interactive and
user/item content information. However, previous hybrid methods regularly
suffered poor efficiency bottlenecking in online recommendations with
large-scale items, because they were designed to project users and items into
continuous latent space where the online recommendation is expensive. To this
end, we propose a collaborative generated hashing (CGH) framework to improve
the efficiency by denoting users and items as binary codes, then fast hashing
search techniques can be used to speed up the online recommendation. In
addition, the proposed CGH can generate potential users or items for marketing
application where the generative network is designed with the principle of
Minimum Description Length (MDL), which is used to learn compact and
informative binary codes. Extensive experiments on two public datasets show the
advantages for recommendations in various settings over competing baselines and
analyze its feasibility in marketing application.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/zhang2020deep/">Deep Pairwise Hashing For Cold-start Recommendation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Pairwise Hashing For Cold-start Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Pairwise Hashing For Cold-start Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Knowledge and Data Engineering</td>
    <td>9</td>
    <td><p>Recommendation efficiency and data sparsity problems have been regarded as
two challenges of improving performance for online recommendation. Most of the
previous related work focus on improving recommendation accuracy instead of
efficiency. In this paper, we propose a Deep Pairwise Hashing (DPH) to map
users and items to binary vectors in Hamming space, where a user’s preference
for an item can be efficiently calculated by Hamming distance, which
significantly improves the efficiency of online recommendation. To alleviate
data sparsity and cold-start problems, the user-item interactive information
and item content information are unified to learn effective representations of
items and users. Specifically, we first pre-train robust item representation
from item content data by a Denoising Auto-encoder instead of other
deterministic deep learning frameworks; then we finetune the entire framework
by adding a pairwise loss objective with discrete constraints; moreover, DPH
aims to minimize a pairwise ranking loss that is consistent with the ultimate
goal of recommendation. Finally, we adopt the alternating optimization method
to optimize the proposed model with discrete constraints. Extensive experiments
on three different datasets show that DPH can significantly advance the
state-of-the-art frameworks regarding data sparsity and item cold-start
recommendation.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/tseng2020parallel/">Parallel Index-based Structural Graph Clustering And Its Approximation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Parallel Index-based Structural Graph Clustering And Its Approximation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Parallel Index-based Structural Graph Clustering And Its Approximation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tseng Tom, Dhulipala Laxman, Shun Julian</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2021 International Conference on Management of Data</td>
    <td>14</td>
    <td><p>SCAN (Structural Clustering Algorithm for Networks) is a well-studied, widely
used graph clustering algorithm. For large graphs, however, sequential SCAN
variants are prohibitively slow, and parallel SCAN variants do not effectively
share work among queries with different SCAN parameter settings. Since users of
SCAN often explore many parameter settings to find good clusterings, it is
worthwhile to precompute an index that speeds up queries.
  This paper presents a practical and provably efficient parallel index-based
SCAN algorithm based on GS<em>-Index, a recent sequential algorithm. Our parallel
algorithm improves upon the asymptotic work of the sequential algorithm by
using integer sorting. It is also highly parallel, achieving logarithmic span
(parallel time) for both index construction and clustering queries.
Furthermore, we apply locality-sensitive hashing (LSH) to design a novel
approximate SCAN algorithm and prove guarantees for its clustering behavior.
  We present an experimental evaluation of our algorithms on large real-world
graphs. On a 48-core machine with two-way hyper-threading, our parallel index
construction achieves 50–151\(\times\) speedup over the construction of
GS</em>-Index. In fact, even on a single thread, our index construction algorithm
is faster than GS<em>-Index. Our parallel index query implementation achieves
5–32\(\times\) speedup over GS</em>-Index queries across a range of SCAN parameter
values, and our implementation is always faster than ppSCAN, a state-of-the-art
parallel SCAN algorithm. Moreover, our experiments show that applying LSH
results in faster index construction while maintaining good clustering quality.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/ribeiro2020sketchformer/">Sketchformer: Transformer-based Representation For Sketched Structure</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Sketchformer: Transformer-based Representation For Sketched Structure' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Sketchformer: Transformer-based Representation For Sketched Structure' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ribeiro et al.</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>83</td>
    <td><p>Sketchformer is a novel transformer-based representation for encoding
free-hand sketches input in a vector form, i.e. as a sequence of strokes.
Sketchformer effectively addresses multiple tasks: sketch classification,
sketch based image retrieval (SBIR), and the reconstruction and interpolation
of sketches. We report several variants exploring continuous and tokenized
input representations, and contrast their performance. Our learned embedding,
driven by a dictionary learning tokenization scheme, yields state of the art
performance in classification and image retrieval tasks, when compared against
baseline representations driven by LSTM sequence to sequence architectures:
SketchRNN and derivatives. We show that sketch reconstruction and interpolation
are improved significantly by the Sketchformer embedding for complex sketches
with longer stroke sequences.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/lei2020locality/">Locality-sensitive Hashing Scheme Based On Longest Circular Co-substring</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Locality-sensitive Hashing Scheme Based On Longest Circular Co-substring' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Locality-sensitive Hashing Scheme Based On Longest Circular Co-substring' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lei et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data</td>
    <td>24</td>
    <td><p>Locality-Sensitive Hashing (LSH) is one of the most popular methods for
\(c\)-Approximate Nearest Neighbor Search (\(c\)-ANNS) in high-dimensional spaces.
In this paper, we propose a novel LSH scheme based on the Longest Circular
Co-Substring (LCCS) search framework (LCCS-LSH) with a theoretical guarantee.
We introduce a novel concept of LCCS and a new data structure named Circular
Shift Array (CSA) for \(k\)-LCCS search. The insight of LCCS search framework is
that close data objects will have a longer LCCS than the far-apart ones with
high probability. LCCS-LSH is <em>LSH-family-independent</em>, and it supports
\(c\)-ANNS with different kinds of distance metrics. We also introduce a
multi-probe version of LCCS-LSH and conduct extensive experiments over five
real-life datasets. The experimental results demonstrate that LCCS-LSH
outperforms state-of-the-art LSH schemes.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/lee2020metric/">Metric Learning Vs Classification For Disentangled Music Representation Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Metric Learning Vs Classification For Disentangled Music Representation Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Metric Learning Vs Classification For Disentangled Music Representation Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lee et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>11</td>
    <td><p>Deep representation learning offers a powerful paradigm for mapping input
data onto an organized embedding space and is useful for many music information
retrieval tasks. Two central methods for representation learning include deep
metric learning and classification, both having the same goal of learning a
representation that can generalize well across tasks. Along with
generalization, the emerging concept of disentangled representations is also of
great interest, where multiple semantic concepts (e.g., genre, mood,
instrumentation) are learned jointly but remain separable in the learned
representation space. In this paper we present a single representation learning
framework that elucidates the relationship between metric learning,
classification, and disentanglement in a holistic manner. For this, we (1)
outline past work on the relationship between metric learning and
classification, (2) extend this relationship to multi-label data by exploring
three different learning approaches and their disentangled versions, and (3)
evaluate all models on four tasks (training time, similarity retrieval,
auto-tagging, and triplet prediction). We find that classification-based models
are generally advantageous for training time, similarity retrieval, and
auto-tagging, while deep metric learning exhibits better performance for
triplet-prediction. Finally, we show that our proposed approach yields
state-of-the-art results for music auto-tagging.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/le2020city/">City-scale Visual Place Recognition With Deep Local Features Based On Multi-scale Ordered VLAD Pooling</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=City-scale Visual Place Recognition With Deep Local Features Based On Multi-scale Ordered VLAD Pooling' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=City-scale Visual Place Recognition With Deep Local Features Based On Multi-scale Ordered VLAD Pooling' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Le Duc Canh, Youn Chan Hyun</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Conference on Robotics and Automation (ICRA)</td>
    <td>254</td>
    <td><p>Visual place recognition is the task of recognizing a place depicted in an
image based on its pure visual appearance without metadata. In visual place
recognition, the challenges lie upon not only the changes in lighting
conditions, camera viewpoint, and scale but also the characteristic of
scene-level images and the distinct features of the area. To resolve these
challenges, one must consider both the local discriminativeness and the global
semantic context of images. On the other hand, the diversity of the datasets is
also particularly important to develop more general models and advance the
progress of the field. In this paper, we present a fully-automated system for
place recognition at a city-scale based on content-based image retrieval. Our
main contributions to the community lie in three aspects. Firstly, we take a
comprehensive analysis of visual place recognition and sketch out the unique
challenges of the task compared to general image retrieval tasks. Next, we
propose yet a simple pooling approach on top of convolutional neural network
activations to embed the spatial information into the image representation
vector. Finally, we introduce new datasets for place recognition, which are
particularly essential for application-based research. Furthermore, throughout
extensive experiments, various issues in both image retrieval and place
recognition are analyzed and discussed to give some insights into improving the
performance of retrieval models in reality.
  The dataset used in this paper can be found at
https://github.com/canhld94/Daejeon520</p>
</td>
    <td>
      
        ICRA 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/lee2020contextualized/">Contextualized Sparse Representations For Real-time Open-domain Question Answering</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Contextualized Sparse Representations For Real-time Open-domain Question Answering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Contextualized Sparse Representations For Real-time Open-domain Question Answering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lee et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</td>
    <td>36</td>
    <td><p>Open-domain question answering can be formulated as a phrase retrieval
problem, in which we can expect huge scalability and speed benefit but often
suffer from low accuracy due to the limitation of existing phrase
representation models. In this paper, we aim to improve the quality of each
phrase embedding by augmenting it with a contextualized sparse representation
(Sparc). Unlike previous sparse vectors that are term-frequency-based (e.g.,
tf-idf) or directly learned (only few thousand dimensions), we leverage
rectified self-attention to indirectly learn sparse vectors in n-gram
vocabulary space. By augmenting the previous phrase retrieval model (Seo et
al., 2019) with Sparc, we show 4%+ improvement in CuratedTREC and SQuAD-Open.
Our CuratedTREC score is even better than the best known retrieve &amp; read model
with at least 45x faster inference speed.</p>
</td>
    <td>
      
        Graph Based ANN 
      
        ACL 
      
        TACL 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/tu2020deep/">Deep Cross-modal Hashing With Hashing Functions And Unified Hash Codes Jointly Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Cross-modal Hashing With Hashing Functions And Unified Hash Codes Jointly Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Cross-modal Hashing With Hashing Functions And Unified Hash Codes Jointly Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tu et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Knowledge and Data Engineering</td>
    <td>62</td>
    <td><p>Due to their high retrieval efficiency and low storage cost, cross-modal
hashing methods have attracted considerable attention. Generally, compared with
shallow cross-modal hashing methods, deep cross-modal hashing methods can
achieve a more satisfactory performance by integrating feature learning and
hash codes optimizing into a same framework. However, most existing deep
cross-modal hashing methods either cannot learn a unified hash code for the two
correlated data-points of different modalities in a database instance or cannot
guide the learning of unified hash codes by the feedback of hashing function
learning procedure, to enhance the retrieval accuracy. To address the issues
above, in this paper, we propose a novel end-to-end Deep Cross-Modal Hashing
with Hashing Functions and Unified Hash Codes Jointly Learning (DCHUC).
Specifically, by an iterative optimization algorithm, DCHUC jointly learns
unified hash codes for image-text pairs in a database and a pair of hash
functions for unseen query image-text pairs. With the iterative optimization
algorithm, the learned unified hash codes can be used to guide the hashing
function learning procedure; Meanwhile, the learned hashing functions can
feedback to guide the unified hash codes optimizing procedure. Extensive
experiments on three public datasets demonstrate that the proposed method
outperforms the state-of-the-art cross-modal hashing methods.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/kwok2020learning/">Learning To Hash With A Dimension Analysis-based Quantizer For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning To Hash With A Dimension Analysis-based Quantizer For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning To Hash With A Dimension Analysis-based Quantizer For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kwok Yuan</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>10</td>
    <td><p>The last few years have witnessed the rise of the big data era in which approximate nearest neighbor search is a fundamental problem in many applications, such as large-scale image retrieval. Recently, many research results have demonstrated that hashing can achieve promising performance due to its appealing storage and search efficiency. Since complex optimization problems for loss functions are difficult to solve, most hashing methods decompose the hash code learning problem into two steps: projection and quantization. In the quantization step, binary codes are widely used because ranking them by the Hamming distance is very efficient. However, the massive information loss produced by the quantization step should be reduced in applications where high search accuracy is required, such as in image retrieval. Since many two-step hashing methods produce uneven projected dimensions in the projection step, in this paper, we propose a novel dimension analysis-based quantization (DAQ) on two-step hashing methods for image retrieval. We first perform an importance analysis of the projected dimensions and select a subset of them that are more informative than others, and then we divide the selected projected dimensions into several regions with our quantizer. Every region is quantized with its corresponding codebook. Finally, the similarity between two hash codes is estimated by the Manhattan distance between their corresponding codebooks, which is also efficient. We conduct experiments on three public benchmarks containing up to one million descriptors and show that the proposed DAQ method consistently leads to significant accuracy improvements over state-of-the-art quantization methods.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/ryali2020bio/">Bio-inspired Hashing For Unsupervised Similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Bio-inspired Hashing For Unsupervised Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Bio-inspired Hashing For Unsupervised Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ryali et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Cybernetics</td>
    <td>45</td>
    <td><p>The fruit fly Drosophila’s olfactory circuit has inspired a new locality
sensitive hashing (LSH) algorithm, FlyHash. In contrast with classical LSH
algorithms that produce low dimensional hash codes, FlyHash produces sparse
high-dimensional hash codes and has also been shown to have superior empirical
performance compared to classical LSH algorithms in similarity search. However,
FlyHash uses random projections and cannot learn from data. Building on
inspiration from FlyHash and the ubiquity of sparse expansive representations
in neurobiology, our work proposes a novel hashing algorithm BioHash that
produces sparse high dimensional hash codes in a data-driven manner. We show
that BioHash outperforms previously published benchmarks for various hashing
methods. Since our learning algorithm is based on a local and biologically
plausible synaptic plasticity rule, our work provides evidence for the proposal
that LSH might be a computational reason for the abundance of sparse expansive
motifs in a variety of biological systems. We also propose a convolutional
variant BioConvHash that further improves performance. From the perspective of
computer science, BioHash and BioConvHash are fast, scalable and yield
compressed binary representations that are useful for similarity search.</p>
</td>
    <td>
      
        Similarity Search 
      
        Hashing Methods 
      
        SUPERVISED 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/sain2020cross/">Cross-modal Hierarchical Modelling For Fine-grained Sketch Based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-modal Hierarchical Modelling For Fine-grained Sketch Based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-modal Hierarchical Modelling For Fine-grained Sketch Based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sain et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>21</td>
    <td><p>Sketch as an image search query is an ideal alternative to text in capturing
the fine-grained visual details. Prior successes on fine-grained sketch-based
image retrieval (FG-SBIR) have demonstrated the importance of tackling the
unique traits of sketches as opposed to photos, e.g., temporal vs. static,
strokes vs. pixels, and abstract vs. pixel-perfect. In this paper, we study a
further trait of sketches that has been overlooked to date, that is, they are
hierarchical in terms of the levels of detail – a person typically sketches up
to various extents of detail to depict an object. This hierarchical structure
is often visually distinct. In this paper, we design a novel network that is
capable of cultivating sketch-specific hierarchies and exploiting them to match
sketch with photo at corresponding hierarchical levels. In particular, features
from a sketch and a photo are enriched using cross-modal co-attention, coupled
with hierarchical node fusion at every level to form a better embedding space
to conduct retrieval. Experiments on common benchmarks show our method to
outperform state-of-the-arts by a significant margin.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/koohpayegani2020compress/">Compress: Self-supervised Learning By Compressing Representations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Compress: Self-supervised Learning By Compressing Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Compress: Self-supervised Learning By Compressing Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Koohpayegani Soroush Abbasi, Tejankar Ajinkya, Pirsiavash Hamed</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>33</td>
    <td><p>Self-supervised learning aims to learn good representations with unlabeled
data. Recent works have shown that larger models benefit more from
self-supervised learning than smaller models. As a result, the gap between
supervised and self-supervised learning has been greatly reduced for larger
models. In this work, instead of designing a new pseudo task for
self-supervised learning, we develop a model compression method to compress an
already learned, deep self-supervised model (teacher) to a smaller one
(student). We train the student model so that it mimics the relative similarity
between the data points in the teacher’s embedding space. For AlexNet, our
method outperforms all previous methods including the fully supervised model on
ImageNet linear evaluation (59.0% compared to 56.5%) and on nearest neighbor
evaluation (50.7% compared to 41.4%). To the best of our knowledge, this is the
first time a self-supervised AlexNet has outperformed supervised one on
ImageNet classification. Our code is available here:
https://github.com/UMBCvision/CompRess</p>
</td>
    <td>
      
        SUPERVISED 
      
        Self SUPERVISED 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/s2020bag/">A Bag Of Visual Words Model For Medical Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Bag Of Visual Words Model For Medical Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Bag Of Visual Words Model For Medical Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>S Sowmya Kamath, K Karthik</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 1st ACM international workshop on Multimedia indexing and information retrieval for healthcare</td>
    <td>14</td>
    <td><p>Medical Image Retrieval is a challenging field in Visual information
retrieval, due to the multi-dimensional and multi-modal context of the
underlying content. Traditional models often fail to take the intrinsic
characteristics of data into consideration, and have thus achieved limited
accuracy when applied to medical images. The Bag of Visual Words (BoVW) is a
technique that can be used to effectively represent intrinsic image features in
vector space, so that applications like image classification and similar-image
search can be optimized. In this paper, we present a MedIR approach based on
the BoVW model for content-based medical image retrieval. As medical images as
multi-dimensional, they exhibit underlying cluster and manifold information
which enhances semantic relevance and allows for label uniformity. Hence, the
BoVW features extracted for each image are used to train a supervised machine
learning classifier based on positive and negative training images, for
extending content based image retrieval. During experimental validation, the
proposed model performed very well, achieving a Mean Average Precision of
88.89% during top-3 image retrieval experiments.</p>
</td>
    <td>
      
        Image Retrieval 
      
        Alt 
      
        ALT 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/ko2020embedding/">Embedding Expansion: Augmentation In Embedding Space For Deep Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Embedding Expansion: Augmentation In Embedding Space For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Embedding Expansion: Augmentation In Embedding Space For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ko Byungsoo, Gu Geonmo</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>47</td>
    <td><p>Learning the distance metric between pairs of samples has been studied for
image retrieval and clustering. With the remarkable success of pair-based
metric learning losses, recent works have proposed the use of generated
synthetic points on metric learning losses for augmentation and generalization.
However, these methods require additional generative networks along with the
main network, which can lead to a larger model size, slower training speed, and
harder optimization. Meanwhile, post-processing techniques, such as query
expansion and database augmentation, have proposed the combination of feature
points to obtain additional semantic information. In this paper, inspired by
query expansion and database augmentation, we propose an augmentation method in
an embedding space for pair-based metric learning losses, called embedding
expansion. The proposed method generates synthetic points containing augmented
information by a combination of feature points and performs hard negative pair
mining to learn with the most informative feature representations. Because of
its simplicity and flexibility, it can be used for existing metric learning
losses without affecting model size, training speed, or optimization
difficulty. Finally, the combination of embedding expansion and representative
metric learning losses outperforms the state-of-the-art losses and previous
sample generation methods in both image retrieval and clustering tasks. The
implementation is publicly available.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/talreja2020deep/">Deep Hashing For Secure Multimodal Biometrics</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Hashing For Secure Multimodal Biometrics' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Hashing For Secure Multimodal Biometrics' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Talreja Veeru, Valenti Matthew, Nasrabadi Nasser</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Information Forensics and Security</td>
    <td>24</td>
    <td><p>When compared to unimodal systems, multimodal biometric systems have several
advantages, including lower error rate, higher accuracy, and larger population
coverage. However, multimodal systems have an increased demand for integrity
and privacy because they must store multiple biometric traits associated with
each user. In this paper, we present a deep learning framework for
feature-level fusion that generates a secure multimodal template from each
user’s face and iris biometrics. We integrate a deep hashing (binarization)
technique into the fusion architecture to generate a robust binary multimodal
shared latent representation. Further, we employ a hybrid secure architecture
by combining cancelable biometrics with secure sketch techniques and integrate
it with a deep hashing framework, which makes it computationally prohibitive to
forge a combination of multiple biometrics that pass the authentication. The
efficacy of the proposed approach is shown using a multimodal database of face
and iris and it is observed that the matching performance is improved due to
the fusion of multiple biometrics. Furthermore, the proposed approach also
provides cancelability and unlinkability of the templates along with improved
privacy of the biometric data. Additionally, we also test the proposed hashing
function for an image retrieval application using a benchmark dataset. The main
goal of this paper is to develop a method for integrating multimodal fusion,
deep hashing, and biometric security, with an emphasis on structural data from
modalities like face and iris. The proposed approach is in no way a general
biometric security framework that can be applied to all biometric modalities,
as further research is needed to extend the proposed framework to other
unconstrained biometric modalities.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/xu2020hashing/">Hashing Based Answer Selection</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing Based Answer Selection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing Based Answer Selection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xu Dong, Li Wu-jun</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>8</td>
    <td><p>Answer selection is an important subtask of question answering (QA), where
deep models usually achieve better performance. Most deep models adopt
question-answer interaction mechanisms, such as attention, to get vector
representations for answers. When these interaction based deep models are
deployed for online prediction, the representations of all answers need to be
recalculated for each question. This procedure is time-consuming for deep
models with complex encoders like BERT which usually have better accuracy than
simple encoders. One possible solution is to store the matrix representation
(encoder output) of each answer in memory to avoid recalculation. But this will
bring large memory cost. In this paper, we propose a novel method, called
hashing based answer selection (HAS), to tackle this problem. HAS adopts a
hashing strategy to learn a binary matrix representation for each answer, which
can dramatically reduce the memory cost for storing the matrix representations
of answers. Hence, HAS can adopt complex encoders like BERT in the model, but
the online prediction of HAS is still fast with a low memory cost. Experimental
results on three popular answer selection datasets show that HAS can outperform
existing models to achieve state-of-the-art performance.</p>
</td>
    <td>
      
        Hashing Methods 
      
        AAAI 
      
        Graph Based ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/weng2020fast/">Fast Search On Binary Codes By Weighted Hamming Distance</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Search On Binary Codes By Weighted Hamming Distance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Search On Binary Codes By Weighted Hamming Distance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Weng Zhenyu, Zhu Yuesheng, Liu Ruixin</td> <!-- 🔧 You were missing this -->
    <td>2013 IEEE International Conference on Acoustics, Speech and Signal Processing</td>
    <td>11</td>
    <td><p>Weighted Hamming distance, as a similarity measure between binary codes and
binary queries, provides superior accuracy in search tasks than Hamming
distance. However, how to efficiently and accurately find \(K\) binary codes that
have the smallest weighted Hamming distance to the query remains an open issue.
In this paper, a fast search algorithm is proposed to perform the
non-exhaustive search for \(K\) nearest binary codes by weighted Hamming
distance. By using binary codes as direct bucket indices in a hash table, the
search algorithm generates a sequence to probe the buckets based on the
independence characteristic of the weights for each bit. Furthermore, a fast
search framework based on the proposed search algorithm is designed to solve
the problem of long binary codes. Specifically, long binary codes are split
into substrings and multiple hash tables are built on them. Then, the search
algorithm probes the buckets to obtain candidates according to the generated
substring indices, and a merging algorithm is proposed to find the nearest
binary codes by merging the candidates. Theoretical analysis and experimental
results demonstrate that the search algorithm improves the search accuracy
compared to other non-exhaustive algorithms and provides orders-of-magnitude
faster search than the linear scan baseline.</p>
</td>
    <td>
      
        Compact Codes 
      
        ICASSP 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/khudabukhsh2020discovering/">Discovering Bilingual Lexicons In Polyglot Word Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Discovering Bilingual Lexicons In Polyglot Word Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Discovering Bilingual Lexicons In Polyglot Word Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Khudabukhsh Ashiqur R., Palakodety Shriphani, Mitchell Tom M.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</td>
    <td>13</td>
    <td><p>Bilingual lexicons and phrase tables are critical resources for modern
Machine Translation systems. Although recent results show that without any seed
lexicon or parallel data, highly accurate bilingual lexicons can be learned
using unsupervised methods, such methods rely on the existence of large, clean
monolingual corpora. In this work, we utilize a single Skip-gram model trained
on a multilingual corpus yielding polyglot word embeddings, and present a novel
finding that a surprisingly simple constrained nearest-neighbor sampling
technique in this embedding space can retrieve bilingual lexicons, even in
harsh social media data sets predominantly written in English and Romanized
Hindi and often exhibiting code switching. Our method does not require
monolingual corpora, seed lexicons, or any other such resources. Additionally,
across three European language pairs, we observe that polyglot word embeddings
indeed learn a rich semantic representation of words and substantial bilingual
lexicons can be retrieved using our constrained nearest neighbor sampling. We
investigate potential reasons and downstream applications in settings spanning
both clean texts and noisy social media data sets, and in both resource-rich
and under-resourced language pairs.</p>
</td>
    <td>
      
        ACL 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/khandelwal2020nearest/">Nearest Neighbor Machine Translation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Nearest Neighbor Machine Translation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Nearest Neighbor Machine Translation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Khandelwal et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>129</td>
    <td><p>We introduce \(k\)-nearest-neighbor machine translation (\(k\)NN-MT), which
predicts tokens with a nearest neighbor classifier over a large datastore of
cached examples, using representations from a neural translation model for
similarity search. This approach requires no additional training and scales to
give the decoder direct access to billions of examples at test time, resulting
in a highly expressive model that consistently improves performance across many
settings. Simply adding nearest neighbor search improves a state-of-the-art
German-English translation model by 1.5 BLEU. \(k\)NN-MT allows a single model to
be adapted to diverse domains by using a domain-specific datastore, improving
results by an average of 9.2 BLEU over zero-shot transfer, and achieving new
state-of-the-art results – without training on these domains. A massively
multilingual model can also be specialized for particular language pairs, with
improvements of 3 BLEU for translating from English into German and Chinese.
Qualitatively, \(k\)NN-MT is easily interpretable; it combines source and target
context to retrieve highly relevant examples.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/kim2020boosted/">Boosted Locality Sensitive Hashing: Discriminative Binary Codes For Source Separation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Boosted Locality Sensitive Hashing: Discriminative Binary Codes For Source Separation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Boosted Locality Sensitive Hashing: Discriminative Binary Codes For Source Separation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kim Sunwoo, Yang Haici, Kim Minje</td> <!-- 🔧 You were missing this -->
    <td>ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</td>
    <td>9</td>
    <td><p>Speech enhancement tasks have seen significant improvements with the advance
of deep learning technology, but with the cost of increased computational
complexity. In this study, we propose an adaptive boosting approach to learning
locality sensitive hash codes, which represent audio spectra efficiently. We
use the learned hash codes for single-channel speech denoising tasks as an
alternative to a complex machine learning model, particularly to address the
resource-constrained environments. Our adaptive boosting algorithm learns
simple logistic regressors as the weak learners. Once trained, their binary
classification results transform each spectrum of test noisy speech into a bit
string. Simple bitwise operations calculate Hamming distance to find the
K-nearest matching frames in the dictionary of training noisy speech spectra,
whose associated ideal binary masks are averaged to estimate the denoising mask
for that test mixture. Our proposed learning algorithm differs from AdaBoost in
the sense that the projections are trained to minimize the distances between
the self-similarity matrix of the hash codes and that of the original spectra,
rather than the misclassification rate. We evaluate our discriminative hash
codes on the TIMIT corpus with various noise types, and show comparative
performance to deep learning methods in terms of denoising performance and
complexity.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Hashing Methods 
      
        Compact Codes 
      
        ICASSP 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/karunanayake2020multi/">A Multi-modal Neural Embeddings Approach For Detecting Mobile Counterfeit Apps: A Case Study On Google Play Store</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Multi-modal Neural Embeddings Approach For Detecting Mobile Counterfeit Apps: A Case Study On Google Play Store' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Multi-modal Neural Embeddings Approach For Detecting Mobile Counterfeit Apps: A Case Study On Google Play Store' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Karunanayake et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Mobile Computing</td>
    <td>9</td>
    <td><p>Counterfeit apps impersonate existing popular apps in attempts to misguide
users to install them for various reasons such as collecting personal
information or spreading malware. Many counterfeits can be identified once
installed, however even a tech-savvy user may struggle to detect them before
installation. To this end, this paper proposes to leverage the recent advances
in deep learning methods to create image and text embeddings so that
counterfeit apps can be efficiently identified when they are submitted for
publication. We show that a novel approach of combining content embeddings and
style embeddings outperforms the baseline methods for image similarity such as
SIFT, SURF, and various image hashing methods. We first evaluate the
performance of the proposed method on two well-known datasets for evaluating
image similarity methods and show that content, style, and combined embeddings
increase precision@k and recall@k by 10%-15% and 12%-25%, respectively when
retrieving five nearest neighbours. Second, specifically for the app
counterfeit detection problem, combined content and style embeddings achieve
12% and 14% increase in precision@k and recall@k, respectively compared to the
baseline methods. Third, we present an analysis of approximately 1.2 million
apps from Google Play Store and identify a set of potential counterfeits for
top-10,000 popular apps. Under a conservative assumption, we were able to find
2,040 potential counterfeits that contain malware in a set of 49,608 apps that
showed high similarity to one of the top-10,000 popular apps in Google Play
Store. We also find 1,565 potential counterfeits asking for at least five
additional dangerous permissions than the original app and 1,407 potential
counterfeits having at least five extra third party advertisement libraries.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/khojasteh2020deep/">Deep Multimodal Image-text Embeddings For Automatic Cross-media Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Multimodal Image-text Embeddings For Automatic Cross-media Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Multimodal Image-text Embeddings For Automatic Cross-media Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Khojasteh et al.</td> <!-- 🔧 You were missing this -->
    <td>2018 24th International Conference on Pattern Recognition (ICPR)</td>
    <td>17</td>
    <td><p>This paper considers the task of matching images and sentences by learning a
visual-textual embedding space for cross-modal retrieval. Finding such a space
is a challenging task since the features and representations of text and image
are not comparable. In this work, we introduce an end-to-end deep multimodal
convolutional-recurrent network for learning both vision and language
representations simultaneously to infer image-text similarity. The model learns
which pairs are a match (positive) and which ones are a mismatch (negative)
using a hinge-based triplet ranking. To learn about the joint representations,
we leverage our newly extracted collection of tweets from Twitter. The main
characteristic of our dataset is that the images and tweets are not
standardized the same as the benchmarks. Furthermore, there can be a higher
semantic correlation between the pictures and tweets contrary to benchmarks in
which the descriptions are well-organized. Experimental results on MS-COCO
benchmark dataset show that our model outperforms certain methods presented
previously and has competitive performance compared to the state-of-the-art.
The code and dataset have been made available publicly.</p>
</td>
    <td>
      
        Multimodal Retrieval 
      
        Medical Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/sonthalia2020rankability/">On The Rankability Of Visual Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=On The Rankability Of Visual Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=On The Rankability Of Visual Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sonthalia Ankit, Uselis Arnas, Oh Seong Joon</td> <!-- 🔧 You were missing this -->
    <td>Neurocomputing</td>
    <td>22</td>
    <td><p>We study whether visual embedding models capture continuous, ordinal attributes along linear directions, which we term <em>rank axes</em>. We define a model as <em>rankable</em> for an attribute if projecting embeddings onto such an axis preserves the attribute’s order. Across 7 popular encoders and 9 datasets with attributes like age, crowd count, head pose, aesthetics, and recency, we find that many embeddings are inherently rankable. Surprisingly, a small number of samples, or even just two extreme examples, often suffice to recover meaningful rank axes, without full-scale supervision. These findings open up new use cases for image ranking in vector databases and motivate further study into the structure and learning of rankable embeddings. Our code is available at https://github.com/aktsonthalia/rankable-vision-embeddings.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/kaplan2020locality/">Locality Sensitive Hashing For Set-queries, Motivated By Group Recommendations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Locality Sensitive Hashing For Set-queries, Motivated By Group Recommendations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Locality Sensitive Hashing For Set-queries, Motivated By Group Recommendations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kaplan Haim, Tenenbaum Jay</td> <!-- 🔧 You were missing this -->
    <td>Information Sciences</td>
    <td>15</td>
    <td><p>Locality Sensitive Hashing (LSH) is an effective method to index a set of
points such that we can efficiently find the nearest neighbors of a query
point. We extend this method to our novel Set-query LSH (SLSH), such that it
can find the nearest neighbors of a set of points, given as a query.
  Let \( s(x,y) \) be the similarity between two points \( x \) and \( y \). We
define a similarity between a set \( Q\) and a point \( x \) by aggregating the
similarities \( s(p,x) \) for all \( p\in Q \). For example, we can take \( s(p,x) \)
to be the angular similarity between \( p \) and \( x \) (i.e., \(1-{\angle
(x,p)}/{\pi}\)), and aggregate by arithmetic or geometric averaging, or taking
the lowest similarity.
  We develop locality sensitive hash families and data structures for a large
set of such arithmetic and geometric averaging similarities, and analyze their
collision probabilities. We also establish an analogous framework and hash
families for distance functions. Specifically, we give a structure for the
euclidean distance aggregated by either averaging or taking the maximum.
  We leverage SLSH to solve a geometric extension of the approximate near
neighbors problem. In this version, we consider a metric for which the unit
ball is an ellipsoid and its orientation is specified with the query.
  An important application that motivates our work is group recommendation
systems. Such a system embeds movies and users in the same feature space, and
the task of recommending a movie for a group to watch together, translates to a
set-query \( Q \) using an appropriate similarity.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Hashing Methods 
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/kang2020learning/">Learning Multi-granular Quantized Embeddings For Large-vocab Categorical Features In Recommender Systems</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Multi-granular Quantized Embeddings For Large-vocab Categorical Features In Recommender Systems' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Multi-granular Quantized Embeddings For Large-vocab Categorical Features In Recommender Systems' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kang et al.</td> <!-- 🔧 You were missing this -->
    <td>Companion Proceedings of the Web Conference 2020</td>
    <td>32</td>
    <td><p>Recommender system models often represent various sparse features like users,
items, and categorical features via embeddings. A standard approach is to map
each unique feature value to an embedding vector. The size of the produced
embedding table grows linearly with the size of the vocabulary. Therefore, a
large vocabulary inevitably leads to a gigantic embedding table, creating two
severe problems: (i) making model serving intractable in resource-constrained
environments; (ii) causing overfitting problems. In this paper, we seek to
learn highly compact embeddings for large-vocab sparse features in recommender
systems (recsys). First, we show that the novel Differentiable Product
Quantization (DPQ) approach can generalize to recsys problems. In addition, to
better handle the power-law data distribution commonly seen in recsys, we
propose a Multi-Granular Quantized Embeddings (MGQE) technique which learns
more compact embeddings for infrequent items. We seek to provide a new angle to
improve recommendation performance with compact model sizes. Extensive
experiments on three recommendation tasks and two datasets show that we can
achieve on par or better performance, with only ~20% of the original model
size.</p>
</td>
    <td>
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/kanda2020succinct/">Succinct Trit-array Trie For Scalable Trajectory Similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Succinct Trit-array Trie For Scalable Trajectory Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Succinct Trit-array Trie For Scalable Trajectory Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kanda et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 28th International Conference on Advances in Geographic Information Systems</td>
    <td>11</td>
    <td><p>Massive datasets of spatial trajectories representing the mobility of a
diversity of moving objects are ubiquitous in research and industry. Similarity
search of a large collection of trajectories is indispensable for turning these
datasets into knowledge. Locality sensitive hashing (LSH) is a powerful
technique for fast similarity searches. Recent methods employ LSH and attempt
to realize an efficient similarity search of trajectories; however, those
methods are inefficient in terms of search time and memory when applied to
massive datasets. To address this problem, we present the trajectory-indexing
succinct trit-array trie (tSTAT), which is a scalable method leveraging LSH for
trajectory similarity searches. tSTAT quickly performs the search on a tree
data structure called trie. We also present two novel techniques that enable to
dramatically enhance the memory efficiency of tSTAT. One is a node reduction
technique that substantially omits redundant trie nodes while maintaining the
time performance. The other is a space-efficient representation that leverages
the idea behind succinct data structures (i.e., a compressed data structure
supporting fast data operations). We experimentally test tSTAT on its ability
to retrieve similar trajectories for a query from large collections of
trajectories and show that tSTAT performs superiorly in comparison to
state-of-the-art similarity search methods.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/karpusha2020calibrated/">Calibrated Neighborhood Aware Confidence Measure For Deep Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Calibrated Neighborhood Aware Confidence Measure For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Calibrated Neighborhood Aware Confidence Measure For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Karpusha Maryna, Yun Sunghee, Fehervari Istvan</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>102</td>
    <td><p>Deep metric learning has gained promising improvement in recent years
following the success of deep learning. It has been successfully applied to
problems in few-shot learning, image retrieval, and open-set classifications.
However, measuring the confidence of a deep metric learning model and
identifying unreliable predictions is still an open challenge. This paper
focuses on defining a calibrated and interpretable confidence metric that
closely reflects its classification accuracy. While performing similarity
comparison directly in the latent space using the learned distance metric, our
approach approximates the distribution of data points for each class using a
Gaussian kernel smoothing function. The post-processing calibration algorithm
with proposed confidence metric on the held-out validation dataset improves
generalization and robustness of state-of-the-art deep metric learning models
while provides an interpretable estimation of the confidence. Extensive tests
on four popular benchmark datasets (Caltech-UCSD Birds, Stanford Online
Product, Stanford Car-196, and In-shop Clothes Retrieval) show consistent
improvements even at the presence of distribution shifts in test data related
to additional noise or adversarial examples.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/zhang2020faster/">Faster Binary Embeddings For Preserving Euclidean Distances</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Faster Binary Embeddings For Preserving Euclidean Distances' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Faster Binary Embeddings For Preserving Euclidean Distances' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Jinjie, Saab Rayan</td> <!-- 🔧 You were missing this -->
    <td>CVPR 2011</td>
    <td>31</td>
    <td><p>We propose a fast, distance-preserving, binary embedding algorithm to
transform a high-dimensional dataset \(\mathcal{T}\subseteq\mathbb{R}^n\) into
binary sequences in the cube \(\{\pm 1\}^m\). When \(\mathcal{T}\) consists of
well-spread (i.e., non-sparse) vectors, our embedding method applies a stable
noise-shaping quantization scheme to \(A x\) where \(A\in\mathbb{R}^{m\times n}\)
is a sparse Gaussian random matrix. This contrasts with most binary embedding
methods, which usually use \(x\mapsto \mathrm{sign}(Ax)\) for the embedding.
Moreover, we show that Euclidean distances among the elements of \(\mathcal{T}\)
are approximated by the \(\ell_1\) norm on the images of \(\{\pm 1\}^m\) under a
fast linear transformation. This again contrasts with standard methods, where
the Hamming distance is used instead. Our method is both fast and memory
efficient, with time complexity \(O(m)\) and space complexity \(O(m)\). Further, we
prove that the method is accurate and its associated error is comparable to
that of a continuous valued Johnson-Lindenstrauss embedding plus a quantization
error that admits a polynomial decay as the embedding dimension \(m\) increases.
Thus the length of the binary codes required to achieve a desired accuracy is
quite small, and we show it can even be compressed further without compromising
the accuracy. To illustrate our results, we test the proposed method on natural
images and show that it achieves strong performance.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Distance Metric Learning 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/vanblokland2020indexing/">An Indexing Scheme And Descriptor For 3D Object Retrieval Based On Local Shape Querying</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=An Indexing Scheme And Descriptor For 3D Object Retrieval Based On Local Shape Querying' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=An Indexing Scheme And Descriptor For 3D Object Retrieval Based On Local Shape Querying' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>van Blokland Bart Iver, Theoharis Theoharis</td> <!-- 🔧 You were missing this -->
    <td>Computers &amp; Graphics</td>
    <td>14</td>
    <td><p>A binary descriptor indexing scheme based on Hamming distance called the
Hamming tree for local shape queries is presented. A new binary clutter
resistant descriptor named Quick Intersection Count Change Image (QUICCI) is
also introduced. This local shape descriptor is extremely small and fast to
compare. Additionally, a novel distance function called Weighted Hamming
applicable to QUICCI images is proposed for retrieval applications. The
effectiveness of the indexing scheme and QUICCI is demonstrated on 828 million
QUICCI images derived from the SHREC2017 dataset, while the clutter resistance
of QUICCI is shown using the clutterbox experiment.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/yang2020tree/">Tree-augmented Cross-modal Encoding For Complex-query Video Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Tree-augmented Cross-modal Encoding For Complex-query Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Tree-augmented Cross-modal Encoding For Complex-query Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>123</td>
    <td><p>The rapid growth of user-generated videos on the Internet has intensified the
need for text-based video retrieval systems. Traditional methods mainly favor
the concept-based paradigm on retrieval with simple queries, which are usually
ineffective for complex queries that carry far more complex semantics.
Recently, embedding-based paradigm has emerged as a popular approach. It aims
to map the queries and videos into a shared embedding space where
semantically-similar texts and videos are much closer to each other. Despite
its simplicity, it forgoes the exploitation of the syntactic structure of text
queries, making it suboptimal to model the complex queries.
  To facilitate video retrieval with complex queries, we propose a
Tree-augmented Cross-modal Encoding method by jointly learning the linguistic
structure of queries and the temporal representation of videos. Specifically,
given a complex user query, we first recursively compose a latent semantic tree
to structurally describe the text query. We then design a tree-augmented query
encoder to derive structure-aware query representation and a temporal attentive
video encoder to model the temporal characteristics of videos. Finally, both
the query and videos are mapped into a joint embedding space for matching and
ranking. In this approach, we have a better understanding and modeling of the
complex queries, thereby achieving a better video retrieval performance.
Extensive experiments on large scale video retrieval benchmark datasets
demonstrate the effectiveness of our approach.</p>
</td>
    <td>
      
        SIGIR 
      
        Video Retrieval 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/jin2020deep/">Deep Saliency Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Saliency Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Saliency Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jin et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>57</td>
    <td><p>In recent years, hashing methods have been proved to be effective and
efficient for the large-scale Web media search. However, the existing general
hashing methods have limited discriminative power for describing fine-grained
objects that share similar overall appearance but have subtle difference. To
solve this problem, we for the first time introduce the attention mechanism to
the learning of fine-grained hashing codes. Specifically, we propose a novel
deep hashing model, named deep saliency hashing (DSaH), which automatically
mines salient regions and learns semantic-preserving hashing codes
simultaneously. DSaH is a two-step end-to-end model consisting of an attention
network and a hashing network. Our loss function contains three basic
components, including the semantic loss, the saliency loss, and the
quantization loss. As the core of DSaH, the saliency loss guides the attention
network to mine discriminative regions from pairs of images. We conduct
extensive experiments on both fine-grained and general retrieval datasets for
performance evaluation. Experimental results on fine-grained datasets,
including Oxford Flowers-17, Stanford Dogs-120, and CUB Bird demonstrate that
our DSaH performs the best for fine-grained retrieval task and beats the
strongest competitor (DTQ) by approximately 10% on both Stanford Dogs-120 and
CUB Bird. DSaH is also comparable to several state-of-the-art hashing methods
on general datasets, including CIFAR-10 and NUS-WIDE.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/jin2020node2bits/">Node2bits: Compact Time- And Attribute-aware Node Representations For User Stitching</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Node2bits: Compact Time- And Attribute-aware Node Representations For User Stitching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Node2bits: Compact Time- And Attribute-aware Node Representations For User Stitching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jin et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>27</td>
    <td><p>Identity stitching, the task of identifying and matching various online
references (e.g., sessions over different devices and timespans) to the same
user in real-world web services, is crucial for personalization and
recommendations. However, traditional user stitching approaches, such as
grouping or blocking, require quadratic pairwise comparisons between a massive
number of user activities, thus posing both computational and storage
challenges. Recent works, which are often application-specific, heuristically
seek to reduce the amount of comparisons, but they suffer from low precision
and recall. To solve the problem in an application-independent way, we take a
heterogeneous network-based approach in which users (nodes) interact with
content (e.g., sessions, websites), and may have attributes (e.g., location).
We propose node2bits, an efficient framework that represents multi-dimensional
features of node contexts with binary hashcodes. node2bits leverages
feature-based temporal walks to encapsulate short- and long-term interactions
between nodes in heterogeneous web networks, and adopts SimHash to obtain
compact, binary representations and avoid the quadratic complexity for
similarity search. Extensive experiments on large-scale real networks show that
node2bits outperforms traditional techniques and existing works that generate
real-valued embeddings by up to 5.16% in F1 score on user stitching, while
taking only up to 1.56% as much storage.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/jiang2020convolutional/">A Convolutional Neural Network-based Patent Image Retrieval Method For Design Ideation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Convolutional Neural Network-based Patent Image Retrieval Method For Design Ideation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Convolutional Neural Network-based Patent Image Retrieval Method For Design Ideation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jiang et al.</td> <!-- 🔧 You were missing this -->
    <td>Volume 9: 40th Computers and Information in Engineering Conference (CIE)</td>
    <td>8</td>
    <td><p>The patent database is often used in searches of inspirational stimuli for
innovative design opportunities because of its large size, extensive variety
and rich design information in patent documents. However, most patent mining
research only focuses on textual information and ignores visual information.
Herein, we propose a convolutional neural network (CNN)-based patent image
retrieval method. The core of this approach is a novel neural network
architecture named Dual-VGG that is aimed to accomplish two tasks: visual
material type prediction and international patent classification (IPC) class
label prediction. In turn, the trained neural network provides the deep
features in the image embedding vectors that can be utilized for patent image
retrieval and visual mapping. The accuracy of both training tasks and patent
image embedding space are evaluated to show the performance of our model. This
approach is also illustrated in a case study of robot arm design retrieval.
Compared to traditional keyword-based searching and Google image searching, the
proposed method discovers more useful visual information for engineering
design.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/liu2020reinforcing/">Reinforcing Short-length Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Reinforcing Short-length Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Reinforcing Short-length Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Circuits and Systems for Video Technology</td>
    <td>28</td>
    <td><p>Due to the compelling efficiency in retrieval and storage,
similarity-preserving hashing has been widely applied to approximate nearest
neighbor search in large-scale image retrieval. However, existing methods have
poor performance in retrieval using an extremely short-length hash code due to
weak ability of classification and poor distribution of hash bit. To address
this issue, in this study, we propose a novel reinforcing short-length hashing
(RSLH). In this proposed RSLH, mutual reconstruction between the hash
representation and semantic labels is performed to preserve the semantic
information. Furthermore, to enhance the accuracy of hash representation, a
pairwise similarity matrix is designed to make a balance between accuracy and
training expenditure on memory. In addition, a parameter boosting strategy is
integrated to reinforce the precision with hash bits fusion. Extensive
experiments on three large-scale image benchmarks demonstrate the superior
performance of RSLH under various short-length hashing scenarios.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/liu2020joint/">Joint-modal Distribution-based Similarity Hashing For Large-scale Unsupervised Deep Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Joint-modal Distribution-based Similarity Hashing For Large-scale Unsupervised Deep Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Joint-modal Distribution-based Similarity Hashing For Large-scale Unsupervised Deep Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>148</td>
    <td><p>Hashing-based cross-modal search which aims to map multiple modality features into binary codes has attracted increasingly attention due to its storage and search efficiency especially in large-scale database retrieval. Recent unsupervised deep cross-modal hashing methods have shown promising results. However, existing approaches typically suffer from two limitations: (1) They usually learn cross-modal similarity information separately or in a redundant fusion manner, which may fail to capture semantic correlations among instances from different modalities sufficiently and effectively. (2) They seldom consider the sampling and weighting schemes for unsupervised cross-modal hashing, resulting in the lack of satisfactory discriminative ability in hash codes. To overcome these limitations, we propose a novel unsupervised deep cross-modal hashing method called Joint-modal Distribution-based Similarity Hashing (JDSH) for large-scale cross-modal retrieval. Firstly, we propose a novel cross-modal joint-training method by constructing a joint-modal similarity matrix to fully preserve the cross-modal semantic correlations among instances. Secondly, we propose a sampling and weighting scheme termed the Distribution-based Similarity Decision and Weighting (DSDW) method for unsupervised cross-modal hashing, which is able to generate more discriminative hash codes by pushing semantic similar instance pairs closer and pulling semantic dissimilar instance pairs apart. The experimental results demonstrate the superiority of JDSH compared with several unsupervised cross-modal hashing methods on two public datasets NUS-WIDE and MIRFlickr.</p>
</td>
    <td>
      
        Text Retrieval 
      
        Unsupervised 
      
        SUPERVISED 
      
        Multimodal Retrieval 
      
        SIGIR 
      
        Hashing Methods 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/liu2020cross/">Cross-modal Zero-shot Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-modal Zero-shot Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-modal Zero-shot Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Neural Networks and Learning Systems</td>
    <td>83</td>
    <td><p>Hashing has been widely studied for big data retrieval due to its low storage
cost and fast query speed. Zero-shot hashing (ZSH) aims to learn a hashing
model that is trained using only samples from seen categories, but can
generalize well to samples of unseen categories. ZSH generally uses category
attributes to seek a semantic embedding space to transfer knowledge from seen
categories to unseen ones. As a result, it may perform poorly when labeled data
are insufficient. ZSH methods are mainly designed for single-modality data,
which prevents their application to the widely spread multi-modal data. On the
other hand, existing cross-modal hashing solutions assume that all the
modalities share the same category labels, while in practice the labels of
different data modalities may be different. To address these issues, we propose
a general Cross-modal Zero-shot Hashing (CZHash) solution to effectively
leverage unlabeled and labeled multi-modality data with different label spaces.
CZHash first quantifies the composite similarity between instances using label
and feature information. It then defines an objective function to achieve deep
feature learning compatible with the composite similarity preserving, category
attribute space learning, and hashing coding function learning. CZHash further
introduces an alternative optimization procedure to jointly optimize these
learning objectives. Experiments on benchmark multi-modal datasets show that
CZHash significantly outperforms related representative hashing approaches both
on effectiveness and adaptability.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Few Shot & Zero Shot 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/liu2020model/">Model Optimization Boosting Framework For Linear Model Hash Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Model Optimization Boosting Framework For Linear Model Hash Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Model Optimization Boosting Framework For Linear Model Hash Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>22</td>
    <td><p>Efficient hashing techniques have attracted extensive research interests in both storage and retrieval of high dimensional data, such as images and videos. In existing hashing methods, a linear model is commonly utilized owing to its efficiency. To obtain better accuracy, linear-based hashing methods focus on designing a generalized linear objective function with different constraints or penalty terms that consider the inherent characteristics and neighborhood information of samples. Differing from existing hashing methods, in this study, we propose a self-improvement framework called Model Boost (MoBoost) to improve model parameter optimization for linear-based hashing methods without adding new constraints or penalty terms. In the proposed MoBoost, for a linear-based hashing method, we first repeatedly execute the hashing method to obtain several hash codes to training samples. Then, utilizing two novel fusion strategies, these codes are fused into a single set. We also propose two new criteria to evaluate the goodness of hash bits during the fusion process. Based on the fused set of hash codes, we learn new parameters for the linear hash function that can significantly improve the accuracy. In general, the proposed MoBoost can be adopted by existing linear-based hashing methods, achieving more precise and stable performance compared to the original methods, and adopting the proposed MoBoost will incur negligible time and space costs. To evaluate the proposed MoBoost, we performed extensive experiments on four benchmark datasets, and the results demonstrate superior performance.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Tools & Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/mafla2020fine/">Fine-grained Image Classification And Retrieval By Combining Visual And Locally Pooled Textual Features</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fine-grained Image Classification And Retrieval By Combining Visual And Locally Pooled Textual Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fine-grained Image Classification And Retrieval By Combining Visual And Locally Pooled Textual Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mafla et al.</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>30</td>
    <td><p>Text contained in an image carries high-level semantics that can be exploited
to achieve richer image understanding. In particular, the mere presence of text
provides strong guiding content that should be employed to tackle a diversity
of computer vision tasks such as image retrieval, fine-grained classification,
and visual question answering. In this paper, we address the problem of
fine-grained classification and image retrieval by leveraging textual
information along with visual cues to comprehend the existing intrinsic
relation between the two modalities. The novelty of the proposed model consists
of the usage of a PHOC descriptor to construct a bag of textual words along
with a Fisher Vector Encoding that captures the morphology of text. This
approach provides a stronger multimodal representation for this task and as our
experiments demonstrate, it achieves state-of-the-art results on two different
tasks, fine-grained classification and image retrieval.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/weyand2020google/">Google Landmarks Dataset V2 -- A Large-scale Benchmark For Instance-level Recognition And Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Google Landmarks Dataset V2 -- A Large-scale Benchmark For Instance-level Recognition And Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Google Landmarks Dataset V2 -- A Large-scale Benchmark For Instance-level Recognition And Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Weyand et al.</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>231</td>
    <td><p>While image retrieval and instance recognition techniques are progressing
rapidly, there is a need for challenging datasets to accurately measure their
performance – while posing novel challenges that are relevant for practical
applications. We introduce the Google Landmarks Dataset v2 (GLDv2), a new
benchmark for large-scale, fine-grained instance recognition and image
retrieval in the domain of human-made and natural landmarks. GLDv2 is the
largest such dataset to date by a large margin, including over 5M images and
200k distinct instance labels. Its test set consists of 118k images with ground
truth annotations for both the retrieval and recognition tasks. The ground
truth construction involved over 800 hours of human annotator work. Our new
dataset has several challenging properties inspired by real world applications
that previous datasets did not consider: An extremely long-tailed class
distribution, a large fraction of out-of-domain test photos and large
intra-class variability. The dataset is sourced from Wikimedia Commons, the
world’s largest crowdsourced collection of landmark photos. We provide baseline
results for both recognition and retrieval tasks based on state-of-the-art
methods as well as competitive results from a public challenge. We further
demonstrate the suitability of the dataset for transfer learning by showing
that image embeddings trained on it achieve competitive retrieval performance
on independent datasets. The dataset images, ground-truth and metric scoring
code are available at https://github.com/cvdfoundation/google-landmark.</p>
</td>
    <td>
      
        Datasets 
      
        Evaluation 
      
        SCALABILITY 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/wu2020nearest/">Nearest Neighbor Search For Hyperbolic Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Nearest Neighbor Search For Hyperbolic Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Nearest Neighbor Search For Hyperbolic Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu Xian, Charikar Moses</td> <!-- 🔧 You were missing this -->
    <td>Journal of Mathematical Imaging and Vision</td>
    <td>6</td>
    <td><p>Embedding into hyperbolic space is emerging as an effective representation
technique for datasets that exhibit hierarchical structure. This development
motivates the need for algorithms that are able to effectively extract
knowledge and insights from datapoints embedded in negatively curved spaces. We
focus on the problem of nearest neighbor search, a fundamental problem in data
analysis. We present efficient algorithmic solutions that build upon
established methods for nearest neighbor search in Euclidean space, allowing
for easy adoption and integration with existing systems. We prove theoretical
guarantees for our techniques and our experiments demonstrate the effectiveness
of our approach on real datasets over competing algorithms.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/westermann2020sentence/">Sentence Embeddings And High-speed Similarity Search For Fast Computer Assisted Annotation Of Legal Documents</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Sentence Embeddings And High-speed Similarity Search For Fast Computer Assisted Annotation Of Legal Documents' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Sentence Embeddings And High-speed Similarity Search For Fast Computer Assisted Annotation Of Legal Documents' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Westermann et al.</td> <!-- 🔧 You were missing this -->
    <td>Frontiers in Artificial Intelligence and Applications</td>
    <td>22</td>
    <td><p>Human-performed annotation of sentences in legal documents is an important
prerequisite to many machine learning based systems supporting legal tasks.
Typically, the annotation is done sequentially, sentence by sentence, which is
often time consuming and, hence, expensive. In this paper, we introduce a
proof-of-concept system for annotating sentences “laterally.” The approach is
based on the observation that sentences that are similar in meaning often have
the same label in terms of a particular type system. We use this observation in
allowing annotators to quickly view and annotate sentences that are
semantically similar to a given sentence, across an entire corpus of documents.
Here, we present the interface of the system and empirically evaluate the
approach. The experiments show that lateral annotation has the potential to
make the annotation process quicker and more consistent.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/luo2020collaborative/">Collaborative Learning For Extremely Low Bit Asymmetric Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Collaborative Learning For Extremely Low Bit Asymmetric Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Collaborative Learning For Extremely Low Bit Asymmetric Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Luo et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Knowledge and Data Engineering</td>
    <td>14</td>
    <td><p>Hashing techniques are in great demand for a wide range of real-world
applications such as image retrieval and network compression. Nevertheless,
existing approaches could hardly guarantee a satisfactory performance with the
extremely low-bit (e.g., 4-bit) hash codes due to the severe information loss
and the shrink of the discrete solution space. In this paper, we propose a
novel \textit{Collaborative Learning} strategy that is tailored for generating
high-quality low-bit hash codes. The core idea is to jointly distill
bit-specific and informative representations for a group of pre-defined code
lengths. The learning of short hash codes among the group can benefit from the
manifold shared with other long codes, where multiple views from different hash
codes provide the supplementary guidance and regularization, making the
convergence faster and more stable. To achieve that, an asymmetric hashing
framework with two variants of multi-head embedding structures is derived,
termed as Multi-head Asymmetric Hashing (MAH), leading to great efficiency of
training and querying. Extensive experiments on three benchmark datasets have
been conducted to verify the superiority of the proposed MAH, and have shown
that the 8-bit hash codes generated by MAH achieve \(94.3%\) of the MAP (Mean
Average Precision (MAP)) score on the CIFAR-10 dataset, which significantly
surpasses the performance of the 48-bit codes by the state-of-the-arts in image
retrieval tasks.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/tian2020hynet/">Hynet: Learning Local Descriptor With Hybrid Similarity Measure And Triplet Loss</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hynet: Learning Local Descriptor With Hybrid Similarity Measure And Triplet Loss' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hynet: Learning Local Descriptor With Hybrid Similarity Measure And Triplet Loss' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tian et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>33</td>
    <td><p>Recent works show that local descriptor learning benefits from the use of L2
normalisation, however, an in-depth analysis of this effect lacks in the
literature. In this paper, we investigate how L2 normalisation affects the
back-propagated descriptor gradients during training. Based on our
observations, we propose HyNet, a new local descriptor that leads to
state-of-the-art results in matching. HyNet introduces a hybrid similarity
measure for triplet margin loss, a regularisation term constraining the
descriptor norm, and a new network architecture that performs L2 normalisation
of all intermediate feature maps and the output descriptors. HyNet surpasses
previous methods by a significant margin on standard benchmarks that include
patch matching, verification, and retrieval, as well as outperforming full
end-to-end methods on 3D reconstruction tasks.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/mandal2020novel/">A Novel Incremental Cross-modal Hashing Approach</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Novel Incremental Cross-modal Hashing Approach' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Novel Incremental Cross-modal Hashing Approach' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mandal Devraj, Biswas Soma</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>5</td>
    <td><p>Cross-modal retrieval deals with retrieving relevant items from one modality,
when provided with a search query from another modality. Hashing techniques,
where the data is represented as binary bits have specifically gained
importance due to the ease of storage, fast computations and high accuracy. In
real world, the number of data categories is continuously increasing, which
requires algorithms capable of handling this dynamic scenario. In this work, we
propose a novel incremental cross-modal hashing algorithm termed “iCMH”, which
can adapt itself to handle incoming data of new categories. The proposed
approach consists of two sequential stages, namely, learning the hash codes and
training the hash functions. At every stage, a small amount of old category
data termed “exemplars” is is used so as not to forget the old data while
trying to learn for the new incoming data, i.e. to avoid catastrophic
forgetting. In the first stage, the hash codes for the exemplars is used, and
simultaneously, hash codes for the new data is computed such that it maintains
the semantic relations with the existing data. For the second stage, we propose
both a non-deep and deep architectures to learn the hash functions effectively.
Extensive experiments across a variety of cross-modal datasets and comparisons
with state-of-the-art cross-modal algorithms shows the usefulness of our
approach.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/matsubara2020target/">Target-oriented Deformation Of Visual-semantic Embedding Space</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Target-oriented Deformation Of Visual-semantic Embedding Space' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Target-oriented Deformation Of Visual-semantic Embedding Space' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Matsubara Takashi</td> <!-- 🔧 You were missing this -->
    <td>IEICE Transactions on Information and Systems</td>
    <td>8</td>
    <td><p>Multimodal embedding is a crucial research topic for cross-modal
understanding, data mining, and translation. Many studies have attempted to
extract representations from given entities and align them in a shared
embedding space. However, because entities in different modalities exhibit
different abstraction levels and modality-specific information, it is
insufficient to embed related entities close to each other. In this study, we
propose the Target-Oriented Deformation Network (TOD-Net), a novel module that
continuously deforms the embedding space into a new space under a given
condition, thereby adjusting similarities between entities. Unlike methods
based on cross-modal attention, TOD-Net is a post-process applied to the
embedding space learned by existing embedding systems and improves their
performances of retrieval. In particular, when combined with cutting-edge
models, TOD-Net gains the state-of-the-art cross-modal retrieval model
associated with the MSCOCO dataset. Qualitative analysis reveals that TOD-Net
successfully emphasizes entity-specific concepts and retrieves diverse targets
via handling higher levels of diversity than existing models.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/manandhar2020semantic/">Semantic Granularity Metric Learning For Visual Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semantic Granularity Metric Learning For Visual Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semantic Granularity Metric Learning For Visual Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Manandhar Dipu, Bastan Muhammet, Yap Kim-hui</td> <!-- 🔧 You were missing this -->
    <td>Journal of Visual Communication and Image Representation</td>
    <td>10</td>
    <td><p>Deep metric learning applied to various applications has shown promising
results in identification, retrieval and recognition. Existing methods often do
not consider different granularity in visual similarity. However, in many
domain applications, images exhibit similarity at multiple granularities with
visual semantic concepts, e.g. fashion demonstrates similarity ranging from
clothing of the exact same instance to similar looks/design or a common
category. Therefore, training image triplets/pairs used for metric learning
inherently possess different degree of information. However, the existing
methods often treats them with equal importance during training. This hinders
capturing the underlying granularities in feature similarity required for
effective visual search.
  In view of this, we propose a new deep semantic granularity metric learning
(SGML) that develops a novel idea of leveraging attribute semantic space to
capture different granularity of similarity, and then integrate this
information into deep metric learning. The proposed method simultaneously
learns image attributes and embeddings using multitask CNNs. The two tasks are
not only jointly optimized but are further linked by the semantic granularity
similarity mappings to leverage the correlations between the tasks. To this
end, we propose a new soft-binomial deviance loss that effectively integrates
the degree of information in training samples, which helps to capture visual
similarity at multiple granularities. Compared to recent ensemble-based
methods, our framework is conceptually elegant, computationally simple and
provides better performance. We perform extensive experiments on benchmark
metric learning datasets and demonstrate that our method outperforms recent
state-of-the-art methods, e.g., 1-4.5% improvement in Recall@1 over the
previous state-of-the-arts [1],[2] on DeepFashion In-Shop dataset.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/xu2020multi/">Multi-feature Discrete Collaborative Filtering For Fast Cold-start Recommendation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multi-feature Discrete Collaborative Filtering For Fast Cold-start Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multi-feature Discrete Collaborative Filtering For Fast Cold-start Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>21</td>
    <td><p>Hashing is an effective technique to address the large-scale recommendation
problem, due to its high computation and storage efficiency on calculating the
user preferences on items. However, existing hashing-based recommendation
methods still suffer from two important problems: 1) Their recommendation
process mainly relies on the user-item interactions and single specific content
feature. When the interaction history or the content feature is unavailable
(the cold-start problem), their performance will be seriously deteriorated. 2)
Existing methods learn the hash codes with relaxed optimization or adopt
discrete coordinate descent to directly solve binary hash codes, which results
in significant quantization loss or consumes considerable computation time. In
this paper, we propose a fast cold-start recommendation method, called
Multi-Feature Discrete Collaborative Filtering (MFDCF), to solve these
problems. Specifically, a low-rank self-weighted multi-feature fusion module is
designed to adaptively project the multiple content features into binary yet
informative hash codes by fully exploiting their complementarity. Additionally,
we develop a fast discrete optimization algorithm to directly compute the
binary hash codes with simple operations. Experiments on two public
recommendation datasets demonstrate that MFDCF outperforms the
state-of-the-arts on various aspects.</p>
</td>
    <td>
      
        Recommender Systems 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/liu2020shuffle/">Shuffle And Learn: Minimizing Mutual Information For Unsupervised Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Shuffle And Learn: Minimizing Mutual Information For Unsupervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Shuffle And Learn: Minimizing Mutual Information For Unsupervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Fangrui, Liu Zheng</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 1999 Congress on Evolutionary Computation-CEC99 (Cat. No. 99TH8406)</td>
    <td>8</td>
    <td><p>Unsupervised binary representation allows fast data retrieval without any
annotations, enabling practical application like fast person re-identification
and multimedia retrieval. It is argued that conflicts in binary space are one
of the major barriers to high-performance unsupervised hashing as current
methods failed to capture the precise code conflicts in the full domain. A
novel relaxation method called Shuffle and Learn is proposed to tackle code
conflicts in the unsupervised hash. Approximated derivatives for joint
probability and the gradients for the binary layer are introduced to bridge the
update from the hash to the input. Proof on \(\epsilon\)-Convergence of joint
probability with approximated derivatives is provided to guarantee the
preciseness on update applied on the mutual information. The proposed algorithm
is carried out with iterative global updates to minimize mutual information,
diverging the code before regular unsupervised optimization. Experiments
suggest that the proposed method can relax the code optimization from local
optimum and help to generate binary representations that are more
discriminative and informative without any annotations. Performance benchmarks
on image retrieval with the unsupervised binary code are conducted on three
open datasets, and the model achieves state-of-the-art accuracy on image
retrieval task for all those datasets. Datasets and reproducible code are
provided.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        SUPERVISED 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/jin2020ssah/">SSAH: Semi-supervised Adversarial Deep Hashing With Self-paced Hard Sample Generation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=SSAH: Semi-supervised Adversarial Deep Hashing With Self-paced Hard Sample Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=SSAH: Semi-supervised Adversarial Deep Hashing With Self-paced Hard Sample Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jin et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>28</td>
    <td><p>Deep hashing methods have been proved to be effective and efficient for
large-scale Web media search. The success of these data-driven methods largely
depends on collecting sufficient labeled data, which is usually a crucial
limitation in practical cases. The current solutions to this issue utilize
Generative Adversarial Network (GAN) to augment data in semi-supervised
learning. However, existing GAN-based methods treat image generations and
hashing learning as two isolated processes, leading to generation
ineffectiveness. Besides, most works fail to exploit the semantic information
in unlabeled data. In this paper, we propose a novel Semi-supervised Self-pace
Adversarial Hashing method, named SSAH to solve the above problems in a unified
framework. The SSAH method consists of an adversarial network (A-Net) and a
hashing network (H-Net). To improve the quality of generative images, first,
the A-Net learns hard samples with multi-scale occlusions and multi-angle
rotated deformations which compete against the learning of accurate hashing
codes. Second, we design a novel self-paced hard generation policy to gradually
increase the hashing difficulty of generated samples. To make use of the
semantic information in unlabeled ones, we propose a semi-supervised consistent
loss. The experimental results show that our method can significantly improve
state-of-the-art models on both the widely-used hashing datasets and
fine-grained datasets.</p>
</td>
    <td>
      
        Robustness 
      
        Neural Hashing 
      
        AAAI 
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/hoang2020unsupervised/">Unsupervised Deep Cross-modality Spectral Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Deep Cross-modality Spectral Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Deep Cross-modality Spectral Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hoang et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>28</td>
    <td><p>This paper presents a novel framework, namely Deep Cross-modality Spectral
Hashing (DCSH), to tackle the unsupervised learning problem of binary hash
codes for efficient cross-modal retrieval. The framework is a two-step hashing
approach which decouples the optimization into (1) binary optimization and (2)
hashing function learning. In the first step, we propose a novel spectral
embedding-based algorithm to simultaneously learn single-modality and binary
cross-modality representations. While the former is capable of well preserving
the local structure of each modality, the latter reveals the hidden patterns
from all modalities. In the second step, to learn mapping functions from
informative data inputs (images and word embeddings) to binary codes obtained
from the first step, we leverage the powerful CNN for images and propose a
CNN-based deep architecture to learn text modality. Quantitative evaluations on
three standard benchmark datasets demonstrate that the proposed DCSH method
consistently outperforms other state-of-the-art methods.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/henkel2020supporting/">Supporting Large-scale Image Recognition With Out-of-domain Samples</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Supporting Large-scale Image Recognition With Out-of-domain Samples' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Supporting Large-scale Image Recognition With Out-of-domain Samples' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Henkel Christof, Singer Philipp</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>This article presents an efficient end-to-end method to perform
instance-level recognition employed to the task of labeling and ranking
landmark images. In a first step, we embed images in a high dimensional feature
space using convolutional neural networks trained with an additive angular
margin loss and classify images using visual similarity. We then efficiently
re-rank predictions and filter noise utilizing similarity to out-of-domain
images. Using this approach we achieved the 1st place in the 2020 edition of
the Google Landmark Recognition challenge.</p>
</td>
    <td>
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/shand2020locality/">Locality-sensitive Hashing In Function Spaces</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Locality-sensitive Hashing In Function Spaces' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Locality-sensitive Hashing In Function Spaces' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shand Will, Becker Stephen</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Third International Conference on SImilarity Search and APplications</td>
    <td>15</td>
    <td><p>We discuss the problem of performing similarity search over function spaces.
To perform search over such spaces in a reasonable amount of time, we use {\it
locality-sensitive hashing} (LSH). We present two methods that allow LSH
functions on \(\mathbb{R}^N\) to be extended to \(L^p\) spaces: one using function
approximation in an orthonormal basis, and another using (quasi-)Monte
Carlo-style techniques. We use the presented hashing schemes to construct an
LSH family for Wasserstein distance over one-dimensional, continuous
probability distributions.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Similarity Search 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/hansen2020unsupervised/">Unsupervised Semantic Hashing With Pairwise Reconstruction</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Semantic Hashing With Pairwise Reconstruction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Semantic Hashing With Pairwise Reconstruction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hansen et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>22</td>
    <td><p>Semantic Hashing is a popular family of methods for efficient similarity
search in large-scale datasets. In Semantic Hashing, documents are encoded as
short binary vectors (i.e., hash codes), such that semantic similarity can be
efficiently computed using the Hamming distance. Recent state-of-the-art
approaches have utilized weak supervision to train better performing hashing
models. Inspired by this, we present Semantic Hashing with Pairwise
Reconstruction (PairRec), which is a discrete variational autoencoder based
hashing model. PairRec first encodes weakly supervised training pairs (a query
document and a semantically similar document) into two hash codes, and then
learns to reconstruct the same query document from both of these hash codes
(i.e., pairwise reconstruction). This pairwise reconstruction enables our model
to encode local neighbourhood structures within the hash code directly through
the decoder. We experimentally compare PairRec to traditional and
state-of-the-art approaches, and obtain significant performance improvements in
the task of document similarity search.</p>
</td>
    <td>
      
        Text Retrieval 
      
        Unsupervised 
      
        SUPERVISED 
      
        SIGIR 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/hansen2020content/">Content-aware Neural Hashing For Cold-start Recommendation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Content-aware Neural Hashing For Cold-start Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Content-aware Neural Hashing For Cold-start Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hansen et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>36</td>
    <td><p>Content-aware recommendation approaches are essential for providing
meaningful recommendations for \textit{new} (i.e., \textit{cold-start}) items
in a recommender system. We present a content-aware neural hashing-based
collaborative filtering approach (NeuHash-CF), which generates binary hash
codes for users and items, such that the highly efficient Hamming distance can
be used for estimating user-item relevance. NeuHash-CF is modelled as an
autoencoder architecture, consisting of two joint hashing components for
generating user and item hash codes. Inspired from semantic hashing, the item
hashing component generates a hash code directly from an item’s content
information (i.e., it generates cold-start and seen item hash codes in the same
manner). This contrasts existing state-of-the-art models, which treat the two
item cases separately. The user hash codes are generated directly based on user
id, through learning a user embedding matrix. We show experimentally that
NeuHash-CF significantly outperforms state-of-the-art baselines by up to 12%
NDCG and 13% MRR in cold-start recommendation settings, and up to 4% in both
NDCG and MRR in standard settings where all items are present while training.
Our approach uses 2-4x shorter hash codes, while obtaining the same or better
performance compared to the state of the art, thus consequently also enabling a
notable storage reduction.</p>
</td>
    <td>
      
        Text Retrieval 
      
        Recommender Systems 
      
        Neural Hashing 
      
        SIGIR 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/thoma2020geometrically/">Geometrically Mappable Image Features</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Geometrically Mappable Image Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Geometrically Mappable Image Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Thoma et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Robotics and Automation Letters</td>
    <td>5</td>
    <td><p>Vision-based localization of an agent in a map is an important problem in
robotics and computer vision. In that context, localization by learning
matchable image features is gaining popularity due to recent advances in
machine learning. Features that uniquely describe the visual contents of images
have a wide range of applications, including image retrieval and understanding.
In this work, we propose a method that learns image features targeted for
image-retrieval-based localization. Retrieval-based localization has several
benefits, such as easy maintenance and quick computation. However, the
state-of-the-art features only provide visual similarity scores which do not
explicitly reveal the geometric distance between query and retrieved images.
Knowing this distance is highly desirable for accurate localization, especially
when the reference images are sparsely distributed in the scene. Therefore, we
propose a novel loss function for learning image features which are both
visually representative and geometrically relatable. This is achieved by
guiding the learning process such that the feature and geometric distances
between images are directly proportional. In our experiments we show that our
features not only offer significantly better localization accuracy, but also
allow to estimate the trajectory of a query sequence in absence of the
reference images.</p>
</td>
    <td>
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/su%C3%A1rez2020beblid/">BEBLID: Boosted Efficient Binary Local Image Descriptor</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=BEBLID: Boosted Efficient Binary Local Image Descriptor' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=BEBLID: Boosted Efficient Binary Local Image Descriptor' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Suárez et al.</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition Letters</td>
    <td>87</td>
    <td><p>Efficient matching of local image features is a fundamental task in many
computer vision applications. However, the real-time performance of top
matching algorithms is compromised in computationally limited devices, such as
mobile phones or drones, due to the simplicity of their hardware and their
finite energy supply. In this paper we introduce BEBLID, an efficient learned
binary image descriptor. It improves our previous real-valued descriptor,
BELID, making it both more efficient for matching and more accurate. To this
end we use AdaBoost with an improved weak-learner training scheme that produces
better local descriptions. Further, we binarize our descriptor by forcing all
weak-learners to have the same weight in the strong learner combination and
train it in an unbalanced data set to address the asymmetries arising in
matching and retrieval tasks. In our experiments BEBLID achieves an accuracy
close to SIFT and better computational efficiency than ORB, the fastest
algorithm in the literature.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/schroeder2020structured/">Structured Query-based Image Retrieval Using Scene Graphs</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Structured Query-based Image Retrieval Using Scene Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Structured Query-based Image Retrieval Using Scene Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Schroeder Brigit, Tripathi Subarna</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>49</td>
    <td><p>A structured query can capture the complexity of object interactions (e.g.
‘woman rides motorcycle’) unlike single objects (e.g. ‘woman’ or ‘motorcycle’).
Retrieval using structured queries therefore is much more useful than single
object retrieval, but a much more challenging problem. In this paper we present
a method which uses scene graph embeddings as the basis for an approach to
image retrieval. We examine how visual relationships, derived from scene
graphs, can be used as structured queries. The visual relationships are
directed subgraphs of the scene graph with a subject and object as nodes
connected by a predicate relationship. Notably, we are able to achieve high
recall even on low to medium frequency objects found in the long-tailed
COCO-Stuff dataset, and find that adding a visual relationship-inspired loss
boosts our recall by 10% in the best case.</p>
</td>
    <td>
      
        Image Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/schulz2020can/">Can Embeddings Adequately Represent Medical Terminology? New Large-scale Medical Term Similarity Datasets Have The Answer!</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Can Embeddings Adequately Represent Medical Terminology? New Large-scale Medical Term Similarity Datasets Have The Answer!' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Can Embeddings Adequately Represent Medical Terminology? New Large-scale Medical Term Similarity Datasets Have The Answer!' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Schulz Claudia, Juric Damir</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>8</td>
    <td><p>A large number of embeddings trained on medical data have emerged, but it
remains unclear how well they represent medical terminology, in particular
whether the close relationship of semantically similar medical terms is encoded
in these embeddings. To date, only small datasets for testing medical term
similarity are available, not allowing to draw conclusions about the
generalisability of embeddings to the enormous amount of medical terms used by
doctors. We present multiple automatically created large-scale medical term
similarity datasets and confirm their high quality in an annotation study with
doctors. We evaluate state-of-the-art word and contextual embeddings on our new
datasets, comparing multiple vector similarity metrics and word vector
aggregation techniques. Our results show that current embeddings are limited in
their ability to adequately encode medical terms. The novel datasets thus form
a challenging new benchmark for the development of medical embeddings able to
accurately represent the whole medical terminology.</p>
</td>
    <td>
      
        Datasets 
      
        Graph Based ANN 
      
        AAAI 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/gu2020symmetrical/">Symmetrical Synthesis For Deep Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Symmetrical Synthesis For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Symmetrical Synthesis For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gu Geonmo, Ko Byungsoo</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>21</td>
    <td><p>Deep metric learning aims to learn embeddings that contain semantic
similarity information among data points. To learn better embeddings, methods
to generate synthetic hard samples have been proposed. Existing methods of
synthetic hard sample generation are adopting autoencoders or generative
adversarial networks, but this leads to more hyper-parameters, harder
optimization, and slower training speed. In this paper, we address these
problems by proposing a novel method of synthetic hard sample generation called
symmetrical synthesis. Given two original feature points from the same class,
the proposed method firstly generates synthetic points with each other as an
axis of symmetry. Secondly, it performs hard negative pair mining within the
original and synthetic points to select a more informative negative pair for
computing the metric learning loss. Our proposed method is hyper-parameter free
and plug-and-play for existing metric learning losses without network
modification. We demonstrate the superiority of our proposed method over
existing methods for a variety of loss functions on clustering and image
retrieval tasks. Our implementations is publicly available.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/guerrero2020cross/">Cross-modal Retrieval And Synthesis (X-MRS): Closing The Modality Gap In Shared Representation Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-modal Retrieval And Synthesis (X-MRS): Closing The Modality Gap In Shared Representation Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-modal Retrieval And Synthesis (X-MRS): Closing The Modality Gap In Shared Representation Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Guerrero Ricardo, Pham Hai Xuan, Pavlovic Vladimir</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 29th ACM International Conference on Multimedia</td>
    <td>29</td>
    <td><p>Computational food analysis (CFA) naturally requires multi-modal evidence of
a particular food, e.g., images, recipe text, etc. A key to making CFA possible
is multi-modal shared representation learning, which aims to create a joint
representation of the multiple views (text and image) of the data. In this work
we propose a method for food domain cross-modal shared representation learning
that preserves the vast semantic richness present in the food data. Our
proposed method employs an effective transformer-based multilingual recipe
encoder coupled with a traditional image embedding architecture. Here, we
propose the use of imperfect multilingual translations to effectively
regularize the model while at the same time adding functionality across
multiple languages and alphabets. Experimental analysis on the public Recipe1M
dataset shows that the representation learned via the proposed method
significantly outperforms the current state-of-the-arts (SOTA) on retrieval
tasks. Furthermore, the representational power of the learned representation is
demonstrated through a generative food image synthesis model conditioned on
recipe embeddings. Synthesized images can effectively reproduce the visual
appearance of paired samples, indicating that the learned representation
captures the joint semantics of both the textual recipe and its visual content,
thus narrowing the modality gap.</p>
</td>
    <td>
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/tian2020learning/">Learning Decorrelated Hashing Codes For Multimodal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Decorrelated Hashing Codes For Multimodal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Decorrelated Hashing Codes For Multimodal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tian Dayong</td> <!-- 🔧 You were missing this -->
    <td>IEEE Access</td>
    <td>5</td>
    <td><p>In social networks, heterogeneous multimedia data correlate to each other,
such as videos and their corresponding tags in YouTube and image-text pairs in
Facebook. Nearest neighbor retrieval across multiple modalities on large data
sets becomes a hot yet challenging problem. Hashing is expected to be an
efficient solution, since it represents data as binary codes. As the bit-wise
XOR operations can be fast handled, the retrieval time is greatly reduced. Few
existing multimodal hashing methods consider the correlation among hashing
bits. The correlation has negative impact on hashing codes. When the hashing
code length becomes longer, the retrieval performance improvement becomes
slower. In this paper, we propose a minimum correlation regularization (MCR)
for multimodal hashing. First, the sigmoid function is used to embed the data
matrices. Then, the MCR is applied on the output of sigmoid function. As the
output of sigmoid function approximates a binary code matrix, the proposed MCR
can efficiently decorrelate the hashing codes. Experiments show the superiority
of the proposed method becomes greater as the code length increases.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/gerritse2020graph/">Graph-embedding Empowered Entity Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Graph-embedding Empowered Entity Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Graph-embedding Empowered Entity Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gerritse Emma J., Hasibi Faegheh, de Vries Arjen P.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>25</td>
    <td><p>In this research, we improve upon the current state of the art in entity
retrieval by re-ranking the result list using graph embeddings. The paper shows
that graph embeddings are useful for entity-oriented search tasks. We
demonstrate empirically that encoding information from the knowledge graph into
(graph) embeddings contributes to a higher increase in effectiveness of entity
retrieval results than using plain word embeddings. We analyze the impact of
the accuracy of the entity linker on the overall retrieval effectiveness. Our
analysis further deploys the cluster hypothesis to explain the observed
advantages of graph embeddings over the more widely used word embeddings, for
user tasks involving ranking entities.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/zhu2020dual/">Dual-level Semantic Transfer Deep Hashing For Efficient Social Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Dual-level Semantic Transfer Deep Hashing For Efficient Social Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Dual-level Semantic Transfer Deep Hashing For Efficient Social Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhu et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Circuits and Systems for Video Technology</td>
    <td>38</td>
    <td><p>Social network stores and disseminates a tremendous amount of user shared
images. Deep hashing is an efficient indexing technique to support large-scale
social image retrieval, due to its deep representation capability, fast
retrieval speed and low storage cost. Particularly, unsupervised deep hashing
has well scalability as it does not require any manually labelled data for
training. However, owing to the lacking of label guidance, existing methods
suffer from severe semantic shortage when optimizing a large amount of deep
neural network parameters. Differently, in this paper, we propose a Dual-level
Semantic Transfer Deep Hashing (DSTDH) method to alleviate this problem with a
unified deep hash learning framework. Our model targets at learning the
semantically enhanced deep hash codes by specially exploiting the
user-generated tags associated with the social images. Specifically, we design
a complementary dual-level semantic transfer mechanism to efficiently discover
the potential semantics of tags and seamlessly transfer them into binary hash
codes. On the one hand, instance-level semantics are directly preserved into
hash codes from the associated tags with adverse noise removing. Besides, an
image-concept hypergraph is constructed for indirectly transferring the latent
high-order semantic correlations of images and tags into hash codes. Moreover,
the hash codes are obtained simultaneously with the deep representation
learning by the discrete hash optimization strategy. Extensive experiments on
two public social image retrieval datasets validate the superior performance of
our method compared with state-of-the-art hashing methods. The source codes of
our method can be obtained at https://github.com/research2020-1/DSTDH</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/garg2020fast/">Fast, Compact And Highly Scalable Visual Place Recognition Through Sequence-based Matching Of Overloaded Representations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast, Compact And Highly Scalable Visual Place Recognition Through Sequence-based Matching Of Overloaded Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast, Compact And Highly Scalable Visual Place Recognition Through Sequence-based Matching Of Overloaded Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Garg Sourav, Milford Michael</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE International Conference on Robotics and Automation (ICRA)</td>
    <td>14</td>
    <td><p>Visual place recognition algorithms trade off three key characteristics:
their storage footprint, their computational requirements, and their resultant
performance, often expressed in terms of recall rate. Significant prior work
has investigated highly compact place representations, sub-linear computational
scaling and sub-linear storage scaling techniques, but have always involved a
significant compromise in one or more of these regards, and have only been
demonstrated on relatively small datasets. In this paper we present a novel
place recognition system which enables for the first time the combination of
ultra-compact place representations, near sub-linear storage scaling and
extremely lightweight compute requirements. Our approach exploits the
inherently sequential nature of much spatial data in the robotics domain and
inverts the typical target criteria, through intentionally coarse scalar
quantization-based hashing that leads to more collisions but is resolved by
sequence-based matching. For the first time, we show how effective place
recognition rates can be achieved on a new very large 10 million place dataset,
requiring only 8 bytes of storage per place and 37K unitary operations to
achieve over 50% recall for matching a sequence of 100 frames, where a
conventional state-of-the-art approach both consumes 1300 times more compute
and fails catastrophically. We present analysis investigating the effectiveness
of our hashing overload approach under varying sizes of quantized vector
length, comparison of near miss matches with the actual match selections and
characterise the effect of variance re-scaling of data on quantization.</p>
</td>
    <td>
      
        ICRA 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/zhang2020part/">Part-guided Attention Learning For Vehicle Instance Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Part-guided Attention Learning For Vehicle Instance Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Part-guided Attention Learning For Vehicle Instance Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Intelligent Transportation Systems</td>
    <td>56</td>
    <td><p>Vehicle instance retrieval often requires one to recognize the fine-grained
visual differences between vehicles. Besides the holistic appearance of
vehicles which is easily affected by the viewpoint variation and distortion,
vehicle parts also provide crucial cues to differentiate near-identical
vehicles. Motivated by these observations, we introduce a Part-Guided Attention
Network (PGAN) to pinpoint the prominent part regions and effectively combine
the global and part information for discriminative feature learning. PGAN first
detects the locations of different part components and salient regions
regardless of the vehicle identity, which serve as the bottom-up attention to
narrow down the possible searching regions. To estimate the importance of
detected parts, we propose a Part Attention Module (PAM) to adaptively locate
the most discriminative regions with high-attention weights and suppress the
distraction of irrelevant parts with relatively low weights. The PAM is guided
by the instance retrieval loss and therefore provides top-down attention that
enables attention to be calculated at the level of car parts and other salient
regions. Finally, we aggregate the global appearance and part features to
improve the feature performance further. The PGAN combines part-guided
bottom-up and top-down attention, global and part visual features in an
end-to-end framework. Extensive experiments demonstrate that the proposed
method achieves new state-of-the-art vehicle instance retrieval performance on
four large-scale benchmark datasets.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/gao2020complementing/">Complementing Lexical Retrieval With Semantic Residual Embedding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Complementing Lexical Retrieval With Semantic Residual Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Complementing Lexical Retrieval With Semantic Residual Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gao et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>61</td>
    <td><p>This paper presents CLEAR, a retrieval model that seeks to complement
classical lexical exact-match models such as BM25 with semantic matching
signals from a neural embedding matching model. CLEAR explicitly trains the
neural embedding to encode language structures and semantics that lexical
retrieval fails to capture with a novel residual-based embedding learning
method. Empirical evaluations demonstrate the advantages of CLEAR over
state-of-the-art retrieval models, and that it can substantially improve the
end-to-end accuracy and efficiency of reranking pipelines.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/gao2020fashionbert/">Fashionbert: Text And Image Matching With Adaptive Loss For Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fashionbert: Text And Image Matching With Adaptive Loss For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fashionbert: Text And Image Matching With Adaptive Loss For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gao et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>102</td>
    <td><p>In this paper, we address the text and image matching in cross-modal
retrieval of the fashion industry. Different from the matching in the general
domain, the fashion matching is required to pay much more attention to the
fine-grained information in the fashion images and texts. Pioneer approaches
detect the region of interests (i.e., RoIs) from images and use the RoI
embeddings as image representations. In general, RoIs tend to represent the
“object-level” information in the fashion images, while fashion texts are prone
to describe more detailed information, e.g. styles, attributes. RoIs are thus
not fine-grained enough for fashion text and image matching. To this end, we
propose FashionBERT, which leverages patches as image features. With the
pre-trained BERT model as the backbone network, FashionBERT learns high level
representations of texts and images. Meanwhile, we propose an adaptive loss to
trade off multitask learning in the FashionBERT modeling. Two tasks (i.e., text
and image matching and cross-modal retrieval) are incorporated to evaluate
FashionBERT. On the public dataset, experiments demonstrate FashionBERT
achieves significant improvements in performances than the baseline and
state-of-the-art approaches. In practice, FashionBERT is applied in a concrete
cross-modal retrieval application. We provide the detailed matching performance
and inference efficiency analysis.</p>
</td>
    <td>
      
        SIGIR 
      
        Multimodal Retrieval 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/fu2020hard/">Hard Example Generation By Texture Synthesis For Cross-domain Shape Similarity Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hard Example Generation By Texture Synthesis For Cross-domain Shape Similarity Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hard Example Generation By Texture Synthesis For Cross-domain Shape Similarity Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fu et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>6</td>
    <td><p>Image-based 3D shape retrieval (IBSR) aims to find the corresponding 3D shape
of a given 2D image from a large 3D shape database. The common routine is to
map 2D images and 3D shapes into an embedding space and define (or learn) a
shape similarity measure. While metric learning with some adaptation techniques
seems to be a natural solution to shape similarity learning, the performance is
often unsatisfactory for fine-grained shape retrieval. In the paper, we
identify the source of the poor performance and propose a practical solution to
this problem. We find that the shape difference between a negative pair is
entangled with the texture gap, making metric learning ineffective in pushing
away negative pairs. To tackle this issue, we develop a geometry-focused
multi-view metric learning framework empowered by texture synthesis. The
synthesis of textures for 3D shape models creates hard triplets, which suppress
the adverse effects of rich texture in 2D images, thereby push the network to
focus more on discovering geometric characteristics. Our approach shows
state-of-the-art performance on a recently released large-scale 3D-FUTURE[1]
repository, as well as three widely studied benchmarks, including Pix3D[2],
Stanford Cars[3], and Comp Cars[4]. Codes will be made publicly available at:
https://github.com/3D-FRONT-FUTURE/IBSR-texture</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/gabeur2020multi/">Multi-modal Transformer For Video Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multi-modal Transformer For Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multi-modal Transformer For Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gabeur et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>486</td>
    <td><p>The task of retrieving video content relevant to natural language queries
plays a critical role in effectively handling internet-scale datasets. Most of
the existing methods for this caption-to-video retrieval problem do not fully
exploit cross-modal cues present in video. Furthermore, they aggregate
per-frame visual features with limited or no temporal information. In this
paper, we present a multi-modal transformer to jointly encode the different
modalities in video, which allows each of them to attend to the others. The
transformer architecture is also leveraged to encode and model the temporal
information. On the natural language side, we investigate the best practices to
jointly optimize the language embedding together with the multi-modal
transformer. This novel framework allows us to establish state-of-the-art
results for video retrieval on three datasets. More details are available at
http://thoth.inrialpes.fr/research/MMT.</p>
</td>
    <td>
      
        Video Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/sun2020benchmarking/">A Benchmarking Study Of Embedding-based Entity Alignment For Knowledge Graphs</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Benchmarking Study Of Embedding-based Entity Alignment For Knowledge Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Benchmarking Study Of Embedding-based Entity Alignment For Knowledge Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sun et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the VLDB Endowment</td>
    <td>79</td>
    <td><p>Entity alignment seeks to find entities in different knowledge graphs (KGs)
that refer to the same real-world object. Recent advancement in KG embedding
impels the advent of embedding-based entity alignment, which encodes entities
in a continuous embedding space and measures entity similarities based on the
learned embeddings. In this paper, we conduct a comprehensive experimental
study of this emerging field. We survey 23 recent embedding-based entity
alignment approaches and categorize them based on their techniques and
characteristics. We also propose a new KG sampling algorithm, with which we
generate a set of dedicated benchmark datasets with various heterogeneity and
distributions for a realistic evaluation. We develop an open-source library
including 12 representative embedding-based entity alignment approaches, and
extensively evaluate these approaches, to understand their strengths and
limitations. Additionally, for several directions that have not been explored
in current approaches, we perform exploratory experiments and report our
preliminary findings for future studies. The benchmark datasets, open-source
library and experimental results are all accessible online and will be duly
maintained.</p>
</td>
    <td>
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/forcen2020co/">Co-occurrence Of Deep Convolutional Features For Image Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Co-occurrence Of Deep Convolutional Features For Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Co-occurrence Of Deep Convolutional Features For Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Forcen et al.</td> <!-- 🔧 You were missing this -->
    <td>Image and Vision Computing</td>
    <td>19</td>
    <td><p>Image search can be tackled using deep features from pre-trained
Convolutional Neural Networks (CNN). The feature map from the last
convolutional layer of a CNN encodes descriptive information from which a
discriminative global descriptor can be obtained. We propose a new
representation of co-occurrences from deep convolutional features to extract
additional relevant information from this last convolutional layer. Combining
this co-occurrence map with the feature map, we achieve an improved image
representation. We present two different methods to get the co-occurrence
representation, the first one based on direct aggregation of activations, and
the second one, based on a trainable co-occurrence representation. The image
descriptors derived from our methodology improve the performance in very
well-known image retrieval datasets as we prove in the experiments.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/frady2020neuromorphic/">Neuromorphic Nearest-neighbor Search Using Intel's Pohoiki Springs</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Neuromorphic Nearest-neighbor Search Using Intel's Pohoiki Springs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Neuromorphic Nearest-neighbor Search Using Intel's Pohoiki Springs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Frady et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Neuro-inspired Computational Elements Workshop</td>
    <td>47</td>
    <td><p>Neuromorphic computing applies insights from neuroscience to uncover
innovations in computing technology. In the brain, billions of interconnected
neurons perform rapid computations at extremely low energy levels by leveraging
properties that are foreign to conventional computing systems, such as temporal
spiking codes and finely parallelized processing units integrating both memory
and computation. Here, we showcase the Pohoiki Springs neuromorphic system, a
mesh of 768 interconnected Loihi chips that collectively implement 100 million
spiking neurons in silicon. We demonstrate a scalable approximate k-nearest
neighbor (k-NN) algorithm for searching large databases that exploits
neuromorphic principles. Compared to state-of-the-art conventional CPU-based
implementations, we achieve superior latency, index build time, and energy
efficiency when evaluated on several standard datasets containing over 1
million high-dimensional patterns. Further, the system supports adding new data
points to the indexed database online in O(1) time unlike all but brute force
conventional k-NN implementations.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/feng2020unifying/">Unifying Specialist Image Embedding Into Universal Image Embedding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unifying Specialist Image Embedding Into Universal Image Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unifying Specialist Image Embedding Into Universal Image Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Feng et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>Deep image embedding provides a way to measure the semantic similarity of two
images. It plays a central role in many applications such as image search, face
verification, and zero-shot learning. It is desirable to have a universal deep
embedding model applicable to various domains of images. However, existing
methods mainly rely on training specialist embedding models each of which is
applicable to images from a single domain. In this paper, we study an important
but unexplored task: how to train a single universal image embedding model to
match the performance of several specialists on each specialist’s domain.
Simply fusing the training data from multiple domains cannot solve this problem
because some domains become overfitted sooner when trained together using
existing methods. Therefore, we propose to distill the knowledge in multiple
specialists into a universal embedding to solve this problem. In contrast to
existing embedding distillation methods that distill the absolute distances
between images, we transform the absolute distances between images into a
probabilistic distribution and minimize the KL-divergence between the
distributions of the specialists and the universal embedding. Using several
public datasets, we validate that our proposed method accomplishes the goal of
universal image embedding.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/feng2020adversarial/">Adversarial Attack On Deep Product Quantization Network For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Adversarial Attack On Deep Product Quantization Network For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Adversarial Attack On Deep Product Quantization Network For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Feng et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>43</td>
    <td><p>Deep product quantization network (DPQN) has recently received much attention
in fast image retrieval tasks due to its efficiency of encoding
high-dimensional visual features especially when dealing with large-scale
datasets. Recent studies show that deep neural networks (DNNs) are vulnerable
to input with small and maliciously designed perturbations (a.k.a., adversarial
examples). This phenomenon raises the concern of security issues for DPQN in
the testing/deploying stage as well. However, little effort has been devoted to
investigating how adversarial examples affect DPQN. To this end, we propose
product quantization adversarial generation (PQ-AG), a simple yet effective
method to generate adversarial examples for product quantization based
retrieval systems. PQ-AG aims to generate imperceptible adversarial
perturbations for query images to form adversarial queries, whose nearest
neighbors from a targeted product quantizaiton model are not semantically
related to those from the original queries. Extensive experiments show that our
PQ-AQ successfully creates adversarial examples to mislead targeted product
quantization retrieval models. Besides, we found that our PQ-AG significantly
degrades retrieval performance in both white-box and black-box settings.</p>
</td>
    <td>
      
        Quantization 
      
        Robustness 
      
        Image Retrieval 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/fang2020attention/">Attention-based Saliency Hashing For Ophthalmic Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Attention-based Saliency Hashing For Ophthalmic Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Attention-based Saliency Hashing For Ophthalmic Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fang et al.</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</td>
    <td>12</td>
    <td><p>Deep hashing methods have been proved to be effective for the large-scale
medical image search assisting reference-based diagnosis for clinicians.
However, when the salient region plays a maximal discriminative role in
ophthalmic image, existing deep hashing methods do not fully exploit the
learning ability of the deep network to capture the features of salient regions
pointedly. The different grades or classes of ophthalmic images may be share
similar overall performance but have subtle differences that can be
differentiated by mining salient regions. To address this issue, we propose a
novel end-to-end network, named Attention-based Saliency Hashing (ASH), for
learning compact hash-code to represent ophthalmic images. ASH embeds a
spatial-attention module to focus more on the representation of salient regions
and highlights their essential role in differentiating ophthalmic images.
Benefiting from the spatial-attention module, the information of salient
regions can be mapped into the hash-code for similarity calculation. In the
training stage, we input the image pairs to share the weights of the network,
and a pairwise loss is designed to maximize the discriminability of the
hash-code. In the retrieval stage, ASH obtains the hash-code by inputting an
image with an end-to-end manner, then the hash-code is used to similarity
calculation to return the most similar images. Extensive experiments on two
different modalities of ophthalmic image datasets demonstrate that the proposed
ASH can further improve the retrieval performance compared to the
state-of-the-art deep hashing methods due to the huge contributions of the
spatial-attention module.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/fan2020deep/">Deep Polarized Network For Supervised Learning Of Accurate Binary Hashing Codes</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Polarized Network For Supervised Learning Of Accurate Binary Hashing Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Polarized Network For Supervised Learning Of Accurate Binary Hashing Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fan et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</td>
    <td>81</td>
    <td><p>This paper proposes a novel deep polarized network (DPN) for learning to hash, in which each channel in the network outputs is pushed far away
from zero by employing a differentiable bit-wise hinge-like loss which is dubbed as polarization loss. Reformulated within a generic Hamming Distance Metric Learning framework [Norouzi et al.,
2012], the proposed polarization loss bypasses the requirement to prepare pairwise labels for (dis-)similar items and, yet, the proposed loss strictly bounds from above the pairwise Hamming Distance based losses. The intrinsic connection between pairwise and pointwise label information, as
disclosed in this paper, brings about the following methodological improvements: (a) we may directly employ the proposed differentiable polarization loss with no large deviations incurred from
the target Hamming distance based loss; and (b) the subtask of assigning binary codes becomes extremely simple — even random codes assigned to each class suffice to result in state-of-the-art performances, as demonstrated in CIFAR10, NUS-WIDE and ImageNet100 datasets.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
        AAAI 
      
        IJCAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/wang2020consensus/">Consensus-aware Visual-semantic Embedding For Image-text Matching</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Consensus-aware Visual-semantic Embedding For Image-text Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Consensus-aware Visual-semantic Embedding For Image-text Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>149</td>
    <td><p>Image-text matching plays a central role in bridging vision and language.
Most existing approaches only rely on the image-text instance pair to learn
their representations, thereby exploiting their matching relationships and
making the corresponding alignments. Such approaches only exploit the
superficial associations contained in the instance pairwise data, with no
consideration of any external commonsense knowledge, which may hinder their
capabilities to reason the higher-level relationships between image and text.
In this paper, we propose a Consensus-aware Visual-Semantic Embedding (CVSE)
model to incorporate the consensus information, namely the commonsense
knowledge shared between both modalities, into image-text matching.
Specifically, the consensus information is exploited by computing the
statistical co-occurrence correlations between the semantic concepts from the
image captioning corpus and deploying the constructed concept correlation graph
to yield the consensus-aware concept (CAC) representations. Afterwards, CVSE
learns the associations and alignments between image and text based on the
exploited consensus as well as the instance-level representations for both
modalities. Extensive experiments conducted on two public datasets verify that
the exploited consensus makes significant contributions to constructing more
meaningful visual-semantic embeddings, with the superior performances over the
state-of-the-art approaches on the bidirectional image and text retrieval task.
Our code of this paper is available at: https://github.com/BruceW91/CVSE.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/ertl2020probminhash/">Probminhash -- A Class Of Locality-sensitive Hash Algorithms For The (probability) Jaccard Similarity</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Probminhash -- A Class Of Locality-sensitive Hash Algorithms For The (probability) Jaccard Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Probminhash -- A Class Of Locality-sensitive Hash Algorithms For The (probability) Jaccard Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ertl Otmar</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Knowledge and Data Engineering</td>
    <td>23</td>
    <td><p>The probability Jaccard similarity was recently proposed as a natural
generalization of the Jaccard similarity to measure the proximity of sets whose
elements are associated with relative frequencies or probabilities. In
combination with a hash algorithm that maps those weighted sets to compact
signatures which allow fast estimation of pairwise similarities, it constitutes
a valuable method for big data applications such as near-duplicate detection,
nearest neighbor search, or clustering. This paper introduces a class of
one-pass locality-sensitive hash algorithms that are orders of magnitude faster
than the original approach. The performance gain is achieved by calculating
signature components not independently, but collectively. Four different
algorithms are proposed based on this idea. Two of them are statistically
equivalent to the original approach and can be used as drop-in replacements.
The other two may even improve the estimation error by introducing statistical
dependence between signature components. Moreover, the presented techniques can
be specialized for the conventional Jaccard similarity, resulting in highly
efficient algorithms that outperform traditional minwise hashing and that are
able to compete with the state of the art.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/yu2020comprehensive/">Comprehensive Graph-conditional Similarity Preserving Network For Unsupervised Cross-modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Comprehensive Graph-conditional Similarity Preserving Network For Unsupervised Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Comprehensive Graph-conditional Similarity Preserving Network For Unsupervised Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yu et al.</td> <!-- 🔧 You were missing this -->
    <td>Expert Systems with Applications</td>
    <td>18</td>
    <td><p>Unsupervised cross-modal hashing (UCMH) has become a hot topic recently.
Current UCMH focuses on exploring data similarities. However, current UCMH
methods calculate the similarity between two data, mainly relying on the two
data’s cross-modal features. These methods suffer from inaccurate similarity
problems that result in a suboptimal retrieval Hamming space, because the
cross-modal features between the data are not sufficient to describe the
complex data relationships, such as situations where two data have different
feature representations but share the inherent concepts. In this paper, we
devise a deep graph-neighbor coherence preserving network (DGCPN).
Specifically, DGCPN stems from graph models and explores graph-neighbor
coherence by consolidating the information between data and their neighbors.
DGCPN regulates comprehensive similarity preserving losses by exploiting three
types of data similarities (i.e., the graph-neighbor coherence, the coexistent
similarity, and the intra- and inter-modality consistency) and designs a
half-real and half-binary optimization strategy to reduce the quantization
errors during hashing. Essentially, DGCPN addresses the inaccurate similarity
problem by exploring and exploiting the data’s intrinsic relationships in a
graph. We conduct extensive experiments on three public UCMH datasets. The
experimental results demonstrate the superiority of DGCPN, e.g., by improving
the mean average precision from 0.722 to 0.751 on MIRFlickr-25K using 64-bit
hashing codes to retrieve texts from images. We will release the source code
package and the trained model on https://github.com/Atmegal/DGCPN.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/zheng2020dual/">Dual-path Convolutional Image-text Embeddings With Instance Loss</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Dual-path Convolutional Image-text Embeddings With Instance Loss' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Dual-path Convolutional Image-text Embeddings With Instance Loss' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zheng et al.</td> <!-- 🔧 You were missing this -->
    <td>ACM Transactions on Multimedia Computing, Communications, and Applications</td>
    <td>127</td>
    <td><p>Matching images and sentences demands a fine understanding of both
modalities. In this paper, we propose a new system to discriminatively embed
the image and text to a shared visual-textual space. In this field, most
existing works apply the ranking loss to pull the positive image / text pairs
close and push the negative pairs apart from each other. However, directly
deploying the ranking loss is hard for network learning, since it starts from
the two heterogeneous features to build inter-modal relationship. To address
this problem, we propose the instance loss which explicitly considers the
intra-modal data distribution. It is based on an unsupervised assumption that
each image / text group can be viewed as a class. So the network can learn the
fine granularity from every image/text group. The experiment shows that the
instance loss offers better weight initialization for the ranking loss, so that
more discriminative embeddings can be learned. Besides, existing works usually
apply the off-the-shelf features, i.e., word2vec and fixed visual feature. So
in a minor contribution, this paper constructs an end-to-end dual-path
convolutional network to learn the image and text representations. End-to-end
learning allows the system to directly learn from the data and fully utilize
the supervision. On two generic retrieval datasets (Flickr30k and MSCOCO),
experiments demonstrate that our method yields competitive accuracy compared to
state-of-the-art methods. Moreover, in language based person retrieval, we
improve the state of the art by a large margin. The code has been made publicly
available.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/wang2020cosea/">COSEA: Convolutional Code Search With Layer-wise Attention</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=COSEA: Convolutional Code Search With Layer-wise Attention' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=COSEA: Convolutional Code Search With Layer-wise Attention' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>Interspeech 2016</td>
    <td>48</td>
    <td><p>Semantic code search, which aims to retrieve code snippets relevant to a
given natural language query, has attracted many research efforts with the
purpose of accelerating software development. The huge amount of online
publicly available code repositories has prompted the employment of deep
learning techniques to build state-of-the-art code search models. Particularly,
they leverage deep neural networks to embed codes and queries into a unified
semantic vector space and then use the similarity between code’s and query’s
vectors to approximate the semantic correlation between code and the query.
However, most existing studies overlook the code’s intrinsic structural logic,
which indeed contains a wealth of semantic information, and fails to capture
intrinsic features of codes. In this paper, we propose a new deep learning
architecture, COSEA, which leverages convolutional neural networks with
layer-wise attention to capture the valuable code’s intrinsic structural logic.
To further increase the learning efficiency of COSEA, we propose a variant of
contrastive loss for training the code search model, where the ground-truth
code should be distinguished from the most similar negative sample. We have
implemented a prototype of COSEA. Extensive experiments over existing public
datasets of Python and SQL have demonstrated that COSEA can achieve significant
improvements over state-of-the-art methods on code search tasks.</p>
</td>
    <td>
      
        INTERSPEECH 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/yan2020deep/">Deep Multi-view Enhancement Hashing For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Multi-view Enhancement Hashing For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Multi-view Enhancement Hashing For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yan et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>413</td>
    <td><p>Hashing is an efficient method for nearest neighbor search in large-scale
data space by embedding high-dimensional feature descriptors into a similarity
preserving Hamming space with a low dimension. However, large-scale high-speed
retrieval through binary code has a certain degree of reduction in retrieval
accuracy compared to traditional retrieval methods. We have noticed that
multi-view methods can well preserve the diverse characteristics of data.
Therefore, we try to introduce the multi-view deep neural network into the hash
learning field, and design an efficient and innovative retrieval model, which
has achieved a significant improvement in retrieval performance. In this paper,
we propose a supervised multi-view hash model which can enhance the multi-view
information through neural networks. This is a completely new hash learning
method that combines multi-view and deep learning methods. The proposed method
utilizes an effective view stability evaluation method to actively explore the
relationship among views, which will affect the optimization direction of the
entire network. We have also designed a variety of multi-data fusion methods in
the Hamming space to preserve the advantages of both convolution and
multi-view. In order to avoid excessive computing resources on the enhancement
procedure during retrieval, we set up a separate structure called memory
network which participates in training together. The proposed method is
systematically evaluated on the CIFAR-10, NUS-WIDE and MS-COCO datasets, and
the results show that our method significantly outperforms the state-of-the-art
single-view and multi-view hashing methods.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/wang2020deep/">Deep Collaborative Discrete Hashing With Semantic-invariant Structure</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Collaborative Discrete Hashing With Semantic-invariant Structure' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Collaborative Discrete Hashing With Semantic-invariant Structure' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>39</td>
    <td><p>Existing deep hashing approaches fail to fully explore semantic correlations
and neglect the effect of linguistic context on visual attention learning,
leading to inferior performance. This paper proposes a dual-stream learning
framework, dubbed Deep Collaborative Discrete Hashing (DCDH), which constructs
a discriminative common discrete space by collaboratively incorporating the
shared and individual semantics deduced from visual features and semantic
labels. Specifically, the context-aware representations are generated by
employing the outer product of visual embeddings and semantic encodings.
Moreover, we reconstruct the labels and introduce the focal loss to take
advantage of frequent and rare concepts. The common binary code space is built
on the joint learning of the visual representations attended by language, the
semantic-invariant structure construction and the label distribution
correction. Extensive experiments demonstrate the superiority of our method.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/wang2020cross/">Cross-modal Scene Graph Matching For Relationship-aware Image-text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-modal Scene Graph Matching For Relationship-aware Image-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-modal Scene Graph Matching For Relationship-aware Image-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>219</td>
    <td><p>Image-text retrieval of natural scenes has been a popular research topic.
Since image and text are heterogeneous cross-modal data, one of the key
challenges is how to learn comprehensive yet unified representations to express
the multi-modal data. A natural scene image mainly involves two kinds of visual
concepts, objects and their relationships, which are equally essential to
image-text retrieval. Therefore, a good representation should account for both
of them. In the light of recent success of scene graph in many CV and NLP tasks
for describing complex natural scenes, we propose to represent image and text
with two kinds of scene graphs: visual scene graph (VSG) and textual scene
graph (TSG), each of which is exploited to jointly characterize objects and
relationships in the corresponding modality. The image-text retrieval task is
then naturally formulated as cross-modal scene graph matching. Specifically, we
design two particular scene graph encoders in our model for VSG and TSG, which
can refine the representation of each node on the graph by aggregating
neighborhood information. As a result, both object-level and relationship-level
cross-modal features can be obtained, which favorably enables us to evaluate
the similarity of image and text in the two levels in a more plausible way. We
achieve state-of-the-art results on Flickr30k and MSCOCO, which verifies the
advantages of our graph matching based approach for image-text retrieval.</p>
</td>
    <td>
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/dou2020learning/">Learning Global And Local Consistent Representations For Unsupervised Image Retrieval Via Deep Graph Diffusion Networks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Global And Local Consistent Representations For Unsupervised Image Retrieval Via Deep Graph Diffusion Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Global And Local Consistent Representations For Unsupervised Image Retrieval Via Deep Graph Diffusion Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dou et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>7</td>
    <td><p>Diffusion has shown great success in improving accuracy of unsupervised image
retrieval systems by utilizing high-order structures of image manifold.
However, existing diffusion methods suffer from three major limitations: 1)
they usually rely on local structures without considering global manifold
information; 2) they focus on improving pair-wise similarities within existing
images input output transductively while lacking flexibility to learn
representations for novel unseen instances inductively; 3) they fail to scale
to large datasets due to prohibitive memory consumption and computational
burden due to intrinsic high-order operations on the whole graph. In this
paper, to address these limitations, we propose a novel method, Graph Diffusion
Networks (GRAD-Net), that adopts graph neural networks (GNNs), a novel variant
of deep learning algorithms on irregular graphs. GRAD-Net learns semantic
representations by exploiting both local and global structures of image
manifold in an unsupervised fashion. By utilizing sparse coding techniques,
GRAD-Net not only preserves global information on the image manifold, but also
enables scalable training and efficient querying. Experiments on several large
benchmark datasets demonstrate effectiveness of our method over
state-of-the-art diffusion algorithms for unsupervised image retrieval.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Image Retrieval 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/elezi2020group/">The Group Loss For Deep Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=The Group Loss For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=The Group Loss For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Elezi et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>48</td>
    <td><p>Deep metric learning has yielded impressive results in tasks such as
clustering and image retrieval by leveraging neural networks to obtain highly
discriminative feature embeddings, which can be used to group samples into
different classes. Much research has been devoted to the design of smart loss
functions or data mining strategies for training such networks. Most methods
consider only pairs or triplets of samples within a mini-batch to compute the
loss function, which is commonly based on the distance between embeddings. We
propose Group Loss, a loss function based on a differentiable label-propagation
method that enforces embedding similarity across all samples of a group while
promoting, at the same time, low-density regions amongst data points belonging
to different groups. Guided by the smoothness assumption that “similar objects
should belong to the same group”, the proposed loss trains the neural network
for a classification task, enforcing a consistent labelling amongst samples
within a class. We show state-of-the-art results on clustering and image
retrieval on several datasets, and show the potential of our method when
combined with other techniques such as ensembles</p>
</td>
    <td>
      
        Distance Metric Learning 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/zhao2020stacked/">Stacked Convolutional Deep Encoding Network For Video-text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Stacked Convolutional Deep Encoding Network For Video-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Stacked Convolutional Deep Encoding Network For Video-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhao Rui, Zheng Kecheng, Zha Zheng-jun</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE International Conference on Multimedia and Expo (ICME)</td>
    <td>10</td>
    <td><p>Existing dominant approaches for cross-modal video-text retrieval task are to
learn a joint embedding space to measure the cross-modal similarity. However,
these methods rarely explore long-range dependency inside video frames or
textual words leading to insufficient textual and visual details. In this
paper, we propose a stacked convolutional deep encoding network for video-text
retrieval task, which considers to simultaneously encode long-range and
short-range dependency in the videos and texts. Specifically, a multi-scale
dilated convolutional (MSDC) block within our approach is able to encode
short-range temporal cues between video frames or text words by adopting
different scales of kernel size and dilation size of convolutional layer. A
stacked structure is designed to expand the receptive fields by repeatedly
adopting the MSDC block, which further captures the long-range relations
between these cues. Moreover, to obtain more robust textual representations, we
fully utilize the powerful language model named Transformer in two stages:
pretraining phrase and fine-tuning phrase. Extensive experiments on two
different benchmark datasets (MSR-VTT, MSVD) show that our proposed method
outperforms other state-of-the-art approaches.</p>
</td>
    <td>
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/yu2020retrieval/">Retrieval Of Family Members Using Siamese Neural Network</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Retrieval Of Family Members Using Siamese Neural Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Retrieval Of Family Members Using Siamese Neural Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yu et al.</td> <!-- 🔧 You were missing this -->
    <td>2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)</td>
    <td>13</td>
    <td><p>Retrieval of family members in the wild aims at finding family members of the
given subject in the dataset, which is useful in finding the lost children and
analyzing the kinship. However, due to the diversity in age, gender, pose and
illumination of the collected data, this task is always challenging. To solve
this problem, we propose our solution with deep Siamese neural network. Our
solution can be divided into two parts: similarity computation and ranking. In
training procedure, the Siamese network firstly takes two candidate images as
input and produces two feature vectors. And then, the similarity between the
two vectors is computed with several fully connected layers. While in inference
procedure, we try another similarity computing method by dropping the followed
several fully connected layers and directly computing the cosine similarity of
the two feature vectors. After similarity computation, we use the ranking
algorithm to merge the similarity scores with the same identity and output the
ordered list according to their similarities. To gain further improvement, we
try different combinations of backbones, training methods and similarity
computing methods. Finally, we submit the best combination as our solution and
our team(ustc-nelslip) obtains favorable result in the track3 of the RFIW2020
challenge with the first runner-up, which verifies the effectiveness of our
method. Our code is available at: https://github.com/gniknoil/FG2020-kinship</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/dolhansky2020adversarial/">Adversarial Collision Attacks On Image Hashing Functions</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Adversarial Collision Attacks On Image Hashing Functions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Adversarial Collision Attacks On Image Hashing Functions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dolhansky Brian, Ferrer Cristian Canton</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>9</td>
    <td><p>Hashing images with a perceptual algorithm is a common approach to solving
duplicate image detection problems. However, perceptual image hashing
algorithms are differentiable, and are thus vulnerable to gradient-based
adversarial attacks. We demonstrate that not only is it possible to modify an
image to produce an unrelated hash, but an exact image hash collision between a
source and target image can be produced via minuscule adversarial
perturbations. In a white box setting, these collisions can be replicated
across nearly every image pair and hash type (including both deep and
non-learned hashes). Furthermore, by attacking points other than the output of
a hashing function, an attacker can avoid having to know the details of a
particular algorithm, resulting in collisions that transfer across different
hash sizes or model architectures. Using these techniques, an adversary can
poison the image lookup table of a duplicate image detection service, resulting
in undefined or unwanted behavior. Finally, we offer several potential
mitigations to gradient-based image hash attacks.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Robustness 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/dong2020learning/">Learning Space Partitions For Nearest Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Space Partitions For Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Space Partitions For Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dong et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>26</td>
    <td><p>Space partitions of \(\mathbb{R}^d\) underlie a vast and important class of
fast nearest neighbor search (NNS) algorithms. Inspired by recent theoretical
work on NNS for general metric spaces [Andoni, Naor, Nikolov, Razenshteyn,
Waingarten STOC 2018, FOCS 2018], we develop a new framework for building space
partitions reducing the problem to balanced graph partitioning followed by
supervised classification. We instantiate this general approach with the KaHIP
graph partitioner [Sanders, Schulz SEA 2013] and neural networks, respectively,
to obtain a new partitioning procedure called Neural Locality-Sensitive Hashing
(Neural LSH). On several standard benchmarks for NNS, our experiments show that
the partitions obtained by Neural LSH consistently outperform partitions found
by quantization-based and tree-based methods as well as classic, data-oblivious
LSH.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/wang2020online/">Online Collective Matrix Factorization Hashing For Large-scale Cross-media Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Online Collective Matrix Factorization Hashing For Large-scale Cross-media Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Online Collective Matrix Factorization Hashing For Large-scale Cross-media Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>47</td>
    <td><p>Cross-modal hashing has been widely investigated recently for its efficiency in large-scale cross-media retrieval. However, most existing cross-modal hashing methods learn hash functions in a batch-based learning mode. Such mode is not suitable for large-scale data sets due to the large memory consumption and loses its efficiency when training streaming data. Online cross-modal hashing can deal with the above problems by learning hash model in an online learning process. However, existing online cross-modal hashing methods cannot update hash codes of old data by the newly learned model. In this paper, we propose Online Collective Matrix Factorization Hashing (OCMFH) based on collective matrix factorization hashing (CMFH), which can adaptively update hash codes of old data according to dynamic changes of hash model without accessing to old data. Specifically, it learns discriminative hash codes for streaming data by collective matrix factorization in an online optimization scheme. Unlike conventional CMFH which needs to load the entire data points into memory, the proposed OCMFH retrains hash functions only by newly arriving data points. Meanwhile, it generates hash codes of new data and updates hash codes of old data by the latest updated hash model. In such way, hash codes of new data and old data are well-matched. Furthermore, a zero mean strategy is developed to solve the mean-varying problem in the online hash learning process. Extensive experiments on three benchmark data sets demonstrate the effectiveness and efficiency of OCMFH on online cross-media retrieval.</p>
</td>
    <td>
      
        Text Retrieval 
      
        Medical Retrieval 
      
        Multimodal Retrieval 
      
        SIGIR 
      
        Hashing Methods 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/shi2020scalable/">A Scalable Optimization Mechanism For Pairwise Based Discrete Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Scalable Optimization Mechanism For Pairwise Based Discrete Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Scalable Optimization Mechanism For Pairwise Based Discrete Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shi et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>11</td>
    <td><p>Maintaining the pair similarity relationship among originally
high-dimensional data into a low-dimensional binary space is a popular strategy
to learn binary codes. One simiple and intutive method is to utilize two
identical code matrices produced by hash functions to approximate a pairwise
real label matrix. However, the resulting quartic problem is difficult to
directly solve due to the non-convex and non-smooth nature of the objective. In
this paper, unlike previous optimization methods using various relaxation
strategies, we aim to directly solve the original quartic problem using a novel
alternative optimization mechanism to linearize the quartic problem by
introducing a linear regression model. Additionally, we find that gradually
learning each batch of binary codes in a sequential mode, i.e. batch by batch,
is greatly beneficial to the convergence of binary code learning. Based on this
significant discovery and the proposed strategy, we introduce a scalable
symmetric discrete hashing algorithm that gradually and smoothly updates each
batch of binary codes. To further improve the smoothness, we also propose a
greedy symmetric discrete hashing algorithm to update each bit of batch binary
codes. Moreover, we extend the proposed optimization mechanism to solve the
non-convex optimization problems for binary code learning in many other
pairwise based hashing algorithms. Extensive experiments on benchmark
single-label and multi-label databases demonstrate the superior performance of
the proposed mechanism over recent state-of-the-art methods.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/dirksen2020binarized/">Binarized Johnson-lindenstrauss Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Binarized Johnson-lindenstrauss Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Binarized Johnson-lindenstrauss Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dirksen Sjoerd, Stollenwerk Alexander</td> <!-- 🔧 You were missing this -->
    <td>SIAM Journal on Matrix Analysis and Applications</td>
    <td>7</td>
    <td><p>We consider the problem of encoding a set of vectors into a minimal number of
bits while preserving information on their Euclidean geometry. We show that
this task can be accomplished by applying a Johnson-Lindenstrauss embedding and
subsequently binarizing each vector by comparing each entry of the vector to a
uniformly random threshold. Using this simple construction we produce two
encodings of a dataset such that one can query Euclidean information for a pair
of points using a small number of bit operations up to a desired additive error</p>
<ul>
  <li>Euclidean distances in the first case and inner products and squared
Euclidean distances in the second. In the latter case, each point is encoded in
near-linear time. The number of bits required for these encodings is quantified
in terms of two natural complexity parameters of the dataset - its covering
numbers and localized Gaussian complexity - and shown to be near-optimal.</li>
</ul>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/wang2020cluster/">Cluster-wise Unsupervised Hashing For Cross-modal Similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cluster-wise Unsupervised Hashing For Cross-modal Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cluster-wise Unsupervised Hashing For Cross-modal Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Lu, Yang Jie</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition</td>
    <td>25</td>
    <td><p>Large-scale cross-modal hashing similarity retrieval has attracted more and
more attention in modern search applications such as search engines and
autopilot, showing great superiority in computation and storage. However,
current unsupervised cross-modal hashing methods still have some limitations:
(1)many methods relax the discrete constraints to solve the optimization
objective which may significantly degrade the retrieval performance;(2)most
existing hashing model project heterogenous data into a common latent space,
which may always lose sight of diversity in heterogenous data;(3)transforming
real-valued data point to binary codes always results in abundant loss of
information, producing the suboptimal continuous latent space. To overcome
above problems, in this paper, a novel Cluster-wise Unsupervised Hashing (CUH)
method is proposed. Specifically, CUH jointly performs the multi-view
clustering that projects the original data points from different modalities
into its own low-dimensional latent semantic space and finds the cluster
centroid points and the common clustering indicators in its own low-dimensional
space, and learns the compact hash codes and the corresponding linear hash
functions. An discrete optimization framework is developed to learn the unified
binary codes across modalities under the guidance cluster-wise code-prototypes.
The reasonableness and effectiveness of CUH is well demonstrated by
comprehensive experiments on diverse benchmark datasets.</p>
</td>
    <td>
      
        CVPR 
      
        Unsupervised 
      
        Neural Hashing 
      
        SUPERVISED 
      
        Similarity Search 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/daras2020smyrf/">SMYRF: Efficient Attention Using Asymmetric Clustering</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=SMYRF: Efficient Attention Using Asymmetric Clustering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=SMYRF: Efficient Attention Using Asymmetric Clustering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Daras et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>12</td>
    <td><p>We propose a novel type of balanced clustering algorithm to approximate
attention. Attention complexity is reduced from \(O(N^2)\) to \(O(N log N)\),
where \(N\) is the sequence length. Our algorithm, SMYRF, uses Locality Sensitive
Hashing (LSH) in a novel way by defining new Asymmetric transformations and an
adaptive scheme that produces balanced clusters. The biggest advantage of SMYRF
is that it can be used as a drop-in replacement for dense attention layers
without any retraining. On the contrary, prior fast attention methods impose
constraints (e.g. queries and keys share the same vector representations) and
require re-training from scratch. We apply our method to pre-trained
state-of-the-art Natural Language Processing and Computer Vision models and we
report significant memory and speed benefits. Notably, SMYRF-BERT outperforms
(slightly) BERT on GLUE, while using \(50%\) less memory. We also show that
SMYRF can be used interchangeably with dense attention before and after
training. Finally, we use SMYRF to train GANs with attention in high
resolutions. Using a single TPU, we were able to scale attention to 128x128=16k
and 256x256=65k tokens on BigGAN on CelebA-HQ.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/dai2020convolutional/">Convolutional Embedding For Edit Distance</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Convolutional Embedding For Edit Distance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Convolutional Embedding For Edit Distance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dai et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>7</td>
    <td><p>Edit-distance-based string similarity search has many applications such as
spell correction, data de-duplication, and sequence alignment. However,
computing edit distance is known to have high complexity, which makes string
similarity search challenging for large datasets. In this paper, we propose a
deep learning pipeline (called CNN-ED) that embeds edit distance into Euclidean
distance for fast approximate similarity search. A convolutional neural network
(CNN) is used to generate fixed-length vector embeddings for a dataset of
strings and the loss function is a combination of the triplet loss and the
approximation error. To justify our choice of using CNN instead of other
structures (e.g., RNN) as the model, theoretical analysis is conducted to show
that some basic operations in our CNN model preserve edit distance.
Experimental results show that CNN-ED outperforms data-independent CGK
embedding and RNN-based GRU embedding in terms of both accuracy and efficiency
by a large margin. We also show that string similarity search can be
significantly accelerated using CNN-based embeddings, sometimes by orders of
magnitude.</p>
</td>
    <td>
      
        SIGIR 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/wu2020scalable/">Scalable Zero-shot Entity Linking With Dense Entity Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Scalable Zero-shot Entity Linking With Dense Entity Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Scalable Zero-shot Entity Linking With Dense Entity Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</td>
    <td>290</td>
    <td><p>This paper introduces a conceptually simple, scalable, and highly effective
BERT-based entity linking model, along with an extensive evaluation of its
accuracy-speed trade-off. We present a two-stage zero-shot linking algorithm,
where each entity is defined only by a short textual description. The first
stage does retrieval in a dense space defined by a bi-encoder that
independently embeds the mention context and the entity descriptions. Each
candidate is then re-ranked with a cross-encoder, that concatenates the mention
and entity text. Experiments demonstrate that this approach is state of the art
on recent zero-shot benchmarks (6 point absolute gains) and also on more
established non-zero-shot evaluations (e.g. TACKBP-2010), despite its relative
simplicity (e.g. no explicit entity embeddings or manually engineered mention
tables). We also show that bi-encoder linking is very fast with nearest
neighbour search (e.g. linking with 5.9 million candidates in 2 milliseconds),
and that much of the accuracy gain from the more expensive cross-encoder can be
transferred to the bi-encoder via knowledge distillation. Our code and models
are available at https://github.com/facebookresearch/BLINK.</p>
</td>
    <td>
      
        EMNLP 
      
        Few Shot & Zero Shot 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/dadaneh2020pairwise/">Pairwise Supervised Hashing With Bernoulli Variational Auto-encoder And Self-control Gradient Estimator</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Pairwise Supervised Hashing With Bernoulli Variational Auto-encoder And Self-control Gradient Estimator' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Pairwise Supervised Hashing With Bernoulli Variational Auto-encoder And Self-control Gradient Estimator' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dadaneh et al.</td> <!-- 🔧 You were missing this -->
    <td>Uncertainty in Artificial Intelligence Conference (UAI) 2020</td>
    <td>11</td>
    <td><p>Semantic hashing has become a crucial component of fast similarity search in
many large-scale information retrieval systems, in particular, for text data.
Variational auto-encoders (VAEs) with binary latent variables as hashing codes
provide state-of-the-art performance in terms of precision for document
retrieval. We propose a pairwise loss function with discrete latent VAE to
reward within-class similarity and between-class dissimilarity for supervised
hashing. Instead of solving the optimization relying on existing biased
gradient estimators, an unbiased low-variance gradient estimator is adopted to
optimize the hashing function by evaluating the non-differentiable loss
function over two correlated sets of binary hashing codes to control the
variance of gradient estimates. This new semantic hashing framework achieves
superior performance compared to the state-of-the-arts, as demonstrated by our
comprehensive experiments.</p>
</td>
    <td>
      
        Unsupervised 
      
        Neural Hashing 
      
        Hashing Methods 
      
        SUPERVISED 
      
        UAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/cui2020exchnet/">Exchnet: A Unified Hashing Network For Large-scale Fine-grained Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Exchnet: A Unified Hashing Network For Large-scale Fine-grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Exchnet: A Unified Hashing Network For Large-scale Fine-grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cui et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>31</td>
    <td><p>Retrieving content relevant images from a large-scale fine-grained dataset
could suffer from intolerably slow query speed and highly redundant storage
cost, due to high-dimensional real-valued embeddings which aim to distinguish
subtle visual differences of fine-grained objects. In this paper, we study the
novel fine-grained hashing topic to generate compact binary codes for
fine-grained images, leveraging the search and storage efficiency of hash
learning to alleviate the aforementioned problems. Specifically, we propose a
unified end-to-end trainable network, termed as ExchNet. Based on attention
mechanisms and proposed attention constraints, it can firstly obtain both local
and global features to represent object parts and whole fine-grained objects,
respectively. Furthermore, to ensure the discriminative ability and semantic
meaning’s consistency of these part-level features across images, we design a
local feature alignment approach by performing a feature exchanging operation.
Later, an alternative learning algorithm is employed to optimize the whole
ExchNet and then generate the final binary hash codes. Validated by extensive
experiments, our proposal consistently outperforms state-of-the-art generic
hashing methods on five fine-grained datasets, which shows our effectiveness.
Moreover, compared with other approximate nearest neighbor methods, ExchNet
achieves the best speed-up and storage reduction, revealing its efficiency and
practicality.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/christiani2020dartminhash/">Dartminhash: Fast Sketching For Weighted Sets</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Dartminhash: Fast Sketching For Weighted Sets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Dartminhash: Fast Sketching For Weighted Sets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Christiani Tobias</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS)</td>
    <td>16</td>
    <td><p>Weighted minwise hashing is a standard dimensionality reduction technique
with applications to similarity search and large-scale kernel machines. We
introduce a simple algorithm that takes a weighted set \(x \in \mathbb{R}<em>{\geq
0}^{d}\) and computes \(k\) independent minhashes in expected time \(O(k log k +
\Vert x \Vert</em>{0}log( \Vert x \Vert_1 + 1/\Vert x \Vert_1))\), improving upon
the state-of-the-art BagMinHash algorithm (KDD ‘18) and representing the
fastest weighted minhash algorithm for sparse data. Our experiments show
running times that scale better with \(k\) and \(\Vert x \Vert_0\) compared to ICWS
(ICDM ‘10) and BagMinhash, obtaining \(10\)x speedups in common use cases. Our
approach also gives rise to a technique for computing fully independent
locality-sensitive hash values for \((L, K)\)-parameterized approximate near
neighbor search under weighted Jaccard similarity in optimal expected time
\(O(LK + \Vert x \Vert_0)\), improving on prior work even in the case of
unweighted sets.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/cheng2020robust/">Robust Unsupervised Cross-modal Hashing For Multimedia Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Robust Unsupervised Cross-modal Hashing For Multimedia Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Robust Unsupervised Cross-modal Hashing For Multimedia Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cheng Miaomiao, Jing, Ng</td> <!-- 🔧 You were missing this -->
    <td>ACM Transactions on Information Systems</td>
    <td>46</td>
    <td><p>With the quick development of social websites, there are more opportunities to have different media types (such as text, image, video, etc.) describing the same topic from large-scale heterogeneous data sources. To efficiently identify the inter-media correlations for multimedia retrieval, unsupervised cross-modal hashing (UCMH) has gained increased interest due to the significant reduction in computation and storage. However, most UCMH methods assume that the data from different modalities are well paired. As a result, existing UCMH methods may not achieve satisfactory performance when partially paired data are given only. In this article, we propose a new-type of UCMH method called robust unsupervised cross-modal hashing (RUCMH). The major contribution lies in jointly learning modal-specific hash function, exploring the correlations among modalities with partial or even without any pairwise correspondence, and preserving the information of original features as much as possible. The learning process can be modeled via a joint minimization problem, and the corresponding optimization algorithm is presented. A series of experiments is conducted on four real-world datasets (Wiki, MIRFlickr, NUS-WIDE, and MS-COCO). The results demonstrate that RUCMH can significantly outperform the state-of-the-art unsupervised cross-modal hashing methods, especially for the partially paired case, which validates the effectiveness of RUCMH.</p>
</td>
    <td>
      
        Unsupervised 
      
        Medical Retrieval 
      
        SUPERVISED 
      
        Multimodal Retrieval 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/shen2020auto/">Auto-encoding Twin-bottleneck Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Auto-encoding Twin-bottleneck Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Auto-encoding Twin-bottleneck Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shen et al.</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>113</td>
    <td><p>Conventional unsupervised hashing methods usually take advantage of
similarity graphs, which are either pre-computed in the high-dimensional space
or obtained from random anchor points. On the one hand, existing methods
uncouple the procedures of hash function learning and graph construction. On
the other hand, graphs empirically built upon original data could introduce
biased prior knowledge of data relevance, leading to sub-optimal retrieval
performance. In this paper, we tackle the above problems by proposing an
efficient and adaptive code-driven graph, which is updated by decoding in the
context of an auto-encoder. Specifically, we introduce into our framework twin
bottlenecks (i.e., latent variables) that exchange crucial information
collaboratively. One bottleneck (i.e., binary codes) conveys the high-level
intrinsic data structure captured by the code-driven graph to the other (i.e.,
continuous variables for low-level detail information), which in turn
propagates the updated network feedback for the encoder to learn more
discriminative binary codes. The auto-encoding learning objective literally
rewards the code-driven graph to learn an optimal encoder. Moreover, the
proposed model can be simply optimized by gradient descent without violating
the binary constraints. Experiments on benchmarked datasets clearly show the
superiority of our framework over the state-of-the-art hashing methods. Our
source code can be found at https://github.com/ymcidence/TBH.</p>
</td>
    <td>
      
        Hashing Methods 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/thomas2020preserving/">Preserving Semantic Neighborhoods For Robust Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Preserving Semantic Neighborhoods For Robust Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Preserving Semantic Neighborhoods For Robust Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Thomas Christopher, Kovashka Adriana</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>27</td>
    <td><p>The abundance of multimodal data (e.g. social media posts) has inspired
interest in cross-modal retrieval methods. Popular approaches rely on a variety
of metric learning losses, which prescribe what the proximity of image and text
should be, in the learned space. However, most prior methods have focused on
the case where image and text convey redundant information; in contrast,
real-world image-text pairs convey complementary information with little
overlap. Further, images in news articles and media portray topics in a
visually diverse fashion; thus, we need to take special care to ensure a
meaningful image representation. We propose novel within-modality losses which
encourage semantic coherency in both the text and image subspaces, which does
not necessarily align with visual coherency. Our method ensures that not only
are paired images and texts close, but the expected image-image and text-text
relationships are also observed. Our approach improves the results of
cross-modal retrieval on four datasets compared to five baselines.</p>
</td>
    <td>
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/yuan2020central/">Central Similarity Quantization For Efficient Image And Video Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Central Similarity Quantization For Efficient Image And Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Central Similarity Quantization For Efficient Image And Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yuan et al.</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>288</td>
    <td><p>Existing data-dependent hashing methods usually learn hash functions from
pairwise or triplet data relationships, which only capture the data similarity
locally, and often suffer from low learning efficiency and low collision rate.
In this work, we propose a new <em>global</em> similarity metric, termed as
<em>central similarity</em>, with which the hash codes of similar data pairs are
encouraged to approach a common center and those for dissimilar pairs to
converge to different centers, to improve hash learning efficiency and
retrieval accuracy. We principally formulate the computation of the proposed
central similarity metric by introducing a new concept, i.e., <em>hash
center</em> that refers to a set of data points scattered in the Hamming space with
a sufficient mutual distance between each other. We then provide an efficient
method to construct well separated hash centers by leveraging the Hadamard
matrix and Bernoulli distributions. Finally, we propose the Central Similarity
Quantization (CSQ) that optimizes the central similarity between data points
w.r.t.\ their hash centers instead of optimizing the local similarity. CSQ is
generic and applicable to both image and video hashing scenarios. Extensive
experiments on large-scale image and video retrieval tasks demonstrate that CSQ
can generate cohesive hash codes for similar data pairs and dispersed hash
codes for dissimilar pairs, achieving a noticeable boost in retrieval
performance, i.e. 3%-20% in mAP over the previous state-of-the-arts. The code
is at: https://github.com/yuanli2333/Hadamard-Matrix-for-hashing</p>
</td>
    <td>
      
        Quantization 
      
        Video Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/wang2020faster/">Faster Person Re-identification</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Faster Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Faster Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>54</td>
    <td><p>Fast person re-identification (ReID) aims to search person images quickly and
accurately. The main idea of recent fast ReID methods is the hashing algorithm,
which learns compact binary codes and performs fast Hamming distance and
counting sort. However, a very long code is needed for high accuracy (e.g.
2048), which compromises search speed. In this work, we introduce a new
solution for fast ReID by formulating a novel Coarse-to-Fine (CtF) hashing code
search strategy, which complementarily uses short and long codes, achieving
both faster speed and better accuracy. It uses shorter codes to coarsely rank
broad matching similarities and longer codes to refine only a few top
candidates for more accurate instance ReID. Specifically, we design an
All-in-One (AiO) framework together with a Distance Threshold Optimization
(DTO) algorithm. In AiO, we simultaneously learn and enhance multiple codes of
different lengths in a single model. It learns multiple codes in a pyramid
structure, and encourage shorter codes to mimic longer codes by
self-distillation. DTO solves a complex threshold search problem by a simple
optimization process, and the balance between accuracy and speed is easily
controlled by a single parameter. It formulates the optimization target as a
\(F_{\beta}\) score that can be optimised by Gaussian cumulative distribution
functions. Experimental results on 2 datasets show that our proposed method
(CtF) is not only 8% more accurate but also 5x faster than contemporary hashing
ReID methods. Compared with non-hashing ReID methods, CtF is \(50\times\) faster
with comparable accuracy. Code is available at
https://github.com/wangguanan/light-reid.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/singh2020ihashnet/">Ihashnet: Iris Hashing Network Based On Efficient Multi-index Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Ihashnet: Iris Hashing Network Based On Efficient Multi-index Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Ihashnet: Iris Hashing Network Based On Efficient Multi-index Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Singh et al.</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE International Joint Conference on Biometrics (IJCB)</td>
    <td>5</td>
    <td><p>Massive biometric deployments are pervasive in today’s world. But despite the
high accuracy of biometric systems, their computational efficiency degrades
drastically with an increase in the database size. Thus, it is essential to
index them. An ideal indexing scheme needs to generate codes that preserve the
intra-subject similarity as well as inter-subject dissimilarity. Here, in this
paper, we propose an iris indexing scheme using real-valued deep iris features
binarized to iris bar codes (IBC) compatible with the indexing structure.
Firstly, for extracting robust iris features, we have designed a network
utilizing the domain knowledge of ordinal filtering and learning their
nonlinear combinations. Later these real-valued features are binarized.
Finally, for indexing the iris dataset, we have proposed a loss that can
transform the binary feature into an improved feature compatible with the
Multi-Index Hashing scheme. This loss function ensures the hamming distance
equally distributed among all the contiguous disjoint sub-strings. To the best
of our knowledge, this is the first work in the iris indexing domain that
presents an end-to-end iris indexing structure. Experimental results on four
datasets are presented to depict the efficacy of the proposed approach.</p>
</td>
    <td>
      
        Vector Indexing 
      
        Hashing Methods 
      
        Neural Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/chen2020imram/">IMRAM: Iterative Matching With Recurrent Attention Memory For Cross-modal Image-text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=IMRAM: Iterative Matching With Recurrent Attention Memory For Cross-modal Image-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=IMRAM: Iterative Matching With Recurrent Attention Memory For Cross-modal Image-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen et al.</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>337</td>
    <td><p>Enabling bi-directional retrieval of images and texts is important for
understanding the correspondence between vision and language. Existing methods
leverage the attention mechanism to explore such correspondence in a
fine-grained manner. However, most of them consider all semantics equally and
thus align them uniformly, regardless of their diverse complexities. In fact,
semantics are diverse (i.e. involving different kinds of semantic concepts),
and humans usually follow a latent structure to combine them into
understandable languages. It may be difficult to optimally capture such
sophisticated correspondences in existing methods. In this paper, to address
such a deficiency, we propose an Iterative Matching with Recurrent Attention
Memory (IMRAM) method, in which correspondences between images and texts are
captured with multiple steps of alignments. Specifically, we introduce an
iterative matching scheme to explore such fine-grained correspondence
progressively. A memory distillation unit is used to refine alignment knowledge
from early steps to later ones. Experiment results on three benchmark datasets,
i.e. Flickr8K, Flickr30K, and MS COCO, show that our IMRAM achieves
state-of-the-art performance, well demonstrating its effectiveness. Experiments
on a practical business advertisement dataset, named \Ads{}, further validates
the applicability of our method in practical scenarios.</p>
</td>
    <td>
      
        Text Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/chen2020expressing/">Expressing Objects Just Like Words: Recurrent Visual Embedding For Image-text Matching</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Expressing Objects Just Like Words: Recurrent Visual Embedding For Image-text Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Expressing Objects Just Like Words: Recurrent Visual Embedding For Image-text Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen Tianlang, Luo Jiebo</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>58</td>
    <td><p>Existing image-text matching approaches typically infer the similarity of an
image-text pair by capturing and aggregating the affinities between the text
and each independent object of the image. However, they ignore the connections
between the objects that are semantically related. These objects may
collectively determine whether the image corresponds to a text or not. To
address this problem, we propose a Dual Path Recurrent Neural Network (DP-RNN)
which processes images and sentences symmetrically by recurrent neural networks
(RNN). In particular, given an input image-text pair, our model reorders the
image objects based on the positions of their most related words in the text.
In the same way as extracting the hidden features from word embeddings, the
model leverages RNN to extract high-level object features from the reordered
object inputs. We validate that the high-level object features contain useful
joint information of semantically related objects, which benefit the retrieval
task. To compute the image-text similarity, we incorporate a Multi-attention
Cross Matching Model into DP-RNN. It aggregates the affinity between objects
and words with cross-modality guided attention and self-attention. Our model
achieves the state-of-the-art performance on Flickr30K dataset and competitive
performance on MS-COCO dataset. Extensive experiments demonstrate the
effectiveness of our model.</p>
</td>
    <td>
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/chen2020enhanced/">Enhanced Discrete Multi-modal Hashing: More Constraints Yet Less Time To Learn</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Enhanced Discrete Multi-modal Hashing: More Constraints Yet Less Time To Learn' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Enhanced Discrete Multi-modal Hashing: More Constraints Yet Less Time To Learn' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Knowledge and Data Engineering</td>
    <td>27</td>
    <td><p>Due to the exponential growth of multimedia data, multi-modal hashing as a promising technique to make cross-view retrieval scalable is attracting more and more attention. However, most of the existing multi-modal hashing methods either divide the learning process unnaturally into two separate stages or treat the discrete optimization problem simplistically as a continuous one, which leads to suboptimal results. Recently, a few discrete multi-modal hashing methods that try to address such issues have emerged, but they still ignore several important discrete constraints (such as the balance and decorrelation of hash bits). In this paper, we overcome those limitations by proposing a novel method named “Enhanced Discrete Multi-modal Hashing (EDMH)” which learns binary codes and hashing functions simultaneously from the pairwise similarity matrix of data, under the aforementioned discrete constraints. Although the model of EDMH looks a lot more complex than the other models for multi-modal hashing, we are actually able to develop a fast iterative learning algorithm for it, since the subproblems of its optimization all have closed-form solutions after introducing two auxiliary variables. Our experimental results on three real-world datasets have demonstrated that EDMH not only performs much better than state-of-the-art competitors but also runs much faster than them.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/chen2020fine/">Fine-grained Video-text Retrieval With Hierarchical Graph Reasoning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fine-grained Video-text Retrieval With Hierarchical Graph Reasoning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fine-grained Video-text Retrieval With Hierarchical Graph Reasoning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen et al.</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>272</td>
    <td><p>Cross-modal retrieval between videos and texts has attracted growing
attentions due to the rapid emergence of videos on the web. The current
dominant approach for this problem is to learn a joint embedding space to
measure cross-modal similarities. However, simple joint embeddings are
insufficient to represent complicated visual and textual details, such as
scenes, objects, actions and their compositions. To improve fine-grained
video-text retrieval, we propose a Hierarchical Graph Reasoning (HGR) model,
which decomposes video-text matching into global-to-local levels. To be
specific, the model disentangles texts into hierarchical semantic graph
including three levels of events, actions, entities and relationships across
levels. Attention-based graph reasoning is utilized to generate hierarchical
textual embeddings, which can guide the learning of diverse and hierarchical
video representations. The HGR model aggregates matchings from different
video-text levels to capture both global and local details. Experimental
results on three video-text datasets demonstrate the advantages of our model.
Such hierarchical decomposition also enables better generalization across
datasets and improves the ability to distinguish fine-grained semantic
differences.</p>
</td>
    <td>
      
        Text Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/chen2020strongly/">Strongly Constrained Discrete Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Strongly Constrained Discrete Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Strongly Constrained Discrete Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>50</td>
    <td><p>Learning to hash is a fundamental technique widely used in large-scale image retrieval. Most existing methods for learning to hash address the involved discrete optimization problem by the continuous relaxation of the binary constraint, which usually leads to large quantization errors and consequently suboptimal binary codes. A few discrete hashing methods have emerged recently. However, they either completely ignore some useful constraints (specifically the balance and decorrelation of hash bits) or just turn those constraints into regularizers that would make the optimization easier but less accurate. In this paper, we propose a novel supervised hashing method named Strongly Constrained Discrete Hashing (SCDH) which overcomes such limitations. It can learn the binary codes for all examples in the training set, and meanwhile obtain a hash function for unseen samples with the above mentioned constraints preserved. Although the model of SCDH is fairly sophisticated, we are able to find closed-form solutions to all of its optimization subproblems and thus design an efficient algorithm that converges quickly. In addition, we extend SCDH to a kernelized version SCDH K . Our experiments on three large benchmark datasets have demonstrated that not only can SCDH and SCDH K achieve substantially higher MAP scores than state-of-the-art baselines, but they train much faster than those that are also supervised as well.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/chaudhuri2020crossatnet/">Crossatnet - A Novel Cross-attention Based Framework For Sketch-based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Crossatnet - A Novel Cross-attention Based Framework For Sketch-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Crossatnet - A Novel Cross-attention Based Framework For Sketch-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chaudhuri et al.</td> <!-- 🔧 You were missing this -->
    <td>Image and Vision Computing</td>
    <td>28</td>
    <td><p>We propose a novel framework for cross-modal zero-shot learning (ZSL) in the
context of sketch-based image retrieval (SBIR). Conventionally, the SBIR schema
mainly considers simultaneous mappings among the two image views and the
semantic side information. Therefore, it is desirable to consider fine-grained
classes mainly in the sketch domain using highly discriminative and
semantically rich feature space. However, the existing deep generative
modeling-based SBIR approaches majorly focus on bridging the gaps between the
seen and unseen classes by generating pseudo-unseen-class samples. Besides,
violating the ZSL protocol by not utilizing any unseen-class information during
training, such techniques do not pay explicit attention to modeling the
discriminative nature of the shared space. Also, we note that learning a
unified feature space for both the multi-view visual data is a tedious task
considering the significant domain difference between sketches and color
images. In this respect, as a remedy, we introduce a novel framework for
zero-shot SBIR. While we define a cross-modal triplet loss to ensure the
discriminative nature of the shared space, an innovative cross-modal attention
learning strategy is also proposed to guide feature extraction from the image
domain exploiting information from the respective sketch counterpart. In order
to preserve the semantic consistency of the shared space, we consider a graph
CNN-based module that propagates the semantic class topology to the shared
space. To ensure an improved response time during inference, we further explore
the possibility of representing the shared space in terms of hash codes.
Experimental results obtained on the benchmark TU-Berlin and the Sketchy
datasets confirm the superiority of CrossATNet in yielding state-of-the-art
results.</p>
</td>
    <td>
      
        Tools & Libraries 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/song2020deep/">Deep Hashing Learning For Visual And Semantic Retrieval Of Remote Sensing Images</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Hashing Learning For Visual And Semantic Retrieval Of Remote Sensing Images' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Hashing Learning For Visual And Semantic Retrieval Of Remote Sensing Images' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Song Weiwei, Li Shutao, Benediktsson Jon Atli</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Geoscience and Remote Sensing</td>
    <td>59</td>
    <td><p>Driven by the urgent demand for managing remote sensing big data, large-scale
remote sensing image retrieval (RSIR) attracts increasing attention in the
remote sensing field. In general, existing retrieval methods can be regarded as
visual-based retrieval approaches which search and return a set of similar
images from a database to a given query image. Although retrieval methods have
achieved great success, there is still a question that needs to be responded
to: Can we obtain the accurate semantic labels of the returned similar images
to further help analyzing and processing imagery? Inspired by the above
question, in this paper, we redefine the image retrieval problem as visual and
semantic retrieval of images. Specifically, we propose a novel deep hashing
convolutional neural network (DHCNN) to simultaneously retrieve the similar
images and classify their semantic labels in a unified framework. In more
detail, a convolutional neural network (CNN) is used to extract
high-dimensional deep features. Then, a hash layer is perfectly inserted into
the network to transfer the deep features into compact hash codes. In addition,
a fully connected layer with a softmax function is performed on hash layer to
generate class distribution. Finally, a loss function is elaborately designed
to simultaneously consider the label loss of each image and similarity loss of
pairs of images. Experimental results on two remote sensing datasets
demonstrate that the proposed method achieves the state-of-art retrieval and
classification performance.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/chakraborty2020conlsh/">Conlsh: Context Based Locality Sensitive Hashing For Mapping Of Noisy SMRT Reads</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Conlsh: Context Based Locality Sensitive Hashing For Mapping Of Noisy SMRT Reads' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Conlsh: Context Based Locality Sensitive Hashing For Mapping Of Noisy SMRT Reads' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chakraborty Angana, Bandyopadhyay Sanghamitra</td> <!-- 🔧 You were missing this -->
    <td>Computational Biology and Chemistry</td>
    <td>13</td>
    <td><p>Single Molecule Real-Time (SMRT) sequencing is a recent advancement of Next
Gen technology developed by Pacific Bio (PacBio). It comes with an explosion of
long and noisy reads demanding cutting edge research to get most out of it. To
deal with the high error probability of SMRT data, a novel contextual Locality
Sensitive Hashing (conLSH) based algorithm is proposed in this article, which
can effectively align the noisy SMRT reads to the reference genome. Here,
sequences are hashed together based not only on their closeness, but also on
similarity of context. The algorithm has \(\mathcal{O}(n^{\rho+1})\) space
requirement, where \(n\) is the number of sequences in the corpus and \(\rho\) is a
constant. The indexing time and querying time are bounded by \(\mathcal{O}(
\frac{n^{\rho+1} \cdot \ln n}{\ln \frac{1}{P_2}})\) and \(\mathcal{O}(n^\rho)\)
respectively, where \(P_2 &gt; 0\), is a probability value. This algorithm is
particularly useful for retrieving similar sequences, a widely used task in
biology. The proposed conLSH based aligner is compared with rHAT, popularly
used for aligning SMRT reads, and is found to comprehensively beat it in speed
as well as in memory requirements. In particular, it takes approximately
\(24.2%\) less processing time, while saving about \(70.3%\) in peak memory
requirement for H.sapiens PacBio dataset.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Hashing Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/cao2020unifying/">Unifying Deep Local And Global Features For Image Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unifying Deep Local And Global Features For Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unifying Deep Local And Global Features For Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cao Bingyi, Araujo Andre, Sim Jack</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>291</td>
    <td><p>Image retrieval is the problem of searching an image database for items that
are similar to a query image. To address this task, two main types of image
representations have been studied: global and local image features. In this
work, our key contribution is to unify global and local features into a single
deep model, enabling accurate retrieval with efficient feature extraction. We
refer to the new model as DELG, standing for DEep Local and Global features. We
leverage lessons from recent feature learning work and propose a model that
combines generalized mean pooling for global features and attentive selection
for local features. The entire network can be learned end-to-end by carefully
balancing the gradient flow between two heads – requiring only image-level
labels. We also introduce an autoencoder-based dimensionality reduction
technique for local features, which is integrated into the model, improving
training efficiency and matching performance. Comprehensive experiments show
that our model achieves state-of-the-art image retrieval on the Revisited
Oxford and Paris datasets, and state-of-the-art single-model instance-level
recognition on the Google Landmarks dataset v2. Code and models are available
at https://github.com/tensorflow/models/tree/master/research/delf .</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/zou2020transductive/">Transductive Zero-shot Hashing For Multilabel Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Transductive Zero-shot Hashing For Multilabel Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Transductive Zero-shot Hashing For Multilabel Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zou et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Neural Networks and Learning Systems</td>
    <td>14</td>
    <td><p>Hash coding has been widely used in approximate nearest neighbor search for
large-scale image retrieval. Given semantic annotations such as class labels
and pairwise similarities of the training data, hashing methods can learn and
generate effective and compact binary codes. While some newly introduced images
may contain undefined semantic labels, which we call unseen images, zeor-shot
hashing techniques have been studied. However, existing zeor-shot hashing
methods focus on the retrieval of single-label images, and cannot handle
multi-label images. In this paper, for the first time, a novel transductive
zero-shot hashing method is proposed for multi-label unseen image retrieval. In
order to predict the labels of the unseen/target data, a visual-semantic bridge
is built via instance-concept coherence ranking on the seen/source data. Then,
pairwise similarity loss and focal quantization loss are constructed for
training a hashing model using both the seen/source and unseen/target data.
Extensive evaluations on three popular multi-label datasets demonstrate that,
the proposed hashing method achieves significantly better results than the
competing methods.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
        Few Shot & Zero Shot 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/wu2020review/">A Review For Weighted Minhash Algorithms</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Review For Weighted Minhash Algorithms' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Review For Weighted Minhash Algorithms' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Knowledge and Data Engineering</td>
    <td>30</td>
    <td><p>Data similarity (or distance) computation is a fundamental research topic
which underpins many high-level applications based on similarity measures in
machine learning and data mining. However, in large-scale real-world scenarios,
the exact similarity computation has become daunting due to “3V” nature
(volume, velocity and variety) of big data. In such cases, the hashing
techniques have been verified to efficiently conduct similarity estimation in
terms of both theory and practice. Currently, MinHash is a popular technique
for efficiently estimating the Jaccard similarity of binary sets and
furthermore, weighted MinHash is generalized to estimate the generalized
Jaccard similarity of weighted sets. This review focuses on categorizing and
discussing the existing works of weighted MinHash algorithms. In this review,
we mainly categorize the Weighted MinHash algorithms into quantization-based
approaches, “active index”-based ones and others, and show the evolution and
inherent connection of the weighted MinHash algorithms, from the integer
weighted MinHash algorithms to real-valued weighted MinHash ones (particularly
the Consistent Weighted Sampling scheme). Also, we have developed a python
toolbox for the algorithms, and released it in our github. Based on the
toolbox, we experimentally conduct a comprehensive comparative study of the
standard MinHash algorithm and the weighted MinHash ones.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Survey Paper 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/wu2020interpretable/">Interpretable Embedding For Ad-hoc Video Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Embedding For Ad-hoc Video Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Interpretable Embedding For Ad-hoc Video Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu Jiaxin, Ngo Chong-wah</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 28th ACM International Conference on Multimedia</td>
    <td>30</td>
    <td><p>Answering query with semantic concepts has long been the mainstream approach
for video search. Until recently, its performance is surpassed by concept-free
approach, which embeds queries in a joint space as videos. Nevertheless, the
embedded features as well as search results are not interpretable, hindering
subsequent steps in video browsing and query reformulation. This paper
integrates feature embedding and concept interpretation into a neural network
for unified dual-task learning. In this way, an embedding is associated with a
list of semantic concepts as an interpretation of video content. This paper
empirically demonstrates that, by using either the embedding features or
concepts, considerable search improvement is attainable on TRECVid benchmarked
datasets. Concepts are not only effective in pruning false positive videos, but
also highly complementary to concept-free search, leading to large margin of
improvement compared to state-of-the-art approaches.</p>
</td>
    <td>
      
        Video Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/su2020where/">Where To Look And How To Describe: Fashion Image Retrieval With An Attentional Heterogeneous Bilinear Network</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Where To Look And How To Describe: Fashion Image Retrieval With An Attentional Heterogeneous Bilinear Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Where To Look And How To Describe: Fashion Image Retrieval With An Attentional Heterogeneous Bilinear Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Su et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Circuits and Systems for Video Technology</td>
    <td>32</td>
    <td><p>Fashion products typically feature in compositions of a variety of styles at
different clothing parts. In order to distinguish images of different fashion
products, we need to extract both appearance (i.e., “how to describe”) and
localization (i.e.,”where to look”) information, and their interactions. To
this end, we propose a biologically inspired framework for image-based fashion
product retrieval, which mimics the hypothesized twostream visual processing
system of human brain. The proposed attentional heterogeneous bilinear network
(AHBN) consists of two branches: a deep CNN branch to extract fine-grained
appearance attributes and a fully convolutional branch to extract landmark
localization information. A joint channel-wise attention mechanism is further
applied to the extracted heterogeneous features to focus on important channels,
followed by a compact bilinear pooling layer to model the interaction of the
two streams. Our proposed framework achieves satisfactory performance on three
image-based fashion product retrieval benchmarks.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/bai2020targeted/">Targeted Attack For Deep Hashing Based Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Targeted Attack For Deep Hashing Based Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Targeted Attack For Deep Hashing Based Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Bai et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>67</td>
    <td><p>The deep hashing based retrieval method is widely adopted in large-scale
image and video retrieval. However, there is little investigation on its
security. In this paper, we propose a novel method, dubbed deep hashing
targeted attack (DHTA), to study the targeted attack on such retrieval.
Specifically, we first formulate the targeted attack as a point-to-set
optimization, which minimizes the average distance between the hash code of an
adversarial example and those of a set of objects with the target label. Then
we design a novel component-voting scheme to obtain an anchor code as the
representative of the set of hash codes of objects with the target label, whose
optimality guarantee is also theoretically derived. To balance the performance
and perceptibility, we propose to minimize the Hamming distance between the
hash code of the adversarial example and the anchor code under the
\(\ell^\infty\) restriction on the perturbation. Extensive experiments verify
that DHTA is effective in attacking both deep hashing based image retrieval and
video retrieval.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/zhang2020survey/">A Survey On Deep Hashing For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Survey On Deep Hashing For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Survey On Deep Hashing For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Xiaopeng</td> <!-- 🔧 You were missing this -->
    <td>Artificial Intelligence Review</td>
    <td>40</td>
    <td><p>Hashing has been widely used in approximate nearest search for large-scale
database retrieval for its computation and storage efficiency. Deep hashing,
which devises convolutional neural network architecture to exploit and extract
the semantic information or feature of images, has received increasing
attention recently. In this survey, several deep supervised hashing methods for
image retrieval are evaluated and I conclude three main different directions
for deep supervised hashing methods. Several comments are made at the end.
Moreover, to break through the bottleneck of the existing hashing methods, I
propose a Shadow Recurrent Hashing(SRH) method as a try. Specifically, I devise
a CNN architecture to extract the semantic features of images and design a loss
function to encourage similar images projected close. To this end, I propose a
concept: shadow of the CNN output. During optimization process, the CNN output
and its shadow are guiding each other so as to achieve the optimal solution as
much as possible. Several experiments on dataset CIFAR-10 show the satisfying
performance of SRH.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        Survey Paper 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/aum%C3%BCller2020fair/">Fair Near Neighbor Search: Independent Range Sampling In High Dimensions</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fair Near Neighbor Search: Independent Range Sampling In High Dimensions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fair Near Neighbor Search: Independent Range Sampling In High Dimensions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Aumüller Martin, Pagh Rasmus, Silvestri Francesco</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 39th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems</td>
    <td>16</td>
    <td><p>Similarity search is a fundamental algorithmic primitive, widely used in many
computer science disciplines. There are several variants of the similarity
search problem, and one of the most relevant is the \(r\)-near neighbor (\(r\)-NN)
problem: given a radius \(r&gt;0\) and a set of points \(S\), construct a data
structure that, for any given query point \(q\), returns a point \(p\) within
distance at most \(r\) from \(q\). In this paper, we study the \(r\)-NN problem in
the light of fairness. We consider fairness in the sense of equal opportunity:
all points that are within distance \(r\) from the query should have the same
probability to be returned. In the low-dimensional case, this problem was first
studied by Hu, Qiao, and Tao (PODS 2014). Locality sensitive hashing (LSH), the
theoretically strongest approach to similarity search in high dimensions, does
not provide such a fairness guarantee. To address this, we propose efficient
data structures for \(r\)-NN where all points in \(S\) that are near \(q\) have the
same probability to be selected and returned by the query. Specifically, we
first propose a black-box approach that, given any LSH scheme, constructs a
data structure for uniformly sampling points in the neighborhood of a query.
Then, we develop a data structure for fair similarity search under inner
product that requires nearly-linear space and exploits locality sensitive
filters. The paper concludes with an experimental evaluation that highlights
(un)fairness in a recommendation setting on real-world datasets and discusses
the inherent unfairness introduced by solving other variants of the problem.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/yang2020nonlinear/">Nonlinear Robust Discrete Hashing For Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Nonlinear Robust Discrete Hashing For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Nonlinear Robust Discrete Hashing For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>21</td>
    <td><p>Hashing techniques have recently been successfully applied to solve similarity search problems in the information retrieval field because of their significantly reduced storage and high-speed search capabilities. However, the hash codes learned from most recent cross-modal hashing methods lack the ability to comprehensively preserve adequate information, resulting in a less than desirable performance. To solve this limitation, we propose a novel method termed Nonlinear Robust Discrete Hashing (NRDH), for cross-modal retrieval. The main idea behind NRDH is motivated by the success of neural networks, i.e., nonlinear descriptors, in the field of representation learning, and the use of nonlinear descriptors instead of simple linear transformations is more in line with the complex relationships that exist between common latent representation and heterogeneous multimedia data in the real world. In NRDH, we first learn a common latent representation through nonlinear descriptors to encode complementary and consistent information from the features of the heterogeneous multimedia data. Moreover, an asymmetric learning scheme is proposed to correlate the learned hash codes with the common latent representation. Empirically, we demonstrate that NRDH is able to successfully generate a comprehensive common latent representation that significantly improves the quality of the learned hash codes. Then, NRDH adopts a linear learning strategy to fast learn the hash function with the learned hash codes. Extensive experiments performed on two benchmark datasets highlight the superiority of NRDH over several state-of-the-art methods.</p>
</td>
    <td>
      
        SIGIR 
      
        Hashing Methods 
      
        Text Retrieval 
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/charikar2020kernel/">Kernel Density Estimation Through Density Constrained Near Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Kernel Density Estimation Through Density Constrained Near Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Kernel Density Estimation Through Density Constrained Near Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Charikar et al.</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS)</td>
    <td>8</td>
    <td><p>In this paper we revisit the kernel density estimation problem: given a
kernel \(K(x, y)\) and a dataset of \(n\) points in high dimensional Euclidean
space, prepare a data structure that can quickly output, given a query \(q\), a
\((1+\epsilon)\)-approximation to \(\mu:=\frac1{|P|}\sum_{p\in P} K(p, q)\). First,
we give a single data structure based on classical near neighbor search
techniques that improves upon or essentially matches the query time and space
complexity for all radial kernels considered in the literature so far. We then
show how to improve both the query complexity and runtime by using recent
advances in data-dependent near neighbor search.
  We achieve our results by giving a new implementation of the natural
importance sampling scheme. Unlike previous approaches, our algorithm first
samples the dataset uniformly (considering a geometric sequence of sampling
rates), and then uses existing approximate near neighbor search techniques on
the resulting smaller dataset to retrieve the sampled points that lie at an
appropriate distance from the query. We show that the resulting sampled dataset
has strong geometric structure, making approximate near neighbor search return
the required samples much more efficiently than for worst case datasets of the
same size. As an example application, we show that this approach yields a data
structure that achieves query time \(\mu^{-(1+o(1))/4}\) and space complexity
\(\mu^{-(1+o(1))}\) for the Gaussian kernel. Our data dependent approach achieves
query time \(\mu^{-0.173-o(1)}\) and space \(\mu^{-(1+o(1))}\) for the Gaussian
kernel. The data dependent analysis relies on new techniques for tracking the
geometric structure of the input datasets in a recursive hashing process that
we hope will be of interest in other applications in near neighbor search.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/awasthy2020granite/">Granite Embedding Models</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Granite Embedding Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Granite Embedding Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Awasthy et al.</td> <!-- 🔧 You were missing this -->
    <td>International Journal for Numerical and Analytical Methods in Geomechanics</td>
    <td>14</td>
    <td><p>We introduce the Granite Embedding models, a family of encoder-based
embedding models designed for retrieval tasks, spanning dense-retrieval and
sparse retrieval architectures, with both English and Multilingual
capabilities. This report provides the technical details of training these
highly effective 12 layer embedding models, along with their efficient 6 layer
distilled counterparts. Extensive evaluations show that the models, developed
with techniques like retrieval oriented pretraining, contrastive finetuning,
knowledge distillation, and model merging significantly outperform publicly
available models of similar sizes on both internal IBM retrieval and search
tasks, and have equivalent performance on widely used information retrieval
benchmarks, while being trained on high-quality data suitable for enterprise
use. We publicly release all our Granite Embedding models under the Apache 2.0
license, allowing both research and commercial use at
https://huggingface.co/collections/ibm-granite.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/ahle2020subsets/">Subsets And Supermajorities: Optimal Hashing-based Set Similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Subsets And Supermajorities: Optimal Hashing-based Set Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Subsets And Supermajorities: Optimal Hashing-based Set Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ahle Thomas Dybdahl, Knudsen Jakob Bæk Tejs</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS)</td>
    <td>5</td>
    <td><p>We formulate and optimally solve a new generalized Set Similarity Search
problem, which assumes the size of the database and query sets are known in
advance. By creating polylog copies of our data-structure, we optimally solve
any symmetric Approximate Set Similarity Search problem, including approximate
versions of Subset Search, Maximum Inner Product Search (MIPS), Jaccard
Similarity Search and Partial Match.
  Our algorithm can be seen as a natural generalization of previous work on Set
as well as Euclidean Similarity Search, but conceptually it differs by
optimally exploiting the information present in the sets as well as their
complements, and doing so asymmetrically between queries and stored sets. Doing
so we improve upon the best previous work: MinHash [J. Discrete Algorithms
1998], SimHash [STOC 2002], Spherical LSF [SODA 2016, 2017] and Chosen Path
[STOC 2017] by as much as a factor \(n^{0.14}\) in both time and space; or in the
near-constant time regime, in space, by an arbitrarily large polynomial factor.
  Turning the geometric concept, based on Boolean supermajority functions, into
a practical algorithm requires ideas from branching random walks on \(\mathbb
Z^2\), for which we give the first non-asymptotic near tight analysis.
  Our lower bounds follow from new hypercontractive arguments, which can be
seen as characterizing the exact family of similarity search problems for which
supermajorities are optimal. The optimality holds for among all hashing based
data structures in the random setting, and by reductions, for 1 cell and 2 cell
probe data structures. As a side effect, we obtain new hypercontractive bounds
on the directed noise operator \(T^{p_1 \to p_2}_\rho\).</p>
</td>
    <td>
      
        Similarity Search 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/aghazadeh2020distributed/">A Distributed Approximate Nearest Neighbor Method For Real-time Face Recognition</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Distributed Approximate Nearest Neighbor Method For Real-time Face Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Distributed Approximate Nearest Neighbor Method For Real-time Face Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Aghazadeh Aysan, Amirmazlaghani Maryam</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition</td>
    <td>24</td>
    <td><p>Nowadays, face recognition and more generally image recognition have many
applications in the modern world and are widely used in our daily tasks. This
paper aims to propose a distributed approximate nearest neighbor (ANN) method
for real-time face recognition using a big dataset that involves a lot of
classes. The proposed approach is based on using a clustering method to
separate the dataset into different clusters and on specifying the importance
of each cluster by defining cluster weights. To this end, reference instances
are selected from each cluster based on the cluster weights using a maximum
likelihood approach. This process leads to a more informed selection of
instances, so it enhances the performance of the algorithm. Experimental
results confirm the efficiency of the proposed method and its out-performance
in terms of accuracy and the processing time.</p>
</td>
    <td>
      
        Similarity Search 
      
        Efficiency 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/yokoo2020two/">Two-stage Discriminative Re-ranking For Large-scale Landmark Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Two-stage Discriminative Re-ranking For Large-scale Landmark Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Two-stage Discriminative Re-ranking For Large-scale Landmark Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yokoo et al.</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>20</td>
    <td><p>We propose an efficient pipeline for large-scale landmark image retrieval
that addresses the diversity of the dataset through two-stage discriminative
re-ranking. Our approach is based on embedding the images in a feature-space
using a convolutional neural network trained with a cosine softmax loss. Due to
the variance of the images, which include extreme viewpoint changes such as
having to retrieve images of the exterior of a landmark from images of the
interior, this is very challenging for approaches based exclusively on visual
similarity. Our proposed re-ranking approach improves the results in two steps:
in the sort-step, \(k\)-nearest neighbor search with soft-voting to sort the
retrieved results based on their label similarity to the query images, and in
the insert-step, we add additional samples from the dataset that were not
retrieved by image-similarity. This approach allows overcoming the low visual
diversity in retrieved images. In-depth experimental results show that the
proposed approach significantly outperforms existing approaches on the
challenging Google Landmarks Datasets. Using our methods, we achieved 1st place
in the Google Landmark Retrieval 2019 challenge and 3rd place in the Google
Landmark Recognition 2019 challenge on Kaggle. Our code is publicly available
here: https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution</p>
</td>
    <td>
      
        Hybrid ANN Methods 
      
        SCALABILITY 
      
        Re RANKING 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/weng2020online/">Online Hashing With Efficient Updating Of Binary Codes</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Online Hashing With Efficient Updating Of Binary Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Online Hashing With Efficient Updating Of Binary Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Weng Zhenyu, Zhu Yuesheng</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>15</td>
    <td><p>Online hashing methods are efficient in learning the hash functions from the
streaming data. However, when the hash functions change, the binary codes for
the database have to be recomputed to guarantee the retrieval accuracy.
Recomputing the binary codes by accumulating the whole database brings a
timeliness challenge to the online retrieval process. In this paper, we propose
a novel online hashing framework to update the binary codes efficiently without
accumulating the whole database. In our framework, the hash functions are fixed
and the projection functions are introduced to learn online from the streaming
data. Therefore, inefficient updating of the binary codes by accumulating the
whole database can be transformed to efficient updating of the binary codes by
projecting the binary codes into another binary space. The queries and the
binary code database are projected asymmetrically to further improve the
retrieval accuracy. The experiments on two multi-label image databases
demonstrate the effectiveness and the efficiency of our method for multi-label
image retrieval.</p>
</td>
    <td>
      
        Compact Codes 
      
        Hashing Methods 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/zhan2020repbert/">Repbert: Contextualized Text Embeddings For First-stage Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Repbert: Contextualized Text Embeddings For First-stage Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Repbert: Contextualized Text Embeddings For First-stage Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhan et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>60</td>
    <td><p>Although exact term match between queries and documents is the dominant
method to perform first-stage retrieval, we propose a different approach,
called RepBERT, to represent documents and queries with fixed-length
contextualized embeddings. The inner products of query and document embeddings
are regarded as relevance scores. On MS MARCO Passage Ranking task, RepBERT
achieves state-of-the-art results among all initial retrieval techniques. And
its efficiency is comparable to bag-of-words methods.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/tan2020learning/">Learning To Hash With Graph Neural Networks For Recommender Systems</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning To Hash With Graph Neural Networks For Recommender Systems' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning To Hash With Graph Neural Networks For Recommender Systems' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tan et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of The Web Conference 2020</td>
    <td>76</td>
    <td><p>Graph representation learning has attracted much attention in supporting high
quality candidate search at scale. Despite its effectiveness in learning
embedding vectors for objects in the user-item interaction network, the
computational costs to infer users’ preferences in continuous embedding space
are tremendous. In this work, we investigate the problem of hashing with graph
neural networks (GNNs) for high quality retrieval, and propose a simple yet
effective discrete representation learning framework to jointly learn
continuous and discrete codes. Specifically, a deep hashing with GNNs (HashGNN)
is presented, which consists of two components, a GNN encoder for learning node
representations, and a hash layer for encoding representations to hash codes.
The whole architecture is trained end-to-end by jointly optimizing two losses,
i.e., reconstruction loss from reconstructing observed links, and ranking loss
from preserving the relative ordering of hash codes. A novel discrete
optimization strategy based on straight through estimator (STE) with guidance
is proposed. The principal idea is to avoid gradient magnification in
back-propagation of STE with continuous embedding guidance, in which we begin
from learning an easier network that mimic the continuous embedding and let it
evolve during the training until it finally goes back to STE. Comprehensive
experiments over three publicly available and one real-world Alibaba company
datasets demonstrate that our model not only can achieve comparable performance
compared with its continuous counterpart but also runs multiple times faster
during inference.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/nikoli%C4%872020bitpruning/">Bitpruning: Learning Bitlengths For Aggressive And Accurate Quantization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Bitpruning: Learning Bitlengths For Aggressive And Accurate Quantization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Bitpruning: Learning Bitlengths For Aggressive And Accurate Quantization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Nikolić et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>13</td>
    <td><p>Neural networks have demonstrably achieved state-of-the art accuracy using
low-bitlength integer quantization, yielding both execution time and energy
benefits on existing hardware designs that support short bitlengths. However,
the question of finding the minimum bitlength for a desired accuracy remains
open. We introduce a training method for minimizing inference bitlength at any
granularity while maintaining accuracy. Namely, we propose a regularizer that
penalizes large bitlength representations throughout the architecture and show
how it can be modified to minimize other quantifiable criteria, such as number
of operations or memory footprint. We demonstrate that our method learns
thrifty representations while maintaining accuracy. With ImageNet, the method
produces an average per layer bitlength of 4.13, 3.76 and 4.36 bits on AlexNet,
ResNet18 and MobileNet V2 respectively, remaining within 2.0%, 0.5% and 0.5% of
the base TOP-1 accuracy.</p>
</td>
    <td>
      
        Quantization 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/ng2020solar/">SOLAR: Second-order Loss And Attention For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=SOLAR: Second-order Loss And Attention For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=SOLAR: Second-order Loss And Attention For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ng et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>98</td>
    <td><p>Recent works in deep-learning have shown that second-order information is
beneficial in many computer-vision tasks. Second-order information can be
enforced both in the spatial context and the abstract feature dimensions. In
this work, we explore two second-order components. One is focused on
second-order spatial information to increase the performance of image
descriptors, both local and global. It is used to re-weight feature maps, and
thus emphasise salient image locations that are subsequently used for
description. The second component is concerned with a second-order similarity
(SOS) loss, that we extend to global descriptors for image retrieval, and is
used to enhance the triplet loss with hard-negative mining. We validate our
approach on two different tasks and datasets for image retrieval and image
matching. The results show that our two second-order components complement each
other, bringing significant performance improvements in both tasks and lead to
state-of-the-art results across the public benchmarks. Code available at:
http://github.com/tonyngjichun/SOLAR</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/taherkhani2020error/">Error-corrected Margin-based Deep Cross-modal Hashing For Facial Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Error-corrected Margin-based Deep Cross-modal Hashing For Facial Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Error-corrected Margin-based Deep Cross-modal Hashing For Facial Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Taherkhani et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Biometrics, Behavior, and Identity Science</td>
    <td>14</td>
    <td><p>Cross-modal hashing facilitates mapping of heterogeneous multimedia data into
a common Hamming space, which can beutilized for fast and flexible retrieval
across different modalities. In this paper, we propose a novel cross-modal
hashingarchitecture-deep neural decoder cross-modal hashing (DNDCMH), which
uses a binary vector specifying the presence of certainfacial attributes as an
input query to retrieve relevant face images from a database. The DNDCMH
network consists of two separatecomponents: an attribute-based deep cross-modal
hashing (ADCMH) module, which uses a margin (m)-based loss function
toefficiently learn compact binary codes to preserve similarity between
modalities in the Hamming space, and a neural error correctingdecoder (NECD),
which is an error correcting decoder implemented with a neural network. The
goal of NECD network in DNDCMH isto error correct the hash codes generated by
ADCMH to improve the retrieval efficiency. The NECD network is trained such
that it hasan error correcting capability greater than or equal to the margin
(m) of the margin-based loss function. This results in NECD cancorrect the
corrupted hash codes generated by ADCMH up to the Hamming distance of m. We
have evaluated and comparedDNDCMH with state-of-the-art cross-modal hashing
methods on standard datasets to demonstrate the superiority of our method.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/morgado2020deep/">Deep Hashing With Hash-consistent Large Margin Proxy Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Hashing With Hash-consistent Large Margin Proxy Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Hashing With Hash-consistent Large Margin Proxy Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Morgado et al.</td> <!-- 🔧 You were missing this -->
    <td>International Journal of Computer Vision</td>
    <td>6</td>
    <td><p>Image hash codes are produced by binarizing the embeddings of convolutional
neural networks (CNN) trained for either classification or retrieval. While
proxy embeddings achieve good performance on both tasks, they are non-trivial
to binarize, due to a rotational ambiguity that encourages non-binary
embeddings. The use of a fixed set of proxies (weights of the CNN
classification layer) is proposed to eliminate this ambiguity, and a procedure
to design proxy sets that are nearly optimal for both classification and
hashing is introduced. The resulting hash-consistent large margin (HCLM)
proxies are shown to encourage saturation of hashing units, thus guaranteeing a
small binarization error, while producing highly discriminative hash-codes. A
semantic extension (sHCLM), aimed to improve hashing performance in a transfer
scenario, is also proposed. Extensive experiments show that sHCLM embeddings
achieve significant improvements over state-of-the-art hashing procedures on
several small and large datasets, both within and beyond the set of training
classes.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/misraa2020multi/">Multi-modal Retrieval Using Graph Neural Networks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multi-modal Retrieval Using Graph Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multi-modal Retrieval Using Graph Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Misraa et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>Most real world applications of image retrieval such as Adobe Stock, which is
a marketplace for stock photography and illustrations, need a way for users to
find images which are both visually (i.e. aesthetically) and conceptually (i.e.
containing the same salient objects) as a query image. Learning visual-semantic
representations from images is a well studied problem for image retrieval.
Filtering based on image concepts or attributes is traditionally achieved with
index-based filtering (e.g. on textual tags) or by re-ranking after an initial
visual embedding based retrieval. In this paper, we learn a joint vision and
concept embedding in the same high-dimensional space. This joint model gives
the user fine-grained control over the semantics of the result set, allowing
them to explore the catalog of images more rapidly. We model the visual and
concept relationships as a graph structure, which captures the rich information
through node neighborhood. This graph structure helps us learn multi-modal node
embeddings using Graph Neural Networks. We also introduce a novel inference
time control, based on selective neighborhood connectivity allowing the user
control over the retrieval algorithm. We evaluate these multi-modal embeddings
quantitatively on the downstream relevance task of image retrieval on MS-COCO
dataset and qualitatively on MS-COCO and an Adobe Stock dataset.</p>
</td>
    <td>
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/weng2020random/">Random VLAD Based Deep Hashing For Efficient Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Random VLAD Based Deep Hashing For Efficient Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Random VLAD Based Deep Hashing For Efficient Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Weng et al.</td> <!-- 🔧 You were missing this -->
    <td>2016 IEEE International Conference on Multimedia and Expo (ICME)</td>
    <td>9</td>
    <td><p>Image hash algorithms generate compact binary representations that can be
quickly matched by Hamming distance, thus become an efficient solution for
large-scale image retrieval. This paper proposes RV-SSDH, a deep image hash
algorithm that incorporates the classical VLAD (vector of locally aggregated
descriptors) architecture into neural networks. Specifically, a novel neural
network component is formed by coupling a random VLAD layer with a latent hash
layer through a transform layer. This component can be combined with
convolutional layers to realize a hash algorithm. We implement RV-SSDH as a
point-wise algorithm that can be efficiently trained by minimizing
classification error and quantization loss. Comprehensive experiments show this
new architecture significantly outperforms baselines such as NetVLAD and SSDH,
and offers a cost-effective trade-off in the state-of-the-art. In addition, the
proposed random VLAD layer leads to satisfactory accuracy with low complexity,
thus shows promising potentials as an alternative to NetVLAD.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/zheng2020generative/">Generative Semantic Hashing Enhanced Via Boltzmann Machines</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Generative Semantic Hashing Enhanced Via Boltzmann Machines' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Generative Semantic Hashing Enhanced Via Boltzmann Machines' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zheng et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</td>
    <td>10</td>
    <td><p>Generative semantic hashing is a promising technique for large-scale
information retrieval thanks to its fast retrieval speed and small memory
footprint. For the tractability of training, existing generative-hashing
methods mostly assume a factorized form for the posterior distribution,
enforcing independence among the bits of hash codes. From the perspectives of
both model representation and code space size, independence is always not the
best assumption. In this paper, to introduce correlations among the bits of
hash codes, we propose to employ the distribution of Boltzmann machine as the
variational posterior. To address the intractability issue of training, we
first develop an approximate method to reparameterize the distribution of a
Boltzmann machine by augmenting it as a hierarchical concatenation of a
Gaussian-like distribution and a Bernoulli distribution. Based on that, an
asymptotically-exact lower bound is further derived for the evidence lower
bound (ELBO). With these novel techniques, the entire model can be optimized
efficiently. Extensive experimental results demonstrate that by effectively
modeling correlations among different bits within a hash code, our model can
achieve significant performance gains.</p>
</td>
    <td>
      
        Text Retrieval 
      
        ACL 
      
        TACL 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/xuan2020improved/">Improved Embeddings With Easy Positive Triplet Mining</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Improved Embeddings With Easy Positive Triplet Mining' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Improved Embeddings With Easy Positive Triplet Mining' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xuan Hong, Stylianou Abby, Pless Robert</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>131</td>
    <td><p>Deep metric learning seeks to define an embedding where semantically similar
images are embedded to nearby locations, and semantically dissimilar images are
embedded to distant locations. Substantial work has focused on loss functions
and strategies to learn these embeddings by pushing images from the same class
as close together in the embedding space as possible. In this paper, we propose
an alternative, loosened embedding strategy that requires the embedding
function only map each training image to the most similar examples from the
same class, an approach we call “Easy Positive” mining. We provide a collection
of experiments and visualizations that highlight that this Easy Positive mining
leads to embeddings that are more flexible and generalize better to new unseen
data. This simple mining strategy yields recall performance that exceeds state
of the art approaches (including those with complicated loss functions and
ensemble methods) on image retrieval datasets including CUB, Stanford Online
Products, In-Shop Clothes and Hotels-50K.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/zhou2020ladder/">Ladder Loss For Coherent Visual-semantic Embedding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Ladder Loss For Coherent Visual-semantic Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Ladder Loss For Coherent Visual-semantic Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhou et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>27</td>
    <td><p>For visual-semantic embedding, the existing methods normally treat the
relevance between queries and candidates in a bipolar way – relevant or
irrelevant, and all “irrelevant” candidates are uniformly pushed away from the
query by an equal margin in the embedding space, regardless of their various
proximity to the query. This practice disregards relatively discriminative
information and could lead to suboptimal ranking in the retrieval results and
poorer user experience, especially in the long-tail query scenario where a
matching candidate may not necessarily exist. In this paper, we introduce a
continuous variable to model the relevance degree between queries and multiple
candidates, and propose to learn a coherent embedding space, where candidates
with higher relevance degrees are mapped closer to the query than those with
lower relevance degrees. In particular, the new ladder loss is proposed by
extending the triplet loss inequality to a more general inequality chain, which
implements variable push-away margins according to respective relevance
degrees. In addition, a proper Coherent Score metric is proposed to better
measure the ranking results including those “irrelevant” candidates. Extensive
experiments on multiple datasets validate the efficacy of our proposed method,
which achieves significant improvement over existing state-of-the-art methods.</p>
</td>
    <td>
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/takeshita2020secure/">Secure Single-server Nearly-identical Image Deduplication</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Secure Single-server Nearly-identical Image Deduplication' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Secure Single-server Nearly-identical Image Deduplication' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Takeshita Jonathan, Karl Ryan, Jung Taeho</td> <!-- 🔧 You were missing this -->
    <td>2020 29th International Conference on Computer Communications and Networks (ICCCN)</td>
    <td>14</td>
    <td><p>Cloud computing is often utilized for file storage. Clients of cloud storage
services want to ensure the privacy of their data, and both clients and servers
want to use as little storage as possible. Cross-user deduplication is one
method to reduce the amount of storage a server uses. Deduplication and privacy
are naturally conflicting goals, especially for nearly-identical (``fuzzy’’)
deduplication, as some information about the data must be used to perform
deduplication. Prior solutions thus utilize multiple servers, or only function
for exact deduplication. In this paper, we present a single-server protocol for
cross-user nearly-identical deduplication based on secure locality-sensitive
hashing (SLSH). We formally define our ideal security, and rigorously prove our
protocol secure against fully malicious, colluding adversaries with a proof by
simulation. We show experimentally that the individual parts of the protocol
are computationally feasible, and further discuss practical issues of security
and efficiency.</p>
</td>
    <td>
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/park2020mhsan/">MHSAN: Multi-head Self-attention Network For Visual Semantic Embedding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=MHSAN: Multi-head Self-attention Network For Visual Semantic Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=MHSAN: Multi-head Self-attention Network For Visual Semantic Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Park et al.</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>20</td>
    <td><p>Visual-semantic embedding enables various tasks such as image-text retrieval,
image captioning, and visual question answering. The key to successful
visual-semantic embedding is to express visual and textual data properly by
accounting for their intricate relationship. While previous studies have
achieved much advance by encoding the visual and textual data into a joint
space where similar concepts are closely located, they often represent data by
a single vector ignoring the presence of multiple important components in an
image or text. Thus, in addition to the joint embedding space, we propose a
novel multi-head self-attention network to capture various components of visual
and textual data by attending to important parts in data. Our approach achieves
the new state-of-the-art results in image-text retrieval tasks on MS-COCO and
Flicker30K datasets. Through the visualization of the attention maps that
capture distinct semantic components at multiple positions in the image and the
text, we demonstrate that our method achieves an effective and interpretable
visual-semantic joint space.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/paria2020minimizing/">Minimizing Flops To Learn Efficient Sparse Representations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Minimizing Flops To Learn Efficient Sparse Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Minimizing Flops To Learn Efficient Sparse Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Paria et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>14</td>
    <td><p>Deep representation learning has become one of the most widely adopted
approaches for visual search, recommendation, and identification. Retrieval of
such representations from a large database is however computationally
challenging. Approximate methods based on learning compact representations,
have been widely explored for this problem, such as locality sensitive hashing,
product quantization, and PCA. In this work, in contrast to learning compact
representations, we propose to learn high dimensional and sparse
representations that have similar representational capacity as dense embeddings
while being more efficient due to sparse matrix multiplication operations which
can be much faster than dense multiplication. Following the key insight that
the number of operations decreases quadratically with the sparsity of
embeddings provided the non-zero entries are distributed uniformly across
dimensions, we propose a novel approach to learn such distributed sparse
embeddings via the use of a carefully constructed regularization function that
directly minimizes a continuous relaxation of the number of floating-point
operations (FLOPs) incurred during retrieval. Our experiments show that our
approach is competitive to the other baselines and yields a similar or better
speed-vs-accuracy tradeoff on practical datasets.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/imbriaco2019aggregated/">Aggregated Deep Local Features For Remote Sensing Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Aggregated Deep Local Features For Remote Sensing Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Aggregated Deep Local Features For Remote Sensing Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Imbriaco et al.</td> <!-- 🔧 You were missing this -->
    <td>Remote Sensing</td>
    <td>73</td>
    <td><p>Remote Sensing Image Retrieval remains a challenging topic due to the special
nature of Remote Sensing Imagery. Such images contain various different
semantic objects, which clearly complicates the retrieval task. In this paper,
we present an image retrieval pipeline that uses attentive, local convolutional
features and aggregates them using the Vector of Locally Aggregated Descriptors
(VLAD) to produce a global descriptor. We study various system parameters such
as the multiplicative and additive attention mechanisms and descriptor
dimensionality. We propose a query expansion method that requires no external
inputs. Experiments demonstrate that even without training, the local
convolutional features and global representation outperform other systems.
After system tuning, we can achieve state-of-the-art or competitive results.
Furthermore, we observe that our query expansion method increases overall
system performance by about 3%, using only the top-three retrieved images.
Finally, we show how dimensionality reduction produces compact descriptors with
increased retrieval performance and fast retrieval computation times, e.g. 50%
faster than the current systems.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/sun2019supervised/">Supervised Hierarchical Cross-modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Hierarchical Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Supervised Hierarchical Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sun et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>47</td>
    <td><p>Recently, due to the unprecedented growth of multimedia data,
cross-modal hashing has gained increasing attention for the
efficient cross-media retrieval. Typically, existing methods on crossmodal hashing treat labels of one instance independently but
overlook the correlations among labels. Indeed, in many real-world
scenarios, like the online fashion domain, instances (items) are
labeled with a set of categories correlated by certain hierarchy. In
this paper, we propose a new end-to-end solution for supervised
cross-modal hashing, named HiCHNet, which explicitly exploits the
hierarchical labels of instances. In particular, by the pre-established
label hierarchy, we comprehensively characterize each modality
of the instance with a set of layer-wise hash representations. In
essence, hash codes are encouraged to not only preserve the layerwise semantic similarities encoded by the label hierarchy, but also
retain the hierarchical discriminative capabilities. Due to the lack
of benchmark datasets, apart from adapting the existing dataset
FashionVC from fashion domain, we create a dataset from the
online fashion platform Ssense consisting of 15, 696 image-text
pairs labeled by 32 hierarchical categories. Extensive experiments
on two real-world datasets demonstrate the superiority of our model
over the state-of-the-art methods.</p>
</td>
    <td>
      
        SIGIR 
      
        Hashing Methods 
      
        Text Retrieval 
      
        SUPERVISED 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/jacob2019metric/">Metric Learning With HORDE: High-order Regularizer For Deep Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Metric Learning With HORDE: High-order Regularizer For Deep Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Metric Learning With HORDE: High-order Regularizer For Deep Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jacob et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>73</td>
    <td><p>Learning an effective similarity measure between image representations is key
to the success of recent advances in visual search tasks (e.g. verification or
zero-shot learning). Although the metric learning part is well addressed, this
metric is usually computed over the average of the extracted deep features.
This representation is then trained to be discriminative. However, these deep
features tend to be scattered across the feature space. Consequently, the
representations are not robust to outliers, object occlusions, background
variations, etc. In this paper, we tackle this scattering problem with a
distribution-aware regularization named HORDE. This regularizer enforces
visually-close images to have deep features with the same distribution which
are well localized in the feature space. We provide a theoretical analysis
supporting this regularization effect. We also show the effectiveness of our
approach by obtaining state-of-the-art results on 4 well-known datasets
(Cub-200-2011, Cars-196, Stanford Online Products and Inshop Clothes
Retrieval).</p>
</td>
    <td>
      
        Distance Metric Learning 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/ishaq2019clustered/">Clustered Hierarchical Entropy-scaling Search Of Astronomical And Biological Data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Clustered Hierarchical Entropy-scaling Search Of Astronomical And Biological Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Clustered Hierarchical Entropy-scaling Search Of Astronomical And Biological Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ishaq Najib, Student George, Daniels Noah M.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE International Conference on Big Data (Big Data)</td>
    <td>5</td>
    <td><p>Both astronomy and biology are experiencing explosive growth of data,
resulting in a “big data” problem that stands in the way of a “big data”
opportunity for discovery. One common question asked of such data is that of
approximate search (\(\rho-\)nearest neighbors search). We present a hierarchical
search algorithm for such data sets that takes advantage of particular
geometric properties apparent in both astronomical and biological data sets,
namely the metric entropy and fractal dimensionality of the data. We present
CHESS (Clustered Hierarchical Entropy-Scaling Search), a search tool with
virtually no loss in specificity or sensitivity, demonstrating a \(13.6\times\)
speedup over linear search on the Sloan Digital Sky Survey’s APOGEE data set
and a \(68\times\) speedup on the GreenGenes 16S metagenomic data set, as well as
asymptotically fewer distance comparisons on APOGEE when compared to the
FALCONN locality-sensitive hashing library. CHESS demonstrates an asymptotic
complexity not directly dependent on data set size, and is in practice at least
an order of magnitude faster than linear search by performing fewer distance
comparisons. Unlike locality-sensitive hashing approaches, CHESS can work with
any user-defined distance function. CHESS also allows for implicit data
compression, which we demonstrate on the APOGEE data set. We also discuss an
extension allowing for efficient k-nearest neighbors search.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/schall2019deep/">Deep Aggregation Of Regional Convolutional Activations For Content Based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Aggregation Of Regional Convolutional Activations For Content Based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Aggregation Of Regional Convolutional Activations For Content Based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Schall et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE 21st International Workshop on Multimedia Signal Processing (MMSP)</td>
    <td>7</td>
    <td><p>One of the key challenges of deep learning based image retrieval remains in
aggregating convolutional activations into one highly representative feature
vector. Ideally, this descriptor should encode semantic, spatial and low level
information. Even though off-the-shelf pre-trained neural networks can already
produce good representations in combination with aggregation methods,
appropriate fine tuning for the task of image retrieval has shown to
significantly boost retrieval performance. In this paper, we present a simple
yet effective supervised aggregation method built on top of existing regional
pooling approaches. In addition to the maximum activation of a given region, we
calculate regional average activations of extracted feature maps. Subsequently,
weights for each of the pooled feature vectors are learned to perform a
weighted aggregation to a single feature vector. Furthermore, we apply our
newly proposed NRA loss function for deep metric learning to fine tune the
backbone neural network and to learn the aggregation weights. Our method
achieves state-of-the-art results for the INRIA Holidays data set and
competitive results for the Oxford Buildings and Paris data sets while reducing
the training time significantly.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/sarafijanovicdjukic2019fast/">Fast Distance-based Anomaly Detection In Images Using An Inception-like Autoencoder</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Distance-based Anomaly Detection In Images Using An Inception-like Autoencoder' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Distance-based Anomaly Detection In Images Using An Inception-like Autoencoder' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sarafijanovic-djukic Natasa, Davis Jesse</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>32</td>
    <td><p>The goal of anomaly detection is to identify examples that deviate from
normal or expected behavior. We tackle this problem for images. We consider a
two-phase approach. First, using normal examples, a convolutional autoencoder
(CAE) is trained to extract a low-dimensional representation of the images.
Here, we propose a novel architectural choice when designing the CAE, an
Inception-like CAE. It combines convolutional filters of different kernel sizes
and it uses a Global Average Pooling (GAP) operation to extract the
representations from the CAE’s bottleneck layer. Second, we employ a
distanced-based anomaly detector in the low-dimensional space of the learned
representation for the images. However, instead of computing the exact
distance, we compute an approximate distance using product quantization. This
alleviates the high memory and prediction time costs of distance-based anomaly
detectors. We compare our proposed approach to a number of baselines and
state-of-the-art methods on four image datasets, and we find that our approach
resulted in improved predictive performance.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/vo2019composing/">Composing Text And Image For Image Retrieval - An Empirical Odyssey</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Composing Text And Image For Image Retrieval - An Empirical Odyssey' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Composing Text And Image For Image Retrieval - An Empirical Odyssey' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Vo et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>307</td>
    <td><p>In this paper, we study the task of image retrieval, where the input query is
specified in the form of an image plus some text that describes desired
modifications to the input image. For example, we may present an image of the
Eiffel tower, and ask the system to find images which are visually similar but
are modified in small ways, such as being taken at nighttime instead of during
the day. To tackle this task, we learn a similarity metric between a target
image and a source image plus source text, an embedding and composing function
such that target image feature is close to the source image plus text
composition feature. We propose a new way to combine image and text using such
function that is designed for the retrieval task. We show this outperforms
existing approaches on 3 different datasets, namely Fashion-200k, MIT-States
and a new synthetic dataset we create based on CLEVR. We also show that our
approach can be used to classify input queries, in addition to image retrieval.</p>
</td>
    <td>
      
        Image Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/sankar2019transferable/">Transferable Neural Projection Representations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Transferable Neural Projection Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Transferable Neural Projection Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sankar Chinnadhurai, Ravi Sujith, Kozareva Zornitsa</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2019 Conference of the North</td>
    <td>7</td>
    <td><p>Neural word representations are at the core of many state-of-the-art natural
language processing models. A widely used approach is to pre-train, store and
look up word or character embedding matrices. While useful, such
representations occupy huge memory making it hard to deploy on-device and often
do not generalize to unknown words due to vocabulary pruning.
  In this paper, we propose a skip-gram based architecture coupled with
Locality-Sensitive Hashing (LSH) projections to learn efficient dynamically
computable representations. Our model does not need to store lookup tables as
representations are computed on-the-fly and require low memory footprint. The
representations can be trained in an unsupervised fashion and can be easily
transferred to other NLP tasks. For qualitative evaluation, we analyze the
nearest neighbors of the word representations and discover semantically similar
words even with misspellings. For quantitative evaluation, we plug our
transferable projections into a simple LSTM and run it on multiple NLP tasks
and show how our transferable projections achieve better performance compared
to prior work.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/xu2019semantic/">Semantic Adversarial Network For Zero-shot Sketch-based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semantic Adversarial Network For Zero-shot Sketch-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semantic Adversarial Network For Zero-shot Sketch-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xu et al.</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>23</td>
    <td><p>Zero-shot sketch-based image retrieval (ZS-SBIR) is a specific cross-modal
retrieval task for retrieving natural images with free-hand sketches under
zero-shot scenario. Previous works mostly focus on modeling the correspondence
between images and sketches or synthesizing image features with sketch
features. However, both of them ignore the large intra-class variance of
sketches, thus resulting in unsatisfactory retrieval performance. In this
paper, we propose a novel end-to-end semantic adversarial approach for ZS-SBIR.
Specifically, we devise a semantic adversarial module to maximize the
consistency between learned semantic features and category-level word vectors.
Moreover, to preserve the discriminability of synthesized features within each
training category, a triplet loss is employed for the generative module.
Additionally, the proposed model is trained in an end-to-end strategy to
exploit better semantic features suitable for ZS-SBIR. Extensive experiments
conducted on two large-scale popular datasets demonstrate that our proposed
approach remarkably outperforms state-of-the-art approaches by more than 12%
on Sketchy dataset and about 3% on TU-Berlin dataset in the retrieval.</p>
</td>
    <td>
      
        Robustness 
      
        Image Retrieval 
      
        Few Shot & Zero Shot 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/schlegel2019adding/">Adding Cues To Binary Feature Descriptors For Visual Place Recognition</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Adding Cues To Binary Feature Descriptors For Visual Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Adding Cues To Binary Feature Descriptors For Visual Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Schlegel Dominik, Grisetti Giorgio</td> <!-- 🔧 You were missing this -->
    <td>2019 International Conference on Robotics and Automation (ICRA)</td>
    <td>6</td>
    <td><p>In this paper we propose an approach to embed continuous and selector cues in
binary feature descriptors used for visual place recognition. The embedding is
achieved by extending each feature descriptor with a binary string that encodes
a cue and supports the Hamming distance metric. Augmenting the descriptors in
such a way has the advantage of being transparent to the procedure used to
compare them. We present two concrete applications of our methodology,
demonstrating the two considered types of cues. In addition to that, we
conducted on these applications a broad quantitative and comparative evaluation
covering five benchmark datasets and several state-of-the-art image retrieval
approaches in combination with various binary descriptor types.</p>
</td>
    <td>
      
        ICRA 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/husain2019remap/">REMAP: Multi-layer Entropy-guided Pooling Of Dense CNN Features For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=REMAP: Multi-layer Entropy-guided Pooling Of Dense CNN Features For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=REMAP: Multi-layer Entropy-guided Pooling Of Dense CNN Features For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Husain Syed Sameed, Bober Miroslaw</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>52</td>
    <td><p>This paper addresses the problem of very large-scale image retrieval,
focusing on improving its accuracy and robustness. We target enhanced
robustness of search to factors such as variations in illumination, object
appearance and scale, partial occlusions, and cluttered backgrounds -
particularly important when search is performed across very large datasets with
significant variability. We propose a novel CNN-based global descriptor, called
REMAP, which learns and aggregates a hierarchy of deep features from multiple
CNN layers, and is trained end-to-end with a triplet loss. REMAP explicitly
learns discriminative features which are mutually-supportive and complementary
at various semantic levels of visual abstraction. These dense local features
are max-pooled spatially at each layer, within multi-scale overlapping regions,
before aggregation into a single image-level descriptor. To identify the
semantically useful regions and layers for retrieval, we propose to measure the
information gain of each region and layer using KL-divergence. Our system
effectively learns during training how useful various regions and layers are
and weights them accordingly. We show that such relative entropy-guided
aggregation outperforms classical CNN-based aggregation controlled by SGD. The
entire framework is trained in an end-to-end fashion, outperforming the latest
state-of-the-art results. On image retrieval datasets Holidays, Oxford and
MPEG, the REMAP descriptor achieves mAP of 95.5%, 91.5%, and 80.1%
respectively, outperforming any results published to date. REMAP also formed
the core of the winning submission to the Google Landmark Retrieval Challenge
on Kaggle.</p>
</td>
    <td>
      
        Evaluation 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/wen2019adversarial/">Adversarial Cross-modal Retrieval Via Learning And Transferring Single-modal Similarities</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Adversarial Cross-modal Retrieval Via Learning And Transferring Single-modal Similarities' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Adversarial Cross-modal Retrieval Via Learning And Transferring Single-modal Similarities' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wen et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE International Conference on Multimedia and Expo (ICME)</td>
    <td>18</td>
    <td><p>Cross-modal retrieval aims to retrieve relevant data across different
modalities (e.g., texts vs. images). The common strategy is to apply
element-wise constraints between manually labeled pair-wise items to guide the
generators to learn the semantic relationships between the modalities, so that
the similar items can be projected close to each other in the common
representation subspace. However, such constraints often fail to preserve the
semantic structure between unpaired but semantically similar items (e.g. the
unpaired items with the same class label are more similar than items with
different labels). To address the above problem, we propose a novel cross-modal
similarity transferring (CMST) method to learn and preserve the semantic
relationships between unpaired items in an unsupervised way. The key idea is to
learn the quantitative similarities in single-modal representation subspace,
and then transfer them to the common representation subspace to establish the
semantic relationships between unpaired items across modalities. Experiments
show that our method outperforms the state-of-the-art approaches both in the
class-based and pair-based retrieval tasks.</p>
</td>
    <td>
      
        Robustness 
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/huang2019accelerate/">Accelerate Learning Of Deep Hashing With Gradient Attention</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Accelerate Learning Of Deep Hashing With Gradient Attention' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Accelerate Learning Of Deep Hashing With Gradient Attention' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Huang Long-kai, Chen, Pan</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>23</td>
    <td><p>Recent years have witnessed the success of learning to hash in fast large-scale image retrieval. As deep learning has shown its superior performance on many computer vision applications, recent designs of learning-based hashing models have been moving from shallow ones to deep architectures. However, based on our analysis, we find that gradient descent based algorithms used in deep hashing models would potentially cause hash codes of a pair of training instances to be updated towards the directions of each other simultaneously during optimization. In the worst case, the paired hash codes switch their directions after update, and consequently, their corresponding distance in the Hamming space remain unchanged. This makes the overall learning process highly inefficient. To address this issue, we propose a new deep hashing model integrated with a novel gradient attention mechanism. Extensive experimental results on three benchmark datasets show that our proposed algorithm is able to accelerate the learning process and obtain competitive retrieval performance compared with state-of-the-art deep hashing models.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/huang2019multi/">Multi-head Attention With Diversity For Learning Grounded Multilingual Multimodal Representations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multi-head Attention With Diversity For Learning Grounded Multilingual Multimodal Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multi-head Attention With Diversity For Learning Grounded Multilingual Multimodal Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Huang Po-yao, Chang Xiaojun, Hauptmann Alexander</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</td>
    <td>17</td>
    <td><p>With the aim of promoting and understanding the multilingual version of image
search, we leverage visual object detection and propose a model with diverse
multi-head attention to learn grounded multilingual multimodal representations.
Specifically, our model attends to different types of textual semantics in two
languages and visual objects for fine-grained alignments between sentences and
images. We introduce a new objective function which explicitly encourages
attention diversity to learn an improved visual-semantic embedding space. We
evaluate our model in the German-Image and English-Image matching tasks on the
Multi30K dataset, and in the Semantic Textual Similarity task with the English
descriptions of visual content. Results show that our model yields a
significant performance gain over other methods in all of the three tasks.</p>
</td>
    <td>
      
        EMNLP 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/teichmann2019detect/">Detect-to-retrieve: Efficient Regional Aggregation For Image Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Detect-to-retrieve: Efficient Regional Aggregation For Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Detect-to-retrieve: Efficient Regional Aggregation For Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Teichmann et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>118</td>
    <td><p>Retrieving object instances among cluttered scenes efficiently requires
compact yet comprehensive regional image representations. Intuitively, object
semantics can help build the index that focuses on the most relevant regions.
However, due to the lack of bounding-box datasets for objects of interest among
retrieval benchmarks, most recent work on regional representations has focused
on either uniform or class-agnostic region selection. In this paper, we first
fill the void by providing a new dataset of landmark bounding boxes, based on
the Google Landmarks dataset, that includes \(86k\) images with manually curated
boxes from \(15k\) unique landmarks. Then, we demonstrate how a trained landmark
detector, using our new dataset, can be leveraged to index image regions and
improve retrieval accuracy while being much more efficient than existing
regional methods. In addition, we introduce a novel regional aggregated
selective match kernel (R-ASMK) to effectively combine information from
detected regions into an improved holistic image representation. R-ASMK boosts
image retrieval accuracy substantially with no dimensionality increase, while
even outperforming systems that index image regions independently. Our complete
image retrieval system improves upon the previous state-of-the-art by
significant margins on the Revisited Oxford and Paris datasets. Code and data
available at the project webpage:
https://github.com/tensorflow/models/tree/master/research/delf.</p>
</td>
    <td>
      
        Image Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/hu2019separated/">Separated Variational Hashing Networks For Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Separated Variational Hashing Networks For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Separated Variational Hashing Networks For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 27th ACM International Conference on Multimedia</td>
    <td>30</td>
    <td><p>Cross-modal hashing, due to its low storage cost and high query speed, has been successfully used for similarity search in multimedia retrieval applications. It projects high-dimensional data into a shared isomorphic Hamming space with similar binary codes for semantically-similar data. In some applications, all modalities may not be obtained or trained simultaneously for some reasons, such as privacy, secret, storage limitation, and computational resource limitation. However, most existing cross-modal hashing methods need all modalities to jointly learn the common Hamming space, thus hindering them from handling these problems. In this paper, we propose a novel approach called Separated Variational Hashing Networks (SVHNs) to overcome the above challenge. Firstly, it adopts a label network (LabNet) to exploit available and nonspecific label annotations to learn a latent common Hamming space by projecting each semantic label into a common binary representation. Then, each modality-specific network can separately map the samples of the corresponding modality into their binary semantic codes learned by LabNet. We achieve it by conducting variational inference to match the aggregated posterior of the hashing code of LabNet with an arbitrary prior distribution. The effectiveness and efficiency of our SVHNs are verified by extensive experiments carried out on four widely-used multimedia databases, in comparison with 11 state-of-the-art approaches.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/hoang2019simultaneous/">Simultaneous Compression And Quantization: A Joint Approach For Efficient Unsupervised Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Simultaneous Compression And Quantization: A Joint Approach For Efficient Unsupervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Simultaneous Compression And Quantization: A Joint Approach For Efficient Unsupervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hoang et al.</td> <!-- 🔧 You were missing this -->
    <td>Computer Vision and Image Understanding</td>
    <td>11</td>
    <td><p>For unsupervised data-dependent hashing, the two most important requirements
are to preserve similarity in the low-dimensional feature space and to minimize
the binary quantization loss. A well-established hashing approach is Iterative
Quantization (ITQ), which addresses these two requirements in separate steps.
In this paper, we revisit the ITQ approach and propose novel formulations and
algorithms to the problem. Specifically, we propose a novel approach, named
Simultaneous Compression and Quantization (SCQ), to jointly learn to compress
(reduce dimensionality) and binarize input data in a single formulation under
strict orthogonal constraint. With this approach, we introduce a loss function
and its relaxed version, termed Orthonormal Encoder (OnE) and Orthogonal
Encoder (OgE) respectively, which involve challenging binary and orthogonal
constraints. We propose to attack the optimization using novel algorithms based
on recent advances in cyclic coordinate descent approach. Comprehensive
experiments on unsupervised image retrieval demonstrate that our proposed
methods consistently outperform other state-of-the-art hashing methods.
Notably, our proposed methods outperform recent deep neural networks and GAN
based hashing in accuracy, while being very computationally-efficient.</p>
</td>
    <td>
      
        Quantization 
      
        Unsupervised 
      
        Neural Hashing 
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/hegde2019similar/">Similar Image Search For Histopathology: SMILY</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Similar Image Search For Histopathology: SMILY' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Similar Image Search For Histopathology: SMILY' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hegde et al.</td> <!-- 🔧 You were missing this -->
    <td>npj Digital Medicine</td>
    <td>137</td>
    <td><p>The increasing availability of large institutional and public histopathology
image datasets is enabling the searching of these datasets for diagnosis,
research, and education. Though these datasets typically have associated
metadata such as diagnosis or clinical notes, even carefully curated datasets
rarely contain annotations of the location of regions of interest on each
image. Because pathology images are extremely large (up to 100,000 pixels in
each dimension), further laborious visual search of each image may be needed to
find the feature of interest. In this paper, we introduce a deep learning based
reverse image search tool for histopathology images: Similar Medical Images
Like Yours (SMILY). We assessed SMILY’s ability to retrieve search results in
two ways: using pathologist-provided annotations, and via prospective studies
where pathologists evaluated the quality of SMILY search results. As a negative
control in the second evaluation, pathologists were blinded to whether search
results were retrieved by SMILY or randomly. In both types of assessments,
SMILY was able to retrieve search results with similar histologic features,
organ site, and prostate cancer Gleason grade compared with the original query.
SMILY may be a useful general-purpose tool in the pathologist’s arsenal, to
improve the efficiency of searching large archives of histopathology images,
without the need to develop and implement specific tools for each application.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/he2019view/">View N-gram Network For 3D Object Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=View N-gram Network For 3D Object Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=View N-gram Network For 3D Object Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>He et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>56</td>
    <td><p>How to aggregate multi-view representations of a 3D object into an
informative and discriminative one remains a key challenge for multi-view 3D
object retrieval. Existing methods either use view-wise pooling strategies
which neglect the spatial information across different views or employ
recurrent neural networks which may face the efficiency problem. To address
these issues, we propose an effective and efficient framework called View
N-gram Network (VNN). Inspired by n-gram models in natural language processing,
VNN divides the view sequence into a set of visual n-grams, which involve
overlapping consecutive view sub-sequences. By doing so, spatial information
across multiple views is captured, which helps to learn a discriminative global
embedding for each 3D object. Experiments on 3D shape retrieval benchmarks,
including ModelNet10, ModelNet40 and ShapeNetCore55 datasets, demonstrate the
superiority of our proposed method.</p>
</td>
    <td>
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/he2019one/">One Network For Multi-domains: Domain Adaptive Hashing With Intersectant Generative Adversarial Network</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=One Network For Multi-domains: Domain Adaptive Hashing With Intersectant Generative Adversarial Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=One Network For Multi-domains: Domain Adaptive Hashing With Intersectant Generative Adversarial Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>He et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</td>
    <td>9</td>
    <td><p>With the recent explosive increase of digital data, image recognition and
retrieval become a critical practical application. Hashing is an effective
solution to this problem, due to its low storage requirement and high query
speed. However, most of past works focus on hashing in a single (source)
domain. Thus, the learned hash function may not adapt well in a new (target)
domain that has a large distributional difference with the source domain. In
this paper, we explore an end-to-end domain adaptive learning framework that
simultaneously and precisely generates discriminative hash codes and classifies
target domain images. Our method encodes two domains images into a semantic
common space, followed by two independent generative adversarial networks
arming at crosswise reconstructing two domains’ images, reducing domain
disparity and improving alignment in the shared space. We evaluate our
framework on {four} public benchmark datasets, all of which show that our
method is superior to the other state-of-the-art methods on the tasks of object
recognition and image retrieval.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Robustness 
      
        AAAI 
      
        IJCAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/harpeled2019near/">Near Neighbor: Who Is The Fairest Of Them All?</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Near Neighbor: Who Is The Fairest Of Them All?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Near Neighbor: Who Is The Fairest Of Them All?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Har-peled Sariel, Mahabadi Sepideh</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>8</td>
    <td><p>\(\newcommand{\ball}{\mathbb{B}}\newcommand{\dsQ}{{\mathcal{Q}}}\newcommand{\dsS}{{\mathcal{S}}}\)In
this work we study a fair variant of the near neighbor problem. Namely, given a
set of \(n\) points \(P\) and a parameter \(r\), the goal is to preprocess the
points, such that given a query point \(q\), any point in the \(r\)-neighborhood of
the query, i.e., \(\ball(q,r)\), have the same probability of being reported as
the near neighbor.
  We show that LSH based algorithms can be made fair, without a significant
loss in efficiency. Specifically, we show an algorithm that reports a point in
the \(r\)-neighborhood of a query \(q\) with almost uniform probability. The query
time is proportional to \(O\bigl( \mathrm{dns}(q.r) \dsQ(n,c) \bigr)\), and its
space is \(O(\dsS(n,c))\), where \(\dsQ(n,c)\) and \(\dsS(n,c)\) are the query time
and space of an LSH algorithm for \(c\)-approximate near neighbor, and
\(\mathrm{dns}(q,r)\) is a function of the local density around \(q\).
  Our approach works more generally for sampling uniformly from a
sub-collection of sets of a given collection and can be used in a few other
applications. Finally, we run experiments to show performance of our approach
on real data.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/he2019k/">K-nearest Neighbors Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=K-nearest Neighbors Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=K-nearest Neighbors Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>He Xiangyu, Wang, Cheng</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>31</td>
    <td><p>Hashing based approximate nearest neighbor search embeds high dimensional data to compact binary codes, which
enables efficient similarity search and storage. However,
the non-isometry sign(·) function makes it hard to project
the nearest neighbors in continuous data space into the
closest codewords in discrete Hamming space. In this work,
we revisit the sign(·) function from the perspective of space partitioning.
In specific, we bridge the gap between
k-nearest neighbors and binary hashing codes with Shannon entropy. We further propose a novel K-Nearest Neighbors Hashing (KNNH) method to learn binary representations from KNN within the subspaces generated by sign(·).
Theoretical and experimental results show that the KNN relation is of central importance to neighbor preserving embeddings, and the proposed method outperforms the state-of-the-arts on benchmark datasets.</p>
</td>
    <td>
      
        Hashing Methods 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/yu2019discriminative/">Discriminative Supervised Hashing For Cross-modal Similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Discriminative Supervised Hashing For Cross-modal Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Discriminative Supervised Hashing For Cross-modal Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yu Jun, Wu Xiao-jun, Kittler Josef</td> <!-- 🔧 You were missing this -->
    <td>Image and Vision Computing</td>
    <td>15</td>
    <td><p>With the advantage of low storage cost and high retrieval efficiency, hashing
techniques have recently been an emerging topic in cross-modal similarity
search. As multiple modal data reflect similar semantic content, many
researches aim at learning unified binary codes. However, discriminative
hashing features learned by these methods are not adequate. This results in
lower accuracy and robustness. We propose a novel hashing learning framework
which jointly performs classifier learning, subspace learning and matrix
factorization to preserve class-specific semantic content, termed
Discriminative Supervised Hashing (DSH), to learn the discrimative unified
binary codes for multi-modal data. Besides, reducing the loss of information
and preserving the non-linear structure of data, DSH non-linearly projects
different modalities into the common space in which the similarity among
heterogeneous data points can be measured. Extensive experiments conducted on
the three publicly available datasets demonstrate that the framework proposed
in this paper outperforms several state-of -the-art methods.</p>
</td>
    <td>
      
        Unsupervised 
      
        Neural Hashing 
      
        SUPERVISED 
      
        Similarity Search 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/sun2019multi/">Multi-graph Convolution Collaborative Filtering</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multi-graph Convolution Collaborative Filtering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multi-graph Convolution Collaborative Filtering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sun et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE International Conference on Data Mining (ICDM)</td>
    <td>127</td>
    <td><p>Personalized recommendation is ubiquitous, playing an important role in many
online services. Substantial research has been dedicated to learning vector
representations of users and items with the goal of predicting a user’s
preference for an item based on the similarity of the representations.
Techniques range from classic matrix factorization to more recent deep learning
based methods. However, we argue that existing methods do not make full use of
the information that is available from user-item interaction data and the
similarities between user pairs and item pairs. In this work, we develop a
graph convolution-based recommendation framework, named Multi-Graph Convolution
Collaborative Filtering (Multi-GCCF), which explicitly incorporates multiple
graphs in the embedding learning process. Multi-GCCF not only expressively
models the high-order information via a partite user-item interaction graph,
but also integrates the proximal information by building and processing
user-user and item-item graphs. Furthermore, we consider the intrinsic
difference between user nodes and item nodes when performing graph convolution
on the bipartite graph. We conduct extensive experiments on four publicly
accessible benchmarks, showing significant improvements relative to several
state-of-the-art collaborative filtering and graph neural network-based
recommendation models. Further experiments quantitatively verify the
effectiveness of each component of our proposed model and demonstrate that the
learned embeddings capture the important relationship structure.</p>
</td>
    <td>
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/hamann2019hamming/">Hamming Sentence Embeddings For Information Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hamming Sentence Embeddings For Information Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hamming Sentence Embeddings For Information Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hamann Felix, Kurz Nadja, Ulges Adrian</td> <!-- 🔧 You were missing this -->
    <td>2023 International Conference on Advancement in Computation &amp; Computer Technologies (InCACCT)</td>
    <td>6</td>
    <td><p>In retrieval applications, binary hashes are known to offer significant
improvements in terms of both memory and speed. We investigate the compression
of sentence embeddings using a neural encoder-decoder architecture, which is
trained by minimizing reconstruction error. Instead of employing the original
real-valued embeddings, we use latent representations in Hamming space produced
by the encoder for similarity calculations.
  In quantitative experiments on several benchmarks for semantic similarity
tasks, we show that our compressed hamming embeddings yield a comparable
performance to uncompressed embeddings (Sent2Vec, InferSent, Glove-BoW), at
compression ratios of up to 256:1. We further demonstrate that our model
strongly decorrelates input features, and that the compressor generalizes well
when pre-trained on Wikipedia sentences. We publish the source code on Github
and all experimental results.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/hansen2019unsupervised/">Unsupervised Neural Generative Semantic Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Neural Generative Semantic Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Neural Generative Semantic Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hansen et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>25</td>
    <td><p>Fast similarity search is a key component in large-scale information
retrieval, where semantic hashing has become a popular strategy for
representing documents as binary hash codes. Recent advances in this area have
been obtained through neural network based models: generative models trained by
learning to reconstruct the original documents. We present a novel unsupervised
generative semantic hashing approach, \textit{Ranking based Semantic Hashing}
(RBSH) that consists of both a variational and a ranking based component.
Similarly to variational autoencoders, the variational component is trained to
reconstruct the original document conditioned on its generated hash code, and
as in prior work, it only considers documents individually. The ranking
component solves this limitation by incorporating inter-document similarity
into the hash code generation, modelling document ranking through a hinge loss.
To circumvent the need for labelled data to compute the hinge loss, we use a
weak labeller and thus keep the approach fully unsupervised.
  Extensive experimental evaluation on four publicly available datasets against
traditional baselines and recent state-of-the-art methods for semantic hashing
shows that RBSH significantly outperforms all other methods across all
evaluated hash code lengths. In fact, RBSH hash codes are able to perform
similarly to state-of-the-art hash codes while using 2-4x fewer bits.</p>
</td>
    <td>
      
        Text Retrieval 
      
        Unsupervised 
      
        SUPERVISED 
      
        SIGIR 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/sun2019geocapsnet/">Geocapsnet: Aerial To Ground View Image Geo-localization Using Capsule Network</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Geocapsnet: Aerial To Ground View Image Geo-localization Using Capsule Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Geocapsnet: Aerial To Ground View Image Geo-localization Using Capsule Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sun et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>7</td>
    <td><p>The task of cross-view image geo-localization aims to determine the
geo-location (GPS coordinates) of a query ground-view image by matching it with
the GPS-tagged aerial (satellite) images in a reference dataset. Due to the
dramatic changes of viewpoint, matching the cross-view images is challenging.
In this paper, we propose the GeoCapsNet based on the capsule network for
ground-to-aerial image geo-localization. The network first extracts features
from both ground-view and aerial images via standard convolution layers and the
capsule layers further encode the features to model the spatial feature
hierarchies and enhance the representation power. Moreover, we introduce a
simple and effective weighted soft-margin triplet loss with online batch hard
sample mining, which can greatly improve image retrieval accuracy. Experimental
results show that our GeoCapsNet significantly outperforms the state-of-the-art
approaches on two benchmark datasets.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/guo2019hierarchical/">Hierarchical Document Encoder For Parallel Corpus Mining</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hierarchical Document Encoder For Parallel Corpus Mining' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hierarchical Document Encoder For Parallel Corpus Mining' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Guo et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)</td>
    <td>22</td>
    <td><p>We explore using multilingual document embeddings for nearest neighbor mining
of parallel data. Three document-level representations are investigated: (i)
document embeddings generated by simply averaging multilingual sentence
embeddings; (ii) a neural bag-of-words (BoW) document encoding model; (iii) a
hierarchical multilingual document encoder (HiDE) that builds on our
sentence-level model. The results show document embeddings derived from
sentence-level averaging are surprisingly effective for clean datasets, but
suggest models trained hierarchically at the document-level are more effective
on noisy data. Analysis experiments demonstrate our hierarchical models are
very robust to variations in the underlying sentence embedding quality. Using
document embeddings trained with HiDE achieves state-of-the-art performance on
United Nations (UN) parallel document mining, 94.9% P@1 for en-fr and 97.3% P@1
for en-es.</p>
</td>
    <td>
      
        WMT 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/guo2019accelerating/">Accelerating Large-scale Inference With Anisotropic Vector Quantization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Accelerating Large-scale Inference With Anisotropic Vector Quantization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Accelerating Large-scale Inference With Anisotropic Vector Quantization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Guo et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>91</td>
    <td><p>Quantization based techniques are the current state-of-the-art for scaling
maximum inner product search to massive databases. Traditional approaches to
quantization aim to minimize the reconstruction error of the database points.
Based on the observation that for a given query, the database points that have
the largest inner products are more relevant, we develop a family of
anisotropic quantization loss functions. Under natural statistical assumptions,
we show that quantization with these loss functions leads to a new variant of
vector quantization that more greatly penalizes the parallel component of a
datapoint’s residual relative to its orthogonal component. The proposed
approach achieves state-of-the-art results on the public benchmarks available
at \url{ann-benchmarks.com}.</p>
</td>
    <td>
      
        Quantization 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/yao2019efficient/">Efficient Discrete Supervised Hashing For Large-scale Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Discrete Supervised Hashing For Large-scale Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient Discrete Supervised Hashing For Large-scale Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yao et al.</td> <!-- 🔧 You were missing this -->
    <td>Neurocomputing</td>
    <td>29</td>
    <td><p>Supervised cross-modal hashing has gained increasing research interest on
large-scale retrieval task owning to its satisfactory performance and
efficiency. However, it still has some challenging issues to be further
studied: 1) most of them fail to well preserve the semantic correlations in
hash codes because of the large heterogenous gap; 2) most of them relax the
discrete constraint on hash codes, leading to large quantization error and
consequent low performance; 3) most of them suffer from relatively high memory
cost and computational complexity during training procedure, which makes them
unscalable. In this paper, to address above issues, we propose a supervised
cross-modal hashing method based on matrix factorization dubbed Efficient
Discrete Supervised Hashing (EDSH). Specifically, collective matrix
factorization on heterogenous features and semantic embedding with class labels
are seamlessly integrated to learn hash codes. Therefore, the feature based
similarities and semantic correlations can be both preserved in hash codes,
which makes the learned hash codes more discriminative. Then an efficient
discrete optimal algorithm is proposed to handle the scalable issue. Instead of
learning hash codes bit-by-bit, hash codes matrix can be obtained directly
which is more efficient. Extensive experimental results on three public
real-world datasets demonstrate that EDSH produces a superior performance in
both accuracy and scalability over some existing cross-modal hashing methods.</p>
</td>
    <td>
      
        Unsupervised 
      
        Neural Hashing 
      
        SUPERVISED 
      
        Multimodal Retrieval 
      
        Hashing Methods 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/guan2019post/">Post-training 4-bit Quantization On Embedding Tables</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Post-training 4-bit Quantization On Embedding Tables' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Post-training 4-bit Quantization On Embedding Tables' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Guan et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>16</td>
    <td><p>Continuous representations have been widely adopted in recommender systems
where a large number of entities are represented using embedding vectors. As
the cardinality of the entities increases, the embedding components can easily
contain millions of parameters and become the bottleneck in both storage and
inference due to large memory consumption. This work focuses on post-training
4-bit quantization on the continuous embeddings. We propose row-wise uniform
quantization with greedy search and codebook-based quantization that
consistently outperforms state-of-the-art quantization approaches on reducing
accuracy degradation. We deploy our uniform quantization technique on a
production model in Facebook and demonstrate that it can reduce the model size
to only 13.89% of the single-precision version while the model quality stays
neutral.</p>
</td>
    <td>
      
        Quantization 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/gomez2019self/">Self-supervised Learning From Web Data For Multimodal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Self-supervised Learning From Web Data For Multimodal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Self-supervised Learning From Web Data For Multimodal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gomez et al.</td> <!-- 🔧 You were missing this -->
    <td>Multimodal Scene Understanding</td>
    <td>13</td>
    <td><p>Self-Supervised learning from multimodal image and text data allows deep
neural networks to learn powerful features with no need of human annotated
data. Web and Social Media platforms provide a virtually unlimited amount of
this multimodal data. In this work we propose to exploit this free available
data to learn a multimodal image and text embedding, aiming to leverage the
semantic knowledge learnt in the text domain and transfer it to a visual model
for semantic image retrieval. We demonstrate that the proposed pipeline can
learn from images with associated textwithout supervision and analyze the
semantic structure of the learnt joint image and text embedding space. We
perform a thorough analysis and performance comparison of five different state
of the art text embeddings in three different benchmarks. We show that the
embeddings learnt with Web and Social Media data have competitive performances
over supervised methods in the text based image retrieval task, and we clearly
outperform state of the art in the MIRFlickr dataset when training in the
target data. Further, we demonstrate how semantic multimodal image retrieval
can be performed using the learnt embeddings, going beyond classical
instance-level retrieval problems. Finally, we present a new dataset,
InstaCities1M, composed by Instagram images and their associated texts that can
be used for fair comparison of image-text embeddings.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Multimodal Retrieval 
      
        Self SUPERVISED 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/gomez2019learning/">Learning To Learn From Web Data Through Deep Semantic Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning To Learn From Web Data Through Deep Semantic Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning To Learn From Web Data Through Deep Semantic Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gomez et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>23</td>
    <td><p>In this paper we propose to learn a multimodal image and text embedding from
Web and Social Media data, aiming to leverage the semantic knowledge learnt in
the text domain and transfer it to a visual model for semantic image retrieval.
We demonstrate that the pipeline can learn from images with associated text
without supervision and perform a thourough analysis of five different text
embeddings in three different benchmarks. We show that the embeddings learnt
with Web and Social Media data have competitive performances over supervised
methods in the text based image retrieval task, and we clearly outperform state
of the art in the MIRFlickr dataset when training in the target data. Further
we demonstrate how semantic multimodal image retrieval can be performed using
the learnt embeddings, going beyond classical instance-level retrieval
problems. Finally, we present a new dataset, InstaCities1M, composed by
Instagram images and their associated texts that can be used for fair
comparison of image-text embeddings.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/gominski2019challenging/">Challenging Deep Image Descriptors For Retrieval In Heterogeneous Iconographic Collections</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Challenging Deep Image Descriptors For Retrieval In Heterogeneous Iconographic Collections' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Challenging Deep Image Descriptors For Retrieval In Heterogeneous Iconographic Collections' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gominski et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 1st Workshop on Structuring and Understanding of Multimedia heritAge Contents</td>
    <td>7</td>
    <td><p>This article proposes to study the behavior of recent and efficient
state-of-the-art deep-learning based image descriptors for content-based image
retrieval, facing a panel of complex variations appearing in heterogeneous
image datasets, in particular in cultural collections that may involve
multi-source, multi-date and multi-view Permission to make digital</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/gillick2019learning/">Learning Dense Representations For Entity Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Dense Representations For Entity Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Dense Representations For Entity Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gillick et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</td>
    <td>189</td>
    <td><p>We show that it is feasible to perform entity linking by training a dual
encoder (two-tower) model that encodes mentions and entities in the same dense
vector space, where candidate entities are retrieved by approximate nearest
neighbor search. Unlike prior work, this setup does not rely on an alias table
followed by a re-ranker, and is thus the first fully learned entity retrieval
model. We show that our dual encoder, trained using only anchor-text links in
Wikipedia, outperforms discrete alias table and BM25 baselines, and is
competitive with the best comparable results on the standard TACKBP-2010
dataset. In addition, it can retrieve candidates extremely fast, and
generalizes well to a new dataset derived from Wikinews. On the modeling side,
we demonstrate the dramatic value of an unsupervised negative mining algorithm
for this task.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/gildenblat2019self/">Self-supervised Similarity Learning For Digital Pathology</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Self-supervised Similarity Learning For Digital Pathology' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Self-supervised Similarity Learning For Digital Pathology' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gildenblat Jacob, Klaiman Eldad</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>36</td>
    <td><p>Using features extracted from networks pretrained on ImageNet is a common
practice in applications of deep learning for digital pathology. However it
presents the downside of missing domain specific image information. In digital
pathology, supervised training data is expensive and difficult to collect. We
propose a self-supervised method for feature extraction by similarity learning
on whole slide images (WSI) that is simple to implement and allows creation of
robust and compact image descriptors. We train a siamese network, exploiting
image spatial continuity and assuming spatially adjacent tiles in the image are
more similar to each other than distant tiles. Our network outputs feature
vectors of length 128, which allows dramatically lower memory storage and
faster processing than networks pretrained on ImageNet. We apply the method on
digital pathology WSIs from the Camelyon16 train set and assess and compare our
method by measuring image retrieval of tumor tiles and descriptor pair distance
ratio for distant/near tiles in the Camelyon16 test set. We show that our
method yields better retrieval task results than existing ImageNet based and
generic self-supervised feature extraction methods. To the best of our
knowledge, this is also the first published method for self-supervised learning
tailored for digital pathology.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Self SUPERVISED 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/wang2019deep/">Deep Metric Learning By Online Soft Mining And Class-aware Attention</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Metric Learning By Online Soft Mining And Class-aware Attention' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Metric Learning By Online Soft Mining And Class-aware Attention' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>45</td>
    <td><p>Deep metric learning aims to learn a deep embedding that can capture the
semantic similarity of data points. Given the availability of massive training
samples, deep metric learning is known to suffer from slow convergence due to a
large fraction of trivial samples. Therefore, most existing methods generally
resort to sample mining strategies for selecting nontrivial samples to
accelerate convergence and improve performance. In this work, we identify two
critical limitations of the sample mining methods, and provide solutions for
both of them. First, previous mining methods assign one binary score to each
sample, i.e., dropping or keeping it, so they only selects a subset of relevant
samples in a mini-batch. Therefore, we propose a novel sample mining method,
called Online Soft Mining (OSM), which assigns one continuous score to each
sample to make use of all samples in the mini-batch. OSM learns extended
manifolds that preserve useful intraclass variances by focusing on more similar
positives. Second, the existing methods are easily influenced by outliers as
they are generally included in the mined subset. To address this, we introduce
Class-Aware Attention (CAA) that assigns little attention to abnormal data
samples. Furthermore, by combining OSM and CAA, we propose a novel weighted
contrastive loss to learn discriminative embeddings. Extensive experiments on
two fine-grained visual categorisation datasets and two video-based person
re-identification benchmarks show that our method significantly outperforms the
state-of-the-art.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/morozov2019unsupervised/">Unsupervised Neural Quantization For Compressed-domain Similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Neural Quantization For Compressed-domain Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Neural Quantization For Compressed-domain Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Morozov Stanislav, Babenko Artem</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>21</td>
    <td><p>We tackle the problem of unsupervised visual descriptors compression, which
is a key ingredient of large-scale image retrieval systems. While the deep
learning machinery has benefited literally all computer vision pipelines, the
existing state-of-the-art compression methods employ shallow architectures, and
we aim to close this gap by our paper. In more detail, we introduce a DNN
architecture for the unsupervised compressed-domain retrieval, based on
multi-codebook quantization. The proposed architecture is designed to
incorporate both fast data encoding and efficient distances computation via
lookup tables. We demonstrate the exceptional advantage of our scheme over
existing quantization approaches on several datasets of visual descriptors via
outperforming the previous state-of-the-art by a large margin.</p>
</td>
    <td>
      
        Quantization 
      
        Unsupervised 
      
        SUPERVISED 
      
        Similarity Search 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/gattupalli2019weakly/">Weakly Supervised Deep Image Hashing Through Tag Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Weakly Supervised Deep Image Hashing Through Tag Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Weakly Supervised Deep Image Hashing Through Tag Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gattupalli Vijetha, Zhuo Yaoxin, Li Baoxin</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>42</td>
    <td><p>Many approaches to semantic image hashing have been formulated as supervised
learning problems that utilize images and label information to learn the binary
hash codes. However, large-scale labeled image data is expensive to obtain,
thus imposing a restriction on the usage of such algorithms. On the other hand,
unlabelled image data is abundant due to the existence of many Web image
repositories. Such Web images may often come with images tags that contain
useful information, although raw tags, in general, do not readily lead to
semantic labels. Motivated by this scenario, we formulate the problem of
semantic image hashing as a weakly-supervised learning problem. We utilize the
information contained in the user-generated tags associated with the images to
learn the hash codes. More specifically, we extract the word2vec semantic
embeddings of the tags and use the information contained in them for
constraining the learning. Accordingly, we name our model Weakly Supervised
Deep Hashing using Tag Embeddings (WDHT). WDHT is tested for the task of
semantic image retrieval and is compared against several state-of-art models.
Results show that our approach sets a new state-of-art in the area of weekly
supervised image hashing.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
        Image Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/garcia2019how/">How To Read Paintings: Semantic Art Understanding With Multi-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=How To Read Paintings: Semantic Art Understanding With Multi-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=How To Read Paintings: Semantic Art Understanding With Multi-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Garcia Noa, Vogiatzis George</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>93</td>
    <td><p>Automatic art analysis has been mostly focused on classifying artworks into
different artistic styles. However, understanding an artistic representation
involves more complex processes, such as identifying the elements in the scene
or recognizing author influences. We present SemArt, a multi-modal dataset for
semantic art understanding. SemArt is a collection of fine-art painting images
in which each image is associated to a number of attributes and a textual
artistic comment, such as those that appear in art catalogues or museum
collections. To evaluate semantic art understanding, we envisage the Text2Art
challenge, a multi-modal retrieval task where relevant paintings are retrieved
according to an artistic text, and vice versa. We also propose several models
for encoding visual and textual artistic representations into a common semantic
space. Our best approach is able to find the correct image within the top 10
ranked images in the 45.5% of the test samples. Moreover, our models show
remarkable levels of art understanding when compared against human evaluation.</p>
</td>
    <td>
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/garcia2019context/">Context-aware Embeddings For Automatic Art Analysis</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Context-aware Embeddings For Automatic Art Analysis' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Context-aware Embeddings For Automatic Art Analysis' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Garcia Noa, Renoust Benjamin, Nakashima Yuta</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2019 on International Conference on Multimedia Retrieval</td>
    <td>26</td>
    <td><p>Automatic art analysis aims to classify and retrieve artistic representations
from a collection of images by using computer vision and machine learning
techniques. In this work, we propose to enhance visual representations from
neural networks with contextual artistic information. Whereas visual
representations are able to capture information about the content and the style
of an artwork, our proposed context-aware embeddings additionally encode
relationships between different artistic attributes, such as author, school, or
historical period. We design two different approaches for using context in
automatic art analysis. In the first one, contextual data is obtained through a
multi-task learning model, in which several attributes are trained together to
find visual relationships between elements. In the second approach, context is
obtained through an art-specific knowledge graph, which encodes relationships
between artistic attributes. An exhaustive evaluation of both of our models in
several art analysis problems, such as author identification, type
classification, or cross-modal retrieval, show that performance is improved by
up to 7.3% in art classification and 37.24% in retrieval when context-aware
embeddings are used.</p>
</td>
    <td>
      
        Multimodal Retrieval 
      
        Medical Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/garcia2019learning/">Learning Non-metric Visual Similarity For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Non-metric Visual Similarity For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Non-metric Visual Similarity For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Garcia Noa, Vogiatzis George</td> <!-- 🔧 You were missing this -->
    <td>Image and Vision Computing</td>
    <td>38</td>
    <td><p>Measuring visual similarity between two or more instances within a data
distribution is a fundamental task in image retrieval. Theoretically,
non-metric distances are able to generate a more complex and accurate
similarity model than metric distances, provided that the non-linear data
distribution is precisely captured by the system. In this work, we explore
neural networks models for learning a non-metric similarity function for
instance search. We argue that non-metric similarity functions based on neural
networks can build a better model of human visual perception than standard
metric distances. As our proposed similarity function is differentiable, we
explore a real end-to-end trainable approach for image retrieval, i.e. we learn
the weights from the input image pixels to the final similarity score.
Experimental evaluation shows that non-metric similarity networks are able to
learn visual similarities between images and improve performance on top of
state-of-the-art image representations, boosting results in standard image
retrieval datasets with respect standard metric distances.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/vemulapalli2019compact/">A Compact Embedding For Facial Expression Similarity</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Compact Embedding For Facial Expression Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Compact Embedding For Facial Expression Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Vemulapalli Raviteja, Agarwala Aseem</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>102</td>
    <td><p>Most of the existing work on automatic facial expression analysis focuses on
discrete emotion recognition, or facial action unit detection. However, facial
expressions do not always fall neatly into pre-defined semantic categories.
Also, the similarity between expressions measured in the action unit space need
not correspond to how humans perceive expression similarity. Different from
previous work, our goal is to describe facial expressions in a continuous
fashion using a compact embedding space that mimics human visual preferences.
To achieve this goal, we collect a large-scale faces-in-the-wild dataset with
human annotations in the form: Expressions A and B are visually more similar
when compared to expression C, and use this dataset to train a neural network
that produces a compact (16-dimensional) expression embedding. We
experimentally demonstrate that the learned embedding can be successfully used
for various applications such as expression retrieval, photo album
summarization, and emotion recognition. We also show that the embedding learned
using the proposed dataset performs better than several other embeddings
learned using existing emotion or action unit datasets.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/teofili2019lucene/">Lucene For Approximate Nearest-neighbors Search On Arbitrary Dense Vectors</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Lucene For Approximate Nearest-neighbors Search On Arbitrary Dense Vectors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Lucene For Approximate Nearest-neighbors Search On Arbitrary Dense Vectors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Teofili Tommaso, Lin Jimmy</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the first ACM workshop on Information hiding and multimedia security</td>
    <td>7</td>
    <td><p>We demonstrate three approaches for adapting the open-source Lucene search
library to perform approximate nearest-neighbor search on arbitrary dense
vectors, using similarity search on word embeddings as a case study. At its
core, Lucene is built around inverted indexes of a document collection’s
(sparse) term-document matrix, which is incompatible with the lower-dimensional
dense vectors that are common in deep learning applications. We evaluate three
techniques to overcome these challenges that can all be natively integrated
into Lucene: the creation of documents populated with fake words, LSH applied
to lexical realizations of dense vectors, and k-d trees coupled with
dimensionality reduction. Experiments show that the “fake words” approach
represents the best balance between effectiveness and efficiency. These
techniques are integrated into the Anserini open-source toolkit and made
available to the community.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/gao2019beyond/">Beyond Product Quantization: Deep Progressive Quantization For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Beyond Product Quantization: Deep Progressive Quantization For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Beyond Product Quantization: Deep Progressive Quantization For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gao et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</td>
    <td>28</td>
    <td><p>Product Quantization (PQ) has long been a mainstream for generating an
exponentially large codebook at very low memory/time cost. Despite its success,
PQ is still tricky for the decomposition of high-dimensional vector space, and
the retraining of model is usually unavoidable when the code length changes. In
this work, we propose a deep progressive quantization (DPQ) model, as an
alternative to PQ, for large scale image retrieval. DPQ learns the quantization
codes sequentially and approximates the original feature space progressively.
Therefore, we can train the quantization codes with different code lengths
simultaneously. Specifically, we first utilize the label information for
guiding the learning of visual features, and then apply several quantization
blocks to progressively approach the visual features. Each quantization block
is designed to be a layer of a convolutional neural network, and the whole
framework can be trained in an end-to-end manner. Experimental results on the
benchmark datasets show that our model significantly outperforms the
state-of-the-art for image retrieval. Our model is trained once for different
code lengths and therefore requires less computation time. Additional ablation
study demonstrates the effect of each component of our proposed model. Our code
is released at https://github.com/cfm-uestc/DPQ.</p>
</td>
    <td>
      
        Quantization 
      
        Image Retrieval 
      
        AAAI 
      
        IJCAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/zeng2019audio/">Audio-visual Embedding For Cross-modal Musicvideo Retrieval Through Supervised Deep CCA</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Audio-visual Embedding For Cross-modal Musicvideo Retrieval Through Supervised Deep CCA' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Audio-visual Embedding For Cross-modal Musicvideo Retrieval Through Supervised Deep CCA' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zeng Donghuo, Yu Yi, Oyama Keizo</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE International Symposium on Multimedia (ISM)</td>
    <td>33</td>
    <td><p>Deep learning has successfully shown excellent performance in learning joint
representations between different data modalities. Unfortunately, little
research focuses on cross-modal correlation learning where temporal structures
of different data modalities, such as audio and video, should be taken into
account. Music video retrieval by given musical audio is a natural way to
search and interact with music contents. In this work, we study cross-modal
music video retrieval in terms of emotion similarity. Particularly, audio of an
arbitrary length is used to retrieve a longer or full-length music video. To
this end, we propose a novel audio-visual embedding algorithm by Supervised
Deep CanonicalCorrelation Analysis (S-DCCA) that projects audio and video into
a shared space to bridge the semantic gap between audio and video. This also
preserves the similarity between audio and visual contents from different
videos with the same class label and the temporal structure. The contribution
of our approach is mainly manifested in the two aspects: i) We propose to
select top k audio chunks by attention-based Long Short-Term Memory
(LSTM)model, which can represent good audio summarization with local
properties. ii) We propose an end-to-end deep model for cross-modal
audio-visual learning where S-DCCA is trained to learn the semantic correlation
between audio and visual modalities. Due to the lack of music video dataset, we
construct 10K music video dataset from YouTube 8M dataset. Some promising
results such as MAP and precision-recall show that our proposed model can be
applied to music video retrieval.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Video Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/jia2019efficient/">Efficient Task-specific Data Valuation For Nearest Neighbor Algorithms</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Task-specific Data Valuation For Nearest Neighbor Algorithms' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient Task-specific Data Valuation For Nearest Neighbor Algorithms' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jia et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the VLDB Endowment</td>
    <td>129</td>
    <td><p>Given a data set \(\mathcal{D}\) containing millions of data points and a data
consumer who is willing to pay for $\(X\) to train a machine learning (ML) model
over \(\mathcal{D}\), how should we distribute this $\(X\) to each data point to
reflect its “value”? In this paper, we define the “relative value of data” via
the Shapley value, as it uniquely possesses properties with appealing
real-world interpretations, such as fairness, rationality and
decentralizability. For general, bounded utility functions, the Shapley value
is known to be challenging to compute: to get Shapley values for all \(N\) data
points, it requires \(O(2^N)\) model evaluations for exact computation and
\(O(Nlog N)\) for \((\epsilon, \delta)\)-approximation. In this paper, we focus on
one popular family of ML models relying on \(K\)-nearest neighbors (\(K\)NN). The
most surprising result is that for unweighted \(K\)NN classifiers and regressors,
the Shapley value of all \(N\) data points can be computed, exactly, in \(O(Nlog
N)\) time – an exponential improvement on computational complexity! Moreover,
for \((\epsilon, \delta)\)-approximation, we are able to develop an algorithm
based on Locality Sensitive Hashing (LSH) with only sublinear complexity
\(O(N^{h(\epsilon,K)}log N)\) when \(\epsilon\) is not too small and \(K\) is not
too large. We empirically evaluate our algorithms on up to \(10\) million data
points and even our exact algorithm is up to three orders of magnitude faster
than the baseline approximation algorithm. The LSH-based approximation
algorithm can accelerate the value calculation process even further. We then
extend our algorithms to other scenarios such as (1) weighed \(K\)NN classifiers,
(2) different data points are clustered by different data curators, and (3)
there are data analysts providing computation who also requires proper
valuation.</p>
</td>
    <td>
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/zhao2019weakly/">A Weakly Supervised Adaptive Triplet Loss For Deep Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Weakly Supervised Adaptive Triplet Loss For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Weakly Supervised Adaptive Triplet Loss For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhao et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</td>
    <td>22</td>
    <td><p>We address the problem of distance metric learning in visual similarity
search, defined as learning an image embedding model which projects images into
Euclidean space where semantically and visually similar images are closer and
dissimilar images are further from one another. We present a weakly supervised
adaptive triplet loss (ATL) capable of capturing fine-grained semantic
similarity that encourages the learned image embedding models to generalize
well on cross-domain data. The method uses weakly labeled product description
data to implicitly determine fine grained semantic classes, avoiding the need
to annotate large amounts of training data. We evaluate on the Amazon fashion
retrieval benchmark and DeepFashion in-shop retrieval data. The method boosts
the performance of triplet loss baseline by 10.6% on cross-domain data and
out-performs the state-of-art model on all evaluation metrics.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Distance Metric Learning 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/fu2019neurons/">Neurons Merging Layer: Towards Progressive Redundancy Reduction For Deep Supervised Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Neurons Merging Layer: Towards Progressive Redundancy Reduction For Deep Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Neurons Merging Layer: Towards Progressive Redundancy Reduction For Deep Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</td>
    <td>6</td>
    <td><p>Deep supervised hashing has become an active topic in information retrieval.
It generates hashing bits by the output neurons of a deep hashing network.
During binary discretization, there often exists much redundancy between
hashing bits that degenerates retrieval performance in terms of both storage
and accuracy. This paper proposes a simple yet effective Neurons Merging Layer
(NMLayer) for deep supervised hashing. A graph is constructed to represent the
redundancy relationship between hashing bits that is used to guide the learning
of a hashing network. Specifically, it is dynamically learned by a novel
mechanism defined in our active and frozen phases. According to the learned
relationship, the NMLayer merges the redundant neurons together to balance the
importance of each output neuron. Moreover, multiple NMLayers are progressively
trained for a deep hashing network to learn a more compact hashing code from a
long redundant code. Extensive experiments on four datasets demonstrate that
our proposed method outperforms state-of-the-art hashing methods.</p>
</td>
    <td>
      
        IJCAI 
      
        Unsupervised 
      
        Neural Hashing 
      
        AAAI 
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/yu2019learning/">Learning Metrics From Teachers: Compact Networks For Image Embedding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Metrics From Teachers: Compact Networks For Image Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Metrics From Teachers: Compact Networks For Image Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yu et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>121</td>
    <td><p>Metric learning networks are used to compute image embeddings, which are
widely used in many applications such as image retrieval and face recognition.
In this paper, we propose to use network distillation to efficiently compute
image embeddings with small networks. Network distillation has been
successfully applied to improve image classification, but has hardly been
explored for metric learning. To do so, we propose two new loss functions that
model the communication of a deep teacher network to a small student network.
We evaluate our system in several datasets, including CUB-200-2011, Cars-196,
Stanford Online Products and show that embeddings computed using small student
networks perform significantly better than those computed using standard
networks of similar size. Results on a very compact network (MobileNet-0.25),
which can be used on mobile devices, show that the proposed method can greatly
improve Recall@1 results from 27.5% to 44.6%. Furthermore, we investigate
various aspects of distillation for embeddings, including hint and attention
layers, semi-supervised learning and cross quality distillation. (Code is
available at https://github.com/yulu0724/EmbeddingDistillation.)</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/wang2019ranked/">Ranked List Loss For Deep Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Ranked List Loss For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Ranked List Loss For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>231</td>
    <td><p>The objective of deep metric learning (DML) is to learn embeddings that can
capture semantic similarity and dissimilarity information among data points.
Existing pairwise or tripletwise loss functions used in DML are known to suffer
from slow convergence due to a large proportion of trivial pairs or triplets as
the model improves. To improve this, ranking-motivated structured losses are
proposed recently to incorporate multiple examples and exploit the structured
information among them. They converge faster and achieve state-of-the-art
performance. In this work, we unveil two limitations of existing
ranking-motivated structured losses and propose a novel ranked list loss to
solve both of them. First, given a query, only a fraction of data points is
incorporated to build the similarity structure. Consequently, some useful
examples are ignored and the structure is less informative. To address this, we
propose to build a set-based similarity structure by exploiting all instances
in the gallery. The learning setting can be interpreted as few-shot retrieval:
given a mini-batch, every example is iteratively used as a query, and the rest
ones compose the gallery to search, i.e., the support set in few-shot setting.
The rest examples are split into a positive set and a negative set. For every
mini-batch, the learning objective of ranked list loss is to make the query
closer to the positive set than to the negative set by a margin. Second,
previous methods aim to pull positive pairs as close as possible in the
embedding space. As a result, the intraclass data distribution tends to be
extremely compressed. In contrast, we propose to learn a hypersphere for each
class in order to preserve useful similarity structure inside it, which
functions as regularisation. Extensive experiments demonstrate the superiority
of our proposal by comparing with the state-of-the-art methods.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/fehervari2019scalable/">Scalable Logo Recognition Using Proxies</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Scalable Logo Recognition Using Proxies' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Scalable Logo Recognition Using Proxies' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fehervari Istvan, Appalaraju Srikar</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>50</td>
    <td><p>Logo recognition is the task of identifying and classifying logos. Logo
recognition is a challenging problem as there is no clear definition of a logo
and there are huge variations of logos, brands and re-training to cover every
variation is impractical. In this paper, we formulate logo recognition as a
few-shot object detection problem. The two main components in our pipeline are
universal logo detector and few-shot logo recognizer. The universal logo
detector is a class-agnostic deep object detector network which tries to learn
the characteristics of what makes a logo. It predicts bounding boxes on likely
logo regions. These logo regions are then classified by logo recognizer using
nearest neighbor search, trained by triplet loss using proxies. We also
annotated a first of its kind product logo dataset containing 2000 logos from
295K images collected from Amazon called PL2K. Our pipeline achieves 97% recall
with 0.6 mAP on PL2K test dataset and state-of-the-art 0.565 mAP on the
publicly available FlickrLogos-32 test set without fine-tuning.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/shen2019embarrassingly/">Embarrassingly Simple Binary Representation Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Embarrassingly Simple Binary Representation Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Embarrassingly Simple Binary Representation Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shen et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</td>
    <td>20</td>
    <td><p>Recent binary representation learning models usually require sophisticated
binary optimization, similarity measure or even generative models as
auxiliaries. However, one may wonder whether these non-trivial components are
needed to formulate practical and effective hashing models. In this paper, we
answer the above question by proposing an embarrassingly simple approach to
binary representation learning. With a simple classification objective, our
model only incorporates two additional fully-connected layers onto the top of
an arbitrary backbone network, whilst complying with the binary constraints
during training. The proposed model lower-bounds the Information Bottleneck
(IB) between data samples and their semantics, and can be related to many
recent `learning to hash’ paradigms. We show that, when properly designed, even
such a simple network can generate effective binary codes, by fully exploring
data semantics without any held-out alternating updating steps or auxiliary
models. Experiments are conducted on conventional large-scale benchmarks, i.e.,
CIFAR-10, NUS-WIDE, and ImageNet, where the proposed simple model outperforms
the state-of-the-art methods.</p>
</td>
    <td>
      
        Hashing Methods 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/mu2019empirical/">An Empirical Comparison Of FAISS And FENSHSES For Nearest Neighbor Search In Hamming Space</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=An Empirical Comparison Of FAISS And FENSHSES For Nearest Neighbor Search In Hamming Space' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=An Empirical Comparison Of FAISS And FENSHSES For Nearest Neighbor Search In Hamming Space' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mu Cun, Yang Binwei, Yan Zheng</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>11</td>
    <td><p>In this paper, we compare the performances of FAISS and FENSHSES on nearest
neighbor search in Hamming space–a fundamental task with ubiquitous
applications in nowadays eCommerce. Comprehensive evaluations are made in terms
of indexing speed, search latency and RAM consumption. This comparison is
conducted towards a better understanding on trade-offs between nearest neighbor
search systems implemented in main memory and the ones implemented in secondary
memory, which is largely unaddressed in literature.</p>
</td>
    <td>
      
        Similarity Search 
      
        Tools & Libraries 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/fain2019dividing/">Dividing And Conquering Cross-modal Recipe Retrieval: From Nearest Neighbours Baselines To Sota</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Dividing And Conquering Cross-modal Recipe Retrieval: From Nearest Neighbours Baselines To Sota' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Dividing And Conquering Cross-modal Recipe Retrieval: From Nearest Neighbours Baselines To Sota' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fain et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>16</td>
    <td><p>We propose a novel non-parametric method for cross-modal recipe retrieval
which is applied on top of precomputed image and text embeddings. By combining
our method with standard approaches for building image and text encoders,
trained independently with a self-supervised classification objective, we
create a baseline model which outperforms most existing methods on a
challenging image-to-recipe task. We also use our method for comparing image
and text encoders trained using different modern approaches, thus addressing
the issues hindering the development of novel methods for cross-modal recipe
retrieval. We demonstrate how to use the insights from model comparison and
extend our baseline model with standard triplet loss that improves
state-of-the-art on the Recipe1M dataset by a large margin, while using only
precomputed features and with much less complexity than existing methods.
Further, our approach readily generalizes beyond recipe retrieval to other
challenging domains, achieving state-of-the-art performance on Politics and
GoodNews cross-modal retrieval tasks.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/wang2019learning/">Learning Cross-modal Embeddings With Adversarial Networks For Cooking Recipes And Food Images</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Cross-modal Embeddings With Adversarial Networks For Cooking Recipes And Food Images' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Cross-modal Embeddings With Adversarial Networks For Cooking Recipes And Food Images' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>127</td>
    <td><p>Food computing is playing an increasingly important role in human daily life,
and has found tremendous applications in guiding human behavior towards smart
food consumption and healthy lifestyle. An important task under the
food-computing umbrella is retrieval, which is particularly helpful for health
related applications, where we are interested in retrieving important
information about food (e.g., ingredients, nutrition, etc.). In this paper, we
investigate an open research task of cross-modal retrieval between cooking
recipes and food images, and propose a novel framework Adversarial Cross-Modal
Embedding (ACME) to resolve the cross-modal retrieval task in food domains.
Specifically, the goal is to learn a common embedding feature space between the
two modalities, in which our approach consists of several novel ideas: (i)
learning by using a new triplet loss scheme together with an effective sampling
strategy, (ii) imposing modality alignment using an adversarial learning
strategy, and (iii) imposing cross-modal translation consistency such that the
embedding of one modality is able to recover some important information of
corresponding instances in the other modality. ACME achieves the
state-of-the-art performance on the benchmark Recipe1M dataset, validating the
efficacy of the proposed technique.</p>
</td>
    <td>
      
        Robustness 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/engelsma2019fingerprints/">Fingerprints: Fixed Length Representation Via Deep Networks And Domain Knowledge</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fingerprints: Fixed Length Representation Via Deep Networks And Domain Knowledge' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fingerprints: Fixed Length Representation Via Deep Networks And Domain Knowledge' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Engelsma Joshua J., Cao Kai, Jain Anil K.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>10</td>
    <td><p>We learn a discriminative fixed length feature representation of fingerprints
which stands in contrast to commonly used unordered, variable length sets of
minutiae points. To arrive at this fixed length representation, we embed
fingerprint domain knowledge into a multitask deep convolutional neural network
architecture. Empirical results, on two public-domain fingerprint databases
(NIST SD4 and FVC 2004 DB1) show that compared to minutiae representations,
extracted by two state-of-the-art commercial matchers (Verifinger v6.3 and
Innovatrics v2.0.3), our fixed-length representations provide (i) higher search
accuracy: Rank-1 accuracy of 97.9% vs. 97.3% on NIST SD4 against a gallery size
of 2000 and (ii) significantly faster, large scale search: 682,594 matches per
second vs. 22 matches per second for commercial matchers on an i5 3.3 GHz
processor with 8 GB of RAM.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/wang2019fusion/">Fusion-supervised Deep Cross-modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fusion-supervised Deep Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fusion-supervised Deep Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE International Conference on Multimedia and Expo (ICME)</td>
    <td>15</td>
    <td><p>Deep hashing has recently received attention in cross-modal retrieval for its
impressive advantages. However, existing hashing methods for cross-modal
retrieval cannot fully capture the heterogeneous multi-modal correlation and
exploit the semantic information. In this paper, we propose a novel
<em>Fusion-supervised Deep Cross-modal Hashing</em> (FDCH) approach. Firstly,
FDCH learns unified binary codes through a fusion hash network with paired
samples as input, which effectively enhances the modeling of the correlation of
heterogeneous multi-modal data. Then, these high-quality unified hash codes
further supervise the training of the modality-specific hash networks for
encoding out-of-sample queries. Meanwhile, both pair-wise similarity
information and classification information are embedded in the hash networks
under one stream framework, which simultaneously preserves cross-modal
similarity and keeps semantic consistency. Experimental results on two
benchmark datasets demonstrate the state-of-the-art performance of FDCH.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/efremenko2019fast/">Fast And Bayes-consistent Nearest Neighbors</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast And Bayes-consistent Nearest Neighbors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast And Bayes-consistent Nearest Neighbors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Efremenko Klim, Kontorovich Aryeh, Noivirt Moshe</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>9</td>
    <td><p>Research on nearest-neighbor methods tends to focus somewhat dichotomously
either on the statistical or the computational aspects – either on, say, Bayes
consistency and rates of convergence or on techniques for speeding up the
proximity search. This paper aims at bridging these realms: to reap the
advantages of fast evaluation time while maintaining Bayes consistency, and
further without sacrificing too much in the risk decay rate. We combine the
locality-sensitive hashing (LSH) technique with a novel missing-mass argument
to obtain a fast and Bayes-consistent classifier. Our algorithm’s prediction
runtime compares favorably against state of the art approximate NN methods,
while maintaining Bayes-consistency and attaining rates comparable to minimax.
On samples of size \(n\) in \(\R^d\), our pre-processing phase has runtime \(O(d n
log n)\), while the evaluation phase has runtime \(O(dlog n)\) per query point.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/narayana2019huse/">HUSE: Hierarchical Universal Semantic Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=HUSE: Hierarchical Universal Semantic Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=HUSE: Hierarchical Universal Semantic Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Narayana et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>7</td>
    <td><p>There is a recent surge of interest in cross-modal representation learning
corresponding to images and text. The main challenge lies in mapping images and
text to a shared latent space where the embeddings corresponding to a similar
semantic concept lie closer to each other than the embeddings corresponding to
different semantic concepts, irrespective of the modality. Ranking losses are
commonly used to create such shared latent space – however, they do not impose
any constraints on inter-class relationships resulting in neighboring clusters
to be completely unrelated. The works in the domain of visual semantic
embeddings address this problem by first constructing a semantic embedding
space based on some external knowledge and projecting image embeddings onto
this fixed semantic embedding space. These works are confined only to image
domain and constraining the embeddings to a fixed space adds additional burden
on learning. This paper proposes a novel method, HUSE, to learn cross-modal
representation with semantic information. HUSE learns a shared latent space
where the distance between any two universal embeddings is similar to the
distance between their corresponding class embeddings in the semantic embedding
space. HUSE also uses a classification objective with a shared classification
layer to make sure that the image and text embeddings are in the same shared
latent space. Experiments on UPMC Food-101 show our method outperforms previous
state-of-the-art on retrieval, hierarchical precision and classification
results.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/ducau2019automatic/">Automatic Malware Description Via Attribute Tagging And Similarity Embedding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Automatic Malware Description Via Attribute Tagging And Similarity Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Automatic Malware Description Via Attribute Tagging And Similarity Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ducau et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>11</td>
    <td><p>With the rapid proliferation and increased sophistication of malicious
software (malware), detection methods no longer rely only on manually generated
signatures but have also incorporated more general approaches like machine
learning detection. Although powerful for conviction of malicious artifacts,
these methods do not produce any further information about the type of threat
that has been detected neither allows for identifying relationships between
malware samples. In this work, we address the information gap between machine
learning and signature-based detection methods by learning a representation
space for malware samples in which files with similar malicious behaviors
appear close to each other. We do so by introducing a deep learning based
tagging model trained to generate human-interpretable semantic descriptions of
malicious software, which, at the same time provides potentially more useful
and flexible information than malware family names.
  We show that the malware descriptions generated with the proposed approach
correctly identify more than 95% of eleven possible tag descriptions for a
given sample, at a deployable false positive rate of 1% per tag. Furthermore,
we use the learned representation space to introduce a similarity index between
malware files, and empirically demonstrate using dynamic traces from files’
execution, that is not only more effective at identifying samples from the same
families, but also 32 times smaller than those based on raw feature vectors.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/eghbali2019deep/">Deep Spherical Quantization For Image Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Spherical Quantization For Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Spherical Quantization For Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Eghbali Sepehr, Tahvildari Ladan</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>27</td>
    <td><p>Hashing methods, which encode high-dimensional images with compact discrete
codes, have been widely applied to enhance large-scale image retrieval. In this
paper, we put forward Deep Spherical Quantization (DSQ), a novel method to make
deep convolutional neural networks generate supervised and compact binary codes
for efficient image search. Our approach simultaneously learns a mapping that
transforms the input images into a low-dimensional discriminative space, and
quantizes the transformed data points using multi-codebook quantization. To
eliminate the negative effect of norm variance on codebook learning, we force
the network to L_2 normalize the extracted features and then quantize the
resulting vectors using a new supervised quantization technique specifically
designed for points lying on a unit hypersphere. Furthermore, we introduce an
easy-to-implement extension of our quantization technique that enforces
sparsity on the codebooks. Extensive experiments demonstrate that DSQ and its
sparse variant can generate semantically separable compact binary codes
outperforming many state-of-the-art image retrieval methods on three
benchmarks.</p>
</td>
    <td>
      
        Quantization 
      
        Image Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/nawaz2019do/">Do Cross Modal Systems Leverage Semantic Relationships?</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Do Cross Modal Systems Leverage Semantic Relationships?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Do Cross Modal Systems Leverage Semantic Relationships?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Nawaz et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</td>
    <td>9</td>
    <td><p>Current cross-modal retrieval systems are evaluated using R@K measure which
does not leverage semantic relationships rather strictly follows the manually
marked image text query pairs. Therefore, current systems do not generalize
well for the unseen data in the wild. To handle this, we propose a new measure,
SemanticMap, to evaluate the performance of cross-modal systems. Our proposed
measure evaluates the semantic similarity between the image and text
representations in the latent embedding space. We also propose a novel
cross-modal retrieval system using a single stream network for bidirectional
retrieval. The proposed system is based on a deep neural network trained using
extended center loss, minimizing the distance of image and text descriptions in
the latent space from the class centers. In our system, the text descriptions
are also encoded as images which enabled us to use a single stream network for
both text and images. To the best of our knowledge, our work is the first of
its kind in terms of employing a single stream network for cross-modal
retrieval systems. The proposed system is evaluated on two publicly available
datasets including MSCOCO and Flickr30K and has shown comparable results to the
current state-of-the-art methods.</p>
</td>
    <td>
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/fu2019fast/">Fast Approximate Nearest Neighbor Search With The Navigating Spreading-out Graph</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Approximate Nearest Neighbor Search With The Navigating Spreading-out Graph' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Approximate Nearest Neighbor Search With The Navigating Spreading-out Graph' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the VLDB Endowment</td>
    <td>219</td>
    <td><p>Approximate nearest neighbor search (ANNS) is a fundamental problem in databases and data mining. A scalable ANNS algorithm should be both memory-efficient and fast. Some early graph-based approaches have shown attractive theoretical guarantees on search time complexity, but they all suffer from the problem of high indexing time complexity. Recently, some graph-based methods have been proposed to reduce indexing complexity by approximating the traditional graphs; these methods have achieved revolutionary performance on million-scale datasets. Yet, they still can not scale to billion-node databases. In this paper, to further improve the search-efficiency and scalability of graph-based methods, we start by introducing four aspects: (1) ensuring the connectivity of the graph; (2) lowering the average out-degree of the graph for fast traversal; (3) shortening the search path; and (4) reducing the index size. Then, we propose a novel graph structure called Monotonic Relative Neighborhood Graph (MRNG) which guarantees very low search complexity (close to logarithmic time). To further lower the indexing complexity and make it practical for billion-node ANNS problems, we propose a novel graph structure named Navigating Spreading-out Graph (NSG) by approximating the MRNG. The NSG takes the four aspects into account simultaneously. Extensive experiments show that NSG outperforms all the existing algorithms significantly. In addition, NSG shows superior performance in the E-commercial search scenario of Taobao (Alibaba Group) and has been integrated into their search engine at billion-node scale.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/zhai2019learning/">Learning A Unified Embedding For Visual Search At Pinterest</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning A Unified Embedding For Visual Search At Pinterest' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning A Unified Embedding For Visual Search At Pinterest' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhai et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</td>
    <td>33</td>
    <td><p>At Pinterest, we utilize image embeddings throughout our search and
recommendation systems to help our users navigate through visual content by
powering experiences like browsing of related content and searching for exact
products for shopping. In this work we describe a multi-task deep metric
learning system to learn a single unified image embedding which can be used to
power our multiple visual search products. The solution we present not only
allows us to train for multiple application objectives in a single deep neural
network architecture, but takes advantage of correlated information in the
combination of all training data from each application to generate a unified
embedding that outperforms all specialized embeddings previously deployed for
each product. We discuss the challenges of handling images from different
domains such as camera photos, high quality web images, and clean product
catalog images. We also detail how to jointly train for multiple product
objectives and how to leverage both engagement data and human labeled data. In
addition, our trained embeddings can also be binarized for efficient storage
and retrieval without compromising precision and recall. Through comprehensive
evaluations on offline metrics, user studies, and online A/B experiments, we
demonstrate that our proposed unified embedding improves both relevance and
engagement of our visual search products for both browsing and searching
purposes when compared to existing specialized embeddings. Finally, the
deployment of the unified embedding at Pinterest has drastically reduced the
operation and engineering cost of maintaining multiple embeddings while
improving quality.</p>
</td>
    <td>
      
        KDD 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/dourado2019fusion/">Fusion Vectors: Embedding Graph Fusions For Efficient Unsupervised Rank Aggregation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fusion Vectors: Embedding Graph Fusions For Efficient Unsupervised Rank Aggregation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fusion Vectors: Embedding Graph Fusions For Efficient Unsupervised Rank Aggregation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dourado Icaro Cavalcante, Torres Ricardo da Silva</td> <!-- 🔧 You were missing this -->
    <td>Information Processing &amp; Management</td>
    <td>13</td>
    <td><p>The vast increase in amount and complexity of digital content led to a wide
interest in ad-hoc retrieval systems in recent years. Complementary, the
existence of heterogeneous data sources and retrieval models stimulated the
proliferation of increasingly ingenious and effective rank aggregation
functions. Although recently proposed rank aggregation functions are promising
with respect to effectiveness, existing proposals in the area usually overlook
efficiency aspects. We propose an innovative rank aggregation function that is
unsupervised, intrinsically multimodal, and targeted for fast retrieval and top
effectiveness performance. We introduce the concepts of embedding and indexing
of graph-based rank-aggregation representation models, and their application
for search tasks. Embedding formulations are also proposed for graph-based rank
representations. We introduce the concept of fusion vectors, a late-fusion
representation of objects based on ranks, from which an intrinsically
rank-aggregation retrieval model is defined. Next, we present an approach for
fast retrieval based on fusion vectors, thus promoting an efficient rank
aggregation system. Our method presents top effectiveness performance among
state-of-the-art related work, while bringing novel aspects of multimodality
and effectiveness. Consistent speedups are achieved against the recent
baselines in all datasets considered.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/doras2019cover/">Cover Detection Using Dominant Melody Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cover Detection Using Dominant Melody Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cover Detection Using Dominant Melody Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Doras Guillaume, Peeters Geoffroy</td> <!-- 🔧 You were missing this -->
    <td>20th International Society for Music Information Retrieval Conference Delft The Netherlands 2019</td>
    <td>22</td>
    <td><p>Automatic cover detection – the task of finding in an audio database all the
covers of one or several query tracks – has long been seen as a challenging
theoretical problem in the MIR community and as an acute practical problem for
authors and composers societies. Original algorithms proposed for this task
have proven their accuracy on small datasets, but are unable to scale up to
modern real-life audio corpora. On the other hand, faster approaches designed
to process thousands of pairwise comparisons resulted in lower accuracy, making
them unsuitable for practical use.
  In this work, we propose a neural network architecture that is trained to
represent each track as a single embedding vector. The computation burden is
therefore left to the embedding extraction – that can be conducted offline and
stored, while the pairwise comparison task reduces to a simple Euclidean
distance computation. We further propose to extract each track’s embedding out
of its dominant melody representation, obtained by another neural network
trained for this task. We then show that this architecture improves
state-of-the-art accuracy both on small and large datasets, and is able to
scale to query databases of thousands of tracks in a few seconds.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/zhao2019large/">Large-scale Visual Search With Binary Distributed Graph At Alibaba</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Large-scale Visual Search With Binary Distributed Graph At Alibaba' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Large-scale Visual Search With Binary Distributed Graph At Alibaba' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhao et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 28th ACM International Conference on Information and Knowledge Management</td>
    <td>10</td>
    <td><p>Graph-based approximate nearest neighbor search has attracted more and more
attentions due to its online search advantages. Numbers of methods studying the
enhancement of speed and recall have been put forward. However, few of them
focus on the efficiency and scale of offline graph-construction. For a deployed
visual search system with several billions of online images in total, building
a billion-scale offline graph in hours is essential, which is almost
unachievable by most existing methods. In this paper, we propose a novel
algorithm called Binary Distributed Graph to solve this problem. Specifically,
we combine binary codes with graph structure to speedup online and offline
procedures, and achieve comparable performance with the ones in real-value
based scenarios by recalling more binary candidates. Furthermore, the
graph-construction is optimized to completely distributed implementation, which
significantly accelerates the offline process and gets rid of the limitation of
memory and disk within a single machine. Experimental comparisons on Alibaba
Commodity Data Set (more than three billion images) show that the proposed
method outperforms the state-of-the-art with respect to the online/offline
trade-off.</p>
</td>
    <td>
      
        CIKM 
      
        Image Retrieval 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/shin2019semi/">Semi-supervised Feature-level Attribute Manipulation For Fashion Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semi-supervised Feature-level Attribute Manipulation For Fashion Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semi-supervised Feature-level Attribute Manipulation For Fashion Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shin Minchul, Park Sanghyuk, Kim Taeksoo</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>9</td>
    <td><p>With a growing demand for the search by image, many works have studied the
task of fashion instance-level image retrieval (FIR). Furthermore, the recent
works introduce a concept of fashion attribute manipulation (FAM) which
manipulates a specific attribute (e.g color) of a fashion item while
maintaining the rest of the attributes (e.g shape, and pattern). In this way,
users can search not only “the same” items but also “similar” items with the
desired attributes. FAM is a challenging task in that the attributes are hard
to define, and the unique characteristics of a query are hard to be preserved.
Although both FIR and FAM are important in real-life applications, most of the
previous studies have focused on only one of these problem. In this study, we
aim to achieve competitive performance on both FIR and FAM. To do so, we
propose a novel method that converts a query into a representation with the
desired attributes. We introduce a new idea of attribute manipulation at the
feature level, by matching the distribution of manipulated features with real
features. In this fashion, the attribute manipulation can be done independently
from learning a representation from the image. By introducing the feature-level
attribute manipulation, the previous methods for FIR can perform attribute
manipulation without sacrificing their retrieval performance.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/wu2019efficient/">Efficient Inner Product Approximation In Hybrid Spaces</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Inner Product Approximation In Hybrid Spaces' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient Inner Product Approximation In Hybrid Spaces' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>7</td>
    <td><p>Many emerging use cases of data mining and machine learning operate on large
datasets with data from heterogeneous sources, specifically with both sparse
and dense components. For example, dense deep neural network embedding vectors
are often used in conjunction with sparse textual features to provide high
dimensional hybrid representation of documents. Efficient search in such hybrid
spaces is very challenging as the techniques that perform well for sparse
vectors have little overlap with those that work well for dense vectors.
Popular techniques like Locality Sensitive Hashing (LSH) and its data-dependent
variants also do not give good accuracy in high dimensional hybrid spaces. Even
though hybrid scenarios are becoming more prevalent, currently there exist no
efficient techniques in literature that are both fast and accurate. In this
paper, we propose a technique that approximates the inner product computation
in hybrid vectors, leading to substantial speedup in search while maintaining
high accuracy. We also propose efficient data structures that exploit modern
computer architectures, resulting in orders of magnitude faster search than the
existing baselines. The performance of the proposed method is demonstrated on
several datasets including a very large scale industrial dataset containing one
billion vectors in a billion dimensional space, achieving over 10x speedup and
higher accuracy against competitive baselines.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/dong2019document/">Document Hashing With Mixture-prior Generative Models</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Document Hashing With Mixture-prior Generative Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Document Hashing With Mixture-prior Generative Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dong et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</td>
    <td>19</td>
    <td><p>Hashing is promising for large-scale information retrieval tasks thanks to
the efficiency of distance evaluation between binary codes. Generative hashing
is often used to generate hashing codes in an unsupervised way. However,
existing generative hashing methods only considered the use of simple priors,
like Gaussian and Bernoulli priors, which limits these methods to further
improve their performance. In this paper, two mixture-prior generative models
are proposed, under the objective to produce high-quality hashing codes for
documents. Specifically, a Gaussian mixture prior is first imposed onto the
variational auto-encoder (VAE), followed by a separate step to cast the
continuous latent representation of VAE into binary code. To avoid the
performance loss caused by the separate casting, a model using a Bernoulli
mixture prior is further developed, in which an end-to-end training is admitted
by resorting to the straight-through (ST) discrete gradient estimator.
Experimental results on several benchmark datasets demonstrate that the
proposed methods, especially the one using Bernoulli mixture priors,
consistently outperform existing ones by a substantial margin.</p>
</td>
    <td>
      
        Hashing Methods 
      
        EMNLP 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/do2019simultaneous/">Simultaneous Feature Aggregating And Hashing For Compact Binary Code Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Simultaneous Feature Aggregating And Hashing For Compact Binary Code Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Simultaneous Feature Aggregating And Hashing For Compact Binary Code Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Do et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>14</td>
    <td><p>Representing images by compact hash codes is an attractive approach for
large-scale content-based image retrieval. In most state-of-the-art
hashing-based image retrieval systems, for each image, local descriptors are
first aggregated as a global representation vector. This global vector is then
subjected to a hashing function to generate a binary hash code. In previous
works, the aggregating and the hashing processes are designed independently.
Hence these frameworks may generate suboptimal hash codes. In this paper, we
first propose a novel unsupervised hashing framework in which feature
aggregating and hashing are designed simultaneously and optimized jointly.
Specifically, our joint optimization generates aggregated representations that
can be better reconstructed by some binary codes. This leads to more
discriminative binary hash codes and improved retrieval accuracy. In addition,
the proposed method is flexible. It can be extended for supervised hashing.
When the data label is available, the framework can be adapted to learn binary
codes which minimize the reconstruction loss w.r.t. label vectors. Furthermore,
we also propose a fast version of the state-of-the-art hashing method Binary
Autoencoder to be used in our proposed frameworks. Extensive experiments on
benchmark datasets under various settings show that the proposed methods
outperform state-of-the-art unsupervised and supervised hashing methods.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Compact Codes 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/do2019selective/">From Selective Deep Convolutional Features To Compact Binary Representations For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=From Selective Deep Convolutional Features To Compact Binary Representations For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=From Selective Deep Convolutional Features To Compact Binary Representations For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Do et al.</td> <!-- 🔧 You were missing this -->
    <td>ACM Transactions on Multimedia Computing, Communications, and Applications</td>
    <td>30</td>
    <td><p>In the large-scale image retrieval task, the two most important requirements
are the discriminability of image representations and the efficiency in
computation and storage of representations. Regarding the former requirement,
Convolutional Neural Network (CNN) is proven to be a very powerful tool to
extract highly discriminative local descriptors for effective image search.
Additionally, in order to further improve the discriminative power of the
descriptors, recent works adopt fine-tuned strategies. In this paper, taking a
different approach, we propose a novel, computationally efficient, and
competitive framework. Specifically, we firstly propose various strategies to
compute masks, namely SIFT-mask, SUM-mask, and MAX-mask, to select a
representative subset of local convolutional features and eliminate redundant
features. Our in-depth analyses demonstrate that proposed masking schemes are
effective to address the burstiness drawback and improve retrieval accuracy.
Secondly, we propose to employ recent embedding and aggregating methods which
can significantly boost the feature discriminability. Regarding the computation
and storage efficiency, we include a hashing module to produce very compact
binary image representations. Extensive experiments on six image retrieval
benchmarks demonstrate that our proposed framework achieves the
state-of-the-art retrieval performances.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/do2019binary/">Binary Constrained Deep Hashing Network For Image Retrieval Without Manual Annotation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Binary Constrained Deep Hashing Network For Image Retrieval Without Manual Annotation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Binary Constrained Deep Hashing Network For Image Retrieval Without Manual Annotation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Do et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>9</td>
    <td><p>Learning compact binary codes for image retrieval task using deep neural
networks has attracted increasing attention recently. However, training deep
hashing networks for the task is challenging due to the binary constraints on
the hash codes, the similarity preserving property, and the requirement for a
vast amount of labelled images. To the best of our knowledge, none of the
existing methods has tackled all of these challenges completely in a unified
framework. In this work, we propose a novel end-to-end deep learning approach
for the task, in which the network is trained to produce binary codes directly
from image pixels without the need of manual annotation. In particular, to deal
with the non-smoothness of binary constraints, we propose a novel pairwise
constrained loss function, which simultaneously encodes the distances between
pairs of hash codes, and the binary quantization error. In order to train the
network with the proposed loss function, we propose an efficient parameter
learning algorithm. In addition, to provide similar / dissimilar training
images to train the network, we exploit 3D models reconstructed from unlabelled
images for automatic generation of enormous training image pairs. The extensive
experiments on image retrieval benchmark datasets demonstrate the improvements
of the proposed method over the state-of-the-art compact representation methods
on the image retrieval problem.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/do2019compact/">Compact Hash Code Learning With Binary Deep Neural Network</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Compact Hash Code Learning With Binary Deep Neural Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Compact Hash Code Learning With Binary Deep Neural Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Do et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>23</td>
    <td><p>Learning compact binary codes for image retrieval problem using deep neural
networks has recently attracted increasing attention. However, training deep
hashing networks is challenging due to the binary constraints on the hash
codes. In this paper, we propose deep network models and learning algorithms
for learning binary hash codes given image representations under both
unsupervised and supervised manners. The novelty of our network design is that
we constrain one hidden layer to directly output the binary codes. This design
has overcome a challenging problem in some previous works: optimizing
non-smooth objective functions because of binarization. In addition, we propose
to incorporate independence and balance properties in the direct and strict
forms into the learning schemes. We also include a similarity preserving
property in our objective functions. The resulting optimizations involving
these binary, independence, and balance constraints are difficult to solve. To
tackle this difficulty, we propose to learn the networks with alternating
optimization and careful relaxation. Furthermore, by leveraging the powerful
capacity of convolutional neural networks, we propose an end-to-end
architecture that jointly learns to extract visual features and produce binary
hash codes. Experimental results for the benchmark datasets show that the
proposed methods compare favorably or outperform the state of the art.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/ding2019bilinear/">Bilinear Supervised Hashing Based On 2D Image Features</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Bilinear Supervised Hashing Based On 2D Image Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Bilinear Supervised Hashing Based On 2D Image Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ding et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Circuits and Systems for Video Technology</td>
    <td>14</td>
    <td><p>Hashing has been recognized as an efficient representation learning method to
effectively handle big data due to its low computational complexity and memory
cost. Most of the existing hashing methods focus on learning the
low-dimensional vectorized binary features based on the high-dimensional raw
vectorized features. However, studies on how to obtain preferable binary codes
from the original 2D image features for retrieval is very limited. This paper
proposes a bilinear supervised discrete hashing (BSDH) method based on 2D image
features which utilizes bilinear projections to binarize the image matrix
features such that the intrinsic characteristics in the 2D image space are
preserved in the learned binary codes. Meanwhile, the bilinear projection
approximation and vectorization binary codes regression are seamlessly
integrated together to formulate the final robust learning framework.
Furthermore, a discrete optimization strategy is developed to alternatively
update each variable for obtaining the high-quality binary codes. In addition,
two 2D image features, traditional SURF-based FVLAD feature and CNN-based
AlexConv5 feature are designed for further improving the performance of the
proposed BSDH method. Results of extensive experiments conducted on four
benchmark datasets show that the proposed BSDH method almost outperforms all
competing hashing methods with different input features by different evaluation
protocols.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        SUPERVISED 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/dey2019doodle/">Doodle To Search: Practical Zero-shot Sketch-based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Doodle To Search: Practical Zero-shot Sketch-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Doodle To Search: Practical Zero-shot Sketch-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dey et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>74</td>
    <td><p>In this paper, we investigate the problem of zero-shot sketch-based image
retrieval (ZS-SBIR), where human sketches are used as queries to conduct
retrieval of photos from unseen categories. We importantly advance prior arts
by proposing a novel ZS-SBIR scenario that represents a firm step forward in
its practical application. The new setting uniquely recognizes two important
yet often neglected challenges of practical ZS-SBIR, (i) the large domain gap
between amateur sketch and photo, and (ii) the necessity for moving towards
large-scale retrieval. We first contribute to the community a novel ZS-SBIR
dataset, QuickDraw-Extended, that consists of 330,000 sketches and 204,000
photos spanning across 110 categories. Highly abstract amateur human sketches
are purposefully sourced to maximize the domain gap, instead of ones included
in existing datasets that can often be semi-photorealistic. We then formulate a
ZS-SBIR framework to jointly model sketches and photos into a common embedding
space. A novel strategy to mine the mutual information among domains is
specifically engineered to alleviate the domain gap. External semantic
knowledge is further embedded to aid semantic transfer. We show that, rather
surprisingly, retrieval performance significantly outperforms that of
state-of-the-art on existing datasets that can already be achieved using a
reduced version of our model. We further demonstrate the superior performance
of our full model by comparing with a number of alternatives on the newly
proposed dataset. The new dataset, plus all training and testing code of our
model, will be publicly released to facilitate future research</p>
</td>
    <td>
      
        Image Retrieval 
      
        Few Shot & Zero Shot 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/deng2019two/">Two-stream Deep Hashing With Class-specific Centers For Supervised Image Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Two-stream Deep Hashing With Class-specific Centers For Supervised Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Two-stream Deep Hashing With Class-specific Centers For Supervised Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Deng et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Neural Networks and Learning Systems</td>
    <td>75</td>
    <td><p>Hashing has been widely used for large-scale approximate nearest neighbor search due to its storage and search efficiency. Recent supervised hashing research has shown that deep learning-based methods can significantly outperform nondeep methods. Most existing supervised deep hashing methods exploit supervisory signals to generate similar and dissimilar image pairs for training. However, natural images can have large intraclass and small interclass variations, which may degrade the accuracy of hash codes. To address this problem, we propose a novel two-stream ConvNet architecture, which learns hash codes with class-specific representation centers. Our basic idea is that if we can learn a unified binary representation for each class as a center and encourage hash codes of images to be close to the corresponding centers, the intraclass variation will be greatly reduced. Accordingly, we design a neural network that leverages label information and outputs a unified binary representation for each class. Moreover, we also design an image network to learn hash codes from images and force these hash codes to be close to the corresponding class-specific centers. These two neural networks are then seamlessly incorporated to create a unified, end-to-end trainable framework. Extensive experiments on three popular benchmarks corroborate that our proposed method outperforms current state-of-the-art methods.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        Image Retrieval 
      
        SUPERVISED 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/sim%C3%A9oni2019local/">Local Features And Visual Words Emerge In Activations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Local Features And Visual Words Emerge In Activations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Local Features And Visual Words Emerge In Activations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Siméoni Oriane, Avrithis Yannis, Chum Ondrej</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>70</td>
    <td><p>We propose a novel method of deep spatial matching (DSM) for image retrieval.
Initial ranking is based on image descriptors extracted from convolutional
neural network activations by global pooling, as in recent state-of-the-art
work. However, the same sparse 3D activation tensor is also approximated by a
collection of local features. These local features are then robustly matched to
approximate the optimal alignment of the tensors. This happens without any
network modification, additional layers or training. No local feature detection
happens on the original image. No local feature descriptors and no visual
vocabulary are needed throughout the whole process.
  We experimentally show that the proposed method achieves the state-of-the-art
performance on standard benchmarks across different network architectures and
different global pooling methods. The highest gain in performance is achieved
when diffusion on the nearest-neighbor graph of global descriptors is initiated
from spatially verified images.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/wang2019semi/">Semi-supervised Deep Quantization For Cross-modal Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semi-supervised Deep Quantization For Cross-modal Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semi-supervised Deep Quantization For Cross-modal Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Xin, Zhu, Liu</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 27th ACM International Conference on Multimedia</td>
    <td>12</td>
    <td><p>The problem of cross-modal similarity search, which aims at making efficient and accurate queries across multiple domains, has become a significant and important research topic. Composite quantization, a compact coding solution superior to hashing techniques, has shown its effectiveness for similarity search. However, most existing works utilizing composite quantization to search multi-domain content only consider either pairwise similarity information or class label information across different domains, which fails to tackle the semi-supervised problem in composite quantization. In this paper, we address the semi-supervised quantization problem by considering: (i) pairwise similarity information (without class label information) across different domains, which captures the intra-document relation, (ii) cross-domain data with class label which can help capture inter-document relation, and (iii) cross-domain data with neither pairwise similarity nor class label which enables the full use of abundant unlabelled information. To the best of our knowledge, we are the first to consider both supervised information (pairwise similarity + class label) and unsupervised information (neither pairwise similarity nor class label) simultaneously in composite quantization. A challenging problem arises: how can we jointly handle these three sorts of information across multiple domains in an efficient way? To tackle this challenge, we propose a novel semi-supervised deep quantization (SSDQ) model that takes both supervised and unsupervised information into account. The proposed SSDQ model is capable of incorporating the above three kinds of information into one single framework when utilizing composite quantization for accurate and efficient queries across different domains. More specifically, we employ a modified deep autoencoder for better latent representation and formulate pairwise similarity loss, supervised quantization loss as well as unsupervised distribution match loss to handle all three types of information. The extensive experiments demonstrate the significant improvement of SSDQ over several state-of-the-art methods on various datasets.</p>
</td>
    <td>
      
        Quantization 
      
        SUPERVISED 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/das2019semi/">Semi Supervised Phrase Localization In A Bidirectional Caption-image Retrieval Framework</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semi Supervised Phrase Localization In A Bidirectional Caption-image Retrieval Framework' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semi Supervised Phrase Localization In A Bidirectional Caption-image Retrieval Framework' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Das et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>61</td>
    <td><p>We introduce a novel deep neural network architecture that links visual
regions to corresponding textual segments including phrases and words. To
accomplish this task, our architecture makes use of the rich semantic
information available in a joint embedding space of multi-modal data. From this
joint embedding space, we extract the associative localization maps that
develop naturally, without explicitly providing supervision during training for
the localization task. The joint space is learned using a bidirectional ranking
objective that is optimized using a \(N\)-Pair loss formulation. This training
mechanism demonstrates the idea that localization information is learned
inherently while optimizing a Bidirectional Retrieval objective. The model’s
retrieval and localization performance is evaluated on MSCOCO and Flickr30K
Entities datasets. This architecture outperforms the state of the art results
in the semi-supervised phrase localization setting.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Tools & Libraries 
      
        Image Retrieval 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/dalins2019pdq/">PDQ & TMK + PDQF -- A Test Drive Of Facebook's Perceptual Hashing Algorithms</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=PDQ & TMK + PDQF -- A Test Drive Of Facebook's Perceptual Hashing Algorithms' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=PDQ & TMK + PDQF -- A Test Drive Of Facebook's Perceptual Hashing Algorithms' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dalins Janis, Wilson Campbell, Boudry Douglas</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>Efficient and reliable automated detection of modified image and multimedia
files has long been a challenge for law enforcement, compounded by the harm
caused by repeated exposure to psychologically harmful materials. In August
2019 Facebook open-sourced their PDQ and TMK + PDQF algorithms for image and
video similarity measurement, respectively. In this report, we review the
algorithms’ performance on detecting commonly encountered transformations on
real-world case data, sourced from contemporary investigations. We also provide
a reference implementation to demonstrate the potential application and
integration of such algorithms within existing law enforcement systems.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/wang2019multi/">Multi-similarity Loss With General Pair Weighting For Deep Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multi-similarity Loss With General Pair Weighting For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multi-similarity Loss With General Pair Weighting For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>736</td>
    <td><p>A family of loss functions built on pair-based computation have been proposed
in the literature which provide a myriad of solutions for deep metric learning.
In this paper, we provide a general weighting framework for understanding
recent pair-based loss functions. Our contributions are three-fold: (1) we
establish a General Pair Weighting (GPW) framework, which casts the sampling
problem of deep metric learning into a unified view of pair weighting through
gradient analysis, providing a powerful tool for understanding recent
pair-based loss functions; (2) we show that with GPW, various existing
pair-based methods can be compared and discussed comprehensively, with clear
differences and key limitations identified; (3) we propose a new loss called
multi-similarity loss (MS loss) under the GPW, which is implemented in two
iterative steps (i.e., mining and weighting). This allows it to fully consider
three similarities for pair weighting, providing a more principled approach for
collecting and weighting informative pairs. Finally, the proposed MS loss
obtains new state-of-the-art performance on four image retrieval benchmarks,
where it outperforms the most recent approaches, such as
ABE\cite{Kim_2018_ECCV} and HTL by a large margin: 60.6% to 65.7% on CUB200,
and 80.9% to 88.0% on In-Shop Clothes Retrieval dataset at Recall@1. Code is
available at https://github.com/MalongTech/research-ms-loss.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/sharma2019retrieving/">Retrieving Similar E-commerce Images Using Deep Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Retrieving Similar E-commerce Images Using Deep Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Retrieving Similar E-commerce Images Using Deep Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sharma Rishab, Vishvakarma Anirudha</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>20</td>
    <td><p>In this paper, we propose a deep convolutional neural network for learning
the embeddings of images in order to capture the notion of visual similarity.
We present a deep siamese architecture that when trained on positive and
negative pairs of images learn an embedding that accurately approximates the
ranking of images in order of visual similarity notion. We also implement a
novel loss calculation method using an angular loss metrics based on the
problems requirement. The final embedding of the image is combined
representation of the lower and top-level embeddings. We used fractional
distance matrix to calculate the distance between the learned embeddings in
n-dimensional space. In the end, we compare our architecture with other
existing deep architecture and go on to demonstrate the superiority of our
solution in terms of image retrieval by testing the architecture on four
datasets. We also show how our suggested network is better than the other
traditional deep CNNs used for capturing fine-grained image similarities by
learning an optimum embedding.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/mohammadshahi2019aligning/">Aligning Multilingual Word Embeddings For Cross-modal Retrieval Task</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Aligning Multilingual Word Embeddings For Cross-modal Retrieval Task' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Aligning Multilingual Word Embeddings For Cross-modal Retrieval Task' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mohammadshahi Alireza, Lebret Remi, Aberer Karl</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER)</td>
    <td>8</td>
    <td><p>In this paper, we propose a new approach to learn multimodal multilingual
embeddings for matching images and their relevant captions in two languages. We
combine two existing objective functions to make images and captions close in a
joint embedding space while adapting the alignment of word embeddings between
existing languages in our model. We show that our approach enables better
generalization, achieving state-of-the-art performance in text-to-image and
image-to-text retrieval task, and caption-caption similarity task. Two
multimodal multilingual datasets are used for evaluation: Multi30k with German
and English captions and Microsoft-COCO with English and Japanese captions.</p>
</td>
    <td>
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/tian2019sosnet/">Sosnet: Second Order Similarity Regularization For Local Descriptor Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Sosnet: Second Order Similarity Regularization For Local Descriptor Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Sosnet: Second Order Similarity Regularization For Local Descriptor Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tian et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>320</td>
    <td><p>Despite the fact that Second Order Similarity (SOS) has been used with
significant success in tasks such as graph matching and clustering, it has not
been exploited for learning local descriptors. In this work, we explore the
potential of SOS in the field of descriptor learning by building upon the
intuition that a positive pair of matching points should exhibit similar
distances with respect to other points in the embedding space. Thus, we propose
a novel regularization term, named Second Order Similarity Regularization
(SOSR), that follows this principle. By incorporating SOSR into training, our
learned descriptor achieves state-of-the-art performance on several challenging
benchmarks containing distinct tasks ranging from local patch retrieval to
structure from motion. Furthermore, by designing a von Mises-Fischer
distribution based evaluation method, we link the utilization of the descriptor
space to the matching performance, thus demonstrating the effectiveness of our
proposed SOSR. Extensive experimental results, empirical evidence, and in-depth
analysis are provided, indicating that SOSR can significantly boost the
matching performance of the learned descriptor.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/zhong2019compact/">Compact Deep Aggregation For Set Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Compact Deep Aggregation For Set Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Compact Deep Aggregation For Set Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhong Yujie, Arandjelović Relja, Zisserman Andrew</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>8</td>
    <td><p>The objective of this work is to learn a compact embedding of a set of
descriptors that is suitable for efficient retrieval and ranking, whilst
maintaining discriminability of the individual descriptors. We focus on a
specific example of this general problem – that of retrieving images
containing multiple faces from a large scale dataset of images. Here the set
consists of the face descriptors in each image, and given a query for multiple
identities, the goal is then to retrieve, in order, images which contain all
the identities, all but one, \etc
  To this end, we make the following contributions: first, we propose a CNN
architecture – {\em SetNet} – to achieve the objective: it learns face
descriptors and their aggregation over a set to produce a compact fixed length
descriptor designed for set retrieval, and the score of an image is a count of
the number of identities that match the query; second, we show that this
compact descriptor has minimal loss of discriminability up to two faces per
image, and degrades slowly after that – far exceeding a number of baselines;
third, we explore the speed vs.\ retrieval quality trade-off for set retrieval
using this compact descriptor; and, finally, we collect and annotate a large
dataset of images containing various number of celebrities, which we use for
evaluation and is publicly released.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/collomosse2019livesketch/">Livesketch: Query Perturbations For Guided Sketch-based Visual Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Livesketch: Query Perturbations For Guided Sketch-based Visual Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Livesketch: Query Perturbations For Guided Sketch-based Visual Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Collomosse John, Bui Tu, Jin Hailin</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>55</td>
    <td><p>LiveSketch is a novel algorithm for searching large image collections using
hand-sketched queries. LiveSketch tackles the inherent ambiguity of sketch
search by creating visual suggestions that augment the query as it is drawn,
making query specification an iterative rather than one-shot process that helps
disambiguate users’ search intent. Our technical contributions are: a triplet
convnet architecture that incorporates an RNN based variational autoencoder to
search for images using vector (stroke-based) queries; real-time clustering to
identify likely search intents (and so, targets within the search embedding);
and the use of backpropagation from those targets to perturb the input stroke
sequence, so suggesting alterations to the query in order to guide the search.
We show improvements in accuracy and time-to-task over contemporary baselines
using a 67M image corpus.</p>
</td>
    <td>
      
        Image Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/coleman2019sub/">Sub-linear Memory Sketches For Near Neighbor Search On Streaming Data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Sub-linear Memory Sketches For Near Neighbor Search On Streaming Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Sub-linear Memory Sketches For Near Neighbor Search On Streaming Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Coleman Benjamin, Baraniuk Richard G., Shrivastava Anshumali</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of The Web Conference 2020</td>
    <td>11</td>
    <td><p>We present the first sublinear memory sketch that can be queried to find the
nearest neighbors in a dataset. Our online sketching algorithm compresses an N
element dataset to a sketch of size \(O(N^b log^3 N)\) in \(O(N^{(b+1)} log^3
N)\) time, where \(b &lt; 1\). This sketch can correctly report the nearest neighbors
of any query that satisfies a stability condition parameterized by \(b\). We
achieve sublinear memory performance on stable queries by combining recent
advances in locality sensitive hash (LSH)-based estimators, online kernel
density estimation, and compressed sensing. Our theoretical results shed new
light on the memory-accuracy tradeoff for nearest neighbor search, and our
sketch, which consists entirely of short integer arrays, has a variety of
attractive features in practice. We evaluate the memory-recall tradeoff of our
method on a friend recommendation task in the Google Plus social media network.
We obtain orders of magnitude better compression than the random projection
based alternative while retaining the ability to report the nearest neighbors
of practical queries.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/song2019polysemous/">Polysemous Visual-semantic Embedding For Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Polysemous Visual-semantic Embedding For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Polysemous Visual-semantic Embedding For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Song Yale, Soleymani Mohammad</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>223</td>
    <td><p>Visual-semantic embedding aims to find a shared latent space where related
visual and textual instances are close to each other. Most current methods
learn injective embedding functions that map an instance to a single point in
the shared space. Unfortunately, injective embedding cannot effectively handle
polysemous instances with multiple possible meanings; at best, it would find an
average representation of different meanings. This hinders its use in
real-world scenarios where individual instances and their cross-modal
associations are often ambiguous. In this work, we introduce Polysemous
Instance Embedding Networks (PIE-Nets) that compute multiple and diverse
representations of an instance by combining global context with locally-guided
features via multi-head self-attention and residual learning. To learn
visual-semantic embedding, we tie-up two PIE-Nets and optimize them jointly in
the multiple instance learning framework. Most existing work on cross-modal
retrieval focuses on image-text data. Here, we also tackle a more challenging
case of video-text retrieval. To facilitate further research in video-text
retrieval, we release a new dataset of 50K video-sentence pairs collected from
social media, dubbed MRW (my reaction when). We demonstrate our approach on
both image-text and video-text retrieval scenarios using MS-COCO, TGIF, and our
new MRW dataset.</p>
</td>
    <td>
      
        Multimodal Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/song2019deep/">Deep Recurrent Quantization For Generating Sequential Binary Codes</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Recurrent Quantization For Generating Sequential Binary Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Recurrent Quantization For Generating Sequential Binary Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Song et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</td>
    <td>17</td>
    <td><p>Quantization has been an effective technology in ANN (approximate nearest
neighbour) search due to its high accuracy and fast search speed. To meet the
requirement of different applications, there is always a trade-off between
retrieval accuracy and speed, reflected by variable code lengths. However, to
encode the dataset into different code lengths, existing methods need to train
several models, where each model can only produce a specific code length. This
incurs a considerable training time cost, and largely reduces the flexibility
of quantization methods to be deployed in real applications. To address this
issue, we propose a Deep Recurrent Quantization (DRQ) architecture which can
generate sequential binary codes. To the end, when the model is trained, a
sequence of binary codes can be generated and the code length can be easily
controlled by adjusting the number of recurrent iterations. A shared codebook
and a scalar factor is designed to be the learnable weights in the deep
recurrent quantization block, and the whole framework can be trained in an
end-to-end manner. As far as we know, this is the first quantization method
that can be trained once and generate sequential binary codes. Experimental
results on the benchmark datasets show that our model achieves comparable or
even better performance compared with the state-of-the-art for image retrieval.
But it requires significantly less number of parameters and training times. Our
code is published online: https://github.com/cfm-uestc/DRQ.</p>
</td>
    <td>
      
        Compact Codes 
      
        Quantization 
      
        AAAI 
      
        IJCAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/chowdhury2019instance/">Instance-based Inductive Deep Transfer Learning By Cross-dataset Querying With Locality Sensitive Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Instance-based Inductive Deep Transfer Learning By Cross-dataset Querying With Locality Sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Instance-based Inductive Deep Transfer Learning By Cross-dataset Querying With Locality Sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chowdhury Somnath Basu Roy, Annervaz K M, Dukkipati Ambedkar</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)</td>
    <td>5</td>
    <td><p>Supervised learning models are typically trained on a single dataset and the
performance of these models rely heavily on the size of the dataset, i.e.,
amount of data available with the ground truth. Learning algorithms try to
generalize solely based on the data that is presented with during the training.
In this work, we propose an inductive transfer learning method that can augment
learning models by infusing similar instances from different learning tasks in
the Natural Language Processing (NLP) domain. We propose to use instance
representations from a source dataset, \textit{without inheriting anything}
from the source learning model. Representations of the instances of
\textit{source} \&amp; \textit{target} datasets are learned, retrieval of relevant
source instances is performed using soft-attention mechanism and
\textit{locality sensitive hashing}, and then, augmented into the model during
training on the target dataset. Our approach simultaneously exploits the local
\textit{instance level information} as well as the macro statistical viewpoint
of the dataset. Using this approach we have shown significant improvements for
three major news classification datasets over the baseline. Experimental
evaluations also show that the proposed approach reduces dependency on labeled
data by a significant margin for comparable performance. With our proposed
cross dataset learning procedure we show that one can achieve
competitive/better performance than learning from a single dataset.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Hashing Methods 
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/christiani2019fast/">Fast Locality-sensitive Hashing Frameworks For Approximate Near Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Locality-sensitive Hashing Frameworks For Approximate Near Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Locality-sensitive Hashing Frameworks For Approximate Near Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Christiani Tobias</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>14</td>
    <td><p>The Indyk-Motwani Locality-Sensitive Hashing (LSH) framework (STOC 1998) is a
general technique for constructing a data structure to answer approximate near
neighbor queries by using a distribution \(\mathcal{H}\) over locality-sensitive
hash functions that partition space. For a collection of \(n\) points, after
preprocessing, the query time is dominated by \(O(n^{\rho} log n)\) evaluations
of hash functions from \(\mathcal{H}\) and \(O(n^{\rho})\) hash table lookups and
distance computations where \(\rho \in (0,1)\) is determined by the
locality-sensitivity properties of \(\mathcal{H}\). It follows from a recent
result by Dahlgaard et al. (FOCS 2017) that the number of locality-sensitive
hash functions can be reduced to \(O(log^2 n)\), leaving the query time to be
dominated by \(O(n^{\rho})\) distance computations and \(O(n^{\rho} log n)\)
additional word-RAM operations. We state this result as a general framework and
provide a simpler analysis showing that the number of lookups and distance
computations closely match the Indyk-Motwani framework, making it a viable
replacement in practice. Using ideas from another locality-sensitive hashing
framework by Andoni and Indyk (SODA 2006) we are able to reduce the number of
additional word-RAM operations to \(O(n^\rho)\).</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Hashing Methods 
      
        Tools & Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/oguni2019character/">Character 3-gram Mover's Distance: An Effective Method For Detecting Near-duplicate Japanese-language Recipes</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Character 3-gram Mover's Distance: An Effective Method For Detecting Near-duplicate Japanese-language Recipes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Character 3-gram Mover's Distance: An Effective Method For Detecting Near-duplicate Japanese-language Recipes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Oguni Masaki, Seki Yohei, Hirate Yu</td> <!-- 🔧 You were missing this -->
    <td>2010 Asia Pacific Software Engineering Conference</td>
    <td>96</td>
    <td><p>In user-generated recipe websites, users post their-original recipes. Some
recipes, however, are very similar in major components such as the cooking
instructions to other recipes. We refer to such recipes as “near-duplicate
recipes”. In this study, we propose a method that extends the “Word Mover’s
Distance”, which calculates distances between texts based on word embedding, to
character 3-gram embedding. Using a corpus of over 1.21 million recipes, we
learned the word embedding and the character 3-gram embedding by using a
Skip-Gram model with negative sampling and fastText to extract candidate pairs
of near-duplicate recipes. We then annotated these candidates and evaluated the
proposed method against a comparison method. Our results demonstrated that
near-duplicate recipes that were not detected by the comparison method were
successfully detected by the proposed method.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/chiu2019learning/">Learning To Index For Nearest Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning To Index For Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning To Index For Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chiu Chih-yi, Prayoonwong Amorntip, Liao Yin-chih</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>24</td>
    <td><p>In this study, we present a novel ranking model based on learning
neighborhood relationships embedded in the index space. Given a query point,
conventional approximate nearest neighbor search calculates the distances to
the cluster centroids, before ranking the clusters from near to far based on
the distances. The data indexed in the top-ranked clusters are retrieved and
treated as the nearest neighbor candidates for the query. However, the loss of
quantization between the data and cluster centroids will inevitably harm the
search accuracy. To address this problem, the proposed model ranks clusters
based on their nearest neighbor probabilities rather than the query-centroid
distances. The nearest neighbor probabilities are estimated by employing neural
networks to characterize the neighborhood relationships, i.e., the density
function of nearest neighbors with respect to the query. The proposed
probability-based ranking can replace the conventional distance-based ranking
for finding candidate clusters, and the predicted probability can be used to
determine the data quantity to be retrieved from the candidate cluster. Our
experimental results demonstrated that the proposed ranking model could boost
the search performance effectively in billion-scale datasets.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/yang2019asymmetric/">Asymmetric Deep Semantic Quantization For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Asymmetric Deep Semantic Quantization For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Asymmetric Deep Semantic Quantization For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yang et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Access</td>
    <td>5</td>
    <td><p>Due to its fast retrieval and storage efficiency capabilities, hashing has
been widely used in nearest neighbor retrieval tasks. By using deep learning
based techniques, hashing can outperform non-learning based hashing technique
in many applications. However, we argue that the current deep learning based
hashing methods ignore some critical problems (e.g., the learned hash codes are
not discriminative due to the hashing methods being unable to discover rich
semantic information and the training strategy having difficulty optimizing the
discrete binary codes). In this paper, we propose a novel image hashing method,
termed as \textbf{\underline{A}}symmetric \textbf{\underline{D}}eep
\textbf{\underline{S}}emantic \textbf{\underline{Q}}uantization
(\textbf{ADSQ}). \textbf{ADSQ} is implemented using three stream frameworks,
which consist of one <em>LabelNet</em> and two <em>ImgNets</em>. The
<em>LabelNet</em> leverages the power of three fully-connected layers, which are
used to capture rich semantic information between image pairs. For the two
<em>ImgNets</em>, they each adopt the same convolutional neural network
structure, but with different weights (i.e., asymmetric convolutional neural
networks). The two <em>ImgNets</em> are used to generate discriminative compact
hash codes. Specifically, the function of the <em>LabelNet</em> is to capture
rich semantic information that is used to guide the two <em>ImgNets</em> in
minimizing the gap between the real-continuous features and the discrete binary
codes. Furthermore, \textbf{ADSQ} can utilize the most critical semantic
information to guide the feature learning process and consider the consistency
of the common semantic space and Hamming space. Experimental results on three
benchmarks (i.e., CIFAR-10, NUS-WIDE, and ImageNet) demonstrate that the
proposed \textbf{ADSQ} can outperforms current state-of-the-art methods.</p>
</td>
    <td>
      
        Quantization 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/yuan2019signal/">Signal-to-noise Ratio: A Robust Distance Metric For Deep Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Signal-to-noise Ratio: A Robust Distance Metric For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Signal-to-noise Ratio: A Robust Distance Metric For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yuan et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>81</td>
    <td><p>Deep metric learning, which learns discriminative features to process image
clustering and retrieval tasks, has attracted extensive attention in recent
years. A number of deep metric learning methods, which ensure that similar
examples are mapped close to each other and dissimilar examples are mapped
farther apart, have been proposed to construct effective structures for loss
functions and have shown promising results. In this paper, different from the
approaches on learning the loss structures, we propose a robust SNR distance
metric based on Signal-to-Noise Ratio (SNR) for measuring the similarity of
image pairs for deep metric learning. By exploring the properties of our SNR
distance metric from the view of geometry space and statistical theory, we
analyze the properties of our metric and show that it can preserve the semantic
similarity between image pairs, which well justify its suitability for deep
metric learning. Compared with Euclidean distance metric, our SNR distance
metric can further jointly reduce the intra-class distances and enlarge the
inter-class distances for learned features. Leveraging our SNR distance metric,
we propose Deep SNR-based Metric Learning (DSML) to generate discriminative
feature embeddings. By extensive experiments on three widely adopted
benchmarks, including CARS196, CUB200-2011 and CIFAR10, our DSML has shown its
superiority over other state-of-the-art methods. Additionally, we extend our
SNR distance metric to deep hashing learning, and conduct experiments on two
benchmarks, including CIFAR10 and NUS-WIDE, to demonstrate the effectiveness
and generality of our SNR distance metric.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/subramanya2019diskann/">Diskann: Fast Accurate Billion-point Nearest Neighbor Search On A Single Node</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Diskann: Fast Accurate Billion-point Nearest Neighbor Search On A Single Node' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Diskann: Fast Accurate Billion-point Nearest Neighbor Search On A Single Node' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Subramanya et al.</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>36</td>
    <td><p>Current state-of-the-art approximate nearest neighbor search (ANNS) algorithms generate indices that must be stored in main memory for fast high-recall search. This makes them expensive and limits the size of the dataset. We present a new graph-based indexing and search system called DiskANN that can index, store, and search a billion point database on a single workstation with just 64GB RAM and an inexpensive solid-state drive (SSD). Contrary to current wisdom, we demonstrate that the SSD-based indices built by DiskANN can meet all three desiderata for large-scale ANNS: high-recall, low query latency and high density (points indexed per node). On the billion point SIFT1B bigann dataset, DiskANN serves &gt; 5000 queries a second with &lt; 3ms mean latency and 95%+ 1-recall@1 on a 16 core machine, where state-of-the-art billion-point ANNS algorithms with similar memory footprint like FAISS and IVFOADC+G+P plateau at around 50% 1-recall@1. Alternately, in the high recall regime, DiskANN can index and serve 5 − 10x more points per node compared to state-of-the-art graph- based methods such as HNSW and NSG. Finally, as part of our overall DiskANN system, we introduce Vamana, a new graph-based ANNS index that is more versatile than the graph indices even for in-memory indices.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/tarawneh2019deep/">Deep Face Image Retrieval: A Comparative Study With Dictionary Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Face Image Retrieval: A Comparative Study With Dictionary Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Face Image Retrieval: A Comparative Study With Dictionary Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tarawneh et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 10th International Conference on Information and Communication Systems (ICICS)</td>
    <td>35</td>
    <td><p>Facial image retrieval is a challenging task since faces have many similar
features (areas), which makes it difficult for the retrieval systems to
distinguish faces of different people. With the advent of deep learning, deep
networks are often applied to extract powerful features that are used in many
areas of computer vision. This paper investigates the application of different
deep learning models for face image retrieval, namely, Alexlayer6, Alexlayer7,
VGG16layer6, VGG16layer7, VGG19layer6, and VGG19layer7, with two types of
dictionary learning techniques, namely \(K\)-means and \(K\)-SVD. We also
investigate some coefficient learning techniques such as the Homotopy, Lasso,
Elastic Net and SSF and their effect on the face retrieval system. The
comparative results of the experiments conducted on three standard face image
datasets show that the best performers for face image retrieval are Alexlayer7
with \(K\)-means and SSF, Alexlayer6 with \(K\)-SVD and SSF, and Alexlayer6 with
\(K\)-means and SSF. The APR and ARR of these methods were further compared to
some of the state of the art methods based on local descriptors. The
experimental results show that deep learning outperforms most of those methods
and therefore can be recommended for use in practice of face image retrieval</p>
</td>
    <td>
      
        Survey Paper 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/wu2019deep/">Deep Incremental Hashing Network For Efficient Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Incremental Hashing Network For Efficient Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Incremental Hashing Network For Efficient Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>99</td>
    <td><p>Hashing has shown great potential in large-scale image retrieval due to its storage and computation efficiency, especially the recent deep supervised hashing methods. To achieve promising performance, deep supervised hashing methods require a large amount of training data from different classes. However, when images of new categories emerge, existing deep hashing methods have to retrain the CNN model and generate hash codes for all the database images again, which is impractical for large-scale retrieval system.
In this paper, we propose a novel deep hashing framework, called Deep Incremental Hashing Network (DIHN), for learning hash codes in an incremental manner. DIHN learns the hash codes for the new coming images directly, while keeping the old ones unchanged. Simultaneously, a deep hash function for query set is learned by preserving the similarities between training points. Extensive experiments on two widely used image retrieval benchmarks demonstrate that the proposed DIHN framework can significantly decrease the training time while keeping the state-of-the-art retrieval accuracy.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/tanioka2019fast/">A Fast Content-based Image Retrieval Method Using Deep Visual Features</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Fast Content-based Image Retrieval Method Using Deep Visual Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Fast Content-based Image Retrieval Method Using Deep Visual Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tanioka Hiroki</td> <!-- 🔧 You were missing this -->
    <td>2019 International Conference on Document Analysis and Recognition Workshops (ICDARW)</td>
    <td>10</td>
    <td><p>Fast and scalable Content-Based Image Retrieval using visual features is
required for document analysis, Medical image analysis, etc. in the present
age. Convolutional Neural Network (CNN) activations as features achieved their
outstanding performance in this area. Deep Convolutional representations using
the softmax function in the output layer are also ones among visual features.
However, almost all the image retrieval systems hold their index of visual
features on main memory in order to high responsiveness, limiting their
applicability for big data applications. In this paper, we propose a fast
calculation method of cosine similarity with L2 norm indexed in advance on
Elasticsearch. We evaluate our approach with ImageNet Dataset and VGG-16
pre-trained model. The evaluation results show the effectiveness and efficiency
of our proposed method.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/chen2019making/">Making Online Sketching Hashing Even Faster</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Making Online Sketching Hashing Even Faster' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Making Online Sketching Hashing Even Faster' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Knowledge and Data Engineering</td>
    <td>21</td>
    <td><p>Data-dependent hashing methods have demonstrated good performance in various
machine learning applications to learn a low-dimensional representation from
the original data. However, they still suffer from several obstacles: First,
most of existing hashing methods are trained in a batch mode, yielding
inefficiency for training streaming data. Second, the computational cost and
the memory consumption increase extraordinarily in the big data setting, which
perplexes the training procedure. Third, the lack of labeled data hinders the
improvement of the model performance. To address these difficulties, we utilize
online sketching hashing (OSH) and present a FasteR Online Sketching Hashing
(FROSH) algorithm to sketch the data in a more compact form via an independent
transformation. We provide theoretical justification to guarantee that our
proposed FROSH consumes less time and achieves a comparable sketching precision
under the same memory cost of OSH. We also extend FROSH to its distributed
implementation, namely DFROSH, to further reduce the training time cost of
FROSH while deriving the theoretical bound of the sketching precision. Finally,
we conduct extensive experiments on both synthetic and real datasets to
demonstrate the attractive merits of FROSH and DFROSH.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/chen2019improving/">Improving Deep Binary Embedding Networks By Order-aware Reweighting Of Triplets</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Improving Deep Binary Embedding Networks By Order-aware Reweighting Of Triplets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Improving Deep Binary Embedding Networks By Order-aware Reweighting Of Triplets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Circuits and Systems for Video Technology</td>
    <td>12</td>
    <td><p>In this paper, we focus on triplet-based deep binary embedding networks for
image retrieval task. The triplet loss has been shown to be most effective for
the ranking problem. However, most of the previous works treat the triplets
equally or select the hard triplets based on the loss. Such strategies do not
consider the order relations, which is important for retrieval task. To this
end, we propose an order-aware reweighting method to effectively train the
triplet-based deep networks, which up-weights the important triplets and
down-weights the uninformative triplets. First, we present the order-aware
weighting factors to indicate the importance of the triplets, which depend on
the rank order of binary codes. Then, we reshape the triplet loss to the
squared triplet loss such that the loss function will put more weights on the
important triplets. Extensive evaluations on four benchmark datasets show that
the proposed method achieves significant performance compared with the
state-of-the-art baselines.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/chen2019hybrid/">Hybrid-attention Based Decoupled Metric Learning For Zero-shot Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hybrid-attention Based Decoupled Metric Learning For Zero-shot Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hybrid-attention Based Decoupled Metric Learning For Zero-shot Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen Binghui, Deng Weihong</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>66</td>
    <td><p>In zero-shot image retrieval (ZSIR) task, embedding learning becomes more
attractive, however, many methods follow the traditional metric learning idea
and omit the problems behind zero-shot settings. In this paper, we first
emphasize the importance of learning visual discriminative metric and
preventing the partial/selective learning behavior of learner in ZSIR, and then
propose the Decoupled Metric Learning (DeML) framework to achieve these
individually. Instead of coarsely optimizing an unified metric, we decouple it
into multiple attention-specific parts so as to recurrently induce the
discrimination and explicitly enhance the generalization. And they are mainly
achieved by our object-attention module based on random walk graph propagation
and the channel-attention module based on the adversary constraint,
respectively. We demonstrate the necessity of addressing the vital problems in
ZSIR on the popular benchmarks, outperforming the state-of-theart methods by a
significant margin. Code is available at http://www.bhchen.cn</p>
</td>
    <td>
      
        Few Shot & Zero Shot 
      
        Distance Metric Learning 
      
        Image Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/chen2019locality/">Locality-sensitive Hashing For F-divergences: Mutual Information Loss And Beyond</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Locality-sensitive Hashing For F-divergences: Mutual Information Loss And Beyond' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Locality-sensitive Hashing For F-divergences: Mutual Information Loss And Beyond' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms</td>
    <td>62</td>
    <td><p>Computing approximate nearest neighbors in high dimensional spaces is a
central problem in large-scale data mining with a wide range of applications in
machine learning and data science. A popular and effective technique in
computing nearest neighbors approximately is the locality-sensitive hashing
(LSH) scheme. In this paper, we aim to develop LSH schemes for distance
functions that measure the distance between two probability distributions,
particularly for f-divergences as well as a generalization to capture mutual
information loss. First, we provide a general framework to design LHS schemes
for f-divergence distance functions and develop LSH schemes for the generalized
Jensen-Shannon divergence and triangular discrimination in this framework. We
show a two-sided approximation result for approximation of the generalized
Jensen-Shannon divergence by the Hellinger distance, which may be of
independent interest. Next, we show a general method of reducing the problem of
designing an LSH scheme for a Krein kernel (which can be expressed as the
difference of two positive definite kernels) to the problem of maximum inner
product search. We exemplify this method by applying it to the mutual
information loss, due to its several important applications such as model
compression.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/chen2019two/">A Two-step Cross-modal Hashing By Exploiting Label Correlations And Preserving Similarity In Both Steps</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Two-step Cross-modal Hashing By Exploiting Label Correlations And Preserving Similarity In Both Steps' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Two-step Cross-modal Hashing By Exploiting Label Correlations And Preserving Similarity In Both Steps' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 27th ACM International Conference on Multimedia</td>
    <td>45</td>
    <td><p>In this paper, we present a novel Two-stEp Cross-modal Hashing method, TECH for short, for cross-modal retrieval tasks. As a two-step method, it first learns hash codes based on semantic labels, while preserving the similarity in the original space and exploiting the label correlations in the label space. In the light of this, it is able to make better use of label information and generate better binary codes. In addition, different from other two-step methods that mainly focus on the hash codes learning, TECH adopts a new hash function learning strategy in the second step, which also preserves the similarity in the original space. Moreover, with the help of well designed objective function and optimization scheme, it is able to generate hash codes discretely and scalable for large scale data. To the best of our knowledge, it is the first cross-modal hashing method exploiting label correlations, and also the first two-step hashing model preserving the similarity while leaning hash function. Extensive experiments demonstrate that the proposed approach outperforms some state-of-the-art cross-modal hashing methods.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/chen2019deep/">Deep Supervised Hashing With Anchor Graph</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Supervised Hashing With Anchor Graph' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Supervised Hashing With Anchor Graph' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>52</td>
    <td><p>Recently, a series of deep supervised hashing methods were proposed for binary code learning. However, due to the high computation cost and the limited hardware’s memory, these methods will first select a subset from the training set, and then form a mini-batch data to update the network in each iteration. Therefore, the remaining labeled data cannot be fully utilized and the model cannot directly obtain the binary codes of the entire training set for retrieval. To address these problems, this paper proposes an interesting regularized deep model to seamlessly integrate the advantages of deep hashing and efficient binary code learning by using the anchor graph. As such, the deep features and label matrix can be jointly used to optimize the binary codes, and the network can obtain more discriminative feedback from the linear combinations of the learned bits. Moreover, we also reveal the algorithm mechanism and its computation essence. Experiments on three large-scale datasets indicate that the proposed method achieves better retrieval performance with less training time compared to previous deep hashing methods.</p>
</td>
    <td>
      
        Unsupervised 
      
        Neural Hashing 
      
        SUPERVISED 
      
        Hashing Methods 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/chen2019differentiable/">Differentiable Product Quantization For End-to-end Embedding Compression</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Differentiable Product Quantization For End-to-end Embedding Compression' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Differentiable Product Quantization For End-to-end Embedding Compression' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen Ting, Li Lala, Sun Yizhou</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>13</td>
    <td><p>Embedding layers are commonly used to map discrete symbols into continuous
embedding vectors that reflect their semantic meanings. Despite their
effectiveness, the number of parameters in an embedding layer increases
linearly with the number of symbols and poses a critical challenge on memory
and storage constraints. In this work, we propose a generic and end-to-end
learnable compression framework termed differentiable product quantization
(DPQ). We present two instantiations of DPQ that leverage different
approximation techniques to enable differentiability in end-to-end learning.
Our method can readily serve as a drop-in alternative for any existing
embedding layer. Empirically, DPQ offers significant compression ratios
(14-238\(\times\)) at negligible or no performance cost on 10 datasets across
three different language tasks.</p>
</td>
    <td>
      
        Quantization 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/chen2019hadamard/">Hadamard Codebook Based Deep Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hadamard Codebook Based Deep Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hadamard Codebook Based Deep Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 26th ACM international conference on Multimedia</td>
    <td>47</td>
    <td><p>As an approximate nearest neighbor search technique, hashing has been widely
applied in large-scale image retrieval due to its excellent efficiency. Most
supervised deep hashing methods have similar loss designs with embedding
learning, while quantizing the continuous high-dim feature into compact binary
space. We argue that the existing deep hashing schemes are defective in two
issues that seriously affect the performance, i.e., bit independence and bit
balance. The former refers to hash codes of different classes should be
independent of each other, while the latter means each bit should have a
balanced distribution of +1s and -1s. In this paper, we propose a novel
supervised deep hashing method, termed Hadamard Codebook based Deep Hashing
(HCDH), which solves the above two problems in a unified formulation.
Specifically, we utilize an off-the-shelf algorithm to generate a binary
Hadamard codebook to satisfy the requirement of bit independence and bit
balance, which subsequently serves as the desired outputs of the hash functions
learning. We also introduce a projection matrix to solve the inconsistency
between the order of Hadamard matrix and the number of classes. Besides, the
proposed HCDH further exploits the supervised labels by constructing a
classifier on top of the outputs of hash functions. Extensive experiments
demonstrate that HCDH can yield discriminative and balanced binary codes, which
well outperforms many state-of-the-arts on three widely-used benchmarks.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/chen2019vector/">Vector And Line Quantization For Billion-scale Similarity Search On Gpus</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Vector And Line Quantization For Billion-scale Similarity Search On Gpus' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Vector And Line Quantization For Billion-scale Similarity Search On Gpus' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen et al.</td> <!-- 🔧 You were missing this -->
    <td>Future Generation Computer Systems</td>
    <td>7</td>
    <td><p>Billion-scale high-dimensional approximate nearest neighbour (ANN) search has
become an important problem for searching similar objects among the vast amount
of images and videos available online. The existing ANN methods are usually
characterized by their specific indexing structures, including the inverted
index and the inverted multi-index structure. The inverted index structure is
amenable to GPU-based implementations, and the state-of-the-art systems such as
Faiss are able to exploit the massive parallelism offered by GPUs. However, the
inverted index requires high memory overhead to index the dataset effectively.
The inverted multi-index structure is difficult to implement for GPUs, and also
ineffective in dealing with database with different data distributions. In this
paper we propose a novel hierarchical inverted index structure generated by
vector and line quantization methods. Our quantization method improves both
search efficiency and accuracy, while maintaining comparable memory
consumption. This is achieved by reducing search space and increasing the
number of indexed regions. We introduce a new ANN search system, VLQ-ADC, that
is based on the proposed inverted index, and perform extensive evaluation on
two public billion-scale benchmark datasets SIFT1B and DEEP1B. Our evaluation
shows that VLQ-ADC significantly outperforms the state-of-the-art GPU- and
CPU-based systems in terms of both accuracy and search speed. The source code
of VLQ-ADC is available at
https://github.com/zjuchenwei/vector-line-quantization.</p>
</td>
    <td>
      
        Quantization 
      
        SCALABILITY 
      
        Large Scale Search 
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/patel2019tinysearch/">Tinysearch -- Semantics Based Search Engine Using Bert Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Tinysearch -- Semantics Based Search Engine Using Bert Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Tinysearch -- Semantics Based Search Engine Using Bert Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Patel Manish</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>7</td>
    <td><p>Existing search engines use keyword matching or tf-idf based matching to map
the query to the web-documents and rank them. They also consider other factors
such as page rank, hubs-and-authority scores, knowledge graphs to make the
results more meaningful. However, the existing search engines fail to capture
the meaning of query when it becomes large and complex. BERT, introduced by
Google in 2018, provides embeddings for words as well as sentences. In this
paper, I have developed a semantics-oriented search engine using neural
networks and BERT embeddings that can search for query and rank the documents
in the order of the most meaningful to least meaningful. The results shows
improvement over one existing search engine for complex queries for given set
of documents.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/cao2019unsupervised/">Unsupervised Deep Metric Learning Via Auxiliary Rotation Loss</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Deep Metric Learning Via Auxiliary Rotation Loss' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Deep Metric Learning Via Auxiliary Rotation Loss' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cao Xuefei, Chen Bor-chun, Lim Ser-nam</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>12</td>
    <td><p>Deep metric learning is an important area due to its applicability to many
domains such as image retrieval and person re-identification. The main drawback
of such models is the necessity for labeled data. In this work, we propose to
generate pseudo-labels for deep metric learning directly from clustering
assignment and we introduce unsupervised deep metric learning (UDML)
regularized by a self-supervision (SS) task. In particular, we propose to
regularize the training process by predicting image rotations. Our method
(UDML-SS) jointly learns discriminative embeddings, unsupervised clustering
assignments of the embeddings, as well as a self-supervised pretext task.
UDML-SS iteratively cluster embeddings using traditional clustering algorithm
(e.g., k-means), and sampling training pairs based on the cluster assignment
for metric learning, while optimizing self-supervised pretext task in a
multi-task fashion. The role of self-supervision is to stabilize the training
process and encourages the model to learn meaningful feature representations
that are not distorted due to unreliable clustering assignments. The proposed
method performs well on standard benchmarks for metric learning, where it
outperforms current state-of-the-art approaches by a large margin and it also
shows competitive performance with various metric learning loss functions.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Distance Metric Learning 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/stylianou2019visualizing/">Visualizing Deep Similarity Networks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Visualizing Deep Similarity Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Visualizing Deep Similarity Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Stylianou Abby, Souvenir Richard, Pless Robert</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>46</td>
    <td><p>For convolutional neural network models that optimize an image embedding, we
propose a method to highlight the regions of images that contribute most to
pairwise similarity. This work is a corollary to the visualization tools
developed for classification networks, but applicable to the problem domains
better suited to similarity learning. The visualization shows how similarity
networks that are fine-tuned learn to focus on different features. We also
generalize our approach to embedding networks that use different pooling
strategies and provide a simple mechanism to support image similarity searches
on objects or sub-regions in the query image.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/passalis2019deep/">Deep Supervised Hashing Leveraging Quadratic Spherical Mutual Information For Content-based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Supervised Hashing Leveraging Quadratic Spherical Mutual Information For Content-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Supervised Hashing Leveraging Quadratic Spherical Mutual Information For Content-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Passalis Nikolaos, Tefas Anastasios</td> <!-- 🔧 You were missing this -->
    <td>Signal Processing: Image Communication</td>
    <td>14</td>
    <td><p>Several deep supervised hashing techniques have been proposed to allow for
efficiently querying large image databases. However, deep supervised image
hashing techniques are developed, to a great extent, heuristically often
leading to suboptimal results. Contrary to this, we propose an efficient deep
supervised hashing algorithm that optimizes the learned codes using an
information-theoretic measure, the Quadratic Mutual Information (QMI). The
proposed method is adapted to the needs of large-scale hashing and information
retrieval leading to a novel information-theoretic measure, the Quadratic
Spherical Mutual Information (QSMI). Apart from demonstrating the effectiveness
of the proposed method under different scenarios and outperforming existing
state-of-the-art image hashing techniques, this paper provides a structured way
to model the process of information retrieval and develop novel methods adapted
to the needs of each application.</p>
</td>
    <td>
      
        Image Retrieval 
      
        Unsupervised 
      
        Neural Hashing 
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/cao2019end/">End-to-end Latent Fingerprint Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=End-to-end Latent Fingerprint Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=End-to-end Latent Fingerprint Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cao et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Information Forensics and Security</td>
    <td>65</td>
    <td><p>Latent fingerprints are one of the most important and widely used sources of
evidence in law enforcement and forensic agencies. Yet the performance of the
state-of-the-art latent recognition systems is far from satisfactory, and they
often require manual markups to boost the latent search performance. Further,
the COTS systems are proprietary and do not output the true comparison scores
between a latent and reference prints to conduct quantitative evidential
analysis. We present an end-to-end latent fingerprint search system, including
automated region of interest (ROI) cropping, latent image preprocessing,
feature extraction, feature comparison , and outputs a candidate list. Two
separate minutiae extraction models provide complementary minutiae templates.
To compensate for the small number of minutiae in small area and poor quality
latents, a virtual minutiae set is generated to construct a texture template. A
96-dimensional descriptor is extracted for each minutia from its neighborhood.
For computational efficiency, the descriptor length for virtual minutiae is
further reduced to 16 using product quantization. Our end-to-end system is
evaluated on three latent databases: NIST SD27 (258 latents); MSP (1,200
latents), WVU (449 latents) and N2N (10,000 latents) against a background set
of 100K rolled prints, which includes the true rolled mates of the latents with
rank-1 retrieval rates of 65.7%, 69.4%, 65.5%, and 7.6% respectively. A
multi-core solution implemented on 24 cores obtains 1ms per latent to rolled
comparison.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/camara2019spatio/">Spatio-semantic Convnet-based Visual Place Recognition</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Spatio-semantic Convnet-based Visual Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Spatio-semantic Convnet-based Visual Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Camara Luis G., Přeučil Libor</td> <!-- 🔧 You were missing this -->
    <td>2019 European Conference on Mobile Robots (ECMR)</td>
    <td>25</td>
    <td><p>We present a Visual Place Recognition system that follows the two-stage
format common to image retrieval pipelines. The system encodes images of places
by employing the activations of different layers of a pre-trained,
off-the-shelf, VGG16 Convolutional Neural Network (CNN) architecture. In the
first stage of our method and given a query image of a place, a number of top
candidate images is retrieved from a previously stored database of places. In
the second stage, we propose an exhaustive comparison of the query image
against these candidates by encoding semantic and spatial information in the
form of CNN features. Results from our approach outperform by a large margin
state-of-the-art visual place recognition methods on five of the most commonly
used benchmark datasets. The performance gain is especially remarkable on the
most challenging datasets, with more than a twofold recognition improvement
with respect to the latest published work.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/cakir2019hashing/">Hashing With Mutual Information</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing With Mutual Information' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing With Mutual Information' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cakir et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>91</td>
    <td><p>Binary vector embeddings enable fast nearest neighbor retrieval in large
databases of high-dimensional objects, and play an important role in many
practical applications, such as image and video retrieval. We study the problem
of learning binary vector embeddings under a supervised setting, also known as
hashing. We propose a novel supervised hashing method based on optimizing an
information-theoretic quantity: mutual information. We show that optimizing
mutual information can reduce ambiguity in the induced neighborhood structure
in the learned Hamming space, which is essential in obtaining high retrieval
performance. To this end, we optimize mutual information in deep neural
networks with minibatch stochastic gradient descent, with a formulation that
maximally and efficiently utilizes available supervision. Experiments on four
image retrieval benchmarks, including ImageNet, confirm the effectiveness of
our method in learning high-quality binary embeddings for nearest neighbor
retrieval.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/cai2019revisit/">A Revisit Of Hashing Algorithms For Approximate Nearest Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Revisit Of Hashing Algorithms For Approximate Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Revisit Of Hashing Algorithms For Approximate Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cai Deng</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Knowledge and Data Engineering</td>
    <td>29</td>
    <td><p>Approximate Nearest Neighbor Search (ANNS) is a fundamental problem in many
areas of machine learning and data mining. During the past decade, numerous
hashing algorithms are proposed to solve this problem. Every proposed algorithm
claims outperform other state-of-the-art hashing methods. However, the
evaluation of these hashing papers was not thorough enough, and those claims
should be re-examined. The ultimate goal of an ANNS method is returning the
most accurate answers (nearest neighbors) in the shortest time. If implemented
correctly, almost all the hashing methods will have their performance improved
as the code length increases. However, many existing hashing papers only report
the performance with the code length shorter than 128. In this paper, we
carefully revisit the problem of search with a hash index, and analyze the pros
and cons of two popular hash index search procedures. Then we proposed a very
simple but effective two level index structures and make a thorough comparison
of eleven popular hashing algorithms. Surprisingly, the random-projection-based
Locality Sensitive Hashing (LSH) is the best performed algorithm, which is in
contradiction to the claims in all the other ten hashing papers. Despite the
extreme simplicity of random-projection-based LSH, our results show that the
capability of this algorithm has been far underestimated. For the sake of
reproducibility, all the codes used in the paper are released on GitHub, which
can be used as a testing platform for a fair comparison between various hashing
algorithms.</p>
</td>
    <td>
      
        Similarity Search 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/cao2019enhancing/">Enhancing Remote Sensing Image Retrieval With Triplet Deep Metric Learning Network</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Enhancing Remote Sensing Image Retrieval With Triplet Deep Metric Learning Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Enhancing Remote Sensing Image Retrieval With Triplet Deep Metric Learning Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cao et al.</td> <!-- 🔧 You were missing this -->
    <td>International Journal of Remote Sensing</td>
    <td>94</td>
    <td><p>With the rapid growing of remotely sensed imagery data, there is a high
demand for effective and efficient image retrieval tools to manage and exploit
such data. In this letter, we present a novel content-based remote sensing
image retrieval method based on Triplet deep metric learning convolutional
neural network (CNN). By constructing a Triplet network with metric learning
objective function, we extract the representative features of the images in a
semantic space in which images from the same class are close to each other
while those from different classes are far apart. In such a semantic space,
simple metric measures such as Euclidean distance can be used directly to
compare the similarity of images and effectively retrieve images of the same
class. We also investigate a supervised and an unsupervised learning methods
for reducing the dimensionality of the learned semantic features. We present
comprehensive experimental results on two publicly available remote sensing
image retrieval datasets and show that our method significantly outperforms
state-of-the-art.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/peng2019deep/">Deep Reinforcement Learning For Image Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Reinforcement Learning For Image Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Reinforcement Learning For Image Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Peng Yuxin, Zhang Jian, Ye Zhaoda</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>44</td>
    <td><p>Deep hashing methods have received much attention recently, which achieve
promising results by taking advantage of the strong representation power of
deep networks. However, most existing deep hashing methods learn a whole set of
hashing functions independently, while ignore the correlations between
different hashing functions that can promote the retrieval accuracy greatly.
Inspired by the sequential decision ability of deep reinforcement learning, we
propose a new Deep Reinforcement Learning approach for Image Hashing (DRLIH).
Our proposed DRLIH approach models the hashing learning problem as a sequential
decision process, which learns each hashing function by correcting the errors
imposed by previous ones and promotes retrieval accuracy. To the best of our
knowledge, this is the first work to address hashing problem from deep
reinforcement learning perspective. The main contributions of our proposed
DRLIH approach can be summarized as follows: (1) We propose a deep
reinforcement learning hashing network. In the proposed network, we utilize
recurrent neural network (RNN) as agents to model the hashing functions, which
take actions of projecting images into binary codes sequentially, so that the
current hashing function learning can take previous hashing functions’ error
into account. (2) We propose a sequential learning strategy based on proposed
DRLIH. We define the state as a tuple of internal features of RNN’s hidden
layers and image features, which can reflect history decisions made by the
agents. We also propose an action group method to enhance the correlation of
hash functions in the same group. Experiments on three widely-used datasets
demonstrate the effectiveness of our proposed DRLIH approach.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/studer2019comprehensive/">A Comprehensive Study Of Imagenet Pre-training For Historical Document Image Analysis</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Comprehensive Study Of Imagenet Pre-training For Historical Document Image Analysis' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Comprehensive Study Of Imagenet Pre-training For Historical Document Image Analysis' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Studer et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 International Conference on Document Analysis and Recognition (ICDAR)</td>
    <td>66</td>
    <td><p>Automatic analysis of scanned historical documents comprises a wide range of
image analysis tasks, which are often challenging for machine learning due to a
lack of human-annotated learning samples. With the advent of deep neural
networks, a promising way to cope with the lack of training data is to
pre-train models on images from a different domain and then fine-tune them on
historical documents. In the current research, a typical example of such
cross-domain transfer learning is the use of neural networks that have been
pre-trained on the ImageNet database for object recognition. It remains a
mostly open question whether or not this pre-training helps to analyse
historical documents, which have fundamentally different image properties when
compared with ImageNet. In this paper, we present a comprehensive empirical
survey on the effect of ImageNet pre-training for diverse historical document
analysis tasks, including character recognition, style classification,
manuscript dating, semantic segmentation, and content-based retrieval. While we
obtain mixed results for semantic segmentation at pixel-level, we observe a
clear trend across different network architectures that ImageNet pre-training
has a positive effect on classification as well as content-based retrieval.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/bhunia2019texture/">Texture Synthesis Guided Deep Hashing For Texture Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Texture Synthesis Guided Deep Hashing For Texture Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Texture Synthesis Guided Deep Hashing For Texture Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Bhunia et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>17</td>
    <td><p>With the large-scale explosion of images and videos over the internet,
efficient hashing methods have been developed to facilitate memory and time
efficient retrieval of similar images. However, none of the existing works uses
hashing to address texture image retrieval mostly because of the lack of
sufficiently large texture image databases. Our work addresses this problem by
developing a novel deep learning architecture that generates binary hash codes
for input texture images. For this, we first pre-train a Texture Synthesis
Network (TSN) which takes a texture patch as input and outputs an enlarged view
of the texture by injecting newer texture content. Thus it signifies that the
TSN encodes the learnt texture specific information in its intermediate layers.
In the next stage, a second network gathers the multi-scale feature
representations from the TSN’s intermediate layers using channel-wise
attention, combines them in a progressive manner to a dense continuous
representation which is finally converted into a binary hash code with the help
of individual and pairwise label information. The new enlarged texture patches
also help in data augmentation to alleviate the problem of insufficient texture
data and are used to train the second stage of the network. Experiments on
three public texture image retrieval datasets indicate the superiority of our
texture synthesis guided hashing approach over current state-of-the-art
methods.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/fu2019high/">High Dimensional Similarity Search With Satellite System Graph: Efficiency, Scalability, And Unindexed Query Compatibility</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=High Dimensional Similarity Search With Satellite System Graph: Efficiency, Scalability, And Unindexed Query Compatibility' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=High Dimensional Similarity Search With Satellite System Graph: Efficiency, Scalability, And Unindexed Query Compatibility' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fu Cong, Wang Changxu, Cai Deng</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>30</td>
    <td><p>Approximate Nearest Neighbor Search (ANNS) in high dimensional space is
essential in database and information retrieval. Recently, there has been a
surge of interest in exploring efficient graph-based indices for the ANNS
problem. Among them, Navigating Spreading-out Graph (NSG) provides fine
theoretical analysis and achieves state-of-the-art performance. However, we
find there are several limitations with NSG: 1) NSG has no theoretical
guarantee on nearest neighbor search when the query is not indexed in the
database; 2) NSG is too sparse which harms the search performance. In addition,
NSG suffers from high indexing complexity. To address the above problems, we
propose the Satellite System Graphs (SSG) and a practical variant NSSG.
Specifically, we propose a novel pruning strategy to produce SSGs from the
complete graph. SSGs define a new family of MSNETs in which the out-edges of
each node are distributed evenly in all directions. Each node in the graph
builds effective connections to its neighborhood omnidirectionally, whereupon
we derive SSG’s excellent theoretical properties for both indexed and unindexed
queries. We can adaptively adjust the sparsity of an SSG with a hyper-parameter
to optimize the search performance. Further, NSSG is proposed to reduce the
indexing complexity of the SSG for large-scale applications. Both theoretical
and extensive experimental analyses are provided to demonstrate the strengths
of the proposed approach over the existing representative algorithms. Our code
has been released at https://github.com/ZJULearning/SSG.</p>
</td>
    <td>
      
        Similarity Search 
      
        Efficiency 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/zeng2019simultaneous/">Simultaneous Region Localization And Hash Coding For Fine-grained Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Simultaneous Region Localization And Hash Coding For Fine-grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Simultaneous Region Localization And Hash Coding For Fine-grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zeng Haien, Lai Hanjiang, Yin Jian</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>Fine-grained image hashing is a challenging problem due to the difficulties
of discriminative region localization and hash code generation. Most existing
deep hashing approaches solve the two tasks independently. While these two
tasks are correlated and can reinforce each other. In this paper, we propose a
deep fine-grained hashing to simultaneously localize the discriminative regions
and generate the efficient binary codes. The proposed approach consists of a
region localization module and a hash coding module. The region localization
module aims to provide informative regions to the hash coding module. The hash
coding module aims to generate effective binary codes and give feedback for
learning better localizer. Moreover, to better capture subtle differences,
multi-scale regions at different layers are learned without the need of
bounding-box/part annotations. Extensive experiments are conducted on two
public benchmark fine-grained datasets. The results demonstrate significant
improvements in the performance of our method relative to other fine-grained
hashing algorithms.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/talreja2019zero/">Zero-shot Deep Hashing And Neural Network Based Error Correction For Face Template Protection</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Zero-shot Deep Hashing And Neural Network Based Error Correction For Face Template Protection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Zero-shot Deep Hashing And Neural Network Based Error Correction For Face Template Protection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Talreja Veeru, Valenti Matthew C., Nasrabadi Nasser M.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE 10th International Conference on Biometrics Theory, Applications and Systems (BTAS)</td>
    <td>20</td>
    <td><p>In this paper, we present a novel architecture that integrates a deep hashing
framework with a neural network decoder (NND) for application to face template
protection. It improves upon existing face template protection techniques to
provide better matching performance with one-shot and multi-shot enrollment. A
key novelty of our proposed architecture is that the framework can also be used
with zero-shot enrollment. This implies that our architecture does not need to
be re-trained even if a new subject is to be enrolled into the system. The
proposed architecture consists of two major components: a deep hashing (DH)
component, which is used for robust mapping of face images to their
corresponding intermediate binary codes, and a NND component, which corrects
errors in the intermediate binary codes that are caused by differences in the
enrollment and probe biometrics due to factors such as variation in pose,
illumination, and other factors. The final binary code generated by the NND is
then cryptographically hashed and stored as a secure face template in the
database. The efficacy of our approach with zero-shot, one-shot, and multi-shot
enrollments is shown for CMU-PIE, Extended Yale B, WVU multimodal and Multi-PIE
face databases. With zero-shot enrollment, the system achieves approximately
85% genuine accept rates (GAR) at 0.01% false accept rate (FAR), and with
one-shot and multi-shot enrollments, it achieves approximately 99.95% GAR at
0.01% FAR, while providing a high level of template security.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        Few Shot & Zero Shot 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/beck2019distributed/">A Distributed And Approximated Nearest Neighbors Algorithm For An Efficient Large Scale Mean Shift Clustering</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Distributed And Approximated Nearest Neighbors Algorithm For An Efficient Large Scale Mean Shift Clustering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Distributed And Approximated Nearest Neighbors Algorithm For An Efficient Large Scale Mean Shift Clustering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Beck et al.</td> <!-- 🔧 You were missing this -->
    <td>Journal of Parallel and Distributed Computing</td>
    <td>29</td>
    <td><p>In this paper we target the class of modal clustering methods where clusters
are defined in terms of the local modes of the probability density function
which generates the data. The most well-known modal clustering method is the
k-means clustering. Mean Shift clustering is a generalization of the k-means
clustering which computes arbitrarily shaped clusters as defined as the basins
of attraction to the local modes created by the density gradient ascent paths.
Despite its potential, the Mean Shift approach is a computationally expensive
method for unsupervised learning. Thus, we introduce two contributions aiming
to provide clustering algorithms with a linear time complexity, as opposed to
the quadratic time complexity for the exact Mean Shift clustering. Firstly we
propose a scalable procedure to approximate the density gradient ascent.
Second, our proposed scalable cluster labeling technique is presented. Both
propositions are based on Locality Sensitive Hashing (LSH) to approximate
nearest neighbors. These two techniques may be used for moderate sized
datasets. Furthermore, we show that using our proposed approximations of the
density gradient ascent as a pre-processing step in other clustering methods
can also improve dedicated classification metrics. For the latter, a
distributed implementation, written for the Spark/Scala ecosystem is proposed.
For all these considered clustering methods, we present experimental results
illustrating their labeling accuracy and their potential to solve concrete
problems.</p>
</td>
    <td>
      
        Similarity Search 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/barz2019hierarchy/">Hierarchy-based Image Embeddings For Semantic Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hierarchy-based Image Embeddings For Semantic Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hierarchy-based Image Embeddings For Semantic Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Barz Björn, Denzler Joachim</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>52</td>
    <td><p>Deep neural networks trained for classification have been found to learn
powerful image representations, which are also often used for other tasks such
as comparing images w.r.t. their visual similarity. However, visual similarity
does not imply semantic similarity. In order to learn semantically
discriminative features, we propose to map images onto class embeddings whose
pair-wise dot products correspond to a measure of semantic similarity between
classes. Such an embedding does not only improve image retrieval results, but
could also facilitate integrating semantics for other tasks, e.g., novelty
detection or few-shot learning. We introduce a deterministic algorithm for
computing the class centroids directly based on prior world-knowledge encoded
in a hierarchy of classes such as WordNet. Experiments on CIFAR-100, NABirds,
and ImageNet show that our learned semantic image embeddings improve the
semantic consistency of image retrieval results by a large margin.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/baranchuk2019learning/">Learning To Route In Similarity Graphs</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning To Route In Similarity Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning To Route In Similarity Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Baranchuk et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>8</td>
    <td><p>Recently similarity graphs became the leading paradigm for efficient nearest
neighbor search, outperforming traditional tree-based and LSH-based methods.
Similarity graphs perform the search via greedy routing: a query traverses the
graph and in each vertex moves to the adjacent vertex that is the closest to
this query. In practice, similarity graphs are often susceptible to local
minima, when queries do not reach its nearest neighbors, getting stuck in
suboptimal vertices. In this paper we propose to learn the routing function
that overcomes local minima via incorporating information about the graph
global structure. In particular, we augment the vertices of a given graph with
additional representations that are learned to provide the optimal routing from
the start vertex to the query nearest neighbor. By thorough experiments, we
demonstrate that the proposed learnable routing successfully diminishes the
local minima problem and significantly improves the overall search performance.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/berman2019multigrain/">Multigrain: A Unified Image Embedding For Classes And Instances</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multigrain: A Unified Image Embedding For Classes And Instances' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multigrain: A Unified Image Embedding For Classes And Instances' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Berman et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>46</td>
    <td><p>MultiGrain is a network architecture producing compact vector representations
that are suited both for image classification and particular object retrieval.
It builds on a standard classification trunk. The top of the network produces
an embedding containing coarse and fine-grained information, so that images can
be recognized based on the object class, particular object, or if they are
distorted copies. Our joint training is simple: we minimize a cross-entropy
loss for classification and a ranking loss that determines if two images are
identical up to data augmentation, with no need for additional labels. A key
component of MultiGrain is a pooling layer that takes advantage of
high-resolution images with a network trained at a lower resolution.
  When fed to a linear classifier, the learned embeddings provide
state-of-the-art classification accuracy. For instance, we obtain 79.4% top-1
accuracy with a ResNet-50 learned on Imagenet, which is a +1.8% absolute
improvement over the AutoAugment method. When compared with the cosine
similarity, the same embeddings perform on par with the state-of-the-art for
image retrieval at moderate resolutions.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/bajaj2019relemb/">Relemb: A Relevance-based Application Embedding For Mobile App Retrieval And Categorization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Relemb: A Relevance-based Application Embedding For Mobile App Retrieval And Categorization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Relemb: A Relevance-based Application Embedding For Mobile App Retrieval And Categorization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Bajaj et al.</td> <!-- 🔧 You were missing this -->
    <td>Computación y Sistemas</td>
    <td>5</td>
    <td><p>Information Retrieval Systems have revolutionized the organization and
extraction of Information. In recent years, mobile applications (apps) have
become primary tools of collecting and disseminating information. However,
limited research is available on how to retrieve and organize mobile apps on
users’ devices. In this paper, authors propose a novel method to estimate
app-embeddings which are then applied to tasks like app clustering,
classification, and retrieval. Usage of app-embedding for query expansion,
nearest neighbor analysis enables unique and interesting use cases to enhance
end-user experience with mobile apps.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/wray2019fine/">Fine-grained Action Retrieval Through Multiple Parts-of-speech Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fine-grained Action Retrieval Through Multiple Parts-of-speech Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fine-grained Action Retrieval Through Multiple Parts-of-speech Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wray et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>130</td>
    <td><p>We address the problem of cross-modal fine-grained action retrieval between
text and video. Cross-modal retrieval is commonly achieved through learning a
shared embedding space, that can indifferently embed modalities. In this paper,
we propose to enrich the embedding by disentangling parts-of-speech (PoS) in
the accompanying captions. We build a separate multi-modal embedding space for
each PoS tag. The outputs of multiple PoS embeddings are then used as input to
an integrated multi-modal space, where we perform action retrieval. All
embeddings are trained jointly through a combination of PoS-aware and
PoS-agnostic losses. Our proposal enables learning specialised embedding spaces
that offer multiple views of the same embedded entities.
  We report the first retrieval results on fine-grained actions for the
large-scale EPIC dataset, in a generalised zero-shot setting. Results show the
advantage of our approach for both video-to-text and text-to-video action
retrieval. We also demonstrate the benefit of disentangling the PoS for the
generic task of cross-modal video retrieval on the MSR-VTT dataset.</p>
</td>
    <td>
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/backurs2019scalable/">Scalable Nearest Neighbor Search For Optimal Transport</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Scalable Nearest Neighbor Search For Optimal Transport' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Scalable Nearest Neighbor Search For Optimal Transport' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Backurs et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>12</td>
    <td><p>The Optimal Transport (a.k.a. Wasserstein) distance is an increasingly
popular similarity measure for rich data domains, such as images or text
documents. This raises the necessity for fast nearest neighbor search
algorithms according to this distance, which poses a substantial computational
bottleneck on massive datasets. In this work we introduce Flowtree, a fast and
accurate approximation algorithm for the Wasserstein-\(1\) distance. We formally
analyze its approximation factor and running time. We perform extensive
experimental evaluation of nearest neighbor search algorithms in the \(W_1\)
distance on real-world dataset. Our results show that compared to previous
state of the art, Flowtree achieves up to \(7.4\) times faster running time.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/bai2019deep/">Deep-person: Learning Discriminative Deep Features For Person Re-identification</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep-person: Learning Discriminative Deep Features For Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep-person: Learning Discriminative Deep Features For Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Bai et al.</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition</td>
    <td>219</td>
    <td><p>Recently, many methods of person re-identification (Re-ID) rely on part-based
feature representation to learn a discriminative pedestrian descriptor.
However, the spatial context between these parts is ignored for the independent
extractor to each separate part. In this paper, we propose to apply Long
Short-Term Memory (LSTM) in an end-to-end way to model the pedestrian, seen as
a sequence of body parts from head to foot. Integrating the contextual
information strengthens the discriminative ability of local representation. We
also leverage the complementary information between local and global feature.
Furthermore, we integrate both identification task and ranking task in one
network, where a discriminative embedding and a similarity measurement are
learned concurrently. This results in a novel three-branch framework named
Deep-Person, which learns highly discriminative features for person Re-ID.
Experimental results demonstrate that Deep-Person outperforms the
state-of-the-art methods by a large margin on three challenging datasets
including Market-1501, CUHK03, and DukeMTMC-reID. Specifically, combining with
a re-ranking approach, we achieve a 90.84% mAP on Market-1501 under single
query setting.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/wu2019learning/">Learning Product Codebooks Using Vector Quantized Autoencoders For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Product Codebooks Using Vector Quantized Autoencoders For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Product Codebooks Using Vector Quantized Autoencoders For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu Hanwei, Flierl Markus</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE Global Conference on Signal and Information Processing (GlobalSIP)</td>
    <td>15</td>
    <td><p>Vector-Quantized Variational Autoencoders (VQ-VAE)[1] provide an unsupervised
model for learning discrete representations by combining vector quantization
and autoencoders. In this paper, we study the use of VQ-VAE for representation
learning for downstream tasks, such as image retrieval. We first describe the
VQ-VAE in the context of an information-theoretic framework. We show that the
regularization term on the learned representation is determined by the size of
the embedded codebook before the training and it affects the generalization
ability of the model. As a result, we introduce a hyperparameter to balance the
strength of the vector quantizer and the reconstruction error. By tuning the
hyperparameter, the embedded bottleneck quantizer is used as a regularizer that
forces the output of the encoder to share a constrained coding space such that
learned latent features preserve the similarity relations of the data space. In
addition, we provide a search range for finding the best hyperparameter.
Finally, we incorporate the product quantization into the bottleneck stage of
VQ-VAE and propose an end-to-end unsupervised learning model for the image
retrieval task. The product quantizer has the advantage of generating
large-size codebooks. Fast retrieval can be achieved by using the lookup tables
that store the distance between any pair of sub-codewords. State-of-the-art
retrieval results are achieved by the learned codebooks.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/su2019deep/">Deep Joint-semantics Reconstructing Hashing For Large-scale Unsupervised Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Joint-semantics Reconstructing Hashing For Large-scale Unsupervised Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Joint-semantics Reconstructing Hashing For Large-scale Unsupervised Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Su Shupeng, Zhong, Zhang</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>255</td>
    <td><p><img src="https://github.com/zzs1994/DJSRH/blob/master/page_image/DJRSH.png?raw=true" alt="Deep Joint-Semantics Reconstructing Hashing for Large-Scale Unsupervised Cross-Modal Retrieval" title="Deep Joint-Semantics Reconstructing Hashing for Large-Scale Unsupervised Cross-Modal Retrieval" /></p>

<p>Cross-modal hashing encodes the multimedia data into a common binary hash space in which the correlations among the samples from different modalities can be effectively measured. Deep cross-modal hashing further improves the retrieval performance as the deep neural networks can generate more semantic relevant features and hash codes. In this paper, we study the unsupervised deep cross-modal hash coding and propose Deep Joint Semantics Reconstructing Hashing (DJSRH), which has the following two main advantages. First, to learn binary codes that preserve the neighborhood structure of the original data, DJSRH constructs a novel joint-semantics affinity matrix which elaborately integrates the original neighborhood information from different modalities and accordingly is capable to capture the latent intrinsic semantic affinity for the input multi-modal instances. Second, DJSRH later trains the networks to generate binary codes that maximally reconstruct above joint-semantics relations via the proposed reconstructing framework, which is more competent for the batch-wise training as it reconstructs the specific similarity value unlike the common Laplacian constraint merely preserving the similarity order. Extensive experiments demonstrate the significant improvement by DJSRH in various cross-modal retrieval tasks.</p>
</td>
    <td>
      
        SCALABILITY 
      
        Unsupervised 
      
        SUPERVISED 
      
        Multimodal Retrieval 
      
        Hashing Methods 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/patel2019self/">Self-supervised Visual Representations For Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Self-supervised Visual Representations For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Self-supervised Visual Representations For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Patel et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2019 on International Conference on Multimedia Retrieval</td>
    <td>7</td>
    <td><p>Cross-modal retrieval methods have been significantly improved in last years
with the use of deep neural networks and large-scale annotated datasets such as
ImageNet and Places. However, collecting and annotating such datasets requires
a tremendous amount of human effort and, besides, their annotations are usually
limited to discrete sets of popular visual classes that may not be
representative of the richer semantics found on large-scale cross-modal
retrieval datasets. In this paper, we present a self-supervised cross-modal
retrieval framework that leverages as training data the correlations between
images and text on the entire set of Wikipedia articles. Our method consists in
training a CNN to predict: (1) the semantic context of the article in which an
image is more probable to appear as an illustration (global context), and (2)
the semantic context of its caption (local context). Our experiments
demonstrate that the proposed method is not only capable of learning
discriminative visual representations for solving vision tasks like image
classification and object detection, but that the learned representations are
better for cross-modal retrieval when compared to supervised pre-training of
the network on the ImageNet dataset.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Multimodal Retrieval 
      
        Medical Retrieval 
      
        Self SUPERVISED 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/aum%C3%BCller2019ann/">Ann-benchmarks: A Benchmarking Tool For Approximate Nearest Neighbor Algorithms</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Ann-benchmarks: A Benchmarking Tool For Approximate Nearest Neighbor Algorithms' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Ann-benchmarks: A Benchmarking Tool For Approximate Nearest Neighbor Algorithms' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Aumüller Martin, Bernhardsson Erik, Faithfull Alexander</td> <!-- 🔧 You were missing this -->
    <td>Information Systems</td>
    <td>189</td>
    <td><p>This paper describes ANN-Benchmarks, a tool for evaluating the performance of
in-memory approximate nearest neighbor algorithms. It provides a standard
interface for measuring the performance and quality achieved by nearest
neighbor algorithms on different standard data sets. It supports several
different ways of integrating \(k\)-NN algorithms, and its configuration system
automatically tests a range of parameter settings for each algorithm.
Algorithms are compared with respect to many different (approximate) quality
measures, and adding more is easy and fast; the included plotting front-ends
can visualise these as images, \(\LaTeX\) plots, and websites with interactive
plots. ANN-Benchmarks aims to provide a constantly updated overview of the
current state of the art of \(k\)-NN algorithms. In the short term, this overview
allows users to choose the correct \(k\)-NN algorithm and parameters for their
similarity search task; in the longer term, algorithm designers will be able to
use this overview to test and refine automatic parameter tuning. The paper
gives an overview of the system, evaluates the results of the benchmark, and
points out directions for future work. Interestingly, very different approaches
to \(k\)-NN search yield comparable quality-performance trade-offs. The system is
available at http://ann-benchmarks.com .</p>
</td>
    <td>
      
        Similarity Search 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/artetxe2019massively/">Massively Multilingual Sentence Embeddings For Zero-shot Cross-lingual Transfer And Beyond</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Massively Multilingual Sentence Embeddings For Zero-shot Cross-lingual Transfer And Beyond' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Massively Multilingual Sentence Embeddings For Zero-shot Cross-lingual Transfer And Beyond' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Artetxe Mikel, Schwenk Holger</td> <!-- 🔧 You were missing this -->
    <td>Transactions of the Association for Computational Linguistics</td>
    <td>747</td>
    <td><p>We introduce an architecture to learn joint multilingual sentence
representations for 93 languages, belonging to more than 30 different families
and written in 28 different scripts. Our system uses a single BiLSTM encoder
with a shared BPE vocabulary for all languages, which is coupled with an
auxiliary decoder and trained on publicly available parallel corpora. This
enables us to learn a classifier on top of the resulting embeddings using
English annotated data only, and transfer it to any of the 93 languages without
any modification. Our experiments in cross-lingual natural language inference
(XNLI dataset), cross-lingual document classification (MLDoc dataset) and
parallel corpus mining (BUCC dataset) show the effectiveness of our approach.
We also introduce a new test set of aligned sentences in 112 languages, and
show that our sentence embeddings obtain strong results in multilingual
similarity search even for low-resource languages. Our implementation, the
pre-trained encoder and the multilingual test set are available at
https://github.com/facebookresearch/LASER</p>
</td>
    <td>
      
        Few Shot & Zero Shot 
      
        EACL 
      
        TACL 
      
        NAACL 
      
        ACL 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/artetxe2019margin/">Margin-based Parallel Corpus Mining With Multilingual Sentence Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Margin-based Parallel Corpus Mining With Multilingual Sentence Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Margin-based Parallel Corpus Mining With Multilingual Sentence Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Artetxe Mikel, Schwenk Holger</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</td>
    <td>163</td>
    <td><p>Machine translation is highly sensitive to the size and quality of the
training data, which has led to an increasing interest in collecting and
filtering large parallel corpora. In this paper, we propose a new method for
this task based on multilingual sentence embeddings. In contrast to previous
approaches, which rely on nearest neighbor retrieval with a hard threshold over
cosine similarity, our proposed method accounts for the scale inconsistencies
of this measure, considering the margin between a given sentence pair and its
closest candidates instead. Our experiments show large improvements over
existing methods. We outperform the best published results on the BUCC mining
task and the UN reconstruction task by more than 10 F1 and 30 precision points,
respectively. Filtering the English-German ParaCrawl corpus with our approach,
we obtain 31.2 BLEU points on newstest2014, an improvement of more than one
point over the best official filtered version.</p>
</td>
    <td>
      
        ACL 
      
        TACL 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/aum%C3%BCller2019puffinn/">PUFFINN: Parameterless And Universally Fast Finding Of Nearest Neighbors</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=PUFFINN: Parameterless And Universally Fast Finding Of Nearest Neighbors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=PUFFINN: Parameterless And Universally Fast Finding Of Nearest Neighbors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Aumüller et al.</td> <!-- 🔧 You were missing this -->
    <td>2022 29th International Conference on Systems, Signals and Image Processing (IWSSIP)</td>
    <td>7</td>
    <td><p>We present PUFFINN, a parameterless LSH-based index for solving the
\(k\)-nearest neighbor problem with probabilistic guarantees. By parameterless we
mean that the user is only required to specify the amount of memory the index
is supposed to use and the result quality that should be achieved. The index
combines several heuristic ideas known in the literature. By small adaptions to
the query algorithm, we make heuristics rigorous. We perform experiments on
real-world and synthetic inputs to evaluate implementation choices and show
that the implementation satisfies the quality guarantees while being
competitive with other state-of-the-art approaches to nearest neighbor search.
  We describe a novel synthetic data set that is difficult to solve for almost
all existing nearest neighbor search approaches, and for which PUFFINN
significantly outperform previous methods.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/arponen2019shrewd/">SHREWD: Semantic Hierarchy-based Relational Embeddings For Weakly-supervised Deep Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=SHREWD: Semantic Hierarchy-based Relational Embeddings For Weakly-supervised Deep Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=SHREWD: Semantic Hierarchy-based Relational Embeddings For Weakly-supervised Deep Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Arponen Heikki, Bishop Tom E</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>27</td>
    <td><p>Using class labels to represent class similarity is a typical approach to
training deep hashing systems for retrieval; samples from the same or different
classes take binary 1 or 0 similarity values. This similarity does not model
the full rich knowledge of semantic relations that may be present between data
points. In this work we build upon the idea of using semantic hierarchies to
form distance metrics between all available sample labels; for example cat to
dog has a smaller distance than cat to guitar. We combine this type of semantic
distance into a loss function to promote similar distances between the deep
neural network embeddings. We also introduce an empirical Kullback-Leibler
divergence loss term to promote binarization and uniformity of the embeddings.
We test the resulting SHREWD method and demonstrate improvements in
hierarchical retrieval scores using compact, binary hash codes instead of real
valued ones, and show that in a weakly supervised hashing setting we are able
to learn competitively without explicitly relying on class labels, but instead
on similarities between labels.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        SUPERVISED 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/artetxe2019bilingual/">Bilingual Lexicon Induction Through Unsupervised Machine Translation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Bilingual Lexicon Induction Through Unsupervised Machine Translation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Bilingual Lexicon Induction Through Unsupervised Machine Translation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Artetxe Mikel, Labaka Gorka, Agirre Eneko</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</td>
    <td>52</td>
    <td><p>A recent research line has obtained strong results on bilingual lexicon
induction by aligning independently trained word embeddings in two languages
and using the resulting cross-lingual embeddings to induce word translation
pairs through nearest neighbor or related retrieval methods. In this paper, we
propose an alternative approach to this problem that builds on the recent work
on unsupervised machine translation. This way, instead of directly inducing a
bilingual lexicon from cross-lingual embeddings, we use them to build a
phrase-table, combine it with a language model, and use the resulting machine
translation system to generate a synthetic parallel corpus, from which we
extract the bilingual lexicon using statistical word alignment techniques. As
such, our method can work with any word embedding and cross-lingual mapping
technique, and it does not require any additional resource besides the
monolingual corpus used to train the embeddings. When evaluated on the exact
same cross-lingual embeddings, our proposed method obtains an average
improvement of 6 accuracy points over nearest neighbor and 4 points over CSLS
retrieval, establishing a new state-of-the-art in the standard MUSE dataset.</p>
</td>
    <td>
      
        ACL 
      
        TACL 
      
        Unsupervised 
      
        SUPERVISED 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/andr%C3%A92019derived/">Derived Codebooks For High-accuracy Nearest Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Derived Codebooks For High-accuracy Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Derived Codebooks For High-accuracy Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>André Fabien, Kermarrec Anne-marie, Scouarnec Nicolas Le</td> <!-- 🔧 You were missing this -->
    <td>Neurocomputing</td>
    <td>28</td>
    <td><p>High-dimensional Nearest Neighbor (NN) search is central in multimedia search
systems. Product Quantization (PQ) is a widespread NN search technique which
has a high performance and good scalability. PQ compresses high-dimensional
vectors into compact codes thanks to a combination of quantizers. Large
databases can, therefore, be stored entirely in RAM, enabling fast responses to
NN queries. In almost all cases, PQ uses 8-bit quantizers as they offer low
response times. In this paper, we advocate the use of 16-bit quantizers.
Compared to 8-bit quantizers, 16-bit quantizers boost accuracy but they
increase response time by a factor of 3 to 10. We propose a novel approach that
allows 16-bit quantizers to offer the same response time as 8-bit quantizers,
while still providing a boost of accuracy. Our approach builds on two key
ideas: (i) the construction of derived codebooks that allow a fast and
approximate distance evaluation, and (ii) a two-pass NN search procedure which
builds a candidate set using the derived codebooks, and then refines it using
16-bit quantizers. On 1 billion SIFT vectors, with an inverted index, our
approach offers a Recall@100 of 0.85 in 5.2 ms. By contrast, 16-bit quantizers
alone offer a Recall@100 of 0.85 in 39 ms, and 8-bit quantizers a Recall@100 of
0.82 in 3.8 ms.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/pratap2019efficient/">Efficient Sketching Algorithm For Sparse Binary Data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Sketching Algorithm For Sparse Binary Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient Sketching Algorithm For Sparse Binary Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Pratap Rameshwar, Bera Debajyoti, Revanuru Karthik</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE International Conference on Data Mining (ICDM)</td>
    <td>11</td>
    <td><p>Recent advancement of the WWW, IOT, social network, e-commerce, etc. have
generated a large volume of data. These datasets are mostly represented by high
dimensional and sparse datasets. Many fundamental subroutines of common data
analytic tasks such as clustering, classification, ranking, nearest neighbour
search, etc. scale poorly with the dimension of the dataset. In this work, we
address this problem and propose a sketching (alternatively, dimensionality
reduction) algorithm – \(\binsketch\) (Binary Data Sketch) – for sparse binary
datasets. \(\binsketch\) preserves the binary version of the dataset after
sketching and maintains estimates for multiple similarity measures such as
Jaccard, Cosine, Inner-Product similarities, and Hamming distance, on the same
sketch. We present a theoretical analysis of our algorithm and complement it
with extensive experimentation on several real-world datasets. We compare the
performance of our algorithm with the state-of-the-art algorithms on the task
of mean-square-error and ranking. Our proposed algorithm offers a comparable
accuracy while suggesting a significant speedup in the dimensionality reduction
time, with respect to the other candidate algorithms. Our proposal is simple,
easy to implement, and therefore can be adopted in practice.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/wang2019camp/">CAMP: Cross-modal Adaptive Message Passing For Text-image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=CAMP: Cross-modal Adaptive Message Passing For Text-image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=CAMP: Cross-modal Adaptive Message Passing For Text-image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>296</td>
    <td><p>Text-image cross-modal retrieval is a challenging task in the field of
language and vision. Most previous approaches independently embed images and
sentences into a joint embedding space and compare their similarities. However,
previous approaches rarely explore the interactions between images and
sentences before calculating similarities in the joint space. Intuitively, when
matching between images and sentences, human beings would alternatively attend
to regions in images and words in sentences, and select the most salient
information considering the interaction between both modalities. In this paper,
we propose Cross-modal Adaptive Message Passing (CAMP), which adaptively
controls the information flow for message passing across modalities. Our
approach not only takes comprehensive and fine-grained cross-modal interactions
into account, but also properly handles negative pairs and irrelevant
information with an adaptive gating scheme. Moreover, instead of conventional
joint embedding approaches for text-image matching, we infer the matching score
based on the fused features, and propose a hardest negative binary
cross-entropy loss for training. Results on COCO and Flickr30k significantly
surpass state-of-the-art methods, demonstrating the effectiveness of our
approach.</p>
</td>
    <td>
      
        Image Retrieval 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/aggarwal2019multitask/">Multitask Text-to-visual Embedding With Titles And Clickthrough Data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multitask Text-to-visual Embedding With Titles And Clickthrough Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multitask Text-to-visual Embedding With Titles And Clickthrough Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Aggarwal et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</td>
    <td>8</td>
    <td><p>Text-visual (or called semantic-visual) embedding is a central problem in
vision-language research. It typically involves mapping of an image and a text
description to a common feature space through a CNN image encoder and a RNN
language encoder. In this paper, we propose a new method for learning
text-visual embedding using both image titles and click-through data from an
image search engine. We also propose a new triplet loss function by modeling
positive awareness of the embedding, and introduce a novel mini-batch-based
hard negative sampling approach for better data efficiency in the learning
process. Experimental results show that our proposed method outperforms
existing methods, and is also effective for real-world text-to-visual
retrieval.</p>
</td>
    <td>
      
        KDD 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/2019scratch/">SCRATCH: A Scalable Discrete Matrix Factorization Hashing For Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=SCRATCH: A Scalable Discrete Matrix Factorization Hashing For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=SCRATCH: A Scalable Discrete Matrix Factorization Hashing For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chuan-xiang et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Circuits and Systems for Video Technology</td>
    <td>110</td>
    <td><p>In recent years, many hashing methods have been proposed for the cross-modal retrieval task. However, there are still some issues that need to be further explored. For example, some of them relax the binary constraints to generate the hash codes, which may generate large quantization error. Although some discrete schemes have been proposed, most of them are time-consuming. In addition, most of the existing supervised hashing methods use an n x n similarity matrix during the optimization, making them unscalable. To address these issues, in this paper, we present a novel supervised cross-modal hashing method—Scalable disCRete mATrix faCtorization Hashing, SCRATCH for short. It leverages the collective matrix factorization on the kernelized features and the semantic embedding with labels to find a latent semantic space to preserve the intra- and inter-modality similarities. In addition, it incorporates the label matrix instead of the similarity matrix into the loss function. Based on the proposed loss function and the iterative optimization algorithm, it can learn the hash functions and binary codes simultaneously. Moreover, the binary codes can be generated discretely, reducing the quantization error generated by the relaxation scheme. Its time complexity is linear to the size of the dataset, making it scalable to large-scale datasets. Extensive experiments on three benchmark datasets, namely, Wiki, MIRFlickr-25K, and NUS-WIDE, have verified that our proposed SCRATCH model outperforms several state-of-the-art unsupervised and supervised hashing methods for cross-modal retrieval.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/alemu2019multi/">Multi-feature Fusion For Image Retrieval Using Constrained Dominant Sets</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multi-feature Fusion For Image Retrieval Using Constrained Dominant Sets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multi-feature Fusion For Image Retrieval Using Constrained Dominant Sets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Alemu Leulseged Tesfaye, Pelillo Marcello</td> <!-- 🔧 You were missing this -->
    <td>Image and Vision Computing</td>
    <td>23</td>
    <td><p>Aggregating different image features for image retrieval has recently shown
its effectiveness. While highly effective, though, the question of how to
uplift the impact of the best features for a specific query image persists as
an open computer vision problem. In this paper, we propose a computationally
efficient approach to fuse several hand-crafted and deep features, based on the
probabilistic distribution of a given membership score of a constrained cluster
in an unsupervised manner. First, we introduce an incremental nearest neighbor
(NN) selection method, whereby we dynamically select k-NN to the query. We then
build several graphs from the obtained NN sets and employ constrained dominant
sets (CDS) on each graph G to assign edge weights which consider the intrinsic
manifold structure of the graph, and detect false matches to the query.
Finally, we elaborate the computation of feature positive-impact weight (PIW)
based on the dispersive degree of the characteristics vector. To this end, we
exploit the entropy of a cluster membership-score distribution. In addition,
the final NN set bypasses a heuristic voting scheme. Experiments on several
retrieval benchmark datasets show that our method can improve the
state-of-the-art result.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/yan2019deep/">Deep Hashing By Discriminating Hard Examples</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Hashing By Discriminating Hard Examples' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Hashing By Discriminating Hard Examples' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yan et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 27th ACM International Conference on Multimedia</td>
    <td>27</td>
    <td><p>This paper tackles a rarely explored but critical problem within learning to hash, i.e., to learn hash codes that effectively discriminate hard similar and dissimilar examples, to empower large-scale image retrieval. Hard similar examples refer to image pairs from the same semantic class that demonstrate some shared appearance but have different fine-grained appearance. Hard dissimilar examples are image pairs that come from different semantic classes but exhibit similar appearance. These hard examples generally have a small distance due to the shared appearance. Therefore, effective encoding of the hard examples can well discriminate the relevant images within a small Hamming distance, enabling more accurate retrieval in the top-ranked returned images. However, most existing hashing methods cannot capture this key information as their optimization is dominated byeasy examples, i.e., distant similar/dissimilar pairs that share no or limited appearance. To address this problem, we introduce a novel Gamma distribution-enabled and symmetric Kullback-Leibler divergence-based loss, which is dubbed dual hinge loss because it works similarly as imposing two smoothed hinge losses on the respective similar and dissimilar pairs. Specifically, the loss enforces exponentially variant penalization on the hard similar (dissimilar) examples to emphasize and learn their fine-grained difference. It meanwhile imposes a bounding penalization on easy similar (dissimilar) examples to prevent the dominance of the easy examples in the optimization while preserving the high-level similarity (dissimilarity). This enables our model to well encode the key information carried by both easy and hard examples. Extensive empirical results on three widely-used image retrieval datasets show that (i) our method consistently and substantially outperforms state-of-the-art competing methods using hash codes of the same length and (ii) our method can use significantly (e.g., 50%-75%) shorter hash codes to perform substantially better than, or comparably well to, the competing methods.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/plummer2019give/">Give Me A Hint! Navigating Image Databases Using Human-in-the-loop Feedback</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Give Me A Hint! Navigating Image Databases Using Human-in-the-loop Feedback' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Give Me A Hint! Navigating Image Databases Using Human-in-the-loop Feedback' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Plummer et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>15</td>
    <td><p>In this paper, we introduce an attribute-based interactive image search which
can leverage human-in-the-loop feedback to iteratively refine image search
results. We study active image search where human feedback is solicited
exclusively in visual form, without using relative attribute annotations used
by prior work which are not typically found in many datasets. In order to
optimize the image selection strategy, a deep reinforcement model is trained to
learn what images are informative rather than rely on hand-crafted measures
typically leveraged in prior work. Additionally, we extend the recently
introduced Conditional Similarity Network to incorporate global similarity in
training visual embeddings, which results in more natural transitions as the
user explores the learned similarity embeddings. Our experiments demonstrate
the effectiveness of our approach, producing compelling results on both active
image search and image attribute representation tasks.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/lei2019semi/">Semi-heterogeneous Three-way Joint Embedding Network For Sketch-based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semi-heterogeneous Three-way Joint Embedding Network For Sketch-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semi-heterogeneous Three-way Joint Embedding Network For Sketch-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lei et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Circuits and Systems for Video Technology</td>
    <td>49</td>
    <td><p>Sketch-based image retrieval (SBIR) is a challenging task due to the large
cross-domain gap between sketches and natural images. How to align abstract
sketches and natural images into a common high-level semantic space remains a
key problem in SBIR. In this paper, we propose a novel semi-heterogeneous
three-way joint embedding network (Semi3-Net), which integrates three branches
(a sketch branch, a natural image branch, and an edgemap branch) to learn more
discriminative cross-domain feature representations for the SBIR task. The key
insight lies with how we cultivate the mutual and subtle relationships amongst
the sketches, natural images, and edgemaps. A semi-heterogeneous feature
mapping is designed to extract bottom features from each domain, where the
sketch and edgemap branches are shared while the natural image branch is
heterogeneous to the other branches. In addition, a joint semantic embedding is
introduced to embed the features from different domains into a common
high-level semantic space, where all of the three branches are shared. To
further capture informative features common to both natural images and the
corresponding edgemaps, a co-attention model is introduced to conduct common
channel-wise feature recalibration between different domains. A hybrid-loss
mechanism is designed to align the three branches, where an alignment loss and
a sketch-edgemap contrastive loss are presented to encourage the network to
learn invariant cross-domain representations. Experimental results on two
widely used category-level datasets (Sketchy and TU-Berlin Extension)
demonstrate that the proposed method outperforms state-of-the-art methods.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/yang2019efficient/">Efficient Image Retrieval Via Decoupling Diffusion Into Online And Offline Processing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Image Retrieval Via Decoupling Diffusion Into Online And Offline Processing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient Image Retrieval Via Decoupling Diffusion Into Online And Offline Processing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>49</td>
    <td><p>Diffusion is commonly used as a ranking or re-ranking method in retrieval
tasks to achieve higher retrieval performance, and has attracted lots of
attention in recent years. A downside to diffusion is that it performs slowly
in comparison to the naive k-NN search, which causes a non-trivial online
computational cost on large datasets. To overcome this weakness, we propose a
novel diffusion technique in this paper. In our work, instead of applying
diffusion to the query, we pre-compute the diffusion results of each element in
the database, making the online search a simple linear combination on top of
the k-NN search process. Our proposed method becomes 10~ times faster in terms
of online search speed. Moreover, we propose to use late truncation instead of
early truncation in previous works to achieve better retrieval performance.</p>
</td>
    <td>
      
        Image Retrieval 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/luo2019snap/">Snap And Find: Deep Discrete Cross-domain Garment Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Snap And Find: Deep Discrete Cross-domain Garment Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Snap And Find: Deep Discrete Cross-domain Garment Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Luo et al.</td> <!-- 🔧 You were missing this -->
    <td>Neurocomputing</td>
    <td>37</td>
    <td><p>With the increasing number of online stores, there is a pressing need for
intelligent search systems to understand the item photos snapped by customers
and search against large-scale product databases to find their desired items.
However, it is challenging for conventional retrieval systems to match up the
item photos captured by customers and the ones officially released by stores,
especially for garment images. To bridge the customer- and store- provided
garment photos, existing studies have been widely exploiting the clothing
attributes (\textit{e.g.,} black) and landmarks (\textit{e.g.,} collar) to
learn a common embedding space for garment representations. Unfortunately they
omit the sequential correlation of attributes and consume large quantity of
human labors to label the landmarks. In this paper, we propose a deep
multi-task cross-domain hashing termed \textit{DMCH}, in which cross-domain
embedding and sequential attribute learning are modeled simultaneously.
Sequential attribute learning not only provides the semantic guidance for
embedding, but also generates rich attention on discriminative local details
(\textit{e.g.,} black buttons) of clothing items without requiring extra
landmark labels. This leads to promising performance and 306\(\times\) boost on
efficiency when compared with the state-of-the-art models, which is
demonstrated through rigorous experiments on two public fashion datasets.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/zhang2019pairwise/">Pairwise Teacher-student Network For Semi-supervised Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Pairwise Teacher-student Network For Semi-supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Pairwise Teacher-student Network For Semi-supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Shifeng, Li Jianmin, Zhang Bo</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>7</td>
    <td><p>Hashing method maps similar high-dimensional data to binary hashcodes with
smaller hamming distance, and it has received broad attention due to its low
storage cost and fast retrieval speed. Pairwise similarity is easily obtained
and widely used for retrieval, and most supervised hashing algorithms are
carefully designed for the pairwise supervisions. As labeling all data pairs is
difficult, semi-supervised hashing is proposed which aims at learning efficient
codes with limited labeled pairs and abundant unlabeled ones. Existing methods
build graphs to capture the structure of dataset, but they are not working well
for complex data as the graph is built based on the data representations and
determining the representations of complex data is difficult. In this paper, we
propose a novel teacher-student semi-supervised hashing framework in which the
student is trained with the pairwise information produced by the teacher
network. The network follows the smoothness assumption, which achieves
consistent distances for similar data pairs so that the retrieval results are
similar for neighborhood queries. Experiments on large-scale datasets show that
the proposed method reaches impressive gain over the supervised baselines and
is superior to state-of-the-art semi-supervised hashing methods.</p>
</td>
    <td>
      
        Unsupervised 
      
        CVPR 
      
        Neural Hashing 
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/le2019btel/">BTEL: A Binary Tree Encoding Approach For Visual Localization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=BTEL: A Binary Tree Encoding Approach For Visual Localization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=BTEL: A Binary Tree Encoding Approach For Visual Localization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Le Huu, Hoang Tuan, Milford Michael</td> <!-- 🔧 You were missing this -->
    <td>IEEE Robotics and Automation Letters</td>
    <td>5</td>
    <td><p>Visual localization algorithms have achieved significant improvements in
performance thanks to recent advances in camera technology and vision-based
techniques. However, there remains one critical caveat: all current approaches
that are based on image retrieval currently scale at best linearly with the
size of the environment with respect to both storage, and consequentially in
most approaches, query time. This limitation severely curtails the capability
of autonomous systems in a wide range of compute, power, storage, size, weight
or cost constrained applications such as drones. In this work, we present a
novel binary tree encoding approach for visual localization which can serve as
an alternative for existing quantization and indexing techniques. The proposed
tree structure allows us to derive a compressed training scheme that achieves
sub-linearity in both required storage and inference time. The encoding memory
can be easily configured to satisfy different storage constraints. Moreover,
our approach is amenable to an optional sequence filtering mechanism to further
improve the localization results, while maintaining the same amount of storage.
Our system is entirely agnostic to the front-end descriptors, allowing it to be
used on top of recent state-of-the-art image representations. Experimental
results show that the proposed method significantly outperforms
state-of-the-art approaches under limited storage constraints.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/mahajan2019joint/">Joint Wasserstein Autoencoders For Aligning Multimodal Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Joint Wasserstein Autoencoders For Aligning Multimodal Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Joint Wasserstein Autoencoders For Aligning Multimodal Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mahajan et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</td>
    <td>8</td>
    <td><p>One of the key challenges in learning joint embeddings of multiple
modalities, e.g. of images and text, is to ensure coherent cross-modal
semantics that generalize across datasets. We propose to address this through
joint Gaussian regularization of the latent representations. Building on
Wasserstein autoencoders (WAEs) to encode the input in each domain, we enforce
the latent embeddings to be similar to a Gaussian prior that is shared across
the two domains, ensuring compatible continuity of the encoded semantic
representations of images and texts. Semantic alignment is achieved through
supervision from matching image-text pairs. To show the benefits of our
semi-supervised representation, we apply it to cross-modal retrieval and phrase
localization. We not only achieve state-of-the-art accuracy, but significantly
better generalization across datasets, owing to the semantic continuity of the
latent space.</p>
</td>
    <td>
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/laarhoven2019polytopes/">Polytopes, Lattices, And Spherical Codes For The Nearest Neighbor Problem</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Polytopes, Lattices, And Spherical Codes For The Nearest Neighbor Problem' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Polytopes, Lattices, And Spherical Codes For The Nearest Neighbor Problem' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Laarhoven Thijs</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Information Theory</td>
    <td>16</td>
    <td><p>We study locality-sensitive hash methods for the nearest neighbor problem for
the angular distance, focusing on the approach of first projecting down onto a
low-dimensional subspace, and then partitioning the projected vectors according
to Voronoi cells induced by a suitable spherical code. This approach
generalizes and interpolates between the fast but suboptimal hyperplane hashing
of Charikar [STOC’02] and the asymptotically optimal but practically often
slower hash families of Andoni-Indyk [FOCS’06], Andoni-Indyk-Nguyen-Razenshteyn
[SODA’14] and Andoni-Indyk-Laarhoven-Razenshteyn-Schmidt [NIPS’15]. We set up a
framework for analyzing the performance of any spherical code in this context,
and we provide results for various codes from the literature, such as those
related to regular polytopes and root lattices. Similar to hyperplane hashing,
and unlike cross-polytope hashing, our analysis of collision probabilities and
query exponents is exact and does not hide order terms which vanish only for
large \(d\), facilitating an easy parameter selection.
  For the two-dimensional case, we derive closed-form expressions for arbitrary
spherical codes, and we show that the equilateral triangle is optimal,
achieving a better performance than the two-dimensional analogues of hyperplane
and cross-polytope hashing. In three and four dimensions, we numerically find
that the tetrahedron, \(5\)-cell, and \(16\)-cell achieve the best query exponents,
while in five or more dimensions orthoplices appear to outperform regular
simplices, as well as the root lattice families \(A_k\) and \(D_k\). We argue that
in higher dimensions, larger spherical codes will likely exist which will
outperform orthoplices in theory, and we argue why using the \(D_k\) root
lattices will likely lead to better results in practice, due to a better
trade-off between the asymptotic query exponent and the concrete costs of
hashing.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/zhe2019deep/">Deep Class-wise Hashing: Semantics-preserving Hashing Via Class-wise Loss</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Class-wise Hashing: Semantics-preserving Hashing Via Class-wise Loss' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Class-wise Hashing: Semantics-preserving Hashing Via Class-wise Loss' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhe Xuefei, Chen Shifeng, Yan Hong</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Neural Networks and Learning Systems</td>
    <td>36</td>
    <td><p>Deep supervised hashing has emerged as an influential solution to large-scale
semantic image retrieval problems in computer vision. In the light of recent
progress, convolutional neural network based hashing methods typically seek
pair-wise or triplet labels to conduct the similarity preserving learning.
However, complex semantic concepts of visual contents are hard to capture by
similar/dissimilar labels, which limits the retrieval performance. Generally,
pair-wise or triplet losses not only suffer from expensive training costs but
also lack in extracting sufficient semantic information. In this regard, we
propose a novel deep supervised hashing model to learn more compact class-level
similarity preserving binary codes. Our deep learning based model is motivated
by deep metric learning that directly takes semantic labels as supervised
information in training and generates corresponding discriminant hashing code.
Specifically, a novel cubic constraint loss function based on Gaussian
distribution is proposed, which preserves semantic variations while penalizes
the overlap part of different classes in the embedding space. To address the
discrete optimization problem introduced by binary codes, a two-step
optimization strategy is proposed to provide efficient training and avoid the
problem of gradient vanishing. Extensive experiments on four large-scale
benchmark databases show that our model can achieve the state-of-the-art
retrieval performance. Moreover, when training samples are limited, our method
surpasses other supervised deep hashing methods with non-negligible margins.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/kuang2019fashion/">Fashion Retrieval Via Graph Reasoning Networks On A Similarity Pyramid</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fashion Retrieval Via Graph Reasoning Networks On A Similarity Pyramid' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fashion Retrieval Via Graph Reasoning Networks On A Similarity Pyramid' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kuang et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>86</td>
    <td><p>Matching clothing images from customers and online shopping stores has rich
applications in E-commerce. Existing algorithms encoded an image as a global
feature vector and performed retrieval with the global representation. However,
discriminative local information on clothes are submerged in this global
representation, resulting in sub-optimal performance. To address this issue, we
propose a novel Graph Reasoning Network (GRNet) on a Similarity Pyramid, which
learns similarities between a query and a gallery cloth by using both global
and local representations in multiple scales. The similarity pyramid is
represented by a Graph of similarity, where nodes represent similarities
between clothing components at different scales, and the final matching score
is obtained by message passing along edges. In GRNet, graph reasoning is solved
by training a graph convolutional network, enabling to align salient clothing
components to improve clothing retrieval. To facilitate future researches, we
introduce a new benchmark FindFashion, containing rich annotations of bounding
boxes, views, occlusions, and cropping. Extensive experiments show that GRNet
obtains new state-of-the-art results on two challenging benchmarks, e.g.,
pushing the top-1, top-20, and top-50 accuracies on DeepFashion to 26%, 64%,
and 75% (i.e., 4%, 10%, and 10% absolute improvements), outperforming
competitors with large margins. On FindFashion, GRNet achieves considerable
improvements on all empirical settings.</p>
</td>
    <td>
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/sadeh2019joint/">Joint Visual-textual Embedding For Multimodal Style Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Joint Visual-textual Embedding For Multimodal Style Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Joint Visual-textual Embedding For Multimodal Style Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sadeh et al.</td> <!-- 🔧 You were missing this -->
    <td>Expert Systems with Applications</td>
    <td>6</td>
    <td><p>We introduce a multimodal visual-textual search refinement method for fashion
garments. Existing search engines do not enable intuitive, interactive,
refinement of retrieved results based on the properties of a particular
product. We propose a method to retrieve similar items, based on a query item
image and textual refinement properties. We believe this method can be
leveraged to solve many real-life customer scenarios, in which a similar item
in a different color, pattern, length or style is desired. We employ a joint
embedding training scheme in which product images and their catalog textual
metadata are mapped closely in a shared space. This joint visual-textual
embedding space enables manipulating catalog images semantically, based on
textual refinement requirements. We propose a new training objective function,
Mini-Batch Match Retrieval, and demonstrate its superiority over the commonly
used triplet loss. Additionally, we demonstrate the feasibility of adding an
attribute extraction module, trained on the same catalog data, and demonstrate
how to integrate it within the multimodal search to boost its performance. We
introduce an evaluation protocol with an associated benchmark, and compare
several approaches.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/zhang2019pcan/">PCAN: 3D Attention Map Learning Using Contextual Information For Point Cloud Based Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=PCAN: 3D Attention Map Learning Using Contextual Information For Point Cloud Based Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=PCAN: 3D Attention Map Learning Using Contextual Information For Point Cloud Based Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Wenxiao, Xiao Chunxia</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>221</td>
    <td><p>Point cloud based retrieval for place recognition is an emerging problem in
vision field. The main challenge is how to find an efficient way to encode the
local features into a discriminative global descriptor. In this paper, we
propose a Point Contextual Attention Network (PCAN), which can predict the
significance of each local point feature based on point context. Our network
makes it possible to pay more attention to the task-relevent features when
aggregating local features. Experiments on various benchmark datasets show that
the proposed network can provide outperformance than current state-of-the-art
approaches.</p>
</td>
    <td>
      
        Evaluation 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/kordopatiszilos2019visil/">Visil: Fine-grained Spatio-temporal Video Similarity Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Visil: Fine-grained Spatio-temporal Video Similarity Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Visil: Fine-grained Spatio-temporal Video Similarity Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kordopatis-zilos et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>67</td>
    <td><p>In this paper we introduce ViSiL, a Video Similarity Learning architecture
that considers fine-grained Spatio-Temporal relations between pairs of videos
– such relations are typically lost in previous video retrieval approaches
that embed the whole frame or even the whole video into a vector descriptor
before the similarity estimation. By contrast, our Convolutional Neural Network
(CNN)-based approach is trained to calculate video-to-video similarity from
refined frame-to-frame similarity matrices, so as to consider both intra- and
inter-frame relations. In the proposed method, pairwise frame similarity is
estimated by applying Tensor Dot (TD) followed by Chamfer Similarity (CS) on
regional CNN frame features - this avoids feature aggregation before the
similarity calculation between frames. Subsequently, the similarity matrix
between all video frames is fed to a four-layer CNN, and then summarized using
Chamfer Similarity (CS) into a video-to-video similarity score – this avoids
feature aggregation before the similarity calculation between videos and
captures the temporal similarity patterns between matching frame sequences. We
train the proposed network using a triplet loss scheme and evaluate it on five
public benchmark datasets on four different video retrieval problems where we
demonstrate large improvements in comparison to the state of the art. The
implementation of ViSiL is publicly available.</p>
</td>
    <td>
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/komorowski2019random/">Random Binary Trees For Approximate Nearest Neighbour Search In Binary Space</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Random Binary Trees For Approximate Nearest Neighbour Search In Binary Space' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Random Binary Trees For Approximate Nearest Neighbour Search In Binary Space' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Komorowski Michal, Trzcinski Tomasz</td> <!-- 🔧 You were missing this -->
    <td>Applied Soft Computing</td>
    <td>9</td>
    <td><p>Approximate nearest neighbour (ANN) search is one of the most important
problems in computer science fields such as data mining or computer vision. In
this paper, we focus on ANN for high-dimensional binary vectors and we propose
a simple yet powerful search method that uses Random Binary Search Trees
(RBST). We apply our method to a dataset of 1.25M binary local feature
descriptors obtained from a real-life image-based localisation system provided
by Google as a part of Project Tango. An extensive evaluation of our method
against the state-of-the-art variations of Locality Sensitive Hashing (LSH),
namely Uniform LSH and Multi-probe LSH, shows the superiority of our method in
terms of retrieval precision with performance boost of over 20%</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/zhang2019improved/">Improved Deep Hashing With Soft Pairwise Similarity For Multi-label Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Improved Deep Hashing With Soft Pairwise Similarity For Multi-label Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Improved Deep Hashing With Soft Pairwise Similarity For Multi-label Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>144</td>
    <td><p>Hash coding has been widely used in the approximate nearest neighbor search
for large-scale image retrieval. Recently, many deep hashing methods have been
proposed and shown largely improved performance over traditional
feature-learning-based methods. Most of these methods examine the pairwise
similarity on the semantic-level labels, where the pairwise similarity is
generally defined in a hard-assignment way. That is, the pairwise similarity is
‘1’ if they share no less than one class label and ‘0’ if they do not share
any. However, such similarity definition cannot reflect the similarity ranking
for pairwise images that hold multiple labels. In this paper, a new deep
hashing method is proposed for multi-label image retrieval by re-defining the
pairwise similarity into an instance similarity, where the instance similarity
is quantified into a percentage based on the normalized semantic labels. Based
on the instance similarity, a weighted cross-entropy loss and a minimum mean
square error loss are tailored for loss-function construction, and are
efficiently used for simultaneous feature learning and hash coding. Experiments
on three popular datasets demonstrate that, the proposed method outperforms the
competing methods and achieves the state-of-the-art performance in multi-label
image retrieval.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/ko2019benchmark/">A Benchmark On Tricks For Large-scale Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Benchmark On Tricks For Large-scale Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Benchmark On Tricks For Large-scale Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ko et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE 4th International Conference on Image, Vision and Computing (ICIVC)</td>
    <td>15</td>
    <td><p>Many studies have been performed on metric learning, which has become a key
ingredient in top-performing methods of instance-level image retrieval.
Meanwhile, less attention has been paid to pre-processing and post-processing
tricks that can significantly boost performance. Furthermore, we found that
most previous studies used small scale datasets to simplify processing. Because
the behavior of a feature representation in a deep learning model depends on
both domain and data, it is important to understand how model behave in
large-scale environments when a proper combination of retrieval tricks is used.
In this paper, we extensively analyze the effect of well-known pre-processing,
post-processing tricks, and their combination for large-scale image retrieval.
We found that proper use of these tricks can significantly improve model
performance without necessitating complex architecture or introducing loss, as
confirmed by achieving a competitive result on the Google Landmark Retrieval
Challenge 2019.</p>
</td>
    <td>
      
        Evaluation 
      
        Image Retrieval 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/klein2019end/">End-to-end Supervised Product Quantization For Image Search And Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=End-to-end Supervised Product Quantization For Image Search And Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=End-to-end Supervised Product Quantization For Image Search And Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Klein Benjamin, Wolf Lior</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>59</td>
    <td><p>Product Quantization, a dictionary based hashing method, is one of the
leading unsupervised hashing techniques. While it ignores the labels, it
harnesses the features to construct look up tables that can approximate the
feature space. In recent years, several works have achieved state of the art
results on hashing benchmarks by learning binary representations in a
supervised manner. This work presents Deep Product Quantization (DPQ), a
technique that leads to more accurate retrieval and classification than the
latest state of the art methods, while having similar computational complexity
and memory footprint as the Product Quantization method. To our knowledge, this
is the first work to introduce a dictionary-based representation that is
inspired by Product Quantization and which is learned end-to-end, and thus
benefits from the supervised signal. DPQ explicitly learns soft and hard
representations to enable an efficient and accurate asymmetric search, by using
a straight-through estimator. Our method obtains state of the art results on an
extensive array of retrieval and classification experiments.</p>
</td>
    <td>
      
        Quantization 
      
        Image Retrieval 
      
        SUPERVISED 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/roth2019mic/">MIC: Mining Interclass Characteristics For Improved Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=MIC: Mining Interclass Characteristics For Improved Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=MIC: Mining Interclass Characteristics For Improved Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Roth Karsten, Brattoli Biagio, Ommer Björn</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>92</td>
    <td><p>Metric learning seeks to embed images of objects suchthat class-defined
relations are captured by the embeddingspace. However, variability in images is
not just due to different depicted object classes, but also depends on other
latent characteristics such as viewpoint or illumination. In addition to these
structured properties, random noise further obstructs the visual relations of
interest. The common approach to metric learning is to enforce a representation
that is invariant under all factors but the ones of interest. In contrast, we
propose to explicitly learn the latent characteristics that are shared by and
go across object classes. We can then directly explain away structured visual
variability, rather than assuming it to be unknown random noise. We propose a
novel surrogate task to learn visual characteristics shared across classes with
a separate encoder. This encoder is trained jointly with the encoder for class
information by reducing their mutual information. On five standard image
retrieval benchmarks the approach significantly improves upon the
state-of-the-art.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/li2019rethinking/">Rethinking Loss Design For Large-scale 3D Shape Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Rethinking Loss Design For Large-scale 3D Shape Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Rethinking Loss Design For Large-scale 3D Shape Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Zhaoqun, Xu Cheng, Leng Biao</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</td>
    <td>5</td>
    <td><p>Learning discriminative shape representations is a crucial issue for
large-scale 3D shape retrieval. In this paper, we propose the Collaborative
Inner Product Loss (CIP Loss) to obtain ideal shape embedding that
discriminative among different categories and clustered within the same class.
Utilizing simple inner product operation, CIP loss explicitly enforces the
features of the same class to be clustered in a linear subspace, while
inter-class subspaces are constrained to be at least orthogonal. Compared to
previous metric loss functions, CIP loss could provide more clear geometric
interpretation for the embedding than Euclidean margin, and is easy to
implement without normalization operation referring to cosine margin. Moreover,
our proposed loss term can combine with other commonly used loss functions and
can be easily plugged into existing off-the-shelf architectures. Extensive
experiments conducted on the two public 3D object retrieval datasets, ModelNet
and ShapeNetCore 55, demonstrate the effectiveness of our proposal, and our
method has achieved state-of-the-art results on both datasets.</p>
</td>
    <td>
      
        SCALABILITY 
      
        AAAI 
      
        IJCAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/li2019push/">Push For Quantization: Deep Fisher Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Push For Quantization: Deep Fisher Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Push For Quantization: Deep Fisher Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>7</td>
    <td><p>Current massive datasets demand light-weight access for analysis. Discrete
hashing methods are thus beneficial because they map high-dimensional data to
compact binary codes that are efficient to store and process, while preserving
semantic similarity. To optimize powerful deep learning methods for image
hashing, gradient-based methods are required. Binary codes, however, are
discrete and thus have no continuous derivatives. Relaxing the problem by
solving it in a continuous space and then quantizing the solution is not
guaranteed to yield separable binary codes. The quantization needs to be
included in the optimization. In this paper we push for quantization: We
optimize maximum class separability in the binary space. We introduce a margin
on distances between dissimilar image pairs as measured in the binary space. In
addition to pair-wise distances, we draw inspiration from Fisher’s Linear
Discriminant Analysis (Fisher LDA) to maximize the binary distances between
classes and at the same time minimize the binary distance of images within the
same class. Experiments on CIFAR-10, NUS-WIDE and ImageNet100 demonstrate
compact codes comparing favorably to the current state of the art.</p>
</td>
    <td>
      
        Quantization 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/li2019sign/">Sign-full Random Projections</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Sign-full Random Projections' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Sign-full Random Projections' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Ping</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>8</td>
    <td><p>The method of 1-bit (“sign-sign”) random projections has been a popular tool
for efficient search and machine learning on large datasets. Given two \(D\)-dim
data vectors \(u\), \(v\in\mathbb{R}^D\), one can generate \(x = \sum_{i=1}^D u_i
r_i\), and \(y = \sum_{i=1}^D v_i r_i\), where \(r_i\sim N(0,1)\) iid. The
“collision probability” is \({Pr}\left(sgn(x)=sgn(y)\right) =
1-\frac{\cos^{-1}\rho}{\pi}\), where \(\rho = \rho(u,v)\) is the cosine
similarity.
  We develop “sign-full” random projections by estimating \(\rho\) from (e.g.,)
the expectation \(E(sgn(x)y)=\sqrt{\frac{2}{\pi}} \rho\), which can be further
substantially improved by normalizing \(y\). For nonnegative data, we recommend
an interesting estimator based on \(E\left(y_- 1<em>{x\geq 0} + y</em>+ 1_{x&lt;0}\right)\)
and its normalized version. The recommended estimator almost matches the
accuracy of the (computationally expensive) maximum likelihood estimator. At
high similarity (\(\rho\rightarrow1\)), the asymptotic variance of recommended
estimator is only \(\frac{4}{3\pi} \approx 0.4\) of the estimator for sign-sign
projections. At small \(k\) and high similarity, the improvement would be even
much more substantial.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/li2019memory/">Memory-based Neighbourhood Embedding For Visual Recognition</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Memory-based Neighbourhood Embedding For Visual Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Memory-based Neighbourhood Embedding For Visual Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>44</td>
    <td><p>Learning discriminative image feature embeddings is of great importance to
visual recognition. To achieve better feature embeddings, most current methods
focus on designing different network structures or loss functions, and the
estimated feature embeddings are usually only related to the input images. In
this paper, we propose Memory-based Neighbourhood Embedding (MNE) to enhance a
general CNN feature by considering its neighbourhood. The method aims to solve
two critical problems, i.e., how to acquire more relevant neighbours in the
network training and how to aggregate the neighbourhood information for a more
discriminative embedding. We first augment an episodic memory module into the
network, which can provide more relevant neighbours for both training and
testing. Then the neighbours are organized in a tree graph with the target
instance as the root node. The neighbourhood information is gradually
aggregated to the root node in a bottom-up manner, and aggregation weights are
supervised by the class relationships between the nodes. We apply MNE on image
search and few shot learning tasks. Extensive ablation studies demonstrate the
effectiveness of each component, and our method significantly outperforms the
state-of-the-art approaches.</p>
</td>
    <td>
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/li2019coupled/">Coupled Cyclegan: Unsupervised Hashing Network For Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Coupled Cyclegan: Unsupervised Hashing Network For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Coupled Cyclegan: Unsupervised Hashing Network For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>105</td>
    <td><p>In recent years, hashing has attracted more and more attention owing to its
superior capacity of low storage cost and high query efficiency in large-scale
cross-modal retrieval. Benefiting from deep leaning, continuously compelling
results in cross-modal retrieval community have been achieved. However,
existing deep cross-modal hashing methods either rely on amounts of labeled
information or have no ability to learn an accuracy correlation between
different modalities. In this paper, we proposed Unsupervised coupled Cycle
generative adversarial Hashing networks (UCH), for cross-modal retrieval, where
outer-cycle network is used to learn powerful common representation, and
inner-cycle network is explained to generate reliable hash codes. Specifically,
our proposed UCH seamlessly couples these two networks with generative
adversarial mechanism, which can be optimized simultaneously to learn
representation and hash codes. Extensive experiments on three popular benchmark
datasets show that the proposed UCH outperforms the state-of-the-art
unsupervised cross-modal hashing methods.</p>
</td>
    <td>
      
        Unsupervised 
      
        Neural Hashing 
      
        AAAI 
      
        SUPERVISED 
      
        Multimodal Retrieval 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/li2019dual/">Dual Asymmetric Deep Hashing Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Dual Asymmetric Deep Hashing Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Dual Asymmetric Deep Hashing Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Access</td>
    <td>16</td>
    <td><p>Due to the impressive learning power, deep learning has achieved a remarkable
performance in supervised hash function learning. In this paper, we propose a
novel asymmetric supervised deep hashing method to preserve the semantic
structure among different categories and generate the binary codes
simultaneously. Specifically, two asymmetric deep networks are constructed to
reveal the similarity between each pair of images according to their semantic
labels. The deep hash functions are then learned through two networks by
minimizing the gap between the learned features and discrete codes.
Furthermore, since the binary codes in the Hamming space also should keep the
semantic affinity existing in the original space, another asymmetric pairwise
loss is introduced to capture the similarity between the binary codes and
real-value features. This asymmetric loss not only improves the retrieval
performance, but also contributes to a quick convergence at the training phase.
By taking advantage of the two-stream deep structures and two types of
asymmetric pairwise functions, an alternating algorithm is designed to optimize
the deep features and high-quality binary codes efficiently. Experimental
results on three real-world datasets substantiate the effectiveness and
superiority of our approach as compared with state-of-the-art.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/li2019neighborhood/">Neighborhood Preserving Hashing For Scalable Video Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Neighborhood Preserving Hashing For Scalable Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Neighborhood Preserving Hashing For Scalable Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>40</td>
    <td><p>In this paper, we propose a Neighborhood Preserving
Hashing (NPH) method for scalable video retrieval in an
unsupervised manner. Unlike most existing deep video
hashing methods which indiscriminately compress an entire video into a binary code, we embed the spatial-temporal
neighborhood information into the encoding network such
that the neighborhood-relevant visual content of a video can
be preferentially encoded into a binary code under the guidance of the neighborhood information. Specifically, we propose a neighborhood attention mechanism which focuses
on partial useful content of each input frame conditioned
on the neighborhood information. We then integrate the
neighborhood attention mechanism into an RNN-based reconstruction scheme to encourage the binary codes to capture the spatial-temporal structure in a video which is consistent with that in the neighborhood. As a consequence, the
learned hashing functions can map similar videos to similar
binary codes. Extensive experiments on three widely-used
benchmark datasets validate the effectiveness of our proposed approach.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Video Retrieval 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/lu2019online/">Online Multi-modal Hashing With Dynamic Query-adaption</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Online Multi-modal Hashing With Dynamic Query-adaption' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Online Multi-modal Hashing With Dynamic Query-adaption' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>128</td>
    <td><p>Multi-modal hashing is an effective technique to support large-scale multimedia retrieval, due to its capability of encoding heterogeneous multi-modal features into compact and similarity-preserving binary codes. Although great progress has been achieved so far, existing methods still suffer from several problems, including: 1) All existing methods simply adopt fixed modality combination weights in online hashing process to generate the query hash codes. This strategy cannot adaptively capture the variations of different queries. 2) They either suffer from insufficient semantics (for unsupervised methods) or require high computation and storage cost (for the supervised methods, which rely on pair-wise semantic matrix). 3) They solve the hash codes with relaxed optimization strategy or bit-by-bit discrete optimization, which results in significant quantization loss or consumes considerable computation time. To address the above limitations, in this paper, we propose an Online Multi-modal Hashing with Dynamic Query-adaption (OMH-DQ) method in a novel fashion. Specifically, a self-weighted fusion strategy is designed to adaptively preserve the multi-modal feature information into hash codes by exploiting their complementarity. The hash codes are learned with the supervision of pair-wise semantic labels to enhance their discriminative capability, while avoiding the challenging symmetric similarity matrix factorization. Under such learning framework, the binary hash codes can be directly obtained with efficient operations and without quantization errors. Accordingly, our method can benefit from the semantic labels, and simultaneously, avoid the high computation complexity. Moreover, to accurately capture the query variations, at the online retrieval stage, we design a parameter-free online hashing module which can adaptively learn the query hash codes according to the dynamic query contents. Extensive experiments demonstrate the state-of-the-art performance of the proposed approach from various aspects.</p>
</td>
    <td>
      
        SIGIR 
      
        Hashing Methods 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/zhe2019directional/">Directional Statistics-based Deep Metric Learning For Image Classification And Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Directional Statistics-based Deep Metric Learning For Image Classification And Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Directional Statistics-based Deep Metric Learning For Image Classification And Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhe Xuefei, Chen Shifeng, Yan Hong</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition</td>
    <td>72</td>
    <td><p>Deep distance metric learning (DDML), which is proposed to learn image
similarity metrics in an end-to-end manner based on the convolution neural
network, has achieved encouraging results in many computer vision
tasks.\(L2\)-normalization in the embedding space has been used to improve the
performance of several DDML methods. However, the commonly used Euclidean
distance is no longer an accurate metric for \(L2\)-normalized embedding space,
i.e., a hyper-sphere. Another challenge of current DDML methods is that their
loss functions are usually based on rigid data formats, such as the triplet
tuple. Thus, an extra process is needed to prepare data in specific formats. In
addition, their losses are obtained from a limited number of samples, which
leads to a lack of the global view of the embedding space. In this paper, we
replace the Euclidean distance with the cosine similarity to better utilize the
\(L2\)-normalization, which is able to attenuate the curse of dimensionality.
More specifically, a novel loss function based on the von Mises-Fisher
distribution is proposed to learn a compact hyper-spherical embedding space.
Moreover, a new efficient learning algorithm is developed to better capture the
global structure of the embedding space. Experiments for both classification
and retrieval tasks on several standard datasets show that our method achieves
state-of-the-art performance with a simpler training procedure. Furthermore, we
demonstrate that, even with a small number of convolutional layers, our model
can still obtain significantly better classification performance than the
widely used softmax loss.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/zhang2019effective/">Effective Image Retrieval Via Multilinear Multi-index Fusion</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Effective Image Retrieval Via Multilinear Multi-index Fusion' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Effective Image Retrieval Via Multilinear Multi-index Fusion' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>24</td>
    <td><p>Multi-index fusion has demonstrated impressive performances in retrieval task
by integrating different visual representations in a unified framework.
However, previous works mainly consider propagating similarities via neighbor
structure, ignoring the high order information among different visual
representations. In this paper, we propose a new multi-index fusion scheme for
image retrieval. By formulating this procedure as a multilinear based
optimization problem, the complementary information hidden in different indexes
can be explored more thoroughly. Specially, we first build our multiple indexes
from various visual representations. Then a so-called index-specific functional
matrix, which aims to propagate similarities, is introduced for updating the
original index. The functional matrices are then optimized in a unified tensor
space to achieve a refinement, such that the relevant images can be pushed more
closer. The optimization problem can be efficiently solved by the augmented
Lagrangian method with theoretical convergence guarantee. Unlike the
traditional multi-index fusion scheme, our approach embeds the multi-index
subspace structure into the new indexes with sparse constraint, thus it has
little additional memory consumption in online query stage. Experimental
evaluation on three benchmark datasets reveals that the proposed approach
achieves the state-of-the-art performance, i.e., N-score 3.94 on UKBench, mAP
94.1% on Holiday and 62.39% on Market-1501.</p>
</td>
    <td>
      
        Vector Indexing 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/zhang2019semantic/">Semantic Cluster Unary Loss For Efficient Deep Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semantic Cluster Unary Loss For Efficient Deep Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semantic Cluster Unary Loss For Efficient Deep Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Shifeng, Li Jianmin, Zhang Bo</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>13</td>
    <td><p>Hashing method maps similar data to binary hashcodes with smaller hamming
distance, which has received a broad attention due to its low storage cost and
fast retrieval speed. With the rapid development of deep learning, deep hashing
methods have achieved promising results in efficient information retrieval.
Most of the existing deep hashing methods adopt pairwise or triplet losses to
deal with similarities underlying the data, but the training is difficult and
less efficient because \(O(n^2)\) data pairs and \(O(n^3)\) triplets are involved.
To address these issues, we propose a novel deep hashing algorithm with unary
loss which can be trained very efficiently. We first of all introduce a Unary
Upper Bound of the traditional triplet loss, thus reducing the complexity to
\(O(n)\) and bridging the classification-based unary loss and the triplet loss.
Second, we propose a novel Semantic Cluster Deep Hashing (SCDH) algorithm by
introducing a modified Unary Upper Bound loss, named Semantic Cluster Unary
Loss (SCUL). The resultant hashcodes form several compact clusters, which means
hashcodes in the same cluster have similar semantic information. We also
demonstrate that the proposed SCDH is easy to be extended to semi-supervised
settings by incorporating the state-of-the-art semi-supervised learning
algorithms. Experiments on large-scale datasets show that the proposed method
is superior to state-of-the-art hashing algorithms.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/liberman2019search/">Search-based Serving Architecture Of Embeddings-based Recommendations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Search-based Serving Architecture Of Embeddings-based Recommendations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Search-based Serving Architecture Of Embeddings-based Recommendations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liberman et al.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)</td>
    <td>7</td>
    <td><p>Over the past 10 years, many recommendation techniques have been based on
embedding users and items in latent vector spaces, where the inner product of a
(user,item) pair of vectors represents the predicted affinity of the user to
the item. A wealth of literature has focused on the various modeling approaches
that result in embeddings, and has compared their quality metrics, learning
complexity, etc. However, much less attention has been devoted to the issues
surrounding productization of an embeddings-based high throughput, low latency
recommender system. In particular, how the system might keep up with the
changing embeddings as new models are learnt. This paper describes a reference
architecture of a high-throughput, large scale recommendation service which
leverages a search engine as its runtime core. We describe how the search index
and the query builder adapt to changes in the embeddings, which often happen at
a different cadence than index builds. We provide solutions for both id-based
and feature-based embeddings, as well as for batch indexing and incremental
indexing setups. The described system is at the core of a Web content discovery
service that serves tens of billions recommendations per day in response to
billions of user requests.</p>
</td>
    <td>
      
        Recommender Systems 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/tuinhof2019image/">Image Based Fashion Product Recommendation With Deep Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Image Based Fashion Product Recommendation With Deep Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Image Based Fashion Product Recommendation With Deep Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tuinhof Hessel, Pirker Clemens, Haltmeier Markus</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>64</td>
    <td><p>We develop a two-stage deep learning framework that recommends fashion images
based on other input images of similar style. For that purpose, a neural
network classifier is used as a data-driven, visually-aware feature extractor.
The latter then serves as input for similarity-based recommendations using a
ranking algorithm. Our approach is tested on the publicly available Fashion
dataset. Initialization strategies using transfer learning from larger product
databases are presented. Combined with more traditional content-based
recommendation systems, our framework can help to increase robustness and
performance, for example, by better matching a particular customer style.</p>
</td>
    <td>
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/rossetto2019query/">Query By Semantic Sketch</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Query By Semantic Sketch' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Query By Semantic Sketch' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Rossetto Luca, Gasser Ralph, Schuldt Heiko</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>Sketch-based query formulation is very common in image and video retrieval as
these techniques often complement textual retrieval methods that are based on
either manual or machine generated annotations. In this paper, we present a
retrieval approach that allows to query visual media collections by sketching
concept maps, thereby merging sketch-based retrieval with the search for
semantic labels. Users can draw a spatial distribution of different concept
labels, such as “sky”, “sea” or “person” and then use these sketches to find
images or video scenes that exhibit a similar distribution of these concepts.
Hence, this approach does not only take the semantic concepts themselves into
account, but also their semantic relations as well as their spatial context.
The efficient vector representation enables efficient retrieval even in large
multimedia collections. We have integrated the semantic sketch query mode into
our retrieval engine vitrivr and demonstrated its effectiveness.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/rahmani2019category/">Category-aware Location Embedding For Point-of-interest Recommendation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Category-aware Location Embedding For Point-of-interest Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Category-aware Location Embedding For Point-of-interest Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Rahmani et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2019 ACM SIGIR International Conference on Theory of Information Retrieval</td>
    <td>17</td>
    <td><p>Recently, Point of interest (POI) recommendation has gained ever-increasing
importance in various Location-Based Social Networks (LBSNs). With the recent
advances of neural models, much work has sought to leverage neural networks to
learn neural embeddings in a pre-training phase that achieve an improved
representation of POIs and consequently a better recommendation. However,
previous studies fail to capture crucial information about POIs such as
categorical information.
  In this paper, we propose a novel neural model that generates a POI embedding
incorporating sequential and categorical information from POIs. Our model
consists of a check-in module and a category module. The check-in module
captures the geographical influence of POIs derived from the sequence of users’
check-ins, while the category module captures the characteristics of POIs
derived from the category information. To validate the efficacy of the model,
we experimented with two large-scale LBSN datasets. Our experimental results
demonstrate that our approach significantly outperforms state-of-the-art POI
recommendation methods.</p>
</td>
    <td>
      
        SIGIR 
      
        Text Retrieval 
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/lin2019supervised/">Supervised Online Hashing Via Similarity Distribution Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Online Hashing Via Similarity Distribution Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Supervised Online Hashing Via Similarity Distribution Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 26th ACM international conference on Multimedia</td>
    <td>47</td>
    <td><p>Online hashing has attracted extensive research attention when facing
streaming data. Most online hashing methods, learning binary codes based on
pairwise similarities of training instances, fail to capture the semantic
relationship, and suffer from a poor generalization in large-scale applications
due to large variations. In this paper, we propose to model the similarity
distributions between the input data and the hashing codes, upon which a novel
supervised online hashing method, dubbed as Similarity Distribution based
Online Hashing (SDOH), is proposed, to keep the intrinsic semantic relationship
in the produced Hamming space. Specifically, we first transform the discrete
similarity matrix into a probability matrix via a Gaussian-based normalization
to address the extremely imbalanced distribution issue. And then, we introduce
a scaling Student t-distribution to solve the challenging initialization
problem, and efficiently bridge the gap between the known and unknown
distributions. Lastly, we align the two distributions via minimizing the
Kullback-Leibler divergence (KL-diverence) with stochastic gradient descent
(SGD), by which an intuitive similarity constraint is imposed to update hashing
model on the new streaming data with a powerful generalizing ability to the
past data. Extensive experiments on three widely-used benchmarks validate the
superiority of the proposed SDOH over the state-of-the-art methods in the
online retrieval task.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/lin2019situating/">Situating Sentence Embedders With Nearest Neighbor Overlap</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Situating Sentence Embedders With Nearest Neighbor Overlap' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Situating Sentence Embedders With Nearest Neighbor Overlap' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin Lucy H., Smith Noah A.</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition</td>
    <td>16</td>
    <td><p>As distributed approaches to natural language semantics have developed and
diversified, embedders for linguistic units larger than words have come to play
an increasingly important role. To date, such embedders have been evaluated
using benchmark tasks (e.g., GLUE) and linguistic probes. We propose a
comparative approach, nearest neighbor overlap (N2O), that quantifies
similarity between embedders in a task-agnostic manner. N2O requires only a
collection of examples and is simple to understand: two embedders are more
similar if, for the same set of inputs, there is greater overlap between the
inputs’ nearest neighbors. Though applicable to embedders of texts of any size,
we focus on sentence embedders and use N2O to show the effects of different
design choices and architectures.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/lin2019towards/">Towards Optimal Discrete Online Hashing With Balanced Similarity</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Towards Optimal Discrete Online Hashing With Balanced Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Towards Optimal Discrete Online Hashing With Balanced Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>50</td>
    <td><p>When facing large-scale image datasets, online hashing serves as a promising
solution for online retrieval and prediction tasks. It encodes the online
streaming data into compact binary codes, and simultaneously updates the hash
functions to renew codes of the existing dataset. To this end, the existing
methods update hash functions solely based on the new data batch, without
investigating the correlation between such new data and the existing dataset.
In addition, existing works update the hash functions using a relaxation
process in its corresponding approximated continuous space. And it remains as
an open problem to directly apply discrete optimizations in online hashing. In
this paper, we propose a novel supervised online hashing method, termed
Balanced Similarity for Online Discrete Hashing (BSODH), to solve the above
problems in a unified framework. BSODH employs a well-designed hashing
algorithm to preserve the similarity between the streaming data and the
existing dataset via an asymmetric graph regularization. We further identify
the “data-imbalance” problem brought by the constructed asymmetric graph, which
restricts the application of discrete optimization in our problem. Therefore, a
novel balanced similarity is further proposed, which uses two equilibrium
factors to balance the similar and dissimilar weights and eventually enables
the usage of discrete optimizations. Extensive experiments conducted on three
widely-used benchmarks demonstrate the advantages of the proposed method over
the state-of-the-art methods.</p>
</td>
    <td>
      
        Hashing Methods 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/sanakoyeu2019divide/">Divide And Conquer The Embedding Space For Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Divide And Conquer The Embedding Space For Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Divide And Conquer The Embedding Space For Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sanakoyeu et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>122</td>
    <td><p>Learning the embedding space, where semantically similar objects are located
close together and dissimilar objects far apart, is a cornerstone of many
computer vision applications. Existing approaches usually learn a single metric
in the embedding space for all available data points, which may have a very
complex non-uniform distribution with different notions of similarity between
objects, e.g. appearance, shape, color or semantic meaning. Approaches for
learning a single distance metric often struggle to encode all different types
of relationships and do not generalize well. In this work, we propose a novel
easy-to-implement divide and conquer approach for deep metric learning, which
significantly improves the state-of-the-art performance of metric learning. Our
approach utilizes the embedding space more efficiently by jointly splitting the
embedding space and data into \(K\) smaller sub-problems. It divides both, the
data and the embedding space into \(K\) subsets and learns \(K\) separate distance
metrics in the non-overlapping subspaces of the embedding space, defined by
groups of neurons in the embedding layer of the neural network. The proposed
approach increases the convergence speed and improves generalization since the
complexity of each sub-problem is reduced compared to the original one. We show
that our approach outperforms the state-of-the-art by a large margin in
retrieval, clustering and re-identification tasks on CUB200-2011, CARS196,
Stanford Online Products, In-shop Clothes and PKU VehicleID datasets.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/tu2019object/">Object Detection Based Deep Unsupervised Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Object Detection Based Deep Unsupervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Object Detection Based Deep Unsupervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</td>
    <td>10</td>
    <td><p>Recently, similarity-preserving hashing methods have been extensively studied
for large-scale image retrieval. Compared with unsupervised hashing, supervised
hashing methods for labeled data have usually better performance by utilizing
semantic label information. Intuitively, for unlabeled data, it will improve
the performance of unsupervised hashing methods if we can first mine some
supervised semantic ‘label information’ from unlabeled data and then
incorporate the ‘label information’ into the training process. Thus, in this
paper, we propose a novel Object Detection based Deep Unsupervised Hashing
method (ODDUH). Specifically, a pre-trained object detection model is utilized
to mining supervised ‘label information’, which is used to guide the learning
process to generate high-quality hash codes.Extensive experiments on two public
datasets demonstrate that the proposed method outperforms the state-of-the-art
unsupervised hashing methods in the image retrieval task.</p>
</td>
    <td>
      
        IJCAI 
      
        Unsupervised 
      
        Neural Hashing 
      
        AAAI 
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/lu2019fmhash/">Fmhash: Deep Hashing Of In-air-handwriting For User Identification</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fmhash: Deep Hashing Of In-air-handwriting For User Identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fmhash: Deep Hashing Of In-air-handwriting For User Identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lu Duo, Huang Dijiang, Rai Anshul</td> <!-- 🔧 You were missing this -->
    <td>ICC 2019 - 2019 IEEE International Conference on Communications (ICC)</td>
    <td>9</td>
    <td><p>Many mobile systems and wearable devices, such as Virtual Reality (VR) or
Augmented Reality (AR) headsets, lack a keyboard or touchscreen to type an ID
and password for signing into a virtual website. However, they are usually
equipped with gesture capture interfaces to allow the user to interact with the
system directly with hand gestures. Although gesture-based authentication has
been well-studied, less attention is paid to the gesture-based user
identification problem, which is essentially an input method of account ID and
an efficient searching and indexing method of a database of gesture signals. In
this paper, we propose FMHash (i.e., Finger Motion Hash), a user identification
framework that can generate a compact binary hash code from a piece of
in-air-handwriting of an ID string. This hash code enables indexing and fast
search of a large account database using the in-air-handwriting by a hash
table. To demonstrate the effectiveness of the framework, we implemented a
prototype and achieved &gt;99.5% precision and &gt;92.6% recall with exact hash code
match on a dataset of 200 accounts collected by us. The ability of hashing
in-air-handwriting pattern to binary code can be used to achieve convenient
sign-in and sign-up with in-air-handwriting gesture ID on future mobile and
wearable systems connected to the Internet.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/qiao2019deep/">Deep Heterogeneous Hashing For Face Video Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Heterogeneous Hashing For Face Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Heterogeneous Hashing For Face Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Qiao et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>21</td>
    <td><p>Retrieving videos of a particular person with face image as a query via
hashing technique has many important applications. While face images are
typically represented as vectors in Euclidean space, characterizing face videos
with some robust set modeling techniques (e.g. covariance matrices as exploited
in this study, which reside on Riemannian manifold), has recently shown
appealing advantages. This hence results in a thorny heterogeneous spaces
matching problem. Moreover, hashing with handcrafted features as done in many
existing works is clearly inadequate to achieve desirable performance for this
task. To address such problems, we present an end-to-end Deep Heterogeneous
Hashing (DHH) method that integrates three stages including image feature
learning, video modeling, and heterogeneous hashing in a single framework, to
learn unified binary codes for both face images and videos. To tackle the key
challenge of hashing on the manifold, a well-studied Riemannian kernel mapping
is employed to project data (i.e. covariance matrices) into Euclidean space and
thus enables to embed the two heterogeneous representations into a common
Hamming space, where both intra-space discriminability and inter-space
compatibility are considered. To perform network optimization, the gradient of
the kernel mapping is innovatively derived via structured matrix
backpropagation in a theoretically principled way. Experiments on three
challenging datasets show that our method achieves quite competitive
performance compared with existing hashing methods.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Video Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/yang2019feature/">Feature Pyramid Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Feature Pyramid Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Feature Pyramid Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2019 on International Conference on Multimedia Retrieval</td>
    <td>22</td>
    <td><p>In recent years, deep-networks-based hashing has become a leading approach
for large-scale image retrieval. Most deep hashing approaches use the high
layer to extract the powerful semantic representations. However, these methods
have limited ability for fine-grained image retrieval because the semantic
features extracted from the high layer are difficult in capturing the subtle
differences. To this end, we propose a novel two-pyramid hashing architecture
to learn both the semantic information and the subtle appearance details for
fine-grained image search. Inspired by the feature pyramids of convolutional
neural network, a vertical pyramid is proposed to capture the high-layer
features and a horizontal pyramid combines multiple low-layer features with
structural information to capture the subtle differences. To fuse the low-level
features, a novel combination strategy, called consensus fusion, is proposed to
capture all subtle information from several low-layers for finer retrieval.
Extensive evaluation on two fine-grained datasets CUB-200-2011 and Stanford
Dogs demonstrate that the proposed method achieves significant performance
compared with the state-of-art baselines.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Multimodal Retrieval 
      
        Medical Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/lejeune2019adaptive/">Adaptive Estimation For Approximate K-nearest-neighbor Computations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Adaptive Estimation For Approximate K-nearest-neighbor Computations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Adaptive Estimation For Approximate K-nearest-neighbor Computations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lejeune Daniel, Baraniuk Richard G., Heckel Reinhard</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of Machine Learning Research 89 (2019)3099-3107</td>
    <td>8</td>
    <td><p>Algorithms often carry out equally many computations for “easy” and “hard”
problem instances. In particular, algorithms for finding nearest neighbors
typically have the same running time regardless of the particular problem
instance. In this paper, we consider the approximate k-nearest-neighbor
problem, which is the problem of finding a subset of O(k) points in a given set
of points that contains the set of k nearest neighbors of a given query point.
We propose an algorithm based on adaptively estimating the distances, and show
that it is essentially optimal out of algorithms that are only allowed to
adaptively estimate distances. We then demonstrate both theoretically and
experimentally that the algorithm can achieve significant speedups relative to
the naive method.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/ma2019hierarchy/">Hierarchy Neighborhood Discriminative Hashing For An Unified View Of Single-label And Multi-label Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hierarchy Neighborhood Discriminative Hashing For An Unified View Of Single-label And Multi-label Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hierarchy Neighborhood Discriminative Hashing For An Unified View Of Single-label And Multi-label Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ma et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</td>
    <td>22</td>
    <td><p>Recently, deep supervised hashing methods have become popular for large-scale
image retrieval task. To preserve the semantic similarity notion between
examples, they typically utilize the pairwise supervision or the triplet
supervised information for hash learning. However, these methods usually ignore
the semantic class information which can help the improvement of the semantic
discriminative ability of hash codes. In this paper, we propose a novel
hierarchy neighborhood discriminative hashing method. Specifically, we
construct a bipartite graph to build coarse semantic neighbourhood relationship
between the sub-class feature centers and the embeddings features. Moreover, we
utilize the pairwise supervised information to construct the fined semantic
neighbourhood relationship between embeddings features. Finally, we propose a
hierarchy neighborhood discriminative hashing loss to unify the single-label
and multilabel image retrieval problem with a one-stream deep neural network
architecture. Experimental results on two largescale datasets demonstrate that
the proposed method can outperform the state-of-the-art hashing methods.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
        AAAI 
      
        IJCAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/khurshid2019cross/">Cross-view Image Retrieval -- Ground To Aerial Image Retrieval Through Deep Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-view Image Retrieval -- Ground To Aerial Image Retrieval Through Deep Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-view Image Retrieval -- Ground To Aerial Image Retrieval Through Deep Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Khurshid et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>5</td>
    <td><p>Cross-modal retrieval aims to measure the content similarity between
different types of data. The idea has been previously applied to visual, text,
and speech data. In this paper, we present a novel cross-modal retrieval method
specifically for multi-view images, called Cross-view Image Retrieval CVIR. Our
approach aims to find a feature space as well as an embedding space in which
samples from street-view images are compared directly to satellite-view images
(and vice-versa). For this comparison, a novel deep metric learning based
solution “DeepCVIR” has been proposed. Previous cross-view image datasets are
deficient in that they (1) lack class information; (2) were originally
collected for cross-view image geolocalization task with coupled images; (3) do
not include any images from off-street locations. To train, compare, and
evaluate the performance of cross-view image retrieval, we present a new 6
class cross-view image dataset termed as CrossViewRet which comprises of images
including freeway, mountain, palace, river, ship, and stadium with 700
high-resolution dual-view images for each class. Results show that the proposed
DeepCVIR outperforms conventional matching approaches on the CVIR task for the
given dataset and would also serve as the baseline for future research.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/kim2019deep/">Deep Metric Learning Beyond Binary Supervision</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Metric Learning Beyond Binary Supervision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Metric Learning Beyond Binary Supervision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kim et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>87</td>
    <td><p>Metric Learning for visual similarity has mostly adopted binary supervision
indicating whether a pair of images are of the same class or not. Such a binary
indicator covers only a limited subset of image relations, and is not
sufficient to represent semantic similarity between images described by
continuous and/or structured labels such as object poses, image captions, and
scene graphs. Motivated by this, we present a novel method for deep metric
learning using continuous labels. First, we propose a new triplet loss that
allows distance ratios in the label space to be preserved in the learned metric
space. The proposed loss thus enables our model to learn the degree of
similarity rather than just the order. Furthermore, we design a triplet mining
strategy adapted to metric learning with continuous labels. We address three
different image retrieval tasks with continuous labels in terms of human poses,
room layouts and image captions, and demonstrate the superior performance of
our approach compared to previous methods.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/markchit2019effective/">Effective And Efficient Indexing In Cross-modal Hashing-based Datasets</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Effective And Efficient Indexing In Cross-modal Hashing-based Datasets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Effective And Efficient Indexing In Cross-modal Hashing-based Datasets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Markchit Sarawut, Chiu Chih-yi</td> <!-- 🔧 You were missing this -->
    <td>Signal Processing: Image Communication</td>
    <td>6</td>
    <td><p>To overcome the barrier of storage and computation, the hashing technique has
been widely used for nearest neighbor search in multimedia retrieval
applications recently. Particularly, cross-modal retrieval that searches across
different modalities becomes an active but challenging problem. Although dozens
of cross-modal hashing algorithms are proposed to yield compact binary codes,
the exhaustive search is impractical for the real-time purpose, and Hamming
distance computation suffers inaccurate results. In this paper, we propose a
novel search method that utilizes a probability-based index scheme over binary
hash codes in cross-modal retrieval. The proposed hash code indexing scheme
exploits a few binary bits of the hash code as the index code. We construct an
inverted index table based on index codes and train a neural network to improve
the indexing accuracy and efficiency. Experiments are performed on two
benchmark datasets for retrieval across image and text modalities, where hash
codes are generated by three cross-modal hashing methods. Results show the
proposed method effectively boost the performance on these hash methods.</p>
</td>
    <td>
      
        Datasets 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/wiggers2019image/">Image Retrieval And Pattern Spotting Using Siamese Neural Network</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Image Retrieval And Pattern Spotting Using Siamese Neural Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Image Retrieval And Pattern Spotting Using Siamese Neural Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wiggers et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 International Joint Conference on Neural Networks (IJCNN)</td>
    <td>37</td>
    <td><p>This paper presents a novel approach for image retrieval and pattern spotting
in document image collections. The manual feature engineering is avoided by
learning a similarity-based representation using a Siamese Neural Network
trained on a previously prepared subset of image pairs from the ImageNet
dataset. The learned representation is used to provide the similarity-based
feature maps used to find relevant image candidates in the data collection
given an image query. A robust experimental protocol based on the public
Tobacco800 document image collection shows that the proposed method compares
favorably against state-of-the-art document image retrieval methods, reaching
0.94 and 0.83 of mean average precision (mAP) for retrieval and pattern
spotting (IoU=0.7), respectively. Besides, we have evaluated the proposed
method considering feature maps of different sizes, showing the impact of
reducing the number of features in the retrieval performance and
time-consuming.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/keisler2019visual/">Visual Search Over Billions Of Aerial And Satellite Images</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Visual Search Over Billions Of Aerial And Satellite Images' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Visual Search Over Billions Of Aerial And Satellite Images' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Keisler et al.</td> <!-- 🔧 You were missing this -->
    <td>Computer Vision and Image Understanding</td>
    <td>21</td>
    <td><p>We present a system for performing visual search over billions of aerial and
satellite images. The purpose of visual search is to find images that are
visually similar to a query image. We define visual similarity using 512
abstract visual features generated by a convolutional neural network that has
been trained on aerial and satellite imagery. The features are converted to
binary values to reduce data and compute requirements. We employ a hash-based
search using Bigtable, a scalable database service from Google Cloud. Searching
the continental United States at 1-meter pixel resolution, corresponding to
approximately 2 billion images, takes approximately 0.1 seconds. This system
enables real-time visual search over the surface of the earth, and an
interactive demo is available at https://search.descarteslabs.com.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/yang2019gb/">GB-KMV: An Augmented KMV Sketch For Approximate Containment Similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=GB-KMV: An Augmented KMV Sketch For Approximate Containment Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=GB-KMV: An Augmented KMV Sketch For Approximate Containment Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yang et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE 35th International Conference on Data Engineering (ICDE)</td>
    <td>18</td>
    <td><p>In this paper, we study the problem of approximate containment similarity
search. Given two records Q and X, the containment similarity between Q and X
with respect to Q is |Q intersect X|/ |Q|. Given a query record Q and a set of
records S, the containment similarity search finds a set of records from S
whose containment similarity regarding Q are not less than the given threshold.
This problem has many important applications in commercial and scientific
fields such as record matching and domain search. Existing solution relies on
the asymmetric LSH method by transforming the containment similarity to
well-studied Jaccard similarity. In this paper, we use a different framework by
transforming the containment similarity to set intersection. We propose a novel
augmented KMV sketch technique, namely GB-KMV, which is data-dependent and can
achieve a good trade-off between the sketch size and the accuracy. We provide a
set of theoretical analysis to underpin the proposed augmented KMV sketch
technique, and show that it outperforms the state-of-the-art technique LSH-E in
terms of estimation accuracy under practical assumption. Our comprehensive
experiments on real-life datasets verify that GB-KMV is superior to LSH-E in
terms of the space-accuracy trade-off, time-accuracy trade-off, and the sketch
construction time. For instance, with similar estimation accuracy (F-1 score),
GB-KMV is over 100 times faster than LSH-E on some real-life dataset.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/talreja2019learning/">Learning To Authenticate With Deep Multibiometric Hashing And Neural Network Decoding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning To Authenticate With Deep Multibiometric Hashing And Neural Network Decoding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning To Authenticate With Deep Multibiometric Hashing And Neural Network Decoding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Talreja et al.</td> <!-- 🔧 You were missing this -->
    <td>ICC 2019 - 2019 IEEE International Conference on Communications (ICC)</td>
    <td>19</td>
    <td><p>In this paper, we propose a novel multimodal deep hashing neural decoder
(MDHND) architecture, which integrates a deep hashing framework with a neural
network decoder (NND) to create an effective multibiometric authentication
system. The MDHND consists of two separate modules: a multimodal deep hashing
(MDH) module, which is used for feature-level fusion and binarization of
multiple biometrics, and a neural network decoder (NND) module, which is used
to refine the intermediate binary codes generated by the MDH and compensate for
the difference between enrollment and probe biometrics (variations in pose,
illumination, etc.). Use of NND helps to improve the performance of the overall
multimodal authentication system. The MDHND framework is trained in 3 steps
using joint optimization of the two modules. In Step 1, the MDH parameters are
trained and learned to generate a shared multimodal latent code; in Step 2, the
latent codes from Step 1 are passed through a conventional error-correcting
code (ECC) decoder to generate the ground truth to train a neural network
decoder (NND); in Step 3, the NND decoder is trained using the ground truth
from Step 2 and the MDH and NND are jointly optimized. Experimental results on
a standard multimodal dataset demonstrate the superiority of our method
relative to other current multimodal authentication systems</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/kang2019maximum/">Maximum-margin Hamming Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Maximum-margin Hamming Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Maximum-margin Hamming Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kang et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>37</td>
    <td><p>Deep hashing enables computation and memory efficient
image search through end-to-end learning of feature representations and binary codes. While linear scan over binary
hash codes is more efficient than over the high-dimensional
representations, its linear-time complexity is still unacceptable for very large databases. Hamming space retrieval enables constant-time search through hash lookups, where for
each query, there is a Hamming ball centered at the query
and the data points within the ball are returned as relevant.
Since inside the Hamming ball implies retrievable while
outside irretrievable, it is crucial to explicitly characterize
the Hamming ball. The main idea of this work is to directly
embody the Hamming radius into the loss functions, leading
to Maximum-Margin Hamming Hashing (MMHH), a new
model specifically optimized for Hamming space retrieval.
We introduce a max-margin t-distribution loss, where the
t-distribution concentrates more similar data points to be
within the Hamming ball, and the margin characterizes the
Hamming radius such that less penalization is applied to
similar data points within the Hamming ball. The loss function also introduces robustness to data noise, where the similarity supervision may be inaccurate in practical problems.
The model is trained end-to-end using a new semi-batch optimization algorithm tailored to extremely imbalanced data.
Our method yields state-of-the-art results on four datasets
and shows superior performance on noisy data.</p>
</td>
    <td>
      
        Hashing Methods 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/kang2019candidate/">Candidate Generation With Binary Codes For Large-scale Top-n Recommendation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Candidate Generation With Binary Codes For Large-scale Top-n Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Candidate Generation With Binary Codes For Large-scale Top-n Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kang Wang-cheng, Mcauley Julian</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 28th ACM International Conference on Information and Knowledge Management</td>
    <td>53</td>
    <td><p>Generating the Top-N recommendations from a large corpus is computationally
expensive to perform at scale. Candidate generation and re-ranking based
approaches are often adopted in industrial settings to alleviate efficiency
problems. However it remains to be fully studied how well such schemes
approximate complete rankings (or how many candidates are required to achieve a
good approximation), or to develop systematic approaches to generate
high-quality candidates efficiently. In this paper, we seek to investigate
these questions via proposing a candidate generation and re-ranking based
framework (CIGAR), which first learns a preference-preserving binary embedding
for building a hash table to retrieve candidates, and then learns to re-rank
the candidates using real-valued ranking models with a candidate-oriented
objective. We perform a comprehensive study on several large-scale real-world
datasets consisting of millions of users/items and hundreds of millions of
interactions. Our results show that CIGAR significantly boosts the Top-N
accuracy against state-of-the-art recommendation models, while reducing the
query time by orders of magnitude. We hope that this work could draw more
attention to the candidate generation problem in recommender systems.</p>
</td>
    <td>
      
        Recommender Systems 
      
        CIKM 
      
        Compact Codes 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/zhang2019sadih/">SADIH: Semantic-aware Discrete Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=SADIH: Semantic-aware Discrete Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=SADIH: Semantic-aware Discrete Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>17</td>
    <td><p>Due to its low storage cost and fast query speed, hashing has been recognized
to accomplish similarity search in large-scale multimedia retrieval
applications. Particularly supervised hashing has recently received
considerable research attention by leveraging the label information to preserve
the pairwise similarities of data points in the Hamming space. However, there
still remain two crucial bottlenecks: 1) the learning process of the full
pairwise similarity preservation is computationally unaffordable and unscalable
to deal with big data; 2) the available category information of data are not
well-explored to learn discriminative hash functions. To overcome these
challenges, we propose a unified Semantic-Aware DIscrete Hashing (SADIH)
framework, which aims to directly embed the transformed semantic information
into the asymmetric similarity approximation and discriminative hashing
function learning. Specifically, a semantic-aware latent embedding is
introduced to asymmetrically preserve the full pairwise similarities while
skillfully handle the cumbersome n times n pairwise similarity matrix.
Meanwhile, a semantic-aware autoencoder is developed to jointly preserve the
data structures in the discriminative latent semantic space and perform data
reconstruction. Moreover, an efficient alternating optimization algorithm is
proposed to solve the resulting discrete optimization problem. Extensive
experimental results on multiple large-scale datasets demonstrate that our
SADIH can clearly outperform the state-of-the-art baselines with the additional
benefit of lower computational costs.</p>
</td>
    <td>
      
        Hashing Methods 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/marin2019recipe1m/">Recipe1m+: A Dataset For Learning Cross-modal Embeddings For Cooking Recipes And Food Images</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Recipe1m+: A Dataset For Learning Cross-modal Embeddings For Cooking Recipes And Food Images' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Recipe1m+: A Dataset For Learning Cross-modal Embeddings For Cooking Recipes And Food Images' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Marin et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>250</td>
    <td><p>In this paper, we introduce Recipe1M+, a new large-scale, structured corpus
of over one million cooking recipes and 13 million food images. As the largest
publicly available collection of recipe data, Recipe1M+ affords the ability to
train high-capacity modelson aligned, multimodal data. Using these data, we
train a neural network to learn a joint embedding of recipes and images that
yields impressive results on an image-recipe retrieval task. Moreover, we
demonstrate that regularization via the addition of a high-level classification
objective both improves retrieval performance to rival that of humans and
enables semantic vector arithmetic. We postulate that these embeddings will
provide a basis for further exploration of the Recipe1M+ dataset and food and
cooking in general. Code, data and models are publicly available.</p>
</td>
    <td>
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/karaman2019unsupervised/">Unsupervised Rank-preserving Hashing For Large-scale Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Rank-preserving Hashing For Large-scale Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Rank-preserving Hashing For Large-scale Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Karaman et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2019 on International Conference on Multimedia Retrieval</td>
    <td>14</td>
    <td><p>We propose an unsupervised hashing method which aims to produce binary codes
that preserve the ranking induced by a real-valued representation. Such compact
hash codes enable the complete elimination of real-valued feature storage and
allow for significant reduction of the computation complexity and storage cost
of large-scale image retrieval applications. Specifically, we learn a neural
network-based model, which transforms the input representation into a binary
representation. We formalize the training objective of the network in an
intuitive and effective way, considering each training sample as a query and
aiming to obtain the same retrieval results using the produced hash codes as
those obtained with the original features. This training formulation directly
optimizes the hashing model for the target usage of the hash codes it produces.
We further explore the addition of a decoder trained to obtain an approximated
reconstruction of the original features. At test time, we retrieved the most
promising database samples with an efficient graph-based search procedure using
only our hash codes and perform re-ranking using the reconstructed features,
thus without needing to access the original features at all. Experiments
conducted on multiple publicly available large-scale datasets show that our
method consistently outperforms all compared state-of-the-art unsupervised
hashing methods and that the reconstruction procedure can effectively boost the
search accuracy with a minimal constant additional cost.</p>
</td>
    <td>
      
        Image Retrieval 
      
        Unsupervised 
      
        Medical Retrieval 
      
        SUPERVISED 
      
        Multimodal Retrieval 
      
        Hashing Methods 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/j%C3%A4%C3%A4saari2019efficient/">Efficient Autotuning Of Hyperparameters In Approximate Nearest Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Autotuning Of Hyperparameters In Approximate Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient Autotuning Of Hyperparameters In Approximate Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jääsaari Elias, Hyvönen Ville, Roos Teemu</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>11</td>
    <td><p>Approximate nearest neighbor algorithms are used to speed up nearest neighbor
search in a wide array of applications. However, current indexing methods
feature several hyperparameters that need to be tuned to reach an acceptable
accuracy–speed trade-off. A grid search in the parameter space is often
impractically slow due to a time-consuming index-building procedure. Therefore,
we propose an algorithm for automatically tuning the hyperparameters of
indexing methods based on randomized space-partitioning trees. In particular,
we present results using randomized k-d trees, random projection trees and
randomized PCA trees. The tuning algorithm adds minimal overhead to the
index-building process but is able to find the optimal hyperparameters
accurately. We demonstrate that the algorithm is significantly faster than
existing approaches, and that the indexing methods used are competitive with
the state-of-the-art methods in query time while being faster to build.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/magliani2019efficient/">An Efficient Approximate Knn Graph Method For Diffusion On Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=An Efficient Approximate Knn Graph Method For Diffusion On Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=An Efficient Approximate Knn Graph Method For Diffusion On Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Magliani et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>8</td>
    <td><p>The application of the diffusion in many computer vision and artificial
intelligence projects has been shown to give excellent improvements in
performance. One of the main bottlenecks of this technique is the quadratic
growth of the kNN graph size due to the high-quantity of new connections
between nodes in the graph, resulting in long computation times. Several
strategies have been proposed to address this, but none are effective and
efficient. Our novel technique, based on LSH projections, obtains the same
performance as the exact kNN graph after diffusion, but in less time
(approximately 18 times faster on a dataset of a hundred thousand images). The
proposed method was validated and compared with other state-of-the-art on
several public image datasets, including Oxford5k, Paris6k, and Oxford105k.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/juan2019graph/">Graph-rise: Graph-regularized Image Semantic Embedding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Graph-rise: Graph-regularized Image Semantic Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Graph-rise: Graph-regularized Image Semantic Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Juan et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>33</td>
    <td><p>Learning image representations to capture fine-grained semantics has been a
challenging and important task enabling many applications such as image search
and clustering. In this paper, we present Graph-Regularized Image Semantic
Embedding (Graph-RISE), a large-scale neural graph learning framework that
allows us to train embeddings to discriminate an unprecedented O(40M)
ultra-fine-grained semantic labels. Graph-RISE outperforms state-of-the-art
image embedding algorithms on several evaluation tasks, including image
classification and triplet ranking. We provide case studies to demonstrate
that, qualitatively, image retrieval based on Graph-RISE effectively captures
semantics and, compared to the state-of-the-art, differentiates nuances at
levels that are closer to human-perception.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/jun2019combination/">Combination Of Multiple Global Descriptors For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Combination Of Multiple Global Descriptors For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Combination Of Multiple Global Descriptors For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jun et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>41</td>
    <td><p>Recent studies in image retrieval task have shown that ensembling different
models and combining multiple global descriptors lead to performance
improvement. However, training different models for the ensemble is not only
difficult but also inefficient with respect to time and memory. In this paper,
we propose a novel framework that exploits multiple global descriptors to get
an ensemble effect while it can be trained in an end-to-end manner. The
proposed framework is flexible and expandable by the global descriptor, CNN
backbone, loss, and dataset. Moreover, we investigate the effectiveness of
combining multiple global descriptors with quantitative and qualitative
analysis. Our extensive experiments show that the combined descriptor
outperforms a single global descriptor, as it can utilize different types of
feature properties. In the benchmark evaluation, the proposed framework
achieves the state-of-the-art performance on the CARS196, CUB200-2011, In-shop
Clothes, and Stanford Online Products on image retrieval tasks. Our model
implementations and pretrained models are publicly available.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/johnson2019billion/">Billion-scale Similarity Search With Gpus</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Billion-scale Similarity Search With Gpus' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Billion-scale Similarity Search With Gpus' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Johnson Jeff, Douze Matthijs, Jégou Hervé</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Big Data</td>
    <td>2024</td>
    <td><p>Similarity search finds application in specialized database systems handling
complex data such as images or videos, which are typically represented by
high-dimensional features and require specific indexing structures. This paper
tackles the problem of better utilizing GPUs for this task. While GPUs excel at
data-parallel tasks, prior approaches are bottlenecked by algorithms that
expose less parallelism, such as k-min selection, or make poor use of the
memory hierarchy.
  We propose a design for k-selection that operates at up to 55% of theoretical
peak performance, enabling a nearest neighbor implementation that is 8.5x
faster than prior GPU state of the art. We apply it in different similarity
search scenarios, by proposing optimized design for brute-force, approximate
and compressed-domain search based on product quantization. In all these
setups, we outperform the state of the art by large margins. Our implementation
enables the construction of a high accuracy k-NN graph on 95 million images
from the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion
vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced
our approach for the sake of comparison and reproducibility.</p>
</td>
    <td>
      
        Similarity Search 
      
        Large Scale Search 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/yang2019distillhash/">Distillhash: Unsupervised Deep Hashing By Distilling Data Pairs</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Distillhash: Unsupervised Deep Hashing By Distilling Data Pairs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Distillhash: Unsupervised Deep Hashing By Distilling Data Pairs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yang et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>149</td>
    <td><p>Due to the high storage and search efficiency, hashing has become prevalent
for large-scale similarity search. Particularly, deep hashing methods have
greatly improved the search performance under supervised scenarios. In
contrast, unsupervised deep hashing models can hardly achieve satisfactory
performance due to the lack of reliable supervisory similarity signals. To
address this issue, we propose a novel deep unsupervised hashing model, dubbed
DistillHash, which can learn a distilled data set consisted of data pairs,
which have confidence similarity signals. Specifically, we investigate the
relationship between the initial noisy similarity signals learned from local
structures and the semantic similarity labels assigned by a Bayes optimal
classifier. We show that under a mild assumption, some data pairs, of which
labels are consistent with those assigned by the Bayes optimal classifier, can
be potentially distilled. Inspired by this fact, we design a simple yet
effective strategy to distill data pairs automatically and further adopt a
Bayesian learning framework to learn hash functions from the distilled data
set. Extensive experimental results on three widely used benchmark datasets
show that the proposed DistillHash consistently accomplishes the
state-of-the-art search performance.</p>
</td>
    <td>
      
        Unsupervised 
      
        CVPR 
      
        Neural Hashing 
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/revaud2019learning/">Learning With Average Precision: Training Image Retrieval With A Listwise Loss</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning With Average Precision: Training Image Retrieval With A Listwise Loss' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning With Average Precision: Training Image Retrieval With A Listwise Loss' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Revaud et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>348</td>
    <td><p>Image retrieval can be formulated as a ranking problem where the goal is to
order database images by decreasing similarity to the query. Recent deep models
for image retrieval have outperformed traditional methods by leveraging
ranking-tailored loss functions, but important theoretical and practical
problems remain. First, rather than directly optimizing the global ranking,
they minimize an upper-bound on the essential loss, which does not necessarily
result in an optimal mean average precision (mAP). Second, these methods
require significant engineering efforts to work well, e.g. special pre-training
and hard-negative mining. In this paper we propose instead to directly optimize
the global mAP by leveraging recent advances in listwise loss formulations.
Using a histogram binning approximation, the AP can be differentiated and thus
employed to end-to-end learning. Compared to existing losses, the proposed
method considers thousands of images simultaneously at each iteration and
eliminates the need for ad hoc tricks. It also establishes a new state of the
art on many standard retrieval benchmarks. Models and evaluation scripts have
been made available at https://europe.naverlabs.com/Deep-Image-Retrieval/</p>
</td>
    <td>
      
        Evaluation 
      
        Image Retrieval 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/jin2019unsupervised/">Unsupervised Semantic Deep Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Semantic Deep Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Semantic Deep Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jin Sheng</td> <!-- 🔧 You were missing this -->
    <td>Neurocomputing</td>
    <td>25</td>
    <td><p>In recent years, deep hashing methods have been proved to be efficient since
it employs convolutional neural network to learn features and hashing codes
simultaneously. However, these methods are mostly supervised. In real-world
application, it is a time-consuming and overloaded task for annotating a large
number of images. In this paper, we propose a novel unsupervised deep hashing
method for large-scale image retrieval. Our method, namely unsupervised
semantic deep hashing (\textbf{USDH}), uses semantic information preserved in
the CNN feature layer to guide the training of network. We enforce four
criteria on hashing codes learning based on VGG-19 model: 1) preserving
relevant information of feature space in hashing space; 2) minimizing
quantization loss between binary-like codes and hashing codes; 3) improving the
usage of each bit in hashing codes by using maximum information entropy, and 4)
invariant to image rotation. Extensive experiments on CIFAR-10, NUSWIDE have
demonstrated that \textbf{USDH} outperforms several state-of-the-art
unsupervised hashing methods for image retrieval. We also conduct experiments
on Oxford 17 datasets for fine-grained classification to verify its efficiency
for other computer vision tasks.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        SUPERVISED 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/jiang2019evaluation/">On The Evaluation Metric For Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=On The Evaluation Metric For Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=On The Evaluation Metric For Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jiang Qing-yuan, Li Ming-wei, Li Wu-jun</td> <!-- 🔧 You were missing this -->
    <td>Medical Image Analysis</td>
    <td>15</td>
    <td><p>Due to its low storage cost and fast query speed, hashing has been widely
used for large-scale approximate nearest neighbor (ANN) search. Bucket search,
also called hash lookup, can achieve fast query speed with a sub-linear time
cost based on the inverted index table constructed from hash codes. Many
metrics have been adopted to evaluate hashing algorithms. However, all existing
metrics are improper to evaluate the hash codes for bucket search. On one hand,
all existing metrics ignore the retrieval time cost which is an important
factor reflecting the performance of search. On the other hand, some of them,
such as mean average precision (MAP), suffer from the uncertainty problem as
the ranked list is based on integer-valued Hamming distance, and are
insensitive to Hamming radius as these metrics only depend on relative Hamming
distance. Other metrics, such as precision at Hamming radius R, fail to
evaluate global performance as these metrics only depend on one specific
Hamming radius. In this paper, we first point out the problems of existing
metrics which have been ignored by the hashing community, and then propose a
novel evaluation metric called radius aware mean average precision (RAMAP) to
evaluate hash codes for bucket search. Furthermore, two coding strategies are
also proposed to qualitatively show the problems of existing metrics.
Experiments demonstrate that our proposed RAMAP can provide more proper
evaluation than existing metrics.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/jiang2019discrete/">Discrete Latent Factor Model For Cross-modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Discrete Latent Factor Model For Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Discrete Latent Factor Model For Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jiang Qing-yuan, Li Wu-jun</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>143</td>
    <td><p>Due to its storage and retrieval efficiency, cross-modal hashing~(CMH) has
been widely used for cross-modal similarity search in multimedia applications.
According to the training strategy, existing CMH methods can be mainly divided
into two categories: relaxation-based continuous methods and discrete methods.
In general, the training of relaxation-based continuous methods is faster than
discrete methods, but the accuracy of relaxation-based continuous methods is
not satisfactory. On the contrary, the accuracy of discrete methods is
typically better than relaxation-based continuous methods, but the training of
discrete methods is time-consuming. In this paper, we propose a novel CMH
method, called discrete latent factor model based cross-modal hashing~(DLFH),
for cross modal similarity search. DLFH is a discrete method which can directly
learn the binary hash codes for CMH. At the same time, the training of DLFH is
efficient. Experiments on real datasets show that DLFH can achieve
significantly better accuracy than existing methods, and the training time of
DLFH is comparable to that of relaxation-based continuous methods which are
much faster than existing discrete methods.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/zhang2019generic/">Generic Intent Representation In Web Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Generic Intent Representation In Web Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Generic Intent Representation In Web Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>33</td>
    <td><p>This paper presents GEneric iNtent Encoder (GEN Encoder) which learns a
distributed representation space for user intent in search. Leveraging large
scale user clicks from Bing search logs as weak supervision of user intent, GEN
Encoder learns to map queries with shared clicks into similar embeddings
end-to-end and then finetunes on multiple paraphrase tasks. Experimental
results on an intrinsic evaluation task - query intent similarity modeling -
demonstrate GEN Encoder’s robust and significant advantages over previous
representation methods. Ablation studies reveal the crucial role of learning
from implicit user feedback in representing user intent and the contributions
of multi-task learning in representation generality. We also demonstrate that
GEN Encoder alleviates the sparsity of tail search traffic and cuts down half
of the unseen queries by using an efficient approximate nearest neighbor search
to effectively identify previous queries with the same search intent. Finally,
we demonstrate distances between GEN encodings reflect certain information
seeking behaviors in search sessions.</p>
</td>
    <td>
      
        SIGIR 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/massarelli2019safe/">SAFE: Self-attentive Function Embeddings For Binary Similarity</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=SAFE: Self-attentive Function Embeddings For Binary Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=SAFE: Self-attentive Function Embeddings For Binary Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Massarelli et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>155</td>
    <td><p>The binary similarity problem consists in determining if two functions are
similar by only considering their compiled form. Advanced techniques for binary
similarity recently gained momentum as they can be applied in several fields,
such as copyright disputes, malware analysis, vulnerability detection, etc.,
and thus have an immediate practical impact. Current solutions compare
functions by first transforming their binary code in multi-dimensional vector
representations (embeddings), and then comparing vectors through simple and
efficient geometric operations. However, embeddings are usually derived from
binary code using manual feature extraction, that may fail in considering
important function characteristics, or may consider features that are not
important for the binary similarity problem. In this paper we propose SAFE, a
novel architecture for the embedding of functions based on a self-attentive
neural network. SAFE works directly on disassembled binary functions, does not
require manual feature extraction, is computationally more efficient than
existing solutions (i.e., it does not incur in the computational overhead of
building or manipulating control flow graphs), and is more general as it works
on stripped binaries and on multiple architectures. We report the results from
a quantitative and qualitative analysis that show how SAFE provides a
noticeable performance improvement with respect to previous solutions.
Furthermore, we show how clusters of our embedding vectors are closely related
to the semantic of the implemented algorithms, paving the way for further
interesting applications (e.g. semantic-based binary function search).</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/tissier2019near/">Near-lossless Binarization Of Word Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Near-lossless Binarization Of Word Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Near-lossless Binarization Of Word Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tissier Julien, Gravier Christophe, Habrard Amaury</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>43</td>
    <td><p>Word embeddings are commonly used as a starting point in many NLP models to
achieve state-of-the-art performances. However, with a large vocabulary and
many dimensions, these floating-point representations are expensive both in
terms of memory and calculations which makes them unsuitable for use on
low-resource devices. The method proposed in this paper transforms real-valued
embeddings into binary embeddings while preserving semantic information,
requiring only 128 or 256 bits for each vector. This leads to a small memory
footprint and fast vector operations. The model is based on an autoencoder
architecture, which also allows to reconstruct original vectors from the binary
ones. Experimental results on semantic similarity, text classification and
sentiment analysis tasks show that the binarization of word embeddings only
leads to a loss of ~2% in accuracy while vector size is reduced by 97%.
Furthermore, a top-k benchmark demonstrates that using these binary vectors is
30 times faster than using real-valued vectors.</p>
</td>
    <td>
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/liu2019adversarial/">Adversarial Binary Coding For Efficient Person Re-identification</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Adversarial Binary Coding For Efficient Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Adversarial Binary Coding For Efficient Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE International Conference on Multimedia and Expo (ICME)</td>
    <td>36</td>
    <td><p>Person re-identification (ReID) aims at matching persons across different
views/scenes. In addition to accuracy, the matching efficiency has received
more and more attention because of demanding applications using large-scale
data. Several binary coding based methods have been proposed for efficient
ReID, which either learn projections to map high-dimensional features to
compact binary codes, or directly adopt deep neural networks by simply
inserting an additional fully-connected layer with tanh-like activations.
However, the former approach requires time-consuming hand-crafted feature
extraction and complicated (discrete) optimizations; the latter lacks the
necessary discriminative information greatly due to the straightforward
activation functions. In this paper, we propose a simple yet effective
framework for efficient ReID inspired by the recent advances in adversarial
learning. Specifically, instead of learning explicit projections or adding
fully-connected mapping layers, the proposed Adversarial Binary Coding (ABC)
framework guides the extraction of binary codes implicitly and effectively. The
discriminability of the extracted codes is further enhanced by equipping the
ABC with a deep triplet network for the ReID task. More importantly, the ABC
and triplet network are simultaneously optimized in an end-to-end manner.
Extensive experiments on three large-scale ReID benchmarks demonstrate the
superiority of our approach over the state-of-the-art methods.</p>
</td>
    <td>
      
        Robustness 
      
        Compact Codes 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/liu2019compositional/">Compositional Coding For Collaborative Filtering</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Compositional Coding For Collaborative Filtering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Compositional Coding For Collaborative Filtering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>15</td>
    <td><p>Efficiency is crucial to the online recommender systems. Representing users
and items as binary vectors for Collaborative Filtering (CF) can achieve fast
user-item affinity computation in the Hamming space, in recent years, we have
witnessed an emerging research effort in exploiting binary hashing techniques
for CF methods. However, CF with binary codes naturally suffers from low
accuracy due to limited representation capability in each bit, which impedes it
from modeling complex structure of the data.
  In this work, we attempt to improve the efficiency without hurting the model
performance by utilizing both the accuracy of real-valued vectors and the
efficiency of binary codes to represent users/items. In particular, we propose
the Compositional Coding for Collaborative Filtering (CCCF) framework, which
not only gains better recommendation efficiency than the state-of-the-art
binarized CF approaches but also achieves even higher accuracy than the
real-valued CF method. Specifically, CCCF innovatively represents each
user/item with a set of binary vectors, which are associated with a sparse
real-value weight vector. Each value of the weight vector encodes the
importance of the corresponding binary vector to the user/item. The continuous
weight vectors greatly enhances the representation capability of binary codes,
and its sparsity guarantees the processing speed. Furthermore, an integer
weight approximation scheme is proposed to further accelerate the speed. Based
on the CCCF framework, we design an efficient discrete optimization algorithm
to learn its parameters. Extensive experiments on three real-world datasets
show that our method outperforms the state-of-the-art binarized CF methods
(even achieves better performance than the real-valued CF method) by a large
margin in terms of both recommendation accuracy and efficiency.</p>
</td>
    <td>
      
        SIGIR 
      
        Text Retrieval 
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/liu2019stochastic/">Stochastic Attraction-repulsion Embedding For Large Scale Image Localization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Stochastic Attraction-repulsion Embedding For Large Scale Image Localization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Stochastic Attraction-repulsion Embedding For Large Scale Image Localization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Liu, Li Hongdong, Dai Yuchao</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>96</td>
    <td><p>This paper tackles the problem of large-scale image-based localization (IBL)
where the spatial location of a query image is determined by finding out the
most similar reference images in a large database. For solving this problem, a
critical task is to learn discriminative image representation that captures
informative information relevant for localization. We propose a novel
representation learning method having higher location-discriminating power. It
provides the following contributions: 1) we represent a place (location) as a
set of exemplar images depicting the same landmarks and aim to maximize
similarities among intra-place images while minimizing similarities among
inter-place images; 2) we model a similarity measure as a probability
distribution on L_2-metric distances between intra-place and inter-place image
representations; 3) we propose a new Stochastic Attraction and Repulsion
Embedding (SARE) loss function minimizing the KL divergence between the learned
and the actual probability distributions; 4) we give theoretical comparisons
between SARE, triplet ranking and contrastive losses. It provides insights into
why SARE is better by analyzing gradients. Our SARE loss is easy to implement
and pluggable to any CNN. Experiments show that our proposed method improves
the localization performance on standard benchmarks by a large margin.
Demonstrating the broad applicability of our method, we obtained the third
place out of 209 teams in the 2018 Google Landmark Retrieval Challenge. Our
code and model are available at https://github.com/Liumouliu/deepIBL.</p>
</td>
    <td>
      
        SCALABILITY 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/liu2019strong/">A Strong And Robust Baseline For Text-image Matching</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Strong And Robust Baseline For Text-image Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Strong And Robust Baseline For Text-image Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Fangyu, Ye Rongtian</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</td>
    <td>13</td>
    <td><p>We review the current schemes of text-image matching models and propose
improvements for both training and inference. First, we empirically show
limitations of two popular loss (sum and max-margin loss) widely used in
training text-image embeddings and propose a trade-off: a kNN-margin loss which
1) utilizes information from hard negatives and 2) is robust to noise as all
\(K\)-most hardest samples are taken into account, tolerating <em>pseudo</em>
negatives and outliers. Second, we advocate the use of Inverted Softmax
(\textsc{Is}) and Cross-modal Local Scaling (\textsc{Csls}) during inference to
mitigate the so-called hubness problem in high-dimensional embedding space,
enhancing scores of all metrics by a large margin.</p>
</td>
    <td>
      
        ACL 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/liu2019ternary/">Ternary Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Ternary Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Ternary Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Knowledge and Data Engineering</td>
    <td>35</td>
    <td><p>This paper proposes a novel ternary hash encoding for learning to hash
methods, which provides a principled more efficient coding scheme with
performances better than those of the state-of-the-art binary hashing
counterparts. Two kinds of axiomatic ternary logic, Kleene logic and
{\L}ukasiewicz logic are adopted to calculate the Ternary Hamming Distance
(THD) for both the learning/encoding and testing/querying phases. Our work
demonstrates that, with an efficient implementation of ternary logic on
standard binary machines, the proposed ternary hashing is compared favorably to
the binary hashing methods with consistent improvements of retrieval mean
average precision (mAP) ranging from 1% to 5.9% as shown in CIFAR10, NUS-WIDE
and ImageNet100 datasets.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/liu2019optimal/">Optimal Projection Guided Transfer Hashing For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Optimal Projection Guided Transfer Hashing For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Optimal Projection Guided Transfer Hashing For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Ji, Zhang Lei</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Circuits and Systems for Video Technology</td>
    <td>31</td>
    <td><p>Recently, learning to hash has been widely studied for image retrieval thanks
to the computation and storage efficiency of binary codes. For most existing
learning to hash methods, sufficient training images are required and used to
learn precise hashing codes. However, in some real-world applications, there
are not always sufficient training images in the domain of interest. In
addition, some existing supervised approaches need a amount of labeled data,
which is an expensive process in term of time, label and human expertise. To
handle such problems, inspired by transfer learning, we propose a simple yet
effective unsupervised hashing method named Optimal Projection Guided Transfer
Hashing (GTH) where we borrow the images of other different but related domain
i.e., source domain to help learn precise hashing codes for the domain of
interest i.e., target domain. Besides, we propose to seek for the maximum
likelihood estimation (MLE) solution of the hashing functions of target and
source domains due to the domain gap. Furthermore,an alternating optimization
method is adopted to obtain the two projections of target and source domains
such that the domain hashing disparity is reduced gradually. Extensive
experiments on various benchmark databases verify that our method outperforms
many state-of-the-art learning to hash methods. The implementation details are
available at https://github.com/liuji93/GTH.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/liu2019mtfh/">MTFH: A Matrix Tri-factorization Hashing Framework For Efficient Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=MTFH: A Matrix Tri-factorization Hashing Framework For Efficient Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=MTFH: A Matrix Tri-factorization Hashing Framework For Efficient Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>171</td>
    <td><p>Hashing has recently sparked a great revolution in cross-modal retrieval
because of its low storage cost and high query speed. Recent cross-modal
hashing methods often learn unified or equal-length hash codes to represent the
multi-modal data and make them intuitively comparable. However, such unified or
equal-length hash representations could inherently sacrifice their
representation scalability because the data from different modalities may not
have one-to-one correspondence and could be encoded more efficiently by
different hash codes of unequal lengths. To mitigate these problems, this paper
exploits a related and relatively unexplored problem: encode the heterogeneous
data with varying hash lengths and generalize the cross-modal retrieval in
various challenging scenarios. To this end, a generalized and flexible
cross-modal hashing framework, termed Matrix Tri-Factorization Hashing (MTFH),
is proposed to work seamlessly in various settings including paired or unpaired
multi-modal data, and equal or varying hash length encoding scenarios. More
specifically, MTFH exploits an efficient objective function to flexibly learn
the modality-specific hash codes with different length settings, while
synchronously learning two semantic correlation matrices to semantically
correlate the different hash representations for heterogeneous data comparable.
As a result, the derived hash codes are more semantically meaningful for
various challenging cross-modal retrieval tasks. Extensive experiments
evaluated on public benchmark datasets highlight the superiority of MTFH under
various retrieval scenarios and show its competitive performance with the
state-of-the-arts.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Multimodal Retrieval 
      
        Tools & Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/liu2019general/">The General Pair-based Weighting Loss For Deep Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=The General Pair-based Weighting Loss For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=The General Pair-based Weighting Loss For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>550</td>
    <td><p>Deep metric learning aims at learning the distance metric between pair of
samples, through the deep neural networks to extract the semantic feature
embeddings where similar samples are close to each other while dissimilar
samples are farther apart. A large amount of loss functions based on pair
distances have been presented in the literature for guiding the training of
deep metric learning. In this paper, we unify them in a general pair-based
weighting loss function, where the minimizing objective loss is just the
distances weighting of informative pairs. The general pair-based weighting loss
includes two main aspects, (1) samples mining and (2) pairs weighting. Samples
mining aims at selecting the informative positive and negative pair sets to
exploit the structured relationship of samples in a mini-batch and also reduce
the number of non-trivial pairs. Pair weighting aims at assigning different
weights for different pairs according to the pair distances for
discriminatively training the network. We detailedly review those existing
pair-based losses inline with our general loss function, and explore some
possible methods from the perspective of samples mining and pairs weighting.
Finally, extensive experiments on three image retrieval datasets show that our
general pair-based weighting loss obtains new state-of-the-art performance,
demonstrating the effectiveness of the pair-based samples mining and pairs
weighting for deep metric learning.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/liu2019mutual/">Mutual Linear Regression-based Discrete Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Mutual Linear Regression-based Discrete Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Mutual Linear Regression-based Discrete Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Xingbo, Nie Xiushan, Yin Yilong</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 27th ACM International Conference on Multimedia</td>
    <td>17</td>
    <td><p>Label information is widely used in hashing methods because of its
effectiveness of improving the precision. The existing hashing methods always
use two different projections to represent the mutual regression between hash
codes and class labels. In contrast to the existing methods, we propose a novel
learning-based hashing method termed stable supervised discrete hashing with
mutual linear regression (S2DHMLR) in this study, where only one stable
projection is used to describe the linear correlation between hash codes and
corresponding labels. To the best of our knowledge, this strategy has not been
used for hashing previously. In addition, we further use a boosting strategy to
improve the final performance of the proposed method without adding extra
constraints and with little extra expenditure in terms of time and space.
Extensive experiments conducted on three image benchmarks demonstrate the
superior performance of the proposed method.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/liu2019ranking/">Ranking-based Deep Cross-modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Ranking-based Deep Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Ranking-based Deep Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>59</td>
    <td><p>Cross-modal hashing has been receiving increasing interests for its low
storage cost and fast query speed in multi-modal data retrievals. However, most
existing hashing methods are based on hand-crafted or raw level features of
objects, which may not be optimally compatible with the coding process.
Besides, these hashing methods are mainly designed to handle simple pairwise
similarity. The complex multilevel ranking semantic structure of instances
associated with multiple labels has not been well explored yet. In this paper,
we propose a ranking-based deep cross-modal hashing approach (RDCMH). RDCMH
firstly uses the feature and label information of data to derive a
semi-supervised semantic ranking list. Next, to expand the semantic
representation power of hand-crafted features, RDCMH integrates the semantic
ranking information into deep cross-modal hashing and jointly optimizes the
compatible parameters of deep feature representations and of hashing functions.
Experiments on real multi-modal datasets show that RDCMH outperforms other
competitive baselines and achieves the state-of-the-art performance in
cross-modal retrieval applications.</p>
</td>
    <td>
      
        Hashing Methods 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/portaz2019image/">Image Search Using Multilingual Texts: A Cross-modal Learning Approach Between Image And Text</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Image Search Using Multilingual Texts: A Cross-modal Learning Approach Between Image And Text' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Image Search Using Multilingual Texts: A Cross-modal Learning Approach Between Image And Text' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Portaz et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>10</td>
    <td><p>Multilingual (or cross-lingual) embeddings represent several languages in a
unique vector space. Using a common embedding space enables for a shared
semantic between words from different languages. In this paper, we propose to
embed images and texts into a unique distributional vector space, enabling to
search images by using text queries expressing information needs related to the
(visual) content of images, as well as using image similarity. Our framework
forces the representation of an image to be similar to the representation of
the text that describes it. Moreover, by using multilingual embeddings we
ensure that words from two different languages have close descriptors and thus
are attached to similar images. We provide experimental evidence of the
efficiency of our approach by experimenting it on two datasets: Common Objects
in COntext (COCO) [19] and Multi30K [7].</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/liu2019weakly/">Weakly-paired Cross-modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Weakly-paired Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Weakly-paired Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Big Data</td>
    <td>17</td>
    <td><p>Hashing has been widely adopted for large-scale data retrieval in many
domains, due to its low storage cost and high retrieval speed. Existing
cross-modal hashing methods optimistically assume that the correspondence
between training samples across modalities are readily available. This
assumption is unrealistic in practical applications. In addition, these methods
generally require the same number of samples across different modalities, which
restricts their flexibility. We propose a flexible cross-modal hashing approach
(Flex-CMH) to learn effective hashing codes from weakly-paired data, whose
correspondence across modalities are partially (or even totally) unknown.
FlexCMH first introduces a clustering-based matching strategy to explore the
local structure of each cluster, and thus to find the potential correspondence
between clusters (and samples therein) across modalities. To reduce the impact
of an incomplete correspondence, it jointly optimizes in a unified objective
function the potential correspondence, the cross-modal hashing functions
derived from the correspondence, and a hashing quantitative loss. An
alternative optimization technique is also proposed to coordinate the
correspondence and hash functions, and to reinforce the reciprocal effects of
the two objectives. Experiments on publicly multi-modal datasets show that
FlexCMH achieves significantly better results than state-of-the-art methods,
and it indeed offers a high degree of flexibility for practical cross-modal
hashing tasks.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/liu2019furnishing/">Furnishing Your Room By What You See: An End-to-end Furniture Set Retrieval Framework With Rich Annotated Benchmark Dataset</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Furnishing Your Room By What You See: An End-to-end Furniture Set Retrieval Framework With Rich Annotated Benchmark Dataset' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Furnishing Your Room By What You See: An End-to-end Furniture Set Retrieval Framework With Rich Annotated Benchmark Dataset' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>6</td>
    <td><p>Understanding interior scenes has attracted enormous interest in computer
vision community. However, few works focus on the understanding of furniture
within the scenes and a large-scale dataset is also lacked to advance the
field. In this paper, we first fill the gap by presenting DeepFurniture, a
richly annotated large indoor scene dataset, including 24k indoor images, 170k
furniture instances and 20k unique furniture identities. On the dataset, we
introduce a new benchmark, named furniture set retrieval. Given an indoor photo
as input, the task requires to detect all the furniture instances and search a
matched set of furniture identities. To address this challenging task, we
propose a feature and context embedding based framework. It contains 3 major
contributions: (1) An improved Mask-RCNN model with an additional mask-based
classifier is introduced for better utilizing the mask information to relieve
the occlusion problems in furniture detection context. (2) A multi-task style
Siamese network is proposed to train the feature embedding model for retrieval,
which is composed of a classification subnet supervised by self-clustered
pseudo attributes and a verification subnet to estimate whether the input pair
is matched. (3) In order to model the relationship of the furniture entities in
an interior design, a context embedding model is employed to re-rank the
retrieval results. Extensive experiments demonstrate the effectiveness of each
module and the overall system.</p>
</td>
    <td>
      
        Datasets 
      
        Tools & Libraries 
      
        Evaluation 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/shi2018fast/">Fast Locality Sensitive Hashing For Beam Search On GPU</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Locality Sensitive Hashing For Beam Search On GPU' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Locality Sensitive Hashing For Beam Search On GPU' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shi Xing, Xu Shizhen, Knight Kevin</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>9</td>
    <td><p>We present a GPU-based Locality Sensitive Hashing (LSH) algorithm to speed up
beam search for sequence models. We utilize the winner-take-all (WTA) hash,
which is based on relative ranking order of hidden dimensions and thus
resilient to perturbations in numerical values. Our algorithm is designed by
fully considering the underling architecture of CUDA-enabled GPUs
(Algorithm/Architecture Co-design): 1) A parallel Cuckoo hash table is applied
for LSH code lookup (guaranteed O(1) lookup time); 2) Candidate lists are
shared across beams to maximize the parallelism; 3) Top frequent words are
merged into candidate lists to improve performance. Experiments on 4
large-scale neural machine translation models demonstrate that our algorithm
can achieve up to 4x speedup on softmax module, and 2x overall speedup without
hurting BLEU on GPU.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/mu2018towards/">Towards Practical Visual Search Engine Within Elasticsearch</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Towards Practical Visual Search Engine Within Elasticsearch' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Towards Practical Visual Search Engine Within Elasticsearch' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mu et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>11</td>
    <td><p>In this paper, we describe our end-to-end content-based image retrieval
system built upon Elasticsearch, a well-known and popular textual search
engine. As far as we know, this is the first time such a system has been
implemented in eCommerce, and our efforts have turned out to be highly
worthwhile. We end up with a novel and exciting visual search solution that is
extremely easy to be deployed, distributed, scaled and monitored in a
cost-friendly manner. Moreover, our platform is intrinsically flexible in
supporting multimodal searches, where visual and textual information can be
jointly leveraged in retrieval.
  The core idea is to encode image feature vectors into a collection of string
tokens in a way such that closer vectors will share more string tokens in
common. By doing that, we can utilize Elasticsearch to efficiently retrieve
similar images based on similarities within encoded sting tokens. As part of
the development, we propose a novel vector to string encoding method, which is
shown to substantially outperform the previous ones in terms of both precision
and latency.
  First-hand experiences in implementing this Elasticsearch-based platform are
extensively addressed, which should be valuable to practitioners also
interested in building visual search engine on top of Elasticsearch.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/xu2018sketchmate/">Sketchmate: Deep Hashing For Million-scale Human Sketch Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Sketchmate: Deep Hashing For Million-scale Human Sketch Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Sketchmate: Deep Hashing For Million-scale Human Sketch Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xu et al.</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</td>
    <td>127</td>
    <td><p>We propose a deep hashing framework for sketch retrieval that, for the first
time, works on a multi-million scale human sketch dataset. Leveraging on this
large dataset, we explore a few sketch-specific traits that were otherwise
under-studied in prior literature. Instead of following the conventional sketch
recognition task, we introduce the novel problem of sketch hashing retrieval
which is not only more challenging, but also offers a better testbed for
large-scale sketch analysis, since: (i) more fine-grained sketch feature
learning is required to accommodate the large variations in style and
abstraction, and (ii) a compact binary code needs to be learned at the same
time to enable efficient retrieval. Key to our network design is the embedding
of unique characteristics of human sketch, where (i) a two-branch CNN-RNN
architecture is adapted to explore the temporal ordering of strokes, and (ii) a
novel hashing loss is specifically designed to accommodate both the temporal
and abstract traits of sketches. By working with a 3.8M sketch dataset, we show
that state-of-the-art hashing models specifically engineered for static images
fail to perform well on temporal sketch data. Our network on the other hand not
only offers the best retrieval performance on various code sizes, but also
yields the best generalization performance under a zero-shot setting and when
re-purposed for sketch recognition. Such superior performances effectively
demonstrate the benefit of our sketch-specific design.</p>
</td>
    <td>
      
        CVPR 
      
        Neural Hashing 
      
        Large Scale Search 
      
        Hashing Methods 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/zhao2018approximate/">Approximate K-nn Graph Construction: A Generic Online Approach</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Approximate K-nn Graph Construction: A Generic Online Approach' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Approximate K-nn Graph Construction: A Generic Online Approach' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhao Wan-lei, Wang Hui, Ngo Chong-wah</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>20</td>
    <td><p>Nearest neighbor search and k-nearest neighbor graph construction are two
fundamental issues arise from many disciplines such as multimedia information
retrieval, data-mining and machine learning. They become more and more imminent
given the big data emerge in various fields in recent years. In this paper, a
simple but effective solution both for approximate k-nearest neighbor search
and approximate k-nearest neighbor graph construction is presented. These two
issues are addressed jointly in our solution. On the one hand, the approximate
k-nearest neighbor graph construction is treated as a search task. Each sample
along with its k-nearest neighbors are joined into the k-nearest neighbor graph
by performing the nearest neighbor search sequentially on the graph under
construction. On the other hand, the built k-nearest neighbor graph is used to
support k-nearest neighbor search. Since the graph is built online, the dynamic
update on the graph, which is not possible from most of the existing solutions,
is supported. This solution is feasible for various distance measures. Its
effectiveness both as k-nearest neighbor construction and k-nearest neighbor
search approaches is verified across different types of data in different
scales, various dimensions and under different metrics.</p>
</td>
    <td>
      
        Graph Based ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/nawaz2018revisiting/">Revisiting Cross Modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Revisiting Cross Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Revisiting Cross Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Nawaz et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>This paper proposes a cross-modal retrieval system that leverages on image
and text encoding. Most multimodal architectures employ separate networks for
each modality to capture the semantic relationship between them. However, in
our work image-text encoding can achieve comparable results in terms of
cross-modal retrieval without having to use a separate network for each
modality. We show that text encodings can capture semantic relationships
between multiple modalities. In our knowledge, this work is the first of its
kind in terms of employing a single network and fused image-text embedding for
cross-modal retrieval. We evaluate our approach on two famous multimodal
datasets: MS-COCO and Flickr30K.</p>
</td>
    <td>
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/narasimhan2018straight/">Straight To The Facts: Learning Knowledge Base Retrieval For Factual Visual Question Answering</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Straight To The Facts: Learning Knowledge Base Retrieval For Factual Visual Question Answering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Straight To The Facts: Learning Knowledge Base Retrieval For Factual Visual Question Answering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Narasimhan Medhini, Schwing Alexander G.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>100</td>
    <td><p>Question answering is an important task for autonomous agents and virtual
assistants alike and was shown to support the disabled in efficiently
navigating an overwhelming environment. Many existing methods focus on
observation-based questions, ignoring our ability to seamlessly combine
observed content with general knowledge. To understand interactions with a
knowledge base, a dataset has been introduced recently and keyword matching
techniques were shown to yield compelling results despite being vulnerable to
misconceptions due to synonyms and homographs. To address this issue, we
develop a learning-based approach which goes straight to the facts via a
learned embedding space. We demonstrate state-of-the-art results on the
challenging recently introduced fact-based visual question answering dataset,
outperforming competing methods by more than 5%.</p>
</td>
    <td>
      
        Graph Based ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/shi2018face/">Face Clustering: Representation And Pairwise Constraints</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Face Clustering: Representation And Pairwise Constraints' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Face Clustering: Representation And Pairwise Constraints' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shi Yichun, Otto Charles, Jain Anil K.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Information Forensics and Security</td>
    <td>107</td>
    <td><p>Clustering face images according to their identity has two important
applications: (i) grouping a collection of face images when no external labels
are associated with images, and (ii) indexing for efficient large scale face
retrieval. The clustering problem is composed of two key parts: face
representation and choice of similarity for grouping faces. We first propose a
representation based on ResNet, which has been shown to perform very well in
image classification problems. Given this representation, we design a
clustering algorithm, Conditional Pairwise Clustering (ConPaC), which directly
estimates the adjacency matrix only based on the similarity between face
images. This allows a dynamic selection of number of clusters and retains
pairwise similarity between faces. ConPaC formulates the clustering problem as
a Conditional Random Field (CRF) model and uses Loopy Belief Propagation to
find an approximate solution for maximizing the posterior probability of the
adjacency matrix. Experimental results on two benchmark face datasets (LFW and
IJB-B) show that ConPaC outperforms well known clustering algorithms such as
k-means, spectral clustering and approximate rank-order. Additionally, our
algorithm can naturally incorporate pairwise constraints to obtain a
semi-supervised version that leads to improved clustering performance. We also
propose an k-NN variant of ConPaC, which has a linear time complexity given a
k-NN graph, suitable for large datasets.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/engilberge2018finding/">Finding Beans In Burgers: Deep Semantic-visual Embedding With Localization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Finding Beans In Burgers: Deep Semantic-visual Embedding With Localization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Finding Beans In Burgers: Deep Semantic-visual Embedding With Localization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Engilberge et al.</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</td>
    <td>86</td>
    <td><p>Several works have proposed to learn a two-path neural network that maps
images and texts, respectively, to a same shared Euclidean space where geometry
captures useful semantic relationships. Such a multi-modal embedding can be
trained and used for various tasks, notably image captioning. In the present
work, we introduce a new architecture of this type, with a visual path that
leverages recent space-aware pooling mechanisms. Combined with a textual path
which is jointly trained from scratch, our semantic-visual embedding offers a
versatile model. Once trained under the supervision of captioned images, it
yields new state-of-the-art performance on cross-modal retrieval. It also
allows the localization of new concepts from the embedding space into any input
image, delivering state-of-the-art result on the visual grounding of phrases.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/zieba2018bingan/">Bingan: Learning Compact Binary Descriptors With A Regularized GAN</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Bingan: Learning Compact Binary Descriptors With A Regularized GAN' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Bingan: Learning Compact Binary Descriptors With A Regularized GAN' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zieba et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>36</td>
    <td><p>In this paper, we propose a novel regularization method for Generative
Adversarial Networks, which allows the model to learn discriminative yet
compact binary representations of image patches (image descriptors). We employ
the dimensionality reduction that takes place in the intermediate layers of the
discriminator network and train binarized low-dimensional representation of the
penultimate layer to mimic the distribution of the higher-dimensional preceding
layers. To achieve this, we introduce two loss terms that aim at: (i) reducing
the correlation between the dimensions of the binarized low-dimensional
representation of the penultimate layer i. e. maximizing joint entropy) and
(ii) propagating the relations between the dimensions in the high-dimensional
space to the low-dimensional space. We evaluate the resulting binary image
descriptors on two challenging applications, image matching and retrieval, and
achieve state-of-the-art results.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/ertl2018bagminhash/">Bagminhash - Minwise Hashing Algorithm For Weighted Sets</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Bagminhash - Minwise Hashing Algorithm For Weighted Sets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Bagminhash - Minwise Hashing Algorithm For Weighted Sets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ertl Otmar</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</td>
    <td>15</td>
    <td><p>Minwise hashing has become a standard tool to calculate signatures which
allow direct estimation of Jaccard similarities. While very efficient
algorithms already exist for the unweighted case, the calculation of signatures
for weighted sets is still a time consuming task. BagMinHash is a new algorithm
that can be orders of magnitude faster than current state of the art without
any particular restrictions or assumptions on weights or data dimensionality.
Applied to the special case of unweighted sets, it represents the first
efficient algorithm producing independent signature components. A series of
tests finally verifies the new algorithm and also reveals limitations of other
approaches published in the recent past.</p>
</td>
    <td>
      
        KDD 
      
        Hashing Methods 
      
        Locality Sensitive Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/eghbali2018fast/">Fast Cosine Similarity Search In Binary Space With Angular Multi-index Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Cosine Similarity Search In Binary Space With Angular Multi-index Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Cosine Similarity Search In Binary Space With Angular Multi-index Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Eghbali Sepehr, Tahvildari Ladan</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Knowledge and Data Engineering</td>
    <td>15</td>
    <td><p>Given a large dataset of binary codes and a binary query point, we address
how to efficiently find \(K\) codes in the dataset that yield the largest cosine
similarities to the query. The straightforward answer to this problem is to
compare the query with all items in the dataset, but this is practical only for
small datasets. One potential solution to enhance the search time and achieve
sublinear cost is to use a hash table populated with binary codes of the
dataset and then look up the nearby buckets to the query to retrieve the
nearest neighbors. However, if codes are compared in terms of cosine similarity
rather than the Hamming distance, then the main issue is that the order of
buckets to probe is not evident. To examine this issue, we first elaborate on
the connection between the Hamming distance and the cosine similarity. Doing
this allows us to systematically find the probing sequence in the hash table.
However, solving the nearest neighbor search with a single table is only
practical for short binary codes. To address this issue, we propose the angular
multi-index hashing search algorithm which relies on building multiple hash
tables on binary code substrings. The proposed search algorithm solves the
exact angular \(K\) nearest neighbor problem in a time that is often orders of
magnitude faster than the linear scan baseline and even approximation methods.</p>
</td>
    <td>
      
        Vector Indexing 
      
        Hashing Methods 
      
        Distance Metric Learning 
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/kilias2018idel/">IDEL: In-database Entity Linking With Neural Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=IDEL: In-database Entity Linking With Neural Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=IDEL: In-database Entity Linking With Neural Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kilias et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>8</td>
    <td><p>We present a novel architecture, In-Database Entity Linking (IDEL), in which
we integrate the analytics-optimized RDBMS MonetDB with neural text mining
abilities. Our system design abstracts core tasks of most neural entity linking
systems for MonetDB. To the best of our knowledge, this is the first defacto
implemented system integrating entity-linking in a database. We leverage the
ability of MonetDB to support in-database-analytics with user defined functions
(UDFs) implemented in Python. These functions call machine learning libraries
for neural text mining, such as TensorFlow. The system achieves zero cost for
data shipping and transformation by utilizing MonetDB’s ability to embed Python
processes in the database kernel and exchange data in NumPy arrays. IDEL
represents text and relational data in a joint vector space with neural
embeddings and can compensate errors with ambiguous entity representations. For
detecting matching entities, we propose a novel similarity function based on
joint neural embeddings which are learned via minimizing pairwise contrastive
ranking loss. This function utilizes a high dimensional index structures for
fast retrieval of matching entities. Our first implementation and experiments
using the WebNLG corpus show the effectiveness and the potentials of IDEL.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/xu2018iterative/">Iterative Manifold Embedding Layer Learned By Incomplete Data For Large-scale Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Iterative Manifold Embedding Layer Learned By Incomplete Data For Large-scale Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Iterative Manifold Embedding Layer Learned By Incomplete Data For Large-scale Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xu et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>21</td>
    <td><p>Existing manifold learning methods are not appropriate for image retrieval
task, because most of them are unable to process query image and they have much
additional computational cost especially for large scale database. Therefore,
we propose the iterative manifold embedding (IME) layer, of which the weights
are learned off-line by unsupervised strategy, to explore the intrinsic
manifolds by incomplete data. On the large scale database that contains 27000
images, IME layer is more than 120 times faster than other manifold learning
methods to embed the original representations at query time. We embed the
original descriptors of database images which lie on manifold in a high
dimensional space into manifold-based representations iteratively to generate
the IME representations in off-line learning stage. According to the original
descriptors and the IME representations of database images, we estimate the
weights of IME layer by ridge regression. In on-line retrieval stage, we employ
the IME layer to map the original representation of query image with ignorable
time cost (2 milliseconds). We experiment on five public standard datasets for
image retrieval. The proposed IME layer significantly outperforms related
dimension reduction methods and manifold learning methods. Without
post-processing, Our IME layer achieves a boost in performance of
state-of-the-art image retrieval methods with post-processing on most datasets,
and needs less computational cost.</p>
</td>
    <td>
      
        Image Retrieval 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/zhang2018zoom/">Zoom: Ssd-based Vector Search For Optimizing Accuracy, Latency And Memory</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Zoom: Ssd-based Vector Search For Optimizing Accuracy, Latency And Memory' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Zoom: Ssd-based Vector Search For Optimizing Accuracy, Latency And Memory' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Minjia, He Yuxiong</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Circuits and Systems for Video Technology</td>
    <td>33</td>
    <td><p>With the advancement of machine learning and deep learning, vector search
becomes instrumental to many information retrieval systems, to search and find
best matches to user queries based on their semantic similarities.These online
services require the search architecture to be both effective with high
accuracy and efficient with low latency and memory footprint, which existing
work fails to offer. We develop, Zoom, a new vector search solution that
collaboratively optimizes accuracy, latency and memory based on a multiview
approach. (1) A “preview” step generates a small set of good candidates,
leveraging compressed vectors in memory for reduced footprint and fast lookup.
(2) A “fullview” step on SSDs reranks those candidates with their full-length
vector, striking high accuracy. Our evaluation shows that, Zoom achieves an
order of magnitude improvements on efficiency while attaining equal or higher
accuracy, comparing with the state-of-the-art.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/douze2018link/">Link And Code: Fast Indexing With Graphs And Compact Regression Codes</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Link And Code: Fast Indexing With Graphs And Compact Regression Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Link And Code: Fast Indexing With Graphs And Compact Regression Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Douze Matthijs, Sablayrolles Alexandre, Jégou Hervé</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</td>
    <td>31</td>
    <td><p>Similarity search approaches based on graph walks have recently attained
outstanding speed-accuracy trade-offs, taking aside the memory requirements. In
this paper, we revisit these approaches by considering, additionally, the
memory constraint required to index billions of images on a single server. This
leads us to propose a method based both on graph traversal and compact
representations. We encode the indexed vectors using quantization and exploit
the graph structure to refine the similarity estimation.
  In essence, our method takes the best of these two worlds: the search
strategy is based on nested graphs, thereby providing high precision with a
relatively small set of comparisons. At the same time it offers a significant
memory compression. As a result, our approach outperforms the state of the art
on operating points considering 64-128 bytes per vector, as demonstrated by our
results on two billion-scale public benchmarks.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/dorfer2018end/">End-to-end Cross-modality Retrieval With CCA Projections And Pairwise Ranking Loss</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=End-to-end Cross-modality Retrieval With CCA Projections And Pairwise Ranking Loss' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=End-to-end Cross-modality Retrieval With CCA Projections And Pairwise Ranking Loss' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dorfer et al.</td> <!-- 🔧 You were missing this -->
    <td>International Journal of Multimedia Information Retrieval</td>
    <td>32</td>
    <td><p>Cross-modality retrieval encompasses retrieval tasks where the fetched items
are of a different type than the search query, e.g., retrieving pictures
relevant to a given text query. The state-of-the-art approach to cross-modality
retrieval relies on learning a joint embedding space of the two modalities,
where items from either modality are retrieved using nearest-neighbor search.
In this work, we introduce a neural network layer based on Canonical
Correlation Analysis (CCA) that learns better embedding spaces by analytically
computing projections that maximize correlation. In contrast to previous
approaches, the CCA Layer (CCAL) allows us to combine existing objectives for
embedding space learning, such as pairwise ranking losses, with the optimal
projections of CCA. We show the effectiveness of our approach for
cross-modality retrieval on three different scenarios (text-to-image,
audio-sheet-music and zero-shot retrieval), surpassing both Deep CCA and a
multi-view network using freely learned projections optimized by a pairwise
ranking loss, especially when little training data is available (the code for
all three methods is released at: https://github.com/CPJKU/cca_layer).</p>
</td>
    <td>
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/wu2018cycle/">Cycle-consistent Deep Generative Hashing For Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cycle-consistent Deep Generative Hashing For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cycle-consistent Deep Generative Hashing For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu Lin, Wang Yang, Shao Ling</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>189</td>
    <td><p>In this paper, we propose a novel deep generative approach to cross-modal
retrieval to learn hash functions in the absence of paired training samples
through the cycle consistency loss. Our proposed approach employs adversarial
training scheme to lean a couple of hash functions enabling translation between
modalities while assuming the underlying semantic relationship. To induce the
hash codes with semantics to the input-output pair, cycle consistency loss is
further proposed upon the adversarial training to strengthen the correlations
between inputs and corresponding outputs. Our approach is generative to learn
hash functions such that the learned hash codes can maximally correlate each
input-output correspondence, meanwhile can also regenerate the inputs so as to
minimize the information loss. The learning to hash embedding is thus performed
to jointly optimize the parameters of the hash functions across modalities as
well as the associated generative models. Extensive experiments on a variety of
large-scale cross-modal data sets demonstrate that our proposed method achieves
better retrieval results than the state-of-the-arts.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/quinn2018semantic/">Semantic Image Retrieval Via Active Grounding Of Visual Situations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semantic Image Retrieval Via Active Grounding Of Visual Situations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semantic Image Retrieval Via Active Grounding Of Visual Situations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Quinn et al.</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE 12th International Conference on Semantic Computing (ICSC)</td>
    <td>12</td>
    <td><p>We describe a novel architecture for semantic image retrieval—in
particular, retrieval of instances of visual situations. Visual situations are
concepts such as “a boxing match,” “walking the dog,” “a crowd waiting for a
bus,” or “a game of ping-pong,” whose instantiations in images are linked more
by their common spatial and semantic structure than by low-level visual
similarity. Given a query situation description, our architecture—called
Situate—learns models capturing the visual features of expected objects as
well the expected spatial configuration of relationships among objects. Given a
new image, Situate uses these models in an attempt to ground (i.e., to create a
bounding box locating) each expected component of the situation in the image
via an active search procedure. Situate uses the resulting grounding to compute
a score indicating the degree to which the new image is judged to contain an
instance of the situation. Such scores can be used to rank images in a
collection as part of a retrieval system. In the preliminary study described
here, we demonstrate the promise of this system by comparing Situate’s
performance with that of two baseline methods, as well as with a related
semantic image-retrieval system based on “scene graphs.”</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/kim2018attention/">Attention-based Ensemble For Deep Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Attention-based Ensemble For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Attention-based Ensemble For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kim et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>237</td>
    <td><p>Deep metric learning aims to learn an embedding function, modeled as deep
neural network. This embedding function usually puts semantically similar
images close while dissimilar images far from each other in the learned
embedding space. Recently, ensemble has been applied to deep metric learning to
yield state-of-the-art results. As one important aspect of ensemble, the
learners should be diverse in their feature embeddings. To this end, we propose
an attention-based ensemble, which uses multiple attention masks, so that each
learner can attend to different parts of the object. We also propose a
divergence loss, which encourages diversity among the learners. The proposed
method is applied to the standard benchmarks of deep metric learning and
experimental results show that it outperforms the state-of-the-art methods by a
significant margin on image retrieval tasks.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/dong2018cross/">Cross-media Similarity Evaluation For Web Image Retrieval In The Wild</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-media Similarity Evaluation For Web Image Retrieval In The Wild' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-media Similarity Evaluation For Web Image Retrieval In The Wild' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dong Jianfeng, Li Xirong, Xu Duanqing</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>26</td>
    <td><p>In order to retrieve unlabeled images by textual queries, cross-media
similarity computation is a key ingredient. Although novel methods are
continuously introduced, little has been done to evaluate these methods
together with large-scale query log analysis. Consequently, how far have these
methods brought us in answering real-user queries is unclear. Given baseline
methods that compute cross-media similarity using relatively simple text/image
matching, how much progress have advanced models made is also unclear. This
paper takes a pragmatic approach to answering the two questions. Queries are
automatically categorized according to the proposed query visualness measure,
and later connected to the evaluation of multiple cross-media similarity models
on three test sets. Such a connection reveals that the success of the
state-of-the-art is mainly attributed to their good performance on
visual-oriented queries, while these queries account for only a small part of
real-user queries. To quantify the current progress, we propose a simple
text2image method, representing a novel test query by a set of images selected
from large-scale query log. Consequently, computing cross-media similarity
between the test query and a given image boils down to comparing the visual
similarity between the given image and the selected images. Image retrieval
experiments on the challenging Clickture dataset show that the proposed
text2image compares favorably to recent deep learning based alternatives.</p>
</td>
    <td>
      
        Evaluation 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/dong2018video/">Video Retrieval Based On Deep Convolutional Neural Network</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Video Retrieval Based On Deep Convolutional Neural Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Video Retrieval Based On Deep Convolutional Neural Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dong Yj, Li Jg</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 3rd International Conference on Multimedia Systems and Signal Processing</td>
    <td>17</td>
    <td><p>Recently, with the enormous growth of online videos, fast video retrieval
research has received increasing attention. As an extension of image hashing
techniques, traditional video hashing methods mainly depend on hand-crafted
features and transform the real-valued features into binary hash codes. As
videos provide far more diverse and complex visual information than images,
extracting features from videos is much more challenging than that from images.
Therefore, high-level semantic features to represent videos are needed rather
than low-level hand-crafted methods. In this paper, a deep convolutional neural
network is proposed to extract high-level semantic features and a binary hash
function is then integrated into this framework to achieve an end-to-end
optimization. Particularly, our approach also combines triplet loss function
which preserves the relative similarity and difference of videos and
classification loss function as the optimization objective. Experiments have
been performed on two public datasets and the results demonstrate the
superiority of our proposed method compared with other state-of-the-art video
retrieval methods.</p>
</td>
    <td>
      
        Video Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/wu2018local/">Local Density Estimation In High Dimensions</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Local Density Estimation In High Dimensions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Local Density Estimation In High Dimensions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu Xian, Charikar Moses, Natchu Vishnu</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>7</td>
    <td><p>An important question that arises in the study of high dimensional vector
representations learned from data is: given a set \(\mathcal{D}\) of vectors and
a query \(q\), estimate the number of points within a specified distance
threshold of \(q\). We develop two estimators, LSH Count and Multi-Probe Count
that use locality sensitive hashing to preprocess the data to accurately and
efficiently estimate the answers to such questions via importance sampling. A
key innovation is the ability to maintain a small number of hash tables via
preprocessing data structures and algorithms that sample from multiple buckets
in each hash table. We give bounds on the space requirements and sample
complexity of our schemes, and demonstrate their effectiveness in experiments
on a standard word embedding dataset.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/ma2018progressive/">Progressive Generative Hashing For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Progressive Generative Hashing For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Progressive Generative Hashing For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ma et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence</td>
    <td>18</td>
    <td><p>Recent years have witnessed the success of the emerging hashing techniques in large-scale image
retrieval. Owing to the great learning capacity,
deep hashing has become one of the most promising solutions, and achieved attractive performance
in practice. However, without semantic label information, the unsupervised deep hashing still remains
an open question. In this paper, we propose a novel
progressive generative hashing (PGH) framework
to help learn a discriminative hashing network in an
unsupervised way. Different from existing studies,
it first treats the hash codes as a kind of semantic
condition for the similar image generation, and simultaneously feeds the original image and its codes
into the generative adversarial networks (GANs).
The real images together with the synthetic ones
can further help train a discriminative hashing network based on a triplet loss. By iteratively inputting
the learnt codes into the hash conditioned GANs, we can progressively enable the hashing network
to discover the semantic relations. Extensive experiments on the widely-used image datasets demonstrate that PGH can significantly outperform stateof-the-art unsupervised hashing methods.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
        AAAI 
      
        IJCAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/mccauley2018adaptive/">Adaptive Mapreduce Similarity Joins</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Adaptive Mapreduce Similarity Joins' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Adaptive Mapreduce Similarity Joins' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mccauley Samuel, Silvestri Francesco</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 5th ACM SIGMOD Workshop on Algorithms and Systems for MapReduce and Beyond</td>
    <td>6</td>
    <td><p>Similarity joins are a fundamental database operation. Given data sets S and
R, the goal of a similarity join is to find all points x in S and y in R with
distance at most r. Recent research has investigated how locality-sensitive
hashing (LSH) can be used for similarity join, and in particular two recent
lines of work have made exciting progress on LSH-based join performance. Hu,
Tao, and Yi (PODS 17) investigated joins in a massively parallel setting,
showing strong results that adapt to the size of the output. Meanwhile, Ahle,
Aum"uller, and Pagh (SODA 17) showed a sequential algorithm that adapts to the
structure of the data, matching classic bounds in the worst case but improving
them significantly on more structured data. We show that this adaptive strategy
can be adapted to the parallel setting, combining the advantages of these
approaches. In particular, we show that a simple modification to Hu et al.’s
algorithm achieves bounds that depend on the density of points in the dataset
as well as the total outsize of the output. Our algorithm uses no extra
parameters over other LSH approaches (in particular, its execution does not
depend on the structure of the dataset), and is likely to be efficient in
practice.</p>
</td>
    <td>
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/magliani2018efficient/">Efficient Nearest Neighbors Search For Large-scale Landmark Recognition</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Nearest Neighbors Search For Large-scale Landmark Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient Nearest Neighbors Search For Large-scale Landmark Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Magliani Federico, Fontanini Tomaso, Prati Andrea</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>8</td>
    <td><p>The problem of landmark recognition has achieved excellent results in
small-scale datasets. When dealing with large-scale retrieval, issues that were
irrelevant with small amount of data, quickly become fundamental for an
efficient retrieval phase. In particular, computational time needs to be kept
as low as possible, whilst the retrieval accuracy has to be preserved as much
as possible. In this paper we propose a novel multi-index hashing method called
Bag of Indexes (BoI) for Approximate Nearest Neighbors (ANN) search. It allows
to drastically reduce the query time and outperforms the accuracy results
compared to the state-of-the-art methods for large-scale landmark recognition.
It has been demonstrated that this family of algorithms can be applied on
different embedding techniques like VLAD and R-MAC obtaining excellent results
in very short times on different public datasets: Holidays+Flickr1M, Oxford105k
and Paris106k.</p>
</td>
    <td>
      
        Similarity Search 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/dirksen2018fast/">Fast Binary Embeddings With Gaussian Circulant Matrices: Improved Bounds</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Binary Embeddings With Gaussian Circulant Matrices: Improved Bounds' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Binary Embeddings With Gaussian Circulant Matrices: Improved Bounds' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dirksen Sjoerd, Stollenwerk Alexander</td> <!-- 🔧 You were missing this -->
    <td>Discrete &amp; Computational Geometry</td>
    <td>13</td>
    <td><p>We consider the problem of encoding a finite set of vectors into a small
number of bits while approximately retaining information on the angular
distances between the vectors. By deriving improved variance bounds related to
binary Gaussian circulant embeddings, we largely fix a gap in the proof of the
best known fast binary embedding method. Our bounds also show that
well-spreadness assumptions on the data vectors, which were needed in earlier
work on variance bounds, are unnecessary. In addition, we propose a new binary
embedding with a faster running time on sparse data.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/ding2018mean/">Mean Local Group Average Precision (mlgap): A New Performance Metric For Hashing-based Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Mean Local Group Average Precision (mlgap): A New Performance Metric For Hashing-based Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Mean Local Group Average Precision (mlgap): A New Performance Metric For Hashing-based Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ding Pak Lun Kevin, Li Yikang, Li Baoxin</td> <!-- 🔧 You were missing this -->
    <td>Encyclopedia of Database Systems</td>
    <td>8</td>
    <td><p>The research on hashing techniques for visual data is gaining increased
attention in recent years due to the need for compact representations
supporting efficient search/retrieval in large-scale databases such as online
images. Among many possibilities, Mean Average Precision(mAP) has emerged as
the dominant performance metric for hashing-based retrieval. One glaring
shortcoming of mAP is its inability in balancing retrieval accuracy and
utilization of hash codes: pushing a system to attain higher mAP will
inevitably lead to poorer utilization of the hash codes. Poor utilization of
the hash codes hinders good retrieval because of increased collision of samples
in the hash space. This means that a model giving a higher mAP values does not
necessarily do a better job in retrieval. In this paper, we introduce a new
metric named Mean Local Group Average Precision (mLGAP) for better evaluation
of the performance of hashing-based retrieval. The new metric provides a
retrieval performance measure that also reconciles the utilization of hash
codes, leading to a more practically meaningful performance metric than
conventional ones like mAP. To this end, we start by mathematical analysis of
the deficiencies of mAP for hashing-based retrieval. We then propose mLGAP and
show why it is more appropriate for hashing-based retrieval. Experiments on
image retrieval are used to demonstrate the effectiveness of the proposed
metric.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/dey2018learning/">Learning Cross-modal Deep Embeddings For Multi-object Image Retrieval Using Text And Sketch</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Cross-modal Deep Embeddings For Multi-object Image Retrieval Using Text And Sketch' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Cross-modal Deep Embeddings For Multi-object Image Retrieval Using Text And Sketch' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dey et al.</td> <!-- 🔧 You were missing this -->
    <td>2018 24th International Conference on Pattern Recognition (ICPR)</td>
    <td>24</td>
    <td><p>In this work we introduce a cross modal image retrieval system that allows
both text and sketch as input modalities for the query. A cross-modal deep
network architecture is formulated to jointly model the sketch and text input
modalities as well as the the image output modality, learning a common
embedding between text and images and between sketches and images. In addition,
an attention model is used to selectively focus the attention on the different
objects of the image, allowing for retrieval with multiple objects in the
query. Experiments show that the proposed method performs the best in both
single and multiple object image retrieval in standard datasets.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/deng2018triplet/">Triplet-based Deep Hashing Network For Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Triplet-based Deep Hashing Network For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Triplet-based Deep Hashing Network For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Deng et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>376</td>
    <td><p>Given the benefits of its low storage requirements and high retrieval
efficiency, hashing has recently received increasing attention. In
particular,cross-modal hashing has been widely and successfully used in
multimedia similarity search applications. However, almost all existing methods
employing cross-modal hashing cannot obtain powerful hash codes due to their
ignoring the relative similarity between heterogeneous data that contains
richer semantic information, leading to unsatisfactory retrieval performance.
In this paper, we propose a triplet-based deep hashing (TDH) network for
cross-modal retrieval. First, we utilize the triplet labels, which describes
the relative relationships among three instances as supervision in order to
capture more general semantic correlations between cross-modal instances. We
then establish a loss function from the inter-modal view and the intra-modal
view to boost the discriminative abilities of the hash codes. Finally, graph
regularization is introduced into our proposed TDH method to preserve the
original semantic similarity between hash codes in Hamming space. Experimental
results show that our proposed method outperforms several state-of-the-art
approaches on two popular cross-modal datasets.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/song2018binary/">Binary Generative Adversarial Networks For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Binary Generative Adversarial Networks For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Binary Generative Adversarial Networks For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Song Jingkuan</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>164</td>
    <td><p>The most striking successes in image retrieval using deep hashing have mostly
involved discriminative models, which require labels. In this paper, we use
binary generative adversarial networks (BGAN) to embed images to binary codes
in an unsupervised way. By restricting the input noise variable of generative
adversarial networks (GAN) to be binary and conditioned on the features of each
input image, BGAN can simultaneously learn a binary representation per image,
and generate an image plausibly similar to the original one. In the proposed
framework, we address two main problems: 1) how to directly generate binary
codes without relaxation? 2) how to equip the binary representation with the
ability of accurate image retrieval? We resolve these problems by proposing new
sign-activation strategy and a loss function steering the learning process,
which consists of new models for adversarial loss, a content loss, and a
neighborhood structure loss. Experimental results on standard datasets
(CIFAR-10, NUSWIDE, and Flickr) demonstrate that our BGAN significantly
outperforms existing hashing methods by up to 107% in terms of~mAP (See Table
tab.res.map.comp) Our anonymous code is available at:
https://github.com/htconquer/BGAN.</p>
</td>
    <td>
      
        Robustness 
      
        Image Retrieval 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/liong2018deep/">Deep Variational And Structural Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Variational And Structural Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Variational And Structural Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liong et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>56</td>
    <td><p>In this paper, we propose a deep variational and structural hashing (DVStH) method to learn compact binary codes for multimedia retrieval. Unlike most existing deep hashing methods which use a series of convolution and fully-connected layers to learn binary features, we develop a probabilistic framework to infer latent feature representation inside the network. Then, we design a struct layer rather than a bottleneck hash layer, to obtain binary codes through a simple encoding procedure. By doing these, we are able to obtain binary codes discriminatively and generatively. To make it applicable to cross-modal scalable multimedia retrieval, we extend our method to a cross-modal deep variational and structural hashing (CM-DVStH). We design a deep fusion network with a struct layer to maximize the correlation between image-text input pairs during the training stage so that a unified binary vector can be obtained. We then design modality-specific hashing networks to handle the out-of-sample extension scenario. Specifically, we train a network for each modality which outputs a latent representation that is as close as possible to the binary codes which are inferred from the fusion network. Experimental results on five benchmark datasets are presented to show the efficacy of the proposed approach.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/jiang2018asymmetric/">Asymmetric Deep Supervised Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Asymmetric Deep Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Asymmetric Deep Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jiang Qing-yuan, Li Wu-jun</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>248</td>
    <td><p>Hashing has been widely used for large-scale approximate nearest neighbor
search because of its storage and search efficiency. Recent work has found that
deep supervised hashing can significantly outperform non-deep supervised
hashing in many applications. However, most existing deep supervised hashing
methods adopt a symmetric strategy to learn one deep hash function for both
query points and database (retrieval) points. The training of these symmetric
deep supervised hashing methods is typically time-consuming, which makes them
hard to effectively utilize the supervised information for cases with
large-scale database. In this paper, we propose a novel deep supervised hashing
method, called asymmetric deep supervised hashing (ADSH), for large-scale
nearest neighbor search. ADSH treats the query points and database points in an
asymmetric way. More specifically, ADSH learns a deep hash function only for
query points, while the hash codes for database points are directly learned.
The training of ADSH is much more efficient than that of traditional symmetric
deep supervised hashing methods. Experiments show that ADSH can achieve
state-of-the-art performance in real applications.</p>
</td>
    <td>
      
        Unsupervised 
      
        Neural Hashing 
      
        AAAI 
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/curt%C3%B32018segmentation/">Segmentation Of Objects By Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Segmentation Of Objects By Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Segmentation Of Objects By Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Curtó et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>29</td>
    <td><p>We propose a novel approach to address the problem of Simultaneous Detection
and Segmentation introduced in [Hariharan et al 2014]. Using the hierarchical
structures first presented in [Arbel'aez et al 2011] we use an efficient and
accurate procedure that exploits the feature information of the hierarchy using
Locality Sensitive Hashing. We build on recent work that utilizes convolutional
neural networks to detect bounding boxes in an image [Ren et al 2015] and then
use the top similar hierarchical region that best fits each bounding box after
hashing, we call this approach C&amp;Z Segmentation. We then refine our final
segmentation results by automatic hierarchical pruning. C&amp;Z Segmentation
introduces a train-free alternative to Hypercolumns [Hariharan et al 2015]. We
conduct extensive experiments on PASCAL VOC 2012 segmentation dataset, showing
that C&amp;Z gives competitive state-of-the-art segmentations of objects.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/song2018self/">Self-supervised Video Hashing With Hierarchical Binary Auto-encoder</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Self-supervised Video Hashing With Hierarchical Binary Auto-encoder' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Self-supervised Video Hashing With Hierarchical Binary Auto-encoder' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Song et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>165</td>
    <td><p>Existing video hash functions are built on three isolated stages: frame
pooling, relaxed learning, and binarization, which have not adequately explored
the temporal order of video frames in a joint binary optimization model,
resulting in severe information loss. In this paper, we propose a novel
unsupervised video hashing framework dubbed Self-Supervised Video Hashing
(SSVH), that is able to capture the temporal nature of videos in an end-to-end
learning-to-hash fashion. We specifically address two central problems: 1) how
to design an encoder-decoder architecture to generate binary codes for videos;
and 2) how to equip the binary codes with the ability of accurate video
retrieval. We design a hierarchical binary autoencoder to model the temporal
dependencies in videos with multiple granularities, and embed the videos into
binary codes with less computations than the stacked architecture. Then, we
encourage the binary codes to simultaneously reconstruct the visual content and
neighborhood structure of the videos. Experiments on two real-world datasets
(FCVID and YFCC) show that our SSVH method can significantly outperform the
state-of-the-art methods and achieve the currently best performance on the task
of unsupervised video retrieval.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
        Self SUPERVISED 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/lai2018transductive/">Transductive Zero-shot Hashing Via Coarse-to-fine Similarity Mining</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Transductive Zero-shot Hashing Via Coarse-to-fine Similarity Mining' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Transductive Zero-shot Hashing Via Coarse-to-fine Similarity Mining' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lai Hanjiang, Pan Yan</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval</td>
    <td>9</td>
    <td><p>Zero-shot Hashing (ZSH) is to learn hashing models for novel/target classes
without training data, which is an important and challenging problem. Most
existing ZSH approaches exploit transfer learning via an intermediate shared
semantic representations between the seen/source classes and novel/target
classes. However, due to having disjoint, the hash functions learned from the
source dataset are biased when applied directly to the target classes. In this
paper, we study the transductive ZSH, i.e., we have unlabeled data for novel
classes. We put forward a simple yet efficient joint learning approach via
coarse-to-fine similarity mining which transfers knowledges from source data to
target data. It mainly consists of two building blocks in the proposed deep
architecture: 1) a shared two-streams network, which the first stream operates
on the source data and the second stream operates on the unlabeled data, to
learn the effective common image representations, and 2) a coarse-to-fine
module, which begins with finding the most representative images from target
classes and then further detect similarities among these images, to transfer
the similarities of the source data to the target data in a greedy fashion.
Extensive evaluation results on several benchmark datasets demonstrate that the
proposed hashing method achieves significant improvement over the
state-of-the-art methods.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Few Shot & Zero Shot 
      
        Multimodal Retrieval 
      
        Medical Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/christiani2018confirmation/">Confirmation Sampling For Exact Nearest Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Confirmation Sampling For Exact Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Confirmation Sampling For Exact Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Christiani Tobias, Pagh Rasmus, Thorup Mikkel</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>6</td>
    <td><p>Locality-sensitive hashing (LSH), introduced by Indyk and Motwani in STOC
‘98, has been an extremely influential framework for nearest neighbor search in
high-dimensional data sets. While theoretical work has focused on the
approximate nearest neighbor problems, in practice LSH data structures with
suitably chosen parameters are used to solve the exact nearest neighbor problem
(with some error probability). Sublinear query time is often possible in
practice even for exact nearest neighbor search, intuitively because the
nearest neighbor tends to be significantly closer than other data points.
However, theory offers little advice on how to choose LSH parameters outside of
pre-specified worst-case settings.
  We introduce the technique of confirmation sampling for solving the exact
nearest neighbor problem using LSH. First, we give a general reduction that
transforms a sequence of data structures that each find the nearest neighbor
with a small, unknown probability, into a data structure that returns the
nearest neighbor with probability \(1-\delta\), using as few queries as possible.
Second, we present a new query algorithm for the LSH Forest data structure with
\(L\) trees that is able to return the exact nearest neighbor of a query point
within the same time bound as an LSH Forest of \(Ω(L)\) trees with internal
parameters specifically tuned to the query and data.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/song2018cross/">Cross-modal Retrieval With Implicit Concept Association</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-modal Retrieval With Implicit Concept Association' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-modal Retrieval With Implicit Concept Association' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Song Yale, Soleymani Mohammad</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>7</td>
    <td><p>Traditional cross-modal retrieval assumes explicit association of concepts
across modalities, where there is no ambiguity in how the concepts are linked
to each other, e.g., when we do the image search with a query “dogs”, we expect
to see dog images. In this paper, we consider a different setting for
cross-modal retrieval where data from different modalities are implicitly
linked via concepts that must be inferred by high-level reasoning; we call this
setting implicit concept association. To foster future research in this
setting, we present a new dataset containing 47K pairs of animated GIFs and
sentences crawled from the web, in which the GIFs depict physical or emotional
reactions to the scenarios described in the text (called “reaction GIFs”). We
report on a user study showing that, despite the presence of implicit concept
association, humans are able to identify video-sentence pairs with matching
concepts, suggesting the feasibility of our task. Furthermore, we propose a
novel visual-semantic embedding network based on multiple instance learning.
Unlike traditional approaches, we compute multiple embeddings from each
modality, each representing different concepts, and measure their similarity by
considering all possible combinations of visual-semantic embeddings in the
framework of multiple instance learning. We evaluate our approach on two
video-sentence datasets with explicit and implicit concept association and
report competitive results compared to existing approaches on cross-modal
retrieval.</p>
</td>
    <td>
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/royoletelier2018disambiguating/">Disambiguating Music Artists At Scale With Audio Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Disambiguating Music Artists At Scale With Audio Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Disambiguating Music Artists At Scale With Audio Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Royo-letelier et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>We address the problem of disambiguating large scale catalogs through the
definition of an unknown artist clustering task. We explore the use of metric
learning techniques to learn artist embeddings directly from audio, and using a
dedicated homonym artists dataset, we compare our method with a recent approach
that learn similar embeddings using artist classifiers. While both systems have
the ability to disambiguate unknown artists relying exclusively on audio, we
show that our system is more suitable in the case when enough audio data is
available for each artist in the train dataset. We also propose a new negative
sampling method for metric learning that takes advantage of side information
such as music genre during the learning phase and shows promising results for
the artist clustering task.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/lin2018learning/">Learning A Disentangled Embedding For Monocular 3D Shape Retrieval And Pose Estimation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning A Disentangled Embedding For Monocular 3D Shape Retrieval And Pose Estimation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning A Disentangled Embedding For Monocular 3D Shape Retrieval And Pose Estimation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>7</td>
    <td><p>We propose a novel approach to jointly perform 3D shape retrieval and pose
estimation from monocular images.In order to make the method robust to
real-world image variations, e.g. complex textures and backgrounds, we learn an
embedding space from 3D data that only includes the relevant information,
namely the shape and pose. Our approach explicitly disentangles a shape vector
and a pose vector, which alleviates both pose bias for 3D shape retrieval and
categorical bias for pose estimation. We then train a CNN to map the images to
this embedding space, and then retrieve the closest 3D shape from the database
and estimate the 6D pose of the object. Our method achieves 10.3 median error
for pose estimation and 0.592 top-1-accuracy for category agnostic 3D object
retrieval on the Pascal3D+ dataset, outperforming the previous state-of-the-art
methods on both tasks.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/srinivas2018merging/">Merging Datasets Through Deep Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Merging Datasets Through Deep Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Merging Datasets Through Deep Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Srinivas Kavitha, Gale Abraham, Dolby Julian</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>7</td>
    <td><p>Merging datasets is a key operation for data analytics. A frequent
requirement for merging is joining across columns that have different surface
forms for the same entity (e.g., the name of a person might be represented as
“Douglas Adams” or “Adams, Douglas”). Similarly, ontology alignment can require
recognizing distinct surface forms of the same entity, especially when
ontologies are independently developed. However, data management systems are
currently limited to performing merges based on string equality, or at best
using string similarity. We propose an approach to performing merges based on
deep learning models. Our approach depends on (a) creating a deep learning
model that maps surface forms of an entity into a set of vectors such that
alternate forms for the same entity are closest in vector space, (b) indexing
these vectors using a nearest neighbors algorithm to find the forms that can be
potentially joined together. To build these models, we had to adapt techniques
from metric learning due to the characteristics of the data; specifically we
describe novel sample selection techniques and loss functions that work for
this problem. To evaluate our approach, we used Wikidata as ground truth and
built models from datasets with approximately 1.1M people’s names (200K
identities) and 130K company names (70K identities). We developed models that
allow for joins with precision@1 of .75-.81 and recall of .74-.81. We make the
models available for aligning people or companies across multiple datasets.</p>
</td>
    <td>
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/zhang2018query/">Query-adaptive Image Retrieval By Deep Weighted Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Query-adaptive Image Retrieval By Deep Weighted Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Query-adaptive Image Retrieval By Deep Weighted Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Jian, Peng Yuxin</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>55</td>
    <td><p>Hashing methods have attracted much attention for large scale image
retrieval. Some deep hashing methods have achieved promising results by taking
advantage of the strong representation power of deep networks recently.
However, existing deep hashing methods treat all hash bits equally. On one
hand, a large number of images share the same distance to a query image due to
the discrete Hamming distance, which raises a critical issue of image retrieval
where fine-grained rankings are very important. On the other hand, different
hash bits actually contribute to the image retrieval differently, and treating
them equally greatly affects the retrieval accuracy of image. To address the
above two problems, we propose the query-adaptive deep weighted hashing (QaDWH)
approach, which can perform fine-grained ranking for different queries by
weighted Hamming distance. First, a novel deep hashing network is proposed to
learn the hash codes and corresponding class-wise weights jointly, so that the
learned weights can reflect the importance of different hash bits for different
image classes. Second, a query-adaptive image retrieval method is proposed,
which rapidly generates hash bit weights for different query images by fusing
its semantic probability and the learned class-wise weights. Fine-grained image
retrieval is then performed by the weighted Hamming distance, which can provide
more accurate ranking than the traditional Hamming distance. Experiments on
four widely used datasets show that the proposed approach outperforms eight
state-of-the-art hashing methods.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/lee2018stacked/">Stacked Cross Attention For Image-text Matching</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Stacked Cross Attention For Image-text Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Stacked Cross Attention For Image-text Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lee et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>1151</td>
    <td><p>In this paper, we study the problem of image-text matching. Inferring the
latent semantic alignment between objects or other salient stuff (e.g. snow,
sky, lawn) and the corresponding words in sentences allows to capture
fine-grained interplay between vision and language, and makes image-text
matching more interpretable. Prior work either simply aggregates the similarity
of all possible pairs of regions and words without attending differentially to
more and less important words or regions, or uses a multi-step attentional
process to capture limited number of semantic alignments which is less
interpretable. In this paper, we present Stacked Cross Attention to discover
the full latent alignments using both image regions and words in a sentence as
context and infer image-text similarity. Our approach achieves the
state-of-the-art results on the MS-COCO and Flickr30K datasets. On Flickr30K,
our approach outperforms the current best methods by 22.1% relatively in text
retrieval from image query, and 18.2% relatively in image retrieval with text
query (based on Recall@1). On MS-COCO, our approach improves sentence retrieval
by 17.8% relatively and image retrieval by 16.6% relatively (based on Recall@1
using the 5K test set). Code has been made available at:
https://github.com/kuanghuei/SCAN.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/xu2018unsupervised/">Unsupervised Part-based Weighting Aggregation Of Deep Convolutional Features For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Part-based Weighting Aggregation Of Deep Convolutional Features For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Part-based Weighting Aggregation Of Deep Convolutional Features For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>48</td>
    <td><p>In this paper, we propose a simple but effective semantic part-based
weighting aggregation (PWA) for image retrieval. The proposed PWA utilizes the
discriminative filters of deep convolutional layers as part detectors.
Moreover, we propose the effective unsupervised strategy to select some part
detectors to generate the “probabilistic proposals”, which highlight certain
discriminative parts of objects and suppress the noise of background. The final
global PWA representation could then be acquired by aggregating the regional
representations weighted by the selected “probabilistic proposals”
corresponding to various semantic content. We conduct comprehensive experiments
on four standard datasets and show that our unsupervised PWA outperforms the
state-of-the-art unsupervised and supervised aggregation methods. Code is
available at https://github.com/XJhaoren/PWA.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Image Retrieval 
      
        AAAI 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/opitz2018deep/">Deep Metric Learning With BIER: Boosting Independent Embeddings Robustly</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Metric Learning With BIER: Boosting Independent Embeddings Robustly' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Metric Learning With BIER: Boosting Independent Embeddings Robustly' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Opitz et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>164</td>
    <td><p>Learning similarity functions between image pairs with deep neural networks
yields highly correlated activations of embeddings. In this work, we show how
to improve the robustness of such embeddings by exploiting the independence
within ensembles. To this end, we divide the last embedding layer of a deep
network into an embedding ensemble and formulate training this ensemble as an
online gradient boosting problem. Each learner receives a reweighted training
sample from the previous learners. Further, we propose two loss functions which
increase the diversity in our ensemble. These loss functions can be applied
either for weight initialization or during training. Together, our
contributions leverage large embedding sizes more effectively by significantly
reducing correlation of the embedding and consequently increase retrieval
accuracy of the embedding. Our method works with any differentiable loss
function and does not introduce any additional parameters during test time. We
evaluate our metric learning method on image retrieval tasks and show that it
improves over state-of-the-art methods on the CUB 200-2011, Cars-196, Stanford
Online Products, In-Shop Clothes Retrieval and VehicleID datasets.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/chen2018distributed/">Distributed Collaborative Hashing And Its Applications In Ant Financial</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Distributed Collaborative Hashing And Its Applications In Ant Financial' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Distributed Collaborative Hashing And Its Applications In Ant Financial' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</td>
    <td>7</td>
    <td><p>Collaborative filtering, especially latent factor model, has been popularly
used in personalized recommendation. Latent factor model aims to learn user and
item latent factors from user-item historic behaviors. To apply it into real
big data scenarios, efficiency becomes the first concern, including offline
model training efficiency and online recommendation efficiency. In this paper,
we propose a Distributed Collaborative Hashing (DCH) model which can
significantly improve both efficiencies. Specifically, we first propose a
distributed learning framework, following the state-of-the-art parameter server
paradigm, to learn the offline collaborative model. Our model can be learnt
efficiently by distributedly computing subgradients in minibatches on workers
and updating model parameters on servers asynchronously. We then adopt hashing
technique to speedup the online recommendation procedure. Recommendation can be
quickly made through exploiting lookup hash tables. We conduct thorough
experiments on two real large-scale datasets. The experimental results
demonstrate that, comparing with the classic and state-of-the-art (distributed)
latent factor models, DCH has comparable performance in terms of recommendation
accuracy but has both fast convergence speed in offline model training
procedure and realtime efficiency in online recommendation procedure.
Furthermore, the encouraging performance of DCH is also shown for several
real-world applications in Ant Financial.</p>
</td>
    <td>
      
        KDD 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/chen2018darkrank/">Darkrank: Accelerating Deep Metric Learning Via Cross Sample Similarities Transfer</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Darkrank: Accelerating Deep Metric Learning Via Cross Sample Similarities Transfer' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Darkrank: Accelerating Deep Metric Learning Via Cross Sample Similarities Transfer' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen Yuntao, Wang Naiyan, Zhang Zhaoxiang</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>219</td>
    <td><p>We have witnessed rapid evolution of deep neural network architecture design
in the past years. These latest progresses greatly facilitate the developments
in various areas such as computer vision and natural language processing.
However, along with the extraordinary performance, these state-of-the-art
models also bring in expensive computational cost. Directly deploying these
models into applications with real-time requirement is still infeasible.
Recently, Hinton etal. have shown that the dark knowledge within a powerful
teacher model can significantly help the training of a smaller and faster
student network. These knowledge are vastly beneficial to improve the
generalization ability of the student model. Inspired by their work, we
introduce a new type of knowledge – cross sample similarities for model
compression and acceleration. This knowledge can be naturally derived from deep
metric learning model. To transfer them, we bring the “learning to rank”
technique into deep metric learning formulation. We test our proposed DarkRank
method on various metric learning tasks including pedestrian re-identification,
image retrieval and image clustering. The results are quite encouraging. Our
method can improve over the baseline method by a large margin. Moreover, it is
fully compatible with other existing methods. When combined, the performance
can be further boosted.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/chen2018blazingly/">Blazingly Fast Video Object Segmentation With Pixel-wise Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Blazingly Fast Video Object Segmentation With Pixel-wise Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Blazingly Fast Video Object Segmentation With Pixel-wise Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen et al.</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</td>
    <td>283</td>
    <td><p>This paper tackles the problem of video object segmentation, given some user
annotation which indicates the object of interest. The problem is formulated as
pixel-wise retrieval in a learned embedding space: we embed pixels of the same
object instance into the vicinity of each other, using a fully convolutional
network trained by a modified triplet loss as the embedding model. Then the
annotated pixels are set as reference and the rest of the pixels are classified
using a nearest-neighbor approach. The proposed method supports different kinds
of user input such as segmentation mask in the first frame (semi-supervised
scenario), or a sparse set of clicked points (interactive scenario). In the
semi-supervised scenario, we achieve results competitive with the state of the
art but at a fraction of computation cost (275 milliseconds per frame). In the
interactive scenario where the user is able to refine their input iteratively,
the proposed method provides instant response to each input, and reaches
comparable quality to competing methods with much less interaction.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/chen2018deep/">Deep Hashing Via Discrepancy Minimization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Hashing Via Discrepancy Minimization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Hashing Via Discrepancy Minimization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen et al.</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</td>
    <td>60</td>
    <td><p>This paper presents a discrepancy minimizing model to
address the discrete optimization problem in hashing learning. The discrete optimization introduced by binary constraint is an NP-hard mixed integer programming problem.
It is usually addressed by relaxing the binary variables into
continuous variables to adapt to the gradient based learning of hashing functions, especially the training of deep
neural networks. To deal with the objective discrepancy
caused by relaxation, we transform the original binary optimization into differentiable optimization problem over hash
functions through series expansion. This transformation decouples the binary constraint and the similarity preserving
hashing function optimization. The transformed objective
is optimized in a tractable alternating optimization framework with gradual discrepancy minimization. Extensive experimental results on three benchmark datasets validate the
efficacy of the proposed discrepancy minimizing hashing.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/lai2018improved/">Improved Search In Hamming Space Using Deep Multi-index Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Improved Search In Hamming Space Using Deep Multi-index Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Improved Search In Hamming Space Using Deep Multi-index Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lai Hanjiang, Pan Yan</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Circuits and Systems for Video Technology</td>
    <td>12</td>
    <td><p>Similarity-preserving hashing is a widely-used method for nearest neighbour
search in large-scale image retrieval tasks. There has been considerable
research on generating efficient image representation via the
deep-network-based hashing methods. However, the issue of efficient searching
in the deep representation space remains largely unsolved. To this end, we
propose a simple yet efficient deep-network-based multi-index hashing method
for simultaneously learning the powerful image representation and the efficient
searching. To achieve these two goals, we introduce the multi-index hashing
(MIH) mechanism into the proposed deep architecture, which divides the binary
codes into multiple substrings. Due to the non-uniformly distributed codes will
result in inefficiency searching, we add the two balanced constraints at
feature-level and instance-level, respectively. Extensive evaluations on
several benchmark image retrieval datasets show that the learned balanced
binary codes bring dramatic speedups and achieve comparable performance over
the existing baselines.</p>
</td>
    <td>
      
        Vector Indexing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/chaidaroon2018deep/">Deep Semantic Text Hashing With Weak Supervision</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Semantic Text Hashing With Weak Supervision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Semantic Text Hashing With Weak Supervision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chaidaroon Suthee, Ebesu, Fang</td> <!-- 🔧 You were missing this -->
    <td>The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</td>
    <td>29</td>
    <td><p>With an ever increasing amount of data available on the web, fast similarity search has become the critical component for large-scale information retrieval systems. One solution is semantic hashing which designs binary codes to accelerate similarity search. Recently, deep learning has been successfully applied to the semantic hashing problem and produces high-quality compact binary codes compared to traditional methods. However, most state-of-the-art semantic hashing approaches require large amounts of hand-labeled training data which are often expensive and time consuming to collect. The cost of getting labeled data is the key bottleneck in deploying these hashing methods. Motivated by the recent success in machine learning that makes use of weak supervision, we employ unsupervised ranking methods such as BM25 to extract weak signals from training data. We further introduce two deep generative semantic hashing models to leverage weak signals for text hashing. The experimental results on four public datasets show that our models can generate high-quality binary codes without using hand-labeled training data and significantly outperform the competitive unsupervised semantic hashing baselines.</p>
</td>
    <td>
      
        SIGIR 
      
        Hashing Methods 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/radenovi%C4%872018fine/">Fine-tuning CNN Image Retrieval With No Human Annotation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fine-tuning CNN Image Retrieval With No Human Annotation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fine-tuning CNN Image Retrieval With No Human Annotation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Radenović Filip, Tolias Giorgos, Chum Ondřej</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>1178</td>
    <td><p>Image descriptors based on activations of Convolutional Neural Networks
(CNNs) have become dominant in image retrieval due to their discriminative
power, compactness of representation, and search efficiency. Training of CNNs,
either from scratch or fine-tuning, requires a large amount of annotated data,
where a high quality of annotation is often crucial. In this work, we propose
to fine-tune CNNs for image retrieval on a large collection of unordered images
in a fully automated manner. Reconstructed 3D models obtained by the
state-of-the-art retrieval and structure-from-motion methods guide the
selection of the training data. We show that both hard-positive and
hard-negative examples, selected by exploiting the geometry and the camera
positions available from the 3D models, enhance the performance of
particular-object retrieval. CNN descriptor whitening discriminatively learned
from the same training data outperforms commonly used PCA whitening. We propose
a novel trainable Generalized-Mean (GeM) pooling layer that generalizes max and
average pooling and show that it boosts retrieval performance. Applying the
proposed method to the VGG network achieves state-of-the-art performance on the
standard benchmarks: Oxford Buildings, Paris, and Holidays datasets.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/carvalho2018cross/">Cross-modal Retrieval In The Cooking Context: Learning Semantic Text-image Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-modal Retrieval In The Cooking Context: Learning Semantic Text-image Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-modal Retrieval In The Cooking Context: Learning Semantic Text-image Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Carvalho et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>39</td>
    <td><p>Designing powerful tools that support cooking activities has rapidly gained
popularity due to the massive amounts of available data, as well as recent
advances in machine learning that are capable of analyzing them. In this paper,
we propose a cross-modal retrieval model aligning visual and textual data (like
pictures of dishes and their recipes) in a shared representation space. We
describe an effective learning scheme, capable of tackling large-scale
problems, and validate it on the Recipe1M dataset containing nearly 1 million
picture-recipe pairs. We show the effectiveness of our approach regarding
previous state-of-the-art models and present qualitative results over
computational cooking use cases.</p>
</td>
    <td>
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/ceccarello2018fresh/">FRESH: Fr\'echet Similarity With Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=FRESH: Fr\'echet Similarity With Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=FRESH: Fr\'echet Similarity With Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ceccarello Matteo, Driemel Anne, Silvestri Francesco</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>6</td>
    <td><p>This paper studies the \(r\)-range search problem for curves under the
continuous Fr'echet distance: given a dataset \(S\) of \(n\) polygonal curves and
a threshold \(r&gt;0\), construct a data structure that, for any query curve \(q\),
efficiently returns all entries in \(S\) with distance at most \(r\) from \(q\). We
propose FRESH, an approximate and randomized approach for \(r\)-range search,
that leverages on a locality sensitive hashing scheme for detecting candidate
near neighbors of the query curve, and on a subsequent pruning step based on a
cascade of curve simplifications. We experimentally compare \fresh to exact and
deterministic solutions, and we show that high performance can be reached by
suitably relaxing precision and recall.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/wu2018starspace/">Starspace: Embed All The Things!</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Starspace: Embed All The Things!' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Starspace: Embed All The Things!' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>140</td>
    <td><p>We present StarSpace, a general-purpose neural embedding model that can solve
a wide variety of problems: labeling tasks such as text classification, ranking
tasks such as information retrieval/web search, collaborative filtering-based
or content-based recommendation, embedding of multi-relational graphs, and
learning word, sentence or document level embeddings. In each case the model
works by embedding those entities comprised of discrete features and comparing
them against each other – learning similarities dependent on the task.
Empirical results on a number of tasks show that StarSpace is highly
competitive with existing methods, whilst also being generally applicable to
new cases where those methods are not.</p>
</td>
    <td>
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/yang2018shared/">Shared Predictive Cross-modal Deep Quantization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Shared Predictive Cross-modal Deep Quantization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Shared Predictive Cross-modal Deep Quantization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yang et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Neural Networks and Learning Systems</td>
    <td>150</td>
    <td><p>With explosive growth of data volume and ever-increasing diversity of data
modalities, cross-modal similarity search, which conducts nearest neighbor
search across different modalities, has been attracting increasing interest.
This paper presents a deep compact code learning solution for efficient
cross-modal similarity search. Many recent studies have proven that
quantization-based approaches perform generally better than hashing-based
approaches on single-modal similarity search. In this paper, we propose a deep
quantization approach, which is among the early attempts of leveraging deep
neural networks into quantization-based cross-modal similarity search. Our
approach, dubbed shared predictive deep quantization (SPDQ), explicitly
formulates a shared subspace across different modalities and two private
subspaces for individual modalities, and representations in the shared subspace
and the private subspaces are learned simultaneously by embedding them to a
reproducing kernel Hilbert space, where the mean embedding of different
modality distributions can be explicitly compared. In addition, in the shared
subspace, a quantizer is learned to produce the semantics preserving compact
codes with the help of label alignment. Thanks to this novel network
architecture in cooperation with supervised quantization training, SPDQ can
preserve intramodal and intermodal similarities as much as possible and greatly
reduce quantization error. Experiments on two popular benchmarks corroborate
that our approach outperforms state-of-the-art methods.</p>
</td>
    <td>
      
        Quantization 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/zhang2018sch/">SCH-GAN: Semi-supervised Cross-modal Hashing By Generative Adversarial Network</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=SCH-GAN: Semi-supervised Cross-modal Hashing By Generative Adversarial Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=SCH-GAN: Semi-supervised Cross-modal Hashing By Generative Adversarial Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Jian, Peng Yuxin, Yuan Mingkuan</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Cybernetics</td>
    <td>127</td>
    <td><p>Cross-modal hashing aims to map heterogeneous multimedia data into a common
Hamming space, which can realize fast and flexible retrieval across different
modalities. Supervised cross-modal hashing methods have achieved considerable
progress by incorporating semantic side information. However, they mainly have
two limitations: (1) Heavily rely on large-scale labeled cross-modal training
data which are labor intensive and hard to obtain. (2) Ignore the rich
information contained in the large amount of unlabeled data across different
modalities, especially the margin examples that are easily to be incorrectly
retrieved, which can help to model the correlations. To address these problems,
in this paper we propose a novel Semi-supervised Cross-Modal Hashing approach
by Generative Adversarial Network (SCH-GAN). We aim to take advantage of GAN’s
ability for modeling data distributions to promote cross-modal hashing learning
in an adversarial way. The main contributions can be summarized as follows: (1)
We propose a novel generative adversarial network for cross-modal hashing. In
our proposed SCH-GAN, the generative model tries to select margin examples of
one modality from unlabeled data when giving a query of another modality. While
the discriminative model tries to distinguish the selected examples and true
positive examples of the query. These two models play a minimax game so that
the generative model can promote the hashing performance of discriminative
model. (2) We propose a reinforcement learning based algorithm to drive the
training of proposed SCH-GAN. The generative model takes the correlation score
predicted by discriminative model as a reward, and tries to select the examples
close to the margin to promote discriminative model by maximizing the margin
between positive and negative data. Experiments on 3 widely-used datasets
verify the effectiveness of our proposed approach.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/lam2018word2bits/">Word2bits - Quantized Word Vectors</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Word2bits - Quantized Word Vectors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Word2bits - Quantized Word Vectors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lam Maximilian</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>27</td>
    <td><p>Word vectors require significant amounts of memory and storage, posing issues
to resource limited devices like mobile phones and GPUs. We show that high
quality quantized word vectors using 1-2 bits per parameter can be learned by
introducing a quantization function into Word2Vec. We furthermore show that
training with the quantization function acts as a regularizer. We train word
vectors on English Wikipedia (2017) and evaluate them on standard word
similarity and analogy tasks and on question answering (SQuAD). Our quantized
word vectors not only take 8-16x less space than full precision (32 bit) word
vectors but also outperform them on word similarity tasks and question
answering.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/cao2018hashgan/">Hashgan: Deep Learning To Hash With Pair Conditional Wasserstein GAN</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashgan: Deep Learning To Hash With Pair Conditional Wasserstein GAN' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashgan: Deep Learning To Hash With Pair Conditional Wasserstein GAN' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cao et al.</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</td>
    <td>117</td>
    <td><p>Deep learning to hash improves image retrieval performance by end-to-end representation learning and hash coding from training data with pairwise similarity information.
Subject to the scarcity of similarity information that is often
expensive to collect for many application domains, existing
deep learning to hash methods may overfit the training data
and result in substantial loss of retrieval quality. This paper
presents HashGAN, a novel architecture for deep learning
to hash, which learns compact binary hash codes from both
real images and diverse images synthesized by generative
models. The main idea is to augment the training data with
nearly real images synthesized from a new Pair Conditional
Wasserstein GAN (PC-WGAN) conditioned on the pairwise
similarity information. Extensive experiments demonstrate
that HashGAN can generate high-quality binary hash codes
and yield state-of-the-art image retrieval performance on
three benchmarks, NUS-WIDE, CIFAR-10, and MS-COCO.</p>
</td>
    <td>
      
        Hashing Methods 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/cao2018deep/">Deep Priority Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Priority Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Priority Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cao et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 26th ACM international conference on Multimedia</td>
    <td>33</td>
    <td><p>Deep hashing enables image retrieval by end-to-end learning of deep
representations and hash codes from training data with pairwise similarity
information. Subject to the distribution skewness underlying the similarity
information, most existing deep hashing methods may underperform for imbalanced
data due to misspecified loss functions. This paper presents Deep Priority
Hashing (DPH), an end-to-end architecture that generates compact and balanced
hash codes in a Bayesian learning framework. The main idea is to reshape the
standard cross-entropy loss for similarity-preserving learning such that it
down-weighs the loss associated to highly-confident pairs. This idea leads to a
novel priority cross-entropy loss, which prioritizes the training on uncertain
pairs over confident pairs. Also, we propose another priority quantization
loss, which prioritizes hard-to-quantize examples for generation of nearly
lossless hash codes. Extensive experiments demonstrate that DPH can generate
high-quality hash codes and yield state-of-the-art image retrieval results on
three datasets, ImageNet, NUS-WIDE, and MS-COCO.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/cao2018transfer/">Transfer Adversarial Hashing For Hamming Space Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Transfer Adversarial Hashing For Hamming Space Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Transfer Adversarial Hashing For Hamming Space Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cao et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>15</td>
    <td><p>Hashing is widely applied to large-scale image retrieval due to the storage
and retrieval efficiency. Existing work on deep hashing assumes that the
database in the target domain is identically distributed with the training set
in the source domain. This paper relaxes this assumption to a transfer
retrieval setting, which allows the database and the training set to come from
different but relevant domains. However, the transfer retrieval setting will
introduce two technical difficulties: first, the hash model trained on the
source domain cannot work well on the target domain due to the large
distribution gap; second, the domain gap makes it difficult to concentrate the
database points to be within a small Hamming ball. As a consequence, transfer
retrieval performance within Hamming Radius 2 degrades significantly in
existing hashing methods. This paper presents Transfer Adversarial Hashing
(TAH), a new hybrid deep architecture that incorporates a pairwise
\(t\)-distribution cross-entropy loss to learn concentrated hash codes and an
adversarial network to align the data distributions between the source and
target domains. TAH can generate compact transfer hash codes for efficient
image retrieval on both source and target domains. Comprehensive experiments
validate that TAH yields state of the art Hamming space retrieval performance
on standard datasets.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Robustness 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/cakir2018hashing/">Hashing With Binary Matrix Pursuit</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing With Binary Matrix Pursuit' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing With Binary Matrix Pursuit' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cakir Fatih, He Kun, Sclaroff Stan</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>31</td>
    <td><p>We propose theoretical and empirical improvements for two-stage hashing
methods. We first provide a theoretical analysis on the quality of the binary
codes and show that, under mild assumptions, a residual learning scheme can
construct binary codes that fit any neighborhood structure with arbitrary
accuracy. Secondly, we show that with high-capacity hash functions such as
CNNs, binary code inference can be greatly simplified for many standard
neighborhood definitions, yielding smaller optimization problems and more
robust codes. Incorporating our findings, we propose a novel two-stage hashing
method that significantly outperforms previous hashing studies on widely used
image retrieval benchmarks.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/cakaloglu2018text/">Text Embeddings For Retrieval From A Large Knowledge Base</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Text Embeddings For Retrieval From A Large Knowledge Base' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Text Embeddings For Retrieval From A Large Knowledge Base' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cakaloglu Tolgahan, Szegedy Christian, Xu Xiaowei</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>6</td>
    <td><p>Text embedding representing natural language documents in a semantic vector
space can be used for document retrieval using nearest neighbor lookup. In
order to study the feasibility of neural models specialized for retrieval in a
semantically meaningful way, we suggest the use of the Stanford Question
Answering Dataset (SQuAD) in an open-domain question answering context, where
the first task is to find paragraphs useful for answering a given question.
First, we compare the quality of various text-embedding methods on the
performance of retrieval and give an extensive empirical comparison on the
performance of various non-augmented base embedding with, and without IDF
weighting. Our main results are that by training deep residual neural models,
specifically for retrieval purposes, can yield significant gains when it is
used to augment existing embeddings. We also establish that deeper models are
superior to this task. The best base baseline embeddings augmented by our
learned neural approach improves the top-1 paragraph recall of the system by
14%.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/luo2018fast/">Fast Scalable Supervised Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Scalable Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Scalable Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Luo et al.</td> <!-- 🔧 You were missing this -->
    <td>The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</td>
    <td>89</td>
    <td><p>Despite significant progress in supervised hashing, there are three
common limitations of existing methods. First, most pioneer methods discretely learn hash codes bit by bit, making the learning
procedure rather time-consuming. Second, to reduce the large complexity of the n by n pairwise similarity matrix, most methods apply
sampling strategies during training, which inevitably results in information loss and suboptimal performance; some recent methods
try to replace the large matrix with a smaller one, but the size is
still large. Third, among the methods that leverage the pairwise
similarity matrix, most of them only encode the semantic label
information in learning the hash codes, failing to fully capture
the characteristics of data. In this paper, we present a novel supervised hashing method, called Fast Scalable Supervised Hashing
(FSSH), which circumvents the use of the large similarity matrix by
introducing a pre-computed intermediate term whose size is independent with the size of training data. Moreover, FSSH can learn
the hash codes with not only the semantic information but also
the features of data. Extensive experiments on three widely used
datasets demonstrate its superiority over several state-of-the-art
methods in both accuracy and scalability. Our experiment codes
are available at: https://lcbwlx.wixsite.com/fssh.</p>
</td>
    <td>
      
        Text Retrieval 
      
        Unsupervised 
      
        Neural Hashing 
      
        SUPERVISED 
      
        SIGIR 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/uy2018pointnetvlad/">Pointnetvlad: Deep Point Cloud Based Retrieval For Large-scale Place Recognition</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Pointnetvlad: Deep Point Cloud Based Retrieval For Large-scale Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Pointnetvlad: Deep Point Cloud Based Retrieval For Large-scale Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Uy Mikaela Angelina, Lee Gim Hee</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</td>
    <td>484</td>
    <td><p>Unlike its image based counterpart, point cloud based retrieval for place
recognition has remained as an unexplored and unsolved problem. This is largely
due to the difficulty in extracting local feature descriptors from a point
cloud that can subsequently be encoded into a global descriptor for the
retrieval task. In this paper, we propose the PointNetVLAD where we leverage on
the recent success of deep networks to solve point cloud based retrieval for
place recognition. Specifically, our PointNetVLAD is a combination/modification
of the existing PointNet and NetVLAD, which allows end-to-end training and
inference to extract the global descriptor from a given 3D point cloud.
Furthermore, we propose the “lazy triplet and quadruplet” loss functions that
can achieve more discriminative and generalizable global descriptors to tackle
the retrieval task. We create benchmark datasets for point cloud based
retrieval for place recognition, and the experimental results on these datasets
show the feasibility of our PointNetVLAD. Our code and the link for the
benchmark dataset downloads are available in our project website.
http://github.com/mikacuy/pointnetvlad/</p>
</td>
    <td>
      
        SCALABILITY 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/peng2018modality/">Modality-specific Cross-modal Similarity Measurement With Recurrent Attention Network</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Modality-specific Cross-modal Similarity Measurement With Recurrent Attention Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Modality-specific Cross-modal Similarity Measurement With Recurrent Attention Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Peng Yuxin, Qi Jinwei, Yuan Yuxin</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>141</td>
    <td><p>Nowadays, cross-modal retrieval plays an indispensable role to flexibly find
information across different modalities of data. Effectively measuring the
similarity between different modalities of data is the key of cross-modal
retrieval. Different modalities such as image and text have imbalanced and
complementary relationships, which contain unequal amount of information when
describing the same semantics. For example, images often contain more details
that cannot be demonstrated by textual descriptions and vice versa. Existing
works based on Deep Neural Network (DNN) mostly construct one common space for
different modalities to find the latent alignments between them, which lose
their exclusive modality-specific characteristics. Different from the existing
works, we propose modality-specific cross-modal similarity measurement (MCSM)
approach by constructing independent semantic space for each modality, which
adopts end-to-end framework to directly generate modality-specific cross-modal
similarity without explicit common representation. For each semantic space,
modality-specific characteristics within one modality are fully exploited by
recurrent attention network, while the data of another modality is projected
into this space with attention based joint embedding to utilize the learned
attention weights for guiding the fine-grained cross-modal correlation
learning, which can capture the imbalanced and complementary relationships
between different modalities. Finally, the complementarity between the semantic
spaces for different modalities is explored by adaptive fusion of the
modality-specific cross-modal similarities to perform cross-modal retrieval.
Experiments on the widely-used Wikipedia and Pascal Sentence datasets as well
as our constructed large-scale XMediaNet dataset verify the effectiveness of
our proposed approach, outperforming 9 state-of-the-art methods.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/bhatnagar2018fine/">Fine-grained Apparel Classification And Retrieval Without Rich Annotations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fine-grained Apparel Classification And Retrieval Without Rich Annotations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fine-grained Apparel Classification And Retrieval Without Rich Annotations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Bhatnagar Aniket, Aggarwal Sanchit</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>331</td>
    <td><p>The ability to correctly classify and retrieve apparel images has a variety
of applications important to e-commerce, online advertising and internet
search. In this work, we propose a robust framework for fine-grained apparel
classification, in-shop and cross-domain retrieval which eliminates the
requirement of rich annotations like bounding boxes and human-joints or
clothing landmarks, and training of bounding box/ key-landmark detector for the
same. Factors such as subtle appearance differences, variations in human poses,
different shooting angles, apparel deformations, and self-occlusion add to the
challenges in classification and retrieval of apparel items. Cross-domain
retrieval is even harder due to the presence of large variation between online
shopping images, usually taken in ideal lighting, pose, positive angle and
clean background as compared with street photos captured by users in
complicated conditions with poor lighting and cluttered scenes. Our framework
uses compact bilinear CNN with tensor sketch algorithm to generate embeddings
that capture local pairwise feature interactions in a translationally invariant
manner. For apparel classification, we pass the feature embeddings through a
softmax classifier, while, the in-shop and cross-domain retrieval pipelines use
a triplet-loss based optimization approach, such that squared Euclidean
distance between embeddings measures the dissimilarity between the images.
Unlike previous works that relied on bounding box, key clothing landmarks or
human joint detectors to assist the final deep classifier, proposed framework
can be trained directly on the provided category labels or generated triplets
for triplet loss optimization. Lastly, Experimental results on the DeepFashion
fine-grained categorization, and in-shop and consumer-to-shop retrieval
datasets provide a comparative analysis with previous work performed in the
domain.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/li2018deep/">Deep Binary Reconstruction For Cross-modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Binary Reconstruction For Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Binary Reconstruction For Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Xuelong, Hu di, Nie Feiping</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>128</td>
    <td><p>With the increasing demand of massive multimodal data storage and
organization, cross-modal retrieval based on hashing technique has drawn much
attention nowadays. It takes the binary codes of one modality as the query to
retrieve the relevant hashing codes of another modality. However, the existing
binary constraint makes it difficult to find the optimal cross-modal hashing
function. Most approaches choose to relax the constraint and perform
thresholding strategy on the real-value representation instead of directly
solving the original objective. In this paper, we first provide a concrete
analysis about the effectiveness of multimodal networks in preserving the
inter- and intra-modal consistency. Based on the analysis, we provide a
so-called Deep Binary Reconstruction (DBRC) network that can directly learn the
binary hashing codes in an unsupervised fashion. The superiority comes from a
proposed simple but efficient activation function, named as Adaptive Tanh
(ATanh). The ATanh function can adaptively learn the binary codes and be
trained via back-propagation. Extensive experiments on three benchmark datasets
demonstrate that DBRC outperforms several state-of-the-art methods in both
image2text and text2image retrieval task.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/rong2018locality/">Locality-sensitive Hashing For Earthquake Detection: A Case Study Of Scaling Data-driven Science</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Locality-sensitive Hashing For Earthquake Detection: A Case Study Of Scaling Data-driven Science' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Locality-sensitive Hashing For Earthquake Detection: A Case Study Of Scaling Data-driven Science' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Rong et al.</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>6</td>
    <td><p>In this work, we report on a novel application of Locality Sensitive
Hashing (LSH) to seismic data at scale. Based on the high waveform similarity between reoccurring earthquakes, our application
identifies potential earthquakes by searching for similar time series
segments via LSH. However, a straightforward implementation of
this LSH-enabled application has difficulty scaling beyond 3 months
of continuous time series data measured at a single seismic station.
As a case study of a data-driven science workflow, we illustrate how
domain knowledge can be incorporated into the workload to improve
both the efficiency and result quality. We describe several end-toend optimizations of the analysis pipeline from pre-processing to
post-processing, which allow the application to scale to time series data measured at multiple seismic stations. Our optimizations
enable an over 100× speedup in the end-to-end analysis pipeline.
This improved scalability enabled seismologists to perform seismic
analysis on more than ten years of continuous time series data from
over ten seismic stations, and has directly enabled the discovery of
597 new earthquakes near the Diablo Canyon nuclear power plant
in California and 6123 new earthquakes in New Zealand.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/li2018image/">Image Super-resolution Via Feature-augmented Random Forest</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Image Super-resolution Via Feature-augmented Random Forest' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Image Super-resolution Via Feature-augmented Random Forest' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Hailiang, Lam Kin-man, Wang Miaohui</td> <!-- 🔧 You were missing this -->
    <td>Signal Processing: Image Communication</td>
    <td>16</td>
    <td><p>Recent random-forest (RF)-based image super-resolution approaches inherit
some properties from dictionary-learning-based algorithms, but the
effectiveness of the properties in RF is overlooked in the literature. In this
paper, we present a novel feature-augmented random forest (FARF) for image
super-resolution, where the conventional gradient-based features are augmented
with gradient magnitudes and different feature recipes are formulated on
different stages in an RF. The advantages of our method are that, firstly, the
dictionary-learning-based features are enhanced by adding gradient magnitudes,
based on the observation that the non-linear gradient magnitude are with highly
discriminative property. Secondly, generalized locality-sensitive hashing (LSH)
is used to replace principal component analysis (PCA) for feature
dimensionality reduction and original high-dimensional features are employed,
instead of the compressed ones, for the leaf-nodes’ regressors, since
regressors can benefit from higher dimensional features. This
original-compressed coupled feature sets scheme unifies the unsupervised LSH
evaluation on both image super-resolution and content-based image retrieval
(CBIR). Finally, we present a generalized weighted ridge regression (GWRR)
model for the leaf-nodes’ regressors. Experiment results on several public
benchmark datasets show that our FARF method can achieve an average gain of
about 0.3 dB, compared to traditional RF-based methods. Furthermore, a
fine-tuned FARF model can compare to or (in many cases) outperform some recent
stateof-the-art deep-learning-based algorithms.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/zhai2018classification/">Classification Is A Strong Baseline For Deep Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Classification Is A Strong Baseline For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Classification Is A Strong Baseline For Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhai Andrew, Wu Hao-yu</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>132</td>
    <td><p>Deep metric learning aims to learn a function mapping image pixels to
embedding feature vectors that model the similarity between images. Two major
applications of metric learning are content-based image retrieval and face
verification. For the retrieval tasks, the majority of current state-of-the-art
(SOTA) approaches are triplet-based non-parametric training. For the face
verification tasks, however, recent SOTA approaches have adopted
classification-based parametric training. In this paper, we look into the
effectiveness of classification based approaches on image retrieval datasets.
We evaluate on several standard retrieval datasets such as CAR-196,
CUB-200-2011, Stanford Online Product, and In-Shop datasets for image retrieval
and clustering, and establish that our classification-based approach is
competitive across different feature dimensions and base feature networks. We
further provide insights into the performance effects of subsampling classes
for scalable classification-based training, and the effects of binarization,
enabling efficient storage and computation for practical applications.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/belyy2018memoir/">MEMOIR: Multi-class Extreme Classification With Inexact Margin</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=MEMOIR: Multi-class Extreme Classification With Inexact Margin' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=MEMOIR: Multi-class Extreme Classification With Inexact Margin' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Belyy Anton, Sholokhov Aleksei</td> <!-- 🔧 You were missing this -->
    <td>Electronic Journal of Statistics</td>
    <td>13</td>
    <td><p>Multi-class classification with a very large number of classes, or extreme
classification, is a challenging problem from both statistical and
computational perspectives. Most of the classical approaches to multi-class
classification, including one-vs-rest or multi-class support vector machines,
require the exact estimation of the classifier’s margin, at both the training
and the prediction steps making them intractable in extreme classification
scenarios. In this paper, we study the impact of computing an approximate
margin using nearest neighbor (ANN) search structures combined with
locality-sensitive hashing (LSH). This approximation allows to dramatically
reduce both the training and the prediction time without a significant loss in
performance. We theoretically prove that this approximation does not lead to a
significant loss of the risk of the model and provide empirical evidence over
five publicly available large scale datasets, showing that the proposed
approach is highly competitive with respect to state-of-the-art approaches on
time, memory and performance measures.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/li2018design/">The Design And Implementation Of A Real Time Visual Search System On JD E-commerce Platform</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=The Design And Implementation Of A Real Time Visual Search System On JD E-commerce Platform' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=The Design And Implementation Of A Real Time Visual Search System On JD E-commerce Platform' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 19th International Middleware Conference Industry</td>
    <td>34</td>
    <td><p>We present the design and implementation of a visual search system for real
time image retrieval on JD.com, the world’s third largest and China’s largest
e-commerce site. We demonstrate that our system can support real time visual
search with hundreds of billions of product images at sub-second timescales and
handle frequent image updates through distributed hierarchical architecture and
efficient indexing methods. We hope that sharing our practice with our real
production system will inspire the middleware community’s interest and
appreciation for building practical large scale systems for emerging
applications, such as ecommerce visual search.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/lin2018supervised/">Supervised Online Hashing Via Hadamard Codebook Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Online Hashing Via Hadamard Codebook Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Supervised Online Hashing Via Hadamard Codebook Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 26th ACM international conference on Multimedia</td>
    <td>64</td>
    <td><p>In recent years, binary code learning, a.k.a hashing, has received extensive
attention in large-scale multimedia retrieval. It aims to encode
high-dimensional data points to binary codes, hence the original
high-dimensional metric space can be efficiently approximated via Hamming
space. However, most existing hashing methods adopted offline batch learning,
which is not suitable to handle incremental datasets with streaming data or new
instances. In contrast, the robustness of the existing online hashing remains
as an open problem, while the embedding of supervised/semantic information
hardly boosts the performance of the online hashing, mainly due to the defect
of unknown category numbers in supervised learning. In this paper, we proposed
an online hashing scheme, termed Hadamard Codebook based Online Hashing (HCOH),
which aims to solve the above problems towards robust and supervised online
hashing. In particular, we first assign an appropriate high-dimensional binary
codes to each class label, which is generated randomly by Hadamard codes to
each class label, which is generated randomly by Hadamard codes. Subsequently,
LSH is adopted to reduce the length of such Hadamard codes in accordance with
the hash bits, which can adapt the predefined binary codes online, and
theoretically guarantee the semantic similarity. Finally, we consider the
setting of stochastic data acquisition, which facilitates our method to
efficiently learn the corresponding hashing functions via stochastic gradient
descend (SGD) online. Notably, the proposed HCOH can be embedded with
supervised labels and it not limited to a predefined category number. Extensive
experiments on three widely-used benchmarks demonstrate the merits of the
proposed scheme over the state-of-the-art methods. The code is available at
https://github.com/lmbxmu/mycode/tree/master/2018ACMMM_HCOH.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/baranchuk2018revisiting/">Revisiting The Inverted Indices For Billion-scale Approximate Nearest Neighbors</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Revisiting The Inverted Indices For Billion-scale Approximate Nearest Neighbors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Revisiting The Inverted Indices For Billion-scale Approximate Nearest Neighbors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Baranchuk Dmitry, Babenko Artem, Malkov Yury</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>67</td>
    <td><p>This work addresses the problem of billion-scale nearest neighbor search. The
state-of-the-art retrieval systems for billion-scale databases are currently
based on the inverted multi-index, the recently proposed generalization of the
inverted index structure. The multi-index provides a very fine-grained
partition of the feature space that allows extracting concise and accurate
short-lists of candidates for the search queries. In this paper, we argue that
the potential of the simple inverted index was not fully exploited in previous
works and advocate its usage both for the highly-entangled deep descriptors and
relatively disentangled SIFT descriptors. We introduce a new retrieval system
that is based on the inverted index and outperforms the multi-index by a large
margin for the same memory consumption and construction complexity. For
example, our system achieves the state-of-the-art recall rates several times
faster on the dataset of one billion deep descriptors compared to the efficient
implementation of the inverted multi-index from the FAISS library.</p>
</td>
    <td>
      
        Similarity Search 
      
        Large Scale Search 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/aum%C3%BCller2018distance/">Distance-sensitive Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Distance-sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Distance-sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Aumüller et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 37th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems</td>
    <td>18</td>
    <td><p>Locality-sensitive hashing (LSH) is an important tool for managing
high-dimensional noisy or uncertain data, for example in connection with data
cleaning (similarity join) and noise-robust search (similarity search).
However, for a number of problems the LSH framework is not known to yield good
solutions, and instead ad hoc solutions have been designed for particular
similarity and distance measures. For example, this is true for
output-sensitive similarity search/join, and for indexes supporting annulus
queries that aim to report a point close to a certain given distance from the
query point.
  In this paper we initiate the study of distance-sensitive hashing (DSH), a
generalization of LSH that seeks a family of hash functions such that the
probability of two points having the same hash value is a given function of the
distance between them. More precisely, given a distance space \((X,
\text{dist})\) and a “collision probability function” (CPF) \(f\colon
\mathbb{R}\rightarrow [0,1]\) we seek a distribution over pairs of functions
\((h,g)\) such that for every pair of points \(x, y \in X\) the collision
probability is \(\Pr[h(x)=g(y)] = f(\text{dist}(x,y))\). Locality-sensitive
hashing is the study of how fast a CPF can decrease as the distance grows. For
many spaces, \(f\) can be made exponentially decreasing even if we restrict
attention to the symmetric case where \(g=h\). We show that the asymmetry
achieved by having a pair of functions makes it possible to achieve CPFs that
are, for example, increasing or unimodal, and show how this leads to principled
solutions to problems not addressed by the LSH framework. This includes a novel
application to privacy-preserving distance estimation. We believe that the DSH
framework will find further applications in high-dimensional data management.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/loncaric2018learning/">Learning Hash Codes Via Hamming Distance Targets</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Hash Codes Via Hamming Distance Targets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Hash Codes Via Hamming Distance Targets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Loncaric Martin, Liu Bowei, Weber Ryan</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>11</td>
    <td><p>We present a powerful new loss function and training scheme for learning
binary hash codes with any differentiable model and similarity function. Our
loss function improves over prior methods by using log likelihood loss on top
of an accurate approximation for the probability that two inputs fall within a
Hamming distance target. Our novel training scheme obtains a good estimate of
the true gradient by better sampling inputs and evaluating loss terms between
all pairs of inputs in each minibatch. To fully leverage the resulting hashes,
we use multi-indexing. We demonstrate that these techniques provide large
improvements to a similarity search tasks. We report the best results to date
on competitive information retrieval tasks for ImageNet and SIFT 1M, improving
MAP from 73% to 84% and reducing query cost by a factor of 2-8, respectively.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/talreja2018using/">Using Deep Cross Modal Hashing And Error Correcting Codes For Improving The Efficiency Of Attribute Guided Facial Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Using Deep Cross Modal Hashing And Error Correcting Codes For Improving The Efficiency Of Attribute Guided Facial Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Using Deep Cross Modal Hashing And Error Correcting Codes For Improving The Efficiency Of Attribute Guided Facial Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Talreja et al.</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP)</td>
    <td>25</td>
    <td><p>With benefits of fast query speed and low storage cost, hashing-based image
retrieval approaches have garnered considerable attention from the research
community. In this paper, we propose a novel Error-Corrected Deep Cross Modal
Hashing (CMH-ECC) method which uses a bitmap specifying the presence of certain
facial attributes as an input query to retrieve relevant face images from the
database. In this architecture, we generate compact hash codes using an
end-to-end deep learning module, which effectively captures the inherent
relationships between the face and attribute modality. We also integrate our
deep learning module with forward error correction codes to further reduce the
distance between different modalities of the same subject. Specifically, the
properties of deep hashing and forward error correction codes are exploited to
design a cross modal hashing framework with high retrieval performance.
Experimental results using two standard datasets with facial attributes-image
modalities indicate that our CMH-ECC face image retrieval model outperforms
most of the current attribute-based face image retrieval approaches.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Efficiency 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/zhou2018region/">Region Convolutional Features For Multi-label Remote Sensing Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Region Convolutional Features For Multi-label Remote Sensing Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Region Convolutional Features For Multi-label Remote Sensing Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhou Weixun, Deng Xueqing, Shao Zhenfeng</td> <!-- 🔧 You were missing this -->
    <td>International Journal of Intelligent Engineering and Systems</td>
    <td>8</td>
    <td><p>Conventional remote sensing image retrieval (RSIR) systems usually perform
single-label retrieval where each image is annotated by a single label
representing the most significant semantic content of the image. This
assumption, however, ignores the complexity of remote sensing images, where an
image might have multiple classes (i.e., multiple labels), thus resulting in
worse retrieval performance. We therefore propose a novel multi-label RSIR
approach with fully convolutional networks (FCN). In our approach, we first
train a FCN model using a pixel-wise labeled dataset,and the trained FCN is
then used to predict the segmentation maps of each image in the considered
archive. We finally extract region convolutional features of each image based
on its segmentation map.The region features can be either used to perform
region-based retrieval or further post-processed to obtain a feature vector for
similarity measure. The experimental results show that our approach achieves
state-of-the-art performance in contrast to conventional single-label and
recent multi-label RSIR approaches.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/su2018greedy/">Greedy Hash: Towards Fast Optimization For Accurate Hash Coding In CNN</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Greedy Hash: Towards Fast Optimization For Accurate Hash Coding In CNN' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Greedy Hash: Towards Fast Optimization For Accurate Hash Coding In CNN' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Su et al.</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>115</td>
    <td><p>To convert the input into binary code, hashing algorithm has been widely used for approximate nearest neighbor search on large-scale image sets due to its computation and storage efficiency. Deep hashing further improves the retrieval quality by combining the hash coding with deep neural network. However, a major difficulty in deep hashing lies in the discrete constraints imposed on the network output, which generally makes the optimization NP hard. In this work, we adopt the greedy principle to tackle this NP hard problem by iteratively updating the network toward the probable optimal discrete solution in each iteration. A hash coding layer is designed to implement our approach which strictly uses the sign function in forward propagation to maintain the discrete constraints, while in back propagation the gradients are transmitted intactly to the front layer to avoid the vanishing gradients. In addition to the theoretical derivation, we provide a new perspective to visualize and understand the effectiveness and efficiency of our algorithm. Experiments on benchmark datasets show that our scheme outperforms state-of-the-art hashing methods in both supervised and unsupervised tasks.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/novotn%C3%BD2018implementation/">Implementation Notes For The Soft Cosine Measure</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Implementation Notes For The Soft Cosine Measure' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Implementation Notes For The Soft Cosine Measure' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Novotný Vít</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 27th ACM International Conference on Information and Knowledge Management</td>
    <td>27</td>
    <td><p>The standard bag-of-words vector space model (VSM) is efficient, and
ubiquitous in information retrieval, but it underestimates the similarity of
documents with the same meaning, but different terminology. To overcome this
limitation, Sidorov et al. proposed the Soft Cosine Measure (SCM) that
incorporates term similarity relations. Charlet and Damnati showed that the SCM
is highly effective in question answering (QA) systems. However, the
orthonormalization algorithm proposed by Sidorov et al. has an impractical time
complexity of \(\mathcal O(n^4)\), where n is the size of the vocabulary.
  In this paper, we prove a tighter lower worst-case time complexity bound of
\(\mathcal O(n^3)\). We also present an algorithm for computing the similarity
between documents and we show that its worst-case time complexity is \(\mathcal
O(1)\) given realistic conditions. Lastly, we describe implementation in
general-purpose vector databases such as Annoy, and Faiss and in the inverted
indices of text search engines such as Apache Lucene, and ElasticSearch. Our
results enable the deployment of the SCM in real-world information retrieval
systems.</p>
</td>
    <td>
      
        CIKM 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/bouma2018individual/">Individual Common Dolphin Identification Via Metric Embedding Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Individual Common Dolphin Identification Via Metric Embedding Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Individual Common Dolphin Identification Via Metric Embedding Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Bouma et al.</td> <!-- 🔧 You were missing this -->
    <td>2018 International Conference on Image and Vision Computing New Zealand (IVCNZ)</td>
    <td>36</td>
    <td><p>Photo-identification (photo-id) of dolphin individuals is a commonly used
technique in ecological sciences to monitor state and health of individuals, as
well as to study the social structure and distribution of a population.
Traditional photo-id involves a laborious manual process of matching each
dolphin fin photograph captured in the field to a catalogue of known
individuals.
  We examine this problem in the context of open-set recognition and utilise a
triplet loss function to learn a compact representation of fin images in a
Euclidean embedding, where the Euclidean distance metric represents fin
similarity. We show that this compact representation can be successfully learnt
from a fairly small (in deep learning context) training set and still
generalise well to out-of-sample identities (completely new dolphin
individuals), with top-1 and top-5 test set (37 individuals) accuracy of
\(90.5\pm2\) and \(93.6\pm1\) percent. In the presence of 1200 distractors, top-1
accuracy dropped by \(12%\); however, top-5 accuracy saw only a \(2.8%\) drop</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/sun2018part/">Part-based Multi-stream Model For Vehicle Searching</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Part-based Multi-stream Model For Vehicle Searching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Part-based Multi-stream Model For Vehicle Searching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sun Ya, Li Minxian, Lu Jianfeng</td> <!-- 🔧 You were missing this -->
    <td>2018 24th International Conference on Pattern Recognition (ICPR)</td>
    <td>10</td>
    <td><p>Due to the enormous requirement in public security and intelligent
transportation system, searching an identical vehicle has become more and more
important. Current studies usually treat vehicle as an integral object and then
train a distance metric to measure the similarity among vehicles. However,
these raw images may be exactly similar to ones with different identification
and include some pixels in background that may disturb the distance metric
learning. In this paper, we propose a novel and useful method to segment an
original vehicle image into several discriminative foreground parts, and these
parts consist of some fine grained regions that are named discriminative
patches. After that, these parts combined with the raw image are fed into the
proposed deep learning network. We can easily measure the similarity of two
vehicle images by computing the Euclidean distance of the features from FC
layer. Two main contributions of this paper are as follows. Firstly, a method
is proposed to estimate if a patch in a raw vehicle image is discriminative or
not. Secondly, a new Part-based Multi-Stream Model (PMSM) is designed and
optimized for vehicle retrieval and re-identification tasks. We evaluate the
proposed method on the VehicleID dataset, and the experimental results show
that our method can outperform the baseline.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/xuan2018deep/">Deep Randomized Ensembles For Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Randomized Ensembles For Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Randomized Ensembles For Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xuan Hong, Souvenir Richard, Pless Robert</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>124</td>
    <td><p>Learning embedding functions, which map semantically related inputs to nearby
locations in a feature space supports a variety of classification and
information retrieval tasks. In this work, we propose a novel, generalizable
and fast method to define a family of embedding functions that can be used as
an ensemble to give improved results. Each embedding function is learned by
randomly bagging the training labels into small subsets. We show experimentally
that these embedding ensembles create effective embedding functions. The
ensemble output defines a metric space that improves state of the art
performance for image retrieval on CUB-200-2011, Cars-196, In-Shop Clothes
Retrieval and VehicleID.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/sharma2018improving/">Improving Similarity Search With High-dimensional Locality-sensitive Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Improving Similarity Search With High-dimensional Locality-sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Improving Similarity Search With High-dimensional Locality-sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sharma Jaiyam, Navlakha Saket</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>6</td>
    <td><p>We propose a new class of data-independent locality-sensitive hashing (LSH)
algorithms based on the fruit fly olfactory circuit. The fundamental difference
of this approach is that, instead of assigning hashes as dense points in a low
dimensional space, hashes are assigned in a high dimensional space, which
enhances their separability. We show theoretically and empirically that this
new family of hash functions is locality-sensitive and preserves rank
similarity for inputs in any `p space. We then analyze different variations on
this strategy and show empirically that they outperform existing LSH methods
for nearest-neighbors search on six benchmark datasets. Finally, we propose a
multi-probe version of our algorithm that achieves higher performance for the
same query time, or conversely, that maintains performance of prior approaches
while taking significantly less indexing time and memory. Overall, our approach
leverages the advantages of separability provided by high-dimensional spaces,
while still remaining computationally efficient</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Similarity Search 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/abdulahhad2018concept/">Concept Embedding For Information Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Concept Embedding For Information Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Concept Embedding For Information Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Abdulahhad Karam</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications</td>
    <td>23</td>
    <td><p>Concepts are used to solve the term-mismatch problem. However, we need an
effective similarity measure between concepts. Word embedding presents a
promising solution. We present in this study three approaches to build concepts
vectors based on words vectors. We use a vector-based measure to estimate
inter-concepts similarity. Our experiments show promising results. Furthermore,
words and concepts become comparable. This could be used to improve conceptual
indexing process.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/thoreau2018deep/">Deep Similarity Metric Learning For Real-time Pedestrian Tracking</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Similarity Metric Learning For Real-time Pedestrian Tracking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Similarity Metric Learning For Real-time Pedestrian Tracking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Thoreau Michael, Kottege Navinda</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>Tracking by detection is a common approach to solving the Multiple Object
Tracking problem. In this paper we show how learning a deep similarity metric
can improve three key aspects of pedestrian tracking on a multiple object
tracking benchmark. We train a convolutional neural network to learn an
embedding function in a Siamese configuration on a large person
re-identification dataset. The offline-trained embedding network is integrated
in to the tracking formulation to improve performance while retaining real-time
performance. The proposed tracker stores appearance metrics while detections
are strong, using this appearance information to: prevent ID switches,
associate tracklets through occlusion, and propose new detections where
detector confidence is low. This method achieves competitive results in
evaluation, especially among online, real-time approaches. We present an
ablative study showing the impact of each of the three uses of our deep
appearance metric.</p>
</td>
    <td>
      
        Efficiency 
      
        Distance Metric Learning 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/qi2018accurate/">Accurate And Efficient Similarity Search For Large Scale Face Recognition</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Accurate And Efficient Similarity Search For Large Scale Face Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Accurate And Efficient Similarity Search For Large Scale Face Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Qi Ce, Liu Zhizhong, Su Fei</td> <!-- 🔧 You were missing this -->
    <td>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>12</td>
    <td><p>Face verification is a relatively easy task with the help of discriminative
features from deep neural networks. However, it is still a challenge to
recognize faces on millions of identities while keeping high performance and
efficiency. The challenge 2 of MS-Celeb-1M is a classification task. However,
the number of identities is too large and it is not that elegant to treat the
task as an image classification task. We treat the classification task as
similarity search and do experiments on different similarity search strategies.
Similarity search strategy accelerates the speed of searching and boosts the
accuracy of final results. The model used for extracting features is a single
deep neural network pretrained on CASIA-Webface, which is not trained on the
base set or novel set offered by official. Finally, we rank \textbf{3rd}, while
the speed of searching is 1ms/image.</p>
</td>
    <td>
      
        Similarity Search 
      
        SCALABILITY 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/zhang2018unsupervised/">Unsupervised Generative Adversarial Cross-modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Generative Adversarial Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Generative Adversarial Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Jian, Peng Yuxin, Yuan Mingkuan</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>203</td>
    <td><p>Cross-modal hashing aims to map heterogeneous multimedia data into a common
Hamming space, which can realize fast and flexible retrieval across different
modalities. Unsupervised cross-modal hashing is more flexible and applicable
than supervised methods, since no intensive labeling work is involved. However,
existing unsupervised methods learn hashing functions by preserving inter and
intra correlations, while ignoring the underlying manifold structure across
different modalities, which is extremely helpful to capture meaningful nearest
neighbors of different modalities for cross-modal retrieval. To address the
above problem, in this paper we propose an Unsupervised Generative Adversarial
Cross-modal Hashing approach (UGACH), which makes full use of GAN’s ability for
unsupervised representation learning to exploit the underlying manifold
structure of cross-modal data. The main contributions can be summarized as
follows: (1) We propose a generative adversarial network to model cross-modal
hashing in an unsupervised fashion. In the proposed UGACH, given a data of one
modality, the generative model tries to fit the distribution over the manifold
structure, and select informative data of another modality to challenge the
discriminative model. The discriminative model learns to distinguish the
generated data and the true positive data sampled from correlation graph to
achieve better retrieval accuracy. These two models are trained in an
adversarial way to improve each other and promote hashing function learning.
(2) We propose a correlation graph based approach to capture the underlying
manifold structure across different modalities, so that data of different
modalities but within the same manifold can have smaller Hamming distance and
promote retrieval accuracy. Extensive experiments compared with 6
state-of-the-art methods verify the effectiveness of our proposed approach.</p>
</td>
    <td>
      
        Robustness 
      
        Unsupervised 
      
        AAAI 
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/li2018self/">Self-supervised Adversarial Hashing Networks For Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Self-supervised Adversarial Hashing Networks For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Self-supervised Adversarial Hashing Networks For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li et al.</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</td>
    <td>426</td>
    <td><p>Thanks to the success of deep learning, cross-modal retrieval has made
significant progress recently. However, there still remains a crucial
bottleneck: how to bridge the modality gap to further enhance the retrieval
accuracy. In this paper, we propose a self-supervised adversarial hashing
(\textbf{SSAH}) approach, which lies among the early attempts to incorporate
adversarial learning into cross-modal hashing in a self-supervised fashion. The
primary contribution of this work is that two adversarial networks are
leveraged to maximize the semantic correlation and consistency of the
representations between different modalities. In addition, we harness a
self-supervised semantic network to discover high-level semantic information in
the form of multi-label annotations. Such information guides the feature
learning process and preserves the modality relationships in both the common
semantic space and the Hamming space. Extensive experiments carried out on
three benchmark datasets validate that the proposed SSAH surpasses the
state-of-the-art methods.</p>
</td>
    <td>
      
        Self SUPERVISED 
      
        Robustness 
      
        CVPR 
      
        SUPERVISED 
      
        Multimodal Retrieval 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/verma2018diversity/">Diversity In Fashion Recommendation Using Semantic Parsing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Diversity In Fashion Recommendation Using Semantic Parsing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Diversity In Fashion Recommendation Using Semantic Parsing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Verma et al.</td> <!-- 🔧 You were missing this -->
    <td>2018 25th IEEE International Conference on Image Processing (ICIP)</td>
    <td>25</td>
    <td><p>Developing recommendation system for fashion images is challenging due to the
inherent ambiguity associated with what criterion a user is looking at.
Suggesting multiple images where each output image is similar to the query
image on the basis of a different feature or part is one way to mitigate the
problem. Existing works for fashion recommendation have used Siamese or Triplet
network to learn features between a similar pair and a similar-dissimilar
triplet respectively. However, these methods do not provide basic information
such as, how two clothing images are similar, or which parts present in the two
images make them similar. In this paper, we propose to recommend images by
explicitly learning and exploiting part based similarity. We propose a novel
approach of learning discriminative features from weakly-supervised data by
using visual attention over the parts and a texture encoding network. We show
that the learned features surpass the state-of-the-art in retrieval task on
DeepFashion dataset. We then use the proposed model to recommend fashion images
having an explicit variation with respect to similarity of any of the parts.</p>
</td>
    <td>
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/liu2018deep/">Deep Triplet Quantization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Triplet Quantization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Triplet Quantization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 26th ACM international conference on Multimedia</td>
    <td>106</td>
    <td><p>Deep hashing establishes efficient and effective image retrieval by
end-to-end learning of deep representations and hash codes from similarity
data. We present a compact coding solution, focusing on deep learning to
quantization approach that has shown superior performance over hashing
solutions for similarity retrieval. We propose Deep Triplet Quantization (DTQ),
a novel approach to learning deep quantization models from the similarity
triplets. To enable more effective triplet training, we design a new triplet
selection approach, Group Hard, that randomly selects hard triplets in each
image group. To generate compact binary codes, we further apply a triplet
quantization with weak orthogonality during triplet training. The quantization
loss reduces the codebook redundancy and enhances the quantizability of deep
representations through back-propagation. Extensive experiments demonstrate
that DTQ can generate high-quality and compact binary codes, which yields
state-of-the-art image retrieval performance on three benchmark datasets,
NUS-WIDE, CIFAR-10, and MS-COCO.</p>
</td>
    <td>
      
        Quantization 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/tharani2018unsupervised/">Unsupervised Deep Features For Remote Sensing Image Matching Via Discriminator Network</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Deep Features For Remote Sensing Image Matching Via Discriminator Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Deep Features For Remote Sensing Image Matching Via Discriminator Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tharani Mohbat, Khurshid Numan, Taj Murtaza</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>The advent of deep perceptual networks brought about a paradigm shift in
machine vision and image perception. Image apprehension lately carried out by
hand-crafted features in the latent space have been replaced by deep features
acquired from supervised networks for improved understanding. However, such
deep networks require strict supervision with a substantial amount of the
labeled data for authentic training process. These methods perform poorly in
domains lacking labeled data especially in case of remote sensing image
retrieval. Resolving this, we propose an unsupervised encoder-decoder feature
for remote sensing image matching (RSIM). Moreover, we replace the conventional
distance metrics with a deep discriminator network to identify the similarity
of the image pairs. To the best of our knowledge, discriminator network has
never been used before for solving RSIM problem. Results have been validated
with two publicly available benchmark remote sensing image datasets. The
technique has also been investigated for content-based remote sensing image
retrieval (CBRSIR); one of the widely used applications of RSIM. Results
demonstrate that our technique supersedes the state-of-the-art methods used for
unsupervised image matching with mean average precision (mAP) of 81%, and image
retrieval with an overall improvement in mAP score of about 12%.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/jin2018deep/">Deep Ordinal Hashing With Spatial Attention</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Ordinal Hashing With Spatial Attention' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Ordinal Hashing With Spatial Attention' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jin et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>38</td>
    <td><p>Hashing has attracted increasing research attentions in recent years due to
its high efficiency of computation and storage in image retrieval. Recent works
have demonstrated the superiority of simultaneous feature representations and
hash functions learning with deep neural networks. However, most existing deep
hashing methods directly learn the hash functions by encoding the global
semantic information, while ignoring the local spatial information of images.
The loss of local spatial structure makes the performance bottleneck of hash
functions, therefore limiting its application for accurate similarity
retrieval. In this work, we propose a novel Deep Ordinal Hashing (DOH) method,
which learns ordinal representations by leveraging the ranking structure of
feature space from both local and global views. In particular, to effectively
build the ranking structure, we propose to learn the rank correlation space by
exploiting the local spatial information from Fully Convolutional Network (FCN)
and the global semantic information from the Convolutional Neural Network (CNN)
simultaneously. More specifically, an effective spatial attention model is
designed to capture the local spatial information by selectively learning
well-specified locations closely related to target objects. In such hashing
framework,the local spatial and global semantic nature of images are captured
in an end-to-end ranking-to-hashing manner. Experimental results conducted on
three widely-used datasets demonstrate that the proposed DOH method
significantly outperforms the state-of-the-art hashing methods.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/he2018local/">Local Descriptors Optimized For Average Precision</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Local Descriptors Optimized For Average Precision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Local Descriptors Optimized For Average Precision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>He Kun, Lu Yan, Sclaroff Stan</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</td>
    <td>209</td>
    <td><p>Extraction of local feature descriptors is a vital stage in the solution
pipelines for numerous computer vision tasks. Learning-based approaches improve
performance in certain tasks, but still cannot replace handcrafted features in
general. In this paper, we improve the learning of local feature descriptors by
optimizing the performance of descriptor matching, which is a common stage that
follows descriptor extraction in local feature based pipelines, and can be
formulated as nearest neighbor retrieval. Specifically, we directly optimize a
ranking-based retrieval performance metric, Average Precision, using deep
neural networks. This general-purpose solution can also be viewed as a listwise
learning to rank approach, which is advantageous compared to recent local
ranking approaches. On standard benchmarks, descriptors learned with our
formulation achieve state-of-the-art results in patch verification, patch
retrieval, and image matching.</p>
</td>
    <td>
      
        Evaluation 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/he2018hashing/">Hashing As Tie-aware Learning To Rank</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing As Tie-aware Learning To Rank' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing As Tie-aware Learning To Rank' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>He et al.</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</td>
    <td>89</td>
    <td><p>Hashing, or learning binary embeddings of data, is frequently used in nearest
neighbor retrieval. In this paper, we develop learning to rank formulations for
hashing, aimed at directly optimizing ranking-based evaluation metrics such as
Average Precision (AP) and Normalized Discounted Cumulative Gain (NDCG). We
first observe that the integer-valued Hamming distance often leads to tied
rankings, and propose to use tie-aware versions of AP and NDCG to evaluate
hashing for retrieval. Then, to optimize tie-aware ranking metrics, we derive
their continuous relaxations, and perform gradient-based optimization with deep
neural networks. Our results establish the new state-of-the-art for image
retrieval by Hamming ranking in common benchmarks.</p>
</td>
    <td>
      
        Hashing Methods 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/shan2018recurrent/">Recurrent Binary Embedding For Gpu-enabled Exhaustive Retrieval From Billion-scale Semantic Vectors</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Recurrent Binary Embedding For Gpu-enabled Exhaustive Retrieval From Billion-scale Semantic Vectors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Recurrent Binary Embedding For Gpu-enabled Exhaustive Retrieval From Billion-scale Semantic Vectors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shan et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</td>
    <td>10</td>
    <td><p>Rapid advances in GPU hardware and multiple areas of Deep Learning open up a
new opportunity for billion-scale information retrieval with exhaustive search.
Building on top of the powerful concept of semantic learning, this paper
proposes a Recurrent Binary Embedding (RBE) model that learns compact
representations for real-time retrieval. The model has the unique ability to
refine a base binary vector by progressively adding binary residual vectors to
meet the desired accuracy. The refined vector enables efficient implementation
of exhaustive similarity computation with bit-wise operations, followed by a
near- lossless k-NN selection algorithm, also proposed in this paper. The
proposed algorithms are integrated into an end-to-end multi-GPU system that
retrieves thousands of top items from over a billion candidates in real-time.
The RBE model and the retrieval system were evaluated with data from a major
paid search engine. When measured against the state-of-the-art model for binary
representation and the full precision model for semantic embedding, RBE
significantly outperformed the former, and filled in over 80% of the AUC gap
in-between. Experiments comparing with our production retrieval system also
demonstrated superior performance. While the primary focus of this paper is to
build RBE based on a particular class of semantic models, generalizing to other
types is straightforward, as exemplified by two different models at the end of
the paper.</p>
</td>
    <td>
      
        KDD 
      
        Hashing Methods 
      
        Large Scale Search 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/mithun2018webly/">Webly Supervised Joint Embedding For Cross-modal Image-text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Webly Supervised Joint Embedding For Cross-modal Image-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Webly Supervised Joint Embedding For Cross-modal Image-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mithun et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 26th ACM international conference on Multimedia</td>
    <td>73</td>
    <td><p>Cross-modal retrieval between visual data and natural language description
remains a long-standing challenge in multimedia. While recent image-text
retrieval methods offer great promise by learning deep representations aligned
across modalities, most of these methods are plagued by the issue of training
with small-scale datasets covering a limited number of images with ground-truth
sentences. Moreover, it is extremely expensive to create a larger dataset by
annotating millions of images with sentences and may lead to a biased model.
Inspired by the recent success of webly supervised learning in deep neural
networks, we capitalize on readily-available web images with noisy annotations
to learn robust image-text joint representation. Specifically, our main idea is
to leverage web images and corresponding tags, along with fully annotated
datasets, in training for learning the visual-semantic joint embedding. We
propose a two-stage approach for the task that can augment a typical supervised
pair-wise ranking loss based formulation with weakly-annotated web images to
learn a more robust visual-semantic embedding. Experiments on two standard
benchmark datasets demonstrate that our method achieves a significant
performance gain in image-text retrieval compared to state-of-the-art
approaches.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/g%C3%B3mez2018single/">Single Shot Scene Text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Single Shot Scene Text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Single Shot Scene Text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gómez et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>54</td>
    <td><p>Textual information found in scene images provides high level semantic
information about the image and its context and it can be leveraged for better
scene understanding. In this paper we address the problem of scene text
retrieval: given a text query, the system must return all images containing the
queried text. The novelty of the proposed model consists in the usage of a
single shot CNN architecture that predicts at the same time bounding boxes and
a compact text representation of the words in them. In this way, the text based
image retrieval task can be casted as a simple nearest neighbor search of the
query text representation over the outputs of the CNN over the entire image
database. Our experiments demonstrate that the proposed architecture
outperforms previous state-of-the-art while it offers a significant increase in
processing speed.</p>
</td>
    <td>
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/manocha2018content/">Content-based Representations Of Audio Using Siamese Neural Networks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Content-based Representations Of Audio Using Siamese Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Content-based Representations Of Audio Using Siamese Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Manocha et al.</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</td>
    <td>38</td>
    <td><p>In this paper, we focus on the problem of content-based retrieval for audio,
which aims to retrieve all semantically similar audio recordings for a given
audio clip query. This problem is similar to the problem of query by example of
audio, which aims to retrieve media samples from a database, which are similar
to the user-provided example. We propose a novel approach which encodes the
audio into a vector representation using Siamese Neural Networks. The goal is
to obtain an encoding similar for files belonging to the same audio class, thus
allowing retrieval of semantically similar audio. Using simple similarity
measures such as those based on simple euclidean distance and cosine similarity
we show that these representations can be very effectively used for retrieving
recordings similar in audio content.</p>
</td>
    <td>
      
        ICASSP 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/misra2018bernoulli/">Bernoulli Embeddings For Graphs</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Bernoulli Embeddings For Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Bernoulli Embeddings For Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Misra Vinith, Bhatia Sumit</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>9</td>
    <td><p>Just as semantic hashing can accelerate information retrieval, binary valued
embeddings can significantly reduce latency in the retrieval of graphical data.
We introduce a simple but effective model for learning such binary vectors for
nodes in a graph. By imagining the embeddings as independent coin flips of
varying bias, continuous optimization techniques can be applied to the
approximate expected loss. Embeddings optimized in this fashion consistently
outperform the quantization of both spectral graph embeddings and various
learned real-valued embeddings, on both ranking and pre-ranking tasks for a
variety of datasets.</p>
</td>
    <td>
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/meyer2018deep/">Deep Metric Learning And Image Classification With Nearest Neighbour Gaussian Kernels</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Metric Learning And Image Classification With Nearest Neighbour Gaussian Kernels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Metric Learning And Image Classification With Nearest Neighbour Gaussian Kernels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Meyer Benjamin J., Harwood Ben, Drummond Tom</td> <!-- 🔧 You were missing this -->
    <td>2018 25th IEEE International Conference on Image Processing (ICIP)</td>
    <td>29</td>
    <td><p>We present a Gaussian kernel loss function and training algorithm for
convolutional neural networks that can be directly applied to both distance
metric learning and image classification problems. Our method treats all
training features from a deep neural network as Gaussian kernel centres and
computes loss by summing the influence of a feature’s nearby centres in the
feature embedding space. Our approach is made scalable by treating it as an
approximate nearest neighbour search problem. We show how to make end-to-end
learning feasible, resulting in a well formed embedding space, in which
semantically related instances are likely to be located near one another,
regardless of whether or not the network was trained on those classes. Our
approach outperforms state-of-the-art deep metric learning approaches on
embedding learning challenges, as well as conventional softmax classification
on several datasets.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/pang2018deep/">Deep Feature Aggregation And Image Re-ranking With Heat Diffusion For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Feature Aggregation And Image Re-ranking With Heat Diffusion For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Feature Aggregation And Image Re-ranking With Heat Diffusion For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Pang et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>40</td>
    <td><p>Image retrieval based on deep convolutional features has demonstrated
state-of-the-art performance in popular benchmarks. In this paper, we present a
unified solution to address deep convolutional feature aggregation and image
re-ranking by simulating the dynamics of heat diffusion. A distinctive problem
in image retrieval is that repetitive or <em>bursty</em> features tend to
dominate final image representations, resulting in representations less
distinguishable. We show that by considering each deep feature as a heat
source, our unsupervised aggregation method is able to avoid
over-representation of <em>bursty</em> features. We additionally provide a
practical solution for the proposed aggregation method and further show the
efficiency of our method in experimental evaluation. Inspired by the
aforementioned deep feature aggregation method, we also propose a method to
re-rank a number of top ranked images for a given query image by considering
the query as the heat source. Finally, we extensively evaluate the proposed
approach with pre-trained and fine-tuned deep networks on common public
benchmarks and show superior performance compared to previous work.</p>
</td>
    <td>
      
        Hybrid ANN Methods 
      
        Image Retrieval 
      
        Re RANKING 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/gu2018attention/">Attention-aware Generalized Mean Pooling For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Attention-aware Generalized Mean Pooling For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Attention-aware Generalized Mean Pooling For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gu Yinzheng, Li Chuanpeng, Xie Jinbin</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>28</td>
    <td><p>It has been shown that image descriptors extracted by convolutional neural
networks (CNNs) achieve remarkable results for retrieval problems. In this
paper, we apply attention mechanism to CNN, which aims at enhancing more
relevant features that correspond to important keypoints in the input image.
The generated attention-aware features are then aggregated by the previous
state-of-the-art generalized mean (GeM) pooling followed by normalization to
produce a compact global descriptor, which can be efficiently compared to other
image descriptors by the dot product. An extensive comparison of our proposed
approach with state-of-the-art methods is performed on the new challenging
ROxford5k and RParis6k retrieval benchmarks. Results indicate significant
improvement over previous work. In particular, our attention-aware GeM (AGeM)
descriptor outperforms state-of-the-art method on ROxford5k under the `Hard’
evaluation protocal.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/shen2018nash/">NASH: Toward End-to-end Neural Architecture For Generative Semantic Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=NASH: Toward End-to-end Neural Architecture For Generative Semantic Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=NASH: Toward End-to-end Neural Architecture For Generative Semantic Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shen et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</td>
    <td>65</td>
    <td><p>Semantic hashing has become a powerful paradigm for fast similarity search in
many information retrieval systems. While fairly successful, previous
techniques generally require two-stage training, and the binary constraints are
handled ad-hoc. In this paper, we present an end-to-end Neural Architecture for
Semantic Hashing (NASH), where the binary hashing codes are treated as
Bernoulli latent variables. A neural variational inference framework is
proposed for training, where gradients are directly back-propagated through the
discrete latent variable to optimize the hash function. We also draw
connections between proposed method and rate-distortion theory, which provides
a theoretical foundation for the effectiveness of the proposed framework.
Experimental results on three public datasets demonstrate that our method
significantly outperforms several state-of-the-art models on both unsupervised
and supervised scenarios.</p>
</td>
    <td>
      
        ACL 
      
        Text Retrieval 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/jiang2018deep/">Deep Discrete Supervised Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Discrete Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Discrete Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jiang Qing-yuan, Cui Xue, Li Wu-jun</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>65</td>
    <td><p>Hashing has been widely used for large-scale search due to its low storage
cost and fast query speed. By using supervised information, supervised hashing
can significantly outperform unsupervised hashing. Recently, discrete
supervised hashing and deep hashing are two representative progresses in
supervised hashing. On one hand, hashing is essentially a discrete optimization
problem. Hence, utilizing supervised information to directly guide discrete
(binary) coding procedure can avoid sub-optimal solution and improve the
accuracy. On the other hand, deep hashing, which integrates deep feature
learning and hash-code learning into an end-to-end architecture, can enhance
the feedback between feature learning and hash-code learning. The key in
discrete supervised hashing is to adopt supervised information to directly
guide the discrete coding procedure in hashing. The key in deep hashing is to
adopt the supervised information to directly guide the deep feature learning
procedure. However, there have not existed works which can use the supervised
information to directly guide both discrete coding procedure and deep feature
learning procedure in the same framework. In this paper, we propose a novel
deep hashing method, called deep discrete supervised hashing (DDSH), to address
this problem. DDSH is the first deep hashing method which can utilize
supervised information to directly guide both discrete coding procedure and
deep feature learning procedure, and thus enhance the feedback between these
two important procedures. Experiments on three real datasets show that DDSH can
outperform other state-of-the-art baselines, including both discrete hashing
and deep hashing baselines, for image retrieval.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        SUPERVISED 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/wang2018supervised/">Supervised Deep Hashing For Hierarchical Labeled Data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Deep Hashing For Hierarchical Labeled Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Supervised Deep Hashing For Hierarchical Labeled Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>26</td>
    <td><p>Recently, hashing methods have been widely used in large-scale image
retrieval. However, most existing hashing methods did not consider the
hierarchical relation of labels, which means that they ignored the rich
information stored in the hierarchy. Moreover, most of previous works treat
each bit in a hash code equally, which does not meet the scenario of
hierarchical labeled data. In this paper, we propose a novel deep hashing
method, called supervised hierarchical deep hashing (SHDH), to perform hash
code learning for hierarchical labeled data. Specifically, we define a novel
similarity formula for hierarchical labeled data by weighting each layer, and
design a deep convolutional neural network to obtain a hash code for each data
point. Extensive experiments on several real-world public datasets show that
the proposed method outperforms the state-of-the-art baselines in the image
retrieval task.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        SUPERVISED 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/gripon2018associative/">Associative Memories To Accelerate Approximate Nearest Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Associative Memories To Accelerate Approximate Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Associative Memories To Accelerate Approximate Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gripon Vincent, Löwe Matthias, Vermet Franck</td> <!-- 🔧 You were missing this -->
    <td>Applied Sciences</td>
    <td>9</td>
    <td><p>Nearest neighbor search is a very active field in machine learning for it
appears in many application cases, including classification and object
retrieval. In its canonical version, the complexity of the search is linear
with both the dimension and the cardinal of the collection of vectors the
search is performed in. Recently many works have focused on reducing the
dimension of vectors using quantization techniques or hashing, while providing
an approximate result. In this paper we focus instead on tackling the cardinal
of the collection of vectors. Namely, we introduce a technique that partitions
the collection of vectors and stores each part in its own associative memory.
When a query vector is given to the system, associative memories are polled to
identify which one contain the closest match. Then an exhaustive search is
conducted only on the part of vectors stored in the selected associative
memory. We study the effectiveness of the system when messages to store are
generated from i.i.d. uniform \(\pm\)1 random variables or 0-1 sparse i.i.d.
random variables. We also conduct experiment on both synthetic data and real
data and show it is possible to achieve interesting trade-offs between
complexity and accuracy.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/ying2018graph/">Graph Convolutional Neural Networks For Web-scale Recommender Systems</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Graph Convolutional Neural Networks For Web-scale Recommender Systems' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Graph Convolutional Neural Networks For Web-scale Recommender Systems' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ying et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</td>
    <td>3167</td>
    <td><p>Recent advancements in deep neural networks for graph-structured data have
led to state-of-the-art performance on recommender system benchmarks. However,
making these methods practical and scalable to web-scale recommendation tasks
with billions of items and hundreds of millions of users remains a challenge.
Here we describe a large-scale deep recommendation engine that we developed and
deployed at Pinterest. We develop a data-efficient Graph Convolutional Network
(GCN) algorithm PinSage, which combines efficient random walks and graph
convolutions to generate embeddings of nodes (i.e., items) that incorporate
both graph structure as well as node feature information. Compared to prior GCN
approaches, we develop a novel method based on highly efficient random walks to
structure the convolutions and design a novel training strategy that relies on
harder-and-harder training examples to improve robustness and convergence of
the model. We also develop an efficient MapReduce model inference algorithm to
generate embeddings using a trained model. We deploy PinSage at Pinterest and
train it on 7.5 billion examples on a graph with 3 billion nodes representing
pins and boards, and 18 billion edges. According to offline metrics, user
studies and A/B tests, PinSage generates higher-quality recommendations than
comparable deep learning and graph-based alternatives. To our knowledge, this
is the largest application of deep graph embeddings to date and paves the way
for a new generation of web-scale recommender systems based on graph
convolutional architectures.</p>
</td>
    <td>
      
        KDD 
      
        Large Scale Search 
      
        Recommender Systems 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/pisov2018brain/">Brain Tumor Image Retrieval Via Multitask Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Brain Tumor Image Retrieval Via Multitask Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Brain Tumor Image Retrieval Via Multitask Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Pisov et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>Classification-based image retrieval systems are built by training
convolutional neural networks (CNNs) on a relevant classification problem and
using the distance in the resulting feature space as a similarity metric.
However, in practical applications, it is often desirable to have
representations which take into account several aspects of the data (e.g.,
brain tumor type and its localization). In our work, we extend the
classification-based approach with multitask learning: we train a CNN on brain
MRI scans with heterogeneous labels and implement a corresponding tumor image
retrieval system. We validate our approach on brain tumor data which contains
information about tumor types, shapes and localization. We show that our method
allows us to build representations that contain more relevant information about
tumors than single-task classification-based approaches.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/qiu2018foresthash/">Foresthash: Semantic Hashing With Shallow Random Forests And Tiny Convolutional Networks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Foresthash: Semantic Hashing With Shallow Random Forests And Tiny Convolutional Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Foresthash: Semantic Hashing With Shallow Random Forests And Tiny Convolutional Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Qiu et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>8</td>
    <td><p>Hash codes are efficient data representations for coping with the ever
growing amounts of data. In this paper, we introduce a random forest semantic
hashing scheme that embeds tiny convolutional neural networks (CNN) into
shallow random forests, with near-optimal information-theoretic code
aggregation among trees. We start with a simple hashing scheme, where random
trees in a forest act as hashing functions by setting <code class="language-plaintext highlighter-rouge">1' for the visited tree
leaf, and </code>0’ for the rest. We show that traditional random forests fail to
generate hashes that preserve the underlying similarity between the trees,
rendering the random forests approach to hashing challenging. To address this,
we propose to first randomly group arriving classes at each tree split node
into two groups, obtaining a significantly simplified two-class classification
problem, which can be handled using a light-weight CNN weak learner. Such
random class grouping scheme enables code uniqueness by enforcing each class to
share its code with different classes in different trees. A non-conventional
low-rank loss is further adopted for the CNN weak learners to encourage code
consistency by minimizing intra-class variations and maximizing inter-class
distance for the two random class groups. Finally, we introduce an
information-theoretic approach for aggregating codes of individual trees into a
single hash code, producing a near-optimal unique hash for each class. The
proposed approach significantly outperforms state-of-the-art hashing methods
for image retrieval tasks on large-scale public datasets, while performing at
the level of other state-of-the-art image classification techniques while
utilizing a more compact and efficient scalable representation. This work
proposes a principled and robust procedure to train and deploy in parallel an
ensemble of light-weight CNNs, instead of simply going deeper.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/yu2018semi/">Semi-supervised Hashing For Semi-paired Cross-view Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semi-supervised Hashing For Semi-paired Cross-view Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semi-supervised Hashing For Semi-paired Cross-view Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yu Jun, Wu Xiao-jun, Kittler Josef</td> <!-- 🔧 You were missing this -->
    <td>2018 24th International Conference on Pattern Recognition (ICPR)</td>
    <td>10</td>
    <td><p>Recently, hashing techniques have gained importance in large-scale retrieval
tasks because of their retrieval speed. Most of the existing cross-view
frameworks assume that data are well paired. However, the fully-paired
multiview situation is not universal in real applications. The aim of the
method proposed in this paper is to learn the hashing function for semi-paired
cross-view retrieval tasks. To utilize the label information of partial data,
we propose a semi-supervised hashing learning framework which jointly performs
feature extraction and classifier learning. The experimental results on two
datasets show that our method outperforms several state-of-the-art methods in
terms of retrieval accuracy.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        SUPERVISED 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/gillick2018end/">End-to-end Retrieval In Continuous Space</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=End-to-end Retrieval In Continuous Space' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=End-to-end Retrieval In Continuous Space' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gillick Daniel, Presta Alessandro, Tomar Gaurav Singh</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>81</td>
    <td><p>Most text-based information retrieval (IR) systems index objects by words or
phrases. These discrete systems have been augmented by models that use
embeddings to measure similarity in continuous space. But continuous-space
models are typically used just to re-rank the top candidates. We consider the
problem of end-to-end continuous retrieval, where standard approximate nearest
neighbor (ANN) search replaces the usual discrete inverted index, and rely
entirely on distances between learned embeddings. By training simple models
specifically for retrieval, with an appropriate model architecture, we improve
on a discrete baseline by 8% and 26% (MAP) on two similar-question retrieval
tasks. We also discuss the problem of evaluation for retrieval systems, and
show how to modify existing pairwise similarity datasets for this purpose.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/shen2018unsupervised/">Unsupervised Deep Hashing With Similarity-adaptive And Discrete Optimization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Deep Hashing With Similarity-adaptive And Discrete Optimization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Deep Hashing With Similarity-adaptive And Discrete Optimization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shen et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>377</td>
    <td><p>Recent vision and learning studies show that learning compact hash codes can facilitate massive data processing
with significantly reduced storage and computation. Particularly, learning deep hash functions has greatly improved the retrieval
performance, typically under the semantic supervision. In contrast, current unsupervised deep hashing algorithms can hardly achieve
satisfactory performance due to either the relaxed optimization or absence of similarity-sensitive objective. In this work, we propose a
simple yet effective unsupervised hashing framework, named Similarity-Adaptive Deep Hashing (SADH), which alternatingly proceeds
over three training modules: deep hash model training, similarity graph updating and binary code optimization. The key difference from
the widely-used two-step hashing method is that the output representations of the learned deep model help update the similarity graph
matrix, which is then used to improve the subsequent code optimization. In addition, for producing high-quality binary codes, we devise
an effective discrete optimization algorithm which can directly handle the binary constraints with a general hashing loss. Extensive
experiments validate the efficacy of SADH, which consistently outperforms the state-of-the-arts by large gaps.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        SUPERVISED 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/shen2018zero/">Zero-shot Sketch-image Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Zero-shot Sketch-image Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Zero-shot Sketch-image Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shen et al.</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</td>
    <td>154</td>
    <td><p>Recent studies show that large-scale sketch-based image retrieval (SBIR) can
be efficiently tackled by cross-modal binary representation learning methods,
where Hamming distance matching significantly speeds up the process of
similarity search. Providing training and test data subjected to a fixed set of
pre-defined categories, the cutting-edge SBIR and cross-modal hashing works
obtain acceptable retrieval performance. However, most of the existing methods
fail when the categories of query sketches have never been seen during
training. In this paper, the above problem is briefed as a novel but realistic
zero-shot SBIR hashing task. We elaborate the challenges of this special task
and accordingly propose a zero-shot sketch-image hashing (ZSIH) model. An
end-to-end three-network architecture is built, two of which are treated as the
binary encoders. The third network mitigates the sketch-image heterogeneity and
enhances the semantic relations among data by utilizing the Kronecker fusion
layer and graph convolution, respectively. As an important part of ZSIH, we
formulate a generative hashing scheme in reconstructing semantic knowledge
representations for zero-shot retrieval. To the best of our knowledge, ZSIH is
the first zero-shot hashing work suitable for SBIR and cross-modal search.
Comprehensive experiments are conducted on two extended datasets, i.e., Sketchy
and TU-Berlin with a novel zero-shot train-test split. The proposed model
remarkably outperforms related works.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
        Few Shot & Zero Shot 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/zhang2018visual/">Visual Search At Alibaba</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Visual Search At Alibaba' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Visual Search At Alibaba' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</td>
    <td>58</td>
    <td><p>This paper introduces the large scale visual search algorithm and system
infrastructure at Alibaba. The following challenges are discussed under the
E-commercial circumstance at Alibaba (a) how to handle heterogeneous image data
and bridge the gap between real-shot images from user query and the online
images. (b) how to deal with large scale indexing for massive updating data.
(c) how to train deep models for effective feature representation without huge
human annotations. (d) how to improve the user engagement by considering the
quality of the content. We take advantage of large image collection of Alibaba
and state-of-the-art deep learning techniques to perform visual search at
scale. We present solutions and implementation details to overcome those
problems and also share our learnings from building such a large scale
commercial visual search engine. Specifically, model and search-based fusion
approach is introduced to effectively predict categories. Also, we propose a
deep CNN model for joint detection and feature learning by mining user click
behavior. The binary index engine is designed to scale up indexing without
compromising recall and precision. Finally, we apply all the stages into an
end-to-end system architecture, which can simultaneously achieve highly
efficient and scalable performance adapting to real-shot images. Extensive
experiments demonstrate the advancement of each module in our system. We hope
visual search at Alibaba becomes more widely incorporated into today’s
commercial applications.</p>
</td>
    <td>
      
        KDD 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/mohedano2018saliency/">Saliency Weighted Convolutional Features For Instance Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Saliency Weighted Convolutional Features For Instance Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Saliency Weighted Convolutional Features For Instance Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mohedano et al.</td> <!-- 🔧 You were missing this -->
    <td>2018 International Conference on Content-Based Multimedia Indexing (CBMI)</td>
    <td>37</td>
    <td><p>This work explores attention models to weight the contribution of local
convolutional representations for the instance search task. We present a
retrieval framework based on bags of local convolutional features (BLCF) that
benefits from saliency weighting to build an efficient image representation.
The use of human visual attention models (saliency) allows significant
improvements in retrieval performance without the need to conduct region
analysis or spatial verification, and without requiring any feature fine
tuning. We investigate the impact of different saliency models, finding that
higher performance on saliency benchmarks does not necessarily equate to
improved performance when used in instance search tasks. The proposed approach
outperforms the state-of-the-art on the challenging INSTRE benchmark by a large
margin, and provides similar performance on the Oxford and Paris benchmarks
compared to more complex methods that use off-the-shelf representations. The
source code used in this project is available at
https://imatge-upc.github.io/salbow/</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/malkov2018efficient/">Efficient And Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient And Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient And Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Malkov Yu. A., Yashunin D. A.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>1063</td>
    <td><p>We present a new approach for the approximate K-nearest neighbor search based
on navigable small world graphs with controllable hierarchy (Hierarchical NSW,
HNSW). The proposed solution is fully graph-based, without any need for
additional search structures, which are typically used at the coarse search
stage of the most proximity graph techniques. Hierarchical NSW incrementally
builds a multi-layer structure consisting from hierarchical set of proximity
graphs (layers) for nested subsets of the stored elements. The maximum layer in
which an element is present is selected randomly with an exponentially decaying
probability distribution. This allows producing graphs similar to the
previously studied Navigable Small World (NSW) structures while additionally
having the links separated by their characteristic distance scales. Starting
search from the upper layer together with utilizing the scale separation boosts
the performance compared to NSW and allows a logarithmic complexity scaling.
Additional employment of a heuristic for selecting proximity graph neighbors
significantly increases performance at high recall and in case of highly
clustered data. Performance evaluation has demonstrated that the proposed
general metric space search index is able to strongly outperform previous
opensource state-of-the-art vector-only approaches. Similarity of the algorithm
to the skip list structure allows straightforward balanced distributed
implementation.</p>
</td>
    <td>
      
        Similarity Search 
      
        Graph Based ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/liu2018discriminative/">Discriminative Cross-view Binary Representation Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Discriminative Cross-view Binary Representation Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Discriminative Cross-view Binary Representation Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Liu, Qi Hairong</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>5</td>
    <td><p>Learning compact representation is vital and challenging for large scale
multimedia data. Cross-view/cross-modal hashing for effective binary
representation learning has received significant attention with exponentially
growing availability of multimedia content. Most existing cross-view hashing
algorithms emphasize the similarities in individual views, which are then
connected via cross-view similarities. In this work, we focus on the
exploitation of the discriminative information from different views, and
propose an end-to-end method to learn semantic-preserving and discriminative
binary representation, dubbed Discriminative Cross-View Hashing (DCVH), in
light of learning multitasking binary representation for various tasks
including cross-view retrieval, image-to-image retrieval, and image
annotation/tagging. The proposed DCVH has the following key components. First,
it uses convolutional neural network (CNN) based nonlinear hashing functions
and multilabel classification for both images and texts simultaneously. Such
hashing functions achieve effective continuous relaxation during training
without explicit quantization loss by using Direct Binary Embedding (DBE)
layers. Second, we propose an effective view alignment via Hamming distance
minimization, which is efficiently accomplished by bit-wise XOR operation.
Extensive experiments on two image-text benchmark datasets demonstrate that
DCVH outperforms state-of-the-art cross-view hashing algorithms as well as
single-view image hashing algorithms. In addition, DCVH can provide competitive
performance for image annotation/tagging.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/yu2018vlase/">VLASE: Vehicle Localization By Aggregating Semantic Edges</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=VLASE: Vehicle Localization By Aggregating Semantic Edges' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=VLASE: Vehicle Localization By Aggregating Semantic Edges' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yu et al.</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</td>
    <td>41</td>
    <td><p>In this paper, we propose VLASE, a framework to use semantic edge features
from images to achieve on-road localization. Semantic edge features denote edge
contours that separate pairs of distinct objects such as building-sky, road-
sidewalk, and building-ground. While prior work has shown promising results by
utilizing the boundary between prominent classes such as sky and building using
skylines, we generalize this approach to consider semantic edge features that
arise from 19 different classes. Our localization algorithm is simple, yet very
powerful. We extract semantic edge features using a recently introduced CASENet
architecture and utilize VLAD framework to perform image retrieval. Our
experiments show that we achieve improvement over some of the state-of-the-art
localization algorithms such as SIFT-VLAD and its deep variant NetVLAD. We use
ablation study to study the importance of different semantic classes and show
that our unified approach achieves better performance compared to individual
prominent features such as skylines.</p>
</td>
    <td>
      
        IROS 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/yu2018modeling/">Modeling Text With Graph Convolutional Network For Cross-modal Information Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Modeling Text With Graph Convolutional Network For Cross-modal Information Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Modeling Text With Graph Convolutional Network For Cross-modal Information Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yu et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>44</td>
    <td><p>Cross-modal information retrieval aims to find heterogeneous data of various
modalities from a given query of one modality. The main challenge is to map
different modalities into a common semantic space, in which distance between
concepts in different modalities can be well modeled. For cross-modal
information retrieval between images and texts, existing work mostly uses
off-the-shelf Convolutional Neural Network (CNN) for image feature extraction.
For texts, word-level features such as bag-of-words or word2vec are employed to
build deep learning models to represent texts. Besides word-level semantics,
the semantic relations between words are also informative but less explored. In
this paper, we model texts by graphs using similarity measure based on
word2vec. A dual-path neural network model is proposed for couple feature
learning in cross-modal information retrieval. One path utilizes Graph
Convolutional Network (GCN) for text modeling based on graph representations.
The other path uses a neural network with layers of nonlinearities for image
modeling based on off-the-shelf features. The model is trained by a pairwise
similarity loss function to maximize the similarity of relevant text-image
pairs and minimize the similarity of irrelevant pairs. Experimental results
show that the proposed model outperforms the state-of-the-art methods
significantly, with 17% improvement on accuracy for the best case.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/tran2018device/">On-device Scalable Image-based Localization Via Prioritized Cascade Search And Fast One-many RANSAC</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=On-device Scalable Image-based Localization Via Prioritized Cascade Search And Fast One-many RANSAC' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=On-device Scalable Image-based Localization Via Prioritized Cascade Search And Fast One-many RANSAC' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tran et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>38</td>
    <td><p>We present the design of an entire on-device system for large-scale urban
localization using images. The proposed design integrates compact image
retrieval and 2D-3D correspondence search to estimate the location in extensive
city regions. Our design is GPS agnostic and does not require network
connection. In order to overcome the resource constraints of mobile devices, we
propose a system design that leverages the scalability advantage of image
retrieval and accuracy of 3D model-based localization. Furthermore, we propose
a new hashing-based cascade search for fast computation of 2D-3D
correspondences. In addition, we propose a new one-many RANSAC for accurate
pose estimation. The new one-many RANSAC addresses the challenge of repetitive
building structures (e.g. windows, balconies) in urban localization. Extensive
experiments demonstrate that our 2D-3D correspondence search achieves
state-of-the-art localization accuracy on multiple benchmark datasets.
Furthermore, our experiments on a large Google Street View (GSV) image dataset
show the potential of large-scale localization entirely on a typical mobile
device.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/wang2018learning/">Learning Two-branch Neural Networks For Image-text Matching Tasks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Two-branch Neural Networks For Image-text Matching Tasks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Two-branch Neural Networks For Image-text Matching Tasks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>526</td>
    <td><p>Image-language matching tasks have recently attracted a lot of attention in
the computer vision field. These tasks include image-sentence matching, i.e.,
given an image query, retrieving relevant sentences and vice versa, and
region-phrase matching or visual grounding, i.e., matching a phrase to relevant
regions. This paper investigates two-branch neural networks for learning the
similarity between these two data modalities. We propose two network structures
that produce different output representations. The first one, referred to as an
embedding network, learns an explicit shared latent embedding space with a
maximum-margin ranking loss and novel neighborhood constraints. Compared to
standard triplet sampling, we perform improved neighborhood sampling that takes
neighborhood information into consideration while constructing mini-batches.
The second network structure, referred to as a similarity network, fuses the
two branches via element-wise product and is trained with regression loss to
directly predict a similarity score. Extensive experiments show that our
networks achieve high accuracies for phrase localization on the Flickr30K
Entities dataset and for bi-directional image-sentence retrieval on Flickr30K
and MSCOCO datasets.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/woodbridge2018detecting/">Detecting Homoglyph Attacks With A Siamese Neural Network</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Detecting Homoglyph Attacks With A Siamese Neural Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Detecting Homoglyph Attacks With A Siamese Neural Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Woodbridge et al.</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE Security and Privacy Workshops (SPW)</td>
    <td>34</td>
    <td><p>A homoglyph (name spoofing) attack is a common technique used by adversaries
to obfuscate file and domain names. This technique creates process or domain
names that are visually similar to legitimate and recognized names. For
instance, an attacker may create malware with the name svch0st.exe so that in a
visual inspection of running processes or a directory listing, the process or
file name might be mistaken as the Windows system process svchost.exe. There
has been limited published research on detecting homoglyph attacks. Current
approaches rely on string comparison algorithms (such as Levenshtein distance)
that result in computationally heavy solutions with a high number of false
positives. In addition, there is a deficiency in the number of publicly
available datasets for reproducible research, with most datasets focused on
phishing attacks, in which homoglyphs are not always used. This paper presents
a fundamentally different solution to this problem using a Siamese
convolutional neural network (CNN). Rather than leveraging similarity based on
character swaps and deletions, this technique uses a learned metric on strings
rendered as images: a CNN learns features that are optimized to detect visual
similarity of the rendered strings. The trained model is used to convert
thousands of potentially targeted process or domain names to feature vectors.
These feature vectors are indexed using randomized KD-Trees to make similarity
searches extremely fast with minimal computational processing. This technique
shows a considerable 13% to 45% improvement over baseline techniques in terms
of area under the receiver operating characteristic curve (ROC AUC). In
addition, we provide both code and data to further future research.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/indyk2018approximate/">Approximate Nearest Neighbors In Limited Space</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Approximate Nearest Neighbors In Limited Space' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Approximate Nearest Neighbors In Limited Space' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Indyk Piotr, Wagner Tal</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>6</td>
    <td><p>We consider the \((1+\epsilon)\)-approximate nearest neighbor search problem:
given a set \(X\) of \(n\) points in a \(d\)-dimensional space, build a data
structure that, given any query point \(y\), finds a point \(x \in X\) whose
distance to \(y\) is at most \((1+\epsilon) \min_{x \in X} |x-y|\) for an
accuracy parameter \(\epsilon \in (0,1)\). Our main result is a data structure
that occupies only \(O(\epsilon^{-2} n log(n) log(1/\epsilon))\) bits of space,
assuming all point coordinates are integers in the range \(\{-n^{O(1)} \ldots
n^{O(1)}\}\), i.e., the coordinates have \(O(log n)\) bits of precision. This
improves over the best previously known space bound of \(O(\epsilon^{-2} n
log(n)^2)\), obtained via the randomized dimensionality reduction method of
Johnson and Lindenstrauss (1984). We also consider the more general problem of
estimating all distances from a collection of query points to all data points
\(X\), and provide almost tight upper and lower bounds for the space complexity
of this problem.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/loncaric2018convolutional/">Convolutional Hashing For Automated Scene Matching</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Convolutional Hashing For Automated Scene Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Convolutional Hashing For Automated Scene Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Loncaric Martin, Liu Bowei, Weber Ryan</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>42</td>
    <td><p>We present a powerful new loss function and training scheme for learning
binary hash functions. In particular, we demonstrate our method by creating for
the first time a neural network that outperforms state-of-the-art Haar wavelets
and color layout descriptors at the task of automated scene matching. By
accurately relating distance on the manifold of network outputs to distance in
Hamming space, we achieve a 100-fold reduction in nontrivial false positive
rate and significantly higher true positive rate. We expect our insights to
provide large wins for hashing models applied to other information retrieval
hashing tasks as well.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/huynh2018fast/">Fast Binary Embeddings, And Quantized Compressed Sensing With Structured Matrices</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Binary Embeddings, And Quantized Compressed Sensing With Structured Matrices' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Binary Embeddings, And Quantized Compressed Sensing With Structured Matrices' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Huynh Thang, Saab Rayan</td> <!-- 🔧 You were missing this -->
    <td>Communications on Pure and Applied Mathematics</td>
    <td>12</td>
    <td><p>This paper deals with two related problems, namely distance-preserving binary
embeddings and quantization for compressed sensing . First, we propose fast
methods to replace points from a subset \(\mathcal{X} \subset \mathbb{R}^n\),
associated with the Euclidean metric, with points in the cube \(\{\pm 1\}^m\) and
we associate the cube with a pseudo-metric that approximates Euclidean distance
among points in \(\mathcal{X}\). Our methods rely on quantizing fast
Johnson-Lindenstrauss embeddings based on bounded orthonormal systems and
partial circulant ensembles, both of which admit fast transforms. Our
quantization methods utilize noise-shaping, and include Sigma-Delta schemes and
distributed noise-shaping schemes. The resulting approximation errors decay
polynomially and exponentially fast in \(m\), depending on the embedding method.
This dramatically outperforms the current decay rates associated with binary
embeddings and Hamming distances. Additionally, it is the first such binary
embedding result that applies to fast Johnson-Lindenstrauss maps while
preserving \(ℓ₂\) norms.
  Second, we again consider noise-shaping schemes, albeit this time to quantize
compressed sensing measurements arising from bounded orthonormal ensembles and
partial circulant matrices. We show that these methods yield a reconstruction
error that again decays with the number of measurements (and bits), when using
convex optimization for reconstruction. Specifically, for Sigma-Delta schemes,
the error decays polynomially in the number of measurements, and it decays
exponentially for distributed noise-shaping schemes based on beta encoding.
These results are near optimal and the first of their kind dealing with bounded
orthonormal systems.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/iscen2018fast/">Fast Spectral Ranking For Similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Spectral Ranking For Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Spectral Ranking For Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Iscen et al.</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</td>
    <td>37</td>
    <td><p>Despite the success of deep learning on representing images for particular
object retrieval, recent studies show that the learned representations still
lie on manifolds in a high dimensional space. This makes the Euclidean nearest
neighbor search biased for this task. Exploring the manifolds online remains
expensive even if a nearest neighbor graph has been computed offline. This work
introduces an explicit embedding reducing manifold search to Euclidean search
followed by dot product similarity search. This is equivalent to linear graph
filtering of a sparse signal in the frequency domain. To speed up online
search, we compute an approximate Fourier basis of the graph offline. We
improve the state of art on particular object retrieval datasets including the
challenging Instre dataset containing small objects. At a scale of 10^5 images,
the offline cost is only a few hours, while query time is comparable to
standard similarity search.</p>
</td>
    <td>
      
        Similarity Search 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/jeong2018efficient/">Efficient End-to-end Learning For Quantizable Representations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient End-to-end Learning For Quantizable Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient End-to-end Learning For Quantizable Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jeong Yeonwoo, Song Hyun Oh</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>9</td>
    <td><p>Embedding representation learning via neural networks is at the core
foundation of modern similarity based search. While much effort has been put in
developing algorithms for learning binary hamming code representations for
search efficiency, this still requires a linear scan of the entire dataset per
each query and trades off the search accuracy through binarization. To this
end, we consider the problem of directly learning a quantizable embedding
representation and the sparse binary hash code end-to-end which can be used to
construct an efficient hash table not only providing significant search
reduction in the number of data but also achieving the state of the art search
accuracy outperforming previous state of the art deep metric learning methods.
We also show that finding the optimal sparse binary hash code in a mini-batch
can be computed exactly in polynomial time by solving a minimum cost flow
problem. Our results on Cifar-100 and on ImageNet datasets show the state of
the art search accuracy in precision@k and NMI metrics while providing up to
98X and 478X search speedup respectively over exhaustive linear search. The
source code is available at
https://github.com/maestrojeong/Deep-Hash-Table-ICML18</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/zhang2018relationnet2/">Relationnet2: Deep Comparison Columns For Few-shot Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Relationnet2: Deep Comparison Columns For Few-shot Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Relationnet2: Deep Comparison Columns For Few-shot Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>17</td>
    <td><p>Few-shot deep learning is a topical challenge area for scaling visual
recognition to open ended growth of unseen new classes with limited labeled
examples. A promising approach is based on metric learning, which trains a deep
embedding to support image similarity matching. Our insight is that effective
general purpose matching requires non-linear comparison of features at multiple
abstraction levels. We thus propose a new deep comparison network comprised of
embedding and relation modules that learn multiple non-linear distance metrics
based on different levels of features simultaneously. Furthermore, to reduce
over-fitting and enable the use of deeper embeddings, we represent images as
distributions rather than vectors via learning parameterized Gaussian noise
regularization. The resulting network achieves excellent performance on both
miniImageNet and tieredImageNet.</p>
</td>
    <td>
      
        Evaluation 
      
        Few Shot & Zero Shot 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/hu2018hashing/">From Hashing To Cnns: Training Binaryweight Networks Via Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=From Hashing To Cnns: Training Binaryweight Networks Via Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=From Hashing To Cnns: Training Binaryweight Networks Via Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hu Qinghao, Wang Peisong, Cheng Jian</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>42</td>
    <td><p>Deep convolutional neural networks (CNNs) have shown appealing performance on
various computer vision tasks in recent years. This motivates people to deploy
CNNs to realworld applications. However, most of state-of-art CNNs require
large memory and computational resources, which hinders the deployment on
mobile devices. Recent studies show that low-bit weight representation can
reduce much storage and memory demand, and also can achieve efficient network
inference. To achieve this goal, we propose a novel approach named BWNH to
train Binary Weight Networks via Hashing. In this paper, we first reveal the
strong connection between inner-product preserving hashing and binary weight
networks, and show that training binary weight networks can be intrinsically
regarded as a hashing problem. Based on this perspective, we propose an
alternating optimization method to learn the hash codes instead of directly
learning binary weights. Extensive experiments on CIFAR10, CIFAR100 and
ImageNet demonstrate that our proposed BWNH outperforms current state-of-art by
a large margin.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/hu2018deep/">Deep LDA Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep LDA Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep LDA Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hu di, Nie Feiping, Li Xuelong</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>97</td>
    <td><p>The conventional supervised hashing methods based on classification do not
entirely meet the requirements of hashing technique, but Linear Discriminant
Analysis (LDA) does. In this paper, we propose to perform a revised LDA
objective over deep networks to learn efficient hashing codes in a truly
end-to-end fashion. However, the complicated eigenvalue decomposition within
each mini-batch in every epoch has to be faced with when simply optimizing the
deep network w.r.t. the LDA objective. In this work, the revised LDA objective
is transformed into a simple least square problem, which naturally overcomes
the intractable problems and can be easily solved by the off-the-shelf
optimizer. Such deep extension can also overcome the weakness of LDA Hashing in
the limited linear projection and feature learning. Amounts of experiments are
conducted on three benchmark datasets. The proposed Deep LDA Hashing shows
nearly 70 points improvement over the conventional one on the CIFAR-10 dataset.
It also beats several state-of-the-art methods on various metrics.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/hu2018web/">Web-scale Responsive Visual Search At Bing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Web-scale Responsive Visual Search At Bing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Web-scale Responsive Visual Search At Bing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</td>
    <td>49</td>
    <td><p>In this paper, we introduce a web-scale general visual search system deployed
in Microsoft Bing. The system accommodates tens of billions of images in the
index, with thousands of features for each image, and can respond in less than
200 ms. In order to overcome the challenges in relevance, latency, and
scalability in such large scale of data, we employ a cascaded learning-to-rank
framework based on various latest deep learning visual features, and deploy in
a distributed heterogeneous computing platform. Quantitative and qualitative
experiments show that our system is able to support various applications on
Bing website and apps.</p>
</td>
    <td>
      
        KDD 
      
        Image Retrieval 
      
        Large Scale Search 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/long2018deep/">Deep Domain Adaptation Hashing With Adversarial Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Domain Adaptation Hashing With Adversarial Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Domain Adaptation Hashing With Adversarial Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Long et al.</td> <!-- 🔧 You were missing this -->
    <td>The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</td>
    <td>21</td>
    <td><p>The recent advances in deep neural networks have demonstrated high capability in a wide variety of scenarios. Nevertheless, fine-tuning deep models in a new domain still requires a significant amount of labeled data despite expensive labeling efforts. A valid question is how to leverage the source knowledge plus unlabeled or only sparsely labeled target data for learning a new model in target domain. The core problem is to bring the source and target distributions closer in the feature space. In the paper, we facilitate this issue in an adversarial learning framework, in which a domain discriminator is devised to handle domain shift. Particularly, we explore the learning in the context of hashing problem, which has been studied extensively due to its great efficiency in gigantic data. Specifically, a novel Deep Domain Adaptation Hashing with Adversarial learning (DeDAHA) architecture is presented, which mainly consists of three components: a deep convolutional neural networks (CNN) for learning basic image/frame representation followed by an adversary stream on one hand to optimize the domain discriminator, and on the other, to interact with each domain-specific hashing stream for encoding image representation to hash codes. The whole architecture is trained end-to-end by jointly optimizing two types of losses, i.e., triplet ranking loss to preserve the relative similarity ordering in the input triplets and adversarial loss to maximally fool the domain discriminator with the learnt source and target feature distributions. Extensive experiments are conducted on three domain transfer tasks, including cross-domain digits retrieval, image to image and image to video transfers, on several benchmarks. Our DeDAHA framework achieves superior results when compared to the state-of-the-art techniques.</p>
</td>
    <td>
      
        SIGIR 
      
        Hashing Methods 
      
        Robustness 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/hsu2018unsupervised/">Unsupervised Multimodal Representation Learning Across Medical Images And Reports</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Multimodal Representation Learning Across Medical Images And Reports' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Multimodal Representation Learning Across Medical Images And Reports' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hsu et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>26</td>
    <td><p>Joint embeddings between medical imaging modalities and associated radiology
reports have the potential to offer significant benefits to the clinical
community, ranging from cross-domain retrieval to conditional generation of
reports to the broader goals of multimodal representation learning. In this
work, we establish baseline joint embedding results measured via both local and
global retrieval methods on the soon to be released MIMIC-CXR dataset
consisting of both chest X-ray images and the associated radiology reports. We
examine both supervised and unsupervised methods on this task and show that for
document retrieval tasks with the learned representations, only a limited
amount of supervision is needed to yield results comparable to those of
fully-supervised methods.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/zhu2018attention/">Attention-based Pyramid Aggregation Network For Visual Place Recognition</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Attention-based Pyramid Aggregation Network For Visual Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Attention-based Pyramid Aggregation Network For Visual Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 26th ACM international conference on Multimedia</td>
    <td>89</td>
    <td><p>Visual place recognition is challenging in the urban environment and is
usually viewed as a large scale image retrieval task. The intrinsic challenges
in place recognition exist that the confusing objects such as cars and trees
frequently occur in the complex urban scene, and buildings with repetitive
structures may cause over-counting and the burstiness problem degrading the
image representations. To address these problems, we present an Attention-based
Pyramid Aggregation Network (APANet), which is trained in an end-to-end manner
for place recognition. One main component of APANet, the spatial pyramid
pooling, can effectively encode the multi-size buildings containing
geo-information. The other one, the attention block, is adopted as a region
evaluator for suppressing the confusing regional features while highlighting
the discriminative ones. When testing, we further propose a simple yet
effective PCA power whitening strategy, which significantly improves the widely
used PCA whitening by reasonably limiting the impact of over-counting.
Experimental evaluations demonstrate that the proposed APANet outperforms the
state-of-the-art methods on two place recognition benchmarks, and generalizes
well on standard image retrieval datasets.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/schlegel2018hbst/">HBST: A Hamming Distance Embedding Binary Search Tree For Visual Place Recognition</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=HBST: A Hamming Distance Embedding Binary Search Tree For Visual Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=HBST: A Hamming Distance Embedding Binary Search Tree For Visual Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Schlegel Dominik, Grisetti Giorgio</td> <!-- 🔧 You were missing this -->
    <td>IEEE Robotics and Automation Letters</td>
    <td>52</td>
    <td><p>Reliable and efficient Visual Place Recognition is a major building block of
modern SLAM systems. Leveraging on our prior work, in this paper we present a
Hamming Distance embedding Binary Search Tree (HBST) approach for binary
Descriptor Matching and Image Retrieval. HBST allows for descriptor Search and
Insertion in logarithmic time by exploiting particular properties of binary
Feature descriptors. We support the idea behind our search structure with a
thorough analysis on the exploited descriptor properties and their effects on
completeness and complexity of search and insertion. To validate our claims we
conducted comparative experiments for HBST and several state-of-the-art methods
on a broad range of publicly available datasets. HBST is available as a compact
open-source C++ header-only library.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/yan2018norm/">Norm-ranging LSH For Maximum Inner Product Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Norm-ranging LSH For Maximum Inner Product Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Norm-ranging LSH For Maximum Inner Product Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yan et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>24</td>
    <td><p>Neyshabur and Srebro proposed Simple-LSH, which is the state-of-the-art
hashing method for maximum inner product search (MIPS) with performance
guarantee. We found that the performance of Simple-LSH, in both theory and
practice, suffers from long tails in the 2-norm distribution of real datasets.
We propose Norm-ranging LSH, which addresses the excessive normalization
problem caused by long tails in Simple-LSH by partitioning a dataset into
multiple sub-datasets and building a hash index for each sub-dataset
independently. We prove that Norm-ranging LSH has lower query time complexity
than Simple-LSH. We also show that the idea of partitioning the dataset can
improve other hashing based methods for MIPS. To support efficient query
processing on the hash indexes of the sub-datasets, a novel similarity metric
is formulated. Experiments show that Norm-ranging LSH achieves an order of
magnitude speedup over Simple-LSH for the same recall, thus significantly
benefiting applications that involve MIPS.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/iwasaki2018optimization/">Optimization Of Indexing Based On K-nearest Neighbor Graph For Proximity Search In High-dimensional Data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Optimization Of Indexing Based On K-nearest Neighbor Graph For Proximity Search In High-dimensional Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Optimization Of Indexing Based On K-nearest Neighbor Graph For Proximity Search In High-dimensional Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Iwasaki Masajiro, Miyazaki Daisuke</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>39</td>
    <td><p>Searching for high-dimensional vector data with high accuracy is an
inevitable search technology for various types of data. Graph-based indexes are
known to reduce the query time for high-dimensional data. To further improve
the query time by using graphs, we focused on the indegrees and outdegrees of
graphs. While a sufficient number of incoming edges (indegrees) are
indispensable for increasing search accuracy, an excessive number of outgoing
edges (outdegrees) should be suppressed so as to not increase the query time.
Therefore, we propose three degree-adjustment methods: static degree adjustment
of not only outdegrees but also indegrees, dynamic degree adjustment with which
outdegrees are determined by the search accuracy users require, and path
adjustment to remove edges that have alternative search paths to reduce
outdegrees. We also show how to obtain optimal degree-adjustment parameters and
that our methods outperformed previous methods for image and textual data.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/zhu2018exploring/">Exploring Auxiliary Context: Discrete Semantic Transfer Hashing For Scalable Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Exploring Auxiliary Context: Discrete Semantic Transfer Hashing For Scalable Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Exploring Auxiliary Context: Discrete Semantic Transfer Hashing For Scalable Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhu et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Neural Networks and Learning Systems</td>
    <td>154</td>
    <td><p>Unsupervised hashing can desirably support scalable content-based image
retrieval (SCBIR) for its appealing advantages of semantic label independence,
memory and search efficiency. However, the learned hash codes are embedded with
limited discriminative semantics due to the intrinsic limitation of image
representation. To address the problem, in this paper, we propose a novel
hashing approach, dubbed as <em>Discrete Semantic Transfer Hashing</em> (DSTH).
The key idea is to <em>directly</em> augment the semantics of discrete image hash
codes by exploring auxiliary contextual modalities. To this end, a unified
hashing framework is formulated to simultaneously preserve visual similarities
of images and perform semantic transfer from contextual modalities. Further, to
guarantee direct semantic transfer and avoid information loss, we explicitly
impose the discrete constraint, bit–uncorrelation constraint and bit-balance
constraint on hash codes. A novel and effective discrete optimization method
based on augmented Lagrangian multiplier is developed to iteratively solve the
optimization problem. The whole learning process has linear computation
complexity and desirable scalability. Experiments on three benchmark datasets
demonstrate the superiority of DSTH compared with several state-of-the-art
approaches.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/jain2018learning/">Learning A Complete Image Indexing Pipeline</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning A Complete Image Indexing Pipeline' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning A Complete Image Indexing Pipeline' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jain et al.</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</td>
    <td>14</td>
    <td><p>To work at scale, a complete image indexing system comprises two components:
An inverted file index to restrict the actual search to only a subset that
should contain most of the items relevant to the query; An approximate distance
computation mechanism to rapidly scan these lists. While supervised deep
learning has recently enabled improvements to the latter, the former continues
to be based on unsupervised clustering in the literature. In this work, we
propose a first system that learns both components within a unifying neural
framework of structured binary encoding.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/sarfraz2018pose/">A Pose-sensitive Embedding For Person Re-identification With Expanded Cross Neighborhood Re-ranking</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Pose-sensitive Embedding For Person Re-identification With Expanded Cross Neighborhood Re-ranking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Pose-sensitive Embedding For Person Re-identification With Expanded Cross Neighborhood Re-ranking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sarfraz et al.</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</td>
    <td>521</td>
    <td><p>Person re identification is a challenging retrieval task that requires
matching a person’s acquired image across non overlapping camera views. In this
paper we propose an effective approach that incorporates both the fine and
coarse pose information of the person to learn a discriminative embedding. In
contrast to the recent direction of explicitly modeling body parts or
correcting for misalignment based on these, we show that a rather
straightforward inclusion of acquired camera view and/or the detected joint
locations into a convolutional neural network helps to learn a very effective
representation. To increase retrieval performance, re-ranking techniques based
on computed distances have recently gained much attention. We propose a new
unsupervised and automatic re-ranking framework that achieves state-of-the-art
re-ranking performance. We show that in contrast to the current
state-of-the-art re-ranking methods our approach does not require to compute
new rank lists for each image pair (e.g., based on reciprocal neighbors) and
performs well by using simple direct rank list based comparison or even by just
using the already computed euclidean distances between the images. We show that
both our learned representation and our re-ranking method achieve
state-of-the-art performance on a number of challenging surveillance image and
video datasets.
  The code is available online at:
https://github.com/pse-ecn/pose-sensitive-embedding</p>
</td>
    <td>
      
        Hybrid ANN Methods 
      
        Re RANKING 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/vo2018generalization/">Generalization In Metric Learning: Should The Embedding Layer Be The Embedding Layer?</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Generalization In Metric Learning: Should The Embedding Layer Be The Embedding Layer?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Generalization In Metric Learning: Should The Embedding Layer Be The Embedding Layer?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Vo Nam, Hays James</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>This work studies deep metric learning under small to medium scale data as we
believe that better generalization could be a contributing factor to the
improvement of previous fine-grained image retrieval methods; it should be
considered when designing future techniques. In particular, we investigate
using other layers in a deep metric learning system (besides the embedding
layer) for feature extraction and analyze how well they perform on training
data and generalize to testing data. From this study, we suggest a new
regularization practice where one can add or choose a more optimal layer for
feature extraction. State-of-the-art performance is demonstrated on 3
fine-grained image retrieval benchmarks: Cars-196, CUB-200-2011, and Stanford
Online Product.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/svenstrup2017hash/">Hash Embeddings For Efficient Word Representations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hash Embeddings For Efficient Word Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hash Embeddings For Efficient Word Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Svenstrup Dan, Hansen Jonas Meinertz, Winther Ole</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>29</td>
    <td><p>We present hash embeddings, an efficient method for representing words in a
continuous vector form. A hash embedding may be seen as an interpolation
between a standard word embedding and a word embedding created using a random
hash function (the hashing trick). In hash embeddings each token is represented
by \(k\) \(d\)-dimensional embeddings vectors and one \(k\) dimensional weight
vector. The final \(d\) dimensional representation of the token is the product of
the two. Rather than fitting the embedding vectors for each token these are
selected by the hashing trick from a shared pool of \(B\) embedding vectors. Our
experiments show that hash embeddings can easily deal with huge vocabularies
consisting of millions of tokens. When using a hash embedding there is no need
to create a dictionary before training nor to perform any kind of vocabulary
pruning after training. We show that models trained using hash embeddings
exhibit at least the same level of performance as models trained using regular
embeddings across a wide range of tasks. Furthermore, the number of parameters
needed by such an embedding is only a fraction of what is required by a regular
embedding. Since standard embeddings and embeddings constructed using the
hashing trick are actually just special cases of a hash embedding, hash
embeddings can be considered an extension and improvement over the existing
regular embedding types.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/dahlgaard2017fast/">Fast Similarity Sketching</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Similarity Sketching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Similarity Sketching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dahlgaard et al.</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS)</td>
    <td>30</td>
    <td><p>We consider the \(\textit{Similarity Sketching}\) problem: Given a universe
\([u] = \{0,\ldots, u-1\}\) we want a random function \(S\) mapping subsets
\(A\subseteq [u]\) into vectors \(S(A)\) of size \(t\), such that the Jaccard
similarity \(J(A,B) = |A\cap B|/|A\cup B|\) between sets \(A\) and \(B\) is
preserved. More precisely, define \(X_i = [S(A)[i] =
  S(B)[i]]\) and \(X = \sum_{i\in [t]} X_i\). We want \(E[X_i]=J(A,B)\), and we want
\(X\) to be strongly concentrated around \(E[X] = t \cdot J(A,B)\) (i.e.
Chernoff-style bounds). This is a fundamental problem which has found numerous
applications in data mining, large-scale classification, computer vision,
similarity search, etc. via the classic MinHash algorithm. The vectors \(S(A)\)
are also called \(\textit{sketches}\). Strong concentration is critical, for
often we want to sketch many sets \(B_1,\ldots,B_n\) so that we later, for a
query set \(A\), can find (one of) the most similar \(B_i\). It is then critical
that no \(B_i\) looks much more similar to \(A\) due to errors in the sketch.
  The seminal \(t\times\textit{MinHash}\) algorithm uses \(t\) random hash
functions \(h_1,\ldots, h_t\), and stores \(\left ( \min_{a\in A} h_1(A),\ldots,
\min_{a\in A} h_t(A) \right )\) as the sketch of \(A\). The main drawback of
MinHash is, however, its \(O(t\cdot |A|)\) running time, and finding a sketch
with similar properties and faster running time has been the subject of several
papers. (continued…)</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/dahlgaard2017practical/">Practical Hash Functions For Similarity Estimation And Dimensionality Reduction</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Practical Hash Functions For Similarity Estimation And Dimensionality Reduction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Practical Hash Functions For Similarity Estimation And Dimensionality Reduction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dahlgaard Søren, Knudsen Mathias Bæk Tejs, Thorup Mikkel</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>16</td>
    <td><p>Hashing is a basic tool for dimensionality reduction employed in several
aspects of machine learning. However, the perfomance analysis is often carried
out under the abstract assumption that a truly random unit cost hash function
is used, without concern for which concrete hash function is employed. The
concrete hash function may work fine on sufficiently random input. The question
is if it can be trusted in the real world when faced with more structured
input.
  In this paper we focus on two prominent applications of hashing, namely
similarity estimation with the one permutation hashing (OPH) scheme of Li et
al. [NIPS’12] and feature hashing (FH) of Weinberger et al. [ICML’09], both of
which have found numerous applications, i.e. in approximate near-neighbour
search with LSH and large-scale classification with SVM.
  We consider mixed tabulation hashing of Dahlgaard et al.[FOCS’15] which was
proved to perform like a truly random hash function in many applications,
including OPH. Here we first show improved concentration bounds for FH with
truly random hashing and then argue that mixed tabulation performs similar for
sparse input. Our main contribution, however, is an experimental comparison of
different hashing schemes when used inside FH, OPH, and LSH.
  We find that mixed tabulation hashing is almost as fast as the
multiply-mod-prime scheme ax+b mod p. Mutiply-mod-prime is guaranteed to work
well on sufficiently random data, but we demonstrate that in the above
applications, it can lead to bias and poor concentration on both real-world and
synthetic data. We also compare with the popular MurmurHash3, which has no
proven guarantees. Mixed tabulation and MurmurHash3 both perform similar to
truly random hashing in our experiments. However, mixed tabulation is 40%
faster than MurmurHash3, and it has the proven guarantee of good performance on
all possible input.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/sablayrolles2017how/">How Should We Evaluate Supervised Hashing?</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=How Should We Evaluate Supervised Hashing?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=How Should We Evaluate Supervised Hashing?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sablayrolles et al.</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</td>
    <td>97</td>
    <td><p>Hashing produces compact representations for documents, to perform tasks like
classification or retrieval based on these short codes. When hashing is
supervised, the codes are trained using labels on the training data. This paper
first shows that the evaluation protocols used in the literature for supervised
hashing are not satisfactory: we show that a trivial solution that encodes the
output of a classifier significantly outperforms existing supervised or
semi-supervised methods, while using much shorter codes. We then propose two
alternative protocols for supervised hashing: one based on retrieval on a
disjoint set of classes, and another based on transfer learning to new classes.
We provide two baseline methods for image-related tasks to assess the
performance of (semi-)supervised hashing: without coding and with unsupervised
codes. These baselines give a lower- and upper-bound on the performance of a
supervised hashing scheme.</p>
</td>
    <td>
      
        Unsupervised 
      
        Neural Hashing 
      
        ICASSP 
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/matsui2017pqtable/">Pqtable: Non-exhaustive Fast Search For Product-quantized Codes Using Hash Tables</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Pqtable: Non-exhaustive Fast Search For Product-quantized Codes Using Hash Tables' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Pqtable: Non-exhaustive Fast Search For Product-quantized Codes Using Hash Tables' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Matsui Yusuke, Yamasaki Toshihiko, Aizawa Kiyoharu</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>8</td>
    <td><p>In this paper, we propose a product quantization table (PQTable); a fast
search method for product-quantized codes via hash-tables. An identifier of
each database vector is associated with the slot of a hash table by using its
PQ-code as a key. For querying, an input vector is PQ-encoded and hashed, and
the items associated with that code are then retrieved. The proposed PQTable
produces the same results as a linear PQ scan, and is 10^2 to 10^5 times
faster. Although state-of-the-art performance can be achieved by previous
inverted-indexing-based approaches, such methods require manually-designed
parameter setting and significant training; our PQTable is free of these
limitations, and therefore offers a practical and effective solution for
real-world problems. Specifically, when the vectors are highly compressed, our
PQTable achieves one of the fastest search performances on a single CPU to date
with significantly efficient memory usage (0.059 ms per query over 10^9 data
points with just 5.5 GB memory consumption). Finally, we show that our proposed
PQTable can naturally handle the codes of an optimized product quantization
(OPQTable).</p>
</td>
    <td>
      
        Quantization 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/wang2017flash/">FLASH: Randomized Algorithms Accelerated Over CPU-GPU For Ultra-high Dimensional Similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=FLASH: Randomized Algorithms Accelerated Over CPU-GPU For Ultra-high Dimensional Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=FLASH: Randomized Algorithms Accelerated Over CPU-GPU For Ultra-high Dimensional Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>9</td>
    <td><p>We present FLASH (\textbf{F}ast \textbf{L}SH \textbf{A}lgorithm for
\textbf{S}imilarity search accelerated with \textbf{H}PC), a similarity search
system for ultra-high dimensional datasets on a single machine, that does not
require similarity computations and is tailored for high-performance computing
platforms. By leveraging a LSH style randomized indexing procedure and
combining it with several principled techniques, such as reservoir sampling,
recent advances in one-pass minwise hashing, and count based estimations, we
reduce the computational and parallelization costs of similarity search, while
retaining sound theoretical guarantees.
  We evaluate FLASH on several real, high-dimensional datasets from different
domains, including text, malicious URL, click-through prediction, social
networks, etc. Our experiments shed new light on the difficulties associated
with datasets having several million dimensions. Current state-of-the-art
implementations either fail on the presented scale or are orders of magnitude
slower than FLASH. FLASH is capable of computing an approximate k-NN graph,
from scratch, over the full webspam dataset (1.3 billion nonzeros) in less than
10 seconds. Computing a full k-NN graph in less than 10 seconds on the webspam
dataset, using brute-force (\(n^2D\)), will require at least 20 teraflops. We
provide CPU and GPU implementations of FLASH for replicability of our results.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/laarhoven2017graph/">Graph-based Time-space Trade-offs For Approximate Near Neighbors</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Graph-based Time-space Trade-offs For Approximate Near Neighbors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Graph-based Time-space Trade-offs For Approximate Near Neighbors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Laarhoven Thijs</td> <!-- 🔧 You were missing this -->
    <td>34th International Symposium on Computational Geometry (SoCG) pp. 571-5714 2018</td>
    <td>5</td>
    <td><p>We take a first step towards a rigorous asymptotic analysis of graph-based
approaches for finding (approximate) nearest neighbors in high-dimensional
spaces, by analyzing the complexity of (randomized) greedy walks on the
approximate near neighbor graph. For random data sets of size \(n = 2^{o(d)}\) on
the \(d\)-dimensional Euclidean unit sphere, using near neighbor graphs we can
provably solve the approximate nearest neighbor problem with approximation
factor \(c &gt; 1\) in query time \(n^{\rho_q + o(1)}\) and space \(n^{1 + \rho_s +
o(1)}\), for arbitrary \(\rho_q, \rho_s \geq 0\) satisfying \begin{align} (2c^2 -
1) \rho_q + 2 c^2 (c^2 - 1) \sqrt{\rho_s (1 - \rho_s)} \geq c^4. \end{align}
Graph-based near neighbor searching is especially competitive with hash-based
methods for small \(c\) and near-linear memory, and in this regime the asymptotic
scaling of a greedy graph-based search matches the recent optimal hash-based
trade-offs of Andoni-Laarhoven-Razenshteyn-Waingarten [SODA’17]. We further
study how the trade-offs scale when the data set is of size \(n =
2^{\Theta(d)}\), and analyze asymptotic complexities when applying these results
to lattice sieving.</p>
</td>
    <td>
      
        Graph Based ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/conjeti2017learning/">Learning Robust Hash Codes For Multiple Instance Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Robust Hash Codes For Multiple Instance Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Robust Hash Codes For Multiple Instance Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Conjeti et al.</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition Letters</td>
    <td>12</td>
    <td><p>In this paper, for the first time, we introduce a multiple instance (MI) deep
hashing technique for learning discriminative hash codes with weak bag-level
supervision suited for large-scale retrieval. We learn such hash codes by
aggregating deeply learnt hierarchical representations across bag members
through a dedicated MI pool layer. For better trainability and retrieval
quality, we propose a two-pronged approach that includes robust optimization
and training with an auxiliary single instance hashing arm which is
down-regulated gradually. We pose retrieval for tumor assessment as an MI
problem because tumors often coexist with benign masses and could exhibit
complementary signatures when scanned from different anatomical views.
Experimental validations on benchmark mammography and histology datasets
demonstrate improved retrieval performance over the state-of-the-art methods.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/wang2017survey/">A Survey On Learning To Hash</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Survey On Learning To Hash' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Survey On Learning To Hash' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>920</td>
    <td><p>Nearest neighbor search is a problem of finding the data points from the
database such that the distances from them to the query point are the smallest.
Learning to hash is one of the major solutions to this problem and has been
widely studied recently. In this paper, we present a comprehensive survey of
the learning to hash algorithms, categorize them according to the manners of
preserving the similarities into: pairwise similarity preserving, multiwise
similarity preserving, implicit similarity preserving, as well as quantization,
and discuss their relations. We separate quantization from pairwise similarity
preserving as the objective function is very different though quantization, as
we show, can be derived from preserving the pairwise similarities. In addition,
we present the evaluation protocols, and the general performance analysis, and
point out that the quantization algorithms perform superiorly in terms of
search accuracy, search time cost, and space cost. Finally, we introduce a few
emerging topics.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Survey Paper 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/chung2017learning/">Learning Deep Representations Of Medical Images Using Siamese Cnns With Application To Content-based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Deep Representations Of Medical Images Using Siamese Cnns With Application To Content-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Deep Representations Of Medical Images Using Siamese Cnns With Application To Content-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chung Yu-an, Weng Wei-hung</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>63</td>
    <td><p>Deep neural networks have been investigated in learning latent
representations of medical images, yet most of the studies limit their approach
in a single supervised convolutional neural network (CNN), which usually rely
heavily on a large scale annotated dataset for training. To learn image
representations with less supervision involved, we propose a deep Siamese CNN
(SCNN) architecture that can be trained with only binary image pair
information. We evaluated the learned image representations on a task of
content-based medical image retrieval using a publicly available multiclass
diabetic retinopathy fundus image dataset. The experimental results show that
our proposed deep SCNN is comparable to the state-of-the-art single supervised
CNN, and requires much less supervision for training.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/corbi%C3%A8re2017leveraging/">Leveraging Weakly Annotated Data For Fashion Image Retrieval And Label Prediction</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Leveraging Weakly Annotated Data For Fashion Image Retrieval And Label Prediction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Leveraging Weakly Annotated Data For Fashion Image Retrieval And Label Prediction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Corbière et al.</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Conference on Computer Vision Workshops (ICCVW)</td>
    <td>59</td>
    <td><p>In this paper, we present a method to learn a visual representation adapted
for e-commerce products. Based on weakly supervised learning, our model learns
from noisy datasets crawled on e-commerce website catalogs and does not require
any manual labeling. We show that our representation can be used for downward
classification tasks over clothing categories with different levels of
granularity. We also demonstrate that the learnt representation is suitable for
image retrieval. We achieve nearly state-of-art results on the DeepFashion
In-Shop Clothes Retrieval and Categories Attributes Prediction tasks, without
using the provided training set.</p>
</td>
    <td>
      
        Image Retrieval 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/christiani2017framework/">A Framework For Similarity Search With Space-time Tradeoffs Using Locality-sensitive Filtering</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Framework For Similarity Search With Space-time Tradeoffs Using Locality-sensitive Filtering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Framework For Similarity Search With Space-time Tradeoffs Using Locality-sensitive Filtering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Christiani Tobias</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>21</td>
    <td><p>We present a framework for similarity search based on Locality-Sensitive
Filtering (LSF), generalizing the Indyk-Motwani (STOC 1998) Locality-Sensitive
Hashing (LSH) framework to support space-time tradeoffs. Given a family of
filters, defined as a distribution over pairs of subsets of space with certain
locality-sensitivity properties, we can solve the approximate near neighbor
problem in \(d\)-dimensional space for an \(n\)-point data set with query time
\(dn^{\rho_q+o(1)}\), update time \(dn^{\rho_u+o(1)}\), and space usage \(dn + n^{1</p>
<ul>
  <li>\rho_u + o(1)}\). The space-time tradeoff is tied to the tradeoff between
query time and update time, controlled by the exponents \(\rho_q, \rho_u\) that
are determined by the filter family. Locality-sensitive filtering was
introduced by Becker et al. (SODA 2016) together with a framework yielding a
single, balanced, tradeoff between query time and space, further relying on the
assumption of an efficient oracle for the filter evaluation algorithm. We
extend the LSF framework to support space-time tradeoffs and through a
combination of existing techniques we remove the oracle assumption.
Building on a filter family for the unit sphere by Laarhoven (arXiv 2015) we
use a kernel embedding technique by Rahimi &amp; Recht (NIPS 2007) to show a
solution to the \((r,cr)\)-near neighbor problem in \(\ell_s^d\)-space for \(0 &lt; s
\leq 2\) with query and update exponents
\(\rho_q=\frac{c^s(1+\lambda)^2}{(c^s+\lambda)^2}\) and
\(\rho_u=\frac{c^s(1-\lambda)^2}{(c^s+\lambda)^2}\) where \(\lambda\in[-1,1]\) is a
tradeoff parameter. This result improves upon the space-time tradeoff of
Kapralov (PODS 2015) and is shown to be optimal in the case of a balanced
tradeoff. Finally, we show a lower bound for the space-time tradeoff on the
unit sphere that matches Laarhoven’s and our own upper bound in the case of
random data.</li>
</ul>
</td>
    <td>
      
        Similarity Search 
      
        Tools & Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/christiani2017set/">Set Similarity Search Beyond Minhash</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Set Similarity Search Beyond Minhash' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Set Similarity Search Beyond Minhash' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Christiani Tobias, Pagh Rasmus</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing</td>
    <td>45</td>
    <td><p>We consider the problem of approximate set similarity search under
Braun-Blanquet similarity \(B(\mathbf{x}, \mathbf{y}) = |\mathbf{x} \cap
\mathbf{y}| / \max(|\mathbf{x}|, |\mathbf{y}|)\). The \((b_2, b_2)\)-approximate
Braun-Blanquet similarity search problem is to preprocess a collection of sets
\(P\) such that, given a query set \(\mathbf{q}\), if there exists \(\mathbf{x} \in
P\) with \(B(\mathbf{q}, \mathbf{x}) \geq b_1\), then we can efficiently return
\(\mathbf{x}’ \in P\) with \(B(\mathbf{q}, \mathbf{x}’) &gt; b_2\).
  We present a simple data structure that solves this problem with space usage
\(O(n^{1+\rho}log n + \sum_{\mathbf{x} \in P}|\mathbf{x}|)\) and query time
\(O(|\mathbf{q}|n^{\rho} log n)\) where \(n = |P|\) and \(\rho =
log(1/b_1)/log(1/b_2)\). Making use of existing lower bounds for
locality-sensitive hashing by O’Donnell et al. (TOCT 2014) we show that this
value of \(\rho\) is tight across the parameter space, i.e., for every choice of
constants \(0 &lt; b_2 &lt; b_1 &lt; 1\).
  In the case where all sets have the same size our solution strictly improves
upon the value of \(\rho\) that can be obtained through the use of
state-of-the-art data-independent techniques in the Indyk-Motwani
locality-sensitive hashing framework (STOC 1998) such as Broder’s MinHash (CCS
1997) for Jaccard similarity and Andoni et al.’s cross-polytope LSH (NIPS 2015)
for cosine similarity. Surprisingly, even though our solution is
data-independent, for a large part of the parameter space we outperform the
currently best data-dependent method by Andoni and Razenshteyn (STOC 2015).</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/song2017deep/">Deep Metric Learning Via Facility Location</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Metric Learning Via Facility Location' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Metric Learning Via Facility Location' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Song et al.</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>308</td>
    <td><p>Learning the representation and the similarity metric in an end-to-end
fashion with deep networks have demonstrated outstanding results for clustering
and retrieval. However, these recent approaches still suffer from the
performance degradation stemming from the local metric training procedure which
is unaware of the global structure of the embedding space.
  We propose a global metric learning scheme for optimizing the deep metric
embedding with the learnable clustering function and the clustering metric
(NMI) in a novel structured prediction framework.
  Our experiments on CUB200-2011, Cars196, and Stanford online products
datasets show state of the art performance both on the clustering and retrieval
tasks measured in the NMI and Recall@K evaluation metrics.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/kumar2017neural/">Neural Signatures For Licence Plate Re-identification</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Neural Signatures For Licence Plate Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Neural Signatures For Licence Plate Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kumar et al.</td> <!-- 🔧 You were missing this -->
    <td>2020 International Conference for Emerging Technology (INCET)</td>
    <td>7</td>
    <td><p>The problem of vehicle licence plate re-identification is generally
considered as a one-shot image retrieval problem. The objective of this task is
to learn a feature representation (called a “signature”) for licence plates.
Incoming licence plate images are converted to signatures and matched to a
previously collected template database through a distance measure. Then, the
input image is recognized as the template whose signature is “nearest” to the
input signature. The template database is restricted to contain only a single
signature per unique licence plate for our problem.
  We measure the performance of deep convolutional net-based features adapted
from face recognition on this task. In addition, we also test a hybrid approach
combining the Fisher vector with a neural network-based embedding called “f2nn”
trained with the Triplet loss function. We find that the hybrid approach
performs comparably while providing computational benefits. The signature
generated by the hybrid approach also shows higher generalizability to datasets
more dissimilar to the training corpus.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/rygl2017semantic/">Semantic Vector Encoding And Similarity Search Using Fulltext Search Engines</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semantic Vector Encoding And Similarity Search Using Fulltext Search Engines' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semantic Vector Encoding And Similarity Search Using Fulltext Search Engines' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Rygl et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2nd Workshop on Representation Learning for NLP</td>
    <td>12</td>
    <td><p>Vector representations and vector space modeling (VSM) play a central role in
modern machine learning. We propose a novel approach to `vector similarity
searching’ over dense semantic representations of words and documents that can
be deployed on top of traditional inverted-index-based fulltext engines, taking
advantage of their robustness, stability, scalability and ubiquity.
  We show that this approach allows the indexing and querying of dense vectors
in text domains. This opens up exciting avenues for major efficiency gains,
along with simpler deployment, scaling and monitoring.
  The end result is a fast and scalable vector database with a tunable
trade-off between vector search performance and quality, backed by a standard
fulltext engine such as Elasticsearch.
  We empirically demonstrate its querying performance and quality by applying
this solution to the task of semantic searching over a dense vector
representation of the entire English Wikipedia.</p>
</td>
    <td>
      
        Similarity Search 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/oymak2017near/">Near-optimal Sample Complexity Bounds For Circulant Binary Embedding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Near-optimal Sample Complexity Bounds For Circulant Binary Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Near-optimal Sample Complexity Bounds For Circulant Binary Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Oymak Samet</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</td>
    <td>13</td>
    <td><p>Binary embedding is the problem of mapping points from a high-dimensional
space to a Hamming cube in lower dimension while preserving pairwise distances.
An efficient way to accomplish this is to make use of fast embedding techniques
involving Fourier transform e.g.~circulant matrices. While binary embedding has
been studied extensively, theoretical results on fast binary embedding are
rather limited. In this work, we build upon the recent literature to obtain
significantly better dependencies on the problem parameters. A set of \(N\)
points in \(\mathbb{R}^n\) can be properly embedded into the Hamming cube \(\{\pm
1\}^k\) with \(\delta\) distortion, by using \(k\sim\delta^{-3}log N\) samples
which is optimal in the number of points \(N\) and compares well with the optimal
distortion dependency \(\delta^{-2}\). Our optimal embedding result applies in
the regime \(log N\lesssim n^{1/3}\). Furthermore, if the looser condition \(log
N\lesssim \sqrt{n}\) holds, we show that all but an arbitrarily small fraction
of the points can be optimally embedded. We believe our techniques can be
useful to obtain improved guarantees for other nonlinear embedding problems.</p>
</td>
    <td>
      
        Hashing Methods 
      
        ICASSP 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/wu2017sampling/">Sampling Matters In Deep Embedding Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Sampling Matters In Deep Embedding Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Sampling Matters In Deep Embedding Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu et al.</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Conference on Computer Vision (ICCV)</td>
    <td>863</td>
    <td><p>Deep embeddings answer one simple question: How similar are two images?
Learning these embeddings is the bedrock of verification, zero-shot learning,
and visual search. The most prominent approaches optimize a deep convolutional
network with a suitable loss function, such as contrastive loss or triplet
loss. While a rich line of work focuses solely on the loss functions, we show
in this paper that selecting training examples plays an equally important role.
We propose distance weighted sampling, which selects more informative and
stable examples than traditional approaches. In addition, we show that a simple
margin based loss is sufficient to outperform all other loss functions. We
evaluate our approach on the Stanford Online Products, CAR196, and the
CUB200-2011 datasets for image retrieval and clustering, and on the LFW dataset
for face verification. Our method achieves state-of-the-art performance on all
of them.</p>
</td>
    <td>
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/wei2017selective/">Selective Convolutional Descriptor Aggregation For Fine-grained Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Selective Convolutional Descriptor Aggregation For Fine-grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Selective Convolutional Descriptor Aggregation For Fine-grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wei et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>431</td>
    <td><p>Deep convolutional neural network models pre-trained for the ImageNet
classification task have been successfully adopted to tasks in other domains,
such as texture description and object proposal generation, but these tasks
require annotations for images in the new domain. In this paper, we focus on a
novel and challenging task in the pure unsupervised setting: fine-grained image
retrieval. Even with image labels, fine-grained images are difficult to
classify, let alone the unsupervised retrieval task. We propose the Selective
Convolutional Descriptor Aggregation (SCDA) method. SCDA firstly localizes the
main object in fine-grained images, a step that discards the noisy background
and keeps useful deep descriptors. The selected descriptors are then aggregated
and dimensionality reduced into a short feature vector using the best practices
we found. SCDA is unsupervised, using no image label or bounding box
annotation. Experiments on six fine-grained datasets confirm the effectiveness
of SCDA for fine-grained image retrieval. Besides, visualization of the SCDA
features shows that they correspond to visual attributes (even subtle ones),
which might explain SCDA’s high mean average precision in fine-grained
retrieval. Moreover, on general image retrieval datasets, SCDA achieves
comparable retrieval results with state-of-the-art general image retrieval
approaches.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/pachori2017hashing/">Hashing In The Zero Shot Framework With Domain Adaptation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing In The Zero Shot Framework With Domain Adaptation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing In The Zero Shot Framework With Domain Adaptation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Pachori Shubham, Deshpande Ameya, Raman Shanmuganathan</td> <!-- 🔧 You were missing this -->
    <td>Neurocomputing</td>
    <td>24</td>
    <td><p>Techniques to learn hash codes which can store and retrieve large dimensional
multimedia data efficiently have attracted broad research interests in the
recent years. With rapid explosion of newly emerged concepts and online data,
existing supervised hashing algorithms suffer from the problem of scarcity of
ground truth annotations due to the high cost of obtaining manual annotations.
Therefore, we propose an algorithm to learn a hash function from training
images belonging to <code class="language-plaintext highlighter-rouge">seen' classes which can efficiently encode images of
</code>unseen’ classes to binary codes. Specifically, we project the image features
from visual space and semantic features from semantic space into a common
Hamming subspace. Earlier works to generate hash codes have tried to relax the
discrete constraints on hash codes and solve the continuous optimization
problem. However, it often leads to quantization errors. In this work, we use
the max-margin classifier to learn an efficient hash function. To address the
concern of domain-shift which may arise due to the introduction of new classes,
we also introduce an unsupervised domain adaptation model in the proposed
hashing framework. Results on the three datasets show the advantage of using
domain adaptation in learning a high-quality hash function and superiority of
our method for the task of image retrieval performance as compared to several
state-of-the-art hashing methods.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Tools & Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/veit2017conditional/">Conditional Similarity Networks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Conditional Similarity Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Conditional Similarity Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Veit Andreas, Belongie Serge, Karaletsos Theofanis</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>168</td>
    <td><p>What makes images similar? To measure the similarity between images, they are
typically embedded in a feature-vector space, in which their distance preserve
the relative dissimilarity. However, when learning such similarity embeddings
the simplifying assumption is commonly made that images are only compared to
one unique measure of similarity. A main reason for this is that contradicting
notions of similarities cannot be captured in a single space. To address this
shortcoming, we propose Conditional Similarity Networks (CSNs) that learn
embeddings differentiated into semantically distinct subspaces that capture the
different notions of similarities. CSNs jointly learn a disentangled embedding
where features for different similarities are encoded in separate dimensions as
well as masks that select and reweight relevant dimensions to induce a subspace
that encodes a specific similarity notion. We show that our approach learns
interpretable image representations with visually relevant semantic subspaces.
Further, when evaluating on triplet questions from multiple similarity notions
our model even outperforms the accuracy obtained by training individual
specialized networks for each notion separately.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/ji2017cross/">Cross-domain Image Retrieval With Attention Modeling</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-domain Image Retrieval With Attention Modeling' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-domain Image Retrieval With Attention Modeling' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ji et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 25th ACM international conference on Multimedia</td>
    <td>79</td>
    <td><p>With the proliferation of e-commerce websites and the ubiquitousness of smart
phones, cross-domain image retrieval using images taken by smart phones as
queries to search products on e-commerce websites is emerging as a popular
application. One challenge of this task is to locate the attention of both the
query and database images. In particular, database images, e.g. of fashion
products, on e-commerce websites are typically displayed with other
accessories, and the images taken by users contain noisy background and large
variations in orientation and lighting. Consequently, their attention is
difficult to locate. In this paper, we exploit the rich tag information
available on the e-commerce websites to locate the attention of database
images. For query images, we use each candidate image in the database as the
context to locate the query attention. Novel deep convolutional neural network
architectures, namely TagYNet and CtxYNet, are proposed to learn the attention
weights and then extract effective representations of the images. Experimental
results on public datasets confirm that our approaches have significant
improvement over the existing methods in terms of the retrieval accuracy and
efficiency.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/lau2017end/">End-to-end Network For Twitter Geolocation Prediction And Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=End-to-end Network For Twitter Geolocation Prediction And Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=End-to-end Network For Twitter Geolocation Prediction And Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lau et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>7</td>
    <td><p>We propose an end-to-end neural network to predict the geolocation of a
tweet. The network takes as input a number of raw Twitter metadata such as the
tweet message and associated user account information. Our model is language
independent, and despite minimal feature engineering, it is interpretable and
capable of learning location indicative words and timing patterns. Compared to
state-of-the-art systems, our model outperforms them by 2%-6%. Additionally, we
propose extensions to the model to compress representation learnt by the
network into binary codes. Experiments show that it produces compact codes
compared to benchmark hashing algorithms. An implementation of the model is
released publicly.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/laskar2017context/">Context Aware Query Image Representation For Particular Object Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Context Aware Query Image Representation For Particular Object Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Context Aware Query Image Representation For Particular Object Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Laskar Zakaria, Kannala Juho</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>42</td>
    <td><p>The current models of image representation based on Convolutional Neural
Networks (CNN) have shown tremendous performance in image retrieval. Such
models are inspired by the information flow along the visual pathway in the
human visual cortex. We propose that in the field of particular object
retrieval, the process of extracting CNN representations from query images with
a given region of interest (ROI) can also be modelled by taking inspiration
from human vision. Particularly, we show that by making the CNN pay attention
on the ROI while extracting query image representation leads to significant
improvement over the baseline methods on challenging Oxford5k and Paris6k
datasets. Furthermore, we propose an extension to a recently introduced
encoding method for CNN representations, regional maximum activations of
convolutions (R-MAC). The proposed extension weights the regional
representations using a novel saliency measure prior to aggregation. This leads
to further improvement in retrieval accuracy.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/wei2017glad/">GLAD: Global-local-alignment Descriptor For Pedestrian Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=GLAD: Global-local-alignment Descriptor For Pedestrian Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=GLAD: Global-local-alignment Descriptor For Pedestrian Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wei et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>134</td>
    <td><p>The huge variance of human pose and the misalignment of detected human images
significantly increase the difficulty of person Re-Identification (Re-ID).
Moreover, efficient Re-ID systems are required to cope with the massive visual
data being produced by video surveillance systems. Targeting to solve these
problems, this work proposes a Global-Local-Alignment Descriptor (GLAD) and an
efficient indexing and retrieval framework, respectively. GLAD explicitly
leverages the local and global cues in human body to generate a discriminative
and robust representation. It consists of part extraction and descriptor
learning modules, where several part regions are first detected and then deep
neural networks are designed for representation learning on both the local and
global regions. A hierarchical indexing and retrieval framework is designed to
eliminate the huge redundancy in the gallery set, and accelerate the online
Re-ID procedure. Extensive experimental results show GLAD achieves competitive
accuracy compared to the state-of-the-art methods. Our retrieval framework
significantly accelerates the online Re-ID procedure without loss of accuracy.
Therefore, this work has potential to work better on person Re-ID tasks in real
scenarios.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/podlesnaya2017deep/">Deep Learning Based Semantic Video Indexing And Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Learning Based Semantic Video Indexing And Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Learning Based Semantic Video Indexing And Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Podlesnaya Anna, Podlesnyy Sergey</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Networks and Systems</td>
    <td>20</td>
    <td><p>We share the implementation details and testing results for video retrieval
system based exclusively on features extracted by convolutional neural
networks. We show that deep learned features might serve as universal signature
for semantic content of video useful in many search and retrieval tasks. We
further show that graph-based storage structure for video index allows to
efficiently retrieving the content with complicated spatial and temporal search
queries.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/zhou2017learning/">Learning Low Dimensional Convolutional Neural Networks For High-resolution Remote Sensing Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Low Dimensional Convolutional Neural Networks For High-resolution Remote Sensing Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Low Dimensional Convolutional Neural Networks For High-resolution Remote Sensing Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhou et al.</td> <!-- 🔧 You were missing this -->
    <td>Remote Sensing</td>
    <td>139</td>
    <td><p>Learning powerful feature representations for image retrieval has always been
a challenging task in the field of remote sensing. Traditional methods focus on
extracting low-level hand-crafted features which are not only time-consuming
but also tend to achieve unsatisfactory performance due to the content
complexity of remote sensing images. In this paper, we investigate how to
extract deep feature representations based on convolutional neural networks
(CNN) for high-resolution remote sensing image retrieval (HRRSIR). To this end,
two effective schemes are proposed to generate powerful feature representations
for HRRSIR. In the first scheme, the deep features are extracted from the
fully-connected and convolutional layers of the pre-trained CNN models,
respectively; in the second scheme, we propose a novel CNN architecture based
on conventional convolution layers and a three-layer perceptron. The novel CNN
model is then trained on a large remote sensing dataset to learn low
dimensional features. The two schemes are evaluated on several public and
challenging datasets, and the results indicate that the proposed schemes and in
particular the novel CNN are able to achieve state-of-the-art performance.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/chen2017doctag2vec/">Doctag2vec: An Embedding Based Multi-label Learning Approach For Document Tagging</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Doctag2vec: An Embedding Based Multi-label Learning Approach For Document Tagging' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Doctag2vec: An Embedding Based Multi-label Learning Approach For Document Tagging' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2nd Workshop on Representation Learning for NLP</td>
    <td>21</td>
    <td><p>Tagging news articles or blog posts with relevant tags from a collection of
predefined ones is coined as document tagging in this work. Accurate tagging of
articles can benefit several downstream applications such as recommendation and
search. In this work, we propose a novel yet simple approach called DocTag2Vec
to accomplish this task. We substantially extend Word2Vec and Doc2Vec—two
popular models for learning distributed representation of words and documents.
In DocTag2Vec, we simultaneously learn the representation of words, documents,
and tags in a joint vector space during training, and employ the simple
\(k\)-nearest neighbor search to predict tags for unseen documents. In contrast
to previous multi-label learning methods, DocTag2Vec directly deals with raw
text instead of provided feature vector, and in addition, enjoys advantages
like the learning of tag representation, and the ability of handling newly
created tags. To demonstrate the effectiveness of our approach, we conduct
experiments on several datasets and show promising results against
state-of-the-art methods.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/huang2017online/">Online Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Online Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Online Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Huang Long-kai, Yang Qiang, Zheng Wei-shi</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Conference on Computer Vision (ICCV)</td>
    <td>99</td>
    <td><p>Although hash function learning algorithms have achieved great success in
recent years, most existing hash models are off-line, which are not suitable
for processing sequential or online data. To address this problem, this work
proposes an online hash model to accommodate data coming in stream for online
learning. Specifically, a new loss function is proposed to measure the
similarity loss between a pair of data samples in hamming space. Then, a
structured hash model is derived and optimized in a passive-aggressive way.
Theoretical analysis on the upper bound of the cumulative loss for the proposed
online hash model is provided. Furthermore, we extend our online hashing from a
single-model to a multi-model online hashing that trains multiple models so as
to retain diverse online hashing models in order to avoid biased update. The
competitive efficiency and effectiveness of the proposed online hash models are
verified through extensive experiments on several large-scale datasets as
compared to related hashing methods.</p>
</td>
    <td>
      
        Hashing Methods 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/vu2017search/">Search Personalization With Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Search Personalization With Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Search Personalization With Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Vu et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>48</td>
    <td><p>Recent research has shown that the performance of search personalization
depends on the richness of user profiles which normally represent the user’s
topical interests. In this paper, we propose a new embedding approach to
learning user profiles, where users are embedded on a topical interest space.
We then directly utilize the user profiles for search personalization.
Experiments on query logs from a major commercial web search engine demonstrate
that our embedding approach improves the performance of the search engine and
also achieves better search performance than other strong baselines.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/chandrasekaran2017lattice/">Lattice-based Locality Sensitive Hashing Is Optimal</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Lattice-based Locality Sensitive Hashing Is Optimal' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Lattice-based Locality Sensitive Hashing Is Optimal' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chandrasekaran et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the IEEE</td>
    <td>40</td>
    <td><p>Locality sensitive hashing (LSH) was introduced by Indyk and Motwani (STOC
<code class="language-plaintext highlighter-rouge">98) to give the first sublinear time algorithm for the c-approximate nearest
neighbor (ANN) problem using only polynomial space. At a high level, an LSH
family hashes "nearby" points to the same bucket and "far away" points to
different buckets. The quality of measure of an LSH family is its LSH exponent,
which helps determine both query time and space usage.
  In a seminal work, Andoni and Indyk (FOCS </code>06) constructed an LSH family
based on random ball partitioning of space that achieves an LSH exponent of
1/c^2 for the l_2 norm, which was later shown to be optimal by Motwani, Naor
and Panigrahy (SIDMA <code class="language-plaintext highlighter-rouge">07) and O'Donnell, Wu and Zhou (TOCT </code>14). Although
optimal in the LSH exponent, the ball partitioning approach is computationally
expensive. So, in the same work, Andoni and Indyk proposed a simpler and more
practical hashing scheme based on Euclidean lattices and provided computational
results using the 24-dimensional Leech lattice. However, no theoretical
analysis of the scheme was given, thus leaving open the question of finding the
exponent of lattice based LSH.
  In this work, we resolve this question by showing the existence of lattices
achieving the optimal LSH exponent of 1/c^2 using techniques from the geometry
of numbers. At a more conceptual level, our results show that optimal LSH space
partitions can have periodic structure. Understanding the extent to which
additional structure can be imposed on these partitions, e.g. to yield low
space and query complexity, remains an important open problem.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/chandrasekhar2017compression/">Compression Of Deep Neural Networks For Image Instance Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Compression Of Deep Neural Networks For Image Instance Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Compression Of Deep Neural Networks For Image Instance Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chandrasekhar et al.</td> <!-- 🔧 You were missing this -->
    <td>2017 Data Compression Conference (DCC)</td>
    <td>21</td>
    <td><p>Image instance retrieval is the problem of retrieving images from a database
which contain the same object. Convolutional Neural Network (CNN) based
descriptors are becoming the dominant approach for generating {\it global image
descriptors} for the instance retrieval problem. One major drawback of
CNN-based {\it global descriptors} is that uncompressed deep neural network
models require hundreds of megabytes of storage making them inconvenient to
deploy in mobile applications or in custom hardware. In this work, we study the
problem of neural network model compression focusing on the image instance
retrieval task. We study quantization, coding, pruning and weight sharing
techniques for reducing model size for the instance retrieval problem. We
provide extensive experimental results on the trade-off between retrieval
performance and model size for different types of networks on several data sets
providing the most comprehensive study on this topic. We compress models to the
order of a few MBs: two orders of magnitude smaller than the uncompressed
models while achieving negligible loss in retrieval performance.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/chaidaroon2017variational/">Variational Deep Semantic Hashing For Text Documents</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Variational Deep Semantic Hashing For Text Documents' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Variational Deep Semantic Hashing For Text Documents' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chaidaroon Suthee, Fang Yi</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>71</td>
    <td><p>As the amount of textual data has been rapidly increasing over the past
decade, efficient similarity search methods have become a crucial component of
large-scale information retrieval systems. A popular strategy is to represent
original data samples by compact binary codes through hashing. A spectrum of
machine learning methods have been utilized, but they often lack expressiveness
and flexibility in modeling to learn effective representations. The recent
advances of deep learning in a wide range of applications has demonstrated its
capability to learn robust and powerful feature representations for complex
data. Especially, deep generative models naturally combine the expressiveness
of probabilistic generative models with the high capacity of deep neural
networks, which is very suitable for text modeling. However, little work has
leveraged the recent progress in deep learning for text hashing.
  In this paper, we propose a series of novel deep document generative models
for text hashing. The first proposed model is unsupervised while the second one
is supervised by utilizing document labels/tags for hashing. The third model
further considers document-specific factors that affect the generation of
words. The probabilistic generative formulation of the proposed models provides
a principled framework for model extension, uncertainty estimation, simulation,
and interpretability. Based on variational inference and reparameterization,
the proposed models can be interpreted as encoder-decoder deep neural networks
and thus they are capable of learning complex nonlinear distributed
representations of the original documents. We conduct a comprehensive set of
experiments on four public testbeds. The experimental results have demonstrated
the effectiveness of the proposed supervised learning models for text hashing.</p>
</td>
    <td>
      
        SIGIR 
      
        Hashing Methods 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/chadha2017voronoi/">Voronoi-based Compact Image Descriptors: Efficient Region-of-interest Retrieval With VLAD And Deep-learning-based Descriptors</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Voronoi-based Compact Image Descriptors: Efficient Region-of-interest Retrieval With VLAD And Deep-learning-based Descriptors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Voronoi-based Compact Image Descriptors: Efficient Region-of-interest Retrieval With VLAD And Deep-learning-based Descriptors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chadha Aaron, Andreopoulos Yiannis</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>29</td>
    <td><p>We investigate the problem of image retrieval based on visual queries when
the latter comprise arbitrary regions-of-interest (ROI) rather than entire
images. Our proposal is a compact image descriptor that combines the
state-of-the-art in content-based descriptor extraction with a multi-level,
Voronoi-based spatial partitioning of each dataset image. The proposed
multi-level Voronoi-based encoding uses a spatial hierarchical K-means over
interest-point locations, and computes a content-based descriptor over each
cell. In order to reduce the matching complexity with minimal or no sacrifice
in retrieval performance: (i) we utilize the tree structure of the spatial
hierarchical K-means to perform a top-to-bottom pruning for local similarity
maxima; (ii) we propose a new image similarity score that combines relevant
information from all partition levels into a single measure for similarity;
(iii) we combine our proposal with a novel and efficient approach for optimal
bit allocation within quantized descriptor representations. By deriving both a
Voronoi-based VLAD descriptor (termed as Fast-VVLAD) and a Voronoi-based deep
convolutional neural network (CNN) descriptor (termed as Fast-VDCNN), we
demonstrate that our Voronoi-based framework is agnostic to the descriptor
basis, and can easily be slotted into existing frameworks. Via a range of ROI
queries in two standard datasets, it is shown that the Voronoi-based
descriptors achieve comparable or higher mean Average Precision against
conventional grid-based spatial search, while offering more than two-fold
reduction in complexity. Finally, beyond ROI queries, we show that Voronoi
partitioning improves the geometric invariance of compact CNN descriptors,
thereby resulting in competitive performance to the current state-of-the-art on
whole image retrieval.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/han2017beyond/">Beyond SIFT Using Binary Features For Loop Closure Detection</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Beyond SIFT Using Binary Features For Loop Closure Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Beyond SIFT Using Binary Features For Loop Closure Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Han et al.</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</td>
    <td>10</td>
    <td><p>In this paper a binary feature based Loop Closure Detection (LCD) method is
proposed, which for the first time achieves higher precision-recall (PR)
performance compared with state-of-the-art SIFT feature based approaches. The
proposed system originates from our previous work Multi-Index hashing for Loop
closure Detection (MILD), which employs Multi-Index Hashing
(MIH)~\cite{greene1994multi} for Approximate Nearest Neighbor (ANN) search of
binary features. As the accuracy of MILD is limited by repeating textures and
inaccurate image similarity measurement, burstiness handling is introduced to
solve this problem and achieves considerable accuracy improvement.
Additionally, a comprehensive theoretical analysis on MIH used in MILD is
conducted to further explore the potentials of hashing methods for ANN search
of binary features from probabilistic perspective. This analysis provides more
freedom on best parameter choosing in MIH for different application scenarios.
Experiments on popular public datasets show that the proposed approach achieved
the highest accuracy compared with state-of-the-art while running at 30Hz for
databases containing thousands of images.</p>
</td>
    <td>
      
        IROS 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/han2017mild/">MILD: Multi-index Hashing For Loop Closure Detection</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=MILD: Multi-index Hashing For Loop Closure Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=MILD: Multi-index Hashing For Loop Closure Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Han Lei, Fang Lu</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Conference on Multimedia and Expo (ICME)</td>
    <td>9</td>
    <td><p>Loop Closure Detection (LCD) has been proved to be extremely useful in global
consistent visual Simultaneously Localization and Mapping (SLAM) and
appearance-based robot relocalization. Methods exploiting binary features in
bag of words representation have recently gained a lot of popularity for their
efficiency, but suffer from low recall due to the inherent drawback that high
dimensional binary feature descriptors lack well-defined centroids. In this
paper, we propose a realtime LCD approach called MILD (Multi-Index Hashing for
Loop closure Detection), in which image similarity is measured by feature
matching directly to achieve high recall without introducing extra
computational complexity with the aid of Multi-Index Hashing (MIH). A
theoretical analysis of the approximate image similarity measurement using MIH
is presented, which reveals the trade-off between efficiency and accuracy from
a probabilistic perspective. Extensive comparisons with state-of-the-art LCD
methods demonstrate the superiority of MILD in both efficiency and accuracy.</p>
</td>
    <td>
      
        Vector Indexing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/cao2017hashnet/">Hashnet: Deep Learning To Hash By Continuation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashnet: Deep Learning To Hash By Continuation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashnet: Deep Learning To Hash By Continuation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cao et al.</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Conference on Computer Vision (ICCV)</td>
    <td>643</td>
    <td><p>Learning to hash has been widely applied to approximate nearest neighbor
search for large-scale multimedia retrieval, due to its computation efficiency
and retrieval quality. Deep learning to hash, which improves retrieval quality
by end-to-end representation learning and hash encoding, has received
increasing attention recently. Subject to the ill-posed gradient difficulty in
the optimization with sign activations, existing deep learning to hash methods
need to first learn continuous representations and then generate binary hash
codes in a separated binarization step, which suffer from substantial loss of
retrieval quality. This work presents HashNet, a novel deep architecture for
deep learning to hash by continuation method with convergence guarantees, which
learns exactly binary hash codes from imbalanced similarity data. The key idea
is to attack the ill-posed gradient problem in optimizing deep networks with
non-smooth binary activations by continuation method, in which we begin from
learning an easier network with smoothed activation function and let it evolve
during the training, until it eventually goes back to being the original,
difficult to optimize, deep network with the sign activation function.
Comprehensive empirical evidence shows that HashNet can generate exactly binary
hash codes and yield state-of-the-art multimedia retrieval performance on
standard benchmarks.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/cao2017collective/">Collective Deep Quantization For Efficient Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Collective Deep Quantization For Efficient Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Collective Deep Quantization For Efficient Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cao et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>97</td>
    <td><p>Cross-modal similarity retrieval is a problem about designing a retrieval system that supports querying across
content modalities, e.g., using an image to retrieve for
texts. This paper presents a compact coding solution for
efficient cross-modal retrieval, with a focus on the quantization approach which has already shown the superior
performance over the hashing solutions in single-modal
similarity retrieval. We propose a collective deep quantization (CDQ) approach, which is the first attempt to
introduce quantization in end-to-end deep architecture
for cross-modal retrieval. The major contribution lies in
jointly learning deep representations and the quantizers
for both modalities using carefully-crafted hybrid networks and well-specified loss functions. In addition, our
approach simultaneously learns the common quantizer
codebook for both modalities through which the crossmodal correlation can be substantially enhanced. CDQ
enables efficient and effective cross-modal retrieval using inner product distance computed based on the common codebook with fast distance table lookup. Extensive experiments show that CDQ yields state of the art
cross-modal retrieval results on standard benchmarks.</p>
</td>
    <td>
      
        Quantization 
      
        Multimodal Retrieval 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/cao2017correlation/">Correlation Hashing Network For Efficient Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Correlation Hashing Network For Efficient Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Correlation Hashing Network For Efficient Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cao et al.</td> <!-- 🔧 You were missing this -->
    <td>Procedings of the British Machine Vision Conference 2017</td>
    <td>62</td>
    <td><p>Hashing is widely applied to approximate nearest neighbor search for
large-scale multimodal retrieval with storage and computation efficiency.
Cross-modal hashing improves the quality of hash coding by exploiting semantic
correlations across different modalities. Existing cross-modal hashing methods
first transform data into low-dimensional feature vectors, and then generate
binary codes by another separate quantization step. However, suboptimal hash
codes may be generated since the quantization error is not explicitly minimized
and the feature representation is not jointly optimized with the binary codes.
This paper presents a Correlation Hashing Network (CHN) approach to cross-modal
hashing, which jointly learns good data representation tailored to hash coding
and formally controls the quantization error. The proposed CHN is a hybrid deep
architecture that constitutes a convolutional neural network for learning good
image representations, a multilayer perception for learning good text
representations, two hashing layers for generating compact binary codes, and a
structured max-margin loss that integrates all things together to enable
learning similarity-preserving and high-quality hash codes. Extensive empirical
study shows that CHN yields state of the art cross-modal retrieval performance
on standard benchmarks.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/cao2017transitive/">Transitive Hashing Network For Heterogeneous Multimedia Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Transitive Hashing Network For Heterogeneous Multimedia Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Transitive Hashing Network For Heterogeneous Multimedia Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cao Zhangjie, Long Mingsheng, Yang Qiang</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>25</td>
    <td><p>Hashing has been widely applied to large-scale multimedia retrieval due to
the storage and retrieval efficiency. Cross-modal hashing enables efficient
retrieval from database of one modality in response to a query of another
modality. Existing work on cross-modal hashing assumes heterogeneous
relationship across modalities for hash function learning. In this paper, we
relax the strong assumption by only requiring such heterogeneous relationship
in an auxiliary dataset different from the query/database domain. We craft a
hybrid deep architecture to simultaneously learn the cross-modal correlation
from the auxiliary dataset, and align the dataset distributions between the
auxiliary dataset and the query/database domain, which generates transitive
hash codes for heterogeneous multimedia retrieval. Extensive experiments
exhibit that the proposed approach yields state of the art multimedia retrieval
performance on public datasets, i.e. NUS-WIDE, ImageNet-YahooQA.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Medical Retrieval 
      
        Multimodal Retrieval 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/qi2017efficient/">An Efficient Deep Learning Hashing Neural Network For Mobile Visual Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=An Efficient Deep Learning Hashing Neural Network For Mobile Visual Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=An Efficient Deep Learning Hashing Neural Network For Mobile Visual Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Qi Heng, Liu Wu, Liu Liang</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE Global Conference on Signal and Information Processing (GlobalSIP)</td>
    <td>8</td>
    <td><p>Mobile visual search applications are emerging that enable users to sense
their surroundings with smart phones. However, because of the particular
challenges of mobile visual search, achieving a high recognition bitrate has
becomes a consistent target of previous related works. In this paper, we
propose a few-parameter, low-latency, and high-accuracy deep hashing approach
for constructing binary hash codes for mobile visual search. First, we exploit
the architecture of the MobileNet model, which significantly decreases the
latency of deep feature extraction by reducing the number of model parameters
while maintaining accuracy. Second, we add a hash-like layer into MobileNet to
train the model on labeled mobile visual data. Evaluations show that the
proposed system can exceed state-of-the-art accuracy performance in terms of
the MAP. More importantly, the memory consumption is much less than that of
other deep learning models. The proposed method requires only \(13\) MB of memory
for the neural network and achieves a MAP of \(97.80%\) on the mobile location
recognition dataset used for testing.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/cakir2017mihash/">Mihash: Online Hashing With Mutual Information</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Mihash: Online Hashing With Mutual Information' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Mihash: Online Hashing With Mutual Information' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cakir et al.</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Conference on Computer Vision (ICCV)</td>
    <td>99</td>
    <td><p>Learning-based hashing methods are widely used for nearest neighbor
retrieval, and recently, online hashing methods have demonstrated good
performance-complexity trade-offs by learning hash functions from streaming
data. In this paper, we first address a key challenge for online hashing: the
binary codes for indexed data must be recomputed to keep pace with updates to
the hash functions. We propose an efficient quality measure for hash functions,
based on an information-theoretic quantity, mutual information, and use it
successfully as a criterion to eliminate unnecessary hash table updates. Next,
we also show how to optimize the mutual information objective using stochastic
gradient descent. We thus develop a novel hashing method, MIHash, that can be
used in both online and batch settings. Experiments on image retrieval
benchmarks (including a 2.5M image dataset) confirm the effectiveness of our
formulation, both in reducing hash table recomputations and in learning
high-quality hash functions.</p>
</td>
    <td>
      
        Hashing Methods 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/cai2017revisit/">A Revisit On Deep Hashings For Large-scale Content Based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Revisit On Deep Hashings For Large-scale Content Based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Revisit On Deep Hashings For Large-scale Content Based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cai Deng, Gu Xiuye, Wang Chaoqi</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>13</td>
    <td><p>There is a growing trend in studying deep hashing methods for content-based
image retrieval (CBIR), where hash functions and binary codes are learnt using
deep convolutional neural networks and then the binary codes can be used to do
approximate nearest neighbor (ANN) search. All the existing deep hashing papers
report their methods’ superior performance over the traditional hashing methods
according to their experimental results. However, there are serious flaws in
the evaluations of existing deep hashing papers: (1) The datasets they used are
too small and simple to simulate the real CBIR situation. (2) They did not
correctly include the search time in their evaluation criteria, while the
search time is crucial in real CBIR systems. (3) The performance of some
unsupervised hashing algorithms (e.g., LSH) can easily be boosted if one uses
multiple hash tables, which is an important factor should be considered in the
evaluation while most of the deep hashing papers failed to do so.
  We re-evaluate several state-of-the-art deep hashing methods with a carefully
designed experimental setting. Empirical results reveal that the performance of
these deep hashing methods are inferior to multi-table IsoH, a very simple
unsupervised hashing method. Thus, the conclusions in all the deep hashing
papers should be carefully re-examined.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        Image Retrieval 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/brooks2017multi/">Multi-level Spherical Locality Sensitive Hashing For Approximate Near Neighbors</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multi-level Spherical Locality Sensitive Hashing For Approximate Near Neighbors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multi-level Spherical Locality Sensitive Hashing For Approximate Near Neighbors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Brooks Teresa Nicole, Almajalid Rania</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>10</td>
    <td><p>This paper introduces “Multi-Level Spherical LSH”: parameter-free, a
multi-level, data-dependant Locality Sensitive Hashing data structure for
solving the Approximate Near Neighbors Problem (ANN). This data structure uses
a modified version of a multi-probe adaptive querying algorithm, with the
potential of achieving a \(O(n^p + t)\) query run time, for all inputs n where \(t
&lt;= n\).</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/charikar2017hashing/">Hashing-based-estimators For Kernel Density In High Dimensions</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing-based-estimators For Kernel Density In High Dimensions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing-based-estimators For Kernel Density In High Dimensions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Charikar Moses, Siminelakis Paris</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS)</td>
    <td>51</td>
    <td><p>Given a set of points \(P\subset \mathbb{R}^{d}\) and a kernel \(k\), the Kernel
Density Estimate at a point \(x\in\mathbb{R}^{d}\) is defined as
\(\mathrm{KDE}<em>{P}(x)=\frac{1}{|P|}\sum</em>{y\in P} k(x,y)\). We study the problem
of designing a data structure that given a data set \(P\) and a kernel function,
returns <em>approximations to the kernel density</em> of a query point in <em>sublinear
time</em>. We introduce a class of unbiased estimators for kernel density
implemented through locality-sensitive hashing, and give general theorems
bounding the variance of such estimators. These estimators give rise to
efficient data structures for estimating the kernel density in high dimensions
for a variety of commonly used kernels. Our work is the first to provide
data-structures with theoretical guarantees that improve upon simple random
sampling in high dimensions.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/gui2017fast/">Fast Supervised Discrete Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Supervised Discrete Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Supervised Discrete Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gui et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>265</td>
    <td><p>Learning-based hashing algorithms are <code class="language-plaintext highlighter-rouge">hot topics" because they can greatly
increase the scale at which existing methods operate. In this paper, we propose
a new learning-based hashing method called</code>fast supervised discrete hashing”
(FSDH) based on ``supervised discrete hashing” (SDH). Regressing the training
examples (or hash code) to the corresponding class labels is widely used in
ordinary least squares regression. Rather than adopting this method, FSDH uses
a very simple yet effective regression of the class labels of training examples
to the corresponding hash code to accelerate the algorithm. To the best of our
knowledge, this strategy has not previously been used for hashing. Traditional
SDH decomposes the optimization into three sub-problems, with the most critical
sub-problem - discrete optimization for binary hash codes - solved using
iterative discrete cyclic coordinate descent (DCC), which is time-consuming.
However, FSDH has a closed-form solution and only requires a single rather than
iterative hash code-solving step, which is highly efficient. Furthermore, FSDH
is usually faster than SDH for solving the projection matrix for least squares
regression, making FSDH generally faster than SDH. For example, our results
show that FSDH is about 12-times faster than SDH when the number of hashing
bits is 128 on the CIFAR-10 data base, and FSDH is about 151-times faster than
FastHash when the number of hashing bits is 64 on the MNIST data-base. Our
experimental results show that FSDH is not only fast, but also outperforms
other comparative methods.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/liao2017triplet/">Triplet-based Deep Similarity Learning For Person Re-identification</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Triplet-based Deep Similarity Learning For Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Triplet-based Deep Similarity Learning For Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liao et al.</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Conference on Computer Vision Workshops (ICCVW)</td>
    <td>32</td>
    <td><p>In recent years, person re-identification (re-id) catches great attention in
both computer vision community and industry. In this paper, we propose a new
framework for person re-identification with a triplet-based deep similarity
learning using convolutional neural networks (CNNs). The network is trained
with triplet input: two of them have the same class labels and the other one is
different. It aims to learn the deep feature representation, with which the
distance within the same class is decreased, while the distance between the
different classes is increased as much as possible. Moreover, we trained the
model jointly on six different datasets, which differs from common practice -
one model is just trained on one dataset and tested also on the same one.
However, the enormous number of possible triplet data among the large number of
training samples makes the training impossible. To address this challenge, a
double-sampling scheme is proposed to generate triplets of images as effective
as possible. The proposed framework is evaluated on several benchmark datasets.
The experimental results show that, our method is effective for the task of
person re-identification and it is comparable or even outperforms the
state-of-the-art methods.</p>
</td>
    <td>
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/li2017deep/">Deep Supervised Discrete Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Supervised Discrete Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Supervised Discrete Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>171</td>
    <td><p>With the rapid growth of image and video data on the web, hashing has been
extensively studied for image or video search in recent years. Benefit from
recent advances in deep learning, deep hashing methods have achieved promising
results for image retrieval. However, there are some limitations of previous
deep hashing methods (e.g., the semantic information is not fully exploited).
In this paper, we develop a deep supervised discrete hashing algorithm based on
the assumption that the learned binary codes should be ideal for
classification. Both the pairwise label information and the classification
information are used to learn the hash codes within one stream framework. We
constrain the outputs of the last layer to be binary codes directly, which is
rarely investigated in deep hashing algorithm. Because of the discrete nature
of hash codes, an alternating minimization method is used to optimize the
objective function. Experimental results have shown that our method outperforms
current state-of-the-art methods on benchmark datasets.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/liu2017ordinal/">Ordinal Constrained Binary Code Learning For Nearest Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Ordinal Constrained Binary Code Learning For Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Ordinal Constrained Binary Code Learning For Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>23</td>
    <td><p>Recent years have witnessed extensive attention in binary code learning,
a.k.a. hashing, for nearest neighbor search problems. It has been seen that
high-dimensional data points can be quantized into binary codes to give an
efficient similarity approximation via Hamming distance. Among existing
schemes, ranking-based hashing is recent promising that targets at preserving
ordinal relations of ranking in the Hamming space to minimize retrieval loss.
However, the size of the ranking tuples, which shows the ordinal relations, is
quadratic or cubic to the size of training samples. By given a large-scale
training data set, it is very expensive to embed such ranking tuples in binary
code learning. Besides, it remains a dificulty to build ranking tuples
efficiently for most ranking-preserving hashing, which are deployed over an
ordinal graph-based setting. To handle these problems, we propose a novel
ranking-preserving hashing method, dubbed Ordinal Constraint Hashing (OCH),
which efficiently learns the optimal hashing functions with a graph-based
approximation to embed the ordinal relations. The core idea is to reduce the
size of ordinal graph with ordinal constraint projection, which preserves the
ordinal relations through a small data set (such as clusters or random
samples). In particular, to learn such hash functions effectively, we further
relax the discrete constraints and design a specific stochastic gradient decent
algorithm for optimization. Experimental results on three large-scale visual
search benchmark datasets, i.e. LabelMe, Tiny100K and GIST1M, show that the
proposed OCH method can achieve superior performance over the state-of-the-arts
approaches.</p>
</td>
    <td>
      
        Compact Codes 
      
        Similarity Search 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/lin2017structured/">Structured Learning Of Binary Codes With Column Generation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Structured Learning Of Binary Codes With Column Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Structured Learning Of Binary Codes With Column Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin et al.</td> <!-- 🔧 You were missing this -->
    <td>International Journal of Computer Vision</td>
    <td>11</td>
    <td><p>Hashing methods aim to learn a set of hash functions which map the original
features to compact binary codes with similarity preserving in the Hamming
space. Hashing has proven a valuable tool for large-scale information
retrieval. We propose a column generation based binary code learning framework
for data-dependent hash function learning. Given a set of triplets that encode
the pairwise similarity comparison information, our column generation based
method learns hash functions that preserve the relative comparison relations
within the large-margin learning framework. Our method iteratively learns the
best hash functions during the column generation procedure. Existing hashing
methods optimize over simple objectives such as the reconstruction error or
graph Laplacian related loss functions, instead of the performance evaluation
criteria of interest—multivariate performance measures such as the AUC and
NDCG. Our column generation based method can be further generalized from the
triplet loss to a general structured learning based framework that allows one
to directly optimize multivariate performance measures. For optimizing general
ranking measures, the resulting optimization problem can involve exponentially
or infinitely many variables and constraints, which is more challenging than
standard structured output learning. We use a combination of column generation
and cutting-plane techniques to solve the optimization problem. To speed-up the
training we further explore stage-wise training and propose to use a simplified
NDCG loss for efficient inference. We demonstrate the generality of our method
by applying it to ranking prediction and image retrieval, and show that it
outperforms a few state-of-the-art hashing methods.</p>
</td>
    <td>
      
        Compact Codes 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/settle2017query/">Query-by-example Search With Discriminative Neural Acoustic Word Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Query-by-example Search With Discriminative Neural Acoustic Word Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Query-by-example Search With Discriminative Neural Acoustic Word Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Settle et al.</td> <!-- 🔧 You were missing this -->
    <td>Interspeech 2017</td>
    <td>56</td>
    <td><p>Query-by-example search often uses dynamic time warping (DTW) for comparing
queries and proposed matching segments. Recent work has shown that comparing
speech segments by representing them as fixed-dimensional vectors — acoustic
word embeddings — and measuring their vector distance (e.g., cosine distance)
can discriminate between words more accurately than DTW-based approaches. We
consider an approach to query-by-example search that embeds both the query and
database segments according to a neural model, followed by nearest-neighbor
search to find the matching segments. Earlier work on embedding-based
query-by-example, using template-based acoustic word embeddings, achieved
competitive performance. We find that our embeddings, based on recurrent neural
networks trained to optimize word discrimination, achieve substantial
improvements in performance and run-time efficiency over the previous
approaches.</p>
</td>
    <td>
      
        INTERSPEECH 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/dai2017stochastic/">Stochastic Generative Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Stochastic Generative Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Stochastic Generative Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dai et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>73</td>
    <td><p>Learning-based binary hashing has become a powerful paradigm for fast search
and retrieval in massive databases. However, due to the requirement of discrete
outputs for the hash functions, learning such functions is known to be very
challenging. In addition, the objective functions adopted by existing hashing
techniques are mostly chosen heuristically. In this paper, we propose a novel
generative approach to learn hash functions through Minimum Description Length
principle such that the learned hash codes maximally compress the dataset and
can also be used to regenerate the inputs. We also develop an efficient
learning algorithm based on the stochastic distributional gradient, which
avoids the notorious difficulty caused by binary output constraints, to jointly
optimize the parameters of the hash function and the associated generative
model. Extensive experiments on a variety of large-scale datasets show that the
proposed method achieves better retrieval results than the existing
state-of-the-art methods.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/li2017fast/">Fast K-nearest Neighbour Search Via Prioritized DCI</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast K-nearest Neighbour Search Via Prioritized DCI' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast K-nearest Neighbour Search Via Prioritized DCI' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Ke, Malik Jitendra</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>14</td>
    <td><p>Most exact methods for k-nearest neighbour search suffer from the curse of
dimensionality; that is, their query times exhibit exponential dependence on
either the ambient or the intrinsic dimensionality. Dynamic Continuous Indexing
(DCI) offers a promising way of circumventing the curse and successfully
reduces the dependence of query time on intrinsic dimensionality from
exponential to sublinear. In this paper, we propose a variant of DCI, which we
call Prioritized DCI, and show a remarkable improvement in the dependence of
query time on intrinsic dimensionality. In particular, a linear increase in
intrinsic dimensionality, or equivalently, an exponential increase in the
number of points near a query, can be mostly counteracted with just a linear
increase in space. We also demonstrate empirically that Prioritized DCI
significantly outperforms prior methods. In particular, relative to
Locality-Sensitive Hashing (LSH), Prioritized DCI reduces the number of
distance evaluations by a factor of 14 to 116 and the memory consumption by a
factor of 21.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/jin2017ranking/">Ranking Based Locality Sensitive Hashing Enabled Cancelable Biometrics: Index-of-max Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Ranking Based Locality Sensitive Hashing Enabled Cancelable Biometrics: Index-of-max Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Ranking Based Locality Sensitive Hashing Enabled Cancelable Biometrics: Index-of-max Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jin et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Information Forensics and Security</td>
    <td>195</td>
    <td><p>In this paper, we propose a ranking based locality sensitive hashing inspired
two-factor cancelable biometrics, dubbed “Index-of-Max” (IoM) hashing for
biometric template protection. With externally generated random parameters, IoM
hashing transforms a real-valued biometric feature vector into discrete index
(max ranked) hashed code. We demonstrate two realizations from IoM hashing
notion, namely Gaussian Random Projection based and Uniformly Random
Permutation based hashing schemes. The discrete indices representation nature
of IoM hashed codes enjoy serveral merits. Firstly, IoM hashing empowers strong
concealment to the biometric information. This contributes to the solid ground
of non-invertibility guarantee. Secondly, IoM hashing is insensitive to the
features magnitude, hence is more robust against biometric features variation.
Thirdly, the magnitude-independence trait of IoM hashing makes the hash codes
being scale-invariant, which is critical for matching and feature alignment.
The experimental results demonstrate favorable accuracy performance on
benchmark FVC2002 and FVC2004 fingerprint databases. The analyses justify its
resilience to the existing and newly introduced security and privacy attacks as
well as satisfy the revocability and unlinkability criteria of cancelable
biometrics.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/zheng2017sift/">SIFT Meets CNN: A Decade Survey Of Instance Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=SIFT Meets CNN: A Decade Survey Of Instance Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=SIFT Meets CNN: A Decade Survey Of Instance Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zheng Liang, Yang Yi, Tian Qi</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>709</td>
    <td><p>In the early days, content-based image retrieval (CBIR) was studied with
global features. Since 2003, image retrieval based on local descriptors (de
facto SIFT) has been extensively studied for over a decade due to the advantage
of SIFT in dealing with image transformations. Recently, image representations
based on the convolutional neural network (CNN) have attracted increasing
interest in the community and demonstrated impressive performance. Given this
time of rapid evolution, this article provides a comprehensive survey of
instance retrieval over the last decade. Two broad categories, SIFT-based and
CNN-based methods, are presented. For the former, according to the codebook
size, we organize the literature into using large/medium-sized/small codebooks.
For the latter, we discuss three lines of methods, i.e., using pre-trained or
fine-tuned CNN models, and hybrid methods. The first two perform a single-pass
of an image to the network, while the last category employs a patch-based
feature extraction scheme. This survey presents milestones in modern instance
retrieval, reviews a broad selection of previous works in different categories,
and provides insights on the connection between SIFT and CNN-based methods.
After analyzing and comparing retrieval performance of different categories on
several datasets, we discuss promising directions towards generic and
specialized instance retrieval.</p>
</td>
    <td>
      
        Survey Paper 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/jimenez2017class/">Class-weighted Convolutional Features For Visual Instance Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Class-weighted Convolutional Features For Visual Instance Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Class-weighted Convolutional Features For Visual Instance Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jimenez Albert, Alvarez Jose M., Giro-i-nieto Xavier</td> <!-- 🔧 You were missing this -->
    <td>Procedings of the British Machine Vision Conference 2017</td>
    <td>71</td>
    <td><p>Image retrieval in realistic scenarios targets large dynamic datasets of
unlabeled images. In these cases, training or fine-tuning a model every time
new images are added to the database is neither efficient nor scalable.
Convolutional neural networks trained for image classification over large
datasets have been proven effective feature extractors for image retrieval. The
most successful approaches are based on encoding the activations of
convolutional layers, as they convey the image spatial information. In this
paper, we go beyond this spatial information and propose a local-aware encoding
of convolutional features based on semantic information predicted in the target
image. To this end, we obtain the most discriminative regions of an image using
Class Activation Maps (CAMs). CAMs are based on the knowledge contained in the
network and therefore, our approach, has the additional advantage of not
requiring external information. In addition, we use CAMs to generate object
proposals during an unsupervised re-ranking stage after a first fast search.
Our experiments on two public available datasets for instance retrieval,
Oxford5k and Paris6k, demonstrate the competitiveness of our approach
outperforming the current state-of-the-art when using off-the-shelf models
trained on ImageNet. The source code and model used in this paper are publicly
available at http://imatge-upc.github.io/retrieval-2017-cam/.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/jain2017subic/">SUBIC: A Supervised, Structured Binary Code For Image Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=SUBIC: A Supervised, Structured Binary Code For Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=SUBIC: A Supervised, Structured Binary Code For Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jain et al.</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Conference on Computer Vision (ICCV)</td>
    <td>61</td>
    <td><p>For large-scale visual search, highly compressed yet meaningful
representations of images are essential. Structured vector quantizers based on
product quantization and its variants are usually employed to achieve such
compression while minimizing the loss of accuracy. Yet, unlike binary hashing
schemes, these unsupervised methods have not yet benefited from the
supervision, end-to-end learning and novel architectures ushered in by the deep
learning revolution. We hence propose herein a novel method to make deep
convolutional neural networks produce supervised, compact, structured binary
codes for visual search. Our method makes use of a novel block-softmax
non-linearity and of batch-based entropy losses that together induce structure
in the learned encodings. We show that our method outperforms state-of-the-art
compact representations based on deep hashing or structured quantization in
single and cross-domain category retrieval, instance retrieval and
classification. We make our code and models publicly available online.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Image Retrieval 
      
        Compact Codes 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/argerich2017generic/">Generic LSH Families For The Angular Distance Based On Johnson-lindenstrauss Projections And Feature Hashing LSH</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Generic LSH Families For The Angular Distance Based On Johnson-lindenstrauss Projections And Feature Hashing LSH' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Generic LSH Families For The Angular Distance Based On Johnson-lindenstrauss Projections And Feature Hashing LSH' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Argerich Luis, Golmar Natalia</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE 38th International Conference on Data Engineering (ICDE)</td>
    <td>11</td>
    <td><p>In this paper we propose the creation of generic LSH families for the angular
distance based on Johnson-Lindenstrauss projections. We show that feature
hashing is a valid J-L projection and propose two new LSH families based on
feature hashing. These new LSH families are tested on both synthetic and real
datasets with very good results and a considerable performance improvement over
other LSH families. While the theoretical analysis is done for the angular
distance, these families can also be used in practice for the euclidean
distance with excellent results [2]. Our tests using real datasets show that
the proposed LSH functions work well for the euclidean distance.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/tolias2017asymmetric/">Asymmetric Feature Maps With Application To Sketch Based Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Asymmetric Feature Maps With Application To Sketch Based Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Asymmetric Feature Maps With Application To Sketch Based Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tolias Giorgos, Chum Ondřej</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>39</td>
    <td><p>We propose a novel concept of asymmetric feature maps (AFM), which allows to
evaluate multiple kernels between a query and database entries without
increasing the memory requirements. To demonstrate the advantages of the AFM
method, we derive a short vector image representation that, due to asymmetric
feature maps, supports efficient scale and translation invariant sketch-based
image retrieval. Unlike most of the short-code based retrieval systems, the
proposed method provides the query localization in the retrieved image. The
efficiency of the search is boosted by approximating a 2D translation search
via trigonometric polynomial of scores by 1D projections. The projections are a
special case of AFM. An order of magnitude speed-up is achieved compared to
traditional trigonometric polynomials. The results are boosted by an
image-based average query expansion, exceeding significantly the state of the
art on standard benchmarks.</p>
</td>
    <td>
      
        Evaluation 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/eberle2017pose/">Pose-driven Deep Models For Person Re-identification</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Pose-driven Deep Models For Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Pose-driven Deep Models For Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Eberle Andreas</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Conference on Computer Vision (ICCV)</td>
    <td>878</td>
    <td><p>Person re-identification (re-id) is the task of recognizing and matching
persons at different locations recorded by cameras with non-overlapping views.
One of the main challenges of re-id is the large variance in person poses and
camera angles since neither of them can be influenced by the re-id system. In
this work, an effective approach to integrate coarse camera view information as
well as fine-grained pose information into a convolutional neural network (CNN)
model for learning discriminative re-id embeddings is introduced. In most
recent work pose information is either explicitly modeled within the re-id
system or explicitly used for pre-processing, for example by pose-normalizing
person images. In contrast, the proposed approach shows that a direct use of
camera view as well as the detected body joint locations into a standard CNN
can be used to significantly improve the robustness of learned re-id
embeddings. On four challenging surveillance and video re-id datasets
significant improvements over the current state of the art have been achieved.
Furthermore, a novel reordering of the MARS dataset, called X-MARS is
introduced to allow cross-validation of models trained for single-image re-id
on tracklet data.</p>
</td>
    <td>
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/kale2017towards/">Towards Semantic Query Segmentation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Towards Semantic Query Segmentation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Towards Semantic Query Segmentation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kale et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>Query Segmentation is one of the critical components for understanding users’
search intent in Information Retrieval tasks. It involves grouping tokens in
the search query into meaningful phrases which help downstream tasks like
search relevance and query understanding. In this paper, we propose a novel
approach to segment user queries using distributed query embeddings. Our key
contribution is a supervised approach to the segmentation task using
low-dimensional feature vectors for queries, getting rid of traditional hand
tuned and heuristic NLP features which are quite expensive.
  We benchmark on a 50,000 human-annotated web search engine query corpus
achieving comparable accuracy to state-of-the-art techniques. The advantage of
our technique is its fast and does not use external knowledge-base like
Wikipedia for score boosting. This helps us generalize our approach to other
domains like eCommerce without any fine-tuning. We demonstrate the
effectiveness of this method on another 50,000 human-annotated eCommerce query
corpus from eBay search logs. Our approach is easy to implement and generalizes
well across different search domains proving the power of low-dimensional
embeddings in query segmentation task, opening up a new direction of research
for this problem.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/shrivastava2017optimal/">Optimal Densification For Fast And Accurate Minwise Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Optimal Densification For Fast And Accurate Minwise Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Optimal Densification For Fast And Accurate Minwise Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shrivastava Anshumali</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>27</td>
    <td><p>Minwise hashing is a fundamental and one of the most successful hashing
algorithm in the literature. Recent advances based on the idea of
densification~\cite{Proc:OneHashLSH_ICML14,Proc:Shrivastava_UAI14} have shown
that it is possible to compute \(k\) minwise hashes, of a vector with \(d\)
nonzeros, in mere \((d + k)\) computations, a significant improvement over the
classical \(O(dk)\). These advances have led to an algorithmic improvement in the
query complexity of traditional indexing algorithms based on minwise hashing.
Unfortunately, the variance of the current densification techniques is
unnecessarily high, which leads to significantly poor accuracy compared to
vanilla minwise hashing, especially when the data is sparse. In this paper, we
provide a novel densification scheme which relies on carefully tailored
2-universal hashes. We show that the proposed scheme is variance-optimal, and
without losing the runtime efficiency, it is significantly more accurate than
existing densification techniques. As a result, we obtain a significantly
efficient hashing scheme which has the same variance and collision probability
as minwise hashing. Experimental evaluations on real sparse and
high-dimensional datasets validate our claims. We believe that given the
significant advantages, our method will replace minwise hashing implementations
in practice.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/liong2017cross/">Cross-modal Deep Variational Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-modal Deep Variational Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-modal Deep Variational Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liong et al.</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Conference on Computer Vision (ICCV)</td>
    <td>98</td>
    <td><p>In this paper, we propose a cross-modal deep variational hashing (CMDVH) method for cross-modality multimedia retrieval. Unlike existing cross-modal hashing methods
which learn a single pair of projections to map each example as a binary vector, we design a couple of deep neural
network to learn non-linear transformations from imagetext input pairs, so that unified binary codes can be obtained. We then design the modality-specific neural networks in a probabilistic manner where we model a latent
variable as close as possible from the inferred binary codes,
which is approximated by a posterior distribution regularized by a known prior. Experimental results on three benchmark datasets show the efficacy of the proposed approach.</p>
</td>
    <td>
      
        Hashing Methods 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/shenoy2017deduplication/">Deduplication In A Massive Clinical Note Dataset</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deduplication In A Massive Clinical Note Dataset' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deduplication In A Massive Clinical Note Dataset' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shenoy et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 17th International Conference on Mining Software Repositories</td>
    <td>18</td>
    <td><p>Duplication, whether exact or partial, is a common issue in many datasets. In
clinical notes data, duplication (and near duplication) can arise for many
reasons, such as the pervasive use of templates, copy-pasting, or notes being
generated by automated procedures. A key challenge in removing such near
duplicates is the size of such datasets; our own dataset consists of more than
10 million notes. To detect and correct such duplicates requires algorithms
that both accurate and highly scalable. We describe a solution based on
Minhashing with Locality Sensitive Hashing. In this paper, we present the
theory behind this method and present a database-inspired approach to make the
method scalable. We also present a clustering technique using disjoint sets to
produce dense clusters, which speeds up our algorithm.</p>
</td>
    <td>
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/driemel2017locality/">Locality-sensitive Hashing Of Curves</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Locality-sensitive Hashing Of Curves' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Locality-sensitive Hashing Of Curves' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Driemel Anne, Silvestri Francesco</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>31</td>
    <td><p>We study data structures for storing a set of polygonal curves in \({\rm R}^d\)
such that, given a query curve, we can efficiently retrieve similar curves from
the set, where similarity is measured using the discrete Fr'echet distance or
the dynamic time warping distance. To this end we devise the first
locality-sensitive hashing schemes for these distance measures. A major
challenge is posed by the fact that these distance measures internally optimize
the alignment between the curves. We give solutions for different types of
alignments including constrained and unconstrained versions. For unconstrained
alignments, we improve over a result by Indyk from 2002 for short curves. Let
\(n\) be the number of input curves and let \(m\) be the maximum complexity of a
curve in the input. In the particular case where \(m \leq \frac{\alpha}{4d} log
n\), for some fixed \(\alpha&gt;0\), our solutions imply an approximate near-neighbor
data structure for the discrete Fr'echet distance that uses space in
\(O(n^{1+\alpha}log n)\) and achieves query time in \(O(n^{\alpha}log^2 n)\) and
constant approximation factor. Furthermore, our solutions provide a trade-off
between approximation quality and computational performance: for any parameter
\(k \in [m]\), we can give a data structure that uses space in \(O(2^{2k}m^{k-1} n
log n + nm)\), answers queries in \(O( 2^{2k} m^{k}log n)\) time and achieves
approximation factor in \(O(m/k)\).</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/gordo2017end/">End-to-end Learning Of Deep Visual Representations For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=End-to-end Learning Of Deep Visual Representations For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=End-to-end Learning Of Deep Visual Representations For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gordo et al.</td> <!-- 🔧 You were missing this -->
    <td>International Journal of Computer Vision</td>
    <td>532</td>
    <td><p>While deep learning has become a key ingredient in the top performing methods
for many computer vision tasks, it has failed so far to bring similar
improvements to instance-level image retrieval. In this article, we argue that
reasons for the underwhelming results of deep methods on image retrieval are
threefold: i) noisy training data, ii) inappropriate deep architecture, and
iii) suboptimal training procedure. We address all three issues.
  First, we leverage a large-scale but noisy landmark dataset and develop an
automatic cleaning method that produces a suitable training set for deep
retrieval. Second, we build on the recent R-MAC descriptor, show that it can be
interpreted as a deep and differentiable architecture, and present improvements
to enhance it. Last, we train this network with a siamese architecture that
combines three streams with a triplet loss. At the end of the training process,
the proposed architecture produces a global image representation in a single
forward pass that is well suited for image retrieval. Extensive experiments
show that our approach significantly outperforms previous retrieval approaches,
including state-of-the-art methods based on costly local descriptor indexing
and spatial verification. On Oxford 5k, Paris 6k and Holidays, we respectively
report 94.7, 96.6, and 94.8 mean average precision. Our representations can
also be heavily compressed using product quantization with little loss in
accuracy. For additional material, please see
www.xrce.xerox.com/Deep-Image-Retrieval.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/zhu2017discrete/">Discrete Multi-modal Hashing With Canonical Views For Robust Mobile Landmark Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Discrete Multi-modal Hashing With Canonical Views For Robust Mobile Landmark Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Discrete Multi-modal Hashing With Canonical Views For Robust Mobile Landmark Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhu et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>9</td>
    <td><p>Mobile landmark search (MLS) recently receives increasing attention for its
great practical values. However, it still remains unsolved due to two important
challenges. One is high bandwidth consumption of query transmission, and the
other is the huge visual variations of query images sent from mobile devices.
In this paper, we propose a novel hashing scheme, named as canonical view based
discrete multi-modal hashing (CV-DMH), to handle these problems via a novel
three-stage learning procedure. First, a submodular function is designed to
measure visual representativeness and redundancy of a view set. With it,
canonical views, which capture key visual appearances of landmark with limited
redundancy, are efficiently discovered with an iterative mining strategy.
Second, multi-modal sparse coding is applied to transform visual features from
multiple modalities into an intermediate representation. It can robustly and
adaptively characterize visual contents of varied landmark images with certain
canonical views. Finally, compact binary codes are learned on intermediate
representation within a tailored discrete binary embedding model which
preserves visual relations of images measured with canonical views and removes
the involved noises. In this part, we develop a new augmented Lagrangian
multiplier (ALM) based optimization method to directly solve the discrete
binary codes. We can not only explicitly deal with the discrete constraint, but
also consider the bit-uncorrelated constraint and balance constraint together.
Experiments on real world landmark datasets demonstrate the superior
performance of CV-DMH over several state-of-the-art methods.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/wang2017subspace/">Subspace Approximation For Approximate Nearest Neighbor Search In NLP</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Subspace Approximation For Approximate Nearest Neighbor Search In NLP' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Subspace Approximation For Approximate Nearest Neighbor Search In NLP' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Jing</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>56</td>
    <td><p>Most natural language processing tasks can be formulated as the approximated
nearest neighbor search problem, such as word analogy, document similarity,
machine translation. Take the question-answering task as an example, given a
question as the query, the goal is to search its nearest neighbor in the
training dataset as the answer. However, existing methods for approximate
nearest neighbor search problem may not perform well owing to the following
practical challenges: 1) there are noise in the data; 2) the large scale
dataset yields a huge retrieval space and high search time complexity.
  In order to solve these problems, we propose a novel approximate nearest
neighbor search framework which i) projects the data to a subspace based
spectral analysis which eliminates the influence of noise; ii) partitions the
training dataset to different groups in order to reduce the search space.
Specifically, the retrieval space is reduced from \(O(n)\) to \(O(log n)\) (where
\(n\) is the number of data points in the training dataset). We prove that the
retrieved nearest neighbor in the projected subspace is the same as the one in
the original feature space. We demonstrate the outstanding performance of our
framework on real-world natural language processing tasks.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/ong2017siamese/">Siamese Network Of Deep Fisher-vector Descriptors For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Siamese Network Of Deep Fisher-vector Descriptors For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Siamese Network Of Deep Fisher-vector Descriptors For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ong Eng-jon, Husain Sameed, Bober Miroslaw</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>37</td>
    <td><p>This paper addresses the problem of large scale image retrieval, with the aim
of accurately ranking the similarity of a large number of images to a given
query image. To achieve this, we propose a novel Siamese network. This network
consists of two computational strands, each comprising of a CNN component
followed by a Fisher vector component. The CNN component produces dense, deep
convolutional descriptors that are then aggregated by the Fisher Vector method.
Crucially, we propose to simultaneously learn both the CNN filter weights and
Fisher Vector model parameters. This allows us to account for the evolving
distribution of deep descriptors over the course of the learning process. We
show that the proposed approach gives significant improvements over the
state-of-the-art methods on the Oxford and Paris image retrieval datasets.
Additionally, we provide a baseline performance measure for both these datasets
with the inclusion of 1 million distractors.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/do2017embedding/">Embedding Based On Function Approximation For Large Scale Image Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Embedding Based On Function Approximation For Large Scale Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Embedding Based On Function Approximation For Large Scale Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Do Thanh-toan, Cheung Ngai-man</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>37</td>
    <td><p>The objective of this paper is to design an embedding method that maps local
features describing an image (e.g. SIFT) to a higher dimensional representation
useful for the image retrieval problem. First, motivated by the relationship
between the linear approximation of a nonlinear function in high dimensional
space and the stateof-the-art feature representation used in image retrieval,
i.e., VLAD, we propose a new approach for the approximation. The embedded
vectors resulted by the function approximation process are then aggregated to
form a single representation for image retrieval. Second, in order to make the
proposed embedding method applicable to large scale problem, we further derive
its fast version in which the embedded vectors can be efficiently computed,
i.e., in the closed-form. We compare the proposed embedding methods with the
state of the art in the context of image search under various settings: when
the images are represented by medium length vectors, short vectors, or binary
vectors. The experimental results show that the proposed embedding methods
outperform existing the state of the art on the standard public image retrieval
benchmarks.</p>
</td>
    <td>
      
        Image Retrieval 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/do2017simultaneous/">Simultaneous Feature Aggregating And Hashing For Large-scale Image Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Simultaneous Feature Aggregating And Hashing For Large-scale Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Simultaneous Feature Aggregating And Hashing For Large-scale Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Do et al.</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>38</td>
    <td><p>In most state-of-the-art hashing-based visual search systems, local image
descriptors of an image are first aggregated as a single feature vector. This
feature vector is then subjected to a hashing function that produces a binary
hash code. In previous work, the aggregating and the hashing processes are
designed independently. In this paper, we propose a novel framework where
feature aggregating and hashing are designed simultaneously and optimized
jointly. Specifically, our joint optimization produces aggregated
representations that can be better reconstructed by some binary codes. This
leads to more discriminative binary hash codes and improved retrieval accuracy.
In addition, we also propose a fast version of the recently-proposed Binary
Autoencoder to be used in our proposed framework. We perform extensive
retrieval experiments on several benchmark datasets with both SIFT and
convolutional features. Our results suggest that the proposed framework
achieves significant improvements over the state of the art.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
        SCALABILITY 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/wang2017deep/">Deep Supervised Hashing With Triplet Labels</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Supervised Hashing With Triplet Labels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Supervised Hashing With Triplet Labels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Xiaofang, Shi Yi, Kitani Kris M.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>192</td>
    <td><p>Hashing is one of the most popular and powerful approximate nearest neighbor
search techniques for large-scale image retrieval. Most traditional hashing
methods first represent images as off-the-shelf visual features and then
produce hashing codes in a separate stage. However, off-the-shelf visual
features may not be optimally compatible with the hash code learning procedure,
which may result in sub-optimal hash codes. Recently, deep hashing methods have
been proposed to simultaneously learn image features and hash codes using deep
neural networks and have shown superior performance over traditional hashing
methods. Most deep hashing methods are given supervised information in the form
of pairwise labels or triplet labels. The current state-of-the-art deep hashing
method DPSH~\cite{li2015feature}, which is based on pairwise labels, performs
image feature learning and hash code learning simultaneously by maximizing the
likelihood of pairwise similarities. Inspired by DPSH~\cite{li2015feature}, we
propose a triplet label based deep hashing method which aims to maximize the
likelihood of the given triplet labels. Experimental results show that our
method outperforms all the baselines on CIFAR-10 and NUS-WIDE datasets,
including the state-of-the-art method DPSH~\cite{li2015feature} and all the
previous triplet label based deep hashing methods.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        SUPERVISED 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/wang2017attention/">An Attention-based Deep Net For Learning To Rank</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=An Attention-based Deep Net For Learning To Rank' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=An Attention-based Deep Net For Learning To Rank' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Baiyang, Klabjan Diego</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>12</td>
    <td><p>In information retrieval, learning to rank constructs a machine-based ranking
model which given a query, sorts the search results by their degree of
relevance or importance to the query. Neural networks have been successfully
applied to this problem, and in this paper, we propose an attention-based deep
neural network which better incorporates different embeddings of the queries
and search results with an attention-based mechanism. This model also applies a
decoder mechanism to learn the ranks of the search results in a listwise
fashion. The embeddings are trained with convolutional neural networks or the
word2vec model. We demonstrate the performance of this model with image
retrieval and text querying data sets.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/gu2017look/">Look, Imagine And Match: Improving Textual-visual Cross-modal Retrieval With Generative Models</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Look, Imagine And Match: Improving Textual-visual Cross-modal Retrieval With Generative Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Look, Imagine And Match: Improving Textual-visual Cross-modal Retrieval With Generative Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gu et al.</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</td>
    <td>283</td>
    <td><p>Textual-visual cross-modal retrieval has been a hot research topic in both
computer vision and natural language processing communities. Learning
appropriate representations for multi-modal data is crucial for the cross-modal
retrieval performance. Unlike existing image-text retrieval approaches that
embed image-text pairs as single feature vectors in a common representational
space, we propose to incorporate generative processes into the cross-modal
feature embedding, through which we are able to learn not only the global
abstract features but also the local grounded features. Extensive experiments
show that our framework can well match images and sentences with complex
content, and achieve the state-of-the-art cross-modal retrieval results on
MSCOCO dataset.</p>
</td>
    <td>
      
        Multimodal Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/depalma2017distributed/">Distributed Stratified Locality Sensitive Hashing For Critical Event Prediction In The Cloud</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Distributed Stratified Locality Sensitive Hashing For Critical Event Prediction In The Cloud' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Distributed Stratified Locality Sensitive Hashing For Critical Event Prediction In The Cloud' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>de Palma Alessandro, Hemberg Erik, O'reilly Una-may</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 21st ACM international conference on Information and knowledge management</td>
    <td>52</td>
    <td><p>The availability of massive healthcare data repositories calls for efficient
tools for data-driven medicine. We introduce a distributed system for
Stratified Locality Sensitive Hashing to perform fast similarity-based
prediction on large medical waveform datasets. Our implementation, for an ICU
use case, prioritizes latency over throughput and is targeted at a cloud
environment. We demonstrate our system on Acute Hypotensive Episode prediction
from Arterial Blood Pressure waveforms. On a dataset of \(1.37\) million points,
we show scaling up to \(40\) processors and a \(21\times\) speedup in number of
comparisons to parallel exhaustive search at the price of a \(10%\) Matthews
correlation coefficient (MCC) loss. Furthermore, if additional MCC loss can be
tolerated, our system achieves speedups up to two orders of magnitude.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Hashing Methods 
      
        CIKM 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/noh2017large/">Large-scale Image Retrieval With Attentive Deep Local Features</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Large-scale Image Retrieval With Attentive Deep Local Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Large-scale Image Retrieval With Attentive Deep Local Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Noh et al.</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Conference on Computer Vision (ICCV)</td>
    <td>806</td>
    <td><p>We propose an attentive local feature descriptor suitable for large-scale
image retrieval, referred to as DELF (DEep Local Feature). The new feature is
based on convolutional neural networks, which are trained only with image-level
annotations on a landmark image dataset. To identify semantically useful local
features for image retrieval, we also propose an attention mechanism for
keypoint selection, which shares most network layers with the descriptor. This
framework can be used for image retrieval as a drop-in replacement for other
keypoint detectors and descriptors, enabling more accurate feature matching and
geometric verification. Our system produces reliable confidence scores to
reject false positives—in particular, it is robust against queries that have
no correct match in the database. To evaluate the proposed descriptor, we
introduce a new large-scale dataset, referred to as Google-Landmarks dataset,
which involves challenges in both database and query such as background
clutter, partial occlusion, multiple landmarks, objects in variable scales,
etc. We show that DELF outperforms the state-of-the-art global and local
descriptors in the large-scale setting by significant margins. Code and dataset
can be found at the project webpage:
https://github.com/tensorflow/models/tree/master/research/delf .</p>
</td>
    <td>
      
        Image Retrieval 
      
        SCALABILITY 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/nedelec2017specializing/">Specializing Joint Representations For The Task Of Product Recommendation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Specializing Joint Representations For The Task Of Product Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Specializing Joint Representations For The Task Of Product Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Nedelec Thomas, Smirnova Elena, Vasile Flavian</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2nd Workshop on Deep Learning for Recommender Systems</td>
    <td>17</td>
    <td><p>We propose a unified product embedded representation that is optimized for
the task of retrieval-based product recommendation. To this end, we introduce a
new way to fuse modality-specific product embeddings into a joint product
embedding, in order to leverage both product content information, such as
textual descriptions and images, and product collaborative filtering signal. By
introducing the fusion step at the very end of our architecture, we are able to
train each modality separately, allowing us to keep a modular architecture that
is preferable in real-world recommendation deployments. We analyze our
performance on normal and hard recommendation setups such as cold-start and
cross-category recommendations and achieve good performance on a large product
shopping dataset.</p>
</td>
    <td>
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/zhao2017scalable/">Scalable Nearest Neighbor Search Based On Knn Graph</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Scalable Nearest Neighbor Search Based On Knn Graph' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Scalable Nearest Neighbor Search Based On Knn Graph' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhao Wan-lei, Yang Jie, Deng Cheng-hao</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>7</td>
    <td><p>Nearest neighbor search is known as a challenging issue that has been studied
for several decades. Recently, this issue becomes more and more imminent in
viewing that the big data problem arises from various fields. In this paper, a
scalable solution based on hill-climbing strategy with the support of k-nearest
neighbor graph (kNN) is presented. Two major issues have been considered in the
paper. Firstly, an efficient kNN graph construction method based on two means
tree is presented. For the nearest neighbor search, an enhanced hill-climbing
procedure is proposed, which sees considerable performance boost over original
procedure. Furthermore, with the support of inverted indexing derived from
residue vector quantization, our method achieves close to 100% recall with high
speed efficiency in two state-of-the-art evaluation benchmarks. In addition, a
comparative study on both the compressional and traditional nearest neighbor
search methods is presented. We show that our method achieves the best
trade-off between search quality, efficiency and memory complexity.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/deng2017learning/">Learning Deep Similarity Models With Focus Ranking For Fabric Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Deep Similarity Models With Focus Ranking For Fabric Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Deep Similarity Models With Focus Ranking For Fabric Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Deng et al.</td> <!-- 🔧 You were missing this -->
    <td>Image and Vision Computing</td>
    <td>37</td>
    <td><p>Fabric image retrieval is beneficial to many applications including clothing
searching, online shopping and cloth modeling. Learning pairwise image
similarity is of great importance to an image retrieval task. With the
resurgence of Convolutional Neural Networks (CNNs), recent works have achieved
significant progresses via deep representation learning with metric embedding,
which drives similar examples close to each other in a feature space, and
dissimilar ones apart from each other. In this paper, we propose a novel
embedding method termed focus ranking that can be easily unified into a CNN for
jointly learning image representations and metrics in the context of
fine-grained fabric image retrieval. Focus ranking aims to rank similar
examples higher than all dissimilar ones by penalizing ranking disorders via
the minimization of the overall cost attributed to similar samples being ranked
below dissimilar ones. At the training stage, training samples are organized
into focus ranking units for efficient optimization. We build a large-scale
fabric image retrieval dataset (FIRD) with about 25,000 images of 4,300
fabrics, and test the proposed model on the FIRD dataset. Experimental results
show the superiority of the proposed model over existing metric embedding
models.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/sivertsen2017fast/">Fast Nearest Neighbor Preserving Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Nearest Neighbor Preserving Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Nearest Neighbor Preserving Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sivertsen Johan</td> <!-- 🔧 You were missing this -->
    <td>ACM Transactions on Algorithms</td>
    <td>62</td>
    <td><p>We show an analog to the Fast Johnson-Lindenstrauss Transform for Nearest
Neighbor Preserving Embeddings in \(ℓ₂\). These are sparse, randomized
embeddings that preserve the (approximate) nearest neighbors. The
dimensionality of the embedding space is bounded not by the size of the
embedded set n, but by its doubling dimension {\lambda}. For most large
real-world datasets this will mean a considerably lower-dimensional embedding
space than possible when preserving all distances. The resulting embeddings can
be used with existing approximate nearest neighbor data structures to yield
speed improvements.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/jiang2017deep/">Deep Cross-modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jiang Qing-yuan, Li Wu-jun</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>752</td>
    <td><p>Due to its low storage cost and fast query speed, cross-modal hashing (CMH)
has been widely used for similarity search in multimedia retrieval
applications. However, almost all existing CMH methods are based on
hand-crafted features which might not be optimally compatible with the
hash-code learning procedure. As a result, existing CMH methods with
handcrafted features may not achieve satisfactory performance. In this paper,
we propose a novel cross-modal hashing method, called deep crossmodal hashing
(DCMH), by integrating feature learning and hash-code learning into the same
framework. DCMH is an end-to-end learning framework with deep neural networks,
one for each modality, to perform feature learning from scratch. Experiments on
two real datasets with text-image modalities show that DCMH can outperform
other baselines to achieve the state-of-the-art performance in cross-modal
retrieval applications.</p>
</td>
    <td>
      
        Hashing Methods 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/hoang2017selective/">Selective Deep Convolutional Features For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Selective Deep Convolutional Features For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Selective Deep Convolutional Features For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hoang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 25th ACM international conference on Multimedia</td>
    <td>58</td>
    <td><p>Convolutional Neural Network (CNN) is a very powerful approach to extract
discriminative local descriptors for effective image search. Recent work adopts
fine-tuned strategies to further improve the discriminative power of the
descriptors. Taking a different approach, in this paper, we propose a novel
framework to achieve competitive retrieval performance. Firstly, we propose
various masking schemes, namely SIFT-mask, SUM-mask, and MAX-mask, to select a
representative subset of local convolutional features and remove a large number
of redundant features. We demonstrate that this can effectively address the
burstiness issue and improve retrieval accuracy. Secondly, we propose to employ
recent embedding and aggregating methods to further enhance feature
discriminability. Extensive experiments demonstrate that our proposed framework
achieves state-of-the-art retrieval accuracy.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/liu2017deep/">Deep Sketch Hashing: Fast Free-hand Sketch-based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Sketch Hashing: Fast Free-hand Sketch-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Sketch Hashing: Fast Free-hand Sketch-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>267</td>
    <td><p>Free-hand sketch-based image retrieval (SBIR) is a specific cross-view
retrieval task, in which queries are abstract and ambiguous sketches while the
retrieval database is formed with natural images. Work in this area mainly
focuses on extracting representative and shared features for sketches and
natural images. However, these can neither cope well with the geometric
distortion between sketches and images nor be feasible for large-scale SBIR due
to the heavy continuous-valued distance computation. In this paper, we speed up
SBIR by introducing a novel binary coding method, named \textbf{Deep Sketch
Hashing} (DSH), where a semi-heterogeneous deep architecture is proposed and
incorporated into an end-to-end binary coding framework. Specifically, three
convolutional neural networks are utilized to encode free-hand sketches,
natural images and, especially, the auxiliary sketch-tokens which are adopted
as bridges to mitigate the sketch-image geometric distortion. The learned DSH
codes can effectively capture the cross-view similarities as well as the
intrinsic semantic correlations between different categories. To the best of
our knowledge, DSH is the first hashing work specifically designed for
category-level SBIR with an end-to-end deep architecture. The proposed DSH is
comprehensively evaluated on two large-scale datasets of TU-Berlin Extension
and Sketchy, and the experiments consistently show DSH’s superior SBIR
accuracies over several state-of-the-art methods, while achieving significantly
reduced retrieval time and memory footprint.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/gella2017image/">Image Pivoting For Learning Multilingual Multimodal Representations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Image Pivoting For Learning Multilingual Multimodal Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Image Pivoting For Learning Multilingual Multimodal Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gella et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</td>
    <td>71</td>
    <td><p>In this paper we propose a model to learn multimodal multilingual
representations for matching images and sentences in different languages, with
the aim of advancing multilingual versions of image search and image
understanding. Our model learns a common representation for images and their
descriptions in two different languages (which need not be parallel) by
considering the image as a pivot between two languages. We introduce a new
pairwise ranking loss function which can handle both symmetric and asymmetric
similarity between the two modalities. We evaluate our models on
image-description ranking for German and English, and on semantic textual
similarity of image descriptions in English. In both cases we achieve
state-of-the-art performance.</p>
</td>
    <td>
      
        EMNLP 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/mu2017deep/">Deep Hashing: A Joint Approach For Image Signature Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Hashing: A Joint Approach For Image Signature Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Hashing: A Joint Approach For Image Signature Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mu Yadong, Liu Zhu</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>9</td>
    <td><p>Similarity-based image hashing represents crucial technique for visual data
storage reduction and expedited image search. Conventional hashing schemes
typically feed hand-crafted features into hash functions, which separates the
procedures of feature extraction and hash function learning. In this paper, we
propose a novel algorithm that concurrently performs feature engineering and
non-linear supervised hashing function learning. Our technical contributions in
this paper are two-folds: 1) deep network optimization is often achieved by
gradient propagation, which critically requires a smooth objective function.
The discrete nature of hash codes makes them not amenable for gradient-based
optimization. To address this issue, we propose an exponentiated hashing loss
function and its bilinear smooth approximation. Effective gradient calculation
and propagation are thereby enabled; 2) pre-training is an important trick in
supervised deep learning. The impact of pre-training on the hash code quality
has never been discussed in current deep hashing literature. We propose a
pre-training scheme inspired by recent advance in deep network based image
classification, and experimentally demonstrate its effectiveness. Comprehensive
quantitative evaluations are conducted on several widely-used image benchmarks.
On all benchmarks, our proposed deep hashing algorithm outperforms all
state-of-the-art competitors by significant margins. In particular, our
algorithm achieves a near-perfect 0.99 in terms of Hamming ranking accuracy
with only 12 bits on MNIST, and a new record of 0.74 on the CIFAR10 dataset. In
comparison, the best accuracies obtained on CIFAR10 by existing hashing
algorithms without or with deep networks are known to be 0.36 and 0.58
respectively.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/faghri2017vse/">VSE++: Improving Visual-semantic Embeddings With Hard Negatives</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=VSE++: Improving Visual-semantic Embeddings With Hard Negatives' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=VSE++: Improving Visual-semantic Embeddings With Hard Negatives' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Faghri et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>562</td>
    <td><p>We present a new technique for learning visual-semantic embeddings for
cross-modal retrieval. Inspired by hard negative mining, the use of hard
negatives in structured prediction, and ranking loss functions, we introduce a
simple change to common loss functions used for multi-modal embeddings. That,
combined with fine-tuning and use of augmented data, yields significant gains
in retrieval performance. We showcase our approach, VSE++, on MS-COCO and
Flickr30K datasets, using ablation studies and comparisons with existing
methods. On MS-COCO our approach outperforms state-of-the-art methods by 8.8%
in caption retrieval and 11.3% in image retrieval (at R@1).</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/razeghi2017privacy/">Privacy Preserving Identification Using Sparse Approximation With Ambiguization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Privacy Preserving Identification Using Sparse Approximation With Ambiguization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Privacy Preserving Identification Using Sparse Approximation With Ambiguization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Razeghi et al.</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE Workshop on Information Forensics and Security (WIFS)</td>
    <td>25</td>
    <td><p>In this paper, we consider a privacy preserving encoding framework for
identification applications covering biometrics, physical object security and
the Internet of Things (IoT). The proposed framework is based on a sparsifying
transform, which consists of a trained linear map, an element-wise
nonlinearity, and privacy amplification. The sparsifying transform and privacy
amplification are not symmetric for the data owner and data user. We
demonstrate that the proposed approach is closely related to sparse ternary
codes (STC), a recent information-theoretic concept proposed for fast
approximate nearest neighbor (ANN) search in high dimensional feature spaces
that being machine learning in nature also offers significant benefits in
comparison to sparse approximation and binary embedding approaches. We
demonstrate that the privacy of the database outsourced to a server as well as
the privacy of the data user are preserved at a low computational cost, storage
and communication burdens.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/ercoli2017compact/">Compact Hash Codes For Efficient Visual Descriptors Retrieval In Large Scale Databases</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Compact Hash Codes For Efficient Visual Descriptors Retrieval In Large Scale Databases' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Compact Hash Codes For Efficient Visual Descriptors Retrieval In Large Scale Databases' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ercoli Simone, Bertini Marco, del Bimbo Alberto</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>42</td>
    <td><p>In this paper we present an efficient method for visual descriptors retrieval
based on compact hash codes computed using a multiple k-means assignment. The
method has been applied to the problem of approximate nearest neighbor (ANN)
search of local and global visual content descriptors, and it has been tested
on different datasets: three large scale public datasets of up to one billion
descriptors (BIGANN) and, supported by recent progress in convolutional neural
networks (CNNs), also on the CIFAR-10 and MNIST datasets. Experimental results
show that, despite its simplicity, the proposed method obtains a very high
performance that makes it superior to more complex state-of-the-art methods.</p>
</td>
    <td>
      
        Hashing Methods 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/ertl2017superminhash/">Superminhash - A New Minwise Hashing Algorithm For Jaccard Similarity Estimation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Superminhash - A New Minwise Hashing Algorithm For Jaccard Similarity Estimation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Superminhash - A New Minwise Hashing Algorithm For Jaccard Similarity Estimation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ertl Otmar</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>13</td>
    <td><p>This paper presents a new algorithm for calculating hash signatures of sets
which can be directly used for Jaccard similarity estimation. The new approach
is an improvement over the MinHash algorithm, because it has a better runtime
behavior and the resulting signatures allow a more precise estimation of the
Jaccard index.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/feng2017deep/">Deep Image Set Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Image Set Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Image Set Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Feng et al.</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>12</td>
    <td><p>In applications involving matching of image sets, the information from
multiple images must be effectively exploited to represent each set.
State-of-the-art methods use probabilistic distribution or subspace to model a
set and use specific distance measure to compare two sets. These methods are
slow to compute and not compact to use in a large scale scenario.
Learning-based hashing is often used in large scale image retrieval as they
provide a compact representation of each sample and the Hamming distance can be
used to efficiently compare two samples. However, most hashing methods encode
each image separately and discard knowledge that multiple images in the same
set represent the same object or person. We investigate the set hashing problem
by combining both set representation and hashing in a single deep neural
network. An image set is first passed to a CNN module to extract image
features, then these features are aggregated using two types of set feature to
capture both set specific and database-wide distribution information. The
computed set feature is then fed into a multilayer perceptron to learn a
compact binary embedding. Triplet loss is used to train the network by forming
set similarity relations using class labels. We extensively evaluate our
approach on datasets used for image matching and show highly competitive
performance compared to state-of-the-art methods.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/ferdowsi2017sparse/">Sparse Ternary Codes For Similarity Search Have Higher Coding Gain Than Dense Binary Codes</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Sparse Ternary Codes For Similarity Search Have Higher Coding Gain Than Dense Binary Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Sparse Ternary Codes For Similarity Search Have Higher Coding Gain Than Dense Binary Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ferdowsi et al.</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Symposium on Information Theory (ISIT)</td>
    <td>12</td>
    <td><p>This paper addresses the problem of Approximate Nearest Neighbor (ANN) search
in pattern recognition where feature vectors in a database are encoded as
compact codes in order to speed-up the similarity search in large-scale
databases. Considering the ANN problem from an information-theoretic
perspective, we interpret it as an encoding, which maps the original feature
vectors to a less entropic sparse representation while requiring them to be as
informative as possible. We then define the coding gain for ANN search using
information-theoretic measures. We next show that the classical approach to
this problem, which consists of binarization of the projected vectors is
sub-optimal. Instead, a properly designed ternary encoding achieves higher
coding gains and lower complexity.</p>
</td>
    <td>
      
        Similarity Search 
      
        Compact Codes 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/xu2017neural/">Neural Network-based Graph Embedding For Cross-platform Binary Code Similarity Detection</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Neural Network-based Graph Embedding For Cross-platform Binary Code Similarity Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Neural Network-based Graph Embedding For Cross-platform Binary Code Similarity Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security</td>
    <td>545</td>
    <td><p>The problem of cross-platform binary code similarity detection aims at
detecting whether two binary functions coming from different platforms are
similar or not. It has many security applications, including plagiarism
detection, malware detection, vulnerability search, etc. Existing approaches
rely on approximate graph matching algorithms, which are inevitably slow and
sometimes inaccurate, and hard to adapt to a new task. To address these issues,
in this work, we propose a novel neural network-based approach to compute the
embedding, i.e., a numeric vector, based on the control flow graph of each
binary function, then the similarity detection can be done efficiently by
measuring the distance between the embeddings for two functions. We implement a
prototype called Gemini. Our extensive evaluation shows that Gemini outperforms
the state-of-the-art approaches by large margins with respect to similarity
detection accuracy. Further, Gemini can speed up prior art’s embedding
generation time by 3 to 4 orders of magnitude and reduce the required training
time from more than 1 week down to 30 minutes to 10 hours. Our real world case
studies demonstrate that Gemini can identify significantly more vulnerable
firmware images than the state-of-the-art, i.e., Genius. Our research showcases
a successful application of deep learning on computer security problems.</p>
</td>
    <td>
      
        Compact Codes 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/yu2017devil/">The Devil Is In The Middle: Exploiting Mid-level Representations For Cross-domain Instance Matching</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=The Devil Is In The Middle: Exploiting Mid-level Representations For Cross-domain Instance Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=The Devil Is In The Middle: Exploiting Mid-level Representations For Cross-domain Instance Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yu et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>74</td>
    <td><p>Many vision problems require matching images of object instances across
different domains. These include fine-grained sketch-based image retrieval
(FG-SBIR) and Person Re-identification (person ReID). Existing approaches
attempt to learn a joint embedding space where images from different domains
can be directly compared. In most cases, this space is defined by the output of
the final layer of a deep neural network (DNN), which primarily contains
features of a high semantic level. In this paper, we argue that both high and
mid-level features are relevant for cross-domain instance matching (CDIM).
Importantly, mid-level features already exist in earlier layers of the DNN.
They just need to be extracted, represented, and fused properly with the final
layer. Based on this simple but powerful idea, we propose a unified framework
for CDIM. Instantiating our framework for FG-SBIR and ReID, we show that our
simple models can easily beat the state-of-the-art models, which are often
equipped with much more elaborate architectures.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/mor%C3%A8re2017nested/">Nested Invariance Pooling And RBM Hashing For Image Instance Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Nested Invariance Pooling And RBM Hashing For Image Instance Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Nested Invariance Pooling And RBM Hashing For Image Instance Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Morère et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval</td>
    <td>17</td>
    <td><p>The goal of this work is the computation of very compact binary hashes for
image instance retrieval. Our approach has two novel contributions. The first
one is Nested Invariance Pooling (NIP), a method inspired from i-theory, a
mathematical theory for computing group invariant transformations with
feed-forward neural networks. NIP is able to produce compact and
well-performing descriptors with visual representations extracted from
convolutional neural networks. We specifically incorporate scale, translation
and rotation invariances but the scheme can be extended to any arbitrary sets
of transformations. We also show that using moments of increasing order
throughout nesting is important. The NIP descriptors are then hashed to the
target code size (32-256 bits) with a Restricted Boltzmann Machine with a novel
batch-level regularization scheme specifically designed for the purpose of
hashing (RBMH). A thorough empirical evaluation with state-of-the-art shows
that the results obtained both with the NIP descriptors and the NIP+RBMH hashes
are consistently outstanding across a wide range of datasets.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Multimodal Retrieval 
      
        Medical Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/jain2017compact/">Compact Environment-invariant Codes For Robust Visual Place Recognition</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Compact Environment-invariant Codes For Robust Visual Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Compact Environment-invariant Codes For Robust Visual Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jain Unnat, Namboodiri Vinay P., Pandey Gaurav</td> <!-- 🔧 You were missing this -->
    <td>2017 14th Conference on Computer and Robot Vision (CRV)</td>
    <td>6</td>
    <td><p>Robust visual place recognition (VPR) requires scene representations that are
invariant to various environmental challenges such as seasonal changes and
variations due to ambient lighting conditions during day and night. Moreover, a
practical VPR system necessitates compact representations of environmental
features. To satisfy these requirements, in this paper we suggest a
modification to the existing pipeline of VPR systems to incorporate supervised
hashing. The modified system learns (in a supervised setting) compact binary
codes from image feature descriptors. These binary codes imbibe robustness to
the visual variations exposed to it during the training phase, thereby, making
the system adaptive to severe environmental changes. Also, incorporating
supervised hashing makes VPR computationally more efficient and easy to
implement on simple hardware. This is because binary embeddings can be learned
over simple-to-compute features and the distance computation is also in the
low-dimensional hamming space of binary codes. We have performed experiments on
several challenging data sets covering seasonal, illumination and viewpoint
variations. We also compare two widely used supervised hashing methods of
CCAITQ and MLH and show that this new pipeline out-performs or closely matches
the state-of-the-art deep learning VPR methods that are based on
high-dimensional features extracted from pre-trained deep convolutional neural
networks.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/hussein2017unified/">Unified Embedding And Metric Learning For Zero-exemplar Event Detection</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unified Embedding And Metric Learning For Zero-exemplar Event Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unified Embedding And Metric Learning For Zero-exemplar Event Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hussein Noureldien, Gavves Efstratios, Smeulders Arnold W. M.</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>10</td>
    <td><p>Event detection in unconstrained videos is conceived as a content-based video
retrieval with two modalities: textual and visual. Given a text describing a
novel event, the goal is to rank related videos accordingly. This task is
zero-exemplar, no video examples are given to the novel event.
  Related works train a bank of concept detectors on external data sources.
These detectors predict confidence scores for test videos, which are ranked and
retrieved accordingly. In contrast, we learn a joint space in which the visual
and textual representations are embedded. The space casts a novel event as a
probability of pre-defined events. Also, it learns to measure the distance
between an event and its related videos.
  Our model is trained end-to-end on publicly available EventNet. When applied
to TRECVID Multimedia Event Detection dataset, it outperforms the
state-of-the-art by a considerable margin.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/andr%C3%A92017accelerated/">Accelerated Nearest Neighbor Search With Quick ADC</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Accelerated Nearest Neighbor Search With Quick ADC' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Accelerated Nearest Neighbor Search With Quick ADC' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>André Fabien Technicolor, Kermarrec Anne-marie Inria, Scouarnec Nicolas Le Technicolor</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval</td>
    <td>14</td>
    <td><p>Efficient Nearest Neighbor (NN) search in high-dimensional spaces is a
foundation of many multimedia retrieval systems. Because it offers low
responses times, Product Quantization (PQ) is a popular solution. PQ compresses
high-dimensional vectors into short codes using several sub-quantizers, which
enables in-RAM storage of large databases. This allows fast answers to NN
queries, without accessing the SSD or HDD. The key feature of PQ is that it can
compute distances between short codes and high-dimensional vectors using
cache-resident lookup tables. The efficiency of this technique, named
Asymmetric Distance Computation (ADC), remains limited because it performs many
cache accesses.
  In this paper, we introduce Quick ADC, a novel technique that achieves a 3 to
6 times speedup over ADC by exploiting Single Instruction Multiple Data (SIMD)
units available in current CPUs. Efficiently exploiting SIMD requires
algorithmic changes to the ADC procedure. Namely, Quick ADC relies on two key
modifications of ADC: (i) the use 4-bit sub-quantizers instead of the standard
8-bit sub-quantizers and (ii) the quantization of floating-point distances.
This allows Quick ADC to exceed the performance of state-of-the-art systems,
e.g., it achieves a Recall@100 of 0.94 in 3.4 ms on 1 billion SIFT descriptors
(128-bit codes).</p>
</td>
    <td>
      
        Similarity Search 
      
        Multimodal Retrieval 
      
        Medical Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/andoni2017optimal/">Optimal Hashing-based Time-space Trade-offs For Approximate Near Neighbors</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Optimal Hashing-based Time-space Trade-offs For Approximate Near Neighbors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Optimal Hashing-based Time-space Trade-offs For Approximate Near Neighbors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Andoni et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>66</td>
    <td><p>[See the paper for the full abstract.]
  We show tight upper and lower bounds for time-space trade-offs for the
\(c\)-Approximate Near Neighbor Search problem. For the \(d\)-dimensional Euclidean
space and \(n\)-point datasets, we develop a data structure with space \(n^{1 +
\rho_u + o(1)} + O(dn)\) and query time \(n^{\rho_q + o(1)} + d n^{o(1)}\) for
every \(\rho_u, \rho_q \geq 0\) such that: \begin{equation} c^2 \sqrt{\rho_q} +
(c^2 - 1) \sqrt{\rho_u} = \sqrt{2c^2 - 1}. \end{equation}
  This is the first data structure that achieves sublinear query time and
near-linear space for every approximation factor \(c &gt; 1\), improving upon
[Kapralov, PODS 2015]. The data structure is a culmination of a long line of
work on the problem for all space regimes; it builds on Spherical
Locality-Sensitive Filtering [Becker, Ducas, Gama, Laarhoven, SODA 2016] and
data-dependent hashing [Andoni, Indyk, Nguyen, Razenshteyn, SODA 2014] [Andoni,
Razenshteyn, STOC 2015].
  Our matching lower bounds are of two types: conditional and unconditional.
First, we prove tightness of the whole above trade-off in a restricted model of
computation, which captures all known hashing-based approaches. We then show
unconditional cell-probe lower bounds for one and two probes that match the
above trade-off for \(\rho_q = 0\), improving upon the best known lower bounds
from [Panigrahy, Talwar, Wieder, FOCS 2010]. In particular, this is the first
space lower bound (for any static data structure) for two probes which is not
polynomially smaller than the one-probe bound. To show the result for two
probes, we establish and exploit a connection to locally-decodable codes.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/appalaraju2017image/">Image Similarity Using Deep CNN And Curriculum Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Image Similarity Using Deep CNN And Curriculum Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Image Similarity Using Deep CNN And Curriculum Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Appalaraju Srikar, Chaoji Vineet</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>58</td>
    <td><p>Image similarity involves fetching similar looking images given a reference
image. Our solution called SimNet, is a deep siamese network which is trained
on pairs of positive and negative images using a novel online pair mining
strategy inspired by Curriculum learning. We also created a multi-scale CNN,
where the final image embedding is a joint representation of top as well as
lower layer embedding’s. We go on to show that this multi-scale siamese network
is better at capturing fine grained image similarities than traditional CNN’s.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/vo2017revisiting/">Revisiting IM2GPS In The Deep Learning Era</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Revisiting IM2GPS In The Deep Learning Era' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Revisiting IM2GPS In The Deep Learning Era' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Vo Nam, Jacobs Nathan, Hays James</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Conference on Computer Vision (ICCV)</td>
    <td>118</td>
    <td><p>Image geolocalization, inferring the geographic location of an image, is a
challenging computer vision problem with many potential applications. The
recent state-of-the-art approach to this problem is a deep image classification
approach in which the world is spatially divided into cells and a deep network
is trained to predict the correct cell for a given image. We propose to combine
this approach with the original Im2GPS approach in which a query image is
matched against a database of geotagged images and the location is inferred
from the retrieved set. We estimate the geographic location of a query image by
applying kernel density estimation to the locations of its nearest neighbors in
the reference database. Interestingly, we find that the best features for our
retrieval task are derived from networks trained with classification loss even
though we do not use a classification approach at test time. Training with
classification loss outperforms several deep feature learning methods (e.g.
Siamese networks with contrastive of triplet loss) more typical for retrieval
applications. Our simple approach achieves state-of-the-art geolocalization
accuracy while also requiring significantly less training data.</p>
</td>
    <td>
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/tsai2017learning/">Learning Robust Visual-semantic Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Robust Visual-semantic Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Robust Visual-semantic Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tsai Yao-hung Hubert, Huang Liang-kang, Salakhutdinov Ruslan</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Conference on Computer Vision (ICCV)</td>
    <td>147</td>
    <td><p>Many of the existing methods for learning joint embedding of images and text
use only supervised information from paired images and its textual attributes.
Taking advantage of the recent success of unsupervised learning in deep neural
networks, we propose an end-to-end learning framework that is able to extract
more robust multi-modal representations across domains. The proposed method
combines representation learning models (i.e., auto-encoders) together with
cross-domain learning criteria (i.e., Maximum Mean Discrepancy loss) to learn
joint embeddings for semantic and visual features. A novel technique of
unsupervised-data adaptation inference is introduced to construct more
comprehensive embeddings for both labeled and unlabeled data. We evaluate our
method on Animals with Attributes and Caltech-UCSD Birds 200-2011 dataset with
a wide range of applications, including zero and few-shot image recognition and
retrieval, from inductive to transductive settings. Empirically, we show that
our framework improves over the current state of the art on many of the
considered tasks.</p>
</td>
    <td>
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/amato2017aggregating/">Aggregating Binary Local Descriptors For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Aggregating Binary Local Descriptors For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Aggregating Binary Local Descriptors For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Amato Giuseppe, Falchi Fabrizio, Vadicamo Lucia</td> <!-- 🔧 You were missing this -->
    <td>Multimedia Tools and Applications</td>
    <td>11</td>
    <td><p>Content-Based Image Retrieval based on local features is computationally
expensive because of the complexity of both extraction and matching of local
feature. On one hand, the cost for extracting, representing, and comparing
local visual descriptors has been dramatically reduced by recently proposed
binary local features. On the other hand, aggregation techniques provide a
meaningful summarization of all the extracted feature of an image into a single
descriptor, allowing us to speed up and scale up the image search. Only a few
works have recently mixed together these two research directions, defining
aggregation methods for binary local features, in order to leverage on the
advantage of both approaches. In this paper, we report an extensive comparison
among state-of-the-art aggregation methods applied to binary features. Then, we
mathematically formalize the application of Fisher Kernels to Bernoulli Mixture
Models. Finally, we investigate the combination of the aggregated binary
features with the emerging Convolutional Neural Network (CNN) features. Our
results show that aggregation methods on binary features are effective and
represent a worthwhile alternative to the direct matching. Moreover, the
combination of the CNN with the Fisher Vector (FV) built upon binary features
allowed us to obtain a relative improvement over the CNN results that is in
line with that recently obtained using the combination of the CNN with the FV
built upon SIFTs. The advantage of using the FV built upon binary features is
that the extraction process of binary features is about two order of magnitude
faster than SIFTs.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/zoran2017learning/">Learning Deep Nearest Neighbor Representations Using Differentiable Boundary Trees</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Deep Nearest Neighbor Representations Using Differentiable Boundary Trees' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Deep Nearest Neighbor Representations Using Differentiable Boundary Trees' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zoran Daniel, Lakshminarayanan Balaji, Blundell Charles</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>9</td>
    <td><p>Nearest neighbor (kNN) methods have been gaining popularity in recent years
in light of advances in hardware and efficiency of algorithms. There is a
plethora of methods to choose from today, each with their own advantages and
disadvantages. One requirement shared between all kNN based methods is the need
for a good representation and distance measure between samples.
  We introduce a new method called differentiable boundary tree which allows
for learning deep kNN representations. We build on the recently proposed
boundary tree algorithm which allows for efficient nearest neighbor
classification, regression and retrieval. By modelling traversals in the tree
as stochastic events, we are able to form a differentiable cost function which
is associated with the tree’s predictions. Using a deep neural network to
transform the data and back-propagating through the tree allows us to learn
good representations for kNN methods.
  We demonstrate that our method is able to learn suitable representations
allowing for very efficient trees with a clearly interpretable structure.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/zhu2017part/">Part-based Deep Hashing For Large-scale Person Re-identification</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Part-based Deep Hashing For Large-scale Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Part-based Deep Hashing For Large-scale Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhu et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>70</td>
    <td><p>Large-scale is a trend in person re-identification (re-id). It is important
that real-time search be performed in a large gallery. While previous methods
mostly focus on discriminative learning, this paper makes the attempt in
integrating deep learning and hashing into one framework to evaluate the
efficiency and accuracy for large-scale person re-id. We integrate spatial
information for discriminative visual representation by partitioning the
pedestrian image into horizontal parts. Specifically, Part-based Deep Hashing
(PDH) is proposed, in which batches of triplet samples are employed as the
input of the deep hashing architecture. Each triplet sample contains two
pedestrian images (or parts) with the same identity and one pedestrian image
(or part) of the different identity. A triplet loss function is employed with a
constraint that the Hamming distance of pedestrian images (or parts) with the
same identity is smaller than ones with the different identity. In the
experiment, we show that the proposed Part-based Deep Hashing method yields
very competitive re-id accuracy on the large-scale Market-1501 and
Market-1501+500K datasets.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/ahle2017optimal/">Optimal Las Vegas Locality Sensitive Data Structures</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Optimal Las Vegas Locality Sensitive Data Structures' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Optimal Las Vegas Locality Sensitive Data Structures' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ahle Thomas Dybdahl</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS)</td>
    <td>13</td>
    <td><p>We show that approximate similarity (near neighbour) search can be solved in
high dimensions with performance matching state of the art (data independent)
Locality Sensitive Hashing, but with a guarantee of no false negatives.
  Specifically, we give two data structures for common problems.
  For \(c\)-approximate near neighbour in Hamming space we get query time
\(dn^{1/c+o(1)}\) and space \(dn^{1+1/c+o(1)}\) matching that of
\cite{indyk1998approximate} and answering a long standing open question
from~\cite{indyk2000dimensionality} and~\cite{pagh2016locality} in the
affirmative.
  By means of a new deterministic reduction from \(\ell_1\) to Hamming we also
solve \(\ell_1\) and \(ℓ₂\) with query time \(d^2n^{1/c+o(1)}\) and space \(d^2
n^{1+1/c+o(1)}\).
  For \((s_1,s_2)\)-approximate Jaccard similarity we get query time
\(dn^{\rho+o(1)}\) and space \(dn^{1+\rho+o(1)}\),
\(\rho=log\frac{1+s_1}{2s_1}\big/log\frac{1+s_2}{2s_2}\), when sets have equal
size, matching the performance of~\cite{tobias2016}.
  The algorithms are based on space partitions, as with classic LSH, but we
construct these using a combination of brute force, tensoring, perfect hashing
and splitter functions `a la~\cite{naor1995splitters}. We also show a new
dimensionality reduction lemma with 1-sided error.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/ahle2017parameter/">Parameter-free Locality Sensitive Hashing For Spherical Range Reporting</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Parameter-free Locality Sensitive Hashing For Spherical Range Reporting' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Parameter-free Locality Sensitive Hashing For Spherical Range Reporting' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ahle Thomas D., Aumüller Martin, Pagh Rasmus</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>19</td>
    <td><p>We present a data structure for <em>spherical range reporting</em> on a point set
\(S\), i.e., reporting all points in \(S\) that lie within radius \(r\) of a given
query point \(q\). Our solution builds upon the Locality-Sensitive Hashing (LSH)
framework of Indyk and Motwani, which represents the asymptotically best
solutions to near neighbor problems in high dimensions. While traditional LSH
data structures have several parameters whose optimal values depend on the
distance distribution from \(q\) to the points of \(S\), our data structure is
parameter-free, except for the space usage, which is configurable by the user.
Nevertheless, its expected query time basically matches that of an LSH data
structure whose parameters have been <em>optimally chosen for the data and query</em>
in question under the given space constraints. In particular, our data
structure provides a smooth trade-off between hard queries (typically addressed
by standard LSH) and easy queries such as those where the number of points to
report is a constant fraction of \(S\), or where almost all points in \(S\) are far
away from the query point. In contrast, known data structures fix LSH
parameters based on certain parameters of the input alone.
  The algorithm has expected query time bounded by \(O(t (n/t)^\rho)\), where \(t\)
is the number of points to report and \(\rho\in (0,1)\) depends on the data
distribution and the strength of the LSH family used. We further present a
parameter-free way of using multi-probing, for LSH families that support it,
and show that for many such families this approach allows us to get expected
query time close to \(O(n^\rho+t)\), which is the best we can hope to achieve
using LSH. The previously best running time in high dimensions was \(Ω(t
n^\rho)\). For many data distributions where the intrinsic dimensionality of the
point set close to \(q\) is low, we can give improved upper bounds on the
expected query time.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/hong2017content/">Content-based Video-music Retrieval Using Soft Intra-modal Structure Constraint</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Content-based Video-music Retrieval Using Soft Intra-modal Structure Constraint' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Content-based Video-music Retrieval Using Soft Intra-modal Structure Constraint' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hong Sungeun, Im Woobin, Yang Hyun S.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>10</td>
    <td><p>Up to now, only limited research has been conducted on cross-modal retrieval
of suitable music for a specified video or vice versa. Moreover, much of the
existing research relies on metadata such as keywords, tags, or associated
description that must be individually produced and attached posterior. This
paper introduces a new content-based, cross-modal retrieval method for video
and music that is implemented through deep neural networks. We train the
network via inter-modal ranking loss such that videos and music with similar
semantics end up close together in the embedding space. However, if only the
inter-modal ranking constraint is used for embedding, modality-specific
characteristics can be lost. To address this problem, we propose a novel soft
intra-modal structure loss that leverages the relative distance relationship
between intra-modal samples before embedding. We also introduce reasonable
quantitative and qualitative experimental protocols to solve the lack of
standard protocols for less-mature video-music related tasks. Finally, we
construct a large-scale 200K video-music pair benchmark. All the datasets and
source code can be found in our online repository
(https://github.com/csehong/VM-NET).</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/shankar2017deep/">Deep Learning Based Large Scale Visual Recommendation And Search For E-commerce</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Learning Based Large Scale Visual Recommendation And Search For E-commerce' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Learning Based Large Scale Visual Recommendation And Search For E-commerce' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shankar et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>59</td>
    <td><p>In this paper, we present a unified end-to-end approach to build a large
scale Visual Search and Recommendation system for e-commerce. Previous works
have targeted these problems in isolation. We believe a more effective and
elegant solution could be obtained by tackling them together. We propose a
unified Deep Convolutional Neural Network architecture, called VisNet, to learn
embeddings to capture the notion of visual similarity, across several semantic
granularities. We demonstrate the superiority of our approach for the task of
image retrieval, by comparing against the state-of-the-art on the Exact
Street2Shop dataset. We then share the design decisions and trade-offs made
while deploying the model to power Visual Recommendations across a catalog of
50M products, supporting 2K queries a second at Flipkart, India’s largest
e-commerce company. The deployment of our solution has yielded a significant
business impact, as measured by the conversion-rate.</p>
</td>
    <td>
      
        Similarity Search 
      
        Recommender Systems 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/wu2017structured/">Structured Deep Hashing With Convolutional Neural Networks For Fast Person Re-identification</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Structured Deep Hashing With Convolutional Neural Networks For Fast Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Structured Deep Hashing With Convolutional Neural Networks For Fast Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu Lin, Wang Yang</td> <!-- 🔧 You were missing this -->
    <td>Computer Vision and Image Understanding</td>
    <td>71</td>
    <td><p>Given a pedestrian image as a query, the purpose of person re-identification
is to identify the correct match from a large collection of gallery images
depicting the same person captured by disjoint camera views. The critical
challenge is how to construct a robust yet discriminative feature
representation to capture the compounded variations in pedestrian appearance.
To this end, deep learning methods have been proposed to extract hierarchical
features against extreme variability of appearance. However, existing methods
in this category generally neglect the efficiency in the matching stage whereas
the searching speed of a re-identification system is crucial in real-world
applications. In this paper, we present a novel deep hashing framework with
Convolutional Neural Networks (CNNs) for fast person re-identification.
Technically, we simultaneously learn both CNN features and hash functions/codes
to get robust yet discriminative features and similarity-preserving hash codes.
Thereby, person re-identification can be resolved by efficiently computing and
ranking the Hamming distances between images. A structured loss function
defined over positive pairs and hard negatives is proposed to formulate a novel
optimization problem so that fast convergence and more stable optimized
solution can be obtained. Extensive experiments on two benchmarks CUHK03
\cite{FPNN} and Market-1501 \cite{Market1501} show that the proposed deep
architecture is efficacy over state-of-the-arts.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/venkateswara2017deep/">Deep Hashing Network For Unsupervised Domain Adaptation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Hashing Network For Unsupervised Domain Adaptation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Hashing Network For Unsupervised Domain Adaptation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Venkateswara et al.</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>1690</td>
    <td><p>In recent years, deep neural networks have emerged as a dominant machine
learning tool for a wide variety of application domains. However, training a
deep neural network requires a large amount of labeled data, which is an
expensive process in terms of time, labor and human expertise. Domain
adaptation or transfer learning algorithms address this challenge by leveraging
labeled data in a different, but related source domain, to develop a model for
the target domain. Further, the explosive growth of digital data has posed a
fundamental challenge concerning its storage and retrieval. Due to its storage
and retrieval efficiency, recent years have witnessed a wide application of
hashing in a variety of computer vision applications. In this paper, we first
introduce a new dataset, Office-Home, to evaluate domain adaptation algorithms.
The dataset contains images of a variety of everyday objects from multiple
domains. We then propose a novel deep learning framework that can exploit
labeled source data and unlabeled target data to learn informative hash codes,
to accurately classify unseen target data. To the best of our knowledge, this
is the first research effort to exploit the feature learning capabilities of
deep neural networks to learn representative hash codes to address the domain
adaptation problem. Our extensive empirical studies on multiple transfer tasks
corroborate the usefulness of the framework in learning efficient hash codes
which outperform existing competitive baselines for unsupervised domain
adaptation.</p>
</td>
    <td>
      
        Unsupervised 
      
        CVPR 
      
        Neural Hashing 
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/zhang2017ssdh/">SSDH: Semi-supervised Deep Hashing For Large Scale Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=SSDH: Semi-supervised Deep Hashing For Large Scale Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=SSDH: Semi-supervised Deep Hashing For Large Scale Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Jian, Peng Yuxin</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Circuits and Systems for Video Technology</td>
    <td>150</td>
    <td><p>Hashing methods have been widely used for efficient similarity retrieval on
large scale image database. Traditional hashing methods learn hash functions to
generate binary codes from hand-crafted features, which achieve limited
accuracy since the hand-crafted features cannot optimally represent the image
content and preserve the semantic similarity. Recently, several deep hashing
methods have shown better performance because the deep architectures generate
more discriminative feature representations. However, these deep hashing
methods are mainly designed for supervised scenarios, which only exploit the
semantic similarity information, but ignore the underlying data structures. In
this paper, we propose the semi-supervised deep hashing (SSDH) approach, to
perform more effective hash function learning by simultaneously preserving
semantic similarity and underlying data structures. The main contributions are
as follows: (1) We propose a semi-supervised loss to jointly minimize the
empirical error on labeled data, as well as the embedding error on both labeled
and unlabeled data, which can preserve the semantic similarity and capture the
meaningful neighbors on the underlying data structures for effective hashing.
(2) A semi-supervised deep hashing network is designed to extensively exploit
both labeled and unlabeled data, in which we propose an online graph
construction method to benefit from the evolving deep features during training
to better capture semantic neighbors. To the best of our knowledge, the
proposed deep network is the first deep hashing method that can perform hash
code learning and feature learning simultaneously in a semi-supervised fashion.
Experimental results on 5 widely-used datasets show that our proposed approach
outperforms the state-of-the-art hashing methods.</p>
</td>
    <td>
      
        Image Retrieval 
      
        Neural Hashing 
      
        SUPERVISED 
      
        Hashing Methods 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/huang2017unsupervised/">Unsupervised Triplet Hashing For Fast Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Triplet Hashing For Fast Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Triplet Hashing For Fast Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Huang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the on Thematic Workshops of ACM Multimedia 2017</td>
    <td>54</td>
    <td><p>Hashing has played a pivotal role in large-scale image retrieval. With the
development of Convolutional Neural Network (CNN), hashing learning has shown
great promise. But existing methods are mostly tuned for classification, which
are not optimized for retrieval tasks, especially for instance-level retrieval.
In this study, we propose a novel hashing method for large-scale image
retrieval. Considering the difficulty in obtaining labeled datasets for image
retrieval task in large scale, we propose a novel CNN-based unsupervised
hashing method, namely Unsupervised Triplet Hashing (UTH). The unsupervised
hashing network is designed under the following three principles: 1) more
discriminative representations for image retrieval; 2) minimum quantization
loss between the original real-valued feature descriptors and the learned hash
codes; 3) maximum information entropy for the learned hash codes. Extensive
experiments on CIFAR-10, MNIST and In-shop datasets have shown that UTH
outperforms several state-of-the-art unsupervised hashing methods in terms of
retrieval accuracy.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
        Image Retrieval 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/yang2017utilizing/">Utilizing Embeddings For Ad-hoc Retrieval By Document-to-document Similarity</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Utilizing Embeddings For Ad-hoc Retrieval By Document-to-document Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Utilizing Embeddings For Ad-hoc Retrieval By Document-to-document Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yang Chenhao, He Ben, Ran Yanhua</td> <!-- 🔧 You were missing this -->
    <td>Information Retrieval</td>
    <td>8</td>
    <td><p>Latent semantic representations of words or paragraphs, namely the
embeddings, have been widely applied to information retrieval (IR). One of the
common approaches of utilizing embeddings for IR is to estimate the
document-to-query (D2Q) similarity in their embeddings. As words with similar
syntactic usage are usually very close to each other in the embeddings space,
although they are not semantically similar, the D2Q similarity approach may
suffer from the problem of “multiple degrees of similarity”. To this end, this
paper proposes a novel approach that estimates a semantic relevance score (SEM)
based on document-to-document (D2D) similarity of embeddings. As Word or
Para2Vec generates embeddings by the context of words/paragraphs, the D2D
similarity approach turns the task of document ranking into the estimation of
similarity between content within different documents. Experimental results on
standard TREC test collections show that our proposed approach outperforms
strong baselines.</p>
</td>
    <td>
      
        SIGIR 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/grzegorczyk2017binary/">Binary Paragraph Vectors</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Binary Paragraph Vectors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Binary Paragraph Vectors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Grzegorczyk Karol, Kurdziel Marcin</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2nd Workshop on Representation Learning for NLP</td>
    <td>5</td>
    <td><p>Recently Le &amp; Mikolov described two log-linear models, called Paragraph
Vector, that can be used to learn state-of-the-art distributed representations
of documents. Inspired by this work, we present Binary Paragraph Vector models:
simple neural networks that learn short binary codes for fast information
retrieval. We show that binary paragraph vectors outperform autoencoder-based
binary codes, despite using fewer bits. We also evaluate their precision in
transfer learning settings, where binary codes are inferred for documents
unrelated to the training corpus. Results from these experiments indicate that
binary paragraph vectors can capture semantics relevant for various
domain-specific documents. Finally, we present a model that simultaneously
learns short binary codes and longer, real-valued representations. This model
can be used to rapidly retrieve a short list of highly relevant documents from
a large document collection.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/zhong2017re/">Re-ranking Person Re-identification With K-reciprocal Encoding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Re-ranking Person Re-identification With K-reciprocal Encoding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Re-ranking Person Re-identification With K-reciprocal Encoding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhong et al.</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>1369</td>
    <td><p>When considering person re-identification (re-ID) as a retrieval process,
re-ranking is a critical step to improve its accuracy. Yet in the re-ID
community, limited effort has been devoted to re-ranking, especially those
fully automatic, unsupervised solutions. In this paper, we propose a
k-reciprocal encoding method to re-rank the re-ID results. Our hypothesis is
that if a gallery image is similar to the probe in the k-reciprocal nearest
neighbors, it is more likely to be a true match. Specifically, given an image,
a k-reciprocal feature is calculated by encoding its k-reciprocal nearest
neighbors into a single vector, which is used for re-ranking under the Jaccard
distance. The final distance is computed as the combination of the original
distance and the Jaccard distance. Our re-ranking method does not require any
human interaction or any labeled data, so it is applicable to large-scale
datasets. Experiments on the large-scale Market-1501, CUHK03, MARS, and PRW
datasets confirm the effectiveness of our method.</p>
</td>
    <td>
      
        Hybrid ANN Methods 
      
        Re RANKING 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/wygocki2017fast/">On Fast Bounded Locality Sensitive Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=On Fast Bounded Locality Sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=On Fast Bounded Locality Sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wygocki Piotr</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</td>
    <td>85</td>
    <td><p>In this paper, we examine the hash functions expressed as scalar products,
i.e., \(f(x)=&lt;v,x&gt;\), for some bounded random vector \(v\). Such hash functions
have numerous applications, but often there is a need to optimize the choice of
the distribution of \(v\). In the present work, we focus on so-called
anti-concentration bounds, i.e. the upper bounds of \(\mathbb{P}\left[|&lt;v,x&gt;| &lt;
\alpha \right]\). In many applications, \(v\) is a vector of independent random
variables with standard normal distribution. In such case, the distribution of
\(&lt;v,x&gt;\) is also normal and it is easy to approximate \(\mathbb{P}\left[|&lt;v,x&gt;| &lt;
\alpha \right]\). Here, we consider two bounded distributions in the context of
the anti-concentration bounds. Particularly, we analyze \(v\) being a random
vector from the unit ball in \(l_{\infty}\) and \(v\) being a random vector from
the unit sphere in \(l_{2}\). We show optimal up to a constant anti-concentration
measures for functions \(f(x)=&lt;v,x&gt;\).
  As a consequence of our research, we obtain new best results for \newline
\textit{\(c\)-approximate nearest neighbors without false negatives} for \(l_p\) in
high dimensional space for all \(p\in[1,\infty]\), for
\(c=Ω(\max\{\sqrt{d},d^{1/p}\})\). These results improve over those
presented in [16]. Finally, our paper reports progress on answering the open
problem by Pagh~[17], who considered the nearest neighbor search without false
negatives for the Hamming distance.</p>
</td>
    <td>
      
        KDD 
      
        Hashing Methods 
      
        Locality Sensitive Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/liu2017end/">End-to-end Binary Representation Learning Via Direct Binary Embedding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=End-to-end Binary Representation Learning Via Direct Binary Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=End-to-end Binary Representation Learning Via Direct Binary Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Conference on Image Processing (ICIP)</td>
    <td>7</td>
    <td><p>Learning binary representation is essential to large-scale computer vision
tasks. Most existing algorithms require a separate quantization constraint to
learn effective hashing functions. In this work, we present Direct Binary
Embedding (DBE), a simple yet very effective algorithm to learn binary
representation in an end-to-end fashion. By appending an ingeniously designed
DBE layer to the deep convolutional neural network (DCNN), DBE learns binary
code directly from the continuous DBE layer activation without quantization
error. By employing the deep residual network (ResNet) as DCNN component, DBE
captures rich semantics from images. Furthermore, in the effort of handling
multilabel images, we design a joint cross entropy loss that includes both
softmax cross entropy and weighted binary cross entropy in consideration of the
correlation and independence of labels, respectively. Extensive experiments
demonstrate the significant superiority of DBE over state-of-the-art methods on
tasks of natural object recognition, image retrieval and image annotation.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/zhang2017hashgan/">Hashgan:attention-aware Deep Adversarial Hashing For Cross Modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashgan:attention-aware Deep Adversarial Hashing For Cross Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashgan:attention-aware Deep Adversarial Hashing For Cross Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>16</td>
    <td><p>As the rapid growth of multi-modal data, hashing methods for cross-modal
retrieval have received considerable attention. Deep-networks-based cross-modal
hashing methods are appealing as they can integrate feature learning and hash
coding into end-to-end trainable frameworks. However, it is still challenging
to find content similarities between different modalities of data due to the
heterogeneity gap. To further address this problem, we propose an adversarial
hashing network with attention mechanism to enhance the measurement of content
similarities by selectively focusing on informative parts of multi-modal data.
The proposed new adversarial network, HashGAN, consists of three building
blocks: 1) the feature learning module to obtain feature representations, 2)
the generative attention module to generate an attention mask, which is used to
obtain the attended (foreground) and the unattended (background) feature
representations, 3) the discriminative hash coding module to learn hash
functions that preserve the similarities between different modalities. In our
framework, the generative module and the discriminative module are trained in
an adversarial way: the generator is learned to make the discriminator cannot
preserve the similarities of multi-modal data w.r.t. the background feature
representations, while the discriminator aims to preserve the similarities of
multi-modal data w.r.t. both the foreground and the background feature
representations. Extensive evaluations on several benchmark datasets
demonstrate that the proposed HashGAN brings substantial improvements over
other state-of-the-art cross-modal hashing methods.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Robustness 
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/liu2017discretely/">Discretely Coding Semantic Rank Orders For Supervised Image Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Discretely Coding Semantic Rank Orders For Supervised Image Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Discretely Coding Semantic Rank Orders For Supervised Image Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>49</td>
    <td><p>Learning to hash has been recognized to accomplish highly efficient storage and retrieval for large-scale visual data. Particularly, ranking-based hashing techniques have recently attracted broad research attention because ranking accuracy among the retrieved data is well explored and their objective is more applicable to realistic search tasks. However, directly optimizing discrete hash codes without continuous-relaxations on a nonlinear ranking objective is infeasible by either traditional optimization methods or even recent discrete hashing algorithms. To address this challenging issue, in this paper, we introduce a novel supervised hashing method, dubbed Discrete Semantic Ranking Hashing (DSeRH), which aims to directly embed semantic rank orders into binary codes. In DSeRH, a generalized Adaptive Discrete Minimization (ADM) approach is proposed to discretely optimize binary codes with the quadratic nonlinear ranking objective in an iterative manner and is guaranteed to converge quickly. Additionally, instead of using 0/1 independent labels to form rank orders as in previous works, we generate the listwise rank orders from the high-level semantic word embeddings which can quantitatively capture the intrinsic correlation between different categories. We evaluate our DSeRH, coupled with both linear and deep convolutional neural network (CNN) hash functions, on three image datasets, i.e., CIFAR-10, SUN397 and ImageNet100, and the results manifest that DSeRH can outperform the state-of-the-art ranking-based hashing methods.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
        Image Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/qiu2017deep/">Deep Semantic Hashing With Generative Adversarial Networks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Semantic Hashing With Generative Adversarial Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Semantic Hashing With Generative Adversarial Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Qiu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>97</td>
    <td><p>Hashing has been a widely-adopted technique for nearest neighbor search in
large-scale image retrieval tasks. Recent research has shown that leveraging
supervised information can lead to high quality hashing. However, the cost of
annotating data is often an obstacle when applying supervised hashing to a new
domain. Moreover, the results can suffer from the robustness problem as the
data at training and test stage could come from similar but different
distributions. This paper studies the exploration of generating synthetic data
through semi-supervised generative adversarial networks (GANs), which leverages
largely unlabeled and limited labeled training data to produce highly
compelling data with intrinsic invariance and global coherence, for better
understanding statistical structures of natural data. We demonstrate that the
above two limitations can be well mitigated by applying the synthetic data for
hashing. Specifically, a novel deep semantic hashing with GANs (DSH-GANs) is
presented, which mainly consists of four components: a deep convolution neural
networks (CNN) for learning image representations, an adversary stream to
distinguish synthetic images from real ones, a hash stream for encoding image
representations to hash codes and a classification stream. The whole
architecture is trained end-to-end by jointly optimizing three losses, i.e.,
adversarial loss to correct label of synthetic or real for each sample, triplet
ranking loss to preserve the relative similarity ordering in the input
real-synthetic triplets and classification loss to classify each sample
accurately. Extensive experiments conducted on both CIFAR-10 and NUS-WIDE image
benchmarks validate the capability of exploiting synthetic images for hashing.
Our framework also achieves superior results when compared to state-of-the-art
deep hash models.</p>
</td>
    <td>
      
        SIGIR 
      
        Hashing Methods 
      
        Robustness 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/luo2017arrays/">Arrays Of (locality-sensitive) Count Estimators (ACE): High-speed Anomaly Detection Via Cache Lookups</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Arrays Of (locality-sensitive) Count Estimators (ACE): High-speed Anomaly Detection Via Cache Lookups' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Arrays Of (locality-sensitive) Count Estimators (ACE): High-speed Anomaly Detection Via Cache Lookups' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Luo Chen, Shrivastava Anshumali</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>Anomaly detection is one of the frequent and important subroutines deployed
in large-scale data processing systems. Even being a well-studied topic,
existing techniques for unsupervised anomaly detection require storing
significant amounts of data, which is prohibitive from memory and latency
perspective. In the big-data world existing methods fail to address the new set
of memory and latency constraints. In this paper, we propose ACE (Arrays of
(locality-sensitive) Count Estimators) algorithm that can be 60x faster than
the ELKI package~\cite{DBLP:conf/ssd/AchtertBKSZ09}, which has the fastest
implementation of the unsupervised anomaly detection algorithms. ACE algorithm
requires less than \(4MB\) memory, to dynamically compress the full data
information into a set of count arrays. These tiny \(4MB\) arrays of counts are
sufficient for unsupervised anomaly detection. At the core of the ACE
algorithm, there is a novel statistical estimator which is derived from the
sampling view of Locality Sensitive Hashing(LSH). This view is significantly
different and efficient than the widely popular view of LSH for near-neighbor
search. We show the superiority of ACE algorithm over 11 popular baselines on 3
benchmark datasets, including the KDD-Cup99 data which is the largest available
benchmark comprising of more than half a million entries with ground truth
anomaly labels.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/wang2016unsupervised/">Unsupervised Cross-media Hashing With Structure Preservation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Cross-media Hashing With Structure Preservation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Cross-media Hashing With Structure Preservation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Xiangyu, Chia Alex Yong-sang</td> <!-- 🔧 You were missing this -->
    <td>Chinese Journal of Electronics</td>
    <td>22</td>
    <td><p>Recent years have seen the exponential growth of heterogeneous multimedia
data. The need for effective and accurate data retrieval from heterogeneous
data sources has attracted much research interest in cross-media retrieval.
Here, given a query of any media type, cross-media retrieval seeks to find
relevant results of different media types from heterogeneous data sources. To
facilitate large-scale cross-media retrieval, we propose a novel unsupervised
cross-media hashing method. Our method incorporates local affinity and distance
repulsion constraints into a matrix factorization framework. Correspondingly,
the proposed method learns hash functions that generates unified hash codes
from different media types, while ensuring intrinsic geometric structure of the
data distribution is preserved. These hash codes empower the similarity between
data of different media types to be evaluated directly. Experimental results on
two large-scale multimedia datasets demonstrate the effectiveness of the
proposed method, where we outperform the state-of-the-art methods.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/salvador2016faster/">Faster R-CNN Features For Instance Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Faster R-CNN Features For Instance Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Faster R-CNN Features For Instance Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Salvador et al.</td> <!-- 🔧 You were missing this -->
    <td>2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>121</td>
    <td><p>Image representations derived from pre-trained Convolutional Neural Networks
(CNNs) have become the new state of the art in computer vision tasks such as
instance retrieval. This work explores the suitability for instance retrieval
of image- and region-wise representations pooled from an object detection CNN
such as Faster R-CNN. We take advantage of the object proposals learned by a
Region Proposal Network (RPN) and their associated CNN features to build an
instance search pipeline composed of a first filtering stage followed by a
spatial reranking. We further investigate the suitability of Faster R-CNN
features when the network is fine-tuned for the same objects one wants to
retrieve. We assess the performance of our proposed system with the Oxford
Buildings 5k, Paris Buildings 6k and a subset of TRECVid Instance Search 2013,
achieving competitive results.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/douze2016polysemous/">Polysemous Codes</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Polysemous Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Polysemous Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Douze Matthijs, Jégou Hervé, Perronnin Florent</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>45</td>
    <td><p>This paper considers the problem of approximate nearest neighbor search in
the compressed domain. We introduce polysemous codes, which offer both the
distance estimation quality of product quantization and the efficient
comparison of binary codes with Hamming distance. Their design is inspired by
algorithms introduced in the 90’s to construct channel-optimized vector
quantizers. At search time, this dual interpretation accelerates the search.
Most of the indexed vectors are filtered out with Hamming distance, letting
only a fraction of the vectors to be ranked with an asymmetric distance
estimator.
  The method is complementary with a coarse partitioning of the feature space
such as the inverted multi-index. This is shown by our experiments performed on
several public benchmarks such as the BIGANN dataset comprising one billion
vectors, for which we report state-of-the-art results for query times below
0.3\,millisecond per core. Last but not least, our approach allows the
approximate computation of the k-NN graph associated with the Yahoo Flickr
Creative Commons 100M, described by CNN image descriptors, in less than 8 hours
on a single machine.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/castrejon2016learning/">Learning Aligned Cross-modal Representations From Weakly Aligned Data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Aligned Cross-modal Representations From Weakly Aligned Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Aligned Cross-modal Representations From Weakly Aligned Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Castrejon et al.</td> <!-- 🔧 You were missing this -->
    <td>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>198</td>
    <td><p>People can recognize scenes across many different modalities beyond natural
images. In this paper, we investigate how to learn cross-modal scene
representations that transfer across modalities. To study this problem, we
introduce a new cross-modal scene dataset. While convolutional neural networks
can categorize cross-modal scenes well, they also learn an intermediate
representation not aligned across modalities, which is undesirable for
cross-modal transfer applications. We present methods to regularize cross-modal
convolutional neural networks so that they have a shared representation that is
agnostic of the modality. Our experiments suggest that our scene representation
can help transfer representations across modalities for retrieval. Moreover,
our visualizations suggest that units emerge in the shared representation that
tend to activate on consistent concepts independently of the modality.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/huang2016local/">Local Similarity-aware Deep Feature Embedding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Local Similarity-aware Deep Feature Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Local Similarity-aware Deep Feature Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Huang Chen, Loy Chen Change, Tang Xiaoou</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>124</td>
    <td><p>Existing deep embedding methods in vision tasks are capable of learning a
compact Euclidean space from images, where Euclidean distances correspond to a
similarity metric. To make learning more effective and efficient, hard sample
mining is usually employed, with samples identified through computing the
Euclidean feature distance. However, the global Euclidean distance cannot
faithfully characterize the true feature similarity in a complex visual feature
space, where the intraclass distance in a high-density region may be larger
than the interclass distance in low-density regions. In this paper, we
introduce a Position-Dependent Deep Metric (PDDM) unit, which is capable of
learning a similarity metric adaptive to local feature structure. The metric
can be used to select genuinely hard samples in a local neighborhood to guide
the deep embedding learning in an online and robust manner. The new layer is
appealing in that it is pluggable to any convolutional networks and is trained
end-to-end. Our local similarity-aware feature embedding not only demonstrates
faster convergence and boosted performance on two complex image retrieval
datasets, its large margin nature also leads to superior generalization results
under the large and open set scenarios of transfer learning and zero-shot
learning on ImageNet 2010 and ImageNet-10K datasets.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/carreiraperpi%C3%B1%C3%A1n2016ensemble/">An Ensemble Diversity Approach To Supervised Binary Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=An Ensemble Diversity Approach To Supervised Binary Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=An Ensemble Diversity Approach To Supervised Binary Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Carreira-perpiñán Miguel Á., Raziperchikolaei Ramin</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>8</td>
    <td><p>Binary hashing is a well-known approach for fast approximate nearest-neighbor
search in information retrieval. Much work has focused on affinity-based
objective functions involving the hash functions or binary codes. These
objective functions encode neighborhood information between data points and are
often inspired by manifold learning algorithms. They ensure that the hash
functions differ from each other through constraints or penalty terms that
encourage codes to be orthogonal or dissimilar across bits, but this couples
the binary variables and complicates the already difficult optimization. We
propose a much simpler approach: we train each hash function (or bit)
independently from each other, but introduce diversity among them using
techniques from classifier ensembles. Surprisingly, we find that not only is
this faster and trivially parallelizable, but it also improves over the more
complex, coupled objective function, and achieves state-of-the-art precision
and recall in experiments with image retrieval.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/mitra2016dual/">A Dual Embedding Space Model For Document Ranking</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Dual Embedding Space Model For Document Ranking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Dual Embedding Space Model For Document Ranking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mitra et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>110</td>
    <td><p>A fundamental goal of search engines is to identify, given a query, documents
that have relevant text. This is intrinsically difficult because the query and
the document may use different vocabulary, or the document may contain query
words without being relevant. We investigate neural word embeddings as a source
of evidence in document ranking. We train a word2vec embedding model on a large
unlabelled query corpus, but in contrast to how the model is commonly used, we
retain both the input and the output projections, allowing us to leverage both
the embedding spaces to derive richer distributional relationships. During
ranking we map the query words into the input space and the document words into
the output space, and compute a query-document relevance score by aggregating
the cosine similarities across all the query-document word pairs.
  We postulate that the proposed Dual Embedding Space Model (DESM) captures
evidence on whether a document is about a query term in addition to what is
modelled by traditional term-frequency based approaches. Our experiments show
that the DESM can re-rank top documents returned by a commercial Web search
engine, like Bing, better than a term-matching based signal like TF-IDF.
However, when ranking a larger set of candidate documents, we find the
embeddings-based approach is prone to false positives, retrieving documents
that are only loosely related to the query. We demonstrate that this problem
can be solved effectively by ranking based on a linear mixture of the DESM and
the word counting features.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/sharma2016stacked/">Stacked Autoencoders For Medical Image Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Stacked Autoencoders For Medical Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Stacked Autoencoders For Medical Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sharma et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>32</td>
    <td><p>Medical images can be a valuable resource for reliable information to support
medical diagnosis. However, the large volume of medical images makes it
challenging to retrieve relevant information given a particular scenario. To
solve this challenge, content-based image retrieval (CBIR) attempts to
characterize images (or image regions) with invariant content information in
order to facilitate image search. This work presents a feature extraction
technique for medical images using stacked autoencoders, which encode images to
binary vectors. The technique is applied to the IRMA dataset, a collection of
14,410 x-ray images in order to demonstrate the ability of autoencoders to
retrieve similar x-rays given test queries. Using IRMA dataset as a benchmark,
it was found that stacked autoencoders gave excellent results with a retrieval
error of 376 for 1,733 test images with a compression of 74.61%.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/hao2016what/">What Is The Best Practice For Cnns Applied To Visual Instance Retrieval?</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=What Is The Best Practice For Cnns Applied To Visual Instance Retrieval?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=What Is The Best Practice For Cnns Applied To Visual Instance Retrieval?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hao et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>9</td>
    <td><p>Previous work has shown that feature maps of deep convolutional neural
networks (CNNs) can be interpreted as feature representation of a particular
image region. Features aggregated from these feature maps have been exploited
for image retrieval tasks and achieved state-of-the-art performances in recent
years. The key to the success of such methods is the feature representation.
However, the different factors that impact the effectiveness of features are
still not explored thoroughly. There are much less discussion about the best
combination of them.
  The main contribution of our paper is the thorough evaluations of the various
factors that affect the discriminative ability of the features extracted from
CNNs. Based on the evaluation results, we also identify the best choices for
different factors and propose a new multi-scale image feature representation
method to encode the image effectively. Finally, we show that the proposed
method generalises well and outperforms the state-of-the-art methods on four
typical datasets used for visual instance retrieval.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/gordo2016deep/">Deep Image Retrieval: Learning Global Representations For Image Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Image Retrieval: Learning Global Representations For Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Image Retrieval: Learning Global Representations For Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gordo et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>753</td>
    <td><p>We propose a novel approach for instance-level image retrieval. It produces a
global and compact fixed-length representation for each image by aggregating
many region-wise descriptors. In contrast to previous works employing
pre-trained deep networks as a black box to produce features, our method
leverages a deep architecture trained for the specific task of image retrieval.
Our contribution is twofold: (i) we leverage a ranking framework to learn
convolution and projection weights that are used to build the region features;
and (ii) we employ a region proposal network to learn which regions should be
pooled to form the final global descriptor. We show that using clean training
data is key to the success of our approach. To that aim, we use a large scale
but noisy landmark dataset and develop an automatic cleaning approach. The
proposed architecture produces a global image representation in a single
forward pass. Our approach significantly outperforms previous approaches based
on global descriptors on standard datasets. It even surpasses most prior works
based on costly local descriptor indexing and spatial verification. Additional
material is available at www.xrce.xerox.com/Deep-Image-Retrieval.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/wurzer2016randomised/">Randomised Relevance Model</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Randomised Relevance Model' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Randomised Relevance Model' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wurzer Dominik, Osborne Miles, Lavrenko Victor</td> <!-- 🔧 You were missing this -->
    <td>The Lancet Oncology</td>
    <td>81</td>
    <td><p>Relevance Models are well-known retrieval models and capable of producing
competitive results. However, because they use query expansion they can be very
slow. We address this slowness by incorporating two variants of locality
sensitive hashing (LSH) into the query expansion process. Results on two
document collections suggest that we can obtain large reductions in the amount
of work, with a small reduction in effectiveness. Our approach is shown to be
additive when pruning query terms.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/gehrig2016visual/">Visual Place Recognition With Probabilistic Vertex Voting</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Visual Place Recognition With Probabilistic Vertex Voting' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Visual Place Recognition With Probabilistic Vertex Voting' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gehrig et al.</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Conference on Robotics and Automation (ICRA)</td>
    <td>36</td>
    <td><p>We propose a novel scoring concept for visual place recognition based on
nearest neighbor descriptor voting and demonstrate how the algorithm naturally
emerges from the problem formulation. Based on the observation that the number
of votes for matching places can be evaluated using a binomial distribution
model, loop closures can be detected with high precision. By casting the
problem into a probabilistic framework, we not only remove the need for
commonly employed heuristic parameters but also provide a powerful score to
classify matching and non-matching places. We present methods for both a 2D-2D
pose-graph vertex matching and a 2D-3D landmark matching based on the above
scoring. The approach maintains accuracy while being efficient enough for
online application through the use of compact (low dimensional) descriptors and
fast nearest neighbor retrieval techniques. The proposed methods are evaluated
on several challenging datasets in varied environments, showing
state-of-the-art results with high precision and high recall.</p>
</td>
    <td>
      
        ICRA 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/cao2016where/">Where To Focus: Query Adaptive Matching For Instance Retrieval Using Convolutional Feature Maps</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Where To Focus: Query Adaptive Matching For Instance Retrieval Using Convolutional Feature Maps' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Where To Focus: Query Adaptive Matching For Instance Retrieval Using Convolutional Feature Maps' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cao et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>19</td>
    <td><p>Instance retrieval requires one to search for images that contain a
particular object within a large corpus. Recent studies show that using image
features generated by pooling convolutional layer feature maps (CFMs) of a
pretrained convolutional neural network (CNN) leads to promising performance
for this task. However, due to the global pooling strategy adopted in those
works, the generated image feature is less robust to image clutter and tends to
be contaminated by the irrelevant image patterns. In this article, we alleviate
this drawback by proposing a novel reranking algorithm using CFMs to refine the
retrieval result obtained by existing methods. Our key idea, called query
adaptive matching (QAM), is to first represent the CFMs of each image by a set
of base regions which can be freely combined into larger regions-of-interest.
Then the similarity between the query and a candidate image is measured by the
best similarity score that can be attained by comparing the query feature and
the feature pooled from a combined region. We show that the above procedure can
be cast as an optimization problem and it can be solved efficiently with an
off-the-shelf solver. Besides this general framework, we also propose two
practical ways to create the base regions. One is based on the property of the
CFM and the other one is based on a multi-scale spatial pyramid scheme. Through
extensive experiments, we show that our reranking approaches bring substantial
performance improvement and by applying them we can outperform the state of the
art on several instance retrieval benchmarks.</p>
</td>
    <td>
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/salvi2016bloom/">Bloom Filters And Compact Hash Codes For Efficient And Distributed Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Bloom Filters And Compact Hash Codes For Efficient And Distributed Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Bloom Filters And Compact Hash Codes For Efficient And Distributed Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Salvi et al.</td> <!-- 🔧 You were missing this -->
    <td>2016 IEEE International Symposium on Multimedia (ISM)</td>
    <td>5</td>
    <td><p>This paper presents a novel method for efficient image retrieval, based on a
simple and effective hashing of CNN features and the use of an indexing
structure based on Bloom filters. These filters are used as gatekeepers for the
database of image features, allowing to avoid to perform a query if the query
features are not stored in the database and speeding up the query process,
without affecting retrieval performance. Thanks to the limited memory
requirements the system is suitable for mobile applications and distributed
databases, associating each filter to a distributed portion of the database.
Experimental validation has been performed on three standard image retrieval
datasets, outperforming state-of-the-art hashing methods in terms of precision,
while the proposed indexing method obtains a \(2\times\) speedup.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/luo2016ssh/">SSH (sketch, Shingle, & Hash) For Indexing Massive-scale Time Series</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=SSH (sketch, Shingle, & Hash) For Indexing Massive-scale Time Series' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=SSH (sketch, Shingle, & Hash) For Indexing Massive-scale Time Series' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Luo Chen, Shrivastava Anshumali</td> <!-- 🔧 You were missing this -->
    <td>Data Mining and Knowledge Discovery</td>
    <td>18</td>
    <td><p>Similarity search on time series is a frequent operation in large-scale
data-driven applications. Sophisticated similarity measures are standard for
time series matching, as they are usually misaligned. Dynamic Time Warping or
DTW is the most widely used similarity measure for time series because it
combines alignment and matching at the same time. However, the alignment makes
DTW slow. To speed up the expensive similarity search with DTW, branch and
bound based pruning strategies are adopted. However, branch and bound based
pruning are only useful for very short queries (low dimensional time series),
and the bounds are quite weak for longer queries. Due to the loose bounds
branch and bound pruning strategy boils down to a brute-force search.
  To circumvent this issue, we design SSH (Sketch, Shingle, &amp; Hashing), an
efficient and approximate hashing scheme which is much faster than the
state-of-the-art branch and bound searching technique: the UCR suite. SSH uses
a novel combination of sketching, shingling and hashing techniques to produce
(probabilistic) indexes which align (near perfectly) with DTW similarity
measure. The generated indexes are then used to create hash buckets for
sub-linear search. Our results show that SSH is very effective for longer time
sequence and prunes around 95% candidates, leading to the massive speedup in
search with DTW. Empirical results on two large-scale benchmark time series
data show that our proposed method can be around 20 times faster than the
state-of-the-art package (UCR suite) without any significant loss in accuracy.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/cao2016correlation/">Correlation Autoencoder Hashing For Supervised Cross-modal Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Correlation Autoencoder Hashing For Supervised Cross-modal Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Correlation Autoencoder Hashing For Supervised Cross-modal Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cao et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval</td>
    <td>98</td>
    <td><p>Hashing is widely applied to approximate nearest neighbor search for large-scale multimodal retrieval with storage and computation efficiency. Cross-modal hashing improves the quality of hash coding by exploiting semantic correlations across different modalities. Existing cross-modal hashing methods first transform data into low-dimensional feature vectors, and then generate binary codes by another separate quantization step. However, suboptimal hash codes may be generated since the quantization error is not explicitly minimized and the feature representation is not jointly optimized with the binary codes.
This paper presents a Correlation Hashing Network (CHN) approach to cross-modal hashing, which jointly learns good data representation tailored to hash coding and formally controls the quantization error. The proposed CHN is a hybrid deep architecture that constitutes a convolutional neural network for learning good image representations, a multilayer perception for learning good text representations, two hashing layers for generating compact binary codes, and a structured max-margin loss that integrates all things together to enable learning similarity-preserving and high-quality hash codes. Extensive empirical study shows that CHN yields state of the art cross-modal retrieval performance on standard benchmarks.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
        Multimodal Retrieval 
      
        Medical Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/uchida2016local/">Local Feature Detectors, Descriptors, And Image Representations: A Survey</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Local Feature Detectors, Descriptors, And Image Representations: A Survey' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Local Feature Detectors, Descriptors, And Image Representations: A Survey' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Uchida Yusuke</td> <!-- 🔧 You were missing this -->
    <td>Studies in Computational Intelligence</td>
    <td>74</td>
    <td><p>With the advances in both stable interest region detectors and robust and
distinctive descriptors, local feature-based image or object retrieval has
become a popular research topic. %All of the local feature-based image
retrieval system involves two important processes: local feature extraction and
image representation. The other key technology for image retrieval systems is
image representation such as the bag-of-visual words (BoVW), Fisher vector, or
Vector of Locally Aggregated Descriptors (VLAD) framework. In this paper, we
review local features and image representations for image retrieval. Because
many and many methods are proposed in this area, these methods are grouped into
several classes and summarized. In addition, recent deep learning-based
approaches for image retrieval are briefly reviewed.</p>
</td>
    <td>
      
        Survey Paper 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/zhou2016transfer/">Transfer Hashing With Privileged Information</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Transfer Hashing With Privileged Information' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Transfer Hashing With Privileged Information' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhou et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>25</td>
    <td><p>Most existing learning to hash methods assume that there are sufficient data,
either labeled or unlabeled, on the domain of interest (i.e., the target
domain) for training. However, this assumption cannot be satisfied in some
real-world applications. To address this data sparsity issue in hashing,
inspired by transfer learning, we propose a new framework named Transfer
Hashing with Privileged Information (THPI). Specifically, we extend the
standard learning to hash method, Iterative Quantization (ITQ), in a transfer
learning manner, namely ITQ+. In ITQ+, a new slack function is learned from
auxiliary data to approximate the quantization error in ITQ. We developed an
alternating optimization approach to solve the resultant optimization problem
for ITQ+. We further extend ITQ+ to LapITQ+ by utilizing the geometry structure
among the auxiliary data for learning more precise binary codes in the target
domain. Extensive experiments on several benchmark datasets verify the
effectiveness of our proposed approaches through comparisons with several
state-of-the-art baselines.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/mohedano2016bags/">Bags Of Local Convolutional Features For Scalable Instance Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Bags Of Local Convolutional Features For Scalable Instance Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Bags Of Local Convolutional Features For Scalable Instance Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mohedano et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval</td>
    <td>154</td>
    <td><p>This work proposes a simple instance retrieval pipeline based on encoding the
convolutional features of CNN using the bag of words aggregation scheme (BoW).
Assigning each local array of activations in a convolutional layer to a visual
word produces an \textit{assignment map}, a compact representation that relates
regions of an image with a visual word. We use the assignment map for fast
spatial reranking, obtaining object localizations that are used for query
expansion. We demonstrate the suitability of the BoW representation based on
local CNN features for instance retrieval, achieving competitive performance on
the Oxford and Paris buildings benchmarks. We show that our proposed system for
CNN feature aggregation with BoW outperforms state-of-the-art techniques using
sum pooling at a subset of the challenging TRECVid INS benchmark.</p>
</td>
    <td>
      
        Multimodal Retrieval 
      
        Medical Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/bury2016efficient/">Efficient Similarity Search In Dynamic Data Streams</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Similarity Search In Dynamic Data Streams' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient Similarity Search In Dynamic Data Streams' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Bury Marc, Schwiegelshohn Chris, Sorella Mara</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Knowledge and Data Engineering</td>
    <td>8</td>
    <td><p>The Jaccard index is an important similarity measure for item sets and
Boolean data. On large datasets, an exact similarity computation is often
infeasible for all item pairs both due to time and space constraints, giving
rise to faster approximate methods. The algorithm of choice used to quickly
compute the Jaccard index \(\frac{\vert A \cap B \vert}{\vert A\cup B\vert}\) of
two item sets \(A\) and \(B\) is usually a form of min-hashing. Most min-hashing
schemes are maintainable in data streams processing only additions, but none
are known to work when facing item-wise deletions. In this paper, we
investigate scalable approximation algorithms for rational set similarities, a
broad class of similarity measures including Jaccard. Motivated by a result of
Chierichetti and Kumar [J. ACM 2015] who showed any rational set similarity \(S\)
admits a locality sensitive hashing (LSH) scheme if and only if the
corresponding distance \(1-S\) is a metric, we can show that there exists a space
efficient summary maintaining a \((1\pm \epsilon)\) multiplicative
approximation to \(1-S\) in dynamic data streams. This in turn also yields a
\(\epsilon\) additive approximation of the similarity. The existence of these
approximations hints at, but does not directly imply a LSH scheme in dynamic
data streams. Our second and main contribution now lies in the design of such a
LSH scheme maintainable in dynamic data streams. The scheme is space efficient,
easy to implement and to the best of our knowledge the first of its kind able
to process deletions.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/cao2016deep/">Deep Visual-semantic Hashing For Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Visual-semantic Hashing For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Visual-semantic Hashing For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cao et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</td>
    <td>264</td>
    <td><p>Due to the storage and retrieval efficiency, hashing has been
widely applied to approximate nearest neighbor search for
large-scale multimedia retrieval. Cross-modal hashing, which
enables efficient retrieval of images in response to text queries
or vice versa, has received increasing attention recently. Most
existing work on cross-modal hashing does not capture the
spatial dependency of images and temporal dynamics of text
sentences for learning powerful feature representations and
cross-modal embeddings that mitigate the heterogeneity of
different modalities. This paper presents a new Deep Visual Semantic Hashing (DVSH) model that generates compact
hash codes of images and sentences in an end-to-end deep
learning architecture, which capture the intrinsic cross-modal
correspondences between visual data and natural language.
DVSH is a hybrid deep architecture that constitutes a visual semantic fusion network for learning joint embedding space
of images and text sentences, and two modality-specific hashing networks for learning hash functions to generate compact
binary codes. Our architecture effectively unifies joint multimodal embedding and cross-modal hashing, which is based
on a novel combination of Convolutional Neural Networks
over images, Recurrent Neural Networks over sentences, and
a structured max-margin objective that integrates all things
together to enable learning of similarity-preserving and highquality hash codes. Extensive empirical evidence shows that
our DVSH approach yields state of the art results in crossmodal retrieval experiments on image-sentences datasets,
i.e. standard IAPR TC-12 and large-scale Microsoft COCO.</p>
</td>
    <td>
      
        KDD 
      
        Hashing Methods 
      
        Text Retrieval 
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/riazi2016sub/">Sub-linear Privacy-preserving Near-neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Sub-linear Privacy-preserving Near-neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Sub-linear Privacy-preserving Near-neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Riazi et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>11</td>
    <td><p>In Near-Neighbor Search (NNS), a new client queries a database (held by a
server) for the most similar data (near-neighbors) given a certain similarity
metric. The Privacy-Preserving variant (PP-NNS) requires that neither server
nor the client shall learn information about the other party’s data except what
can be inferred from the outcome of NNS. The overwhelming growth in the size of
current datasets and the lack of a truly secure server in the online world
render the existing solutions impractical; either due to their high
computational requirements or non-realistic assumptions which potentially
compromise privacy. PP-NNS having query time {\it sub-linear} in the size of
the database has been suggested as an open research direction by Li et al.
(CCSW’15). In this paper, we provide the first such algorithm, called Secure
Locality Sensitive Indexing (SLSI) which has a sub-linear query time and the
ability to handle honest-but-curious parties. At the heart of our proposal lies
a secure binary embedding scheme generated from a novel probabilistic
transformation over locality sensitive hashing family. We provide information
theoretic bound for the privacy guarantees and support our theoretical claims
using substantial empirical evidence on real-world datasets.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/zhang2016collaborative/">Collaborative Quantization For Cross-modal Similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Collaborative Quantization For Cross-modal Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Collaborative Quantization For Cross-modal Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Ting, Wang Jingdong</td> <!-- 🔧 You were missing this -->
    <td>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>62</td>
    <td><p>Cross-modal similarity search is a problem about designing a search system
supporting querying across content modalities, e.g., using an image to search
for texts or using a text to search for images. This paper presents a compact
coding solution for efficient search, with a focus on the quantization approach
which has already shown the superior performance over the hashing solutions in
the single-modal similarity search. We propose a cross-modal quantization
approach, which is among the early attempts to introduce quantization into
cross-modal search. The major contribution lies in jointly learning the
quantizers for both modalities through aligning the quantized representations
for each pair of image and text belonging to a document. In addition, our
approach simultaneously learns the common space for both modalities in which
quantization is conducted to enable efficient and effective search using the
Euclidean distance computed in the common space with fast distance table
lookup. Experimental results compared with several competitive algorithms over
three benchmark datasets demonstrate that the proposed approach achieves the
state-of-the-art performance.</p>
</td>
    <td>
      
        Quantization 
      
        Similarity Search 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/liu2016query/">Query-adaptive Hash Code Ranking For Large-scale Multi-view Visual Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Query-adaptive Hash Code Ranking For Large-scale Multi-view Visual Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Query-adaptive Hash Code Ranking For Large-scale Multi-view Visual Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>74</td>
    <td><p>Hash based nearest neighbor search has become attractive in many
applications. However, the quantization in hashing usually degenerates the
discriminative power when using Hamming distance ranking. Besides, for
large-scale visual search, existing hashing methods cannot directly support the
efficient search over the data with multiple sources, and while the literature
has shown that adaptively incorporating complementary information from diverse
sources or views can significantly boost the search performance. To address the
problems, this paper proposes a novel and generic approach to building multiple
hash tables with multiple views and generating fine-grained ranking results at
bitwise and tablewise levels. For each hash table, a query-adaptive bitwise
weighting is introduced to alleviate the quantization loss by simultaneously
exploiting the quality of hash functions and their complement for nearest
neighbor search. From the tablewise aspect, multiple hash tables are built for
different data views as a joint index, over which a query-specific rank fusion
is proposed to rerank all results from the bitwise ranking by diffusing in a
graph. Comprehensive experiments on image search over three well-known
benchmarks show that the proposed method achieves up to 17.11% and 20.28%
performance gains on single and multiple table search over state-of-the-art
methods.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/jain2016approximate/">Approximate Search With Quantized Sparse Representations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Approximate Search With Quantized Sparse Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Approximate Search With Quantized Sparse Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jain et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>17</td>
    <td><p>This paper tackles the task of storing a large collection of vectors, such as
visual descriptors, and of searching in it. To this end, we propose to
approximate database vectors by constrained sparse coding, where possible atom
weights are restricted to belong to a finite subset. This formulation
encompasses, as particular cases, previous state-of-the-art methods such as
product or residual quantization. As opposed to traditional sparse coding
methods, quantized sparse coding includes memory usage as a design constraint,
thereby allowing us to index a large collection such as the BIGANN
billion-sized benchmark. Our experiments, carried out on standard benchmarks,
show that our formulation leads to competitive solutions when considering
different trade-offs between learning/coding time, index size and search
quality.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/xia2016unsupervised/">Unsupervised Deep Hashing For Large-scale Visual Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Deep Hashing For Large-scale Visual Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Deep Hashing For Large-scale Visual Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xia et al.</td> <!-- 🔧 You were missing this -->
    <td>2016 Sixth International Conference on Image Processing Theory, Tools and Applications (IPTA)</td>
    <td>17</td>
    <td><p>Learning based hashing plays a pivotal role in large-scale visual search.
However, most existing hashing algorithms tend to learn shallow models that do
not seek representative binary codes. In this paper, we propose a novel hashing
approach based on unsupervised deep learning to hierarchically transform
features into hash codes. Within the heterogeneous deep hashing framework, the
autoencoder layers with specific constraints are considered to model the
nonlinear mapping between features and binary codes. Then, a Restricted
Boltzmann Machine (RBM) layer with constraints is utilized to reduce the
dimension in the hamming space. Extensive experiments on the problem of visual
search demonstrate the competitiveness of our proposed approach compared to
state-of-the-art.</p>
</td>
    <td>
      
        Image Retrieval 
      
        Unsupervised 
      
        Neural Hashing 
      
        SUPERVISED 
      
        Hashing Methods 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/li20162/">2-bit Random Projections, Nonlinear Estimators, And Approximate Near Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=2-bit Random Projections, Nonlinear Estimators, And Approximate Near Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=2-bit Random Projections, Nonlinear Estimators, And Approximate Near Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Ping, Mitzenmacher Michael, Shrivastava Anshumali</td> <!-- 🔧 You were missing this -->
    <td>Neurocomputing</td>
    <td>5</td>
    <td><p>The method of random projections has become a standard tool for machine
learning, data mining, and search with massive data at Web scale. The effective
use of random projections requires efficient coding schemes for quantizing
(real-valued) projected data into integers. In this paper, we focus on a simple
2-bit coding scheme. In particular, we develop accurate nonlinear estimators of
data similarity based on the 2-bit strategy. This work will have important
practical applications. For example, in the task of near neighbor search, a
crucial step (often called re-ranking) is to compute or estimate data
similarities once a set of candidate data points have been identified by hash
table techniques. This re-ranking step can take advantage of the proposed
coding scheme and estimator.
  As a related task, in this paper, we also study a simple uniform quantization
scheme for the purpose of building hash tables with projected data. Our
analysis shows that typically only a small number of bits are needed. For
example, when the target similarity level is high, 2 or 3 bits might be
sufficient. When the target similarity level is not so high, it is preferable
to use only 1 or 2 bits. Therefore, a 2-bit scheme appears to be overall a good
choice for the task of sublinear time approximate near neighbor search via hash
tables.
  Combining these results, we conclude that 2-bit random projections should be
recommended for approximate near neighbor search and similarity estimation.
Extensive experimental results are provided.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/kennedy2016fast/">Fast Cross-polytope Locality-sensitive Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Cross-polytope Locality-sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Cross-polytope Locality-sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kennedy Christopher, Ward Rachel</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>9</td>
    <td><p>We provide a variant of cross-polytope locality sensitive hashing with
respect to angular distance which is provably optimal in asymptotic sensitivity
and enjoys \(\mathcal{O}(d \ln d )\) hash computation time. Building on a recent
result (by Andoni, Indyk, Laarhoven, Razenshteyn, Schmidt, 2015), we show that
optimal asymptotic sensitivity for cross-polytope LSH is retained even when the
dense Gaussian matrix is replaced by a fast Johnson-Lindenstrauss transform
followed by discrete pseudo-rotation, reducing the hash computation time from
\(\mathcal{O}(d^2)\) to \(\mathcal{O}(d \ln d )\). Moreover, our scheme achieves
the optimal rate of convergence for sensitivity. By incorporating a
low-randomness Johnson-Lindenstrauss transform, our scheme can be modified to
require only \(\mathcal{O}(\ln^9(d))\) random bits</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/fredriksson2016geometric/">Geometric Near-neighbor Access Tree (GNAT) Revisited</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Geometric Near-neighbor Access Tree (GNAT) Revisited' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Geometric Near-neighbor Access Tree (GNAT) Revisited' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fredriksson Kimmo</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>6</td>
    <td><p>Geometric Near-neighbor Access Tree (GNAT) is a metric space indexing method
based on hierarchical hyperplane partitioning of the space. While GNAT is very
efficient in proximity searching, it has a bad reputation of being a memory
hog. We show that this is partially based on too coarse analysis, and that the
memory requirements can be lowered while at the same time improving the search
efficiency. We also show how to make GNAT memory adaptive in a smooth way, and
that the hyperplane partitioning can be replaced with ball partitioning, which
can further improve the search performance. We conclude with experimental
results showing the new methods can give significant performance boost.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/sankaranarayanan2016triplet/">Triplet Similarity Embedding For Face Verification</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Triplet Similarity Embedding For Face Verification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Triplet Similarity Embedding For Face Verification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sankaranarayanan Swami, Alavi Azadeh, Chellappa Rama</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>47</td>
    <td><p>In this work, we present an unconstrained face verification algorithm and
evaluate it on the recently released IJB-A dataset that aims to push the
boundaries of face verification methods. The proposed algorithm couples a deep
CNN-based approach with a low-dimensional discriminative embedding learnt using
triplet similarity constraints in a large margin fashion. Aside from yielding
performance improvement, this embedding provides significant advantages in
terms of memory and post-processing operations like hashing and visualization.
Experiments on the IJB-A dataset show that the proposed algorithm outperforms
state of the art methods in verification and identification metrics, while
requiring less training time.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/lin2016cross/">Cross-domain Visual Matching Via Generalized Similarity Measure And Feature Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-domain Visual Matching Via Generalized Similarity Measure And Feature Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-domain Visual Matching Via Generalized Similarity Measure And Feature Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>153</td>
    <td><p>Cross-domain visual data matching is one of the fundamental problems in many
real-world vision tasks, e.g., matching persons across ID photos and
surveillance videos. Conventional approaches to this problem usually involves
two steps: i) projecting samples from different domains into a common space,
and ii) computing (dis-)similarity in this space based on a certain distance.
In this paper, we present a novel pairwise similarity measure that advances
existing models by i) expanding traditional linear projections into affine
transformations and ii) fusing affine Mahalanobis distance and Cosine
similarity by a data-driven combination. Moreover, we unify our similarity
measure with feature representation learning via deep convolutional neural
networks. Specifically, we incorporate the similarity measure matrix into the
deep architecture, enabling an end-to-end way of model optimization. We
extensively evaluate our generalized similarity model in several challenging
cross-domain matching tasks: person re-identification under different views and
face verification over different modalities (i.e., faces from still images and
videos, older and younger faces, and sketch and photo portraits). The
experimental results demonstrate superior performance of our model over other
state-of-the-art methods.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/paulin2016convolutional/">Convolutional Patch Representations For Image Retrieval: An Unsupervised Approach</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Convolutional Patch Representations For Image Retrieval: An Unsupervised Approach' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Convolutional Patch Representations For Image Retrieval: An Unsupervised Approach' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Paulin et al.</td> <!-- 🔧 You were missing this -->
    <td>International Journal of Computer Vision</td>
    <td>62</td>
    <td><p>Convolutional neural networks (CNNs) have recently received a lot of
attention due to their ability to model local stationary structures in natural
images in a multi-scale fashion, when learning all model parameters with
supervision. While excellent performance was achieved for image classification
when large amounts of labeled visual data are available, their success for
un-supervised tasks such as image retrieval has been moderate so far. Our paper
focuses on this latter setting and explores several methods for learning patch
descriptors without supervision with application to matching and instance-level
retrieval. To that effect, we propose a new family of convolutional descriptors
for patch representation , based on the recently introduced convolutional
kernel networks. We show that our descriptor, named Patch-CKN, performs better
than SIFT as well as other convolutional networks learned by artificially
introducing supervision and is significantly faster to train. To demonstrate
its effectiveness, we perform an extensive evaluation on standard benchmarks
for patch and image retrieval where we obtain state-of-the-art results. We also
introduce a new dataset called RomePatches, which allows to simultaneously
study descriptor performance for patch and image retrieval.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Image Retrieval 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/shrivastava2016exact/">Exact Weighted Minwise Hashing In Constant Time</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Exact Weighted Minwise Hashing In Constant Time' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Exact Weighted Minwise Hashing In Constant Time' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shrivastava Anshumali</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>7</td>
    <td><p>Weighted minwise hashing (WMH) is one of the fundamental subroutine, required
by many celebrated approximation algorithms, commonly adopted in industrial
practice for large scale-search and learning. The resource bottleneck of the
algorithms is the computation of multiple (typically a few hundreds to
thousands) independent hashes of the data. The fastest hashing algorithm is by
Ioffe \cite{Proc:Ioffe_ICDM10}, which requires one pass over the entire data
vector, \(O(d)\) (\(d\) is the number of non-zeros), for computing one hash.
However, the requirement of multiple hashes demands hundreds or thousands
passes over the data. This is very costly for modern massive dataset.
  In this work, we break this expensive barrier and show an expected constant
amortized time algorithm which computes \(k\) independent and unbiased WMH in
time \(O(k)\) instead of \(O(dk)\) required by Ioffe’s method. Moreover, our
proposal only needs a few bits (5 - 9 bits) of storage per hash value compared
to around \(64\) bits required by the state-of-art-methodologies. Experimental
evaluations, on real datasets, show that for computing 500 WMH, our proposal
can be 60000x faster than the Ioffe’s method without losing any accuracy. Our
method is also around 100x faster than approximate heuristics capitalizing on
the efficient “densified” one permutation hashing schemes
\cite{Proc:OneHashLSH_ICML14}. Given the simplicity of our approach and its
significant advantages, we hope that it will replace existing implementations
in practice.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/moran2016enhancing/">Enhancing First Story Detection Using Word Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Enhancing First Story Detection Using Word Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Enhancing First Story Detection Using Word Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Moran et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</td>
    <td>29</td>
    <td><p>In this paper we show how word embeddings can be used to increase the effectiveness of a state-of-the art Locality Sensitive Hashing (LSH) based first story detection (FSD) system over a standard tweet corpus. Vocabulary mismatch, in which related tweets use different words, is a serious hindrance to the effectiveness of a modern FSD system. In this case, a tweet could be flagged as a first story even if a related tweet, which uses different but synonymous words, was already returned as a first story. In this work, we propose a novel approach to mitigate this problem of lexical variation, based on tweet expansion. In particular, we propose to expand tweets with semantically related paraphrases identified via automatically mined word embeddings over a background tweet corpus. Through experimentation on a large data stream comprised of 50 million tweets, we show that FSD effectiveness can be improved by 9.5% over a state-of-the-art FSD system.</p>
</td>
    <td>
      
        SIGIR 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/vasile2016meta/">Meta-prod2vec - Product Embeddings Using Side-information For Recommendation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Meta-prod2vec - Product Embeddings Using Side-information For Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Meta-prod2vec - Product Embeddings Using Side-information For Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Vasile Flavian, Smirnova Elena, Conneau Alexis</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>120</td>
    <td><p>We propose Meta-Prod2vec, a novel method to compute item similarities for
recommendation that leverages existing item metadata. Such scenarios are
frequently encountered in applications such as content recommendation, ad
targeting and web search. Our method leverages past user interactions with
items and their attributes to compute low-dimensional embeddings of items.
Specifically, the item metadata is in- jected into the model as side
information to regularize the item embeddings. We show that the new item
representa- tions lead to better performance on recommendation tasks on an open
music dataset.</p>
</td>
    <td>
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/liu2016generating/">Generating Binary Tags For Fast Medical Image Retrieval Based On Convolutional Nets And Radon Transform</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Generating Binary Tags For Fast Medical Image Retrieval Based On Convolutional Nets And Radon Transform' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Generating Binary Tags For Fast Medical Image Retrieval Based On Convolutional Nets And Radon Transform' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Xinran, Tizhoosh Hamid R., Kofman Jonathan</td> <!-- 🔧 You were missing this -->
    <td>2016 International Joint Conference on Neural Networks (IJCNN)</td>
    <td>62</td>
    <td><p>Content-based image retrieval (CBIR) in large medical image archives is a
challenging and necessary task. Generally, different feature extraction methods
are used to assign expressive and invariant features to each image such that
the search for similar images comes down to feature classification and/or
matching. The present work introduces a new image retrieval method for medical
applications that employs a convolutional neural network (CNN) with recently
introduced Radon barcodes. We combine neural codes for global classification
with Radon barcodes for the final retrieval. We also examine image search based
on regions of interest (ROI) matching after image retrieval. The IRMA dataset
with more than 14,000 x-rays images is used to evaluate the performance of our
method. Experimental results show that our approach is superior to many
published works.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/xu2016binary/">Binary Subspace Coding For Query-by-image Video Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Binary Subspace Coding For Query-by-image Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Binary Subspace Coding For Query-by-image Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 25th ACM international conference on Multimedia</td>
    <td>7</td>
    <td><p>The query-by-image video retrieval (QBIVR) task has been attracting
considerable research attention recently. However, most existing methods
represent a video by either aggregating or projecting all its frames into a
single datum point, which may easily cause severe information loss. In this
paper, we propose an efficient QBIVR framework to enable an effective and
efficient video search with image query. We first define a
similarity-preserving distance metric between an image and its orthogonal
projection in the subspace of the video, which can be equivalently transformed
to a Maximum Inner Product Search (MIPS) problem.
  Besides, to boost the efficiency of solving the MIPS problem, we propose two
asymmetric hashing schemes, which bridge the domain gap of images and videos.
The first approach, termed Inner-product Binary Coding (IBC), preserves the
inner relationships of images and videos in a common Hamming space. To further
improve the retrieval efficiency, we devise a Bilinear Binary Coding (BBC)
approach, which employs compact bilinear projections instead of a single large
projection matrix. Extensive experiments have been conducted on four real-world
video datasets to verify the effectiveness of our proposed approaches as
compared to the state-of-the-arts.</p>
</td>
    <td>
      
        Video Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/srivastava20163d/">3D Binary Signatures</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=3D Binary Signatures' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=3D Binary Signatures' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Srivastava Siddharth, Lall Brejesh</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Tenth Indian Conference on Computer Vision, Graphics and Image Processing</td>
    <td>8</td>
    <td><p>In this paper, we propose a novel binary descriptor for 3D point clouds. The
proposed descriptor termed as 3D Binary Signature (3DBS) is motivated from the
matching efficiency of the binary descriptors for 2D images. 3DBS describes
keypoints from point clouds with a binary vector resulting in extremely fast
matching. The method uses keypoints from standard keypoint detectors. The
descriptor is built by constructing a Local Reference Frame and aligning a
local surface patch accordingly. The local surface patch constitutes of
identifying nearest neighbours based upon an angular constraint among them. The
points are ordered with respect to the distance from the keypoints. The normals
of the ordered pairs of these keypoints are projected on the axes and the
relative magnitude is used to assign a binary digit. The vector thus
constituted is used as a signature for representing the keypoints. The matching
is done by using hamming distance. We show that 3DBS outperforms state of the
art descriptors on various evaluation metrics.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/cheng2016adaptive/">Adaptive Training Of Random Mapping For Data Quantization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Adaptive Training Of Random Mapping For Data Quantization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Adaptive Training Of Random Mapping For Data Quantization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cheng Miao, Tsoi Ah Chung</td> <!-- 🔧 You were missing this -->
    <td>2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>36</td>
    <td><p>Data quantization learns encoding results of data with certain requirements,
and provides a broad perspective of many real-world applications to data
handling. Nevertheless, the results of encoder is usually limited to
multivariate inputs with the random mapping, and side information of binary
codes are hardly to mostly depict the original data patterns as possible. In
the literature, cosine based random quantization has attracted much attentions
due to its intrinsic bounded results. Nevertheless, it usually suffers from the
uncertain outputs, and information of original data fails to be fully preserved
in the reduced codes. In this work, a novel binary embedding method, termed
adaptive training quantization (ATQ), is proposed to learn the ideal transform
of random encoder, where the limitation of cosine random mapping is tackled. As
an adaptive learning idea, the reduced mapping is adaptively calculated with
idea of data group, while the bias of random transform is to be improved to
hold most matching information. Experimental results show that the proposed
method is able to obtain outstanding performance compared with other random
quantization methods.</p>
</td>
    <td>
      
        Quantization 
      
        Evaluation 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/pachori2016zero/">Zero Shot Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Zero Shot Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Zero Shot Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Pachori Shubham, Raman Shanmuganathan</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 24th ACM international conference on Multimedia</td>
    <td>138</td>
    <td><p>This paper provides a framework to hash images containing instances of
unknown object classes. In many object recognition problems, we might have
access to huge amount of data. It may so happen that even this huge data
doesn’t cover the objects belonging to classes that we see in our day to day
life. Zero shot learning exploits auxiliary information (also called as
signatures) in order to predict the labels corresponding to unknown classes. In
this work, we attempt to generate the hash codes for images belonging to unseen
classes, information of which is available only through the textual corpus. We
formulate this as an unsupervised hashing formulation as the exact labels are
not available for the instances of unseen classes. We show that the proposed
solution is able to generate hash codes which can predict labels corresponding
to unseen classes with appreciably good precision.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/pagh2016approximate/">Approximate Furthest Neighbor With Application To Annulus Query</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Approximate Furthest Neighbor With Application To Annulus Query' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Approximate Furthest Neighbor With Application To Annulus Query' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Pagh et al.</td> <!-- 🔧 You were missing this -->
    <td>Information Systems</td>
    <td>10</td>
    <td><p>Much recent work has been devoted to approximate nearest neighbor queries.
Motivated by applications in recommender systems, we consider approximate
furthest neighbor (AFN) queries and present a simple, fast, and highly
practical data structure for answering AFN queries in high- dimensional
Euclidean space. The method builds on the technique of In- dyk (SODA 2003),
storing random projections to provide sublinear query time for AFN. However, we
introduce a different query algorithm, improving on Indyk’s approximation
factor and reducing the running time by a logarithmic factor. We also present a
variation based on a query- independent ordering of the database points; while
this does not have the provable approximation factor of the query-dependent
data structure, it offers significant improvement in time and space complexity.
We give a theoretical analysis, and experimental results. As an application,
the query-dependent approach is used for deriving a data structure for the
approximate annulus query problem, which is defined as follows: given an input
set S and two parameters r &gt; 0 and w &gt;= 1, construct a data structure that
returns for each query point q a point p in S such that the distance between p
and q is at least r/w and at most wr.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/otani2016learning/">Learning Joint Representations Of Videos And Sentences With Web Image Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Joint Representations Of Videos And Sentences With Web Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Joint Representations Of Videos And Sentences With Web Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Otani et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>92</td>
    <td><p>Our objective is video retrieval based on natural language queries. In
addition, we consider the analogous problem of retrieving sentences or
generating descriptions given an input video. Recent work has addressed the
problem by embedding visual and textual inputs into a common space where
semantic similarities correlate to distances. We also adopt the embedding
approach, and make the following contributions: First, we utilize web image
search in sentence embedding process to disambiguate fine-grained visual
concepts. Second, we propose embedding models for sentence, image, and video
inputs whose parameters are learned simultaneously. Finally, we show how the
proposed model can be applied to description generation. Overall, we observe a
clear improvement over the state-of-the-art methods in the video and sentence
retrieval tasks. In description generation, the performance level is comparable
to the current state-of-the-art, although our embeddings were trained for the
retrieval tasks.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/pham2016scalability/">Scalability And Total Recall With Fast Coveringlsh</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Scalability And Total Recall With Fast Coveringlsh' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Scalability And Total Recall With Fast Coveringlsh' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Pham Ninh, Pagh Rasmus</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</td>
    <td>11</td>
    <td><p>Locality-sensitive hashing (LSH) has emerged as the dominant algorithmic
technique for similarity search with strong performance guarantees in
high-dimensional spaces. A drawback of traditional LSH schemes is that they may
have <em>false negatives</em>, i.e., the recall is less than 100%. This limits
the applicability of LSH in settings requiring precise performance guarantees.
Building on the recent theoretical “CoveringLSH” construction that eliminates
false negatives, we propose a fast and practical covering LSH scheme for
Hamming space called <em>Fast CoveringLSH (fcLSH)</em>. Inheriting the design
benefits of CoveringLSH our method avoids false negatives and always reports
all near neighbors. Compared to CoveringLSH we achieve an asymptotic
improvement to the hash function computation time from \(\mathcal{O}(dL)\) to
\(\mathcal{O}(d + Llog{L})\), where \(d\) is the dimensionality of data and \(L\) is
the number of hash tables. Our experiments on synthetic and real-world data
sets demonstrate that <em>fcLSH</em> is comparable (and often superior) to
traditional hashing-based approaches for search radius up to 20 in
high-dimensional Hamming space.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Evaluation 
      
        CIKM 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/pacuk2016locality/">Locality-sensitive Hashing Without False Negatives For L_p</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Locality-sensitive Hashing Without False Negatives For L_p' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Locality-sensitive Hashing Without False Negatives For L_p' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Pacuk et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>7</td>
    <td><p>In this paper, we show a construction of locality-sensitive hash functions
without false negatives, i.e., which ensure collision for every pair of points
within a given radius \(R\) in \(d\) dimensional space equipped with \(l_p\) norm
when \(p \in [1,\infty]\). Furthermore, we show how to use these hash functions
to solve the \(c\)-approximate nearest neighbor search problem without false
negatives. Namely, if there is a point at distance \(R\), we will certainly
report it and points at distance greater than \(cR\) will not be reported for
\(c=Ω(\sqrt{d},d^{1-\frac{1}{p}})\). The constructed algorithms work: - with
preprocessing time \(\mathcal{O}(n log(n))\) and sublinear expected query time,</p>
<ul>
  <li>with preprocessing time \(\mathcal{O}(\mathrm{poly}(n))\) and expected query
time \(\mathcal{O}(log(n))\). Our paper reports progress on answering the open
problem presented by Pagh [8] who considered the nearest neighbor search
without false negatives for the Hamming distance.</li>
</ul>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/kang2016column/">Column Sampling Based Discrete Supervised Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Column Sampling Based Discrete Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Column Sampling Based Discrete Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kang Wang-cheng, Li, Zhou</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>299</td>
    <td><p>By leveraging semantic (label) information, supervised hashing has demonstrated better accuracy than unsupervised hashing in many real applications. Because the hashing-code learning problem is essentially a discrete optimization problem which is hard to solve, most existing supervised hashing methods try to solve a relaxed continuous optimization problem by dropping the discrete constraints.
However, these methods typically suffer from poor performance due to the errors caused by the relaxation. Some other methods try to directly solve the discrete optimization problem. However, they are typically time-consuming and unscalable. In this paper, we propose a novel method, called column sampling based discrete supervised hashing (COSDISH), to directly learn the discrete hashing code from semantic information.
COSDISH is an iterative method, in each iteration of which several columns are sampled from the semantic similarity matrix and then the hashing code is decomposed into two parts which can be alternately optimized in a discrete way. Theoretical analysis shows that the learning (optimization) algorithm of COSDISH has a constant-approximation bound in each step of the alternating optimization procedure. Empirical results on datasets with semantic labels illustrate that COSDISH can outperform the state-of-the-art methods in real applications like image retrieval.</p>
</td>
    <td>
      
        Unsupervised 
      
        Neural Hashing 
      
        AAAI 
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/wang2016comprehensive/">A Comprehensive Survey On Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Comprehensive Survey On Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Comprehensive Survey On Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>230</td>
    <td><p>In recent years, cross-modal retrieval has drawn much attention due to the
rapid growth of multimodal data. It takes one type of data as the query to
retrieve relevant data of another type. For example, a user can use a text to
retrieve relevant pictures or videos. Since the query and its retrieved results
can be of different modalities, how to measure the content similarity between
different modalities of data remains a challenge. Various methods have been
proposed to deal with such a problem. In this paper, we first review a number
of representative methods for cross-modal retrieval and classify them into two
main groups: 1) real-valued representation learning, and 2) binary
representation learning. Real-valued representation learning methods aim to
learn real-valued common representations for different modalities of data. To
speed up the cross-modal retrieval, a number of binary representation learning
methods are proposed to map different modalities of data into a common Hamming
space. Then, we introduce several multimodal datasets in the community, and
show the experimental results on two commonly used multimodal datasets. The
comparison reveals the characteristic of different kinds of cross-modal
retrieval methods, which is expected to benefit both practical applications and
future research. Finally, we discuss open problems and future research
directions.</p>
</td>
    <td>
      
        Survey Paper 
      
        Multimodal Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/radenovi%C4%872016cnn/">CNN Image Retrieval Learns From Bow: Unsupervised Fine-tuning With Hard Examples</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=CNN Image Retrieval Learns From Bow: Unsupervised Fine-tuning With Hard Examples' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=CNN Image Retrieval Learns From Bow: Unsupervised Fine-tuning With Hard Examples' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Radenović Filip, Tolias Giorgos, Chum Ondřej</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>523</td>
    <td><p>Convolutional Neural Networks (CNNs) achieve state-of-the-art performance in
many computer vision tasks. However, this achievement is preceded by extreme
manual annotation in order to perform either training from scratch or
fine-tuning for the target task. In this work, we propose to fine-tune CNN for
image retrieval from a large collection of unordered images in a fully
automated manner. We employ state-of-the-art retrieval and
Structure-from-Motion (SfM) methods to obtain 3D models, which are used to
guide the selection of the training data for CNN fine-tuning. We show that both
hard positive and hard negative examples enhance the final performance in
particular object retrieval with compact codes.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Image Retrieval 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/zhu2016deep/">Deep Hashing Network For Efficient Similarity Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Hashing Network For Efficient Similarity Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Hashing Network For Efficient Similarity Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>636</td>
    <td><p>Due to the storage and retrieval efficiency, hashing has been widely deployed to approximate nearest neighbor search for large-scale multimedia retrieval. Supervised hashing, which improves the quality of hash coding by exploiting the semantic similarity on data pairs, has received increasing attention recently. For most existing supervised hashing methods for image retrieval, an image is first represented as a vector of hand-crafted or machine-learned features, followed by another separate quantization step that generates binary codes.
However, suboptimal hash coding may be produced, because the quantization error is not statistically minimized and the feature representation is not optimally compatible with the binary coding. In this paper, we propose a novel Deep Hashing Network (DHN) architecture for supervised hashing, in which we jointly learn good image representation tailored to hash coding and formally control the quantization error.
The DHN model constitutes four key components: (1) a sub-network with multiple convolution-pooling layers to capture image representations; (2) a fully-connected hashing layer to generate compact binary hash codes; (3) a pairwise cross-entropy loss layer for similarity-preserving learning; and (4) a pairwise quantization loss for controlling hashing quality. Extensive experiments on standard image retrieval datasets show the proposed DHN model yields substantial boosts over latest state-of-the-art hashing methods.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        AAAI 
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/joulin2016fasttext/">Fasttext.zip: Compressing Text Classification Models</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fasttext.zip: Compressing Text Classification Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fasttext.zip: Compressing Text Classification Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Joulin et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>872</td>
    <td><p>We consider the problem of producing compact architectures for text
classification, such that the full model fits in a limited amount of memory.
After considering different solutions inspired by the hashing literature, we
propose a method built upon product quantization to store word embeddings.
While the original technique leads to a loss in accuracy, we adapt this method
to circumvent quantization artefacts. Our experiments carried out on several
benchmarks show that our approach typically requires two orders of magnitude
less memory than fastText while being only slightly inferior with respect to
accuracy. As a result, it outperforms the state of the art by a good margin in
terms of the compromise between memory usage and accuracy.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/liu2016generalized/">Generalized Residual Vector Quantization For Large Scale Data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Generalized Residual Vector Quantization For Large Scale Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Generalized Residual Vector Quantization For Large Scale Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Shicong, Shao Junru, Lu Hongtao</td> <!-- 🔧 You were missing this -->
    <td>2016 IEEE International Conference on Multimedia and Expo (ICME)</td>
    <td>5</td>
    <td><p>Vector quantization is an essential tool for tasks involving large scale
data, for example, large scale similarity search, which is crucial for
content-based information retrieval and analysis. In this paper, we propose a
novel vector quantization framework that iteratively minimizes quantization
error. First, we provide a detailed review on a relevant vector quantization
method named \textit{residual vector quantization} (RVQ). Next, we propose
\textit{generalized residual vector quantization} (GRVQ) to further improve
over RVQ. Many vector quantization methods can be viewed as the special cases
of our proposed framework. We evaluate GRVQ on several large scale benchmark
datasets for large scale search, classification and object retrieval. We
compared GRVQ with existing methods in detail. Extensive experiments
demonstrate our GRVQ framework substantially outperforms existing methods in
term of quantization accuracy and computation efficiency.</p>
</td>
    <td>
      
        Quantization 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/wang2016affinity/">Affinity Preserving Quantization For Hashing: A Vector Quantization Approach To Learning Compact Binary Codes</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Affinity Preserving Quantization For Hashing: A Vector Quantization Approach To Learning Compact Binary Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Affinity Preserving Quantization For Hashing: A Vector Quantization Approach To Learning Compact Binary Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>11</td>
    <td><p>Hashing techniques are powerful for approximate nearest
neighbour (ANN) search. Existing quantization methods in
hashing are all focused on scalar quantization (SQ) which
is inferior in utilizing the inherent data distribution. In this
paper, we propose a novel vector quantization (VQ) method
named affinity preserving quantization (APQ) to improve the
quantization quality of projection values, which has significantly
boosted the performance of state-of-the-art hashing
techniques. In particular, our method incorporates the neighbourhood
structure in the pre- and post-projection data space
into vector quantization. APQ minimizes the quantization errors
of projection values as well as the loss of affinity property
of original space. An effective algorithm has been proposed
to solve the joint optimization problem in APQ, and
the extension to larger binary codes has been resolved by applying
product quantization to APQ. Extensive experiments
have shown that APQ consistently outperforms the state-of-the-art
quantization methods, and has significantly improved
the performance of various hashing techniques.</p>
</td>
    <td>
      
        Compact Codes 
      
        Quantization 
      
        Hashing Methods 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/yang2016zero/">Zero-shot Hashing Via Transferring Supervised Knowledge</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Zero-shot Hashing Via Transferring Supervised Knowledge' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Zero-shot Hashing Via Transferring Supervised Knowledge' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 24th ACM international conference on Multimedia</td>
    <td>138</td>
    <td><p>Hashing has shown its efficiency and effectiveness in facilitating
large-scale multimedia applications. Supervised knowledge e.g. semantic labels
or pair-wise relationship) associated to data is capable of significantly
improving the quality of hash codes and hash functions. However, confronted
with the rapid growth of newly-emerging concepts and multimedia data on the
Web, existing supervised hashing approaches may easily suffer from the scarcity
and validity of supervised information due to the expensive cost of manual
labelling. In this paper, we propose a novel hashing scheme, termed
<em>zero-shot hashing</em> (ZSH), which compresses images of “unseen” categories
to binary codes with hash functions learned from limited training data of
“seen” categories. Specifically, we project independent data labels i.e.
0/1-form label vectors) into semantic embedding space, where semantic
relationships among all the labels can be precisely characterized and thus seen
supervised knowledge can be transferred to unseen classes. Moreover, in order
to cope with the semantic shift problem, we rotate the embedded space to more
suitably align the embedded semantics with the low-level visual feature space,
thereby alleviating the influence of semantic gap. In the meantime, to exert
positive effects on learning high-quality hash functions, we further propose to
preserve local structural property and discrete nature in binary codes.
Besides, we develop an efficient alternating algorithm to solve the ZSH model.
Extensive experiments conducted on various real-life datasets show the superior
zero-shot image retrieval performance of ZSH as compared to several
state-of-the-art hashing methods.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
        Few Shot & Zero Shot 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/roy2016representing/">Representing Documents And Queries As Sets Of Word Embedded Vectors For Information Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Representing Documents And Queries As Sets Of Word Embedded Vectors For Information Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Representing Documents And Queries As Sets Of Word Embedded Vectors For Information Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Roy et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>11</td>
    <td><p>A major difficulty in applying word vector embeddings in IR is in devising an
effective and efficient strategy for obtaining representations of compound
units of text, such as whole documents, (in comparison to the atomic words),
for the purpose of indexing and scoring documents. Instead of striving for a
suitable method for obtaining a single vector representation of a large
document of text, we rather aim for developing a similarity metric that makes
use of the similarities between the individual embedded word vectors in a
document and a query. More specifically, we represent a document and a query as
sets of word vectors, and use a standard notion of similarity measure between
these sets, computed as a function of the similarities between each constituent
word pair from these sets. We then make use of this similarity measure in
combination with standard IR based similarities for document ranking. The
results of our initial experimental investigations shows that our proposed
method improves MAP by up to \(5.77%\), in comparison to standard text-based
language model similarity, on the TREC ad-hoc dataset.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/nouredanesh2016gabor/">Gabor Barcodes For Medical Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Gabor Barcodes For Medical Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Gabor Barcodes For Medical Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Nouredanesh Mina, Tizhoosh Hamid R., Banijamali Ershad</td> <!-- 🔧 You were missing this -->
    <td>2016 IEEE International Conference on Image Processing (ICIP)</td>
    <td>9</td>
    <td><p>In recent years, advances in medical imaging have led to the emergence of
massive databases, containing images from a diverse range of modalities. This
has significantly heightened the need for automated annotation of the images on
one side, and fast and memory-efficient content-based image retrieval systems
on the other side. Binary descriptors have recently gained more attention as a
potential vehicle to achieve these goals. One of the recently introduced binary
descriptors for tagging of medical images are Radon barcodes (RBCs) that are
driven from Radon transform via local thresholding. Gabor transform is also a
powerful transform to extract texture-based information. Gabor features have
exhibited robustness against rotation, scale, and also photometric
disturbances, such as illumination changes and image noise in many
applications. This paper introduces Gabor Barcodes (GBCs), as a novel framework
for the image annotation. To find the most discriminative GBC for a given query
image, the effects of employing Gabor filters with different parameters, i.e.,
different sets of scales and orientations, are investigated, resulting in
different barcode lengths and retrieval performances. The proposed method has
been evaluated on the IRMA dataset with 193 classes comprising of 12,677 x-ray
images for indexing, and 1,733 x-rays images for testing. A total error score
as low as \(351\) (\(\approx 80%\) accuracy for the first hit) was achieved.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/zhuang2016fast/">Fast Training Of Triplet-based Deep Binary Embedding Networks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Training Of Triplet-based Deep Binary Embedding Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Training Of Triplet-based Deep Binary Embedding Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhuang et al.</td> <!-- 🔧 You were missing this -->
    <td>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>136</td>
    <td><p>In this paper, we aim to learn a mapping (or embedding) from images to a
compact binary space in which Hamming distances correspond to a ranking measure
for the image retrieval task.
  We make use of a triplet loss because this has been shown to be most
effective for ranking problems.
  However, training in previous works can be prohibitively expensive due to the
fact that optimization is directly performed on the triplet space, where the
number of possible triplets for training is cubic in the number of training
examples.
  To address this issue, we propose to formulate high-order binary codes
learning as a multi-label classification problem by explicitly separating
learning into two interleaved stages.
  To solve the first stage, we design a large-scale high-order binary codes
inference algorithm to reduce the high-order objective to a standard binary
quadratic problem such that graph cuts can be used to efficiently infer the
binary code which serve as the label of each training datum.
  In the second stage we propose to map the original image to compact binary
codes via carefully designed deep convolutional neural networks (CNNs) and the
hashing function fitting can be solved by training binary CNN classifiers.
  An incremental/interleaved optimization strategy is proffered to ensure that
these two steps are interactive with each other during training for better
accuracy.
  We conduct experiments on several benchmark datasets, which demonstrate both
improved training time (by as much as two orders of magnitude) as well as
producing state-of-the-art hashing for various retrieval tasks.</p>
</td>
    <td>
      
        Hashing Methods 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/zhou2016generic/">A Generic Inverted Index Framework For Similarity Search On The GPU - Technical Report</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Generic Inverted Index Framework For Similarity Search On The GPU - Technical Report' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Generic Inverted Index Framework For Similarity Search On The GPU - Technical Report' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhou et al.</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE 34th International Conference on Data Engineering (ICDE)</td>
    <td>11</td>
    <td><p>We propose a novel generic inverted index framework on the GPU (called
GENIE), aiming to reduce the programming complexity of the GPU for parallel
similarity search of different data types. Not every data type and similarity
measure are supported by GENIE, but many popular ones are. We present the
system design of GENIE, and demonstrate similarity search with GENIE on several
data types along with a theoretical analysis of search results. A new concept
of locality sensitive hashing (LSH) named \(\tau\)-ANN search, and a novel data
structure c-PQ on the GPU are also proposed for achieving this purpose.
Extensive experiments on different real-life datasets demonstrate the
efficiency and effectiveness of our framework. The implemented system has been
released as open source.</p>
</td>
    <td>
      
        Similarity Search 
      
        Tools & Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/yu2016variable/">Variable-length Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Variable-length Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Variable-length Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yu et al.</td> <!-- 🔧 You were missing this -->
    <td>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>29</td>
    <td><p>Hashing has emerged as a popular technique for large-scale similarity search.
Most learning-based hashing methods generate compact yet correlated hash codes.
However, this redundancy is storage-inefficient. Hence we propose a lossless
variable-length hashing (VLH) method that is both storage- and
search-efficient. Storage efficiency is achieved by converting the fixed-length
hash code into a variable-length code. Search efficiency is obtained by using a
multiple hash table structure. With VLH, we are able to deliberately add
redundancy into hash codes to improve retrieval performance with little
sacrifice in storage efficiency or search complexity. In particular, we propose
a block K-means hashing (B-KMH) method to obtain significantly improved
retrieval performance with no increase in storage and marginal increase in
computational cost.</p>
</td>
    <td>
      
        Hashing Methods 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/lindgren2016leveraging/">Leveraging Sparsity For Efficient Submodular Data Summarization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Leveraging Sparsity For Efficient Submodular Data Summarization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Leveraging Sparsity For Efficient Submodular Data Summarization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lindgren Erik M., Wu Shanshan, Dimakis Alexandros G.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>12</td>
    <td><p>The facility location problem is widely used for summarizing large datasets
and has additional applications in sensor placement, image retrieval, and
clustering. One difficulty of this problem is that submodular optimization
algorithms require the calculation of pairwise benefits for all items in the
dataset. This is infeasible for large problems, so recent work proposed to only
calculate nearest neighbor benefits. One limitation is that several strong
assumptions were invoked to obtain provable approximation guarantees. In this
paper we establish that these extra assumptions are not necessary—solving the
sparsified problem will be almost optimal under the standard assumptions of the
problem. We then analyze a different method of sparsification that is a better
model for methods such as Locality Sensitive Hashing to accelerate the nearest
neighbor computations and extend the use of the problem to a broader family of
similarities. We validate our approach by demonstrating that it rapidly
generates interpretable summaries.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/conjeti2016deep/">Deep Residual Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Residual Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Residual Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Conjeti et al.</td> <!-- 🔧 You were missing this -->
    <td>IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences</td>
    <td>8</td>
    <td><p>Hashing aims at generating highly compact similarity preserving code words
which are well suited for large-scale image retrieval tasks.
  Most existing hashing methods first encode the images as a vector of
hand-crafted features followed by a separate binarization step to generate hash
codes. This two-stage process may produce sub-optimal encoding. In this paper,
for the first time, we propose a deep architecture for supervised hashing
through residual learning, termed Deep Residual Hashing (DRH), for an
end-to-end simultaneous representation learning and hash coding. The DRH model
constitutes four key elements: (1) a sub-network with multiple stacked residual
blocks; (2) hashing layer for binarization; (3) supervised retrieval loss
function based on neighbourhood component analysis for similarity preserving
embedding; and (4) hashing related losses and regularisation to control the
quantization error and improve the quality of hash coding. We present results
of extensive experiments on a large public chest x-ray image database with
co-morbidities and discuss the outcome showing substantial improvements over
the latest state-of-the art methods.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/koutaki2016fast/">Fast Supervised Discrete Hashing And Its Analysis</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Supervised Discrete Hashing And Its Analysis' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Supervised Discrete Hashing And Its Analysis' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Koutaki Gou, Shirai Keiichiro, Ambai Mitsuru</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>7</td>
    <td><p>In this paper, we propose a learning-based supervised discrete hashing
method. Binary hashing is widely used for large-scale image retrieval as well
as video and document searches because the compact representation of binary
code is essential for data storage and reasonable for query searches using
bit-operations. The recently proposed Supervised Discrete Hashing (SDH)
efficiently solves mixed-integer programming problems by alternating
optimization and the Discrete Cyclic Coordinate descent (DCC) method. We show
that the SDH model can be simplified without performance degradation based on
some preliminary experiments; we call the approximate model for this the “Fast
SDH” (FSDH) model. We analyze the FSDH model and provide a mathematically exact
solution for it. In contrast to SDH, our model does not require an alternating
optimization algorithm and does not depend on initial values. FSDH is also
easier to implement than Iterative Quantization (ITQ). Experimental results
involving a large-scale database showed that FSDH outperforms conventional SDH
in terms of precision, recall, and computation time.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/ning2016scalable/">Scalable Image Retrieval By Sparse Product Quantization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Scalable Image Retrieval By Sparse Product Quantization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Scalable Image Retrieval By Sparse Product Quantization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ning et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>40</td>
    <td><p>Fast Approximate Nearest Neighbor (ANN) search technique for high-dimensional
feature indexing and retrieval is the crux of large-scale image retrieval. A
recent promising technique is Product Quantization, which attempts to index
high-dimensional image features by decomposing the feature space into a
Cartesian product of low dimensional subspaces and quantizing each of them
separately. Despite the promising results reported, their quantization approach
follows the typical hard assignment of traditional quantization methods, which
may result in large quantization errors and thus inferior search performance.
Unlike the existing approaches, in this paper, we propose a novel approach
called Sparse Product Quantization (SPQ) to encoding the high-dimensional
feature vectors into sparse representation. We optimize the sparse
representations of the feature vectors by minimizing their quantization errors,
making the resulting representation is essentially close to the original data
in practice. Experiments show that the proposed SPQ technique is not only able
to compress data, but also an effective encoding technique. We obtain
state-of-the-art results for ANN search on four public image datasets and the
promising results of content-based image retrieval further validate the
efficacy of our proposed method.</p>
</td>
    <td>
      
        Quantization 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/lai2016instance/">Instance-aware Hashing For Multi-label Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Instance-aware Hashing For Multi-label Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Instance-aware Hashing For Multi-label Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lai et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>78</td>
    <td><p>Similarity-preserving hashing is a commonly used method for nearest neighbour
search in large-scale image retrieval. For image retrieval, deep-networks-based
hashing methods are appealing since they can simultaneously learn effective
image representations and compact hash codes. This paper focuses on
deep-networks-based hashing for multi-label images, each of which may contain
objects of multiple categories. In most existing hashing methods, each image is
represented by one piece of hash code, which is referred to as semantic
hashing. This setting may be suboptimal for multi-label image retrieval. To
solve this problem, we propose a deep architecture that learns
\textbf{instance-aware} image representations for multi-label image data, which
are organized in multiple groups, with each group containing the features for
one category. The instance-aware representations not only bring advantages to
semantic hashing, but also can be used in category-aware hashing, in which an
image is represented by multiple pieces of hash codes and each piece of code
corresponds to a category. Extensive evaluations conducted on several benchmark
datasets demonstrate that, for both semantic hashing and category-aware
hashing, the proposed method shows substantial improvement over the
state-of-the-art supervised and unsupervised hashing methods.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/fu2016efanna/">EFANNA : An Extremely Fast Approximate Nearest Neighbor Search Algorithm Based On Knn Graph</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=EFANNA : An Extremely Fast Approximate Nearest Neighbor Search Algorithm Based On Knn Graph' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=EFANNA : An Extremely Fast Approximate Nearest Neighbor Search Algorithm Based On Knn Graph' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fu Cong, Cai Deng</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>65</td>
    <td><p>Approximate nearest neighbor (ANN) search is a fundamental problem in many
areas of data mining, machine learning and computer vision. The performance of
traditional hierarchical structure (tree) based methods decreases as the
dimensionality of data grows, while hashing based methods usually lack
efficiency in practice. Recently, the graph based methods have drawn
considerable attention. The main idea is that <em>a neighbor of a neighbor is
also likely to be a neighbor</em>, which we refer as <em>NN-expansion</em>. These
methods construct a \(k\)-nearest neighbor (\(k\)NN) graph offline. And at online
search stage, these methods find candidate neighbors of a query point in some
way (\eg, random selection), and then check the neighbors of these candidate
neighbors for closer ones iteratively. Despite some promising results, there
are mainly two problems with these approaches: 1) These approaches tend to
converge to local optima. 2) Constructing a \(k\)NN graph is time consuming. We
find that these two problems can be nicely solved when we provide a good
initialization for NN-expansion. In this paper, we propose EFANNA, an extremely
fast approximate nearest neighbor search algorithm based on \(k\)NN Graph. Efanna
nicely combines the advantages of hierarchical structure based methods and
nearest-neighbor-graph based methods. Extensive experiments have shown that
EFANNA outperforms the state-of-art algorithms both on approximate nearest
neighbor search and approximate nearest neighbor graph construction. To the
best of our knowledge, EFANNA is the fastest algorithm so far both on
approximate nearest neighbor graph construction and approximate nearest
neighbor search. A library EFANNA based on this research is released on Github.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/fu2016auto/">Auto-jacobin: Auto-encoder Jacobian Binary Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Auto-jacobin: Auto-encoder Jacobian Binary Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Auto-jacobin: Auto-encoder Jacobian Binary Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fu et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>220</td>
    <td><p>Binary codes can be used to speed up nearest neighbor search tasks in large
scale data sets as they are efficient for both storage and retrieval. In this
paper, we propose a robust auto-encoder model that preserves the geometric
relationships of high-dimensional data sets in Hamming space. This is done by
considering a noise-removing function in a region surrounding the manifold
where the training data points lie. This function is defined with the property
that it projects the data points near the manifold into the manifold wisely,
and we approximate this function by its first order approximation. Experimental
results show that the proposed method achieves better than state-of-the-art
results on three large scale high dimensional data sets.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/zhu2016radon/">Radon Features And Barcodes For Medical Image Retrieval Via SVM</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Radon Features And Barcodes For Medical Image Retrieval Via SVM' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Radon Features And Barcodes For Medical Image Retrieval Via SVM' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhu Shujin, Tizhoosh H. R.</td> <!-- 🔧 You were missing this -->
    <td>2016 International Joint Conference on Neural Networks (IJCNN)</td>
    <td>12</td>
    <td><p>For more than two decades, research has been performed on content-based image
retrieval (CBIR). By combining Radon projections and the support vector
machines (SVM), a content-based medical image retrieval method is presented in
this work. The proposed approach employs the normalized Radon projections with
corresponding image category labels to build an SVM classifier, and the Radon
barcode database which encodes every image in a binary format is also generated
simultaneously to tag all images. To retrieve similar images when a query image
is given, Radon projections and the barcode of the query image are generated.
Subsequently, the k-nearest neighbor search method is applied to find the
images with minimum Hamming distance of the Radon barcode within the same class
predicted by the trained SVM classifier that uses Radon features. The
performance of the proposed method is validated by using the IRMA 2009 dataset
with 14,410 x-ray images in 57 categories. The results demonstrate that our
method has the capacity to retrieve similar responses for the correctly
identified query image and even for those mistakenly classified by SVM. The
approach further is very fast and has low memory requirement.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/curtin2016fast/">Fast Approximate Furthest Neighbors With Data-dependent Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Approximate Furthest Neighbors With Data-dependent Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Approximate Furthest Neighbors With Data-dependent Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Curtin Ryan R., Gardner Andrew B.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>9</td>
    <td><p>We present a novel hashing strategy for approximate furthest neighbor search
that selects projection bases using the data distribution. This strategy leads
to an algorithm, which we call DrusillaHash, that is able to outperform
existing approximate furthest neighbor strategies. Our strategy is motivated by
an empirical study of the behavior of the furthest neighbor search problem,
which lends intuition for where our algorithm is most useful. We also present a
variant of the algorithm that gives an absolute approximation guarantee; to our
knowledge, this is the first such approximate furthest neighbor hashing
approach to give such a guarantee. Performance studies indicate that
DrusillaHash can achieve comparable levels of approximation to other algorithms
while giving up to an order of magnitude speedup. An implementation is
available in the mlpack machine learning library (found at
http://www.mlpack.org).</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/tizhoosh2016barcodes/">Barcodes For Medical Image Retrieval Using Autoencoded Radon Transform</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Barcodes For Medical Image Retrieval Using Autoencoded Radon Transform' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Barcodes For Medical Image Retrieval Using Autoencoded Radon Transform' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tizhoosh et al.</td> <!-- 🔧 You were missing this -->
    <td>2016 23rd International Conference on Pattern Recognition (ICPR)</td>
    <td>16</td>
    <td><p>Using content-based binary codes to tag digital images has emerged as a
promising retrieval technology. Recently, Radon barcodes (RBCs) have been
introduced as a new binary descriptor for image search. RBCs are generated by
binarization of Radon projections and by assembling them into a vector, namely
the barcode. A simple local thresholding has been suggested for binarization.
In this paper, we put forward the idea of “autoencoded Radon barcodes”. Using
images in a training dataset, we autoencode Radon projections to perform
binarization on outputs of hidden layers. We employed the mini-batch stochastic
gradient descent approach for the training. Each hidden layer of the
autoencoder can produce a barcode using a threshold determined based on the
range of the logistic function used. The compressing capability of autoencoders
apparently reduces the redundancies inherent in Radon projections leading to
more accurate retrieval results. The IRMA dataset with 14,410 x-ray images is
used to validate the performance of the proposed method. The experimental
results, containing comparison with RBCs, SURF and BRISK, show that autoencoded
Radon barcode (ARBC) has the capacity to capture important information and to
learn richer representations resulting in lower retrieval errors for image
retrieval measured with the accuracy of the first hit only.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/mor%C3%A8re2016group/">Group Invariant Deep Representations For Image Instance Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Group Invariant Deep Representations For Image Instance Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Group Invariant Deep Representations For Image Instance Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Morère et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>6</td>
    <td><p>Most image instance retrieval pipelines are based on comparison of vectors
known as global image descriptors between a query image and the database
images. Due to their success in large scale image classification,
representations extracted from Convolutional Neural Networks (CNN) are quickly
gaining ground on Fisher Vectors (FVs) as state-of-the-art global descriptors
for image instance retrieval. While CNN-based descriptors are generally
remarked for good retrieval performance at lower bitrates, they nevertheless
present a number of drawbacks including the lack of robustness to common object
transformations such as rotations compared with their interest point based FV
counterparts.
  In this paper, we propose a method for computing invariant global descriptors
from CNNs. Our method implements a recently proposed mathematical theory for
invariance in a sensory cortex modeled as a feedforward neural network. The
resulting global descriptors can be made invariant to multiple arbitrary
transformation groups while retaining good discriminativeness.
  Based on a thorough empirical evaluation using several publicly available
datasets, we show that our method is able to significantly and consistently
improve retrieval results every time a new type of invariance is incorporated.
We also show that our method which has few parameters is not prone to
overfitting: improvements generalize well across datasets with different
properties with regard to invariances. Finally, we show that our descriptors
are able to compare favourably to other state-of-the-art compact descriptors in
similar bitranges, exceeding the highest retrieval results reported in the
literature on some datasets. A dedicated dimensionality reduction step
–quantization or hashing– may be able to further improve the competitiveness
of the descriptors.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/do2016learning/">Learning To Hash With Binary Deep Neural Network</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning To Hash With Binary Deep Neural Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning To Hash With Binary Deep Neural Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Do Thanh-toan, Doan Anh-dzung, Cheung Ngai-man</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>168</td>
    <td><p>This work proposes deep network models and learning algorithms for
unsupervised and supervised binary hashing. Our novel network design constrains
one hidden layer to directly output the binary codes. This addresses a
challenging issue in some previous works: optimizing non-smooth objective
functions due to binarization. Moreover, we incorporate independence and
balance properties in the direct and strict forms in the learning. Furthermore,
we include similarity preserving property in our objective function. Our
resulting optimization with these binary, independence, and balance constraints
is difficult to solve. We propose to attack it with alternating optimization
and careful relaxation. Experimental results on three benchmark datasets show
that our proposed methods compare favorably with the state of the art.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/zheng2016discriminatively/">A Discriminatively Learned CNN Embedding For Person Re-identification</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Discriminatively Learned CNN Embedding For Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Discriminatively Learned CNN Embedding For Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zheng Zhedong, Zheng Liang, Yang Yi</td> <!-- 🔧 You were missing this -->
    <td>ACM Transactions on Multimedia Computing, Communications, and Applications</td>
    <td>486</td>
    <td><p>We revisit two popular convolutional neural networks (CNN) in person
re-identification (re-ID), i.e, verification and classification models. The two
models have their respective advantages and limitations due to different loss
functions. In this paper, we shed light on how to combine the two models to
learn more discriminative pedestrian descriptors. Specifically, we propose a
new siamese network that simultaneously computes identification loss and
verification loss. Given a pair of training images, the network predicts the
identities of the two images and whether they belong to the same identity. Our
network learns a discriminative embedding and a similarity measurement at the
same time, thus making full usage of the annotations. Albeit simple, the
learned embedding improves the state-of-the-art performance on two public
person re-ID benchmarks. Further, we show our architecture can also be applied
in image retrieval.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/song2016deep/">Deep Metric Learning Via Lifted Structured Feature Embedding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Metric Learning Via Lifted Structured Feature Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Metric Learning Via Lifted Structured Feature Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Song et al.</td> <!-- 🔧 You were missing this -->
    <td>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>1648</td>
    <td><p>Learning the distance metric between pairs of examples is of great importance
for learning and visual recognition. With the remarkable success from the state
of the art convolutional neural networks, recent works have shown promising
results on discriminatively training the networks to learn semantic feature
embeddings where similar examples are mapped close to each other and dissimilar
examples are mapped farther apart. In this paper, we describe an algorithm for
taking full advantage of the training batches in the neural network training by
lifting the vector of pairwise distances within the batch to the matrix of
pairwise distances. This step enables the algorithm to learn the state of the
art feature embedding by optimizing a novel structured prediction objective on
the lifted problem. Additionally, we collected Online Products dataset: 120k
images of 23k classes of online products for metric learning. Our experiments
on the CUB-200-2011, CARS196, and Online Products datasets demonstrate
significant improvement over existing deep feature embedding methods on all
experimented embedding sizes with the GoogLeNet network.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/wieschollek2016efficient/">Efficient Large-scale Approximate Nearest Neighbor Search On The GPU</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Large-scale Approximate Nearest Neighbor Search On The GPU' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient Large-scale Approximate Nearest Neighbor Search On The GPU' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wieschollek et al.</td> <!-- 🔧 You were missing this -->
    <td>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>54</td>
    <td><p>We present a new approach for efficient approximate nearest neighbor (ANN)
search in high dimensional spaces, extending the idea of Product Quantization.
We propose a two-level product and vector quantization tree that reduces the
number of vector comparisons required during tree traversal. Our approach also
includes a novel highly parallelizable re-ranking method for candidate vectors
by efficiently reusing already computed intermediate values. Due to its small
memory footprint during traversal, the method lends itself to an efficient,
parallel GPU implementation. This Product Quantization Tree (PQT) approach
significantly outperforms recent state of the art methods for high dimensional
nearest neighbor queries on standard reference datasets. Ours is the first work
that demonstrates GPU performance superior to CPU performance on high
dimensional, large scale ANN problems in time-critical real-world applications,
like loop-closing in videos.</p>
</td>
    <td>
      
        Similarity Search 
      
        SCALABILITY 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/wu2016robust/">Robust Hashing For Multi-view Data: Jointly Learning Low-rank Kernelized Similarity Consensus And Hash Functions</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Robust Hashing For Multi-view Data: Jointly Learning Low-rank Kernelized Similarity Consensus And Hash Functions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Robust Hashing For Multi-view Data: Jointly Learning Low-rank Kernelized Similarity Consensus And Hash Functions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu Lin, Wang Yang</td> <!-- 🔧 You were missing this -->
    <td>Image and Vision Computing</td>
    <td>49</td>
    <td><p>Learning hash functions/codes for similarity search over multi-view data is
attracting increasing attention, where similar hash codes are assigned to the
data objects characterizing consistently neighborhood relationship across
views. Traditional methods in this category inherently suffer three
limitations: 1) they commonly adopt a two-stage scheme where similarity matrix
is first constructed, followed by a subsequent hash function learning; 2) these
methods are commonly developed on the assumption that data samples with
multiple representations are noise-free,which is not practical in real-life
applications; 3) they often incur cumbersome training model caused by the
neighborhood graph construction using all \(N\) points in the database (\(O(N)\)).
In this paper, we motivate the problem of jointly and efficiently training the
robust hash functions over data objects with multi-feature representations
which may be noise corrupted. To achieve both the robustness and training
efficiency, we propose an approach to effectively and efficiently learning
low-rank kernelized \footnote{We use kernelized similarity rather than kernel,
as it is not a squared symmetric matrix for data-landmark affinity matrix.}
hash functions shared across views. Specifically, we utilize landmark graphs to
construct tractable similarity matrices in multi-views to automatically
discover neighborhood structure in the data. To learn robust hash functions, a
latent low-rank kernel function is used to construct hash functions in order to
accommodate linearly inseparable data. In particular, a latent kernelized
similarity matrix is recovered by rank minimization on multiple kernel-based
similarity matrices. Extensive experiments on real-world multi-view datasets
validate the efficacy of our method in the presence of error corruptions.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/szeto2016binary/">Binary Codes For Tagging X-ray Images Via Deep De-noising Autoencoders</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Binary Codes For Tagging X-ray Images Via Deep De-noising Autoencoders' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Binary Codes For Tagging X-ray Images Via Deep De-noising Autoencoders' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sze-to Antonio, Tizhoosh Hamid R., Wong Andrew K. C.</td> <!-- 🔧 You were missing this -->
    <td>2016 International Joint Conference on Neural Networks (IJCNN)</td>
    <td>11</td>
    <td><p>A Content-Based Image Retrieval (CBIR) system which identifies similar
medical images based on a query image can assist clinicians for more accurate
diagnosis. The recent CBIR research trend favors the construction and use of
binary codes to represent images. Deep architectures could learn the non-linear
relationship among image pixels adaptively, allowing the automatic learning of
high-level features from raw pixels. However, most of them require class
labels, which are expensive to obtain, particularly for medical images. The
methods which do not need class labels utilize a deep autoencoder for binary
hashing, but the code construction involves a specific training algorithm and
an ad-hoc regularization technique. In this study, we explored using a deep
de-noising autoencoder (DDA), with a new unsupervised training scheme using
only backpropagation and dropout, to hash images into binary codes. We
conducted experiments on more than 14,000 x-ray images. By using class labels
only for evaluating the retrieval results, we constructed a 16-bit DDA and a
512-bit DDA independently. Comparing to other unsupervised methods, we
succeeded to obtain the lowest total error by using the 512-bit codes for
retrieval via exhaustive search, and speed up 9.27 times with the use of the
16-bit codes while keeping a comparable total error. We found that our new
training scheme could reduce the total retrieval error significantly by 21.9%.
To further boost the image retrieval performance, we developed Radon
Autoencoder Barcode (RABC) which are learned from the Radon projections of
images using a de-noising autoencoder. Experimental results demonstrated its
superior performance in retrieval when it was combined with DDA binary codes.</p>
</td>
    <td>
      
        Compact Codes 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/gui2016supervised/">Supervised Discrete Hashing With Relaxation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Discrete Hashing With Relaxation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Supervised Discrete Hashing With Relaxation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gui et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Neural Networks and Learning Systems</td>
    <td>92</td>
    <td><p>Data-dependent hashing has recently attracted attention due to being able to
support efficient retrieval and storage of high-dimensional data such as
documents, images, and videos. In this paper, we propose a novel learning-based
hashing method called “Supervised Discrete Hashing with Relaxation” (SDHR)
based on “Supervised Discrete Hashing” (SDH). SDH uses ordinary least squares
regression and traditional zero-one matrix encoding of class label information
as the regression target (code words), thus fixing the regression target. In
SDHR, the regression target is instead optimized. The optimized regression
target matrix satisfies a large margin constraint for correct classification of
each example. Compared with SDH, which uses the traditional zero-one matrix,
SDHR utilizes the learned regression target matrix and, therefore, more
accurately measures the classification error of the regression model and is
more flexible. As expected, SDHR generally outperforms SDH. Experimental
results on two large-scale image datasets (CIFAR-10 and MNIST) and a
large-scale and challenging face dataset (FRGC) demonstrate the effectiveness
and efficiency of SDHR.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/wang2016supervised/">Supervised Quantization For Similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Quantization For Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Supervised Quantization For Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>82</td>
    <td><p>In this paper, we address the problem of searching for semantically similar
images from a large database. We present a compact coding approach, supervised
quantization. Our approach simultaneously learns feature selection that
linearly transforms the database points into a low-dimensional discriminative
subspace, and quantizes the data points in the transformed space. The
optimization criterion is that the quantized points not only approximate the
transformed points accurately, but also are semantically separable: the points
belonging to a class lie in a cluster that is not overlapped with other
clusters corresponding to other classes, which is formulated as a
classification problem. The experiments on several standard datasets show the
superiority of our approach over the state-of-the art supervised hashing and
unsupervised quantization algorithms.</p>
</td>
    <td>
      
        Quantization 
      
        Similarity Search 
      
        SUPERVISED 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/liu2016supervised/">Supervised Matrix Factorization For Cross-modality Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Matrix Factorization For Cross-modality Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Supervised Matrix Factorization For Cross-modality Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>65</td>
    <td><p>Matrix factorization has been recently utilized for the task of multi-modal
hashing for cross-modality visual search, where basis functions are learned to
map data from different modalities to the same Hamming embedding. In this
paper, we propose a novel cross-modality hashing algorithm termed Supervised
Matrix Factorization Hashing (SMFH) which tackles the multi-modal hashing
problem with a collective non-matrix factorization across the different
modalities. In particular, SMFH employs a well-designed binary code learning
algorithm to preserve the similarities among multi-modal original features
through a graph regularization. At the same time, semantic labels, when
available, are incorporated into the learning procedure. We conjecture that all
these would facilitate to preserve the most relevant information during the
binary quantization process, and hence improve the retrieval accuracy. We
demonstrate the superior performance of SMFH on three cross-modality visual
search benchmarks, i.e., the PASCAL-Sentence, Wiki, and NUS-WIDE, with
quantitative comparison to various state-of-the-art methods</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/esen2016large/">Large-scale Video Search With Efficient Temporal Voting Structure</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Large-scale Video Search With Efficient Temporal Voting Structure' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Large-scale Video Search With Efficient Temporal Voting Structure' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Esen Ersin, Ozkan Savas, Atil Ilkay</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 25th ACM international conference on Multimedia</td>
    <td>9</td>
    <td><p>In this work, we propose a fast content-based video querying system for
large-scale video search. The proposed system is distinguished from similar
works with two major contributions. First contribution is superiority of joint
usage of repeated content representation and efficient hashing mechanisms.
Repeated content representation is utilized with a simple yet robust feature,
which is based on edge energy of frames. Each of the representation is
converted into hash code with Hamming Embedding method for further queries.
Second contribution is novel queue-based voting scheme that leads to modest
memory requirements with gradual memory allocation capability, contrary to
complete brute-force temporal voting schemes. This aspect enables us to make
queries on large video databases conveniently, even on commodity computers with
limited memory capacity. Our results show that the system can respond to video
queries on a large video database with fast query times, high recall rate and
very low memory and disk requirements.</p>
</td>
    <td>
      
        Video Retrieval 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/zhang2016efficient/">Efficient Training Of Very Deep Neural Networks For Supervised Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Training Of Very Deep Neural Networks For Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient Training Of Very Deep Neural Networks For Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Ziming, Chen, Saligrama</td> <!-- 🔧 You were missing this -->
    <td>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>111</td>
    <td><p>In this paper, we propose training very deep neural networks (DNNs) for supervised learning of hash codes. Existing methods in this context train relatively “shallow” networks limited by the issues arising in back propagation (e.e. vanishing gradients) as well as computational efficiency. We propose a novel and efficient training algorithm inspired by alternating direction method of multipliers (ADMM) that overcomes some of these limitations. Our method decomposes the training process into independent layer-wise local updates through auxiliary variables. Empirically we observe that our training algorithm always converges and its computational complexity is linearly proportional to the number of edges in the networks. Empirically we manage to train DNNs with 64 hidden layers and 1024 nodes per layer for supervised hashing in about 3 hours using a single GPU. Our proposed very deep supervised hashing (VDSH) method significantly outperforms the state-of-the-art on several benchmark datasets.</p>
</td>
    <td>
      
        Unsupervised 
      
        CVPR 
      
        Neural Hashing 
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/murray2016interferences/">Interferences In Match Kernels</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Interferences In Match Kernels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Interferences In Match Kernels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Murray et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>30</td>
    <td><p>We consider the design of an image representation that embeds and aggregates
a set of local descriptors into a single vector. Popular representations of
this kind include the bag-of-visual-words, the Fisher vector and the VLAD. When
two such image representations are compared with the dot-product, the
image-to-image similarity can be interpreted as a match kernel. In match
kernels, one has to deal with interference, i.e. with the fact that even if two
descriptors are unrelated, their matching score may contribute to the overall
similarity.
  We formalise this problem and propose two related solutions, both aimed at
equalising the individual contributions of the local descriptors in the final
representation. These methods modify the aggregation stage by including a set
of per-descriptor weights. They differ by the objective function that is
optimised to compute those weights. The first is a “democratisation” strategy
that aims at equalising the relative importance of each descriptor in the set
comparison metric. The second one involves equalising the match of a single
descriptor to the aggregated vector.
  These concurrent methods give a substantial performance boost over the state
of the art in image search with short or mid-size vectors, as demonstrated by
our experiments on standard public image retrieval benchmarks.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/tian2016global/">Global Hashing System For Fast Image Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Global Hashing System For Fast Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Global Hashing System For Fast Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tian Dayong, Tao Dacheng</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>14</td>
    <td><p>Hashing methods have been widely investigated for fast approximate nearest
neighbor searching in large data sets. Most existing methods use binary vectors
in lower dimensional spaces to represent data points that are usually real
vectors of higher dimensionality. We divide the hashing process into two steps.
Data points are first embedded in a low-dimensional space, and the global
positioning system method is subsequently introduced but modified for binary
embedding. We devise dataindependent and data-dependent methods to distribute
the satellites at appropriate locations. Our methods are based on finding the
tradeoff between the information losses in these two steps. Experiments show
that our data-dependent method outperforms other methods in different-sized
data sets from 100k to 10M. By incorporating the orthogonality of the code
matrix, both our data-independent and data-dependent methods are particularly
impressive in experiments on longer bits.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/liang2016optimizing/">Optimizing Top Precision Performance Measure Of Content-based Image Retrieval By Learning Similarity Function</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Optimizing Top Precision Performance Measure Of Content-based Image Retrieval By Learning Similarity Function' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Optimizing Top Precision Performance Measure Of Content-based Image Retrieval By Learning Similarity Function' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liang et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>52</td>
    <td><p>In this paper we study the problem of content-based image retrieval. In this
problem, the most popular performance measure is the top precision measure, and
the most important component of a retrieval system is the similarity function
used to compare a query image against a database image. However, up to now,
there is no existing similarity learning method proposed to optimize the top
precision measure. To fill this gap, in this paper, we propose a novel
similarity learning method to maximize the top precision measure. We model this
problem as a minimization problem with an objective function as the combination
of the losses of the relevant images ranked behind the top-ranked irrelevant
image, and the squared Frobenius norm of the similarity function parameter.
This minimization problem is solved as a quadratic programming problem. The
experiments over two benchmark data sets show the advantages of the proposed
method over other similarity learning methods when the top precision is used as
the performance measure.</p>
</td>
    <td>
      
        Evaluation 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/do2016binary/">Binary Hashing With Semidefinite Relaxation And Augmented Lagrangian</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Binary Hashing With Semidefinite Relaxation And Augmented Lagrangian' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Binary Hashing With Semidefinite Relaxation And Augmented Lagrangian' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Do et al.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>20</td>
    <td><p>This paper proposes two approaches for inferencing binary codes in two-step
(supervised, unsupervised) hashing. We first introduce an unified formulation
for both supervised and unsupervised hashing. Then, we cast the learning of one
bit as a Binary Quadratic Problem (BQP). We propose two approaches to solve
BQP. In the first approach, we relax BQP as a semidefinite programming problem
which its global optimum can be achieved. We theoretically prove that the
objective value of the binary solution achieved by this approach is well
bounded. In the second approach, we propose an augmented Lagrangian based
approach to solve BQP directly without relaxing the binary constraint.
Experimental results on three benchmark datasets show that our proposed methods
compare favorably with the state of the art.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/wang2015semantic/">Semantic Topic Multimodal Hashing For Cross-media Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semantic Topic Multimodal Hashing For Cross-media Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semantic Topic Multimodal Hashing For Cross-media Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang di, Gao, He</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>149</td>
    <td><p>Multimodal hashing is essential to cross-media
similarity search for its low storage cost and fast
query speed. Most existing multimodal hashing
methods embedded heterogeneous data into a common low-dimensional Hamming space, and then
rounded the continuous embeddings to obtain the
binary codes. Yet they usually neglect the inherent discrete nature of hashing for relaxing the discrete constraints, which will cause degraded retrieval performance especially for long codes. For
this purpose, a novel Semantic Topic Multimodal
Hashing (STMH) is developed by considering latent semantic information in coding procedure.
It
first discovers clustering patterns of texts and robust factorizes the matrix of images to obtain multiple semantic topics of texts and concepts of images.
Then the learned multimodal semantic features are
transformed into a common subspace by their correlations. Finally, each bit of unified hash code
can be generated directly by figuring out whether a
topic or concept is contained in a text or an image.
Therefore, the obtained model by STMH is more
suitable for hashing scheme as it directly learns discrete hash codes in the coding process. Experimental results demonstrate that the proposed method
outperforms several state-of-the-art methods.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Multimodal Retrieval 
      
        Medical Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/jing2015visual/">Visual Search At Pinterest</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Visual Search At Pinterest' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Visual Search At Pinterest' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jing et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</td>
    <td>144</td>
    <td><p>We demonstrate that, with the availability of distributed computation
platforms such as Amazon Web Services and open-source tools, it is possible for
a small engineering team to build, launch and maintain a cost-effective,
large-scale visual search system with widely available tools. We also
demonstrate, through a comprehensive set of live experiments at Pinterest, that
content recommendation powered by visual search improve user engagement. By
sharing our implementation details and the experiences learned from launching a
commercial visual search engines from scratch, we hope visual search are more
widely incorporated into today’s commercial applications.</p>
</td>
    <td>
      
        KDD 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/helmer2015similarity/">A Similarity Measure For Weaving Patterns In Textiles</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Similarity Measure For Weaving Patterns In Textiles' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Similarity Measure For Weaving Patterns In Textiles' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Helmer Sven, Ngo Vuong M.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>6</td>
    <td><p>We propose a novel approach for measuring the similarity between weaving
patterns that can provide similarity-based search functionality for textile
archives. We represent textile structures using hypergraphs and extract
multisets of k-neighborhoods from these graphs. The resulting multisets are
then compared using Jaccard coefficients, Hamming distances, and cosine
measures. We evaluate the different variants of our similarity measure
experimentally, showing that it can be implemented efficiently and illustrating
its quality using it to cluster and query a data set containing more than a
thousand textile samples.</p>
</td>
    <td>
      
        SIGIR 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/wang2015hamming/">Hamming Compatible Quantization For Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hamming Compatible Quantization For Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hamming Compatible Quantization For Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang et al.</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>20</td>
    <td><p>Hashing is one of the effective techniques for fast
Approximate Nearest Neighbour (ANN) search.
Traditional single-bit quantization (SBQ) in most
hashing methods incurs lots of quantization error
which seriously degrades the search performance.
To address the limitation of SBQ, researchers have
proposed promising multi-bit quantization (MBQ)
methods to quantize each projection dimension
with multiple bits. However, some MBQ methods
need to adopt specific distance for binary code
matching instead of the original Hamming distance,
which would significantly decrease the retrieval
speed. Two typical MBQ methods Hierarchical
Quantization and Double Bit Quantization
retain the Hamming distance, but both of them only
consider the projection dimensions during quantization,
ignoring the neighborhood structure of raw
data inherent in Euclidean space. In this paper,
we propose a multi-bit quantization method named
Hamming Compatible Quantization (HCQ) to preserve
the capability of similarity metric between
Euclidean space and Hamming space by utilizing
the neighborhood structure of raw data. Extensive
experiment results have shown our approach significantly
improves the performance of various stateof-the-art
hashing methods while maintaining fast
retrieval speed.</p>
</td>
    <td>
      
        Quantization 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/zhao2015deep/">Deep Semantic Ranking Based Hashing For Multi-label Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Semantic Ranking Based Hashing For Multi-label Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Semantic Ranking Based Hashing For Multi-label Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhao et al.</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>463</td>
    <td><p>With the rapid growth of web images, hashing has received
increasing interests in large scale image retrieval.
Research efforts have been devoted to learning compact binary
codes that preserve semantic similarity based on labels.
However, most of these hashing methods are designed
to handle simple binary similarity. The complex multilevel
semantic structure of images associated with multiple labels
have not yet been well explored. Here we propose a deep
semantic ranking based method for learning hash functions
that preserve multilevel semantic similarity between multilabel
images. In our approach, deep convolutional neural
network is incorporated into hash functions to jointly
learn feature representations and mappings from them to
hash codes, which avoids the limitation of semantic representation
power of hand-crafted features. Meanwhile, a
ranking list that encodes the multilevel similarity information
is employed to guide the learning of such deep hash
functions. An effective scheme based on surrogate loss is
used to solve the intractable optimization problem of nonsmooth
and multivariate ranking measures involved in the
learning procedure. Experimental results show the superiority
of our proposed approach over several state-of-theart
hashing methods in term of ranking evaluation metrics
when tested on multi-label image datasets.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/schroff2015facenet/">Facenet: A Unified Embedding For Face Recognition And Clustering</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Facenet: A Unified Embedding For Face Recognition And Clustering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Facenet: A Unified Embedding For Face Recognition And Clustering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Schroff Florian, Kalenichenko Dmitry, Philbin James</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>8312</td>
    <td><p>Despite significant recent advances in the field of face recognition,
implementing face verification and recognition efficiently at scale presents
serious challenges to current approaches. In this paper we present a system,
called FaceNet, that directly learns a mapping from face images to a compact
Euclidean space where distances directly correspond to a measure of face
similarity. Once this space has been produced, tasks such as face recognition,
verification and clustering can be easily implemented using standard techniques
with FaceNet embeddings as feature vectors.
  Our method uses a deep convolutional network trained to directly optimize the
embedding itself, rather than an intermediate bottleneck layer as in previous
deep learning approaches. To train, we use triplets of roughly aligned matching
/ non-matching face patches generated using a novel online triplet mining
method. The benefit of our approach is much greater representational
efficiency: we achieve state-of-the-art face recognition performance using only
128-bytes per face.
  On the widely used Labeled Faces in the Wild (LFW) dataset, our system
achieves a new record accuracy of 99.63%. On YouTube Faces DB it achieves
95.12%. Our system cuts the error rate in comparison to the best published
result by 30% on both datasets.
  We also introduce the concept of harmonic embeddings, and a harmonic triplet
loss, which describe different versions of face embeddings (produced by
different networks) that are compatible to each other and allow for direct
comparison between each other.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/shi2015transferring/">Transferring A Semantic Representation For Person Re-identification And Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Transferring A Semantic Representation For Person Re-identification And Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Transferring A Semantic Representation For Person Re-identification And Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shi Zhiyuan, Hospedales Timothy M., Xiang Tao</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>192</td>
    <td><p>Learning semantic attributes for person re-identification and
description-based person search has gained increasing interest due to
attributes’ great potential as a pose and view-invariant representation.
However, existing attribute-centric approaches have thus far underperformed
state-of-the-art conventional approaches. This is due to their non-scalable
need for extensive domain (camera) specific annotation. In this paper we
present a new semantic attribute learning approach for person re-identification
and search. Our model is trained on existing fashion photography datasets –
either weakly or strongly labelled. It can then be transferred and adapted to
provide a powerful semantic description of surveillance person detections,
without requiring any surveillance domain supervision. The resulting
representation is useful for both unsupervised and supervised person
re-identification, achieving state-of-the-art and near state-of-the-art
performance respectively. Furthermore, as a semantic representation it allows
description-based person search to be integrated within the same framework.</p>
</td>
    <td>
      
        Similarity Search 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/lin2015semantics/">Semantics-preserving Hashing For Cross-view Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semantics-preserving Hashing For Cross-view Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semantics-preserving Hashing For Cross-view Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin Zijia, Ding, Wang</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>540</td>
    <td><p>With benefits of low storage costs and high query speeds,
hashing methods are widely researched for efficiently retrieving large-scale data, which commonly contains multiple views, e.g. a news report with images, videos and texts.
In this paper, we study the problem of cross-view retrieval
and propose an effective Semantics-Preserving Hashing
method, termed SePH. Given semantic affinities of training data as supervised information, SePH transforms them
into a probability distribution and approximates it with tobe-learnt hash codes in Hamming space via minimizing the
Kullback-Leibler divergence. Then kernel logistic regression with a sampling strategy is utilized to learn the nonlinear projections from features in each view to the learnt
hash codes. And for any unseen instance, predicted hash
codes and their corresponding output probabilities from observed views are utilized to determine its unified hash code,
using a novel probabilistic approach. Extensive experiments conducted on three benchmark datasets well demonstrate the effectiveness and reasonableness of SePH.</p>
</td>
    <td>
      
        Hashing Methods 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/lin2015deep/">Deep Learning Of Binary Hash Codes For Fast Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Learning Of Binary Hash Codes For Fast Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Learning Of Binary Hash Codes For Fast Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin et al.</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>626</td>
    <td><p>Approximate nearest neighbor search is an efficient strategy for large-scale image retrieval. Encouraged by the recent advances in convolutional neural networks (CNNs), we propose an effective deep learning framework to generate binary hash codes for fast image retrieval. Our idea is that when the data labels are available, binary codes can be learned by employing a hidden layer for representing the latent concepts that dominate the class labels.
he utilization of the CNN also allows for learning image representations. Unlike other supervised methods that require pair-wised inputs for binary code learning, our method learns hash codes and image representations in a point-wised manner, making it suitable for large-scale datasets. Experimental results show that our method outperforms several state-of-the-art hashing algorithms on the CIFAR-10 and MNIST datasets. We further demonstrate its scalability and efficacy on a large-scale dataset of 1 million clothing images.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/mukherjee2015nmf/">An NMF Perspective On Binary Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=An NMF Perspective On Binary Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=An NMF Perspective On Binary Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mukherjee et al.</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE International Conference on Computer Vision (ICCV)</td>
    <td>14</td>
    <td><p>The pervasiveness of massive data repositories has led
to much interest in efficient methods for indexing, search,
and retrieval. For image data, a rapidly developing body of
work for these applications shows impressive performance
with methods that broadly fall under the umbrella term of
Binary Hashing. Given a distance matrix, a binary hashing
algorithm solves for a binary code for the given set of examples, whose Hamming distance nicely approximates the
original distances. The formulation is non-convex — so existing solutions adopt spectral relaxations or perform coordinate descent (or quantization) on a surrogate objective
that is numerically more tractable. In this paper, we first
derive an Augmented Lagrangian approach to optimize the
standard binary Hashing objective (i.e., maintain fidelity
with a given distance matrix). With appropriate step sizes,
we find that this scheme already yields results that match or
substantially outperform state of the art methods on most
benchmarks used in the literature. Then, to allow the model
to scale to large datasets, we obtain an interesting reformulation of the binary hashing objective as a non-negative matrix factorization. Later, this leads to a simple multiplicative updates algorithm — whose parallelization properties
are exploited to obtain a fast GPU based implementation.
We give a probabilistic analysis of our initialization scheme
and present a range of experiments to show that the method
is simple to implement and competes favorably with available methods (both for optimization and generalization).</p>
</td>
    <td>
      
        Hashing Methods 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/yi2015binary/">Binary Embedding: Fundamental Limits And Fast Algorithm</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Binary Embedding: Fundamental Limits And Fast Algorithm' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Binary Embedding: Fundamental Limits And Fast Algorithm' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yi Xinyang, Caramanis Constantine, Price Eric</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>19</td>
    <td><p>Binary embedding is a nonlinear dimension reduction methodology where high
dimensional data are embedded into the Hamming cube while preserving the
structure of the original space. Specifically, for an arbitrary \(N\) distinct
points in \(\mathbb{S}^{p-1}\), our goal is to encode each point using
\(m\)-dimensional binary strings such that we can reconstruct their geodesic
distance up to \(\delta\) uniform distortion. Existing binary embedding
algorithms either lack theoretical guarantees or suffer from running time
\(O\big(mp\big)\). We make three contributions: (1) we establish a lower bound
that shows any binary embedding oblivious to the set of points requires \(m =
Ω(\frac{1}{\delta^2}log{N})\) bits and a similar lower bound for
non-oblivious embeddings into Hamming distance; (2) [DELETED, see comment]; (3)
we also provide an analytic result about embedding a general set of points \(K
\subseteq \mathbb{S}^{p-1}\) with even infinite size. Our theoretical findings
are supported through experiments on both synthetic and real data sets.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/yu2015learning/">Learning Cross Space Mapping Via DNN Using Large Scale Click-through Logs</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Cross Space Mapping Via DNN Using Large Scale Click-through Logs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Cross Space Mapping Via DNN Using Large Scale Click-through Logs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yu et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>8</td>
    <td><p>The gap between low-level visual signals and high-level semantics has been
progressively bridged by continuous development of deep neural network (DNN).
With recent progress of DNN, almost all image classification tasks have
achieved new records of accuracy. To extend the ability of DNN to image
retrieval tasks, we proposed a unified DNN model for image-query similarity
calculation by simultaneously modeling image and query in one network. The
unified DNN is named the cross space mapping (CSM) model, which contains two
parts, a convolutional part and a query-embedding part. The image and query are
mapped to a common vector space via these two parts respectively, and
image-query similarity is naturally defined as an inner product of their
mappings in the space. To ensure good generalization ability of the DNN, we
learn weights of the DNN from a large number of click-through logs which
consists of 23 million clicked image-query pairs between 1 million images and
11.7 million queries. Both the qualitative results and quantitative results on
an image retrieval evaluation task with 1000 queries demonstrate the
superiority of the proposed method.</p>
</td>
    <td>
      
        Evaluation 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/jiang2015scalable/">Scalable Graph Hashing With Feature Transformation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Scalable Graph Hashing With Feature Transformation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Scalable Graph Hashing With Feature Transformation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jiang Q., Li</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>208</td>
    <td><p>Hashing has been widely used for approximate nearest
neighbor (ANN) search in big data applications
because of its low storage cost and fast retrieval
speed. The goal of hashing is to map the data
points from the original space into a binary-code
space where the similarity (neighborhood structure)
in the original space is preserved. By directly
exploiting the similarity to guide the hashing
code learning procedure, graph hashing has attracted
much attention. However, most existing graph
hashing methods cannot achieve satisfactory performance
in real applications due to the high complexity
for graph modeling. In this paper, we propose
a novel method, called scalable graph hashing
with feature transformation (SGH), for large-scale
graph hashing. Through feature transformation, we
can effectively approximate the whole graph without
explicitly computing the similarity graph matrix,
based on which a sequential learning method
is proposed to learn the hash functions in a bit-wise
manner. Experiments on two datasets with one million
data points show that our SGH method can
outperform the state-of-the-art methods in terms of
both accuracy and scalability.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/zheng2015person/">Person Re-identification Meets Image Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Person Re-identification Meets Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Person Re-identification Meets Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zheng et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>60</td>
    <td><p>For long time, person re-identification and image search are two separately
studied tasks. However, for person re-identification, the effectiveness of
local features and the “query-search” mode make it well posed for image search
techniques.
  In the light of recent advances in image search, this paper proposes to treat
person re-identification as an image search problem. Specifically, this paper
claims two major contributions. 1) By designing an unsupervised Bag-of-Words
representation, we are devoted to bridging the gap between the two tasks by
integrating techniques from image search in person re-identification. We show
that our system sets up an effective yet efficient baseline that is amenable to
further supervised/unsupervised improvements. 2) We contribute a new high
quality dataset which uses DPM detector and includes a number of distractor
images. Our dataset reaches closer to realistic settings, and new perspectives
are provided.
  Compared with approaches that rely on feature-feature match, our method is
faster by over two orders of magnitude. Moreover, on three datasets, we report
competitive results compared with the state-of-the-art methods.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/xu2015convolutional/">Convolutional Neural Networks For Text Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Convolutional Neural Networks For Text Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Convolutional Neural Networks For Text Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xu et al.</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>33</td>
    <td><p>Hashing, as a popular approximate nearest neighbor
search, has been widely used for large-scale similarity search. Recently, a spectrum of machine learning
methods are utilized to learn similarity-preserving
binary codes. However, most of them directly encode the explicit features, keywords, which fail to
preserve the accurate semantic similarities in binary code beyond keyword matching, especially on
short texts. Here we propose a novel text hashing
framework with convolutional neural networks. In
particular, we first embed the keyword features into
compact binary code with a locality preserving constraint. Meanwhile word features and position features are together fed into a convolutional network to
learn the implicit features which are further incorporated with the explicit features to fit the pre-trained
binary code. Such base method can be successfully
accomplished without any external tags/labels, and
other three model variations are designed to integrate tags/labels. Experimental results show the
superiority of our proposed approach over several
state-of-the-art hashing methods when tested on one
short text dataset as well as one normal text dataset.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/andoni2015practical/">Practical And Optimal LSH For Angular Distance</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Practical And Optimal LSH For Angular Distance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Practical And Optimal LSH For Angular Distance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Andoni A., Indyk, Laarhoven</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>238</td>
    <td><p>We show the existence of a Locality-Sensitive Hashing (LSH) family for the angular
distance that yields an approximate Near Neighbor Search algorithm with the
asymptotically optimal running time exponent. Unlike earlier algorithms with this
property (e.g., Spherical LSH [1, 2]), our algorithm is also practical, improving
upon the well-studied hyperplane LSH [3] in practice. We also introduce a multiprobe
version of this algorithm and conduct an experimental evaluation on real
and synthetic data sets.
We complement the above positive results with a fine-grained lower bound for the
quality of any LSH family for angular distance. Our lower bound implies that the
above LSH family exhibits a trade-off between evaluation time and quality that is
close to optimal for a natural class of LSH functions.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/kehl2015hashmod/">Hashmod: A Hashing Method For Scalable 3D Object Detection</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashmod: A Hashing Method For Scalable 3D Object Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashmod: A Hashing Method For Scalable 3D Object Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kehl et al.</td> <!-- 🔧 You were missing this -->
    <td>Procedings of the British Machine Vision Conference 2015</td>
    <td>46</td>
    <td><p>We present a scalable method for detecting objects and estimating their 3D
poses in RGB-D data. To this end, we rely on an efficient representation of
object views and employ hashing techniques to match these views against the
input frame in a scalable way. While a similar approach already exists for 2D
detection, we show how to extend it to estimate the 3D pose of the detected
objects. In particular, we explore different hashing strategies and identify
the one which is more suitable to our problem. We show empirically that the
complexity of our method is sublinear with the number of objects and we enable
detection and pose estimation of many 3D objects with high accuracy while
outperforming the state-of-the-art in terms of runtime.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/ding2015knn/">Knn Hashing With Factorized Neighborhood Representation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Knn Hashing With Factorized Neighborhood Representation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Knn Hashing With Factorized Neighborhood Representation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ding et al.</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE International Conference on Computer Vision (ICCV)</td>
    <td>14</td>
    <td><p>Hashing is very effective for many tasks in reducing the
processing time and in compressing massive databases. Although lots of approaches have been developed to learn
data-dependent hash functions in recent years, how to learn
hash functions to yield good performance with acceptable
computational and memory cost is still a challenging problem. Based on the observation that retrieval precision is
highly related to the kNN classification accuracy, this paper
proposes a novel kNN-based supervised hashing method,
which learns hash functions by directly maximizing the kNN
accuracy of the Hamming-embedded training data. To make
it scalable well to large problem, we propose a factorized
neighborhood representation to parsimoniously model the
neighborhood relationships inherent in training data. Considering that real-world data are often linearly inseparable,
we further kernelize this basic model to improve its performance. As a result, the proposed method is able to learn
accurate hashing functions with tolerable computation and
storage cost. Experiments on four benchmarks demonstrate
that our method outperforms the state-of-the-arts.</p>
</td>
    <td>
      
        Hashing Methods 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/ding2015deep/">Deep Feature Learning With Relative Distance Comparison For Person Re-identification</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Feature Learning With Relative Distance Comparison For Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Feature Learning With Relative Distance Comparison For Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ding et al.</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition</td>
    <td>729</td>
    <td><p>Identifying the same individual across different scenes is an important yet
difficult task in intelligent video surveillance. Its main difficulty lies in
how to preserve similarity of the same person against large appearance and
structure variation while discriminating different individuals. In this paper,
we present a scalable distance driven feature learning framework based on the
deep neural network for person re-identification, and demonstrate its
effectiveness to handle the existing challenges. Specifically, given the
training images with the class labels (person IDs), we first produce a large
number of triplet units, each of which contains three images, i.e. one person
with a matched reference and a mismatched reference. Treating the units as the
input, we build the convolutional neural network to generate the layered
representations, and follow with the \(L2\) distance metric. By means of
parameter optimization, our framework tends to maximize the relative distance
between the matched pair and the mismatched pair for each triplet unit.
Moreover, a nontrivial issue arising with the framework is that the triplet
organization cubically enlarges the number of training triplets, as one image
can be involved into several triplet units. To overcome this problem, we
develop an effective triplet generation scheme and an optimized gradient
descent algorithm, making the computational load mainly depends on the number
of original images instead of the number of triplets. On several challenging
databases, our approach achieves very promising results and outperforms other
state-of-the-art approaches.</p>
</td>
    <td>
      
        Evaluation 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/zhang2015bit/">Bit-scalable Deep Hashing With Regularized Similarity Learning For Image Retrieval And Person Re-identification</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Bit-scalable Deep Hashing With Regularized Similarity Learning For Image Retrieval And Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Bit-scalable Deep Hashing With Regularized Similarity Learning For Image Retrieval And Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang et al.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>418</td>
    <td><p>Extracting informative image features and learning
effective approximate hashing functions are two crucial steps in
image retrieval . Conventional methods often study these two
steps separately, e.g., learning hash functions from a predefined
hand-crafted feature space. Meanwhile, the bit lengths of output
hashing codes are preset in most previous methods, neglecting the
significance level of different bits and restricting their practical
flexibility. To address these issues, we propose a supervised
learning framework to generate compact and bit-scalable hashing
codes directly from raw images. We pose hashing learning as
a problem of regularized similarity learning. Specifically, we
organize the training images into a batch of triplet samples,
each sample containing two images with the same label and one
with a different label. With these triplet samples, we maximize
the margin between matched pairs and mismatched pairs in the
Hamming space. In addition, a regularization term is introduced
to enforce the adjacency consistency, i.e., images of similar
appearances should have similar codes. The deep convolutional
neural network is utilized to train the model in an end-to-end
fashion, where discriminative image features and hash functions
are simultaneously optimized. Furthermore, each bit of our
hashing codes is unequally weighted so that we can manipulate
the code lengths by truncating the insignificant bits. Our
framework outperforms state-of-the-arts on public benchmarks
of similar image search and also achieves promising results in
the application of person re-identification in surveillance. It is
also shown that the generated bit-scalable hashing codes well
preserve the discriminative powers with shorter code lengths.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/li2015feature/">Feature Learning Based Deep Supervised Hashing With Pairwise Labels</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Feature Learning Based Deep Supervised Hashing With Pairwise Labels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Feature Learning Based Deep Supervised Hashing With Pairwise Labels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Wu-jun, Kang</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>509</td>
    <td><p>Recent years have witnessed wide application of
hashing for large-scale image retrieval. However,
most existing hashing methods are based on handcrafted features which might not be optimally compatible with the hashing procedure. Recently, deep
hashing methods have been proposed to perform simultaneous feature learning and hash-code learning with deep neural networks, which have shown
better performance than traditional hashing methods with hand-crafted features. Most of these deep
hashing methods are supervised whose supervised
information is given with triplet labels. For another common application scenario with pairwise labels, there have not existed methods for simultaneous feature learning and hash-code learning. In this
paper, we propose a novel deep hashing method,
called deep pairwise-supervised hashing (DPSH),
to perform simultaneous feature learning and hashcode learning for applications with pairwise labels.
Experiments on real datasets show that our DPSH
method can outperform other methods to achieve
the state-of-the-art performance in image retrieval
applications.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        SUPERVISED 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/liu2015multi/">Multi-view Complementary Hash Tables For Nearest Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multi-view Complementary Hash Tables For Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multi-view Complementary Hash Tables For Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE International Conference on Computer Vision (ICCV)</td>
    <td>57</td>
    <td><p>Recent years have witnessed the success of hashing techniques in fast nearest neighbor search. In practice many
applications (e.g., visual search, object detection, image
matching, etc.) have enjoyed the benefits of complementary hash tables and information fusion over multiple views.
However, most of prior research mainly focused on compact hash code cleaning, and rare work studies how to build
multiple complementary hash tables, much less to adaptively integrate information stemming from multiple views.
In
this paper we first present a novel multi-view complementary hash table method that learns complementary hash tables from the data with multiple views. For single multiview table, using exemplar based feature fusion, we approximate the inherent data similarities with a low-rank matrix,
and learn discriminative hash functions in an efficient way.
To build complementary tables and meanwhile maintain scalable training and fast out-of-sample extension, an exemplar reweighting scheme is introduced to update the induced low-rank similarity in the sequential table construction framework, which indeed brings mutual benefits between tables by placing greater importance on exemplars
shared by mis-separated neighbors. Extensive experiments
on three large-scale image datasets demonstrate that the
proposed method significantly outperforms various naive
solutions and state-of-the-art multi-table methods.</p>
</td>
    <td>
      
        Similarity Search 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/song2015top/">Top Rank Supervised Binary Coding For Visual Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Top Rank Supervised Binary Coding For Visual Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Top Rank Supervised Binary Coding For Visual Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Song et al.</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE International Conference on Computer Vision (ICCV)</td>
    <td>76</td>
    <td><p>In recent years, binary coding techniques are becoming
increasingly popular because of their high efficiency in handling large-scale computer vision applications. It has been
demonstrated that supervised binary coding techniques that
leverage supervised information can significantly enhance
the coding quality, and hence greatly benefit visual search
tasks. Typically, a modern binary coding method seeks
to learn a group of coding functions which compress data
samples into binary codes. However, few methods pursued
the coding functions such that the precision at the top of
a ranking list according to Hamming distances of the generated binary codes is optimized.
In this paper, we propose a novel supervised binary coding approach, namely
Top Rank Supervised Binary Coding (Top-RSBC), which
explicitly focuses on optimizing the precision of top positions in a Hamming-distance ranking list towards preserving the supervision information. The core idea is to train
the disciplined coding functions, by which the mistakes at
the top of a Hamming-distance ranking list are penalized
more than those at the bottom. To solve such coding functions, we relax the original discrete optimization objective
with a continuous surrogate, and derive a stochastic gradient descent to optimize the surrogate objective. To further reduce the training time cost, we also design an online
learning algorithm to optimize the surrogate objective more
efficiently. Empirical studies based upon three benchmark
image datasets demonstrate that the proposed binary coding approach achieves superior image search accuracy over
the state-of-the-arts.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Image Retrieval 
      
        Compact Codes 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/cakir2015adaptive/">Adaptive Hashing For Fast Similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Adaptive Hashing For Fast Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Adaptive Hashing For Fast Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cakir F., Sclaroff</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE International Conference on Computer Vision (ICCV)</td>
    <td>88</td>
    <td><p>With the staggering growth in image and video datasets,
algorithms that provide fast similarity search and compact
storage are crucial. Hashing methods that map the
data into Hamming space have shown promise; however,
many of these methods employ a batch-learning strategy
in which the computational cost and memory requirements
may become intractable and infeasible with larger and
larger datasets. To overcome these challenges, we propose
an online learning algorithm based on stochastic gradient
descent in which the hash functions are updated iteratively
with streaming data. In experiments with three image retrieval
benchmarks, our online algorithm attains retrieval
accuracy that is comparable to competing state-of-the-art
batch-learning solutions, while our formulation is orders
of magnitude faster and being online it is adaptable to the
variations of the data. Moreover, our formulation yields improved
retrieval performance over a recently reported online
hashing technique, Online Kernel Hashing.</p>
</td>
    <td>
      
        Similarity Search 
      
        Hashing Methods 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/fu2015improved/">An Improved System For Sentence-level Novelty Detection In Textual Streams</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=An Improved System For Sentence-level Novelty Detection In Textual Streams' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=An Improved System For Sentence-level Novelty Detection In Textual Streams' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fu et al.</td> <!-- 🔧 You were missing this -->
    <td>2015 International Conference on Smart and Sustainable City and Big Data (ICSSC)</td>
    <td>5</td>
    <td><p>Novelty detection in news events has long been a difficult problem. A number
of models performed well on specific data streams but certain issues are far
from being solved, particularly in large data streams from the WWW where
unpredictability of new terms requires adaptation in the vector space model. We
present a novel event detection system based on the Incremental Term
Frequency-Inverse Document Frequency (TF-IDF) weighting incorporated with
Locality Sensitive Hashing (LSH). Our system could efficiently and effectively
adapt to the changes within the data streams of any new terms with continual
updates to the vector space model. Regarding miss probability, our proposed
novelty detection framework outperforms a recognised baseline system by
approximately 16% when evaluating a benchmark dataset from Google News.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/leng2015hashing/">Hashing For Distributed Data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing For Distributed Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing For Distributed Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Leng et al.</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>30</td>
    <td><p>Recently, hashing based approximate nearest
neighbors search has attracted much attention.
Extensive centralized hashing algorithms have
been proposed and achieved promising performance. However, due to the large scale of many
applications, the data is often stored or even collected in a distributed manner. Learning hash
functions by aggregating all the data into a fusion
center is infeasible because of the prohibitively
expensive communication and computation overhead.
In this paper, we develop a novel hashing
model to learn hash functions in a distributed setting. We cast a centralized hashing model as a
set of subproblems with consensus constraints.
We find these subproblems can be analytically
solved in parallel on the distributed compute nodes. Since no training data is transmitted across
the nodes in the learning process, the communication cost of our model is independent to the data size. Extensive experiments on several large
scale datasets containing up to 100 million samples demonstrate the efficacy of our method.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/lai2015simultaneous/">Simultaneous Feature Learning And Hash Coding With Deep Neural Networks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Simultaneous Feature Learning And Hash Coding With Deep Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Simultaneous Feature Learning And Hash Coding With Deep Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lai et al.</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>916</td>
    <td><p>Similarity-preserving hashing is a widely-used method
for nearest neighbour search in large-scale image retrieval
tasks. For most existing hashing methods, an image is
first encoded as a vector of hand-engineering visual features,
followed by another separate projection or quantization
step that generates binary codes. However, such visual
feature vectors may not be optimally compatible with the
coding process, thus producing sub-optimal hashing codes.
In this paper, we propose a deep architecture for supervised
hashing, in which images are mapped into binary codes via
carefully designed deep neural networks. The pipeline of
the proposed deep architecture consists of three building
blocks: 1) a sub-network with a stack of convolution layers
to produce the effective intermediate image features; 2)
a divide-and-encode module to divide the intermediate image
features into multiple branches, each encoded into one
hash bit; and 3) a triplet ranking loss designed to characterize
that one image is more similar to the second image than
to the third one. Extensive evaluations on several benchmark
image datasets show that the proposed simultaneous
feature learning and hash coding pipeline brings substantial
improvements over other state-of-the-art supervised or
unsupervised hashing methods.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/moran2015regularised/">Regularised Cross-modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Regularised Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Regularised Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Moran S., Lavrenko</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>18</td>
    <td><p>In this paper we propose Regularised Cross-Modal Hashing (RCMH) a new cross-modal hashing scheme that projects annotation and visual feature descriptors into a common Hamming space. RCMH optimises the intra-modality similarity of data-points in the annotation modality using an iterative three-step hashing algorithm: in the first step each training image is assigned a K-bit hashcode based on hyperplanes learnt at the previous iteration; in the second step the binary bits are smoothed by a formulation of graph regularisation so that similar data-points have similar bits; in the third step a set of binary classifiers are trained to predict the regularised bits with maximum margin. Visual descriptors are projected into the annotation Hamming space by a set of binary classifiers learnt using the bits of the corresponding annotations as labels. RCMH is shown to consistently improve retrieval effectiveness over state-of-the-art baselines.</p>
</td>
    <td>
      
        SIGIR 
      
        Hashing Methods 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/liong2015deep/">Deep Hashing For Compact Binary Codes Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Hashing For Compact Binary Codes Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Hashing For Compact Binary Codes Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liong et al.</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>579</td>
    <td><p>In this paper, we propose a new deep hashing (DH) approach
to learn compact binary codes for large scale visual
search. Unlike most existing binary codes learning methods
which seek a single linear projection to map each sample
into a binary vector, we develop a deep neural network
to seek multiple hierarchical non-linear transformations to
learn these binary codes, so that the nonlinear relationship
of samples can be well exploited. Our model is learned under
three constraints at the top layer of the deep network:
1) the loss between the original real-valued feature descriptor
and the learned binary vector is minimized, 2) the binary
codes distribute evenly on each bit, and 3) different bits
are as independent as possible. To further improve the discriminative
power of the learned binary codes, we extend
DH into supervised DH (SDH) by including one discriminative
term into the objective function of DH which simultaneously
maximizes the inter-class variations and minimizes
the intra-class variations of the learned binary codes. Experimental
results show the superiority of the proposed approach
over the state-of-the-arts.</p>
</td>
    <td>
      
        Neural Hashing 
      
        Hashing Methods 
      
        Compact Codes 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/carreiraperpinan2015hashing/">Hashing With Binary Autoencoders</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing With Binary Autoencoders' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing With Binary Autoencoders' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Carreira-perpinan M., Raziperchikolaei</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>142</td>
    <td><p>An attractive approach for fast search in image
databases is binary hashing, where each high-dimensional,
real-valued image is mapped onto a low-dimensional, binary
vector and the search is done in this binary space.
Finding the optimal hash function is difficult because it involves
binary constraints, and most approaches approximate
the optimization by relaxing the constraints and then
binarizing the result. Here, we focus on the binary autoencoder
model, which seeks to reconstruct an image from the
binary code produced by the hash function. We show that
the optimization can be simplified with the method of auxiliary
coordinates. This reformulates the optimization as
alternating two easier steps: one that learns the encoder
and decoder separately, and one that optimizes the code for
each image. Image retrieval experiments show the resulting
hash function outperforms or is competitive with state-ofthe-art
methods for binary hashing.</p>
</td>
    <td>
      
        Hashing Methods 
      
        CVPR 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/liu2014discrete/">Discrete Graph Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Discrete Graph Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Discrete Graph Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>496</td>
    <td><p>Hashing has emerged as a popular technique for fast nearest neighbor search in gigantic
databases. In particular, learning based hashing has received considerable
attention due to its appealing storage and search efficiency. However, the performance
of most unsupervised learning based hashing methods deteriorates rapidly
as the hash code length increases. We argue that the degraded performance is due
to inferior optimization procedures used to achieve discrete binary codes. This
paper presents a graph-based unsupervised hashing model to preserve the neighborhood
structure of massive data in a discrete code space. We cast the graph
hashing problem into a discrete optimization framework which directly learns the
binary codes. A tractable alternating maximization algorithm is then proposed to
explicitly deal with the discrete constraints, yielding high-quality codes to well
capture the local neighborhoods. Extensive experiments performed on four large
datasets with up to one million samples show that our discrete optimization based
graph hashing method obtains superior search accuracy over state-of-the-art unsupervised
hashing methods, especially for longer codes.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/irie2014locally/">Locally Linear Hashing For Extracting Non-linear Manifolds</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Locally Linear Hashing For Extracting Non-linear Manifolds' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Locally Linear Hashing For Extracting Non-linear Manifolds' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Irie et al.</td> <!-- 🔧 You were missing this -->
    <td>2014 IEEE Conference on Computer Vision and Pattern Recognition</td>
    <td>94</td>
    <td><p>Previous efforts in hashing intend to preserve data variance
or pairwise affinity, but neither is adequate in capturing
the manifold structures hidden in most visual data. In
this paper, we tackle this problem by reconstructing the locally
linear structures of manifolds in the binary Hamming
space, which can be learned by locality-sensitive sparse
coding. We cast the problem as a joint minimization of
reconstruction error and quantization loss, and show that,
despite its NP-hardness, a local optimum can be obtained
efficiently via alternative optimization. Our method distinguishes
itself from existing methods in its remarkable ability
to extract the nearest neighbors of the query from the
same manifold, instead of from the ambient space. On extensive
experiments on various image benchmarks, our results
improve previous state-of-the-art by 28-74% typically,
and 627% on the Yale face data.</p>
</td>
    <td>
      
        Hashing Methods 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/wang2014composite/">Composite Quantization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Composite Quantization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Composite Quantization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Jingdong, Zhang Ting</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>197</td>
    <td><p>This paper studies the compact coding approach to approximate nearest
neighbor search. We introduce a composite quantization framework. It uses the
composition of several (\(M\)) elements, each of which is selected from a
different dictionary, to accurately approximate a \(D\)-dimensional vector, thus
yielding accurate search, and represents the data vector by a short code
composed of the indices of the selected elements in the corresponding
dictionaries. Our key contribution lies in introducing a near-orthogonality
constraint, which makes the search efficiency is guaranteed as the cost of the
distance computation is reduced to \(O(M)\) from \(O(D)\) through a distance table
lookup scheme. The resulting approach is called near-orthogonal composite
quantization. We theoretically justify the equivalence between near-orthogonal
composite quantization and minimizing an upper bound of a function formed by
jointly considering the quantization error and the search cost according to a
generalized triangle inequality. We empirically show the efficacy of the
proposed approach over several benchmark datasets. In addition, we demonstrate
the superior performances in other three applications: combination with
inverted multi-index, quantizing the query for mobile search, and inner-product
similarity search.</p>
</td>
    <td>
      
        Quantization 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/lin2014optimizing/">Optimizing Ranking Measures For Compact Binary Code Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Optimizing Ranking Measures For Compact Binary Code Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Optimizing Ranking Measures For Compact Binary Code Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin Guosheng, Shen, Wu.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>23</td>
    <td><p>Hashing has proven a valuable tool for large-scale information retrieval. Despite much success, existing hashing methods optimize over simple objectives such as the reconstruction error or graph Laplacian related loss functions, instead of the performance evaluation criteria of interest—multivariate performance measures such as the AUC and NDCG. Here we present a general framework (termed StructHash) that allows one to directly optimize multivariate performance measures.
The resulting optimization problem can involve exponentially or infinitely many variables and constraints, which is more challenging than standard structured output learning. To solve the StructHash optimization problem, we use a combination of column generation and cutting-plane techniques. We demonstrate the generality of StructHash by applying it to ranking prediction and image retrieval, and show that it outperforms a few state-of-the-art hashing methods.</p>
</td>
    <td>
      
        Compact Codes 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/lin2014fast/">Fast Supervised Hashing With Decision Trees For High-dimensional Data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Supervised Hashing With Decision Trees For High-dimensional Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Supervised Hashing With Decision Trees For High-dimensional Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin et al.</td> <!-- 🔧 You were missing this -->
    <td>2014 IEEE Conference on Computer Vision and Pattern Recognition</td>
    <td>453</td>
    <td><p>Supervised hashing aims to map the original features to
compact binary codes that are able to preserve label based
similarity in the Hamming space. Non-linear hash functions
have demonstrated their advantage over linear ones due to
their powerful generalization capability. In the literature,
kernel functions are typically used to achieve non-linearity
in hashing, which achieve encouraging retrieval performance at the price of slow evaluation and training time.
Here we propose to use boosted decision trees for achieving
non-linearity in hashing, which are fast to train and evaluate, hence more suitable for hashing with high dimensional
data. In our approach, we first propose sub-modular formulations for the hashing binary code inference problem
and an efficient GraphCut based block search method for
solving large-scale inference.
Then we learn hash functions by training boosted decision trees to fit the binary
codes. Experiments demonstrate that our proposed method
significantly outperforms most state-of-the-art methods in
retrieval precision and training time. Especially for highdimensional data, our method is orders of magnitude faster
than many methods in terms of training time.</p>
</td>
    <td>
      
        Unsupervised 
      
        CVPR 
      
        Neural Hashing 
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/zhu2014personalized/">Personalized Recommendation With Corrected Similarity</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Personalized Recommendation With Corrected Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Personalized Recommendation With Corrected Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhu Xuzhen, Tian Hui, Cai Shimin</td> <!-- 🔧 You were missing this -->
    <td>Journal of Statistical Mechanics: Theory and Experiment</td>
    <td>25</td>
    <td><p>Personalized recommendation attracts a surge of interdisciplinary researches.
Especially, similarity based methods in applications of real recommendation
systems achieve great success. However, the computations of similarities are
overestimated or underestimated outstandingly due to the defective strategy of
unidirectional similarity estimation. In this paper, we solve this drawback by
leveraging mutual correction of forward and backward similarity estimations,
and propose a new personalized recommendation index, i.e., corrected similarity
based inference (CSI). Through extensive experiments on four benchmark
datasets, the results show a greater improvement of CSI in comparison with
these mainstream baselines. And the detailed analysis is presented to unveil
and understand the origin of such difference between CSI and mainstream
indices.</p>
</td>
    <td>
      
        Recommender Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/zhang2014supervised/">Supervised Hashing With Latent Factor Models</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Hashing With Latent Factor Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Supervised Hashing With Latent Factor Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</td>
    <td>285</td>
    <td><p>Due to its low storage cost and fast query speed, hashing
has been widely adopted for approximate nearest neighbor
search in large-scale datasets. Traditional hashing methods
try to learn the hash codes in an unsupervised way where
the metric (Euclidean) structure of the training data is preserved.
Very recently, supervised hashing methods, which
try to preserve the semantic structure constructed from the
semantic labels of the training points, have exhibited higher
accuracy than unsupervised methods. In this paper, we
propose a novel supervised hashing method, called latent
factor hashing (LFH), to learn similarity-preserving binary
codes based on latent factor models. An algorithm with
convergence guarantee is proposed to learn the parameters
of LFH. Furthermore, a linear-time variant with stochastic
learning is proposed for training LFH on large-scale datasets.
Experimental results on two large datasets with semantic
labels show that LFH can achieve superior accuracy than
state-of-the-art methods with comparable training time.</p>
</td>
    <td>
      
        Text Retrieval 
      
        Unsupervised 
      
        Neural Hashing 
      
        SUPERVISED 
      
        SIGIR 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/zhang2014large/">Large-scale Supervised Multimodal Hashing With Semantic Correlation Maximization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Large-scale Supervised Multimodal Hashing With Semantic Correlation Maximization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Large-scale Supervised Multimodal Hashing With Semantic Correlation Maximization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang D., Li</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>619</td>
    <td><p>Due to its low storage cost and fast query speed, hashing
has been widely adopted for similarity search in multimedia
data. In particular, more and more attentions
have been payed to multimodal hashing for search in
multimedia data with multiple modalities, such as images
with tags. Typically, supervised information of semantic
labels is also available for the data points in
many real applications. Hence, many supervised multimodal
hashing (SMH) methods have been proposed
to utilize such semantic labels to further improve the
search accuracy. However, the training time complexity
of most existing SMH methods is too high, which
makes them unscalable to large-scale datasets. In this
paper, a novel SMH method, called semantic correlation
maximization (SCM), is proposed to seamlessly integrate
semantic labels into the hashing learning procedure
for large-scale data modeling. Experimental results
on two real-world datasets show that SCM can signifi-
cantly outperform the state-of-the-art SMH methods, in
terms of both accuracy and scalability.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
        AAAI 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/rippel2014learning/">Learning Ordered Representations With Nested Dropout</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Ordered Representations With Nested Dropout' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Ordered Representations With Nested Dropout' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Rippel Oren, Gelbart Michael A., Adams Ryan P.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>49</td>
    <td><p>In this paper, we study ordered representations of data in which different
dimensions have different degrees of importance. To learn these representations
we introduce nested dropout, a procedure for stochastically removing coherent
nested sets of hidden units in a neural network. We first present a sequence of
theoretical results in the simple case of a semi-linear autoencoder. We
rigorously show that the application of nested dropout enforces identifiability
of the units, which leads to an exact equivalence with PCA. We then extend the
algorithm to deep models and demonstrate the relevance of ordered
representations to a number of applications. Specifically, we use the ordered
property of the learned codes to construct hash-based data structures that
permit very fast retrieval, achieving retrieval in time logarithmic in the
database size and independent of the dimensionality of the representation. This
allows codes that are hundreds of times longer than currently feasible for
retrieval. We therefore avoid the diminished quality associated with short
codes, while still performing retrieval that is competitive in speed with
existing methods. We also show that ordered representations are a promising way
to learn adaptive compression for efficient online data reconstruction.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/xia2014supervised/">Supervised Hashing Via Image Representation Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Hashing Via Image Representation Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Supervised Hashing Via Image Representation Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xia et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>978</td>
    <td><p>Hashing is a popular approximate nearest neighbor
search approach for large-scale image retrieval.
Supervised hashing, which incorporates similarity/dissimilarity
information on entity pairs to improve
the quality of hashing function learning, has recently
received increasing attention. However, in the existing
supervised hashing methods for images, an input
image is usually encoded by a vector of hand-crafted
visual features. Such hand-crafted feature vectors
do not necessarily preserve the accurate semantic
similarities of images pairs, which may often degrade
the performance of hashing function learning. In this
paper, we propose a supervised hashing method for
image retrieval, in which we automatically learn a good
image representation tailored to hashing as well as a
set of hash functions. The proposed method has two
stages. In the first stage, given the pairwise similarity
matrix S over training images, we propose a scalable
coordinate descent method to decompose S into a
product of HHT where H is a matrix with each of its
rows being the approximate hash code associated to
a training image. In the second stage, we propose to
simultaneously learn a good feature representation for
the input images as well as a set of hash functions, via
a deep convolutional network tailored to the learned
hash codes in H and optionally the discrete class labels
of the images. Extensive empirical evaluations on three
benchmark datasets with different kinds of images
show that the proposed method has superior performance
gains over several state-of-the-art supervised
and unsupervised hashing methods.</p>
</td>
    <td>
      
        Unsupervised 
      
        Neural Hashing 
      
        AAAI 
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/yi2014deep/">Deep Metric Learning For Practical Person Re-identification</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Metric Learning For Practical Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Metric Learning For Practical Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yi Dong, Lei Zhen, Li Stan Z.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>143</td>
    <td><p>Various hand-crafted features and metric learning methods prevail in the
field of person re-identification. Compared to these methods, this paper
proposes a more general way that can learn a similarity metric from image
pixels directly. By using a “siamese” deep neural network, the proposed method
can jointly learn the color feature, texture feature and metric in a unified
framework. The network has a symmetry structure with two sub-networks which are
connected by Cosine function. To deal with the big variations of person images,
binomial deviance is used to evaluate the cost between similarities and labels,
which is proved to be robust to outliers.
  Compared to existing researches, a more practical setting is studied in the
experiments that is training and test on different datasets (cross dataset
person re-identification). Both in “intra dataset” and “cross dataset”
settings, the superiorities of the proposed method are illustrated on VIPeR and
PRID.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/shrivastava2014densifying/">Densifying One Permutation Hashing Via Rotation For Fast Near Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Densifying One Permutation Hashing Via Rotation For Fast Near Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Densifying One Permutation Hashing Via Rotation For Fast Near Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shrivastava A., Li</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>86</td>
    <td><p>The query complexity of locality sensitive hashing
(LSH) based similarity search is dominated
by the number of hash evaluations, and this number
grows with the data size (Indyk &amp; Motwani,
1998). In industrial applications such as search
where the data are often high-dimensional and
binary (e.g., text n-grams), minwise hashing is
widely adopted, which requires applying a large
number of permutations on the data. This is
costly in computation and energy-consumption.
In this paper, we propose a hashing technique
which generates all the necessary hash evaluations
needed for similarity search, using one
single permutation. The heart of the proposed
hash function is a “rotation” scheme which densifies
the sparse sketches of one permutation
hashing (Li et al., 2012) in an unbiased fashion
thereby maintaining the LSH property. This
makes the obtained sketches suitable for hash table
construction. This idea of rotation presented
in this paper could be of independent interest for
densifying other types of sparse sketches.
Using our proposed hashing method, the query
time of a (K, L)-parameterized LSH is reduced
from the typical O(dKL) complexity to merely
O(KL + dL), where d is the number of nonzeros
of the data vector, K is the number of hashes
in each hash table, and L is the number of hash
tables. Our experimental evaluation on real data
confirms that the proposed scheme significantly
reduces the query processing time over minwise
hashing without loss in retrieval accuracies.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/xiong2014adaptive/">Adaptive Quantization For Hashing: An Information-based Approach To Learning Binary Codes</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Adaptive Quantization For Hashing: An Information-based Approach To Learning Binary Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Adaptive Quantization For Hashing: An Information-based Approach To Learning Binary Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xiong et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2014 SIAM International Conference on Data Mining</td>
    <td>13</td>
    <td><p>Large-scale data mining and retrieval applications have
increasingly turned to compact binary data representations
as a way to achieve both fast queries and efficient
data storage; many algorithms have been proposed for
learning effective binary encodings. Most of these algorithms
focus on learning a set of projection hyperplanes
for the data and simply binarizing the result from each
hyperplane, but this neglects the fact that informativeness
may not be uniformly distributed across the projections.
In this paper, we address this issue by proposing
a novel adaptive quantization (AQ) strategy that
adaptively assigns varying numbers of bits to different
hyperplanes based on their information content. Our
method provides an information-based schema that preserves
the neighborhood structure of data points, and
we jointly find the globally optimal bit-allocation for
all hyperplanes. In our experiments, we compare with
state-of-the-art methods on four large-scale datasets
and find that our adaptive quantization approach significantly
improves on traditional hashing methods.</p>
</td>
    <td>
      
        Quantization 
      
        Hashing Methods 
      
        Compact Codes 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/csurka2014unsupervised/">Unsupervised Visual And Textual Information Fusion In Multimedia Retrieval - A Graph-based Point Of View</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Visual And Textual Information Fusion In Multimedia Retrieval - A Graph-based Point Of View' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Visual And Textual Information Fusion In Multimedia Retrieval - A Graph-based Point Of View' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Csurka Gabriela, Ah-pine Julien, Clinchant Stéphane</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>6</td>
    <td><p>Multimedia collections are more than ever growing in size and diversity.
Effective multimedia retrieval systems are thus critical to access these
datasets from the end-user perspective and in a scalable way. We are interested
in repositories of image/text multimedia objects and we study multimodal
information fusion techniques in the context of content based multimedia
information retrieval. We focus on graph based methods which have proven to
provide state-of-the-art performances. We particularly examine two of such
methods : cross-media similarities and random walk based scores. From a
theoretical viewpoint, we propose a unifying graph based framework which
encompasses the two aforementioned approaches. Our proposal allows us to
highlight the core features one should consider when using a graph based
technique for the combination of visual and textual information. We compare
cross-media and random walk based results using three different real-world
datasets. From a practical standpoint, our extended empirical analysis allow us
to provide insights and guidelines about the use of graph based methods for
multimodal information fusion in content based multimedia information
retrieval.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Graph Based ANN 
      
        Multimodal Retrieval 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/kang2014cross/">Cross-modal Similarity Learning : A Low Rank Bilinear Formulation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-modal Similarity Learning : A Low Rank Bilinear Formulation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-modal Similarity Learning : A Low Rank Bilinear Formulation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kang et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>The cross-media retrieval problem has received much attention in recent years
due to the rapid increasing of multimedia data on the Internet. A new approach
to the problem has been raised which intends to match features of different
modalities directly. In this research, there are two critical issues: how to
get rid of the heterogeneity between different modalities and how to match the
cross-modal features of different dimensions. Recently metric learning methods
show a good capability in learning a distance metric to explore the
relationship between data points. However, the traditional metric learning
algorithms only focus on single-modal features, which suffer difficulties in
addressing the cross-modal features of different dimensions. In this paper, we
propose a cross-modal similarity learning algorithm for the cross-modal feature
matching. The proposed method takes a bilinear formulation, and with the
nuclear-norm penalization, it achieves low-rank representation. Accordingly,
the accelerated proximal gradient algorithm is successfully imported to find
the optimal solution with a fast convergence rate O(1/t^2). Experiments on
three well known image-text cross-media retrieval databases show that the
proposed method achieves the best performance compared to the state-of-the-art
algorithms.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/liu2014collaborative/">Collaborative Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Collaborative Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Collaborative Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>2014 IEEE Conference on Computer Vision and Pattern Recognition</td>
    <td>130</td>
    <td><p>Hashing technique has become a promising approach for
fast similarity search. Most of existing hashing research
pursue the binary codes for the same type of entities by
preserving their similarities. In practice, there are many
scenarios involving nearest neighbor search on the data
given in matrix form, where two different types of, yet
naturally associated entities respectively correspond to its
two dimensions or views. To fully explore the duality
between the two views, we propose a collaborative hashing
scheme for the data in matrix form to enable fast search
in various applications such as image search using bag of
words and recommendation using user-item ratings. By
simultaneously preserving both the entity similarities in
each view and the interrelationship between views, our
collaborative hashing effectively learns the compact binary
codes and the explicit hash functions for out-of-sample
extension in an alternating optimization way. Extensive
evaluations are conducted on three well-known datasets
for search inside a single view and search across different
views, demonstrating that our proposed method outperforms
state-of-the-art baselines, with significant accuracy
gains ranging from 7.67% to 45.87% relatively.</p>
</td>
    <td>
      
        Hashing Methods 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/yu2014circulant/">Circulant Binary Embedding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Circulant Binary Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Circulant Binary Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yu et al.</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>132</td>
    <td><p>Binary embedding of high-dimensional data requires
long codes to preserve the discriminative
power of the input space. Traditional binary coding
methods often suffer from very high computation
and storage costs in such a scenario. To
address this problem, we propose Circulant Binary
Embedding (CBE) which generates binary
codes by projecting the data with a circulant matrix.
The circulant structure enables the use of
Fast Fourier Transformation to speed up the computation.
Compared to methods that use unstructured
matrices, the proposed method improves
the time complexity from O(d^2
) to O(d log d),
and the space complexity from O(d^2) to O(d)
where d is the input dimensionality. We also
propose a novel time-frequency alternating optimization
to learn data-dependent circulant projections,
which alternatively minimizes the objective
in original and Fourier domains. We show
by extensive experiments that the proposed approach
gives much better performance than the
state-of-the-art approaches for fixed time, and
provides much faster computation with no performance
degradation for fixed number of bits.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/ding2014collective/">Collective Matrix Factorization Hashing For Multimodal Data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Collective Matrix Factorization Hashing For Multimodal Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Collective Matrix Factorization Hashing For Multimodal Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ding G., Guo, Zhou</td> <!-- 🔧 You were missing this -->
    <td>2014 IEEE Conference on Computer Vision and Pattern Recognition</td>
    <td>638</td>
    <td><p>Nearest neighbor search methods based on hashing have
attracted considerable attention for effective and efficient
large-scale similarity search in computer vision and information
retrieval community. In this paper, we study the
problems of learning hash functions in the context of multimodal
data for cross-view similarity search. We put forward
a novel hashing method, which is referred to Collective
Matrix Factorization Hashing (CMFH). CMFH learns unified
hash codes by collective matrix factorization with latent
factor model from different modalities of one instance,
which can not only supports cross-view search but also increases
the search accuracy by merging multiple view information
sources. We also prove that CMFH, a similaritypreserving
hashing learning method, has upper and lower
boundaries. Extensive experiments verify that CMFH significantly
outperforms several state-of-the-art methods on
three different datasets.</p>
</td>
    <td>
      
        Hashing Methods 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/shrivastava2014asymmetric/">Asymmetric LSH (ALSH) For Sublinear Time Maximum Inner Product Search (MIPS).</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Asymmetric LSH (ALSH) For Sublinear Time Maximum Inner Product Search (MIPS).' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Asymmetric LSH (ALSH) For Sublinear Time Maximum Inner Product Search (MIPS).' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shrivastava A., Li</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>268</td>
    <td><p>We present the first provably sublinear time hashing algorithm for approximate
Maximum Inner Product Search (MIPS). Searching with (un-normalized) inner
product as the underlying similarity measure is a known difficult problem and
finding hashing schemes for MIPS was considered hard. While the existing Locality
Sensitive Hashing (LSH) framework is insufficient for solving MIPS, in this
paper we extend the LSH framework to allow asymmetric hashing schemes. Our
proposal is based on a key observation that the problem of finding maximum inner
products, after independent asymmetric transformations, can be converted into
the problem of approximate near neighbor search in classical settings. This key
observation makes efficient sublinear hashing scheme for MIPS possible. Under
the extended asymmetric LSH (ALSH) framework, this paper provides an example
of explicit construction of provably fast hashing scheme for MIPS. Our proposed
algorithm is simple and easy to implement. The proposed hashing scheme
leads to significant computational savings over the two popular conventional LSH
schemes: (i) Sign Random Projection (SRP) and (ii) hashing based on p-stable
distributions for L2 norm (L2LSH), in the collaborative filtering task of item recommendations
on Netflix and Movielens (10M) datasets.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/zhang2013binary/">Binary Code Ranking With Weighted Hamming Distance</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Binary Code Ranking With Weighted Hamming Distance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Binary Code Ranking With Weighted Hamming Distance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang et al.</td> <!-- 🔧 You were missing this -->
    <td>2013 IEEE Conference on Computer Vision and Pattern Recognition</td>
    <td>98</td>
    <td><p>Binary hashing has been widely used for efficient similarity search due to its query and storage efficiency. In most
existing binary hashing methods, the high-dimensional data are embedded into Hamming space and the distance or
similarity of two points are approximated by the Hamming
distance between their binary codes. The Hamming distance calculation is efficient, however, in practice, there are
often lots of results sharing the same Hamming distance to
a query, which makes this distance measure ambiguous and
poses a critical issue for similarity search where ranking is
important. In this paper, we propose a weighted Hamming
distance ranking algorithm (WhRank) to rank the binary
codes of hashing methods. By assigning different bit-level
weights to different hash bits, the returned binary codes
are ranked at a finer-grained binary code level. We give
an algorithm to learn the data-adaptive and query-sensitive
weight for each hash bit. Evaluations on two large-scale
image data sets demonstrate the efficacy of our weighted
Hamming distance for binary code ranking.</p>
</td>
    <td>
      
        Compact Codes 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/xu2013harmonious/">Harmonious Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Harmonious Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Harmonious Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xu et al.</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>49</td>
    <td><p>Hashing-based fast nearest neighbor search technique
has attracted great attention in both research
and industry areas recently. Many existing hashing
approaches encode data with projection-based hash
functions and represent each projected dimension
by 1-bit. However, the dimensions with high variance
hold large energy or information of data but
treated equivalently as dimensions with low variance,
which leads to a serious information loss. In
this paper, we introduce a novel hashing algorithm
called Harmonious Hashing which aims at learning
hash functions with low information loss. Specifically,
we learn a set of optimized projections to
preserve the maximum cumulative energy and meet
the constraint of equivalent variance on each dimension
as much as possible. In this way, we could
minimize the information loss after binarization.
Despite the extreme simplicity, our method outperforms
superiorly to many state-of-the-art hashing
methods in large-scale and high-dimensional nearest
neighbor search experiments.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/moran2013variable/">Variable Bit Quantisation For LSH</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Variable Bit Quantisation For LSH' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Variable Bit Quantisation For LSH' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Moran S., Lavrenko, Osborne</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>21</td>
    <td><p>We introduce a scheme for optimally allocating
a variable number of bits per
LSH hyperplane. Previous approaches assign
a constant number of bits per hyperplane.
This neglects the fact that a subset
of hyperplanes may be more informative
than others. Our method, dubbed Variable
Bit Quantisation (VBQ), provides a datadriven
non-uniform bit allocation across
hyperplanes. Despite only using a fraction
of the available hyperplanes, VBQ outperforms
uniform quantisation by up to 168%
for retrieval across standard text and image
datasets.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Quantization 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/zadeh2013dimension/">Dimension Independent Similarity Computation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Dimension Independent Similarity Computation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Dimension Independent Similarity Computation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zadeh Reza Bosagh, Goel Ashish</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>53</td>
    <td><p>We present a suite of algorithms for Dimension Independent Similarity
Computation (DISCO) to compute all pairwise similarities between very high
dimensional sparse vectors. All of our results are provably independent of
dimension, meaning apart from the initial cost of trivially reading in the
data, all subsequent operations are independent of the dimension, thus the
dimension can be very large. We study Cosine, Dice, Overlap, and the Jaccard
similarity measures. For Jaccard similiarity we include an improved version of
MinHash. Our results are geared toward the MapReduce framework. We empirically
validate our theorems at large scale using data from the social networking site
Twitter. At time of writing, our algorithms are live in production at
twitter.com.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/lin2013general/">A General Two-step Approach To Learning-based Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A General Two-step Approach To Learning-based Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A General Two-step Approach To Learning-based Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin et al.</td> <!-- 🔧 You were missing this -->
    <td>2013 IEEE International Conference on Computer Vision</td>
    <td>199</td>
    <td><p>Most existing approaches to hashing apply a single form of hash function, and an optimization process which
is typically deeply coupled to this specific form. This tight coupling restricts the flexibility of the method to
respond to the data, and can result in complex optimization problems that are difficult to solve. Here we propose
a flexible yet simple framework that is able to accommodate different types of loss functions and hash functions.
This framework allows a number of existing approaches to hashing to be placed in context, and simplifies the
development of new problem-specific hashing methods. Our framework decomposes hashing learning problem
into two steps: hash bit learning and hash function learning based on the learned bits. The first step can typically
be formulated as binary quadratic problems, and the second step can be accomplished by training standard binary
classifiers. Both problems have been extensively studied in the literature. Our extensive experiments demonstrate
that the proposed framework is effective, flexible and outperforms the state-of-the-art.</p>
</td>
    <td>
      
        Hashing Methods 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/fan2013supervised/">Supervised Binary Hash Code Learning With Jensen Shannon Divergence</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Binary Hash Code Learning With Jensen Shannon Divergence' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Supervised Binary Hash Code Learning With Jensen Shannon Divergence' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fan Lixin</td> <!-- 🔧 You were missing this -->
    <td>2013 IEEE International Conference on Computer Vision</td>
    <td>15</td>
    <td><p>This paper proposes to learn binary hash codes within
a statistical learning framework, in which an upper bound
of the probability of Bayes decision errors is derived for
different forms of hash functions and a rigorous proof of
the convergence of the upper bound is presented. Consequently, minimizing such an upper bound leads to consistent
performance improvements of existing hash code learning
algorithms, regardless of whether original algorithms are
unsupervised or supervised. This paper also illustrates a
fast hash coding method that exploits simple binary tests to
achieve orders of magnitude improvement in coding speed
as compared to projection based methods.</p>
</td>
    <td>
      
        SUPERVISED 
      
        Hashing Methods 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/zhu2013linear/">Linear Cross-modal Hashing For Efficient Multimedia Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Linear Cross-modal Hashing For Efficient Multimedia Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Linear Cross-modal Hashing For Efficient Multimedia Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhu et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 21st ACM international conference on Multimedia</td>
    <td>278</td>
    <td><p>Most existing cross-modal hashing methods suffer from the scalability issue in the training phase. In this paper, we propose a novel 
cross-modal hashing approach with a linear time complexity to the training data size, to enable scalable indexing for multimedia 
search across multiple modals. Taking both the intra-similarity in each modal and the inter-similarity across different modals 
into consideration, the proposed approach aims at effectively learning hash functions from large-scale training datasets. 
More specifically, for each modal, we first partition the training data into \(k\) clusters and then represent each training data 
point with its distances to \(k\) centroids of the clusters. Interestingly, such a k-dimensional data representation can reduce 
the time complexity of the training phase from traditional O(n2) or higher to O(n), where \(n\) is the training data size, leading to 
practical learning on large-scale datasets. We further prove that this new representation preserves the intra-similarity in each modal. 
To preserve the inter-similarity among data points across different modals, we transform the derived data representations into a 
common binary subspace in which binary codes from all the modals are “consistent” and comparable. The transformation simultaneously 
outputs the hash functions for all modals, which are used to convert unseen data into binary codes. Given a query of one modal, 
it is first mapped into the binary codes using the modal’s hash functions, followed by matching the database binary codes of any other 
modals. Experimental results on two benchmark datasets confirm the scalability and the effectiveness of the proposed approach in 
comparison with the state of the art.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/jin2013complementary/">Complementary Projection Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Complementary Projection Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Complementary Projection Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jin et al.</td> <!-- 🔧 You were missing this -->
    <td>2013 IEEE International Conference on Computer Vision</td>
    <td>59</td>
    <td><p>Recently, hashing techniques have been widely applied
to solve the approximate nearest neighbors search problem
in many vision applications. Generally, these hashing
approaches generate 2^c buckets, where c is the length
of the hash code. A good hashing method should satisfy
the following two requirements: 1) mapping the nearby
data points into the same bucket or nearby (measured by
the Hamming distance) buckets. 2) all the data points are
evenly distributed among all the buckets. In this paper,
we propose a novel algorithm named Complementary Projection
Hashing (CPH) to find the optimal hashing functions
which explicitly considers the above two requirements.
Specifically, CPH aims at sequentially finding a series of hyperplanes
(hashing functions) which cross the sparse region
of the data. At the same time, the data points are evenly distributed
in the hypercubes generated by these hyperplanes.
The experiments comparing with the state-of-the-art hashing
methods demonstrate the effectiveness of the proposed
method.</p>
</td>
    <td>
      
        Hashing Methods 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/neyshabur2013power/">The Power Of Asymmetry In Binary Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=The Power Of Asymmetry In Binary Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=The Power Of Asymmetry In Binary Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Neyshabur B., Salakhutdinov, Srebro</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>57</td>
    <td><p>When approximating binary similarity using the hamming distance between short
binary hashes, we show that even if the similarity is symmetric, we can have
shorter and more accurate hashes by using two distinct code maps. I.e. by approximating the similarity between x and x
0
as the hamming distance between f(x)
and g(x0), for two distinct binary codes f, g, rather than as the hamming distance
between f(x) and f(x0).</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/song2013inter/">Inter-media Hashing For Large-scale Retrieval From Heterogeneous Data Sources</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Inter-media Hashing For Large-scale Retrieval From Heterogeneous Data Sources' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Inter-media Hashing For Large-scale Retrieval From Heterogeneous Data Sources' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Song et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data</td>
    <td>619</td>
    <td><p>In this paper, we present a new multimedia retrieval paradigm to innovate large-scale search of heterogenous multimedia data. It is able to return results of different media types from heterogeneous data sources, e.g., using a query image to retrieve relevant text documents or images from different data sources. This utilizes the widely available data from different sources and caters for the current users’ demand of receiving a result list simultaneously containing multiple types of data to obtain a comprehensive understanding of the query’s results. To enable large-scale inter-media retrieval, we propose a novel inter-media hashing (IMH) model to explore the correlations among multiple media types from different data sources and tackle the scalability issue. To this end, multimedia data from heterogeneous data sources are transformed into a common Hamming space, in which fast search can be easily implemented by XOR and bit-count operations. Furthermore, we integrate a linear regression model to learn hashing functions so that the hash codes for new data points can be efficiently generated. Experiments conducted on real-world large-scale multimedia datasets demonstrate the superiority of our proposed method compared with state-of-the-art techniques.</p>
</td>
    <td>
      
        Hashing Methods 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/liu2013hash/">Hash Bit Selection: A Unified Solution For Selection Problems In Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hash Bit Selection: A Unified Solution For Selection Problems In Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hash Bit Selection: A Unified Solution For Selection Problems In Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>2013 IEEE Conference on Computer Vision and Pattern Recognition</td>
    <td>91</td>
    <td><p>Hashing based methods recently have been shown promising for large-scale nearest neighbor search. However, good designs involve difficult decisions of many unknowns – data features, hashing algorithms, parameter settings, kernels, etc. In this paper, we provide a unified solution as hash bit selection, i.e., selecting the most informative hash bits from a pool of candidates that may have been generated under various conditions mentioned above. We represent the candidate bit pool as a vertex- and edge-weighted graph with the pooled bits as vertices. Then we formulate the bit selection problem as quadratic programming over the graph, and solve it efficiently by replicator dynamics. Extensive experiments show that our bit selection approach can achieve superior performance over both naive selection methods and state-of-the-art methods under each scenario, usually with significant accuracy gains from 10% to 50% relatively.</p>
</td>
    <td>
      
        Hashing Methods 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/gong2013learning/">Learning Binary Codes For High-dimensional Data Using Bilinear Projections</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Binary Codes For High-dimensional Data Using Bilinear Projections' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Binary Codes For High-dimensional Data Using Bilinear Projections' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gong et al.</td> <!-- 🔧 You were missing this -->
    <td>2013 IEEE Conference on Computer Vision and Pattern Recognition</td>
    <td>183</td>
    <td><p>Recent advances in visual recognition indicate that to
achieve good retrieval and classification accuracy on largescale
datasets like ImageNet, extremely high-dimensional
visual descriptors, e.g., Fisher Vectors, are needed. We
present a novel method for converting such descriptors to
compact similarity-preserving binary codes that exploits
their natural matrix structure to reduce their dimensionality
using compact bilinear projections instead of a single
large projection matrix. This method achieves comparable
retrieval and classification accuracy to the original descriptors
and to the state-of-the-art Product Quantization
approach while having orders of magnitude faster code generation
time and smaller memory footprint.</p>
</td>
    <td>
      
        Compact Codes 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/li2013learning/">Learning Hash Functions Using Column Generation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Hash Functions Using Column Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Hash Functions Using Column Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li et al.</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>93</td>
    <td><p>Fast nearest neighbor searching is becoming
an increasingly important tool in solving
many large-scale problems. Recently
a number of approaches to learning datadependent
hash functions have been developed.
In this work, we propose a column
generation based method for learning datadependent
hash functions on the basis of
proximity comparison information. Given a
set of triplets that encode the pairwise proximity
comparison information, our method
learns hash functions that preserve the relative
comparison relationships in the data
as well as possible within the large-margin
learning framework. The learning procedure
is implemented using column generation and
hence is named CGHash. At each iteration
of the column generation procedure, the best
hash function is selected. Unlike most other
hashing methods, our method generalizes to
new data points naturally; and has a training
objective which is convex, thus ensuring
that the global optimum can be identi-
fied. Experiments demonstrate that the proposed
method learns compact binary codes
and that its retrieval performance compares
favorably with state-of-the-art methods when
tested on a few benchmark datasets.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/sundaram2013streaming/">Streaming Similarity Search Over One Billion Tweets Using Parallel Locality-sensitive Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Streaming Similarity Search Over One Billion Tweets Using Parallel Locality-sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Streaming Similarity Search Over One Billion Tweets Using Parallel Locality-sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sundaram et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the VLDB Endowment</td>
    <td>105</td>
    <td><p>Finding nearest neighbors has become an important operation on databases, with applications to text search, multimedia indexing,
and many other areas. One popular algorithm for similarity search, especially for high dimensional data (where spatial indexes like kdtrees do not perform well) is Locality Sensitive Hashing (LSH), an
approximation algorithm for finding similar objects. In this paper, we describe a new variant of LSH, called Parallel
LSH (PLSH) designed to be extremely efficient, capable of scaling out on multiple nodes and multiple cores, and which supports highthroughput streaming of new data. Our approach employs several
novel ideas, including: cache-conscious hash table layout, using a 2-level merge algorithm for hash table construction; an efficient
algorithm for duplicate elimination during hash-table querying; an insert-optimized hash table structure and efficient data expiration
algorithm for streaming data; and a performance model that accurately estimates performance of the algorithm and can be used to
optimize parameter settings. We show that on a workload where we perform similarity search on a dataset of &gt; 1 Billion tweets, with
hundreds of millions of new tweets per day, we can achieve query times of 1–2.5 ms. We show that this is an order of magnitude faster
than existing indexing schemes, such as inverted indexes. To the best of our knowledge, this is the fastest implementation of LSH,
with table construction times up to 3.7x faster and query times that are 8.3x faster than a basic implementation.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Similarity Search 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/moran2013neighbourhood/">Neighbourhood Preserving Quantisation For LSH</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Neighbourhood Preserving Quantisation For LSH' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Neighbourhood Preserving Quantisation For LSH' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Moran S., Lavrenko, Osborne</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval</td>
    <td>22</td>
    <td><p>We introduce a scheme for optimally allocating multiple bits per hyperplane for Locality Sensitive Hashing (LSH). Existing approaches binarise LSH projections by thresholding at zero yielding a single bit per dimension. We demonstrate that this is a sub-optimal bit allocation approach that can easily destroy the neighbourhood structure in the original feature space. Our proposed method, dubbed Neighbourhood Preserving Quantization (NPQ), assigns multiple bits per hyperplane based upon adaptively learned thresholds. NPQ exploits a pairwise affinity matrix to discretise each dimension such that nearest neighbours in the original feature space fall within the same quantisation thresholds and are therefore assigned identical bits. NPQ is not only applicable to LSH, but can also be applied to any low-dimensional projection scheme. Despite using half the number of hyperplanes, NPQ is shown to improve LSH-based retrieval accuracy by up to 65% compared to the state-of-the-art.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        SIGIR 
      
        Text Retrieval 
      
        Quantization 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2012</td>
    <td>
      <a href="/publications/kong2012isotropic/">Isotropic Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Isotropic Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Isotropic Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kong W., Li</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>260</td>
    <td><p>Most existing hashing methods adopt some projection functions to project the original data into several dimensions of real values, and then each of these projected dimensions is quantized into one bit (zero or one) by thresholding. Typically, the variances of different projected dimensions are different for existing projection functions such as principal component analysis (PCA). Using the same number of bits for different projected dimensions is unreasonable because larger-variance dimensions will carry more information. Although this viewpoint has been widely accepted by many researchers, it is still not verified by either theory or experiment because no methods have been proposed to find a projection with equal variances for different dimensions. In this paper, we propose a novel method, called isotropic hashing (IsoHash), to learn projection functions which can produce projected dimensions with isotropic variances (equal variances). Experimental results on real data sets show that IsoHash can outperform its counterpart with different variances for different dimensions, which verifies the viewpoint that projections with isotropic variances will be better than those with anisotropic variances.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2012</td>
    <td>
      <a href="/publications/heo2012spherical/">Spherical Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Spherical Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Spherical Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Heo et al.</td> <!-- 🔧 You were missing this -->
    <td>2012 IEEE Conference on Computer Vision and Pattern Recognition</td>
    <td>380</td>
    <td><p>Many binary code encoding schemes based on hashing
have been actively studied recently, since they can provide
efficient similarity search, especially nearest neighbor
search, and compact data representations suitable for handling
large scale image databases in many computer vision
problems. Existing hashing techniques encode highdimensional
data points by using hyperplane-based hashing
functions. In this paper we propose a novel hyperspherebased
hashing function, spherical hashing, to map more
spatially coherent data points into a binary code compared
to hyperplane-based hashing functions. Furthermore, we
propose a new binary code distance function, spherical
Hamming distance, that is tailored to our hyperspherebased
binary coding scheme, and design an efficient iterative
optimization process to achieve balanced partitioning
of data points for each hash function and independence between
hashing functions. Our extensive experiments show
that our spherical hashing technique significantly outperforms
six state-of-the-art hashing techniques based on hyperplanes
across various image benchmarks of sizes ranging
from one to 75 million of GIST descriptors. The performance
gains are consistent and large, up to 100% improvements.
The excellent results confirm the unique merits of
the proposed idea in using hyperspheres to encode proximity
regions in high-dimensional spaces. Finally, our method
is intuitive and easy to implement.</p>
</td>
    <td>
      
        Hashing Methods 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2012</td>
    <td>
      <a href="/publications/gong2012iterative/">Iterative Quantization: A Procrustean Approach To Learning Binary Codes</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Iterative Quantization: A Procrustean Approach To Learning Binary Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Iterative Quantization: A Procrustean Approach To Learning Binary Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gong Y., Lazebnik</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>1834</td>
    <td><p>This paper addresses the problem of learning similarity preserving binary codes for efficient retrieval in large-scale image collections. We propose a simple and efficient alternating minimization scheme for finding a rotation of zerocentered data so as to minimize the quantization error of
mapping this data to the vertices of a zero-centered binary
hypercube. This method, dubbed iterative quantization
(ITQ), has connections to multi-class spectral clustering
and to the orthogonal Procrustes problem, and it can be
used both with unsupervised data embeddings such as PCA
and supervised embeddings such as canonical correlation
analysis (CCA). Our experiments show that the resulting
binary coding schemes decisively outperform several other
state-of-the-art methods.</p>
</td>
    <td>
      
        Quantization 
      
        Compact Codes 
      
    </td>
    </tr>      
    
     <tr>
  <td>2012</td>
    <td>
      <a href="/publications/kong2012manhattan/">Manhattan Hashing For Large-scale Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Manhattan Hashing For Large-scale Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Manhattan Hashing For Large-scale Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kong W., Li, Guo</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval</td>
    <td>105</td>
    <td><p>Hashing is used to learn binary-code representation for data with
expectation of preserving the neighborhood structure in the original
feature space. Due to its fast query speed and reduced storage
cost, hashing has been widely used for efficient nearest neighbor
search in a large variety of applications like text and image retrieval.
Most existing hashing methods adopt Hamming distance to
measure the similarity (neighborhood) between points in the hashcode
space. However, one problem with Hamming distance is that
it may destroy the neighborhood structure in the original feature
space, which violates the essential goal of hashing. In this paper,
Manhattan hashing (MH), which is based on Manhattan distance, is
proposed to solve the problem of Hamming distance based hashing.
The basic idea of MH is to encode each projected dimension with
multiple bits of natural binary code (NBC), based on which the
Manhattan distance between points in the hashcode space is calculated
for nearest neighbor search. MH can effectively preserve the
neighborhood structure in the data to achieve the goal of hashing.
To the best of our knowledge, this is the first work to adopt Manhattan
distance with NBC for hashing. Experiments on several largescale
image data sets containing up to one million points show that
our MH method can significantly outperform other state-of-the-art
methods.</p>
</td>
    <td>
      
        Text Retrieval 
      
        Image Retrieval 
      
        SIGIR 
      
        Hashing Methods 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2012</td>
    <td>
      <a href="/publications/weiss2012multidimensional/">Multidimensional Spectral Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multidimensional Spectral Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multidimensional Spectral Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Weiss Y., Fergus, Torralba</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>161</td>
    <td><p>en a surge of interest in methods based on “semantic hashing”,
i.e. compact binary codes of data-points so that the Hamming distance
between codewords correlates with similarity. In reviewing and
comparing existing methods, we show that their relative performance can
change drastically depending on the definition of ground-truth neighbors.
Motivated by this finding, we propose a new formulation for learning binary
codes which seeks to reconstruct the affinity between datapoints,
rather than their distances. We show that this criterion is intractable
to solve exactly, but a spectral relaxation gives an algorithm where the
bits correspond to thresholded eigenvectors of the affinity matrix, and
as the number of datapoints goes to infinity these eigenvectors converge
to eigenfunctions of Laplace-Beltrami operators, similar to the recently
proposed Spectral Hashing (SH) method. Unlike SH whose performance
may degrade as the number of bits increases, the optimal code using
our formulation is guaranteed to faithfully reproduce the affinities as
the number of bits increases. We show that the number of eigenfunctions
needed may increase exponentially with dimension, but introduce a “kernel
trick” to allow us to compute with an exponentially large number of
bits but using only memory and computation that grows linearly with
dimension. Experiments shows that MDSH outperforms the state-of-the
art, especially in the challenging regime of small distance thresholds.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2012</td>
    <td>
      <a href="/publications/zhen2012co/">Co-regularized Hashing For Multimodal Data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Co-regularized Hashing For Multimodal Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Co-regularized Hashing For Multimodal Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhen Y., Yeung</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>195</td>
    <td><p>Hashing-based methods provide a very promising approach to large-scale similarity
search. To obtain compact hash codes, a recent trend seeks to learn the hash
functions from data automatically. In this paper, we study hash function learning
in the context of multimodal data. We propose a novel multimodal hash function
learning method, called Co-Regularized Hashing (CRH), based on a boosted coregularization
framework. The hash functions for each bit of the hash codes are
learned by solving DC (difference of convex functions) programs, while the learning
for multiple bits proceeds via a boosting procedure so that the bias introduced
by the hash functions can be sequentially minimized. We empirically compare
CRH with two state-of-the-art multimodal hash function learning methods on two
publicly available data sets.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2012</td>
    <td>
      <a href="/publications/norouzi2012hamming/">Hamming Distance Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hamming Distance Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hamming Distance Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Norouzi M., Fleet, Salakhutdinov</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>540</td>
    <td><p>Motivated by large-scale multimedia applications we propose to learn mappings
from high-dimensional data to binary codes that preserve semantic similarity.
Binary codes are well suited to large-scale applications as they are storage efficient and permit exact sub-linear kNN search. The framework is applicable
to broad families of mappings, and uses a flexible form of triplet ranking loss.
We overcome discontinuous optimization of the discrete mappings by minimizing
a piecewise-smooth upper bound on empirical loss, inspired by latent structural
SVMs. We develop a new loss-augmented inference algorithm that is quadratic in
the code length. We show strong retrieval performance on CIFAR-10 and MNIST,
with promising classification results using no more than kNN on the binary codes.</p>
</td>
    <td>
      
        Distance Metric Learning 
      
    </td>
    </tr>      
    
     <tr>
  <td>2012</td>
    <td>
      <a href="/publications/grauman2012learning/">Learning Binary Hash Codes For Large-scale Image Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Binary Hash Codes For Large-scale Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Binary Hash Codes For Large-scale Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Grauman Kristen, Fergus</td> <!-- 🔧 You were missing this -->
    <td>Studies in Computational Intelligence</td>
    <td>102</td>
    <td><p>Algorithms to rapidly search massive image or video collections are critical for many vision applications, including visual search, content-based retrieval, and non-parametric models for object recognition. Recent work shows that learned binary projections are a powerful way to index large collections according to their content. The basic idea is to formulate the projections so as to approximately preserve a given similarity function of interest. Having done so, one can then search the data efficiently using hash tables, or by exploring the Hamming ball volume around a novel query. Both enable sub-linear time retrieval with respect to the database size. Further, depending on the design of the projections, in some cases it is possible to bound the number of database examples that must be searched in order to achieve a given level of accuracy.</p>

<p>This chapter overviews data structures for fast search with binary codes, and then describes several supervised and unsupervised strategies for generating the codes. In particular, we review supervised methods that integrate metric learning, boosting, and neural networks into the hash key construction, and unsupervised methods based on spectral analysis or kernelized random projections that compute affinity-preserving binary codes.Whether learning from explicit semantic supervision or exploiting the structure among unlabeled data, these methods make scalable retrieval possible for a variety of robust visual similarity measures.We focus on defining the algorithms, and illustrate the main points with results using millions of images.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Image Retrieval 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2012</td>
    <td>
      <a href="/publications/liu2012supervised/">Supervised Hashing With Kernels</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Hashing With Kernels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Supervised Hashing With Kernels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>2012 IEEE Conference on Computer Vision and Pattern Recognition</td>
    <td>1447</td>
    <td><p>Recent years have witnessed the growing popularity of
hashing in large-scale vision problems. It has been shown
that the hashing quality could be boosted by leveraging supervised
information into hash function learning. However,
the existing supervised methods either lack adequate performance
or often incur cumbersome model training. In this
paper, we propose a novel kernel-based supervised hashing
model which requires a limited amount of supervised information,
i.e., similar and dissimilar data pairs, and a feasible
training cost in achieving high quality hashing. The idea
is to map the data to compact binary codes whose Hamming
distances are minimized on similar pairs and simultaneously
maximized on dissimilar pairs. Our approach is
distinct from prior works by utilizing the equivalence between
optimizing the code inner products and the Hamming
distances. This enables us to sequentially and efficiently
train the hash functions one bit at a time, yielding very
short yet discriminative codes. We carry out extensive experiments
on two image benchmarks with up to one million
samples, demonstrating that our approach significantly outperforms
the state-of-the-arts in searching both metric distance
neighbors and semantically similar neighbors, with
accuracy gains ranging from 13% to 46%.</p>
</td>
    <td>
      
        Unsupervised 
      
        CVPR 
      
        Neural Hashing 
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2012</td>
    <td>
      <a href="/publications/petrovic2012using/">Using Paraphrases For Improving First Story Detection In News And Twitter</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Using Paraphrases For Improving First Story Detection In News And Twitter' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Using Paraphrases For Improving First Story Detection In News And Twitter' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Petrovic S., Osborne, Lavrenko</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>91</td>
    <td><p>First story detection (FSD) involves identifying
first stories about events from a continuous
stream of documents. A major problem in this
task is the high degree of lexical variation in
documents which makes it very difficult to detect
stories that talk about the same event but
expressed using different words. We suggest
using paraphrases to alleviate this problem,
making this the first work to use paraphrases
for FSD. We show a novel way of integrating
paraphrases with locality sensitive hashing
(LSH) in order to obtain an efficient FSD system
that can scale to very large datasets. Our
system achieves state-of-the-art results on the
first story detection task, beating both the best
supervised and unsupervised systems. To test
our approach on large data, we construct a corpus
of events for Twitter, consisting of 50 million
documents, and show that paraphrasing is
also beneficial in this domain.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2011</td>
    <td>
      <a href="/publications/liu2011hashing/">Hashing With Graphs</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing With Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing With Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu et al.</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>861</td>
    <td><p>Hashing is becoming increasingly popular for
efficient nearest neighbor search in massive
databases. However, learning short codes
that yield good search performance is still
a challenge. Moreover, in many cases realworld
data lives on a low-dimensional manifold,
which should be taken into account
to capture meaningful nearest neighbors. In
this paper, we propose a novel graph-based
hashing method which automatically discovers
the neighborhood structure inherent in
the data to learn appropriate compact codes.
To make such an approach computationally
feasible, we utilize Anchor Graphs to obtain
tractable low-rank adjacency matrices. Our
formulation allows constant time hashing of a
new data point by extrapolating graph Laplacian
eigenvectors to eigenfunctions. Finally,
we describe a hierarchical threshold learning
procedure in which each eigenfunction yields
multiple bits, leading to higher search accuracy.
Experimental comparison with the
other state-of-the-art methods on two large
datasets demonstrates the efficacy of the proposed
method.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2011</td>
    <td>
      <a href="/publications/joly2011random/">Random Maximum Margin Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Random Maximum Margin Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Random Maximum Margin Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Joly A., Buisson</td> <!-- 🔧 You were missing this -->
    <td>CVPR 2011</td>
    <td>150</td>
    <td><p>Following the success of hashing methods for multidimensional
indexing, more and more works are interested
in embedding visual feature space in compact hash codes.
Such approaches are not an alternative to using index structures
but a complementary way to reduce both the memory
usage and the distance computation cost. Several data
dependent hash functions have notably been proposed to
closely fit data distribution and provide better selectivity
than usual random projections such as LSH. However, improvements
occur only for relatively small hash code sizes
up to 64 or 128 bits. As discussed in the paper, this is mainly
due to the lack of independence between the produced hash
functions. We introduce a new hash function family that
attempts to solve this issue in any kernel space. Rather
than boosting the collision probability of close points, our
method focus on data scattering. By training purely random
splits of the data, regardless the closeness of the training
samples, it is indeed possible to generate consistently
more independent hash functions. On the other side, the
use of large margin classifiers allows to maintain good generalization
performances. Experiments show that our new
Random Maximum Margin Hashing scheme (RMMH) outperforms
four state-of-the-art hashing methods, notably in
kernel spaces.</p>
</td>
    <td>
      
        Hashing Methods 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2011</td>
    <td>
      <a href="/publications/norouzi2011minimal/">Minimal Loss Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Minimal Loss Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Minimal Loss Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Norouzi M., Fleet</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>730</td>
    <td><p>We propose a method for learning similaritypreserving
hash functions that map highdimensional
data onto binary codes. The
formulation is based on structured prediction
with latent variables and a hinge-like
loss function. It is efficient to train for large
datasets, scales well to large code lengths,
and outperforms state-of-the-art methods.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2011</td>
    <td>
      <a href="/publications/zhang2011composite/">Composite Hashing With Multiple Information Sources</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Composite Hashing With Multiple Information Sources' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Composite Hashing With Multiple Information Sources' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang D., Wang, Si</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval</td>
    <td>221</td>
    <td><p>Similarity search applications with a large amount of text
and image data demands an efficient and effective solution.
One useful strategy is to represent the examples in databases
as compact binary codes through semantic hashing, which
has attracted much attention due to its fast query/search
speed and drastically reduced storage requirement. All of
the current semantic hashing methods only deal with the
case when each example is represented by one type of features.
However, examples are often described from several
different information sources in many real world applications.
For example, the characteristics of a webpage can be
derived from both its content part and its associated links.
To address the problem of learning good hashing codes in
this scenario, we propose a novel research problem – Composite
Hashing with Multiple Information Sources (CHMIS).
The focus of the new research problem is to design an algorithm
for incorporating the features from different information
sources into the binary hashing codes efficiently and
effectively. In particular, we propose an algorithm CHMISAW
(CHMIS with Adjusted Weights) for learning the codes.
The proposed algorithm integrates information from several
different sources into the binary hashing codes by adjusting
the weights on each individual source for maximizing
the coding performance, and enables fast conversion from
query examples to their binary hashing codes. Experimental
results on five different datasets demonstrate the superior
performance of the proposed method against several other
state-of-the-art semantic hashing techniques.</p>
</td>
    <td>
      
        SIGIR 
      
        Hashing Methods 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2011</td>
    <td>
      <a href="/publications/kumar2011learning/">Learning Hash Functions For Cross-view Similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Hash Functions For Cross-view Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Hash Functions For Cross-view Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kumar S., Udupa</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>438</td>
    <td><p>Many applications in Multilingual and Multimodal
Information Access involve searching large
databases of high dimensional data objects with
multiple (conditionally independent) views. In this
work we consider the problem of learning hash
functions for similarity search across the views
for such applications. We propose a principled
method for learning a hash function for each view
given a set of multiview training data objects. The
hash functions map similar objects to similar codes
across the views thus enabling cross-view similarity
search. We present results from an extensive
empirical study of the proposed approach
which demonstrate its effectiveness on Japanese
language People Search and Multilingual People
Search problems.</p>
</td>
    <td>
      
        Similarity Search 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2010</td>
    <td>
      <a href="/publications/kato2010solving/">Solving \(k\)-nearest Neighbor Problem On Multiple Graphics Processors</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Solving \(k\)-nearest Neighbor Problem On Multiple Graphics Processors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Solving \(k\)-nearest Neighbor Problem On Multiple Graphics Processors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kato Kimikazu, Hosino Tikara</td> <!-- 🔧 You were missing this -->
    <td>2010 10th IEEE/ACM International Conference on Cluster, Cloud and Grid Computing</td>
    <td>37</td>
    <td><p>The recommendation system is a software system to predict customers’ unknown
preferences from known preferences. In the recommendation system, customers’
preferences are encoded into vectors, and finding the nearest vectors to each
vector is an essential part. This vector-searching part of the problem is
called a \(k\)-nearest neighbor problem. We give an effective algorithm to solve
this problem on multiple graphics processor units (GPUs).
  Our algorithm consists of two parts: an \(N\)-body problem and a partial sort.
For a algorithm of the \(N\)-body problem, we applied the idea of a known
algorithm for the \(N\)-body problem in physics, although another trick is need
to overcome the problem of small sized shared memory. For the partial sort, we
give a novel GPU algorithm which is effective for small \(k\). In our partial
sort algorithm, a heap is accessed in parallel by threads with a low cost of
synchronization. Both of these two parts of our algorithm utilize maximal power
of coalesced memory access, so that a full bandwidth is achieved.
  By an experiment, we show that when the size of the problem is large, an
implementation of the algorithm on two GPUs runs more than 330 times faster
than a single core implementation on a latest CPU. We also show that our
algorithm scales well with respect to the number of GPUs.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2010</td>
    <td>
      <a href="/publications/zhang2010self/">Self-taught Hashing For Fast Similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Self-taught Hashing For Fast Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Self-taught Hashing For Fast Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang et al.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval</td>
    <td>352</td>
    <td><p>The ability of fast similarity search at large scale is of great
importance to many Information Retrieval (IR) applications.
A promising way to accelerate similarity search is semantic
hashing which designs compact binary codes for a large number
of documents so that semantically similar documents
are mapped to similar codes (within a short Hamming distance).
Although some recently proposed techniques are
able to generate high-quality codes for documents known
in advance, obtaining the codes for previously unseen documents
remains to be a very challenging problem. In this
paper, we emphasise this issue and propose a novel SelfTaught
Hashing (STH) approach to semantic hashing: we
first find the optimal l-bit binary codes for all documents in
the given corpus via unsupervised learning, and then train
l classifiers via supervised learning to predict the l-bit code
for any query document unseen before. Our experiments on
three real-world text datasets show that the proposed approach
using binarised Laplacian Eigenmap (LapEig) and
linear Support Vector Machine (SVM) outperforms stateof-the-art
techniques significantly.</p>
</td>
    <td>
      
        SIGIR 
      
        Hashing Methods 
      
        Text Retrieval 
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2010</td>
    <td>
      <a href="/publications/pauleve2010locality/">Locality Sensitive Hashing: A Comparison Of Hash Function Types And Querying Mechanisms</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Locality Sensitive Hashing: A Comparison Of Hash Function Types And Querying Mechanisms' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Locality Sensitive Hashing: A Comparison Of Hash Function Types And Querying Mechanisms' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Pauleve Loic, Jegou, Amsaleg</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition Letters</td>
    <td>299</td>
    <td><p>It is well known that high-dimensional nearest-neighbor retrieval is very expensive. Dramatic performance gains are obtained using
approximate search schemes, such as the popular Locality-Sensitive Hashing (LSH). Several extensions have been proposed to
address the limitations of this algorithm, in particular, by choosing more appropriate hash functions to better partition the vector
space. All the proposed extensions, however, rely on a structured quantizer for hashing, poorly fitting real data sets, limiting
its performance in practice. In this paper, we compare several families of space hashing functions in a real setup, namely when
searching for high-dimension SIFT descriptors. The comparison of random projections, lattice quantizers, k-means and hierarchical
k-means reveal that unstructured quantizer significantly improves the accuracy of LSH, as it closely fits the data in the feature space.
We then compare two querying mechanisms introduced in the literature with the one originally proposed in LSH, and discuss their
respective merits and limitations.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Hashing Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2010</td>
    <td>
      <a href="/publications/andoni2010approximate/">Approximate Nearest Neighbor Search In High Dimensions</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Approximate Nearest Neighbor Search In High Dimensions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Approximate Nearest Neighbor Search In High Dimensions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Andoni Alexandr, Indyk Piotr, Razenshteyn Ilya</td> <!-- 🔧 You were missing this -->
    <td>2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</td>
    <td>108</td>
    <td><p>The nearest neighbor problem is defined as follows: Given a set \(P\) of \(n\)
points in some metric space \((X,D)\), build a data structure that, given any
point \(q\), returns a point in \(P\) that is closest to \(q\) (its “nearest
neighbor” in \(P\)). The data structure stores additional information about the
set \(P\), which is then used to find the nearest neighbor without computing all
distances between \(q\) and \(P\). The problem has a wide range of applications in
machine learning, computer vision, databases and other fields.
  To reduce the time needed to find nearest neighbors and the amount of memory
used by the data structure, one can formulate the {\em approximate} nearest
neighbor problem, where the the goal is to return any point \(p’ \in P\) such
that the distance from \(q\) to \(p’\) is at most \(c \cdot \min_{p \in P} D(q,p)\),
for some \(c \geq 1\). Over the last two decades, many efficient solutions to
this problem were developed. In this article we survey these developments, as
well as their connections to questions in geometric functional analysis and
combinatorial geometry.</p>
</td>
    <td>
      
        Similarity Search 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2010</td>
    <td>
      <a href="/publications/jain2010hashing/">Hashing Hyperplane Queries To Near Points With Applications To Large-scale Active Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing Hyperplane Queries To Near Points With Applications To Large-scale Active Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing Hyperplane Queries To Near Points With Applications To Large-scale Active Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jain P., Vijayanarasimhan, Grauman</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>69</td>
    <td><p>We consider the problem of retrieving the database points nearest to a given hyperplane query without exhaustively scanning the 
database. We propose two hashing-based solutions. Our first approach maps the data to two-bit binary keys that are locality-sensitive for the angle between the hyperplane normal and a database point. Our second approach embeds the data into a vector space where the Euclidean norm reflects the desired distance between the original points and hyperplane query. Both use hashing to retrieve near points in sub-linear time. Our first method’s preprocessing stage is more efficient, while the second has stronger accuracy guarantees. We apply both to pool-based active learning: taking the current hyperplane classifier as a query, our algorithm identifies those points (approximately) satisfying the well-known minimal distance-to-hyperplane selection criterion. We empirically demonstrate our methods’ tradeoffs, and show that they make it practical to perform active selection with millions 
of unlabeled points.</p>
</td>
    <td>
      
        Hashing Methods 
      
        SCALABILITY 
      
    </td>
    </tr>      
    
     <tr>
  <td>2010</td>
    <td>
      <a href="/publications/wang2010semi/">Semi-supervised Hashing For Scalable Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semi-supervised Hashing For Scalable Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semi-supervised Hashing For Scalable Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang J., Kumar, Chang</td> <!-- 🔧 You were missing this -->
    <td>2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</td>
    <td>626</td>
    <td><p>Large scale image search has recently attracted considerable
attention due to easy availability of huge amounts of
data. Several hashing methods have been proposed to allow
approximate but highly efficient search. Unsupervised
hashing methods show good performance with metric distances
but, in image search, semantic similarity is usually
given in terms of labeled pairs of images. There exist supervised
hashing methods that can handle such semantic similarity
but they are prone to overfitting when labeled data
is small or noisy. Moreover, these methods are usually very
slow to train. In this work, we propose a semi-supervised
hashing method that is formulated as minimizing empirical
error on the labeled data while maximizing variance
and independence of hash bits over the labeled and unlabeled
data. The proposed method can handle both metric as
well as semantic similarity. The experimental results on two
large datasets (up to one million samples) demonstrate its
superior performance over state-of-the-art supervised and
unsupervised methods.</p>
</td>
    <td>
      
        Image Retrieval 
      
        CVPR 
      
        Unsupervised 
      
        Neural Hashing 
      
        SUPERVISED 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2010</td>
    <td>
      <a href="/publications/wang2010sequential/">Sequential Projection Learning For Hashing With Compact Codes</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Sequential Projection Learning For Hashing With Compact Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Sequential Projection Learning For Hashing With Compact Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang J., Kumar, Chang</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>328</td>
    <td><p>Hashing based Approximate Nearest Neighbor
(ANN) search has attracted much attention
due to its fast query time and drastically
reduced storage. However, most of the hashing
methods either use random projections or
extract principal directions from the data to
derive hash functions. The resulting embedding
suffers from poor discrimination when
compact codes are used. In this paper, we
propose a novel data-dependent projection
learning method such that each hash function
is designed to correct the errors made by
the previous one sequentially. The proposed
method easily adapts to both unsupervised
and semi-supervised scenarios and shows significant
performance gains over the state-ofthe-art
methods on two large datasets containing
up to 1 million points.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Compact Codes 
      
    </td>
    </tr>      
    
     <tr>
  <td>2010</td>
    <td>
      <a href="/publications/petrovic2010streaming/">Streaming First Story Detection With Application To Twitter</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Streaming First Story Detection With Application To Twitter' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Streaming First Story Detection With Application To Twitter' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Petrovic S., Osborne, Lavrenko</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>565</td>
    <td><p>With the recent rise in popularity and size of
social media, there is a growing need for systems
that can extract useful information from
this amount of data. We address the problem
of detecting new events from a stream of
Twitter posts. To make event detection feasible
on web-scale corpora, we present an algorithm
based on locality-sensitive hashing which
is able overcome the limitations of traditional
approaches, while maintaining competitive results.
In particular, a comparison with a stateof-the-art
system on the first story detection
task shows that we achieve over an order of
magnitude speedup in processing time, while
retaining comparable performance. Event detection
experiments on a collection of 160 million
Twitter posts show that celebrity deaths
are the fastest spreading news on Twitter.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
     <tr>
  <td>2010</td>
    <td>
      <a href="/publications/zvedeniouk2010angle/">Angle Tree: Nearest Neighbor Search In High Dimensions With Low Intrinsic Dimensionality</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Angle Tree: Nearest Neighbor Search In High Dimensions With Low Intrinsic Dimensionality' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Angle Tree: Nearest Neighbor Search In High Dimensions With Low Intrinsic Dimensionality' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zvedeniouk Ilia, Chawla Sanjay</td> <!-- 🔧 You were missing this -->
    <td>Information Systems</td>
    <td>14</td>
    <td><p>We propose an extension of tree-based space-partitioning indexing structures
for data with low intrinsic dimensionality embedded in a high dimensional
space. We call this extension an Angle Tree. Our extension can be applied to
both classical kd-trees as well as the more recent rp-trees. The key idea of
our approach is to store the angle (the “dihedral angle”) between the data
region (which is a low dimensional manifold) and the random hyperplane that
splits the region (the “splitter”). We show that the dihedral angle can be used
to obtain a tight lower bound on the distance between the query point and any
point on the opposite side of the splitter. This in turn can be used to
efficiently prune the search space. We introduce a novel randomized strategy to
efficiently calculate the dihedral angle with a high degree of accuracy.
Experiments and analysis on real and synthetic data sets shows that the Angle
Tree is the most efficient known indexing structure for nearest neighbor
queries in terms of preprocessing and space usage while achieving high accuracy
and fast search time.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2009</td>
    <td>
      <a href="/publications/jegou2009searching/">Searching With Quantization: Approximate Nearest Neighbor Search Using Short Codes And Distance Estimators</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Searching With Quantization: Approximate Nearest Neighbor Search Using Short Codes And Distance Estimators' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Searching With Quantization: Approximate Nearest Neighbor Search Using Short Codes And Distance Estimators' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jegou H., Douze, Schmid</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>17</td>
    <td><p>We propose an approximate nearest neighbor search method based
on quantization. It uses, in particular, product quantizer to produce short codes
and corresponding distance estimators approximating the Euclidean distance
between the orginal vectors. The method is advantageously used in an asymmetric
manner, by computing the distance between a vector and code, unlike
competing techniques such as spectral hashing that only compare codes.
Our approach approximates the Euclidean distance based on memory efficient codes and, thus, permits efficient nearest neighbor search. Experiments
performed on SIFT and GIST image descriptors show excellent search accuracy.
The method is shown to outperform two state-of-the-art approaches of the literature.
Timings measured when searching a vector set of 2 billion vectors are
shown to be excellent given the high accuracy of the method.</p>
</td>
    <td>
      
        Quantization 
      
        Compact Codes 
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2009</td>
    <td>
      <a href="/publications/kulis2009kernelized/">Kernelized Locality-sensitive Hashing For Scalable Image Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Kernelized Locality-sensitive Hashing For Scalable Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Kernelized Locality-sensitive Hashing For Scalable Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kulis B., Grauman</td> <!-- 🔧 You were missing this -->
    <td>2009 IEEE 12th International Conference on Computer Vision</td>
    <td>907</td>
    <td><p>Fast retrieval methods are critical for large-scale and
data-driven vision applications. Recent work has explored
ways to embed high-dimensional features or complex distance
functions into a low-dimensional Hamming space
where items can be efficiently searched. However, existing
methods do not apply for high-dimensional kernelized
data when the underlying feature embedding for the kernel
is unknown. We show how to generalize locality-sensitive
hashing to accommodate arbitrary kernel functions, making
it possible to preserve the algorithm’s sub-linear time similarity
search guarantees for a wide class of useful similarity
functions. Since a number of successful image-based kernels
have unknown or incomputable embeddings, this is especially
valuable for image retrieval tasks. We validate our
technique on several large-scale datasets, and show that it
enables accurate and fast performance for example-based
object classification, feature matching, and content-based
retrieval.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Hashing Methods 
      
        Image Retrieval 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2009</td>
    <td>
      <a href="/publications/raginsky2009locality/">Locality-sensitive Binary Codes From Shift-invariant Kernels</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Locality-sensitive Binary Codes From Shift-invariant Kernels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Locality-sensitive Binary Codes From Shift-invariant Kernels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Raginsky M., Lazebnik</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>633</td>
    <td><p>This paper addresses the problem of designing binary codes for high-dimensional
data such that vectors that are similar in the original space map to similar binary
strings. We introduce a simple distribution-free encoding scheme based on
random projections, such that the expected Hamming distance between the binary
codes of two vectors is related to the value of a shift-invariant kernel (e.g., a
Gaussian kernel) between the vectors. We present a full theoretical analysis of the
convergence properties of the proposed scheme, and report favorable experimental
performance as compared to a recent state-of-the-art method, spectral hashing.</p>
</td>
    <td>
      
        Compact Codes 
      
    </td>
    </tr>      
    
     <tr>
  <td>2009</td>
    <td>
      <a href="/publications/jain2009fast/">Fast Similarity Search For Learned Metrics</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Similarity Search For Learned Metrics' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Similarity Search For Learned Metrics' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jain P., Kulis, Grauman</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>264</td>
    <td><p>We propose a method to efficiently index into a large database of examples according to a learned metric.
Given a collection of examples, we learn a Mahalanobis distance using an information-theoretic metric
learning technique that adapts prior knowledge about pairwise distances to incorporate similarity and dissimilarity
constraints. To enable sub-linear time similarity search under the learned metric, we show how
to encode a learned Mahalanobis parameterization into randomized locality-sensitive hash functions. We
further formulate an indirect solution that enables metric learning and hashing for sparse input vector spaces
whose high dimensionality make it infeasible to learn an explicit weighting over the feature dimensions.
We demonstrate the approach applied to systems and image datasets, and show that our learned metrics
improve accuracy relative to commonly-used metric baselines, while our hashing construction permits effi-
cient indexing with a learned distance and very large databases.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2009</td>
    <td>
      <a href="/publications/kulis2009learning/">Learning To Hash With Binary Reconstructive Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning To Hash With Binary Reconstructive Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning To Hash With Binary Reconstructive Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kulis B., Darrell</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>841</td>
    <td><p>Fast retrieval methods are increasingly critical for many large-scale analysis tasks, and there have been
several recent methods that attempt to learn hash functions for fast and accurate nearest neighbor searches.
In this paper, we develop an algorithm for learning hash functions based on explicitly minimizing the
reconstruction error between the original distances and the Hamming distances of the corresponding binary
embeddings. We develop a scalable coordinate-descent algorithm for our proposed hashing objective that
is able to efficiently learn hash functions in a variety of settings. Unlike existing methods such as semantic
hashing and spectral hashing, our method is easily kernelized and does not require restrictive assumptions
about the underlying distribution of the data. We present results over several domains to demonstrate that
our method outperforms existing state-of-the-art techniques.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2008</td>
    <td>
      <a href="/publications/salakhutdinov2008semantic/">Semantic Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semantic Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semantic Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Salakhutdinov R., Hinton</td> <!-- 🔧 You were missing this -->
    <td>International Journal of Approximate Reasoning</td>
    <td>1272</td>
    <td><p>We show how to learn a deep graphical model of the word-count
vectors obtained from a large set of documents. The values of the
latent variables in the deepest layer are easy to infer and give a
much better representation of each document than Latent Semantic
Analysis. When the deepest layer is forced to use a small number of
binary variables (e.g. 32), the graphical model performs “semantic
hashing”: Documents are mapped to memory addresses in such a
way that semantically similar documents are located at nearby addresses.
Documents similar to a query document can then be found
by simply accessing all the addresses that differ by only a few bits
from the address of the query document. This way of extending the
efficiency of hash-coding to approximate matching is much faster
than locality sensitive hashing, which is the fastest current method.
By using semantic hashing to filter the documents given to TF-IDF,
we achieve higher accuracy than applying TF-IDF to the entire document
set.</p>
</td>
    <td>
      
        Hashing Methods 
      
        Text Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2008</td>
    <td>
      <a href="/publications/andoni2008near/">Near-optimal Hashing Algorithms For Approximate Nearest Neighbor In High Dimensions</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Near-optimal Hashing Algorithms For Approximate Nearest Neighbor In High Dimensions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Near-optimal Hashing Algorithms For Approximate Nearest Neighbor In High Dimensions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Andoni A., Indyk</td> <!-- 🔧 You were missing this -->
    <td>Communications of the ACM</td>
    <td>1420</td>
    <td><p>We present an algorithm for the c-approximate nearest neighbor problem in a d-dimensional Euclidean space, achieving query time of O(dn 1c2/+o(1)) and space O(dn + n1+1c2/+o(1)). This almost matches the lower bound for hashing-based algorithm recently obtained in (R. Motwani et al., 2006). We also obtain a space-efficient version of the algorithm, which uses dn+n logO(1) n space, with a query time of dnO(1/c2). Finally, we discuss practical variants of the algorithms that utilize fast bounded-distance decoders for the Leech lattice</p>
</td>
    <td>
      
        Similarity Search 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2008</td>
    <td>
      <a href="/publications/weiss2008spectral/">Spectral Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Spectral Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Spectral Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Weiss Y., Torralba, Fergus</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>2154</td>
    <td><p>Semantic hashing seeks compact binary codes of data-points so that the
Hamming distance between codewords correlates with semantic similarity.
In this paper, we show that the problem of finding a best code for a given
dataset is closely related to the problem of graph partitioning and can
be shown to be NP hard. By relaxing the original problem, we obtain a
spectral method whose solutions are simply a subset of thresholded eigenvectors
of the graph Laplacian. By utilizing recent results on convergence
of graph Laplacian eigenvectors to the Laplace-Beltrami eigenfunctions of
manifolds, we show how to efficiently calculate the code of a novel datapoint.
Taken together, both learning the code and applying it to a novel
point are extremely simple. Our experiments show that our codes outperform
the state-of-the art.</p>
</td>
    <td>
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2008</td>
    <td>
      <a href="/publications/black2008compositional/">Compositional Sketch Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Compositional Sketch Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Compositional Sketch Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Black et al.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>75</td>
    <td><p>We present an algorithm for searching image collections using free-hand
sketches that describe the appearance and relative positions of multiple
objects. Sketch based image retrieval (SBIR) methods predominantly match
queries containing a single, dominant object invariant to its position within
an image. Our work exploits drawings as a concise and intuitive representation
for specifying entire scene compositions. We train a convolutional neural
network (CNN) to encode masked visual features from sketched objects, pooling
these into a spatial descriptor encoding the spatial relationships and
appearances of objects in the composition. Training the CNN backbone as a
Siamese network under triplet loss yields a metric search embedding for
measuring compositional similarity which may be efficiently leveraged for
visual search by applying product quantization.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2007</td>
    <td>
      <a href="/publications/lv2007multi/">Multi-probe LSH: Efficient Indexing For High-dimensional Similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multi-probe LSH: Efficient Indexing For High-dimensional Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multi-probe LSH: Efficient Indexing For High-dimensional Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lv et al.</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>620</td>
    <td><p>Similarity indices for high-dimensional data are very desirable for building content-based search systems for featurerich data such as audio, images, videos, and other sensor
data. Recently, locality sensitive hashing (LSH) and its
variations have been proposed as indexing techniques for
approximate similarity search. A significant drawback of
these approaches is the requirement for a large number of
hash tables in order to achieve good search quality. This paper proposes a new indexing scheme called multi-probe LSH
that overcomes this drawback. Multi-probe LSH is built on
the well-known LSH technique, but it intelligently probes
multiple buckets that are likely to contain query results in
a hash table. Our method is inspired by and improves upon
recent theoretical work on entropy-based LSH designed to
reduce the space requirement of the basic LSH method. We
have implemented the multi-probe LSH method and evaluated the implementation with two different high-dimensional
datasets. Our evaluation shows that the multi-probe LSH
method substantially improves upon previously proposed
methods in both space and time efficiency. To achieve the
same search quality, multi-probe LSH has a similar timeefficiency as the basic LSH method while reducing the number of hash tables by an order of magnitude. In comparison
with the entropy-based LSH method, to achieve the same
search quality, multi-probe LSH uses less query time and 5
to 8 times fewer number of hash tables.</p>
</td>
    <td>
      
        Locality Sensitive Hashing 
      
        Similarity Search 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2006</td>
    <td>
      <a href="/publications/wang2006inverted/">Inverted Semantic-index For Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Inverted Semantic-index For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Inverted Semantic-index For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Ying</td> <!-- 🔧 You were missing this -->
    <td>2006 10th International Database Engineering and Applications Symposium (IDEAS'06)</td>
    <td>14</td>
    <td><p>This paper addresses the construction of inverted index for large-scale image
retrieval. The inverted index proposed by J. Sivic brings a significant
acceleration by reducing distance computations with only a small fraction of
the database. The state-of-the-art inverted indices aim to build finer
partitions that produce a concise and accurate candidate list. However,
partitioning in these frameworks is generally achieved by unsupervised
clustering methods which ignore the semantic information of images. In this
paper, we replace the clustering method with image classification, during the
construction of codebook. We then propose a merging and splitting method to
solve the problem that the number of partitions is unchangeable in the inverted
semantic-index. Next, we combine our semantic-index with the product
quantization (PQ) so as to alleviate the accuracy loss caused by PQ
compression. Finally, we evaluate our model on large-scale image retrieval
benchmarks. Experiment results demonstrate that our model can significantly
improve the retrieval accuracy by generating high-quality candidate lists.</p>
</td>
    <td>
      
        Image Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2006</td>
    <td>
      <a href="/publications/li2006very/">Very Sparse Random Projections</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Very Sparse Random Projections' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Very Sparse Random Projections' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Ping, Hastie, Church</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</td>
    <td>632</td>
    <td><p>There has been considerable interest in random projections, an approximate algorithm for estimating distances between pairs of points in a high-dimensional vector space. Let A in Rn x D be our n points in D dimensions. The method multiplies A by a random matrix R in RD x k, reducing the D dimensions down to just k for speeding up the computation. R typically consists of entries of standard normal N(0,1). It is well known that random projections preserve pairwise distances (in the expectation). Achlioptas proposed sparse random projections by replacing the N(0,1) entries in R with entries in -1,0,1 with probabilities 1/6, 2/3, 1/6, achieving a threefold speedup in processing time.We recommend using R of entries in -1,0,1 with probabilities 1/2√D, 1-1√D, 1/2√D for achieving a significant √D-fold speedup, with little loss in accuracy.</p>
</td>
    <td>
      
        KDD 
      
        Locality Sensitive Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2006</td>
    <td>
      <a href="/publications/indyk2006simultaneous/">Simultaneous Nearest Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Simultaneous Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Simultaneous Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Indyk et al.</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition Letters</td>
    <td>195</td>
    <td><p>Motivated by applications in computer vision and databases, we introduce and
study the Simultaneous Nearest Neighbor Search (SNN) problem. Given a set of
data points, the goal of SNN is to design a data structure that, given a
collection of queries, finds a collection of close points that are compatible
with each other. Formally, we are given \(k\) query points \(Q=q_1,\cdots,q_k\),
and a compatibility graph \(G\) with vertices in \(Q\), and the goal is to return
data points \(p_1,\cdots,p_k\) that minimize (i) the weighted sum of the
distances from \(q_i\) to \(p_i\) and (ii) the weighted sum, over all edges \((i,j)\)
in the compatibility graph \(G\), of the distances between \(p_i\) and \(p_j\). The
problem has several applications, where one wants to return a set of consistent
answers to multiple related queries. This generalizes well-studied
computational problems, including NN, Aggregate NN and the 0-extension problem.
  In this paper we propose and analyze the following general two-step method
for designing efficient data structures for SNN. In the first step, for each
query point \(q_i\) we find its (approximate) nearest neighbor point \(\hat{p}_i\);
this can be done efficiently using existing approximate nearest neighbor
structures. In the second step, we solve an off-line optimization problem over
sets \(q_1,\cdots,q_k\) and \(\hat{p}_1,\cdots,\hat{p}_k\); this can be done
efficiently given that \(k\) is much smaller than \(n\). Even though
\(\hat{p}_1,\cdots,\hat{p}_k\) might not constitute the optimal answers to
queries \(q_1,\cdots,q_k\), we show that, for the unweighted case, the resulting
algorithm is \(O(log k/log log k)\)-approximation. Also, we show that the
approximation factor can be in fact reduced to a constant for compatibility
graphs frequently occurring in practice.
  Finally, we show that the “empirical approximation factor” provided by the
above approach is very close to 1.</p>
</td>
    <td>
      
        Similarity Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2006</td>
    <td>
      <a href="/publications/godil2006retrieval/">Retrieval And Clustering From A 3D Human Database Based On Body And Head Shape</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Retrieval And Clustering From A 3D Human Database Based On Body And Head Shape' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Retrieval And Clustering From A 3D Human Database Based On Body And Head Shape' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Godil Afzal, Ressler Sandy</td> <!-- 🔧 You were missing this -->
    <td>SAE Technical Paper Series</td>
    <td>19</td>
    <td><p>In this paper, we describe a framework for similarity based retrieval and
clustering from a 3D human database. Our technique is based on both body and
head shape representation and the retrieval is based on similarity of both of
them. The 3D human database used in our study is the CAESAR anthropometric
database which contains approximately 5000 bodies. We have developed a
web-based interface for specifying the queries to interact with the retrieval
system. Our approach performs the similarity based retrieval in a reasonable
amount of time and is a practical approach.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2004</td>
    <td>
      <a href="/publications/han2004hashing/">Hashing For Protein Structure Similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing For Protein Structure Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing For Protein Structure Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Han Jin, Li Wu-jun</td> <!-- 🔧 You were missing this -->
    <td>Bioinformatics</td>
    <td>50</td>
    <td><p>Protein structure similarity search (PSSS), which tries to search proteins
with similar structures, plays a crucial role across diverse domains from drug
design to protein function prediction and molecular evolution. Traditional
alignment-based PSSS methods, which directly calculate alignment on the protein
structures, are highly time-consuming with high memory cost. Recently,
alignment-free methods, which represent protein structures as fixed-length
real-valued vectors, are proposed for PSSS. Although these methods have lower
time and memory cost than alignment-based methods, their time and memory cost
is still too high for large-scale PSSS, and their accuracy is unsatisfactory.
In this paper, we propose a novel method, called
\(\underline{\text{p}}\)r\(\underline{\text{o}}\)tein
\(\underline{\text{s}}\)tructure \(\underline{\text{h}}\)ashing (POSH), for PSSS.
POSH learns a binary vector representation for each protein structure, which
can dramatically reduce the time and memory cost for PSSS compared with
real-valued vector representation based methods. Furthermore, in POSH we also
propose expressive hand-crafted features and a structure encoder to well model
both node and edge interactions in proteins. Experimental results on real
datasets show that POSH can outperform other methods to achieve
state-of-the-art accuracy. Furthermore, POSH achieves a memory saving of more
than six times and speed improvement of more than four times, compared with
other methods.</p>
</td>
    <td>
      
        Similarity Search 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2004</td>
    <td>
      <a href="/publications/fernandez2004active/">Active Image Indexing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Active Image Indexing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Active Image Indexing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fernandez et al.</td> <!-- 🔧 You were missing this -->
    <td>Journal of Geophysical Research: Atmospheres</td>
    <td>142</td>
    <td><p>Image copy detection and retrieval from large databases leverage two
components. First, a neural network maps an image to a vector representation,
that is relatively robust to various transformations of the image. Second, an
efficient but approximate similarity search algorithm trades scalability (size
and speed) against quality of the search, thereby introducing a source of
error. This paper improves the robustness of image copy detection with active
indexing, that optimizes the interplay of these two components. We reduce the
quantization loss of a given image representation by making imperceptible
changes to the image before its release. The loss is back-propagated through
the deep neural network back to the image, under perceptual constraints. These
modifications make the image more retrievable. Our experiments show that the
retrieval and copy detection of activated images is significantly improved. For
instance, activation improves by \(+40%\) the Recall1@1 on various image
transformations, and for several popular indexing structures based on product
quantization and locality sensitivity hashing.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>1999</td>
    <td>
      <a href="/publications/gionis1999similarity/">Similarity Search In High Dimensions Via Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Similarity Search In High Dimensions Via Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Similarity Search In High Dimensions Via Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gionis A., Indyk, Motwani</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>3205</td>
    <td><p>The nearest- or near-neighbor query problems arise in a large variety of database applications, usually in the context of similarity searching. Of late, there has been increasing interest in building search/index structures for performing similarity search over high-dimensional data, e.g., image databases, document collections, time-series databases, and genome databases. Unfortunately,
all known techniques for solving this problem fall prey to the curse of dimensionality. That is, the data structures scale poorly with data dimensionality;
in fact, if the number of dimensions exceeds 10 to 20, searching in k-d trees and related structures involves the inspection of a large fraction of the database, thereby doing no better than brute-force linear search. It has been suggested that since the selection of features and the choice of a distance metric in typical applications is rather heuristic, determining an approximate nearest neighbor should suffice for most practical purposes. In this paper, we examine a novel scheme for approximate similarity search based on hashing. The basic idea is to hash the points from the database so as to ensure that the probability of collision is much higher for objects that are close to each other than for those that are far apart. We provide experimental evidence that our
method gives significant improvement in running time over other methods for searching in highdimensional spaces based on hierarchical tree decomposition.
Experimental results also indicate that our scheme scales well even for a relatively large number of dimensions (more than 50).</p>
</td>
    <td>
      
        Similarity Search 
      
        Hashing Methods 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>1997</td>
    <td>
      <a href="/publications/zheng1997visual/">Visual Similarity Attention</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Visual Similarity Attention' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Visual Similarity Attention' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zheng et al.</td> <!-- 🔧 You were missing this -->
    <td>Perception &amp; Psychophysics</td>
    <td>127</td>
    <td><p>While there has been substantial progress in learning suitable distance
metrics, these techniques in general lack transparency and decision reasoning,
i.e., explaining why the input set of images is similar or dissimilar. In this
work, we solve this key problem by proposing the first method to generate
generic visual similarity explanations with gradient-based attention. We
demonstrate that our technique is agnostic to the specific similarity model
type, e.g., we show applicability to Siamese, triplet, and quadruplet models.
Furthermore, we make our proposed similarity attention a principled part of the
learning process, resulting in a new paradigm for learning similarity
functions. We demonstrate that our learning mechanism results in more
generalizable, as well as explainable, similarity models. Finally, we
demonstrate the generality of our framework by means of experiments on a
variety of tasks, including image retrieval, person re-identification, and
low-shot semantic segmentation.</p>
</td>
    <td>
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>1995</td>
    <td>
      <a href="/publications/horaud1995polyhedral/">Polyhedral Object Recognition By Indexing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Polyhedral Object Recognition By Indexing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Polyhedral Object Recognition By Indexing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Horaud Radu, Sossa Humberto</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition</td>
    <td>48</td>
    <td><p>In computer vision, the indexing problem is the problem of recognizing a few
objects in a large database of objects while avoiding the help of the classical
image-feature-to-object-feature matching paradigm. In this paper we address the
problem of recognizing 3-D polyhedral objects from 2-D images by indexing. Both
the objects to be recognized and the images are represented by weighted graphs.
The indexing problem is therefore the problem of determining whether a graph
extracted from the image is present or absent in a database of model graphs. We
introduce a novel method for performing this graph indexing process which is
based both on polynomial characterization of binary and weighted graphs and on
hashing. We describe in detail this polynomial characterization and then we
show how it can be used in the context of polyhedral object recognition. Next
we describe a practical recognition-by-indexing system that includes the
organization of the database, the representation of polyhedral objects in terms
of 2-D characteristic views, the representation of this views in terms of
weighted graphs, and the associated image processing. Finally, some
experimental results allow the evaluation of the system performance.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
    
  </tbody>
</table>

<!-- CSS Styles -->
<style>
  /* Hide the table initially */

  #allPapers {
  display: none;
  table-layout: fixed;
  width: 100%;
  }
  /* Style the loading indicator */
  #loading {
    position: fixed;
    top: 50%;
    left: 50%;
    transform: translate(-50%, -50%);
    font-size: 1.5em;
    text-align: center;
  }
</style>

<!-- JavaScript -->
<script>
  var datatable;
  var searchInitialized = false;

  function searchTable() {
    // Check if datatable is initialized
    if (datatable && searchInitialized) {
      var hash = decodeURIComponent(window.location.hash.substr(1));
      if (hash) {
        datatable.search(hash).draw();
      } else {
        datatable.search('').draw();  // Clear search if no hash is present
      }
    } else {
      // Retry if datatable is not yet initialized
      setTimeout(searchTable, 500);
    }
  }

  $(document).ready(function() {
    // Show the loading indicator
    $('#loading').show();

    // Initialize the DataTable
    datatable = $('#allPapers').DataTable({
      paging: false,
      pageLength: 100,
      searching: true,
      order: [[0, 'desc'], [4, 'desc']],  // Default sort: Year desc, then Citations desc
      columnDefs: [
        {
          targets: [5, 6],  // Adjusted for new column indices
          visible: false,
          searchable: true
        },
        {
          targets: 4,  // Citation count column
          type: 'num'
        }
      ],
      // Callback when DataTable is initialized
      initComplete: function(settings, json) {
        // Hide the loading indicator
        $('#loading').hide();
        // Show the table
        $('#allPapers').show();
        // Set searchInitialized to true after initialization
        searchInitialized = true;
        // Perform the initial search based on the hash (if any)
        searchTable();
      }
    });
  });

  // Update search whenever the hash changes
  $(window).on('hashchange', function() {
    searchTable();
  });
</script>


    </div>

  </body>
</html>
