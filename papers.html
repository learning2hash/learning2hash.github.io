<!DOCTYPE html>
<html lang="en-us">

  <head>
<!-- Begin Web-Stat code v 7.0 -->
<span id="wts2185304"></span>
<script>var wts=document.createElement('script');wts.async=true;
wts.src='https://app.ardalio.com/log7.js';document.head.appendChild(wts);
wts.onload = function(){ wtslog7(2185304,4); };
</script><noscript><a href="https://www.web-stat.com">
<img src="https://app.ardalio.com/7/4/2185304.png" 
alt="Web-Stat web statistics"></a></noscript>
<!-- End Web-Stat code v 7.0 -->
  <!-- Hotjar Tracking Code for https://learning2hash.github.io/ -->
<script>
    (function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:1843243,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109544763-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-109544763-1');
</script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [["\\(","\\)"]],
        displayMath: [["\\[","\\]"]]
      }
    });
  </script>
  <script type="text/javascript" async
    src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="keywords" content="machine learning, hashing, approximate nearest neighbour search, lsh, learning-to-hash">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Search all Publications on Machine Learning for Hashing | Awesome Learning to Hash</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Search all Publications on Machine Learning for Hashing" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A list of research papers for machine learning models for hashing." />
<meta property="og:description" content="A list of research papers for machine learning models for hashing." />
<link rel="canonical" href="https://learning2hash.github.io/papers.html" />
<meta property="og:url" content="https://learning2hash.github.io/papers.html" />
<meta property="og:site_name" content="Awesome Learning to Hash" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Search all Publications on Machine Learning for Hashing" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"A list of research papers for machine learning models for hashing.","headline":"Search all Publications on Machine Learning for Hashing","url":"https://learning2hash.github.io/papers.html"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link rel="search" href="/public/opensearchdescription.xml" 
      type="application/opensearchdescription+xml" 
      title="learning2hash" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
  integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
  crossorigin="anonymous"></script>
  
  <link rel="stylesheet" type="text/css" href="//cdn.datatables.net/1.10.16/css/jquery.dataTables.min.css">
  <script type="text/javascript" charset="utf8" src="//cdn.datatables.net/1.10.16/js/jquery.dataTables.min.js"></script>
</head>


  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <body class="theme-base-0c layout-reverse">

    <a href='/contributing.html' class='ribbon'>Add your paper to Learning2Hash</a>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Awesome Learning to Hash
        </a>
      </h1>
      <p class="lead">A Webpage dedicated to the latest research on Hash Function Learning. Maintained by <a href="http://sjmoran.github.io/">Sean Moran</a>.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="sidebar-item">
        <p style="font-size: 12px">
          Search related work 
          <input type='text' id='searchTarget' size="16"/> 
          <button onClick="search();">Go</button>
        </p>
      </div>
      <a class="sidebar-nav-item" href="/papers.html">All Papers</a>
      <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
      <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>
      <a class="sidebar-nav-item" href="/topic-viz.html">Topic-based Explorer</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/">Core Taxonomy</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/datasets.html">Datasets</a>
      <a class="sidebar-nav-item" href="/tutorial.html">Tutorial</a>
      <a class="sidebar-nav-item" href="/resources.html">Resources, Courses &#38; Events</a>
      <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
    </nav>

    <div class="sidebar-item">
      <p style="font-size: 12px">
        Contact <a href="http://www.seanjmoran.com">Sean Moran</a> about this survey or website.
        <span style="font-size: 9px">
          Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
        </span>
      </p>
    </div>
  </div>
</div>

<script>
$("#searchTarget").keydown(function (e) {	
  if (e.keyCode == 13) {
    search();
  }
});

function search() {
  try {
    ga('send', 'event', 'search', 'search', $("#searchTarget").val());
  } finally {
    window.location = "/papers.html#" + $("#searchTarget").val();
  }
}
</script>


    <div class="content container">
      <p>
Search across all paper titles, abstracts, authors by using the search field.
Please consider <a href="/contributing.html">contributing</a> by updating
the information of existing papers or adding new work. 
</p>
</br>
<table id="allPapers">
<thead><th>Year</th><th>Title</th><th>Authors</th><th>Venue</th><th>Abstract</th></thead><tbody>



<tr>
	<td>2024</td>
	<td><a href="/publications/moran2024enhancing/">Enhancing First Story Detection Using Word Embeddings</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Enhancing First Story Detection Using Word Embeddings' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Enhancing First Story Detection Using Word Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Enhancing%20First%20Story%20Detection%20Using%20Word%20Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Moran S., Mccreadie, Macdonald, Ounis</td>
	<td>Arxiv</td>
	<td><p>In this paper we show how word embeddings can be used to increase the effectiveness of a state-of-the art Locality Sensitive Hashing (LSH) based first story detection (FSD) system over a standard tweet corpus. Vocabulary mismatch in which related tweets use different words is a serious hindrance to the effectiveness of a modern FSD system. In this case a tweet could be flagged as a first story even if a related tweet which uses different but synonymous words was already returned as a first story. In this work we propose a novel approach to mitigate this problem of lexical variation based on tweet expansion. In particular we propose to expand tweets with semantically related paraphrases identified via automatically mined word embeddings over a background tweet corpus. Through experimentation on a large data stream comprised of 50 million tweets we show that FSD effectiveness can be improved by 9.537; over a state-of-the-art FSD system.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/microsoft2024microsoft/">Microsoft SPACEV-1B</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Microsoft SPACEV-1B' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Microsoft SPACEV-1B' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Microsoft%20SPACEV-1B' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Microsoft Microsoft</td>
	<td>Arxiv</td>
	<td><p>Microsoft SPACEV-1B is a new web search related dataset released by Microsoft Bing for this competition. It consists of document and query vectors encoded by Microsoft SpaceV Superior model to capture generic intent representation.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/moran2024graph/">Graph Regularised Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Graph Regularised Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Graph Regularised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Graph%20Regularised%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Moran S., Lavrenko</td>
	<td>Arxiv</td>
	<td><p>In this paper we propose a two-step iterative scheme Graph Regularised Hashing (GRH) for incrementally adjusting the positioning of the hashing hypersurfaces to better conform to the supervisory signal in the first step the binary bits are regularised using a data similarity graph so that similar data points receive similar bits. In the second step the regularised hashcodes form targets for a set of binary classifiers which shift the position of each hypersurface so as to separate opposite bits with maximum margin. GRH exhibits superior retrieval accuracy to competing hashing methods.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/ma2024progressive/">Progressive Generative Hashing For Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Progressive Generative Hashing For Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Progressive Generative Hashing For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Progressive%20Generative%20Hashing%20For%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Ma Yuqing, He, Ding, Hu, Li, Liu</td>
	<td>Arxiv</td>
	<td><p>Recent years have witnessed the success of the emerging hashing techniques in large-scale image retrieval. Owing to the great learning capacity deep hashing has become one of the most promising solutions and achieved attractive performance in practice. However without semantic label information the unsupervised deep hashing still remains an open question. In this paper we propose a novel progressive generative hashing (PGH) framework to help learn a discriminative hashing network in an unsupervised way. Different from existing studies it first treats the hash codes as a kind of semantic condition for the similar image generation and simultaneously feeds the original image and its codes into the generative adversarial networks (GANs). The real images together with the synthetic ones can further help train a discriminative hashing network based on a triplet loss. By iteratively inputting the learnt codes into the hash conditioned GANs we can progressively enable the hashing network to discover the semantic relations. Extensive experiments on the widely-used image datasets demonstrate that PGH can significantly outperform stateof-the-art unsupervised hashing methods.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/ma2024harr/">HARR Learning Discriminative And High-quality Hash Codes For Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=HARR Learning Discriminative And High-quality Hash Codes For Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=HARR Learning Discriminative And High-quality Hash Codes For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=HARR%20Learning%20Discriminative%20And%20High-quality%20Hash%20Codes%20For%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Ma Zeyu, Wang, Luo, Gu, Chen, Li, Hua, Lu</td>
	<td>Arxiv</td>
	<td><p>This article studies deep unsupervised hashing which has attracted increasing attention in large-scale image retrieval. The majority of recent approaches usually reconstruct semantic similarity information which then guides the hash code learning. However they still fail to achieve satisfactory performance in reality for two reasons. On the one hand without accurate supervised information these methods usually fail to produce independent and robust hash codes with semantics information well preserved which may hinder effective image retrieval. On the other hand due to discrete constraints how to effectively optimize the hashing network in an end-to-end manner with small quantization errors remains a problem. To address these difficulties we propose a novel unsupervised hashing method called HARR to learn discriminative and high-quality hash codes. To comprehensively explore semantic similarity structure HARR adopts the Winner-Take-All hash to model the similarity structure. Then similarity-preserving hash codes are learned under the reliable guidance of the reconstructed similarity structure. Additionally we improve the quality of hash codes by a bit correlation reduction module which forces the cross-correlation matrix between a batch of hash codes under different augmentations to approach the identity matrix. In this way the generated hash bits are expected to be invariant to disturbances with minimal redundancy which can be further interpreted as an instantiation of the information bottleneck principle. Finally for effective hashing network training we minimize the cosine distances between real-value network outputs and their binary codes for small quantization errors. Extensive experiments demonstrate the effectiveness of our proposed HARR.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/moran2024learning/">Learning To Project And Binarise For Hashing-based Approximate Nearest Neighbour Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning To Project And Binarise For Hashing-based Approximate Nearest Neighbour Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning To Project And Binarise For Hashing-based Approximate Nearest Neighbour Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20To%20Project%20And%20Binarise%20For%20Hashing-based%20Approximate%20Nearest%20Neighbour%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Moran S.</td>
	<td>Arxiv</td>
	<td><p>In this paper we focus on improving the effectiveness of hashing-based approximate nearest neighbour search. Generating similarity preserving hashcodes for images has been shown to be an effective and efficient method for searching through large datasets. Hashcode generation generally involves two steps bucketing the input feature space with a set of hyperplanes followed by quantising the projection of the data-points onto the normal vectors to those hyperplanes. This procedure results in the makeup of the hashcodes depending on the positions of the data-points with respect to the hyperplanes in the feature space allowing a degree of locality to be encoded into the hashcodes. In this paper we study the effect of learning both the hyperplanes and the thresholds as part of the same model. Most previous research either learn the hyperplanes assuming a fixed set of thresholds or vice-versa. In our experiments over two standard image datasets we find statistically significant increases in retrieval effectiveness versus a host of state-of-the-art data-dependent and independent hashing models.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/luo2024fast/">Fast Scalable Supervised Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Fast Scalable Supervised Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Fast Scalable Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Fast%20Scalable%20Supervised%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Luo Xin, Nie, He, Wu, Chen, Xu</td>
	<td>Arxiv</td>
	<td><p>Despite significant progress in supervised hashing there are three common limitations of existing methods. First most pioneer methods discretely learn hash codes bit by bit making the learning procedure rather time-consuming. Second to reduce the large complexity of the n by n pairwise similarity matrix most methods apply sampling strategies during training which inevitably results in information loss and suboptimal performance; some recent methods try to replace the large matrix with a smaller one but the size is still large. Third among the methods that leverage the pairwise similarity matrix most of them only encode the semantic label information in learning the hash codes failing to fully capture the characteristics of data. In this paper we present a novel supervised hashing method called Fast Scalable Supervised Hashing (FSSH) which circumvents the use of the large similarity matrix by introducing a pre-computed intermediate term whose size is independent with the size of training data. Moreover FSSH can learn the hash codes with not only the semantic information but also the features of data. Extensive experiments on three widely used datasets demonstrate its superiority over several state-of-the-art methods in both accuracy and scalability. Our experiment codes are available at https://lcbwlx.wixsite.com/fssh.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/lu2024label/">Label Self-adaption Hashing For Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Label Self-adaption Hashing For Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Label Self-adaption Hashing For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Label%20Self-adaption%20Hashing%20For%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Lu Jianglin, Lai, Wang, Zhou</td>
	<td>Arxiv</td>
	<td><p>Hashing has attracted widespread attention in image retrieval because of its fast retrieval speed and low storage cost. Compared with supervised methods unsupervised hashing methods are more reasonable and suitable for large-scale image retrieval since it is always difficult and expensive to collect true labels of the massive data. Without label information however unsupervised hashing methods can not guarantee the quality of learned binary codes. To resolve this dilemma this paper proposes a novel unsupervised hashing method called Label Self-Adaption Hashing (LSAH) which contains effective hashing function learning part and self-adaption label generation part. In the first part we utilize anchor graph to keep the local structure of the data and introduce joint sparsity into the model to extract effective features for high-quality binary code learning. In the second part a self-adaptive cluster label matrix is learned from the data under the assumption that the nearest neighbor points should have a large probability to be in the same cluster. Therefore the proposed LSAH can make full use of the potential discriminative information of the data to guide the learning of binary code. It is worth noting that LSAH can learn effective binary codes hashing function and cluster labels simultaneously in a unified optimization framework. To solve the resulting optimization problem an Augmented Lagrange Multiplier based iterative algorithm is elaborately designed. Extensive experiments on three large-scale data sets indicate the promising performance of the proposed LSAH.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/lu2024online/">Online Multi-modal Hashing With Dynamic Query-adaption</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Online Multi-modal Hashing With Dynamic Query-adaption' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Online Multi-modal Hashing With Dynamic Query-adaption' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Online%20Multi-modal%20Hashing%20With%20Dynamic%20Query-adaption' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Lu Xu, Zhu, Cheng, Zhang</td>
	<td>Arxiv</td>
	<td><p>Multi-modal hashing is an effective technique to support large-scale multimedia retrieval due to its capability of encoding heterogeneous multi-modal features into compact and similarity-preserving binary codes. Although great progress has been achieved so far existing methods still suffer from several problems including 1) All existing methods simply adopt fixed modality combination weights in online hashing process to generate the query hash codes. This strategy cannot adaptively capture the variations of different queries. 2) They either suffer from insufficient semantics (for unsupervised methods) or require high computation and storage cost (for the supervised methods which rely on pair-wise semantic matrix). 3) They solve the hash codes with relaxed optimization strategy or bit-by-bit discrete optimization which results in significant quantization loss or consumes considerable computation time. To address the above limitations in this paper we propose an Online Multi-modal Hashing with Dynamic Query-adaption (OMH-DQ) method in a novel fashion. Specifically a self-weighted fusion strategy is designed to adaptively preserve the multi-modal feature information into hash codes by exploiting their complementarity. The hash codes are learned with the supervision of pair-wise semantic labels to enhance their discriminative capability while avoiding the challenging symmetric similarity matrix factorization. Under such learning framework the binary hash codes can be directly obtained with efficient operations and without quantization errors. Accordingly our method can benefit from the semantic labels and simultaneously avoid the high computation complexity. Moreover to accurately capture the query variations at the online retrieval stage we design a parameter-free online hashing module which can adaptively learn the query hash codes according to the dynamic query contents. Extensive experiments demonstrate the state-of-the-art performance of the proposed approach from various aspects.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/luo2024survey/">A Survey On Deep Hashing Methods</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Survey On Deep Hashing Methods' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Survey On Deep Hashing Methods' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20Survey%20On%20Deep%20Hashing%20Methods' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Luo Xiao, Wang, Wu, Chen, Deng, Huang, Hua</td>
	<td>Arxiv</td>
	<td><p>Nearest neighbor search aims at obtaining the samples in the database with the smallest distances from them to the queries which is a basic task in a range of fields including computer vision and data mining. Hashing is one of the most widely used methods for its computational and storage efficiency. With the development of deep learning deep hashing methods show more advantages than traditional methods. In this survey we detailedly investigate current deep hashing algorithms including deep supervised hashing and deep unsupervised hashing. Specifically we categorize deep supervised hashing methods into pairwise methods ranking-based methods pointwise methods as well as quantization according to how measuring the similarities of the learned hash codes. Moreover deep unsupervised hashing is categorized into similarity reconstruction-based methods pseudo-label-based methods and prediction-free self-supervised learning-based methods based on their semantic learning manners. We also introduce three related important topics including semi-supervised deep hashing domain adaption deep hashing and multi-modal deep hashing. Meanwhile we present some commonly used public datasets and the scheme to measure the performance of deep hashing algorithms. Finally we discuss some potential research directions in conclusion.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/liu2024supervised/">Supervised Hashing With Kernels</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Hashing With Kernels' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Supervised Hashing With Kernels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Supervised%20Hashing%20With%20Kernels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Liu W., Wang, Ji, Jiang, Chang</td>
	<td>Arxiv</td>
	<td><p>Recent years have witnessed the growing popularity of hashing in large-scale vision problems. It has been shown that the hashing quality could be boosted by leveraging supervised information into hash function learning. However the existing supervised methods either lack adequate performance or often incur cumbersome model training. In this paper we propose a novel kernel-based supervised hashing model which requires a limited amount of supervised information i.e. similar and dissimilar data pairs and a feasible training cost in achieving high quality hashing. The idea is to map the data to compact binary codes whose Hamming distances are minimized on similar pairs and simultaneously maximized on dissimilar pairs. Our approach is distinct from prior works by utilizing the equivalence between optimizing the code inner products and the Hamming distances. This enables us to sequentially and efficiently train the hash functions one bit at a time yielding very short yet discriminative codes. We carry out extensive experiments on two image benchmarks with up to one million samples demonstrating that our approach significantly outperforms the state-of-the-arts in searching both metric distance neighbors and semantically similar neighbors with accuracy gains ranging from 1337; to 4637;.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/long2024deep/">Deep Domain Adaptation Hashing With Adversarial Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Domain Adaptation Hashing With Adversarial Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Domain Adaptation Hashing With Adversarial Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Domain%20Adaptation%20Hashing%20With%20Adversarial%20Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Long Fuchen, Yao, Dai, Tian, Luo, Mei</td>
	<td>Arxiv</td>
	<td><p>The recent advances in deep neural networks have demonstrated high capability in a wide variety of scenarios. Nevertheless fine-tuning deep models in a new domain still requires a significant amount of labeled data despite expensive labeling efforts. A valid question is how to leverage the source knowledge plus unlabeled or only sparsely labeled target data for learning a new model in target domain. The core problem is to bring the source and target distributions closer in the feature space. In the paper we facilitate this issue in an adversarial learning framework in which a domain discriminator is devised to handle domain shift. Particularly we explore the learning in the context of hashing problem which has been studied extensively due to its great efficiency in gigantic data. Specifically a novel Deep Domain Adaptation Hashing with Adversarial learning (DeDAHA) architecture is presented which mainly consists of three components a deep convolutional neural networks (CNN) for learning basic image/frame representation followed by an adversary stream on one hand to optimize the domain discriminator and on the other to interact with each domain-specific hashing stream for encoding image representation to hash codes. The whole architecture is trained end-to-end by jointly optimizing two types of losses i.e. triplet ranking loss to preserve the relative similarity ordering in the input triplets and adversarial loss to maximally fool the domain discriminator with the learnt source and target feature distributions. Extensive experiments are conducted on three domain transfer tasks including cross-domain digits retrieval image to image and image to video transfers on several benchmarks. Our DeDAHA framework achieves superior results when compared to the state-of-the-art techniques.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/liu2024moboost/">Moboost A Self-improvement Framework For Linear-based Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Moboost A Self-improvement Framework For Linear-based Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Moboost A Self-improvement Framework For Linear-based Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Moboost%20A%20Self-improvement%20Framework%20For%20Linear-based%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Liu Xingbo, Nie, Xi, Zhu, Yin</td>
	<td>Arxiv</td>
	<td><p>The linear model is commonly utilized in hashing methods owing to its efficiency. To obtain better accuracy linear-based hashing methods focus on designing a generalized linear objective function with different constraints or penalty terms that consider neighborhood information. In this study we propose a novel generalized framework called Model Boost (MoBoost) which can achieve the self-improvement of the linear-based hashing. The proposed MoBoost is used to improve model parameter optimization for linear-based hashing methods without adding new constraints or penalty terms. In the proposed MoBoost given a linear-based hashing method we first execute the method several times to get several different hash codes for training samples and then combine these different hash codes into one set utilizing one novel fusion strategy. Based on this set of hash codes we learn some new parameters for the linear hash function that can significantly improve accuracy. The proposed MoBoost can be generally adopted in existing linear-based hashing methods achieving more precise and stable performance compared to the original methods while imposing negligible added expenditure in terms of time and space. Extensive experiments are performed based on three benchmark datasets and the results demonstrate the superior performance of the proposed framework.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/liu2024joint/">Joint-modal Distribution-based Similarity Hashing For Large-scale Unsupervised Deep Cross-modal Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Joint-modal Distribution-based Similarity Hashing For Large-scale Unsupervised Deep Cross-modal Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Joint-modal Distribution-based Similarity Hashing For Large-scale Unsupervised Deep Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Joint-modal%20Distribution-based%20Similarity%20Hashing%20For%20Large-scale%20Unsupervised%20Deep%20Cross-modal%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Liu Song, Qian, Guan, Zhan, Ying</td>
	<td>Arxiv</td>
	<td><p>Hashing-based cross-modal search which aims to map multiple modality features into binary codes has attracted increasingly attention due to its storage and search efficiency especially in large-scale database retrieval. Recent unsupervised deep cross-modal hashing methods have shown promising results. However existing approaches typically suffer from two limitations (1) They usually learn cross-modal similarity information separately or in a redundant fusion manner which may fail to capture semantic correlations among instances from different modalities sufficiently and effectively. (2) They seldom consider the sampling and weighting schemes for unsupervised cross-modal hashing resulting in the lack of satisfactory discriminative ability in hash codes. To overcome these limitations we propose a novel unsupervised deep cross-modal hashing method called Joint-modal Distribution-based Similarity Hashing (JDSH) for large-scale cross-modal retrieval. Firstly we propose a novel cross-modal joint-training method by constructing a joint-modal similarity matrix to fully preserve the cross-modal semantic correlations among instances. Secondly we propose a sampling and weighting scheme termed the Distribution-based Similarity Decision and Weighting (DSDW) method for unsupervised cross-modal hashing which is able to generate more discriminative hash codes by pushing semantic similar instance pairs closer and pulling semantic dissimilar instance pairs apart. The experimental results demonstrate the superiority of JDSH compared with several unsupervised cross-modal hashing methods on two public datasets NUS-WIDE and MIRFlickr.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/liu2024model/">Model Optimization Boosting Framework For Linear Model Hash Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Model Optimization Boosting Framework For Linear Model Hash Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Model Optimization Boosting Framework For Linear Model Hash Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Model%20Optimization%20Boosting%20Framework%20For%20Linear%20Model%20Hash%20Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Liu Xingbo, Nie, Zhou, Nie, Yin</td>
	<td>Arxiv</td>
	<td><p>Efficient hashing techniques have attracted extensive research interests in both storage and retrieval of high dimensional data such as images and videos. In existing hashing methods a linear model is commonly utilized owing to its efficiency. To obtain better accuracy linear-based hashing methods focus on designing a generalized linear objective function with different constraints or penalty terms that consider the inherent characteristics and neighborhood information of samples. Differing from existing hashing methods in this study we propose a self-improvement framework called Model Boost (MoBoost) to improve model parameter optimization for linear-based hashing methods without adding new constraints or penalty terms. In the proposed MoBoost for a linear-based hashing method we first repeatedly execute the hashing method to obtain several hash codes to training samples. Then utilizing two novel fusion strategies these codes are fused into a single set. We also propose two new criteria to evaluate the goodness of hash bits during the fusion process. Based on the fused set of hash codes we learn new parameters for the linear hash function that can significantly improve the accuracy. In general the proposed MoBoost can be adopted by existing linear-based hashing methods achieving more precise and stable performance compared to the original methods and adopting the proposed MoBoost will incur negligible time and space costs. To evaluate the proposed MoBoost we performed extensive experiments on four benchmark datasets and the results demonstrate superior performance.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/liu2024hash/">Hash Bit Selection A Unified Solution For Selection Problems In Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Hash Bit Selection A Unified Solution For Selection Problems In Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Hash Bit Selection A Unified Solution For Selection Problems In Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Hash%20Bit%20Selection%20A%20Unified%20Solution%20For%20Selection%20Problems%20In%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Liu X., He, Lang, Chang</td>
	<td>Arxiv</td>
	<td><p>Hashing based methods recently have been shown promising for large-scale nearest neighbor search. However good designs involve difficult decisions of many unknowns â€“ data features hashing algorithms parameter settings kernels etc. In this paper we provide a unified solution as hash bit selection i.e. selecting the most informative hash bits from a pool of candidates that may have been generated under various conditions mentioned above. We represent the candidate bit pool as a vertex- and edge-weighted graph with the pooled bits as vertices. Then we formulate the bit selection problem as quadratic programming over the graph and solve it efficiently by replicator dynamics. Extensive experiments show that our bit selection approach can achieve superior performance over both naive selection methods and state-of-the-art methods under each scenario usually with significant accuracy gains from 1037; to 5037; relatively.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/liu2024discrete/">Discrete Graph Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Discrete Graph Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Discrete Graph Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Discrete%20Graph%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Liu W., Mu, Kumar, Chang</td>
	<td>Arxiv</td>
	<td><p>Hashing has emerged as a popular technique for fast nearest neighbor search in gigantic databases. In particular learning based hashing has received considerable attention due to its appealing storage and search efficiency. However the performance of most unsupervised learning based hashing methods deteriorates rapidly as the hash code length increases. We argue that the degraded performance is due to inferior optimization procedures used to achieve discrete binary codes. This paper presents a graph-based unsupervised hashing model to preserve the neighborhood structure of massive data in a discrete code space. We cast the graph hashing problem into a discrete optimization framework which directly learns the binary codes. A tractable alternating maximization algorithm is then proposed to explicitly deal with the discrete constraints yielding high-quality codes to well capture the local neighborhoods. Extensive experiments performed on four large datasets with up to one million samples show that our discrete optimization based graph hashing method obtains superior search accuracy over state-of-the-art unsupervised hashing methods especially for longer codes.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/liu2024collaborative/">Collaborative Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Collaborative Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Collaborative Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Collaborative%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Liu X., He, Deng, Lang</td>
	<td>Arxiv</td>
	<td><p>Hashing technique has become a promising approach for fast similarity search. Most of existing hashing research pursue the binary codes for the same type of entities by preserving their similarities. In practice there are many scenarios involving nearest neighbor search on the data given in matrix form where two different types of yet naturally associated entities respectively correspond to its two dimensions or views. To fully explore the duality between the two views we propose a collaborative hashing scheme for the data in matrix form to enable fast search in various applications such as image search using bag of words and recommendation using user-item ratings. By simultaneously preserving both the entity similarities in each view and the interrelationship between views our collaborative hashing effectively learns the compact binary codes and the explicit hash functions for out-of-sample extension in an alternating optimization way. Extensive evaluations are conducted on three well-known datasets for search inside a single view and search across different views demonstrating that our proposed method outperforms state-of-the-art baselines with significant accuracy gains ranging from 7.6737; to 45.8737; relatively.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/liu2024discretely/">Discretely Coding Semantic Rank Orders For Supervised Image Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Discretely Coding Semantic Rank Orders For Supervised Image Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Discretely Coding Semantic Rank Orders For Supervised Image Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Discretely%20Coding%20Semantic%20Rank%20Orders%20For%20Supervised%20Image%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Liu Li, Shao, Shen, Yu</td>
	<td>Arxiv</td>
	<td><p>Learning to hash has been recognized to accomplish highly efficient storage and retrieval for large-scale visual data. Particularly ranking-based hashing techniques have recently attracted broad research attention because ranking accuracy among the retrieved data is well explored and their objective is more applicable to realistic search tasks. However directly optimizing discrete hash codes without continuous-relaxations on a nonlinear ranking objective is infeasible by either traditional optimization methods or even recent discrete hashing algorithms. To address this challenging issue in this paper we introduce a novel supervised hashing method dubbed Discrete Semantic Ranking Hashing (DSeRH) which aims to directly embed semantic rank orders into binary codes. In DSeRH a generalized Adaptive Discrete Minimization (ADM) approach is proposed to discretely optimize binary codes with the quadratic nonlinear ranking objective in an iterative manner and is guaranteed to converge quickly. Additionally instead of using 0/1 independent labels to form rank orders as in previous works we generate the listwise rank orders from the high-level semantic word embeddings which can quantitatively capture the intrinsic correlation between different categories. We evaluate our DSeRH coupled with both linear and deep convolutional neural network (CNN) hash functions on three image datasets i.e. CIFAR-10 SUN397 and ImageNet100 and the results manifest that DSeRH can outperform the state-of-the-art ranking-based hashing methods.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/liu2024hashing/">Hashing With Graphs</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Hashing With Graphs' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Hashing With Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Hashing%20With%20Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Liu W., Wang, Kumar, Chang</td>
	<td>Arxiv</td>
	<td><p>Hashing is becoming increasingly popular for efficient nearest neighbor search in massive databases. However learning short codes that yield good search performance is still a challenge. Moreover in many cases realworld data lives on a low-dimensional manifold which should be taken into account to capture meaningful nearest neighbors. In this paper we propose a novel graph-based hashing method which automatically discovers the neighborhood structure inherent in the data to learn appropriate compact codes. To make such an approach computationally feasible we utilize Anchor Graphs to obtain tractable low-rank adjacency matrices. Our formulation allows constant time hashing of a new data point by extrapolating graph Laplacian eigenvectors to eigenfunctions. Finally we describe a hierarchical threshold learning procedure in which each eigenfunction yields multiple bits leading to higher search accuracy. Experimental comparison with the other state-of-the-art methods on two large datasets demonstrates the efficacy of the proposed method.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/liu2024multi/">Multi-view Complementary Hash Tables For Nearest Neighbor Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Multi-view Complementary Hash Tables For Nearest Neighbor Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Multi-view Complementary Hash Tables For Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Multi-view%20Complementary%20Hash%20Tables%20For%20Nearest%20Neighbor%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Liu Xianglong, Huang, Deng, Land</td>
	<td>Arxiv</td>
	<td><p>Recent years have witnessed the success of hashing techniques in fast nearest neighbor search. In practice many applications (e.g. visual search object detection image matching etc.) have enjoyed the benefits of complementary hash tables and information fusion over multiple views. However most of prior research mainly focused on compact hash code cleaning and rare work studies how to build multiple complementary hash tables much less to adaptively integrate information stemming from multiple views. In this paper we first present a novel multi-view complementary hash table method that learns complementary hash tables from the data with multiple views. For single multiview table using exemplar based feature fusion we approximate the inherent data similarities with a low-rank matrix and learn discriminative hash functions in an efficient way. To build complementary tables and meanwhile maintain scalable training and fast out-of-sample extension an exemplar reweighting scheme is introduced to update the induced low-rank similarity in the sequential table construction framework which indeed brings mutual benefits between tables by placing greater importance on exemplars shared by mis-separated neighbors. Extensive experiments on three large-scale image datasets demonstrate that the proposed method significantly outperforms various naive solutions and state-of-the-art multi-table methods.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/lv2024multi/">Multi-probe LSH Efficient Indexing For High-dimensional Similarity Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Multi-probe LSH Efficient Indexing For High-dimensional Similarity Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Multi-probe LSH Efficient Indexing For High-dimensional Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Multi-probe%20LSH%20Efficient%20Indexing%20For%20High-dimensional%20Similarity%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Lv Q., Josephson, Wang, Charikar, Li</td>
	<td>Arxiv</td>
	<td><p>Similarity indices for high-dimensional data are very desirable for building content-based search systems for featurerich data such as audio images videos and other sensor data. Recently locality sensitive hashing (LSH) and its variations have been proposed as indexing techniques for approximate similarity search. A significant drawback of these approaches is the requirement for a large number of hash tables in order to achieve good search quality. This paper proposes a new indexing scheme called multi-probe LSH that overcomes this drawback. Multi-probe LSH is built on the well-known LSH technique but it intelligently probes multiple buckets that are likely to contain query results in a hash table. Our method is inspired by and improves upon recent theoretical work on entropy-based LSH designed to reduce the space requirement of the basic LSH method. We have implemented the multi-probe LSH method and evaluated the implementation with two different high-dimensional datasets. Our evaluation shows that the multi-probe LSH method substantially improves upon previously proposed methods in both space and time efficiency. To achieve the same search quality multi-probe LSH has a similar timeefficiency as the basic LSH method while reducing the number of hash tables by an order of magnitude. In comparison with the entropy-based LSH method to achieve the same search quality multi-probe LSH uses less query time and 5 to 8 times fewer number of hash tables.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/moran2024neighbourhood/">Neighbourhood Preserving Quantisation For LSH</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Neighbourhood Preserving Quantisation For LSH' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Neighbourhood Preserving Quantisation For LSH' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Neighbourhood%20Preserving%20Quantisation%20For%20LSH' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Moran S., Lavrenko, Osborne</td>
	<td>Arxiv</td>
	<td><p>We introduce a scheme for optimally allocating multiple bits per hyperplane for Locality Sensitive Hashing (LSH). Existing approaches binarise LSH projections by thresholding at zero yielding a single bit per dimension. We demonstrate that this is a sub-optimal bit allocation approach that can easily destroy the neighbourhood structure in the original feature space. Our proposed method dubbed Neighbourhood Preserving Quantization (NPQ) assigns multiple bits per hyperplane based upon adaptively learned thresholds. NPQ exploits a pairwise affinity matrix to discretise each dimension such that nearest neighbours in the original feature space fall within the same quantisation thresholds and are therefore assigned identical bits. NPQ is not only applicable to LSH but can also be applied to any low-dimensional projection scheme. Despite using half the number of hyperplanes NPQ is shown to improve LSH-based retrieval accuracy by up to 6537; compared to the state-of-the-art.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/liong2024cross/">Cross-modal Deep Variational Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Cross-modal Deep Variational Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Cross-modal Deep Variational Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Cross-modal%20Deep%20Variational%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Liong Venice, Lu, Tan, Zhou</td>
	<td>Arxiv</td>
	<td><p>In this paper we propose a cross-modal deep variational hashing (CMDVH) method for cross-modality multimedia retrieval. Unlike existing cross-modal hashing methods which learn a single pair of projections to map each example as a binary vector we design a couple of deep neural network to learn non-linear transformations from imagetext input pairs so that unified binary codes can be obtained. We then design the modality-specific neural networks in a probabilistic manner where we model a latent variable as close as possible from the inferred binary codes which is approximated by a posterior distribution regularized by a known prior. Experimental results on three benchmark datasets show the efficacy of the proposed approach.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/lin2024semantics/">Semantics-preserving Hashing For Cross-view Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Semantics-preserving Hashing For Cross-view Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Semantics-preserving Hashing For Cross-view Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Semantics-preserving%20Hashing%20For%20Cross-view%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Lin Zijia, Ding, Wang</td>
	<td>Arxiv</td>
	<td><p>With benefits of low storage costs and high query speeds hashing methods are widely researched for efficiently retrieving large-scale data which commonly contains multiple views e.g. a news report with images videos and texts. In this paper we study the problem of cross-view retrieval and propose an effective Semantics-Preserving Hashing method termed SePH. Given semantic affinities of training data as supervised information SePH transforms them into a probability distribution and approximates it with tobe-learnt hash codes in Hamming space via minimizing the Kullback-Leibler divergence. Then kernel logistic regression with a sampling strategy is utilized to learn the nonlinear projections from features in each view to the learnt hash codes. And for any unseen instance predicted hash codes and their corresponding output probabilities from observed views are utilized to determine its unified hash code using a novel probabilistic approach. Extensive experiments conducted on three benchmark datasets well demonstrate the effectiveness and reasonableness of SePH.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/liong2024deep/">Deep Variational And Structural Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Variational And Structural Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Variational And Structural Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Variational%20And%20Structural%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Liong Venice, Lu, Duan, Tan</td>
	<td>Arxiv</td>
	<td><p>In this paper we propose a deep variational and structural hashing (DVStH) method to learn compact binary codes for multimedia retrieval. Unlike most existing deep hashing methods which use a series of convolution and fully-connected layers to learn binary features we develop a probabilistic framework to infer latent feature representation inside the network. Then we design a struct layer rather than a bottleneck hash layer to obtain binary codes through a simple encoding procedure. By doing these we are able to obtain binary codes discriminatively and generatively. To make it applicable to cross-modal scalable multimedia retrieval we extend our method to a cross-modal deep variational and structural hashing (CM-DVStH). We design a deep fusion network with a struct layer to maximize the correlation between image-text input pairs during the training stage so that a unified binary vector can be obtained. We then design modality-specific hashing networks to handle the out-of-sample extension scenario. Specifically we train a network for each modality which outputs a latent representation that is as close as possible to the binary codes which are inferred from the fusion network. Experimental results on five benchmark datasets are presented to show the efficacy of the proposed approach.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/lin2024general/">A General Two-step Approach To Learning-based Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A General Two-step Approach To Learning-based Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A General Two-step Approach To Learning-based Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20General%20Two-step%20Approach%20To%20Learning-based%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Lin G., Shen, Suter, Hengel</td>
	<td>Arxiv</td>
	<td><p>Most existing approaches to hashing apply a single form of hash function and an optimization process which is typically deeply coupled to this specific form. This tight coupling restricts the flexibility of the method to respond to the data and can result in complex optimization problems that are difficult to solve. Here we propose a flexible yet simple framework that is able to accommodate different types of loss functions and hash functions. This framework allows a number of existing approaches to hashing to be placed in context and simplifies the development of new problem-specific hashing methods. Our framework decomposes hashing learning problem into two steps hash bit learning and hash function learning based on the learned bits. The first step can typically be formulated as binary quadratic problems and the second step can be accomplished by training standard binary classifiers. Both problems have been extensively studied in the literature. Our extensive experiments demonstrate that the proposed framework is effective flexible and outperforms the state-of-the-art.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/lin2024fast/">Fast Supervised Hashing With Decision Trees For High-dimensional Data</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Fast Supervised Hashing With Decision Trees For High-dimensional Data' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Fast Supervised Hashing With Decision Trees For High-dimensional Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Fast%20Supervised%20Hashing%20With%20Decision%20Trees%20For%20High-dimensional%20Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Lin Guosheng, Shen, Shi, Hengel, Suter.</td>
	<td>Arxiv</td>
	<td><p>Supervised hashing aims to map the original features to compact binary codes that are able to preserve label based similarity in the Hamming space. Non-linear hash functions have demonstrated their advantage over linear ones due to their powerful generalization capability. In the literature kernel functions are typically used to achieve non-linearity in hashing which achieve encouraging retrieval performance at the price of slow evaluation and training time. Here we propose to use boosted decision trees for achieving non-linearity in hashing which are fast to train and evaluate hence more suitable for hashing with high dimensional data. In our approach we first propose sub-modular formulations for the hashing binary code inference problem and an efficient GraphCut based block search method for solving large-scale inference. Then we learn hash functions by training boosted decision trees to fit the binary codes. Experiments demonstrate that our proposed method significantly outperforms most state-of-the-art methods in retrieval precision and training time. Especially for highdimensional data our method is orders of magnitude faster than many methods in terms of training time.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/lin2024microsoft/">Microsoft COCO Common Objects In Context</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Microsoft COCO Common Objects In Context' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Microsoft COCO Common Objects In Context' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Microsoft%20COCO%20Common%20Objects%20In%20Context' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Lin Tsung-yi, Maire, Belongie, Bourdev, Girshick, Hays, Perona, Ramanan, Zitnick, Dollar</td>
	<td>Arxiv</td>
	<td><p>We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL ImageNet and SUN. Finally we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/lin2024deep/">Deep Learning Of Binary Hash Codes For Fast Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Learning Of Binary Hash Codes For Fast Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Learning Of Binary Hash Codes For Fast Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Learning%20Of%20Binary%20Hash%20Codes%20For%20Fast%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Lin Kevin, Yang, Hsiao, Chen</td>
	<td>Arxiv</td>
	<td><p>Approximate nearest neighbor search is an efficient strategy for large-scale image retrieval. Encouraged by the recent advances in convolutional neural networks (CNNs) we propose an effective deep learning framework to generate binary hash codes for fast image retrieval. Our idea is that when the data labels are available binary codes can be learned by employing a hidden layer for representing the latent concepts that dominate the class labels. he utilization of the CNN also allows for learning image representations. Unlike other supervised methods that require pair-wised inputs for binary code learning our method learns hash codes and image representations in a point-wised manner making it suitable for large-scale datasets. Experimental results show that our method outperforms several state-of-the-art hashing algorithms on the CIFAR-10 and MNIST datasets. We further demonstrate its scalability and efficacy on a large-scale dataset of 1 million clothing images.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/lin2024optimizing/">Optimizing Ranking Measures For Compact Binary Code Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Optimizing Ranking Measures For Compact Binary Code Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Optimizing Ranking Measures For Compact Binary Code Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Optimizing%20Ranking%20Measures%20For%20Compact%20Binary%20Code%20Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Lin Guosheng, Shen, Wu.</td>
	<td>Arxiv</td>
	<td><p>Hashing has proven a valuable tool for large-scale information retrieval. Despite much success existing hashing methods optimize over simple objectives such as the reconstruction error or graph Laplacian related loss functions instead of the performance evaluation criteria of interestâ€”multivariate performance measures such as the AUC and NDCG. Here we present a general framework (termed StructHash) that allows one to directly optimize multivariate performance measures. The resulting optimization problem can involve exponentially or infinitely many variables and constraints which is more challenging than standard structured output learning. To solve the StructHash optimization problem we use a combination of column generation and cutting-plane techniques. We demonstrate the generality of StructHash by applying it to ranking prediction and image retrieval and show that it outperforms a few state-of-the-art hashing methods.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/li2024two/">Two Birds One Stone Jointly Learning Binary Code For Large-scale Face Image Retrieval And Attributes Prediction</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Two Birds One Stone Jointly Learning Binary Code For Large-scale Face Image Retrieval And Attributes Prediction' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Two Birds One Stone Jointly Learning Binary Code For Large-scale Face Image Retrieval And Attributes Prediction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Two%20Birds%20One%20Stone%20Jointly%20Learning%20Binary%20Code%20For%20Large-scale%20Face%20Image%20Retrieval%20And%20Attributes%20Prediction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Li Yan, Wang, Liu, Jiang, Chen</td>
	<td>Arxiv</td>
	<td><p>We address the challenging large-scale content-based face image retrieval problem intended as searching images based on the presence of specific subject given one face image of him/her. To this end one natural demand is a supervised binary code learning method. While the learned codes might be discriminating people often have a further expectation that whether some semantic message (e.g. visual attributes) can be read from the human-incomprehensible codes. For this purpose we propose a novel binary code learning framework by jointly encoding identity discriminability and a number of facial attributes into unified binary code. In this way the learned binary codes can be applied to not only fine-grained face image retrieval but also facial attributes prediction which is the very innovation of this work just like killing two birds with one stone. To evaluate the effectiveness of the proposed method extensive experiments are conducted on a new purified large-scale web celebrity database named CFW 60K with abundant manual identity and attributes annotation and experimental results exhibit the superiority of our method over state-of-the-art.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/li2024push/">Push For Quantization Deep Fisher Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Push For Quantization Deep Fisher Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Push For Quantization Deep Fisher Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Push%20For%20Quantization%20Deep%20Fisher%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Li Yunqiang, Pei, Zha, Gemert</td>
	<td>Arxiv</td>
	<td><p>Current massive datasets demand light-weight access for analysis. Discrete hashing methods are thus beneficial because they map high-dimensional data to compact binary codes that are efficient to store and process while preserving semantic similarity. To optimize powerful deep learning methods for image hashing gradient-based methods are required. Binary codes however are discrete and thus have no continuous derivatives. Relaxing the problem by solving it in a continuous space and then quantizing the solution is not guaranteed to yield separable binary codes. The quantization needs to be included in the optimization. In this paper we push for quantization We optimize maximum class separability in the binary space. We introduce a margin on distances between dissimilar image pairs as measured in the binary space. In addition to pair-wise distances we draw inspiration from Fishers Linear Discriminant Analysis (Fisher LDA) to maximize the binary distances between classes and at the same time minimize the binary distance of images within the same class. Experiments on CIFAR-10 NUS-WIDE and ImageNet100 demonstrate compact codes comparing favorably to the current state of the art.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/li2024neighborhood/">Neighborhood Preserving Hashing For Scalable Video Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Neighborhood Preserving Hashing For Scalable Video Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Neighborhood Preserving Hashing For Scalable Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Neighborhood%20Preserving%20Hashing%20For%20Scalable%20Video%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Li Shuyan, Chen, Lu, Li, Zhou</td>
	<td>Arxiv</td>
	<td><p>In this paper we propose a Neighborhood Preserving Hashing (NPH) method for scalable video retrieval in an unsupervised manner. Unlike most existing deep video hashing methods which indiscriminately compress an entire video into a binary code we embed the spatial-temporal neighborhood information into the encoding network such that the neighborhood-relevant visual content of a video can be preferentially encoded into a binary code under the guidance of the neighborhood information. Specifically we propose a neighborhood attention mechanism which focuses on partial useful content of each input frame conditioned on the neighborhood information. We then integrate the neighborhood attention mechanism into an RNN-based reconstruction scheme to encourage the binary codes to capture the spatial-temporal structure in a video which is consistent with that in the neighborhood. As a consequence the learned hashing functions can map similar videos to similar binary codes. Extensive experiments on three widely-used benchmark datasets validate the effectiveness of our proposed approach.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/li2024self/">Self-supervised Video Hashing Via Bidirectional Transformers</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Self-supervised Video Hashing Via Bidirectional Transformers' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Self-supervised Video Hashing Via Bidirectional Transformers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Self-supervised%20Video%20Hashing%20Via%20Bidirectional%20Transformers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Li Shuyan, Li, Lu, Zhou</td>
	<td>Arxiv</td>
	<td><p>Most existing unsupervised video hashing methods are built on unidirectional models with less reliable training objectives which underuse the correlations among frames and the similarity structure between videos. To enable efficient scalable video retrieval we propose a self-supervised video Hashing method based on Bidirectional Transformers (BTH). Based on the encoder-decoder structure of transformers we design a visual cloze task to fully exploit the bidirectional correlations between frames. To unveil the similarity structure between unlabeled video data we further develop a similarity reconstruction task by establishing reliable and effective similarity connections in the video space. Furthermore we develop a cluster assignment task to exploit the structural statistics of the whole dataset such that more discriminative binary codes can be learned. Extensive experiments implemented on three public benchmark datasets FCVID ActivityNet and YFCC demonstrate the superiority of our proposed approach.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/li2024very/">Very Sparse Random Projections</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Very Sparse Random Projections' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Very Sparse Random Projections' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Very%20Sparse%20Random%20Projections' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Li Ping, Hastie, Church</td>
	<td>Arxiv</td>
	<td><p>There has been considerable interest in random projections an approximate algorithm for estimating distances between pairs of points in a high-dimensional vector space. Let A in Rn x D be our n points in D dimensions. The method multiplies A by a random matrix R in RD x k reducing the D dimensions down to just k for speeding up the computation. R typically consists of entries of standard normal N(01). It is well known that random projections preserve pairwise distances (in the expectation). Achlioptas proposed sparse random projections by replacing the N(01) entries in R with entries in -101 with probabilities 1/6 2/3 1/6 achieving a threefold speedup in processing time.We recommend using R of entries in -101 with probabilities 1/2âˆšD 1-1âˆšD 1/2âˆšD for achieving a significant âˆšD-fold speedup with little loss in accuracy.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/li2024deep/">Deep Unsupervised Image Hashing By Maximizing Bit Entropy</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Unsupervised Image Hashing By Maximizing Bit Entropy' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Unsupervised Image Hashing By Maximizing Bit Entropy' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Unsupervised%20Image%20Hashing%20By%20Maximizing%20Bit%20Entropy' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Li Yunqiang, Gemert</td>
	<td>Arxiv</td>
	<td><p>Unsupervised hashing is important for indexing huge image or video collections without having expensive annotations available. Hashing aims to learn short binary codes for compact storage and efficient semantic retrieval. We propose an unsupervised deep hashing layer called Bi-half Net that maximizes entropy of the binary codes. Entropy is maximal when both possible values of the bit are uniformly (half-half) distributed. To maximize bit entropy we do not add a term to the loss function as this is difficult to optimize and tune. Instead we design a new parameter-free network layer to explicitly force continuous image features to approximate the optimal half-half bit distribution. This layer is shown to minimize a penalized term of the Wasserstein distance between the learned continuous image features and the optimal half-half bit distribution. Experimental results on the image datasets Flickr25k Nus-wide Cifar-10 Mscoco Mnist and the video datasets Ucf-101 and Hmdb-51 show that our approach leads to compact codes and compares favorably to the current state-of-the-art.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/li2024consistent/">0-bit Consistent Weighted Sampling</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=0-bit Consistent Weighted Sampling' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=0-bit Consistent Weighted Sampling' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=0-bit%20Consistent%20Weighted%20Sampling' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Li P.</td>
	<td>Arxiv</td>
	<td><p>We develop 0-bit consistent weighted sampling (CWS) for efficiently estimating min-max kernel which is a generalization of the resemblance kernel originally designed for binary data. Because the estimator of 0-bit CWS constitutes a positive definite kernel this method can be naturally applied to large-scale data mining problems. Basically if we feed the sampled data from 0-bit CWS to a highly efficient linear classifier (e.g. linear SVM) we effectively (and approximately) train a nonlinear classifier based on the min-max kernel. The accuracy improves as we increase the sample size. In this paper we first demonstrate through an extensive classification study using kernel machines that the min-max kernel often provides an effective measure of similarity for nonnegative data. This helps justify the use of min-max kernel. However as the min-max kernel is nonlinear and might be difficult to be used for industrial applications with massive data we propose to linearize the min-max kernel via 0-bit CWS a simplification of the original CWS method. The previous remarkable work on consistent weighted sampling (CWS) produces samples in the form of (i t) where the i records the location (and in fact also the weights) information analogous to the samples produced by classical minwise hashing on binary data. Because the t is theoretically unbounded it was not immediately clear how to effectively implement CWS for building large-scale linear classifiers. We provide a simple solution by discarding t (which we refer to as the 0-bit scheme). Via an extensive empirical study we show that this 0-bit scheme does not lose essential information. We then apply 0-bit CWS for building linear classifiers to approximate min-max kernel classifiers as extensively validated on a wide range of public datasets. We expect this work will generate interests among data mining practitioners who would like to efficiently utilize the nonlinear information of non-binary and nonnegative data.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/li2024feature/">Feature Learning Based Deep Supervised Hashing With Pairwise Labels</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Feature Learning Based Deep Supervised Hashing With Pairwise Labels' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Feature Learning Based Deep Supervised Hashing With Pairwise Labels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Feature%20Learning%20Based%20Deep%20Supervised%20Hashing%20With%20Pairwise%20Labels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Li Wu-jun, Kang</td>
	<td>Arxiv</td>
	<td><p>Recent years have witnessed wide application of hashing for large-scale image retrieval. However most existing hashing methods are based on handcrafted features which might not be optimally compatible with the hashing procedure. Recently deep hashing methods have been proposed to perform simultaneous feature learning and hash-code learning with deep neural networks which have shown better performance than traditional hashing methods with hand-crafted features. Most of these deep hashing methods are supervised whose supervised information is given with triplet labels. For another common application scenario with pairwise labels there have not existed methods for simultaneous feature learning and hash-code learning. In this paper we propose a novel deep hashing method called deep pairwise-supervised hashing (DPSH) to perform simultaneous feature learning and hashcode learning for applications with pairwise labels. Experiments on real datasets show that our DPSH method can outperform other methods to achieve the state-of-the-art performance in image retrieval applications.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/li2024learning/">Learning Hash Functions Using Column Generation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning Hash Functions Using Column Generation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning Hash Functions Using Column Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20Hash%20Functions%20Using%20Column%20Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Li X., Lin, Shen, Hengel, Dick</td>
	<td>Arxiv</td>
	<td><p>Fast nearest neighbor searching is becoming an increasingly important tool in solving many large-scale problems. Recently a number of approaches to learning datadependent hash functions have been developed. In this work we propose a column generation based method for learning datadependent hash functions on the basis of proximity comparison information. Given a set of triplets that encode the pairwise proximity comparison information our method learns hash functions that preserve the relative comparison relationships in the data as well as possible within the large-margin learning framework. The learning procedure is implemented using column generation and hence is named CGHash. At each iteration of the column generation procedure the best hash function is selected. Unlike most other hashing methods our method generalizes to new data points naturally; and has a training objective which is convex thus ensuring that the global optimum can be identi- fied. Experiments demonstrate that the proposed method learns compact binary codes and that its retrieval performance compares favorably with state-of-the-art methods when tested on a few benchmark datasets.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/lecun2024mnist/">The MNIST Database Of Handwritten Digits</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=The MNIST Database Of Handwritten Digits' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=The MNIST Database Of Handwritten Digits' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=The%20MNIST%20Database%20Of%20Handwritten%20Digits' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Lecun Y., Cortes, Burges</td>
	<td>Arxiv</td>
	<td><p>The MNIST database of handwritten digits available from this page has a training set of 60000 examples and a test set of 10000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/lai2024simultaneous/">Simultaneous Feature Learning And Hash Coding With Deep Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Simultaneous Feature Learning And Hash Coding With Deep Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Simultaneous Feature Learning And Hash Coding With Deep Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Simultaneous%20Feature%20Learning%20And%20Hash%20Coding%20With%20Deep%20Neural%20Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Lai H., Pan, Liu, Yan</td>
	<td>Arxiv</td>
	<td><p>Similarity-preserving hashing is a widely-used method for nearest neighbour search in large-scale image retrieval tasks. For most existing hashing methods an image is first encoded as a vector of hand-engineering visual features followed by another separate projection or quantization step that generates binary codes. However such visual feature vectors may not be optimally compatible with the coding process thus producing sub-optimal hashing codes. In this paper we propose a deep architecture for supervised hashing in which images are mapped into binary codes via carefully designed deep neural networks. The pipeline of the proposed deep architecture consists of three building blocks 1) a sub-network with a stack of convolution layers to produce the effective intermediate image features; 2) a divide-and-encode module to divide the intermediate image features into multiple branches each encoded into one hash bit; and 3) a triplet ranking loss designed to characterize that one image is more similar to the second image than to the third one. Extensive evaluations on several benchmark image datasets show that the proposed simultaneous feature learning and hash coding pipeline brings substantial improvements over other state-of-the-art supervised or unsupervised hashing methods.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/kwok2024learning/">Learning To Hash With A Dimension Analysis-based Quantizer For Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning To Hash With A Dimension Analysis-based Quantizer For Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning To Hash With A Dimension Analysis-based Quantizer For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20To%20Hash%20With%20A%20Dimension%20Analysis-based%20Quantizer%20For%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Kwok Yuan</td>
	<td>Arxiv</td>
	<td><p>The last few years have witnessed the rise of the big data era in which approximate nearest neighbor search is a fundamental problem in many applications such as large-scale image retrieval. Recently many research results have demonstrated that hashing can achieve promising performance due to its appealing storage and search efficiency. Since complex optimization problems for loss functions are difficult to solve most hashing methods decompose the hash code learning problem into two steps projection and quantization. In the quantization step binary codes are widely used because ranking them by the Hamming distance is very efficient. However the massive information loss produced by the quantization step should be reduced in applications where high search accuracy is required such as in image retrieval. Since many two-step hashing methods produce uneven projected dimensions in the projection step in this paper we propose a novel dimension analysis-based quantization (DAQ) on two-step hashing methods for image retrieval. We first perform an importance analysis of the projected dimensions and select a subset of them that are more informative than others and then we divide the selected projected dimensions into several regions with our quantizer. Every region is quantized with its corresponding codebook. Finally the similarity between two hash codes is estimated by the Manhattan distance between their corresponding codebooks which is also efficient. We conduct experiments on three public benchmarks containing up to one million descriptors and show that the proposed DAQ method consistently leads to significant accuracy improvements over state-of-the-art quantization methods.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/kumar2024learning/">Learning Hash Functions For Cross-view Similarity Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning Hash Functions For Cross-view Similarity Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning Hash Functions For Cross-view Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20Hash%20Functions%20For%20Cross-view%20Similarity%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Kumar S., Udupa</td>
	<td>Arxiv</td>
	<td><p>Many applications in Multilingual and Multimodal Information Access involve searching large databases of high dimensional data objects with multiple (conditionally independent) views. In this work we consider the problem of learning hash functions for similarity search across the views for such applications. We propose a principled method for learning a hash function for each view given a set of multiview training data objects. The hash functions map similar objects to similar codes across the views thus enabling cross-view similarity search. We present results from an extensive empirical study of the proposed approach which demonstrate its effectiveness on Japanese language People Search and Multilingual People Search problems.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/kulis2024kernelized/">Kernelized Locality-sensitive Hashing For Scalable Image Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Kernelized Locality-sensitive Hashing For Scalable Image Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Kernelized Locality-sensitive Hashing For Scalable Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Kernelized%20Locality-sensitive%20Hashing%20For%20Scalable%20Image%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Kulis B., Grauman</td>
	<td>Arxiv</td>
	<td><p>Fast retrieval methods are critical for large-scale and data-driven vision applications. Recent work has explored ways to embed high-dimensional features or complex distance functions into a low-dimensional Hamming space where items can be efficiently searched. However existing methods do not apply for high-dimensional kernelized data when the underlying feature embedding for the kernel is unknown. We show how to generalize locality-sensitive hashing to accommodate arbitrary kernel functions making it possible to preserve the algorithmâ€™s sub-linear time similarity search guarantees for a wide class of useful similarity functions. Since a number of successful image-based kernels have unknown or incomputable embeddings this is especially valuable for image retrieval tasks. We validate our technique on several large-scale datasets and show that it enables accurate and fast performance for example-based object classification feature matching and content-based retrieval.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/kulis2024learning/">Learning To Hash With Binary Reconstructive Embeddings</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning To Hash With Binary Reconstructive Embeddings' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning To Hash With Binary Reconstructive Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20To%20Hash%20With%20Binary%20Reconstructive%20Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Kulis B., Darrell</td>
	<td>Arxiv</td>
	<td><p>Fast retrieval methods are increasingly critical for many large-scale analysis tasks and there have been several recent methods that attempt to learn hash functions for fast and accurate nearest neighbor searches. In this paper we develop an algorithm for learning hash functions based on explicitly minimizing the reconstruction error between the original distances and the Hamming distances of the corresponding binary embeddings. We develop a scalable coordinate-descent algorithm for our proposed hashing objective that is able to efficiently learn hash functions in a variety of settings. Unlike existing methods such as semantic hashing and spectral hashing our method is easily kernelized and does not require restrictive assumptions about the underlying distribution of the data. We present results over several domains to demonstrate that our method outperforms existing state-of-the-art techniques.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/kusupati2024llc/">LLC Accurate Multi-purpose Learnt Low-dimensional Binary Codes</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=LLC Accurate Multi-purpose Learnt Low-dimensional Binary Codes' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=LLC Accurate Multi-purpose Learnt Low-dimensional Binary Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=LLC%20Accurate%20Multi-purpose%20Learnt%20Low-dimensional%20Binary%20Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Kusupati Aditya, Wallingford, Ramanujan, Somani, Park, Pillutla, Jain, Kakade, Farhadi</td>
	<td>Arxiv</td>
	<td><p>Learning binary representations of instances and classes is a classical problem with several high potential applications. In modern settings the compression of high-dimensional neural representations to low-dimensional binary codes is a challenging task and often require large bit-codes to be accurate. In this work we propose a novel method for Learning Low-dimensional binary Codes (LLC) for instances as well as classes. Our method does not require any side-information like annotated attributes or label meta-data and learns extremely low-dimensional binary codes (~20 bits for ImageNet-1K). The learnt codes are super-efficient while still ensuring nearly optimal classification accuracy for ResNet50 on ImageNet-1K. We demonstrate that the learnt codes capture intrinsically important features in the data by discovering an intuitive taxonomy over classes. We further quantitatively measure the quality of our codes by applying it to the efficient image retrieval as well as out-of-distribution (OOD) detection problems. For ImageNet-100 retrieval problem our learnt binary codes outperform 16 bit HashNet using only 10 bits and also are as accurate as 10 dimensional real representations. Finally our learnt binary codes can perform OOD detection out-of-the-box as accurately as a baseline that needs ~3000 samples to tune its threshold while we require none.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/leng2024hashing/">Hashing For Distributed Data</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Hashing For Distributed Data' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Hashing For Distributed Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Hashing%20For%20Distributed%20Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Leng Cong, Wu, Cheng, Lu</td>
	<td>Arxiv</td>
	<td><p>Recently hashing based approximate nearest neighbors search has attracted much attention. Extensive centralized hashing algorithms have been proposed and achieved promising performance. However due to the large scale of many applications the data is often stored or even collected in a distributed manner. Learning hash functions by aggregating all the data into a fusion center is infeasible because of the prohibitively expensive communication and computation overhead. In this paper we develop a novel hashing model to learn hash functions in a distributed setting. We cast a centralized hashing model as a set of subproblems with consensus constraints. We find these subproblems can be analytically solved in parallel on the distributed compute nodes. Since no training data is transmitted across the nodes in the learning process the communication cost of our model is independent to the data size. Extensive experiments on several large scale datasets containing up to 100 million samples demonstrate the efficacy of our method.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/moran2024regularised/">Regularised Cross-modal Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Regularised Cross-modal Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Regularised Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Regularised%20Cross-modal%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Moran S., Lavrenko</td>
	<td>Arxiv</td>
	<td><p>In this paper we propose Regularised Cross-Modal Hashing (RCMH) a new cross-modal hashing scheme that projects annotation and visual feature descriptors into a common Hamming space. RCMH optimises the intra-modality similarity of data-points in the annotation modality using an iterative three-step hashing algorithm in the first step each training image is assigned a K-bit hashcode based on hyperplanes learnt at the previous iteration; in the second step the binary bits are smoothed by a formulation of graph regularisation so that similar data-points have similar bits; in the third step a set of binary classifiers are trained to predict the regularised bits with maximum margin. Visual descriptors are projected into the annotation Hamming space by a set of binary classifiers learnt using the bits of the corresponding annotations as labels. RCMH is shown to consistently improve retrieval effectiveness over state-of-the-art baselines.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/kong2024double/">Double-bit Quantisation For Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Double-bit Quantisation For Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Double-bit Quantisation For Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Double-bit%20Quantisation%20For%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Kong W., Li</td>
	<td>Arxiv</td>
	<td><p>Hashing which tries to learn similarity-preserving binary codes for data representation has been widely used for efficient nearest neighbor search in massive databases due to its fast query speed and low storage cost. Because it is NP hard to directly compute the best binary codes for a given data set mainstream hashing methods typically adopt a two-stage strategy. In the first stage several projected dimensions of real values are generated. Then in the second stage the real values will be quantized into binary codes by thresholding. Currently most existing methods use one single bit to quantize each projected dimension. One problem with this single-bit quantization (SBQ) is that the threshold typically lies in the region of the highest point density and consequently a lot of neighboring points close to the threshold will be hashed to totally different bits which is unexpected according to the principle of hashing. In this paper we propose a novel quantization strategy called double-bit quantization (DBQ) to solve the problem of SBQ. The basic idea of DBQ is to quantize each projected dimension into double bits with adaptively learned thresholds. Extensive experiments on two real data sets show that our DBQ strategy can signifi- cantly outperform traditional SBQ strategy for hashing.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/kapralov2024adversarial/">On The Adversarial Robustness Of Locality-sensitive Hashing In Hamming Space</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=On The Adversarial Robustness Of Locality-sensitive Hashing In Hamming Space' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=On The Adversarial Robustness Of Locality-sensitive Hashing In Hamming Space' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=On%20The%20Adversarial%20Robustness%20Of%20Locality-sensitive%20Hashing%20In%20Hamming%20Space' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Kapralov Michael, Makarov Mikhail, Sohler Christian</td>
	<td>Arxiv</td>
	<td><p>Locality-sensitive hashing~IndykMotwani98 is a classical data structure for approximate nearest neighbor search. It allows after a close to linear time preprocessing of the input dataset to find an approximately nearest neighbor of any fixed query in sublinear time in the dataset size. The resulting data structure is randomized and succeeds with high probability for every fixed query. In many modern applications of nearest neighbor search the queries are chosen adaptively. In this paper we study the robustness of the locality-sensitive hashing to adaptive queries in Hamming space. We present a simple adversary that can under mild assumptions on the initial point set provably find a query to the approximate near neighbor search data structure that the data structure fails on. Crucially our adaptive algorithm finds the hard query exponentially faster than random sampling.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/kang2024maximum/">Maximum-margin Hamming Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Maximum-margin Hamming Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Maximum-margin Hamming Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Maximum-margin%20Hamming%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Kang Rong, Cao, (b), Wang, Yu</td>
	<td>Arxiv</td>
	<td><p>Deep hashing enables computation and memory efficient image search through end-to-end learning of feature representations and binary codes. While linear scan over binary hash codes is more efficient than over the high-dimensional representations its linear-time complexity is still unacceptable for very large databases. Hamming space retrieval enables constant-time search through hash lookups where for each query there is a Hamming ball centered at the query and the data points within the ball are returned as relevant. Since inside the Hamming ball implies retrievable while outside irretrievable it is crucial to explicitly characterize the Hamming ball. The main idea of this work is to directly embody the Hamming radius into the loss functions leading to Maximum-Margin Hamming Hashing (MMHH) a new model specifically optimized for Hamming space retrieval. We introduce a max-margin t-distribution loss where the t-distribution concentrates more similar data points to be within the Hamming ball and the margin characterizes the Hamming radius such that less penalization is applied to similar data points within the Hamming ball. The loss function also introduces robustness to data noise where the similarity supervision may be inaccurate in practical problems. The model is trained end-to-end using a new semi-batch optimization algorithm tailored to extremely imbalanced data. Our method yields state-of-the-art results on four datasets and shows superior performance on noisy data.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/kang2024column/">Column Sampling Based Discrete Supervised Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Column Sampling Based Discrete Supervised Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Column Sampling Based Discrete Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Column%20Sampling%20Based%20Discrete%20Supervised%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Kang Wang-cheng, Li, Zhou</td>
	<td>Arxiv</td>
	<td><p>By leveraging semantic (label) information supervised hashing has demonstrated better accuracy than unsupervised hashing in many real applications. Because the hashing-code learning problem is essentially a discrete optimization problem which is hard to solve most existing supervised hashing methods try to solve a relaxed continuous optimization problem by dropping the discrete constraints. However these methods typically suffer from poor performance due to the errors caused by the relaxation. Some other methods try to directly solve the discrete optimization problem. However they are typically time-consuming and unscalable. In this paper we propose a novel method called column sampling based discrete supervised hashing (COSDISH) to directly learn the discrete hashing code from semantic information. COSDISH is an iterative method in each iteration of which several columns are sampled from the semantic similarity matrix and then the hashing code is decomposed into two parts which can be alternately optimized in a discrete way. Theoretical analysis shows that the learning (optimization) algorithm of COSDISH has a constant-approximation bound in each step of the alternating optimization procedure. Empirical results on datasets with semantic labels illustrate that COSDISH can outperform the state-of-the-art methods in real applications like image retrieval.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/kong2024isotropic/">Isotropic Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Isotropic Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Isotropic Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Isotropic%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Kong W., Li</td>
	<td>Arxiv</td>
	<td><p>Most existing hashing methods adopt some projection functions to project the original data into several dimensions of real values and then each of these projected dimensions is quantized into one bit (zero or one) by thresholding. Typically the variances of different projected dimensions are different for existing projection functions such as principal component analysis (PCA). Using the same number of bits for different projected dimensions is unreasonable because larger-variance dimensions will carry more information. Although this viewpoint has been widely accepted by many researchers it is still not verified by either theory or experiment because no methods have been proposed to find a projection with equal variances for different dimensions. In this paper we propose a novel method called isotropic hashing (IsoHash) to learn projection functions which can produce projected dimensions with isotropic variances (equal variances). Experimental results on real data sets show that IsoHash can outperform its counterpart with different variances for different dimensions which verifies the viewpoint that projections with isotropic variances will be better than those with anisotropic variances.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/joly2024random/">Random Maximum Margin Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Random Maximum Margin Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Random Maximum Margin Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Random%20Maximum%20Margin%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Joly A., Buisson</td>
	<td>Arxiv</td>
	<td><p>Following the success of hashing methods for multidimensional indexing more and more works are interested in embedding visual feature space in compact hash codes. Such approaches are not an alternative to using index structures but a complementary way to reduce both the memory usage and the distance computation cost. Several data dependent hash functions have notably been proposed to closely fit data distribution and provide better selectivity than usual random projections such as LSH. However improvements occur only for relatively small hash code sizes up to 64 or 128 bits. As discussed in the paper this is mainly due to the lack of independence between the produced hash functions. We introduce a new hash function family that attempts to solve this issue in any kernel space. Rather than boosting the collision probability of close points our method focus on data scattering. By training purely random splits of the data regardless the closeness of the training samples it is indeed possible to generate consistently more independent hash functions. On the other side the use of large margin classifiers allows to maintain good generalization performances. Experiments show that our new Random Maximum Margin Hashing scheme (RMMH) outperforms four state-of-the-art hashing methods notably in kernel spaces.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/jin2024deep/">Deep Saliency Hashing For Fine-grained Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Saliency Hashing For Fine-grained Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Saliency Hashing For Fine-grained Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Saliency%20Hashing%20For%20Fine-grained%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Jin Sheng, Yao, Sun, Zhou, Zhang, Hua</td>
	<td>Arxiv</td>
	<td><p>In recent years hashing methods have been proved to be effective and efficient for the large-scale Web media search. However the existing general hashing methods have limited discriminative power for describing fine-grained objects that share similar overall appearance but have subtle difference. To solve this problem we for the first time introduce the attention mechanism to the learning of fine-grained hashing codes. Specifically we propose a novel deep hashing model named deep saliency hashing (DSaH) which automatically mines salient regions and learns semanticpreserving hashing codes simultaneously. DSaH is a twostep end-to-end model consisting of an attention network and a hashing network. Our loss function contains three basic components including the semantic loss the saliency loss and the quantization loss. As the core of DSaH the saliency loss guides the attention network to mine discriminative regions from pairs of images. We conduct extensive experiments on both fine-grained and general retrieval datasets for performance evaluation. Experimental results on fine grained dataset including Oxford Flowers-17 Stanford Dogs-120 and CUB Bird demonstrate that our DSaH performs the best for fine-grained retrieval task and beats strongest competitor (DTQ) by approximately 1037; on both Stanford Dogs-120 and CUB Bird. DSaH is also comparable to several state-of-the-art hashing methods on general datasets including CIFAR-10 and NUS-WIDE.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/jin2024complementary/">Complementary Projection Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Complementary Projection Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Complementary Projection Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Complementary%20Projection%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Jin Z., Hu, Lin, Zhang, Lin, Cai, Li</td>
	<td>Arxiv</td>
	<td><p>Recently hashing techniques have been widely applied to solve the approximate nearest neighbors search problem in many vision applications. Generally these hashing approaches generate 2^c buckets where c is the length of the hash code. A good hashing method should satisfy the following two requirements 1) mapping the nearby data points into the same bucket or nearby (measured by the Hamming distance) buckets. 2) all the data points are evenly distributed among all the buckets. In this paper we propose a novel algorithm named Complementary Projection Hashing (CPH) to find the optimal hashing functions which explicitly considers the above two requirements. Specifically CPH aims at sequentially finding a series of hyperplanes (hashing functions) which cross the sparse region of the data. At the same time the data points are evenly distributed in the hypercubes generated by these hyperplanes. The experiments comparing with the state-of-the-art hashing methods demonstrate the effectiveness of the proposed method.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/jin2024ssah/">SSAH Semi-supervised Adversarial Deep Hashing With Self-paced Hard Sample Generation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=SSAH Semi-supervised Adversarial Deep Hashing With Self-paced Hard Sample Generation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=SSAH Semi-supervised Adversarial Deep Hashing With Self-paced Hard Sample Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=SSAH%20Semi-supervised%20Adversarial%20Deep%20Hashing%20With%20Self-paced%20Hard%20Sample%20Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Jin Sheng, Zhou, Liu, Chen, Sun, Yao, Hua</td>
	<td>Arxiv</td>
	<td><p>Deep hashing methods have been proved to be effective and efficient for large-scale Web media search. The success of these data-driven methods largely depends on collecting sufficient labeled data which is usually a crucial limitation in practical cases. The current solutions to this issue utilize Generative Adversarial Network (GAN) to augment data in semi-supervised learning. However existing GAN-based methods treat image generations and hashing learning as two isolated processes leading to generation ineffectiveness. Besides most works fail to exploit the semantic information in unlabeled data. In this paper we propose a novel Semi-supervised Self-pace Adversarial Hashing method named SSAH to solve the above problems in a unified framework. The SSAH method consists of an adversarial network (A-Net) and a hashing network (H-Net). To improve the quality of generative images first the A-Net learns hard samples with multi-scale occlusions and multi-angle rotated deformations which compete against the learning of accurate hashing codes. Second we design a novel self-paced hard generation policy to gradually increase the hashing difficulty of generated samples. To make use of the semantic information in unlabeled ones we propose a semi-supervised consistent loss. The experimental results show that our method can significantly improve state-of-the-art models on both the widely-used hashing datasets and fine-grained datasets.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/jiang2024deep/">Deep Cross-modal Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Cross-modal Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Cross-modal%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Jiang Qing-yuan, Li</td>
	<td>Arxiv</td>
	<td><p>Due to its low storage cost and fast query speed crossmodal hashing (CMH) has been widely used for similarity search in multimedia retrieval applications. However most existing CMH methods are based on hand-crafted features which might not be optimally compatible with the hash-code learning procedure. As a result existing CMH methods with hand-crafted features may not achieve satisfactory performance. In this paper we propose a novel CMH method called deep cross-modal hashing (DCMH) by integrating feature learning and hash-code learning into the same framework. DCMH is an end-to-end learning framework with deep neural networks one for each modality to perform feature learning from scratch. Experiments on three real datasets with image-text modalities show that DCMH can outperform other baselines to achieve the state-of-the-art performance in cross-modal retrieval applications.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/jiang2024scalable/">Scalable Graph Hashing With Feature Transformation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Scalable Graph Hashing With Feature Transformation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Scalable Graph Hashing With Feature Transformation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Scalable%20Graph%20Hashing%20With%20Feature%20Transformation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Jiang Q., Li</td>
	<td>Arxiv</td>
	<td><p>Hashing has been widely used for approximate nearest neighbor (ANN) search in big data applications because of its low storage cost and fast retrieval speed. The goal of hashing is to map the data points from the original space into a binary-code space where the similarity (neighborhood structure) in the original space is preserved. By directly exploiting the similarity to guide the hashing code learning procedure graph hashing has attracted much attention. However most existing graph hashing methods cannot achieve satisfactory performance in real applications due to the high complexity for graph modeling. In this paper we propose a novel method called scalable graph hashing with feature transformation (SGH) for large-scale graph hashing. Through feature transformation we can effectively approximate the whole graph without explicitly computing the similarity graph matrix based on which a sequential learning method is proposed to learn the hash functions in a bit-wise manner. Experiments on two datasets with one million data points show that our SGH method can outperform the state-of-the-art methods in terms of both accuracy and scalability.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/jin2024unsupervised/">Unsupervised Discrete Hashing With Affinity Similarity</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Discrete Hashing With Affinity Similarity' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Unsupervised Discrete Hashing With Affinity Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Unsupervised%20Discrete%20Hashing%20With%20Affinity%20Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Jin Sheng, Yao, Zhou, Liu, Huang, Hua</td>
	<td>Arxiv</td>
	<td><p>In recent years supervised hashing has been validated to greatly boost the performance of image retrieval. However the label-hungry property requires massive label collection making it intractable in practical scenarios. To liberate the model training procedure from laborious manual annotations some unsupervised methods are proposed. However the following two factors make unsupervised algorithms inferior to their supervised counterparts (1) Without manually-defined labels it is difficult to capture the semantic information across data which is of crucial importance to guide robust binary code learning. (2) The widely adopted relaxation on binary constraints results in quantization error accumulation in the optimization procedure. To address the above-mentioned problems in this paper we propose a novel Unsupervised Discrete Hashing method (UDH). Specifically to capture the semantic information we propose a balanced graph-based semantic loss which explores the affinity priors in the original feature space. Then we propose a novel self-supervised loss termed orthogonal consistent loss which can leverage semantic loss of instance and impose independence of codes. Moreover by integrating the discrete optimization into the proposed unsupervised framework the binary constraints are consistently preserved alleviating the influence of quantization errors. Extensive experiments demonstrate that UDH outperforms state-of-the-art unsupervised methods for image retrieval.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/kong2024manhattan/">Manhattan Hashing For Large-scale Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Manhattan Hashing For Large-scale Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Manhattan Hashing For Large-scale Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Manhattan%20Hashing%20For%20Large-scale%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Kong W., Li, Guo</td>
	<td>Arxiv</td>
	<td><p>Hashing is used to learn binary-code representation for data with expectation of preserving the neighborhood structure in the original feature space. Due to its fast query speed and reduced storage cost hashing has been widely used for efficient nearest neighbor search in a large variety of applications like text and image retrieval. Most existing hashing methods adopt Hamming distance to measure the similarity (neighborhood) between points in the hashcode space. However one problem with Hamming distance is that it may destroy the neighborhood structure in the original feature space which violates the essential goal of hashing. In this paper Manhattan hashing (MH) which is based on Manhattan distance is proposed to solve the problem of Hamming distance based hashing. The basic idea of MH is to encode each projected dimension with multiple bits of natural binary code (NBC) based on which the Manhattan distance between points in the hashcode space is calculated for nearest neighbor search. MH can effectively preserve the neighborhood structure in the data to achieve the goal of hashing. To the best of our knowledge this is the first work to adopt Manhattan distance with NBC for hashing. Experiments on several largescale image data sets containing up to one million points show that our MH method can significantly outperform other state-of-the-art methods.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/jegou2024searching/">Searching With Quantization Approximate Nearest Neighbor Search Using Short Codes And Distance Estimators</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Searching With Quantization Approximate Nearest Neighbor Search Using Short Codes And Distance Estimators' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Searching With Quantization Approximate Nearest Neighbor Search Using Short Codes And Distance Estimators' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Searching%20With%20Quantization%20Approximate%20Nearest%20Neighbor%20Search%20Using%20Short%20Codes%20And%20Distance%20Estimators' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Jegou H., Douze, Schmid</td>
	<td>Arxiv</td>
	<td><p>We propose an approximate nearest neighbor search method based on quantization. It uses in particular product quantizer to produce short codes and corresponding distance estimators approximating the Euclidean distance between the orginal vectors. The method is advantageously used in an asymmetric manner by computing the distance between a vector and code unlike competing techniques such as spectral hashing that only compare codes. Our approach approximates the Euclidean distance based on memory efficient codes and thus permits efficient nearest neighbor search. Experiments performed on SIFT and GIST image descriptors show excellent search accuracy. The method is shown to outperform two state-of-the-art approaches of the literature. Timings measured when searching a vector set of 2 billion vectors are shown to be excellent given the high accuracy of the method.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/jia2024fast/">Fast Online Hashing With Multi-label Projection</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Fast Online Hashing With Multi-label Projection' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Fast Online Hashing With Multi-label Projection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Fast%20Online%20Hashing%20With%20Multi-label%20Projection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Jia Wenzhe, Cao, Liu, Gui</td>
	<td>Arxiv</td>
	<td><p>Hashing has been widely researched to solve the large-scale approximate nearest neighbor search problem owing to its time and storage superiority. In recent years a number of online hashing methods have emerged which can update the hash functions to adapt to the new stream data and realize dynamic retrieval. However existing online hashing methods are required to update the whole database with the latest hash functions when a query arrives which leads to low retrieval efficiency with the continuous increase of the stream data. On the other hand these methods ignore the supervision relationship among the examples especially in the multi-label case. In this paper we propose a novel Fast Online Hashing (FOH) method which only updates the binary codes of a small part of the database. To be specific we first build a query pool in which the nearest neighbors of each central point are recorded. When a new query arrives only the binary codes of the corresponding potential neighbors are updated. In addition we create a similarity matrix which takes the multi-label supervision information into account and bring in the multi-label projection loss to further preserve the similarity among the multi-label data. The experimental results on two common benchmarks show that the proposed FOH can achieve dramatic superiority on query time up to 6.28 seconds less than state-of-the-art baselines with competitive retrieval accuracy.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/jegou2024datasets/">Datasets For Approximate Nearest Neighbor Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Datasets For Approximate Nearest Neighbor Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Datasets For Approximate Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Datasets%20For%20Approximate%20Nearest%20Neighbor%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Jegou Herve, Amsaleg</td>
	<td>Arxiv</td>
	<td><p>BIGANN consists of SIFT descriptors applied to images from extracted from a large image dataset.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/jain2024fast/">Fast Similarity Search For Learned Metrics</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Fast Similarity Search For Learned Metrics' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Fast Similarity Search For Learned Metrics' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Fast%20Similarity%20Search%20For%20Learned%20Metrics' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Jain P., Kulis, Grauman</td>
	<td>Arxiv</td>
	<td><p>We propose a method to efficiently index into a large database of examples according to a learned metric. Given a collection of examples we learn a Mahalanobis distance using an information-theoretic metric learning technique that adapts prior knowledge about pairwise distances to incorporate similarity and dissimilarity constraints. To enable sub-linear time similarity search under the learned metric we show how to encode a learned Mahalanobis parameterization into randomized locality-sensitive hash functions. We further formulate an indirect solution that enables metric learning and hashing for sparse input vector spaces whose high dimensionality make it infeasible to learn an explicit weighting over the feature dimensions. We demonstrate the approach applied to systems and image datasets and show that our learned metrics improve accuracy relative to commonly-used metric baselines while our hashing construction permits effi- cient indexing with a learned distance and very large databases.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/jain2024hashing/">Hashing Hyperplane Queries To Near Points With Applications To Large-scale Active Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Hashing Hyperplane Queries To Near Points With Applications To Large-scale Active Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Hashing Hyperplane Queries To Near Points With Applications To Large-scale Active Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Hashing%20Hyperplane%20Queries%20To%20Near%20Points%20With%20Applications%20To%20Large-scale%20Active%20Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Jain P., Vijayanarasimhan, Grauman</td>
	<td>Arxiv</td>
	<td><p>We consider the problem of retrieving the database points nearest to a given hyperplane query without exhaustively scanning the database. We propose two hashing-based solutions. Our first approach maps the data to two-bit binary keys that are locality-sensitive for the angle between the hyperplane normal and a database point. Our second approach embeds the data into a vector space where the Euclidean norm reflects the desired distance between the original points and hyperplane query. Both use hashing to retrieve near points in sub-linear time. Our first methods preprocessing stage is more efficient while the second has stronger accuracy guarantees. We apply both to pool-based active learning taking the current hyperplane classifier as a query our algorithm identifies those points (approximately) satisfying the well-known minimal distance-to-hyperplane selection criterion. We empirically demonstrate our methods tradeoffs and show that they make it practical to perform active selection with millions of unlabeled points.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/jegou2024microsoft/">Microsoft Turing-anns-1b</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Microsoft Turing-anns-1b' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Microsoft Turing-anns-1b' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Microsoft%20Turing-anns-1b' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Jegou Herve</td>
	<td>Arxiv</td>
	<td><p>Microsoft Turing-ANNS-1B is a new dataset being released by the Microsoft Turing team for this competition. It consists of Bing queries encoded by Turing AGI v5 that trains Transformers to capture similarity of intent in web search queries. An early version of the RNN-based AGI Encoder is described in a SIGIR19 paper and a blogpost.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/irie2024locally/">Locally Linear Hashing For Extracting Non-linear Manifolds</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Locally Linear Hashing For Extracting Non-linear Manifolds' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Locally Linear Hashing For Extracting Non-linear Manifolds' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Locally%20Linear%20Hashing%20For%20Extracting%20Non-linear%20Manifolds' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Irie G., Li, Wu, Chang</td>
	<td>Arxiv</td>
	<td><p>Previous efforts in hashing intend to preserve data variance or pairwise affinity but neither is adequate in capturing the manifold structures hidden in most visual data. In this paper we tackle this problem by reconstructing the locally linear structures of manifolds in the binary Hamming space which can be learned by locality-sensitive sparse coding. We cast the problem as a joint minimization of reconstruction error and quantization loss and show that despite its NP-hardness a local optimum can be obtained efficiently via alternative optimization. Our method distinguishes itself from existing methods in its remarkable ability to extract the nearest neighbors of the query from the same manifold instead of from the ambient space. On extensive experiments on various image benchmarks our results improve previous state-of-the-art by 28-7437; typically and 62737; on the Yale face data.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/huang2024accelerate/">Accelerate Learning Of Deep Hashing With Gradient Attention</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Accelerate Learning Of Deep Hashing With Gradient Attention' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Accelerate Learning Of Deep Hashing With Gradient Attention' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Accelerate%20Learning%20Of%20Deep%20Hashing%20With%20Gradient%20Attention' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Huang Long-kai, Chen, Pan</td>
	<td>Arxiv</td>
	<td><p>Recent years have witnessed the success of learning to hash in fast large-scale image retrieval. As deep learning has shown its superior performance on many computer vision applications recent designs of learning-based hashing models have been moving from shallow ones to deep architectures. However based on our analysis we find that gradient descent based algorithms used in deep hashing models would potentially cause hash codes of a pair of training instances to be updated towards the directions of each other simultaneously during optimization. In the worst case the paired hash codes switch their directions after update and consequently their corresponding distance in the Hamming space remain unchanged. This makes the overall learning process highly inefficient. To address this issue we propose a new deep hashing model integrated with a novel gradient attention mechanism. Extensive experimental results on three benchmark datasets show that our proposed algorithm is able to accelerate the learning process and obtain competitive retrieval performance compared with state-of-the-art deep hashing models.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/huiskes2024mir/">The MIR Flickr Retrieval Evaluation.</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=The MIR Flickr Retrieval Evaluation.' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=The MIR Flickr Retrieval Evaluation.' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=The%20MIR%20Flickr%20Retrieval%20Evaluation.' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Huiskes M., Lew</td>
	<td>Arxiv</td>
	<td><p>In most well known image retrieval test sets the imagery typically cannot be freely distributed or is not representative of a large community of users. In this paper we present a collection for the MIR community comprising 25000 images from the Flickr website which are redistributable for research purposes and represent a real community of users both in the image content and image tags. We have extracted the tags and EXIF image metadata and also make all of these publicly available. In addition we discuss several challenges for benchmarking retrieval and classification methods.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/hu2024separated/">Separated Variational Hashing Networks For Cross-modal Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Separated Variational Hashing Networks For Cross-modal Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Separated Variational Hashing Networks For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Separated%20Variational%20Hashing%20Networks%20For%20Cross-modal%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Hu Peng, Wang, Zhen, Peng</td>
	<td>Arxiv</td>
	<td><p>Cross-modal hashing due to its low storage cost and high query speed has been successfully used for similarity search in multimedia retrieval applications. It projects high-dimensional data into a shared isomorphic Hamming space with similar binary codes for semantically-similar data. In some applications all modalities may not be obtained or trained simultaneously for some reasons such as privacy secret storage limitation and computational resource limitation. However most existing cross-modal hashing methods need all modalities to jointly learn the common Hamming space thus hindering them from handling these problems. In this paper we propose a novel approach called Separated Variational Hashing Networks (SVHNs) to overcome the above challenge. Firstly it adopts a label network (LabNet) to exploit available and nonspecific label annotations to learn a latent common Hamming space by projecting each semantic label into a common binary representation. Then each modality-specific network can separately map the samples of the corresponding modality into their binary semantic codes learned by LabNet. We achieve it by conducting variational inference to match the aggregated posterior of the hashing code of LabNet with an arbitrary prior distribution. The effectiveness and efficiency of our SVHNs are verified by extensive experiments carried out on four widely-used multimedia databases in comparison with 11 state-of-the-art approaches.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/hu2024creating/">Creating Something From Nothing Unsupervised Knowledge Distillation For Cross-modal Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Creating Something From Nothing Unsupervised Knowledge Distillation For Cross-modal Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Creating Something From Nothing Unsupervised Knowledge Distillation For Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Creating%20Something%20From%20Nothing%20Unsupervised%20Knowledge%20Distillation%20For%20Cross-modal%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Hu Hengtong, Xie, Hong, Tian</td>
	<td>Arxiv</td>
	<td><p>In recent years cross-modal hashing (CMH) has attracted increasing attentions mainly because its potential ability of mapping contents from different modalities especially in vision and language into the same space so that it becomes efficient in cross-modal data retrieval. There are two main frameworks for CMH differing from each other in whether semantic supervision is required. Compared to the unsupervised methods the supervised methods often enjoy more accurate results but require much heavier labors in data annotation. In this paper we propose a novel approach that enables guiding a supervised method using outputs produced by an unsupervised method. Specifically we make use of teacher-student optimization for propagating knowledge. Experiments are performed on two popular CMH benchmarks i.e. the MIRFlickr and NUS-WIDE datasets. Our approach outperforms all existing unsupervised methods by a large margin</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/hemati2024non/">A Non-alternating Graph Hashing Algorithm For Large Scale Image Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Non-alternating Graph Hashing Algorithm For Large Scale Image Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Non-alternating Graph Hashing Algorithm For Large Scale Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20Non-alternating%20Graph%20Hashing%20Algorithm%20For%20Large%20Scale%20Image%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Hemati Sobhan, Mehdizavareh, Chenouri, Tizhoosh</td>
	<td>Arxiv</td>
	<td><p>In the era of big data methods for improving memory and computational efficiency have become crucial for successful deployment of technologies. Hashing is one of the most effective approaches to deal with computational limitations that come with big data. One natural way for formulating this problem is spectral hashing that directly incorporates affinity to learn binary codes. However due to binary constraints the optimization becomes intractable. To mitigate this challenge different relaxation approaches have been proposed to reduce the computational load of obtaining binary codes and still attain a good solution. The problem with all existing relaxation methods is resorting to one or more additional auxiliary variables to attain high quality binary codes while relaxing the problem. The existence of auxiliary variables leads to coordinate descent approach which increases the computational complexity. We argue that introducing these variables is unnecessary. To this end we propose a novel relaxed formulation for spectral hashing that adds no additional variables to the problem. Furthermore instead of solving the problem in original space where number of variables is equal to the data points we solve the problem in a much smaller space and retrieve the binary codes from this solution. This trick reduces both the memory and computational complexity at the same time. We apply two optimization techniques namely projected gradient and optimization on manifold to obtain the solution. Using comprehensive experiments on four public datasets we show that the proposed efficient spectral hashing (ESH) algorithm achieves highly competitive retrieval performance compared with state of the art at low complexity.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/he2024k/">K-nearest Neighbors Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=K-nearest Neighbors Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=K-nearest Neighbors Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=K-nearest%20Neighbors%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>He Xiangyu, Wang, Cheng</td>
	<td>Arxiv</td>
	<td><p>Hashing based approximate nearest neighbor search embeds high dimensional data to compact binary codes which enables efficient similarity search and storage. However the non-isometry sign(Â·) function makes it hard to project the nearest neighbors in continuous data space into the closest codewords in discrete Hamming space. In this work we revisit the sign(Â·) function from the perspective of space partitioning. In specific we bridge the gap between k-nearest neighbors and binary hashing codes with Shannon entropy. We further propose a novel K-Nearest Neighbors Hashing (KNNH) method to learn binary representations from KNN within the subspaces generated by sign(Â·). Theoretical and experimental results show that the KNN relation is of central importance to neighbor preserving embeddings and the proposed method outperforms the state-of-the-arts on benchmark datasets.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/he2024hybridhash/">Hybridhash Hybrid Convolutional And Self-attention Deep Hashing For Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Hybridhash Hybrid Convolutional And Self-attention Deep Hashing For Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Hybridhash Hybrid Convolutional And Self-attention Deep Hashing For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Hybridhash%20Hybrid%20Convolutional%20And%20Self-attention%20Deep%20Hashing%20For%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>He Chao, Wei Hongxi</td>
	<td>Arxiv</td>
	<td><p>Deep image hashing aims to map input images into simple binary hash codes via deep neural networks and thus enable effective large-scale image retrieval. Recently hybrid networks that combine convolution and Transformer have achieved superior performance on various computer tasks and have attracted extensive attention from researchers. Nevertheless the potential benefits of such hybrid networks in image retrieval still need to be verified. To this end we propose a hybrid convolutional and self-attention deep hashing method known as HybridHash. Specifically we propose a backbone network with stage-wise architecture in which the block aggregation function is introduced to achieve the effect of local self-attention and reduce the computational complexity. The interaction module has been elaborately designed to promote the communication of information between image blocks and to enhance the visual representations. We have conducted comprehensive experiments on three widely used datasets CIFAR-10 NUS-WIDE and IMAGENET. The experimental results demonstrate that the method proposed in this paper has superior performance with respect to state-of-the-art deep hashing methods. Source code is available https://github.com/shuaichaochao/HybridHash.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/hemati2024beyond/">Beyond Neighbourhood-preserving Transformations For Quantization-based Unsupervised Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Beyond Neighbourhood-preserving Transformations For Quantization-based Unsupervised Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Beyond Neighbourhood-preserving Transformations For Quantization-based Unsupervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Beyond%20Neighbourhood-preserving%20Transformations%20For%20Quantization-based%20Unsupervised%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Hemati Sobhan, Tizhoosh</td>
	<td>Arxiv</td>
	<td><p>An effective unsupervised hashing algorithm leads to compact binary codes preserving the neighborhood structure of data as much as possible. One of the most established schemes for unsupervised hashing is to reduce the dimensionality of data and then find a rigid (neighbourhood-preserving) transformation that reduces the quantization error. Although employing rigid transformations is effective we may not reduce quantization loss to the ultimate limits. As well reducing dimensionality and quantization loss in two separate steps seems to be sub-optimal. Motivated by these shortcomings we propose to employ both rigid and non-rigid transformations to reduce quantization error and dimensionality simultaneously. We relax the orthogonality constraint on the projection in a PCA-formulation and regularize this by a quantization term. We show that both the non-rigid projection matrix and rotation matrix contribute towards minimizing quantization loss but in different ways. A scalable nested coordinate descent approach is proposed to optimize this mixed-integer optimization problem. We evaluate the proposed method on five public benchmark datasets providing almost half a million images. Comparative results indicate that the proposed method mostly outperforms state-of-art linear methods and competes with end-to-end deep solutions.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/heo2024spherical/">Spherical Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Spherical Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Spherical Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Spherical%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Heo J., Lee, He, Chang, Yoon</td>
	<td>Arxiv</td>
	<td><p>Many binary code encoding schemes based on hashing have been actively studied recently since they can provide efficient similarity search especially nearest neighbor search and compact data representations suitable for handling large scale image databases in many computer vision problems. Existing hashing techniques encode highdimensional data points by using hyperplane-based hashing functions. In this paper we propose a novel hyperspherebased hashing function spherical hashing to map more spatially coherent data points into a binary code compared to hyperplane-based hashing functions. Furthermore we propose a new binary code distance function spherical Hamming distance that is tailored to our hyperspherebased binary coding scheme and design an efficient iterative optimization process to achieve balanced partitioning of data points for each hash function and independence between hashing functions. Our extensive experiments show that our spherical hashing technique significantly outperforms six state-of-the-art hashing techniques based on hyperplanes across various image benchmarks of sizes ranging from one to 75 million of GIST descriptors. The performance gains are consistent and large up to 10037; improvements. The excellent results confirm the unique merits of the proposed idea in using hyperspheres to encode proximity regions in high-dimensional spaces. Finally our method is intuitive and easy to implement.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/hoe2024one/">One Loss For All Deep Hashing With A Single Cosine Similarity Based Learning Objective</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=One Loss For All Deep Hashing With A Single Cosine Similarity Based Learning Objective' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=One Loss For All Deep Hashing With A Single Cosine Similarity Based Learning Objective' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=One%20Loss%20For%20All%20Deep%20Hashing%20With%20A%20Single%20Cosine%20Similarity%20Based%20Learning%20Objective' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Hoe Jiun, Ng, Zhang, Chan Chee, Song Yi-zhe, Xiang Tao</td>
	<td>Arxiv</td>
	<td><p>A deep hashing model typically has two main learning objectives to make the learned binary hash codes discriminative and to minimize a quantization error. With further constraints such as bit balance and code orthogonality it is not uncommon for existing models to employ a large number (4) of losses. This leads to difficulties in model training and subsequently impedes their effectiveness. In this work we propose a novel deep hashing model with only a single learning objective. Specifically we show that maximizing the cosine similarity between the continuous codes and their corresponding binary orthogonal codes can ensure both hash code discriminativeness and quantization error minimization. Further with this learning objective code balancing can be achieved by simply using a Batch Normalization (BN) layer and multi-label classification is also straightforward with label smoothing. The result is an one-loss deep hashing model that removes all the hassles of tuning the weights of various losses. Importantly extensive experiments show that our model is highly effective outperforming the state-of-the-art multi-loss hashing models on three large-scale instance retrieval benchmarks often by significant margins.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/hansen2024content/">Content-aware Neural Hashing For Cold-start Recommendation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Content-aware Neural Hashing For Cold-start Recommendation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Content-aware Neural Hashing For Cold-start Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Content-aware%20Neural%20Hashing%20For%20Cold-start%20Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Hansen Casper, Hansen, Simonsen, Alstrup, Lioma</td>
	<td>Arxiv</td>
	<td><p>Content-aware recommendation approaches are essential for providing meaningful recommendations for new (i.e. cold-start) items in a recommender system. We present a content-aware neural hashing-based collaborative filtering approach (NeuHash-CF) which generates binary hash codes for users and items such that the highly efficient Hamming distance can be used for estimating user-item relevance. NeuHash-CF is modelled as an autoencoder architecture consisting of two joint hashing components for generating user and item hash codes. Inspired from semantic hashing the item hashing component generates a hash code directly from an items content information (i.e. it generates cold-start and seen item hash codes in the same manner). This contrasts existing state-of-the-art models which treat the two item cases separately. The user hash codes are generated directly based on user id through learning a user embedding matrix. We show experimentally that NeuHash-CF significantly outperforms state-of-the-art baselines by up to 1237; NDCG and 1337; MRR in cold-start recommendation settings and up to 437; in both NDCG and MRR in standard settings where all items are present while training. Our approach uses 2-4x shorter hash codes while obtaining the same or better performance compared to the state of the art thus consequently also enabling a notable storage reduction.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/gong2024learning/">Learning Binary Codes For High-dimensional Data Using Bilinear Projections</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning Binary Codes For High-dimensional Data Using Bilinear Projections' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning Binary Codes For High-dimensional Data Using Bilinear Projections' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20Binary%20Codes%20For%20High-dimensional%20Data%20Using%20Bilinear%20Projections' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Gong Y., Kumar, Rowley, Lazebnik</td>
	<td>Arxiv</td>
	<td><p>Recent advances in visual recognition indicate that to achieve good retrieval and classification accuracy on largescale datasets like ImageNet extremely high-dimensional visual descriptors e.g. Fisher Vectors are needed. We present a novel method for converting such descriptors to compact similarity-preserving binary codes that exploits their natural matrix structure to reduce their dimensionality using compact bilinear projections instead of a single large projection matrix. This method achieves comparable retrieval and classification accuracy to the original descriptors and to the state-of-the-art Product Quantization approach while having orders of magnitude faster code generation time and smaller memory footprint.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/ge2024graph/">Graph Cuts For Supervised Binary Coding</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Graph Cuts For Supervised Binary Coding' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Graph Cuts For Supervised Binary Coding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Graph%20Cuts%20For%20Supervised%20Binary%20Coding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Ge T., He, Sun</td>
	<td>Arxiv</td>
	<td><p>Learning short binary codes is challenged by the inherent discrete nature of the problem. The graph cuts algorithm is a well-studied discrete label assignment solution in computer vision but has not yet been applied to solve the binary coding problems. This is partially because it was unclear how to use it to learn the encoding (hashing) functions for out-of-sample generalization. In this paper we formulate supervised binary coding as a single optimization problem that involves both the encoding functions and the binary label assignment. Then we apply the graph cuts algorithm to address the discrete optimization problem involved with no continuous relaxation. This method named as Graph Cuts Coding (GCC) shows competitive results in various datasets.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/gattupalli2024weakly/">Weakly Supervised Deep Image Hashing Through Tag Embeddings</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Weakly Supervised Deep Image Hashing Through Tag Embeddings' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Weakly Supervised Deep Image Hashing Through Tag Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Weakly%20Supervised%20Deep%20Image%20Hashing%20Through%20Tag%20Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Gattupalli Vijetha, Zhuo, Li</td>
	<td>Arxiv</td>
	<td><p>Many approaches to semantic image hashing have been formulated as supervised learning problems that utilize images and label information to learn the binary hash codes. However large-scale labeled image data is expensive to obtain thus imposing a restriction on the usage of such algorithms. On the other hand unlabelled image data is abundant due to the existence of many Web image repositories. Such Web images may often come with images tags that contain useful information although raw tags in general do not readily lead to semantic labels. Motivated by this scenario we formulate the problem of semantic image hashing as a weakly-supervised learning problem. We utilize the information contained in the user-generated tags associated with the images to learn the hash codes. More specifically we extract the word2vec semantic embeddings of the tags and use the information contained in them for constraining the learning. Accordingly we name our model Weakly Supervised Deep Hashing using Tag Embeddings (WDHT). WDHT is tested for the task of semantic image retrieval and is compared against several state-of-art models. Results show that our approach sets a new state-of-art in the area of weekly supervised image hashing.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/gionis2024similarity/">Similarity Search In High Dimensions Via Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Similarity Search In High Dimensions Via Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Similarity Search In High Dimensions Via Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Similarity%20Search%20In%20High%20Dimensions%20Via%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Gionis A., Indyk, Motwani</td>
	<td>Arxiv</td>
	<td><p>The nearest- or near-neighbor query problems arise in a large variety of database applications usually in the context of similarity searching. Of late there has been increasing interest in building search/index structures for performing similarity search over high-dimensional data e.g. image databases document collections time-series databases and genome databases. Unfortunately all known techniques for solving this problem fall prey to the curse of dimensionality. That is the data structures scale poorly with data dimensionality; in fact if the number of dimensions exceeds 10 to 20 searching in k-d trees and related structures involves the inspection of a large fraction of the database thereby doing no better than brute-force linear search. It has been suggested that since the selection of features and the choice of a distance metric in typical applications is rather heuristic determining an approximate nearest neighbor should suffice for most practical purposes. In this paper we examine a novel scheme for approximate similarity search based on hashing. The basic idea is to hash the points from the database so as to ensure that the probability of collision is much higher for objects that are close to each other than for those that are far apart. We provide experimental evidence that our method gives significant improvement in running time over other methods for searching in highdimensional spaces based on hierarchical tree decomposition. Experimental results also indicate that our scheme scales well even for a relatively large number of dimensions (more than 50).</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/gong2024iterative/">Iterative Quantization A Procrustean Approach To Learning Binary Codes</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Iterative Quantization A Procrustean Approach To Learning Binary Codes' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Iterative Quantization A Procrustean Approach To Learning Binary Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Iterative%20Quantization%20A%20Procrustean%20Approach%20To%20Learning%20Binary%20Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Gong Y., Lazebnik</td>
	<td>Arxiv</td>
	<td><p>This paper addresses the problem of learning similarity preserving binary codes for efficient retrieval in large-scale image collections. We propose a simple and efficient alternating minimization scheme for finding a rotation of zerocentered data so as to minimize the quantization error of mapping this data to the vertices of a zero-centered binary hypercube. This method dubbed iterative quantization (ITQ) has connections to multi-class spectral clustering and to the orthogonal Procrustes problem and it can be used both with unsupervised data embeddings such as PCA and supervised embeddings such as canonical correlation analysis (CCA). Our experiments show that the resulting binary coding schemes decisively outperform several other state-of-the-art methods.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/grauman2024learning/">Learning Binary Hash Codes For Large-scale Image Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning Binary Hash Codes For Large-scale Image Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning Binary Hash Codes For Large-scale Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20Binary%20Hash%20Codes%20For%20Large-scale%20Image%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Grauman Kristen, Fergus</td>
	<td>Arxiv</td>
	<td><p>Algorithms to rapidly search massive image or video collections are critical for many vision applications including visual search content-based retrieval and non-parametric models for object recognition. Recent work shows that learned binary projections are a powerful way to index large collections according to their content. The basic idea is to formulate the projections so as to approximately preserve a given similarity function of interest. Having done so one can then search the data efficiently using hash tables or by exploring the Hamming ball volume around a novel query. Both enable sub-linear time retrieval with respect to the database size. Further depending on the design of the projections in some cases it is possible to bound the number of database examples that must be searched in order to achieve a given level of accuracy. This chapter overviews data structures for fast search with binary codes and then describes several supervised and unsupervised strategies for generating the codes. In particular we review supervised methods that integrate metric learning boosting and neural networks into the hash key construction and unsupervised methods based on spectral analysis or kernelized random projections that compute affinity-preserving binary codes.Whether learning from explicit semantic supervision or exploiting the structure among unlabeled data these methods make scalable retrieval possible for a variety of robust visual similarity measures.We focus on defining the algorithms and illustrate the main points with results using millions of images.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/hansen2024unsupervised/">Unsupervised Semantic Hashing With Pairwise Reconstruction</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Semantic Hashing With Pairwise Reconstruction' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Unsupervised Semantic Hashing With Pairwise Reconstruction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Unsupervised%20Semantic%20Hashing%20With%20Pairwise%20Reconstruction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Hansen Casper, Hansen, Simonsen, Alstrup, Lioma</td>
	<td>Arxiv</td>
	<td><p>Semantic Hashing is a popular family of methods for efficient similarity search in large-scale datasets. In Semantic Hashing documents are encoded as short binary vectors (i.e. hash codes) such that semantic similarity can be efficiently computed using the Hamming distance. Recent state-of-the-art approaches have utilized weak supervision to train better performing hashing models. Inspired by this we present Semantic Hashing with Pairwise Reconstruction (PairRec) which is a discrete variational autoencoder based hashing model. PairRec first encodes weakly supervised training pairs (a query document and a semantically similar document) into two hash codes and then learns to reconstruct the same query document from both of these hash codes (i.e. pairwise reconstruction). This pairwise reconstruction enables our model to encode local neighbourhood structures within the hash code directly through the decoder. We experimentally compare PairRec to traditional and state-of-the-art approaches and obtain significant performance improvements in the task of document similarity search.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/fan2024supervised/">Supervised Binary Hash Code Learning With Jensen Shannon Divergence</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Binary Hash Code Learning With Jensen Shannon Divergence' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Supervised Binary Hash Code Learning With Jensen Shannon Divergence' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Supervised%20Binary%20Hash%20Code%20Learning%20With%20Jensen%20Shannon%20Divergence' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Fan Lixin</td>
	<td>Arxiv</td>
	<td><p>This paper proposes to learn binary hash codes within a statistical learning framework in which an upper bound of the probability of Bayes decision errors is derived for different forms of hash functions and a rigorous proof of the convergence of the upper bound is presented. Consequently minimizing such an upper bound leads to consistent performance improvements of existing hash code learning algorithms regardless of whether original algorithms are unsupervised or supervised. This paper also illustrates a fast hash coding method that exploits simple binary tests to achieve orders of magnitude improvement in coding speed as compared to projection based methods.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/fan2024deep/">Deep Polarized Network For Supervised Learning Of Accurate Binary Hashing Codes</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Polarized Network For Supervised Learning Of Accurate Binary Hashing Codes' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Polarized Network For Supervised Learning Of Accurate Binary Hashing Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Polarized%20Network%20For%20Supervised%20Learning%20Of%20Accurate%20Binary%20Hashing%20Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Fan Lixin, Ng, Ju, Zhang, Chan</td>
	<td>Arxiv</td>
	<td><p>This paper proposes a novel deep polarized network (DPN) for learning to hash in which each channel in the network outputs is pushed far away from zero by employing a differentiable bit-wise hinge-like loss which is dubbed as polarization loss. Reformulated within a generic Hamming Distance Metric Learning framework Norouzi et al. 2012 the proposed polarization loss bypasses the requirement to prepare pairwise labels for (dis-)similar items and yet the proposed loss strictly bounds from above the pairwise Hamming Distance based losses. The intrinsic connection between pairwise and pointwise label information as disclosed in this paper brings about the following methodological improvements (a) we may directly employ the proposed differentiable polarization loss with no large deviations incurred from the target Hamming distance based loss; and (b) the subtask of assigning binary codes becomes extremely simple â€” even random codes assigned to each class suffice to result in state-of-the-art performances as demonstrated in CIFAR10 NUS-WIDE and ImageNet100 datasets.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/facebookmeta2024facebook/">Facebook Simsearchnet++</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Facebook Simsearchnet++' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Facebook Simsearchnet++' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Facebook%20Simsearchnet++' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Facebook/meta Facebook/meta</td>
	<td>Arxiv</td>
	<td><p>Facebook SimSearchNet++ is a new dataset released by Facebook for this competition. It consists of features used for image copy detection for integrity purposes. The features are generated by Facebook SimSearchNet++ model.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/dong2024learning/">Learning Space Partitions For Nearest Neighbor Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning Space Partitions For Nearest Neighbor Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning Space Partitions For Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20Space%20Partitions%20For%20Nearest%20Neighbor%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Dong Yihe, Indyk, Razenshteyn, Wagner</td>
	<td>Arxiv</td>
	<td><p>Space partitions of underlie a vast and important class of fast nearest neighbor search (NNS) algorithms. Inspired by recent theoretical work on NNS for general metric spaces (Andoni et al. 2018bc) we develop a new framework for building space partitions reducing the problem to balanced graph partitioning followed by supervised classification. We instantiate this general approach with the KaHIP graph partitioner (Sanders and Schulz 2013) and neural networks respectively to obtain a new partitioning procedure called Neural Locality-Sensitive Hashing (Neural LSH). On several standard benchmarks for NNS (Aumuller et al. 2017) our experiments show that the partitions obtained by Neural LSH consistently outperform partitions found by quantization-based and tree-based methods as well as classic data-oblivious LSH.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/ding2024knn/">Knn Hashing With Factorized Neighborhood Representation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Knn Hashing With Factorized Neighborhood Representation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Knn Hashing With Factorized Neighborhood Representation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Knn%20Hashing%20With%20Factorized%20Neighborhood%20Representation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Ding Kun, Huo, Fan, Pan</td>
	<td>Arxiv</td>
	<td><p>Hashing is very effective for many tasks in reducing the processing time and in compressing massive databases. Although lots of approaches have been developed to learn data-dependent hash functions in recent years how to learn hash functions to yield good performance with acceptable computational and memory cost is still a challenging problem. Based on the observation that retrieval precision is highly related to the kNN classification accuracy this paper proposes a novel kNN-based supervised hashing method which learns hash functions by directly maximizing the kNN accuracy of the Hamming-embedded training data. To make it scalable well to large problem we propose a factorized neighborhood representation to parsimoniously model the neighborhood relationships inherent in training data. Considering that real-world data are often linearly inseparable we further kernelize this basic model to improve its performance. As a result the proposed method is able to learn accurate hashing functions with tolerable computation and storage cost. Experiments on four benchmarks demonstrate that our method outperforms the state-of-the-arts.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/ding2024collective/">Collective Matrix Factorization Hashing For Multimodal Data</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Collective Matrix Factorization Hashing For Multimodal Data' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Collective Matrix Factorization Hashing For Multimodal Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Collective%20Matrix%20Factorization%20Hashing%20For%20Multimodal%20Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Ding G., Guo, Zhou</td>
	<td>Arxiv</td>
	<td><p>Nearest neighbor search methods based on hashing have attracted considerable attention for effective and efficient large-scale similarity search in computer vision and information retrieval community. In this paper we study the problems of learning hash functions in the context of multimodal data for cross-view similarity search. We put forward a novel hashing method which is referred to Collective Matrix Factorization Hashing (CMFH). CMFH learns unified hash codes by collective matrix factorization with latent factor model from different modalities of one instance which can not only supports cross-view search but also increases the search accuracy by merging multiple view information sources. We also prove that CMFH a similaritypreserving hashing learning method has upper and lower boundaries. Extensive experiments verify that CMFH significantly outperforms several state-of-the-art methods on three different datasets.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/deng2024imagenet/">Imagenet A Large-scale Hierarchical Image Database</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Imagenet A Large-scale Hierarchical Image Database' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Imagenet A Large-scale Hierarchical Image Database' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Imagenet%20A%20Large-scale%20Hierarchical%20Image%20Database' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Deng J., Dong, Socher, Li, Li, Fei-fei</td>
	<td>Arxiv</td>
	<td><p>The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index retrieve organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ImageNet a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly we illustrate the usefulness of ImageNet through three simple applications in object recognition image classification and automatic object clustering. We hope that the scale accuracy diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/datar2024locality/">Locality-sensitive Hashing Scheme Based On P-stable Distributions</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Locality-sensitive Hashing Scheme Based On P-stable Distributions' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Locality-sensitive Hashing Scheme Based On P-stable Distributions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Locality-sensitive%20Hashing%20Scheme%20Based%20On%20P-stable%20Distributions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Datar M., Immorlica, Indyk, Mirrokni</td>
	<td>Arxiv</td>
	<td><p>We present a novel Locality-Sensitive Hashing scheme for the Approximate Nearest Neighbor Problem under lp norm based on p-stable distributions.Our scheme improves the running time of the earlier algorithm for the case of the lp norm. It also yields the first known provably efficient approximate NN algorithm for the case p&lt;1. We also show that the algorithm finds the exact near neigbhor in O(log n) time for data satisfying certain bounded growth condition.Unlike earlier schemes our LSH scheme works directly on points in the Euclidean space without embeddings. Consequently the resulting query time bound is free of large factors and is simple and easy to implement. Our experiments (on synthetic data sets) show that the our data structure is up to 40 times faster than kd-tree.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/deng2024two/">Two-stream Deep Hashing With Class-specific Centers For Supervised Image Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Two-stream Deep Hashing With Class-specific Centers For Supervised Image Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Two-stream Deep Hashing With Class-specific Centers For Supervised Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Two-stream%20Deep%20Hashing%20With%20Class-specific%20Centers%20For%20Supervised%20Image%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Deng Cheng, Yang, Liu, Tao</td>
	<td>Arxiv</td>
	<td><p>Hashing has been widely used for large-scale approximate nearest neighbor search due to its storage and search efficiency. Recent supervised hashing research has shown that deep learning-based methods can significantly outperform nondeep methods. Most existing supervised deep hashing methods exploit supervisory signals to generate similar and dissimilar image pairs for training. However natural images can have large intraclass and small interclass variations which may degrade the accuracy of hash codes. To address this problem we propose a novel two-stream ConvNet architecture which learns hash codes with class-specific representation centers. Our basic idea is that if we can learn a unified binary representation for each class as a center and encourage hash codes of images to be close to the corresponding centers the intraclass variation will be greatly reduced. Accordingly we design a neural network that leverages label information and outputs a unified binary representation for each class. Moreover we also design an image network to learn hash codes from images and force these hash codes to be close to the corresponding class-specific centers. These two neural networks are then seamlessly incorporated to create a unified end-to-end trainable framework. Extensive experiments on three popular benchmarks corroborate that our proposed method outperforms current state-of-the-art methods.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/chua2024nus/">NUS-WIDE A Real-world Web Image Database From National University Of Singapore</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=NUS-WIDE A Real-world Web Image Database From National University Of Singapore' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=NUS-WIDE A Real-world Web Image Database From National University Of Singapore' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=NUS-WIDE%20A%20Real-world%20Web%20Image%20Database%20From%20National%20University%20Of%20Singapore' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Chua T., Tang, Hong, Li, Luo, Zheng</td>
	<td>Arxiv</td>
	<td><p>This paper introduces a web image dataset created by NUSs Lab for Media Search. The dataset includes (1) 269648 images and the associated tags from Flickr with a total of 5018 unique tags; (2) six types of low-level features extracted from these images including 64-D color histogram 144-D color correlogram 73-D edge direction histogram 128-D wavelet texture 225-D block-wise color moments extracted over 5x5 fixed grid partitions and 500-D bag of words based on SIFT descriptions; and (3) ground-truth for 81 concepts that can be used for evaluation. Based on this dataset we highlight characteristics of Web image collections and identify four research issues on web image annotation and retrieval. We also provide the baseline results for web image annotation by learning from the tags using the traditional k-NN algorithm. The benchmark results indicate that it is possible to learn effective models from sufficiently large image dataset to facilitate general image retrieval.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/cheng2024robust/">Robust Unsupervised Cross-modal Hashing For Multimedia Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Robust Unsupervised Cross-modal Hashing For Multimedia Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Robust Unsupervised Cross-modal Hashing For Multimedia Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Robust%20Unsupervised%20Cross-modal%20Hashing%20For%20Multimedia%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Cheng Miaomiao, Jing, Ng</td>
	<td>Arxiv</td>
	<td><p>With the quick development of social websites there are more opportunities to have different media types (such as text image video etc.) describing the same topic from large-scale heterogeneous data sources. To efficiently identify the inter-media correlations for multimedia retrieval unsupervised cross-modal hashing (UCMH) has gained increased interest due to the significant reduction in computation and storage. However most UCMH methods assume that the data from different modalities are well paired. As a result existing UCMH methods may not achieve satisfactory performance when partially paired data are given only. In this article we propose a new-type of UCMH method called robust unsupervised cross-modal hashing (RUCMH). The major contribution lies in jointly learning modal-specific hash function exploring the correlations among modalities with partial or even without any pairwise correspondence and preserving the information of original features as much as possible. The learning process can be modeled via a joint minimization problem and the corresponding optimization algorithm is presented. A series of experiments is conducted on four real-world datasets (Wiki MIRFlickr NUS-WIDE and MS-COCO). The results demonstrate that RUCMH can significantly outperform the state-of-the-art unsupervised cross-modal hashing methods especially for the partially paired case which validates the effectiveness of RUCMH.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/chen2024supervised/">Supervised Consensus Anchor Graph Hashing For Cross Modal Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Consensus Anchor Graph Hashing For Cross Modal Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Supervised Consensus Anchor Graph Hashing For Cross Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Supervised%20Consensus%20Anchor%20Graph%20Hashing%20For%20Cross%20Modal%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Chen Rui, Wang</td>
	<td>Arxiv</td>
	<td><p>The target of cross-modal hashing is to embed heterogeneous multimedia data into a common low-dimensional Hamming space which plays a pivotal part in multimedia retrieval due to the emergence of big multimodal data. Recently matrix factorization has achieved great success in cross-modal hashing. However how to effectively use label information and local geometric structure is still a challenging problem for these approaches. To address this issue we propose a cross-modal hashing method based on collective matrix factorization which considers both the label consistency across different modalities and the local geometric consistency in each modality. These two elements are formulated as a graph Laplacian term in the objective function leading to a substantial improvement on the discriminative power of latent semantic features obtained by collective matrix factorization. Moreover the proposed method learns unified hash codes for different modalities of an instance to facilitate cross-modal search and the objective function is solved using an iterative strategy. The experimental results on two benchmark data sets show the effectiveness of the proposed method and its superiority over state-of-the-art cross-modal hashing methods.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/chen2024strongly/">Strongly Constrained Discrete Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Strongly Constrained Discrete Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Strongly Constrained Discrete Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Strongly%20Constrained%20Discrete%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Chen Yong, Tian, Zhang, Wang, Zhang</td>
	<td>Arxiv</td>
	<td><p>Learning to hash is a fundamental technique widely used in large-scale image retrieval. Most existing methods for learning to hash address the involved discrete optimization problem by the continuous relaxation of the binary constraint which usually leads to large quantization errors and consequently suboptimal binary codes. A few discrete hashing methods have emerged recently. However they either completely ignore some useful constraints (specifically the balance and decorrelation of hash bits) or just turn those constraints into regularizers that would make the optimization easier but less accurate. In this paper we propose a novel supervised hashing method named Strongly Constrained Discrete Hashing (SCDH) which overcomes such limitations. It can learn the binary codes for all examples in the training set and meanwhile obtain a hash function for unseen samples with the above mentioned constraints preserved. Although the model of SCDH is fairly sophisticated we are able to find closed-form solutions to all of its optimization subproblems and thus design an efficient algorithm that converges quickly. In addition we extend SCDH to a kernelized version SCDH K . Our experiments on three large benchmark datasets have demonstrated that not only can SCDH and SCDH K achieve substantially higher MAP scores than state-of-the-art baselines but they train much faster than those that are also supervised as well.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/chen2024two/">A Two-step Cross-modal Hashing By Exploiting Label Correlations And Preserving Similarity In Both Steps</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Two-step Cross-modal Hashing By Exploiting Label Correlations And Preserving Similarity In Both Steps' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Two-step Cross-modal Hashing By Exploiting Label Correlations And Preserving Similarity In Both Steps' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20Two-step%20Cross-modal%20Hashing%20By%20Exploiting%20Label%20Correlations%20And%20Preserving%20Similarity%20In%20Both%20Steps' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Chen Zhen-duo, Wang, Li, Luo, Nie, Xin-shun</td>
	<td>Arxiv</td>
	<td><p>In this paper we present a novel Two-stEp Cross-modal Hashing method TECH for short for cross-modal retrieval tasks. As a two-step method it first learns hash codes based on semantic labels while preserving the similarity in the original space and exploiting the label correlations in the label space. In the light of this it is able to make better use of label information and generate better binary codes. In addition different from other two-step methods that mainly focus on the hash codes learning TECH adopts a new hash function learning strategy in the second step which also preserves the similarity in the original space. Moreover with the help of well designed objective function and optimization scheme it is able to generate hash codes discretely and scalable for large scale data. To the best of our knowledge it is the first cross-modal hashing method exploiting label correlations and also the first two-step hashing model preserving the similarity while leaning hash function. Extensive experiments demonstrate that the proposed approach outperforms some state-of-the-art cross-modal hashing methods.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/cui2024exchnet/">Exchnet A Unified Hashing Network For Large-scale Fine-grained Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Exchnet A Unified Hashing Network For Large-scale Fine-grained Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Exchnet A Unified Hashing Network For Large-scale Fine-grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Exchnet%20A%20Unified%20Hashing%20Network%20For%20Large-scale%20Fine-grained%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Cui Quan, Jiang, Wei, Li, Yoshie</td>
	<td>Arxiv</td>
	<td><p>Retrieving content relevant images from a large-scale fine grained dataset could suffer from intolerably slow query speed and highly redundant storage cost due to high-dimensional real-valued embeddings which aim to distinguish subtle visual differences of fine-grained objects. In this paper we study the novel fine-grained hashing topic to generate compact binary codes for fine-grained images leveraging the search and storage efficiency of hash learning to alleviate the aforementioned problems. Specifically we propose a unified end-to-end trainable network termed as ExchNet. Based on attention mechanisms and proposed attention constraints it can firstly obtain both local and global features to represent object parts and whole fine-grained objects respectively. Furthermore to ensure the discriminative ability and semantic meaningâ€™s consistency of these part-level features across images we design a local feature alignment approach by performing a feature exchanging operation. Later an alternative learning algorithm is employed to optimize the whole ExchNet and then generate the final binary hash codes. Validated by extensive experiments our proposal consistently outperforms state-of-the-art generic hashing methods on five fine-grained datasets which shows our effectiveness. Moreover compared with other approximate nearest neighbor methods ExchNet achieves the best speed-up and storage reduction revealing its efficiency and practicality.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/chen2024enhanced/">Enhanced Discrete Multi-modal Hashing More Constraints Yet Less Time To Learn</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Enhanced Discrete Multi-modal Hashing More Constraints Yet Less Time To Learn' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Enhanced Discrete Multi-modal Hashing More Constraints Yet Less Time To Learn' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Enhanced%20Discrete%20Multi-modal%20Hashing%20More%20Constraints%20Yet%20Less%20Time%20To%20Learn' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Chen Yong, Zhang, Tian, Wang, Zhang, Li</td>
	<td>Arxiv</td>
	<td><p>Due to the exponential growth of multimedia data multi-modal hashing as a promising technique to make cross-view retrieval scalable is attracting more and more attention. However most of the existing multi-modal hashing methods either divide the learning process unnaturally into two separate stages or treat the discrete optimization problem simplistically as a continuous one which leads to suboptimal results. Recently a few discrete multi-modal hashing methods that try to address such issues have emerged but they still ignore several important discrete constraints (such as the balance and decorrelation of hash bits). In this paper we overcome those limitations by proposing a novel method named Enhanced Discrete Multi-modal Hashing (EDMH) which learns binary codes and hashing functions simultaneously from the pairwise similarity matrix of data under the aforementioned discrete constraints. Although the model of EDMH looks a lot more complex than the other models for multi-modal hashing we are actually able to develop a fast iterative learning algorithm for it since the subproblems of its optimization all have closed-form solutions after introducing two auxiliary variables. Our experimental results on three real-world datasets have demonstrated that EDMH not only performs much better than state-of-the-art competitors but also runs much faster than them.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/chen2024deep/">Deep Supervised Hashing With Anchor Graph</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Supervised Hashing With Anchor Graph' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Supervised Hashing With Anchor Graph' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Supervised%20Hashing%20With%20Anchor%20Graph' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Chen Yudong, Lai, Ding, Lin, Wong</td>
	<td>Arxiv</td>
	<td><p>Recently a series of deep supervised hashing methods were proposed for binary code learning. However due to the high computation cost and the limited hardwares memory these methods will first select a subset from the training set and then form a mini-batch data to update the network in each iteration. Therefore the remaining labeled data cannot be fully utilized and the model cannot directly obtain the binary codes of the entire training set for retrieval. To address these problems this paper proposes an interesting regularized deep model to seamlessly integrate the advantages of deep hashing and efficient binary code learning by using the anchor graph. As such the deep features and label matrix can be jointly used to optimize the binary codes and the network can obtain more discriminative feedback from the linear combinations of the learned bits. Moreover we also reveal the algorithm mechanism and its computation essence. Experiments on three large-scale datasets indicate that the proposed method achieves better retrieval performance with less training time compared to previous deep hashing methods.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/chen2024locality/">Locality-sensitive Hashing For F-divergences Mutual Information Loss And Beyond</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Locality-sensitive Hashing For F-divergences Mutual Information Loss And Beyond' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Locality-sensitive Hashing For F-divergences Mutual Information Loss And Beyond' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Locality-sensitive%20Hashing%20For%20F-divergences%20Mutual%20Information%20Loss%20And%20Beyond' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Chen L., Esfandiari, Fu, Mirrokni</td>
	<td>Arxiv</td>
	<td><p>Computing approximate nearest neighbors in high dimensional spaces is a central problem in large-scale data mining with a wide range of applications in machine learning and data science. A popular and effective technique in computing nearest neighbors approximately is the locality-sensitive hashing (LSH) scheme. In this paper we aim to develop LSH schemes for distance functions that measure the distance between two probability distributions particularly for f-divergences as well as a generalization to capture mutual information loss. First we provide a general framework to design LHS schemes for f-divergence distance functions and develop LSH schemes for the generalized Jensen-Shannon divergence and triangular discrimination in this framework. We show a two-sided approximation result for approximation of the generalized Jensen-Shannon divergence by the Hellinger distance which may be of independent interest. Next we show a general method of reducing the problem of designing an LSH scheme for a Krein kernel (which can be expressed as the difference of two positive definite kernels) to the problem of maximum inner product search. We exemplify this method by applying it to the mutual information loss due to its several important applications such as model compression.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/torresxirau2024fast/">Fast Approximate Nearest-neighbor Field By Cascaded Spherical Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Fast Approximate Nearest-neighbor Field By Cascaded Spherical Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Fast Approximate Nearest-neighbor Field By Cascaded Spherical Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Fast%20Approximate%20Nearest-neighbor%20Field%20By%20Cascaded%20Spherical%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Torres-xirau I., Salvador, PÃ©rez-pellitero</td>
	<td>Arxiv</td>
	<td><p>We present an efficient and fast algorithm for computing approximate nearest neighbor fields between two images. Our method builds on the concept of Coherency-Sensitive Hashing (CSH) but uses a recent hashing scheme Spherical Hashing (SpH) which is known to be better adapted to the nearest-neighbor problem for natural images. Cascaded Spherical Hashing concatenates different configurations of SpH to build larger Hash Tables with less elements in each bin to achieve higher selectivity. Our method amply outperforms existing techniques like PatchMatch and CSH and the experimental results show that our algorithm is faster and more accurate than existing methods.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/torralba2024million/">80 Million Tiny Images A Large Dataset For Non-parametric Object And Scene Recognition</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=80 Million Tiny Images A Large Dataset For Non-parametric Object And Scene Recognition' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=80 Million Tiny Images A Large Dataset For Non-parametric Object And Scene Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=80%20Million%20Tiny%20Images%20A%20Large%20Dataset%20For%20Non-parametric%20Object%20And%20Scene%20Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Torralba A., Freeman</td>
	<td>Arxiv</td>
	<td><p>With the advent of the Internet billions of images are now freely available online and constitute a dense sampling of the visual world. Using a variety of non-parametric methods we explore this world with the aid of a large dataset of 79302017 images collected from the Web. Motivated by psychophysical results showing the remarkable tolerance of the human visual system to degradations in image resolution the images in the dataset are stored as 32 Ã— 32 color images. Each image is loosely labeled with one of the 75062 non-abstract nouns in English as listed in the Wordnet lexical database. Hence the image database gives a comprehensive coverage of all object categories and scenes. The semantic information from Wordnet can be used in conjunction with nearest-neighbor methods to perform object classification over a range of semantic levels minimizing the effects of labeling noise. For certain classes that are particularly prevalent in the dataset such as people we are able to demonstrate a recognition performance comparable to class-specific Viola-Jones style detectors.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/sundaram2024streaming/">Streaming Similarity Search Over One Billion Tweets Using Parallel Locality-sensitive Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Streaming Similarity Search Over One Billion Tweets Using Parallel Locality-sensitive Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Streaming Similarity Search Over One Billion Tweets Using Parallel Locality-sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Streaming%20Similarity%20Search%20Over%20One%20Billion%20Tweets%20Using%20Parallel%20Locality-sensitive%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Sundaram Narayanan, Turmukhametova, Satish, Mostak, Indyk, Dubey</td>
	<td>Arxiv</td>
	<td><p>Finding nearest neighbors has become an important operation on databases with applications to text search multimedia indexing and many other areas. One popular algorithm for similarity search especially for high dimensional data (where spatial indexes like kdtrees do not perform well) is Locality Sensitive Hashing (LSH) an approximation algorithm for finding similar objects. In this paper we describe a new variant of LSH called Parallel LSH (PLSH) designed to be extremely efficient capable of scaling out on multiple nodes and multiple cores and which supports highthroughput streaming of new data. Our approach employs several novel ideas including cache-conscious hash table layout using a 2-level merge algorithm for hash table construction; an efficient algorithm for duplicate elimination during hash-table querying; an insert-optimized hash table structure and efficient data expiration algorithm for streaming data; and a performance model that accurately estimates performance of the algorithm and can be used to optimize parameter settings. We show that on a workload where we perform similarity search on a dataset of 1 Billion tweets with hundreds of millions of new tweets per day we can achieve query times of 1â€“2.5 ms. We show that this is an order of magnitude faster than existing indexing schemes such as inverted indexes. To the best of our knowledge this is the fastest implementation of LSH with table construction times up to 3.7x faster and query times that are 8.3x faster than a basic implementation.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/sun2024supervised/">Supervised Hierarchical Cross-modal Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Hierarchical Cross-modal Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Supervised Hierarchical Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Supervised%20Hierarchical%20Cross-modal%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Sun Changchang, Song, Feng, Zhao, Nie</td>
	<td>Arxiv</td>
	<td><p>Recently due to the unprecedented growth of multimedia data cross-modal hashing has gained increasing attention for the efficient cross-media retrieval. Typically existing methods on crossmodal hashing treat labels of one instance independently but overlook the correlations among labels. Indeed in many real-world scenarios like the online fashion domain instances (items) are labeled with a set of categories correlated by certain hierarchy. In this paper we propose a new end-to-end solution for supervised cross-modal hashing named HiCHNet which explicitly exploits the hierarchical labels of instances. In particular by the pre-established label hierarchy we comprehensively characterize each modality of the instance with a set of layer-wise hash representations. In essence hash codes are encouraged to not only preserve the layerwise semantic similarities encoded by the label hierarchy but also retain the hierarchical discriminative capabilities. Due to the lack of benchmark datasets apart from adapting the existing dataset FashionVC from fashion domain we create a dataset from the online fashion platform Ssense consisting of 15 696 image-text pairs labeled by 32 hierarchical categories. Extensive experiments on two real-world datasets demonstrate the superiority of our model over the state-of-the-art methods.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/subramanya2024diskann/">Diskann Fast Accurate Billion-point Nearest Neighbor Search On A Single Node</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Diskann Fast Accurate Billion-point Nearest Neighbor Search On A Single Node' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Diskann Fast Accurate Billion-point Nearest Neighbor Search On A Single Node' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Diskann%20Fast%20Accurate%20Billion-point%20Nearest%20Neighbor%20Search%20On%20A%20Single%20Node' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Subramanya Suhas, Devvrit, Simhadri, Krishnawamy, Kadekodi</td>
	<td>Arxiv</td>
	<td><p>Current state-of-the-art approximate nearest neighbor search (ANNS) algorithms generate indices that must be stored in main memory for fast high-recall search. This makes them expensive and limits the size of the dataset. We present a new graph-based indexing and search system called DiskANN that can index store and search a billion point database on a single workstation with just 64GB RAM and an inexpensive solid-state drive (SSD). Contrary to current wisdom we demonstrate that the SSD-based indices built by DiskANN can meet all three desiderata for large-scale ANNS high-recall low query latency and high density (points indexed per node). On the billion point SIFT1B bigann dataset DiskANN serves 5000 queries a second with &lt; 3ms mean latency and 9537;+ 1-recall@1 on a 16 core machine where state-of-the-art billion-point ANNS algorithms with similar memory footprint like FAISS and IVFOADC+G+P plateau at around 5037; 1-recall@1. Alternately in the high recall regime DiskANN can index and serve 5 âˆ’ 10x more points per node compared to state-of-the-art graph- based methods such as HNSW and NSG. Finally as part of our overall DiskANN system we introduce Vamana a new graph-based ANNS index that is more versatile than the graph indices even for in-memory indices.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/su2024deep/">Deep Joint-semantics Reconstructing Hashing For Large-scale Unsupervised Cross-modal Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Joint-semantics Reconstructing Hashing For Large-scale Unsupervised Cross-modal Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Joint-semantics Reconstructing Hashing For Large-scale Unsupervised Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Joint-semantics%20Reconstructing%20Hashing%20For%20Large-scale%20Unsupervised%20Cross-modal%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Su Shupeng, Zhong, Zhang</td>
	<td>Arxiv</td>
	<td><p>!Deep Joint-Semantics Reconstructing Hashing for Large-Scale Unsupervised Cross-Modal Retrieval(https://github.com/zzs1994/DJSRH/blob/master/page_image/DJRSH.png?raw=true Deep Joint-Semantics Reconstructing Hashing for Large-Scale Unsupervised Cross-Modal Retrieval) Cross-modal hashing encodes the multimedia data into a common binary hash space in which the correlations among the samples from different modalities can be effectively measured. Deep cross-modal hashing further improves the retrieval performance as the deep neural networks can generate more semantic relevant features and hash codes. In this paper we study the unsupervised deep cross-modal hash coding and propose Deep Joint Semantics Reconstructing Hashing (DJSRH) which has the following two main advantages. First to learn binary codes that preserve the neighborhood structure of the original data DJSRH constructs a novel joint-semantics affinity matrix which elaborately integrates the original neighborhood information from different modalities and accordingly is capable to capture the latent intrinsic semantic affinity for the input multi-modal instances. Second DJSRH later trains the networks to generate binary codes that maximally reconstruct above joint-semantics relations via the proposed reconstructing framework which is more competent for the batch-wise training as it reconstructs the specific similarity value unlike the common Laplacian constraint merely preserving the similarity order. Extensive experiments demonstrate the significant improvement by DJSRH in various cross-modal retrieval tasks.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/su2024greedy/">Greedy Hash Towards Fast Optimization For Accurate Hash Coding In CNN</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Greedy Hash Towards Fast Optimization For Accurate Hash Coding In CNN' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Greedy Hash Towards Fast Optimization For Accurate Hash Coding In CNN' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Greedy%20Hash%20Towards%20Fast%20Optimization%20For%20Accurate%20Hash%20Coding%20In%20CNN' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Su Shupeng, Zhang, Han, Tian</td>
	<td>Arxiv</td>
	<td><p>To convert the input into binary code hashing algorithm has been widely used for approximate nearest neighbor search on large-scale image sets due to its computation and storage efficiency. Deep hashing further improves the retrieval quality by combining the hash coding with deep neural network. However a major difficulty in deep hashing lies in the discrete constraints imposed on the network output which generally makes the optimization NP hard. In this work we adopt the greedy principle to tackle this NP hard problem by iteratively updating the network toward the probable optimal discrete solution in each iteration. A hash coding layer is designed to implement our approach which strictly uses the sign function in forward propagation to maintain the discrete constraints while in back propagation the gradients are transmitted intactly to the front layer to avoid the vanishing gradients. In addition to the theoretical derivation we provide a new perspective to visualize and understand the effectiveness and efficiency of our algorithm. Experiments on benchmark datasets show that our scheme outperforms state-of-the-art hashing methods in both supervised and unsupervised tasks.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/sun2024deep/">Deep Normalized Cross-modal Hashing With Bi-direction Relation Reasoning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Normalized Cross-modal Hashing With Bi-direction Relation Reasoning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Normalized Cross-modal Hashing With Bi-direction Relation Reasoning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Normalized%20Cross-modal%20Hashing%20With%20Bi-direction%20Relation%20Reasoning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Sun Changchang, Latapie, Liu, Yan</td>
	<td>Arxiv</td>
	<td><p>Due to the continuous growth of large-scale multi-modal data and increasing requirements for retrieval speed deep cross-modal hashing has gained increasing attention recently. Most of existing studies take a similarity matrix as supervision to optimize their models and the inner product between continuous surrogates of hash codes is utilized to depict the similarity in the Hamming space. However all of them merely consider the relevant information to build the similarity matrix ignoring the contribution of the irrelevant one i.e. the categories that samples do not belong to. Therefore they cannot effectively alleviate the effect of dissimilar samples. Moreover due to the modality distribution difference directly utilizing continuous surrogates of hash codes to calculate similarity may induce suboptimal retrieval performance. To tackle these issues in this paper we propose a novel deep normalized cross-modal hashing scheme with bi-direction relation reasoning named Bi_NCMH. Specifically we build the multi-level semantic similarity matrix by considering bi-direction relation i.e. consistent and inconsistent relation. It hence can holistically characterize relations among instances. Besides we execute feature normalization on continuous surrogates of hash codes to eliminate the deviation caused by modality gap which further reduces the negative impact of binarization on retrieval performance. Extensive experiments on two cross-modal benchmark datasets demonstrate the superiority of our model over several state-of-the-art baselines.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/song2024inter/">Inter-media Hashing For Large-scale Retrieval From Heterogeneous Data Sources</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Inter-media Hashing For Large-scale Retrieval From Heterogeneous Data Sources' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Inter-media Hashing For Large-scale Retrieval From Heterogeneous Data Sources' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Inter-media%20Hashing%20For%20Large-scale%20Retrieval%20From%20Heterogeneous%20Data%20Sources' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Song J., Yang, Yang, Huang, Shen</td>
	<td>Arxiv</td>
	<td><p>In this paper we present a new multimedia retrieval paradigm to innovate large-scale search of heterogenous multimedia data. It is able to return results of different media types from heterogeneous data sources e.g. using a query image to retrieve relevant text documents or images from different data sources. This utilizes the widely available data from different sources and caters for the current users demand of receiving a result list simultaneously containing multiple types of data to obtain a comprehensive understanding of the querys results. To enable large-scale inter-media retrieval we propose a novel inter-media hashing (IMH) model to explore the correlations among multiple media types from different data sources and tackle the scalability issue. To this end multimedia data from heterogeneous data sources are transformed into a common Hamming space in which fast search can be easily implemented by XOR and bit-count operations. Furthermore we integrate a linear regression model to learn hashing functions so that the hash codes for new data points can be efficiently generated. Experiments conducted on real-world large-scale multimedia datasets demonstrate the superiority of our proposed method compared with state-of-the-art techniques.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/song2024self/">Self-supervised Video Hashing With Hierarchical Binary Auto-encoder</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Self-supervised Video Hashing With Hierarchical Binary Auto-encoder' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Self-supervised Video Hashing With Hierarchical Binary Auto-encoder' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Self-supervised%20Video%20Hashing%20With%20Hierarchical%20Binary%20Auto-encoder' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Song Jingkuan, Zhang, Li, Gao, Wang, Hong</td>
	<td>Arxiv</td>
	<td><p>Existing video hash functions are built on three isolated stages frame pooling relaxed learning and binarization which have not adequately explored the temporal order of video frames in a joint binary optimization model resulting in severe information loss. In this paper we propose a novel unsupervised video hashing framework dubbed Self-Supervised Video Hashing (SSVH) that is able to capture the temporal nature of videos in an end-to-end learning-to-hash fashion. We specifically address two central problems 1) how to design an encoder-decoder architecture to generate binary codes for videos; and 2) how to equip the binary codes with the ability of accurate video retrieval. We design a hierarchical binary autoencoder to model the temporal dependencies in videos with multiple granularities and embed the videos into binary codes with less computations than the stacked architecture. Then we encourage the binary codes to simultaneously reconstruct the visual content and neighborhood structure of the videos. Experiments on two real-world datasets (FCVID and YFCC) show that our SSVH method can significantly outperform the state-of-the-art methods and achieve the currently best performance on the task of unsupervised video retrieval.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/shrivastava2024asymmetric/">Asymmetric LSH (ALSH) For Sublinear Time Maximum Inner Product Search (MIPS).</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Asymmetric LSH (ALSH) For Sublinear Time Maximum Inner Product Search (MIPS).' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Asymmetric LSH (ALSH) For Sublinear Time Maximum Inner Product Search (MIPS).' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Asymmetric%20LSH%20(ALSH)%20For%20Sublinear%20Time%20Maximum%20Inner%20Product%20Search%20(MIPS).' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Shrivastava A., Li</td>
	<td>Arxiv</td>
	<td><p>We present the first provably sublinear time hashing algorithm for approximate Maximum Inner Product Search (MIPS). Searching with (un-normalized) inner product as the underlying similarity measure is a known difficult problem and finding hashing schemes for MIPS was considered hard. While the existing Locality Sensitive Hashing (LSH) framework is insufficient for solving MIPS in this paper we extend the LSH framework to allow asymmetric hashing schemes. Our proposal is based on a key observation that the problem of finding maximum inner products after independent asymmetric transformations can be converted into the problem of approximate near neighbor search in classical settings. This key observation makes efficient sublinear hashing scheme for MIPS possible. Under the extended asymmetric LSH (ALSH) framework this paper provides an example of explicit construction of provably fast hashing scheme for MIPS. Our proposed algorithm is simple and easy to implement. The proposed hashing scheme leads to significant computational savings over the two popular conventional LSH schemes (i) Sign Random Projection (SRP) and (ii) hashing based on p-stable distributions for L2 norm (L2LSH) in the collaborative filtering task of item recommendations on Netflix and Movielens (10M) datasets.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/shrivastava2024densifying/">Densifying One Permutation Hashing Via Rotation For Fast Near Neighbor Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Densifying One Permutation Hashing Via Rotation For Fast Near Neighbor Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Densifying One Permutation Hashing Via Rotation For Fast Near Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Densifying%20One%20Permutation%20Hashing%20Via%20Rotation%20For%20Fast%20Near%20Neighbor%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Shrivastava A., Li</td>
	<td>Arxiv</td>
	<td><p>The query complexity of locality sensitive hashing (LSH) based similarity search is dominated by the number of hash evaluations and this number grows with the data size (Indyk amp; Motwani 1998). In industrial applications such as search where the data are often high-dimensional and binary (e.g. text n-grams) minwise hashing is widely adopted which requires applying a large number of permutations on the data. This is costly in computation and energy-consumption. In this paper we propose a hashing technique which generates all the necessary hash evaluations needed for similarity search using one single permutation. The heart of the proposed hash function is a rotation scheme which densifies the sparse sketches of one permutation hashing (Li et al. 2012) in an unbiased fashion thereby maintaining the LSH property. This makes the obtained sketches suitable for hash table construction. This idea of rotation presented in this paper could be of independent interest for densifying other types of sparse sketches. Using our proposed hashing method the query time of a (K L)-parameterized LSH is reduced from the typical O(dKL) complexity to merely O(KL + dL) where d is the number of nonzeros of the data vector K is the number of hashes in each hash table and L is the number of hash tables. Our experimental evaluation on real data confirms that the proposed scheme significantly reduces the query processing time over minwise hashing without loss in retrieval accuracies.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/shi2024variable/">Variable-length Quantization Strategy For Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Variable-length Quantization Strategy For Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Variable-length Quantization Strategy For Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Variable-length%20Quantization%20Strategy%20For%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Shi Yang, Nie, Zhou, Xi, Yin</td>
	<td>Arxiv</td>
	<td><p>Hashing is widely used to solve fast Approximate Nearest Neighbor (ANN) search problems involves converting the original real-valued samples to binary-valued representations. The conventional quantization strategies such as Single-Bit Quantization and Multi-Bit quantization are considered ineffective because of their serious information loss. To address this issue we propose a novel variable-length quantization (VLQ) strategy for hashing. In the proposed VLQ technique we divide all samples into different regions in each dimension firstly given the real-valued features of samples. Then we compute the dispersion degrees of these regions. Subsequently we attempt to optimally assign different number of bits to each dimensions to obtain the minimum dispersion degree. Our experiments show that the VLQ strategy achieves not only superior performance over the state-of-the-art methods but also has a faster retrieval speed on public datasets.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/silavong2024deskew/">Deskew-lsh Based Code-to-code Recommendation Engine</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deskew-lsh Based Code-to-code Recommendation Engine' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deskew-lsh Based Code-to-code Recommendation Engine' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deskew-lsh%20Based%20Code-to-code%20Recommendation%20Engine' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Silavong Fran, Moran, Georgiadis, Saphal, Otter</td>
	<td>Arxiv</td>
	<td><p>Machine learning on source code (MLOnCode) is a popular research field that has been driven by the availability of large-scale code repositories and the development of powerful probabilistic and deep learning models for mining source code. Code-to-code recommendation is a task in MLOnCode that aims to recommend relevant diverse and concise code snippets that usefully extend the code currently being written by a developer in their development environment (IDE). Code-to-code recommendation engines hold the promise of increasing developer productivity by reducing context switching from the IDE and increasing code-reuse. Existing code-to-code recommendation engines do not scale gracefully to large codebases exhibiting a linear growth in query time as the code repository increases in size. In addition existing code-to-code recommendation engines fail to account for the global statistics of code repositories in the ranking function such as the distribution of code snippet lengths leading to sub-optimal retrieval results. We address both of these weaknesses with emphSenatus a new code-to-code recommendation engine. At the core of Senatus is emphDe-Skew LSH a new locality sensitive hashing (LSH) algorithm that indexes the data for fast (sub-linear time) retrieval while also counteracting the skewness in the snippet length distribution using novel abstract syntax tree-based feature scoring and selection algorithms. We evaluate Senatus via automatic evaluation and with an expert developer user study and find the recommendations to be of higher quality than competing baselines while achieving faster search. For example on the CodeSearchNet dataset we show that Senatus improves performance by 6.737; F1 and query time 16x is faster compared to Facebook Aroma on the task of code-to-code recommendation.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/song2024top/">Top Rank Supervised Binary Coding For Visual Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Top Rank Supervised Binary Coding For Visual Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Top Rank Supervised Binary Coding For Visual Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Top%20Rank%20Supervised%20Binary%20Coding%20For%20Visual%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Song Dongjin, Liu, Ji, Meyer, Smith</td>
	<td>Arxiv</td>
	<td><p>In recent years binary coding techniques are becoming increasingly popular because of their high efficiency in handling large-scale computer vision applications. It has been demonstrated that supervised binary coding techniques that leverage supervised information can significantly enhance the coding quality and hence greatly benefit visual search tasks. Typically a modern binary coding method seeks to learn a group of coding functions which compress data samples into binary codes. However few methods pursued the coding functions such that the precision at the top of a ranking list according to Hamming distances of the generated binary codes is optimized. In this paper we propose a novel supervised binary coding approach namely Top Rank Supervised Binary Coding (Top-RSBC) which explicitly focuses on optimizing the precision of top positions in a Hamming-distance ranking list towards preserving the supervision information. The core idea is to train the disciplined coding functions by which the mistakes at the top of a Hamming-distance ranking list are penalized more than those at the bottom. To solve such coding functions we relax the original discrete optimization objective with a continuous surrogate and derive a stochastic gradient descent to optimize the surrogate objective. To further reduce the training time cost we also design an online learning algorithm to optimize the surrogate objective more efficiently. Empirical studies based upon three benchmark image datasets demonstrate that the proposed binary coding approach achieves superior image search accuracy over the state-of-the-arts.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/shen2024auto/">Auto-encoding Twin-bottleneck Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Auto-encoding Twin-bottleneck Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Auto-encoding Twin-bottleneck Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Auto-encoding%20Twin-bottleneck%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Shen Yuming, Qin, Chen, Yu, Liu, Zhu, Shen, Shao</td>
	<td>Arxiv</td>
	<td><p>Conventional unsupervised hashing methods usually take advantage of similarity graphs which are either pre-computed in the high-dimensional space or obtained from random anchor points. On the one hand existing methods uncouple the procedures of hash function learning and graph construction. On the other hand graphs empirically built upon original data could introduce biased prior knowledge of data relevance leading to sub-optimal retrieval performance. In this paper we tackle the above problems by proposing an efficient and adaptive code-driven graph which is updated by decoding in the context of an auto-encoder. Specifically we introduce into our framework twin bottlenecks (i.e. latent variables) that exchange crucial information collaboratively. One bottleneck (i.e. binary codes) conveys the high-level intrinsic data structure captured by the code-driven graph to the other (i.e. continuous variables for low-level detail information) which in turn propagates the updated network feedback for the encoder to learn more discriminative binary codes. The auto-encoding learning objective literally rewards the code-driven graph to learn an optimal encoder. Moreover the proposed model can be simply optimized by gradient descent without violating the binary constraints. Experiments on benchmarked datasets clearly show the superiority of our framework over the state-of-the-art hashing methods.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/shen2024embarrassingly/">Embarrassingly Simple Binary Representation Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Embarrassingly Simple Binary Representation Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Embarrassingly Simple Binary Representation Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Embarrassingly%20Simple%20Binary%20Representation%20Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Shen Yuming, Qin, Chen Jiaxin, Liu, Zhu</td>
	<td>Arxiv</td>
	<td><p>Recent binary representation learning models usually require sophisticated binary optimization similarity measure or even generative models as auxiliaries. However one may wonder whether these non-trivial components are needed to formulate practical and effective hashing models. In this paper we answer the above question by proposing an embarrassingly simple approach to binary representation learning. With a simple classification objective our model only incorporates two additional fully-connected layers onto the top of an arbitrary backbone network whilst complying with the binary constraints during training. The proposed model lower-bounds the Information Bottleneck (IB) between data samples and their semantics and can be related to many recent learning to hash paradigms. We show that when properly designed even such a simple network can generate effective binary codes by fully exploring data semantics without any held-out alternating updating steps or auxiliary models. Experiments are conducted on conventional large-scale benchmarks i.e. CIFAR-10 NUS-WIDE and ImageNet where the proposed simple model outperforms the state-of-the-art methods.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/salakhutdinov2024semantic/">Semantic Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Semantic Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Semantic Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Semantic%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Salakhutdinov R., Hinton</td>
	<td>Arxiv</td>
	<td><p>We show how to learn a deep graphical model of the word-count vectors obtained from a large set of documents. The values of the latent variables in the deepest layer are easy to infer and give a much better representation of each document than Latent Semantic Analysis. When the deepest layer is forced to use a small number of binary variables (e.g. 32) the graphical model performs semantic hashing Documents are mapped to memory addresses in such a way that semantically similar documents are located at nearby addresses. Documents similar to a query document can then be found by simply accessing all the addresses that differ by only a few bits from the address of the query document. This way of extending the efficiency of hash-coding to approximate matching is much faster than locality sensitive hashing which is the fastest current method. By using semantic hashing to filter the documents given to TF-IDF we achieve higher accuracy than applying TF-IDF to the entire document set.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/russell2024labelme/">Labelme A Database And Web-based Tool For Image Annotation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Labelme A Database And Web-based Tool For Image Annotation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Labelme A Database And Web-based Tool For Image Annotation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Labelme%20A%20Database%20And%20Web-based%20Tool%20For%20Image%20Annotation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Russell B., Torralba, Murphy, Freeman</td>
	<td>Arxiv</td>
	<td><p>We seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this we developed a web-based tool that allows easy image annotation and instant sharing of such annotations. Using this annotation tool we have collected a large dataset that spans many object categories often containing multiple instances over a wide variety of images. We quantify the contents of the dataset and compare against existing state of the art datasets used for object recognition and detection. Also we show how to extend the dataset to automatically enhance object labels with WordNet discover object parts recover a depth ordering of objects in a scene and increase the number of labels using minimal user supervision and images from the web.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/rastegari2024xnor/">Xnor-net Imagenet Classification Using Binary Convolutional Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Xnor-net Imagenet Classification Using Binary Convolutional Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Xnor-net Imagenet Classification Using Binary Convolutional Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Xnor-net%20Imagenet%20Classification%20Using%20Binary%20Convolutional%20Neural%20Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Rastegari M., Ordonez, Redmon, Farhadi</td>
	<td>Arxiv</td>
	<td><p>We propose two efficient approximations to standard convolutional neural networks Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks the filters are approximated with binary values resulting in 32x memory saving. In XNOR-Networks both the filters and the input to convolutional layers are binary. XNOR-Networks approximate convolutions using primarily binary operations. This results in 58x faster convolutional operations and 32x memory savings. XNOR-Nets offer the possibility of running state-of-the-art networks on CPUs (rather than GPUs) in real-time. Our binary networks are simple accurate efficient and work on challenging visual tasks. We evaluate our approach on the ImageNet classification task. The classification accuracy with a Binary-Weight-Network version of AlexNet is only 2.937; less than the full-precision AlexNet (in top-1 measure). We compare our method with recent network binarization methods BinaryConnect and BinaryNets and outperform these methods by large margins on ImageNet more than 1637; in top-1 accuracy.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/rasiwasia2024new/">A New Approach To Cross-modal Multimedia Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A New Approach To Cross-modal Multimedia Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A New Approach To Cross-modal Multimedia Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20New%20Approach%20To%20Cross-modal%20Multimedia%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Rasiwasia N., Pereira, Coviello, Doyle, Lanckriet, Vasconcelos</td>
	<td>Arxiv</td>
	<td><p>The collected documents are selected sections from the Wikipedias featured articles collection. This is a continuously growing dataset that at the time of collection (October 2009) had 2669 articles spread over 29 categories. Some of the categories are very scarce therefore we considered only the 10 most populated ones. The articles generally have multiple sections and pictures. We have split them into sections based on section headings and assign each image to the section in which it was placed by the author(s). Then this dataset was prunned to keep only sections that contained a single image and at least 70 words. The final corpus contains 2866 multimedia documents. The median text length is 200 words.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/rong2024locality/">Locality-sensitive Hashing For Earthquake Detection A Case Study Of Scaling Data-driven Science</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Locality-sensitive Hashing For Earthquake Detection A Case Study Of Scaling Data-driven Science' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Locality-sensitive Hashing For Earthquake Detection A Case Study Of Scaling Data-driven Science' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Locality-sensitive%20Hashing%20For%20Earthquake%20Detection%20A%20Case%20Study%20Of%20Scaling%20Data-driven%20Science' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Rong Kexin, Yoon, Bergen, Elezabi, Bailis Peter, Levis, Beroza</td>
	<td>Arxiv</td>
	<td><p>In this work we report on a novel application of Locality Sensitive Hashing (LSH) to seismic data at scale. Based on the high waveform similarity between reoccurring earthquakes our application identifies potential earthquakes by searching for similar time series segments via LSH. However a straightforward implementation of this LSH-enabled application has difficulty scaling beyond 3 months of continuous time series data measured at a single seismic station. As a case study of a data-driven science workflow we illustrate how domain knowledge can be incorporated into the workload to improve both the efficiency and result quality. We describe several end-toend optimizations of the analysis pipeline from pre-processing to post-processing which allow the application to scale to time series data measured at multiple seismic stations. Our optimizations enable an over 100Ã— speedup in the end-to-end analysis pipeline. This improved scalability enabled seismologists to perform seismic analysis on more than ten years of continuous time series data from over ten seismic stations and has directly enabled the discovery of 597 new earthquakes near the Diablo Canyon nuclear power plant in California and 6123 new earthquakes in New Zealand.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/ryali2024bio/">Bio-inspired Hashing For Unsupervised Similarity Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Bio-inspired Hashing For Unsupervised Similarity Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Bio-inspired Hashing For Unsupervised Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Bio-inspired%20Hashing%20For%20Unsupervised%20Similarity%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Ryali Chaitanya, Hopfield, Grinberg, Krotov</td>
	<td>Arxiv</td>
	<td><p>The fruit fly Drosophilas olfactory circuit has inspired a new locality sensitive hashing (LSH) algorithm FlyHash. In contrast with classical LSH algorithms that produce low dimensional hash codes FlyHash produces sparse high-dimensional hash codes and has also been shown to have superior empirical performance compared to classical LSH algorithms in similarity search. However FlyHash uses random projections and cannot learn from data. Building on inspiration from FlyHash and the ubiquity of sparse expansive representations in neurobiology our work proposes a novel hashing algorithm BioHash that produces sparse high dimensional hash codes in a data-driven manner. We show that BioHash outperforms previously published benchmarks for various hashing methods. Since our learning algorithm is based on a local and biologically plausible synaptic plasticity rule our work provides evidence for the proposal that LSH might be a computational reason for the abundance of sparse expansive motifs in a variety of biological systems. We also propose a convolutional variant BioConvHash that further improves performance. From the perspective of computer science BioHash and BioConvHash are fast scalable and yield compressed binary representations that are useful for similarity search.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/shen2024nash/">NASH Toward End-to-end Neural Architecture For Generative Semantic Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=NASH Toward End-to-end Neural Architecture For Generative Semantic Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=NASH Toward End-to-end Neural Architecture For Generative Semantic Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=NASH%20Toward%20End-to-end%20Neural%20Architecture%20For%20Generative%20Semantic%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Shen Dinghan, Su, Chapfuwa, Wang, Wang, Carin, Henao</td>
	<td>Arxiv</td>
	<td><p>Semantic hashing has become a powerful paradigm for fast similarity search in many information retrieval systems. While fairly successful previous techniques generally require two-stage training and the binary constraints are handled ad-hoc. In this paper we present an end-to-end Neural Architecture for Semantic Hashing (NASH) where the binary hashing codes are treated as Bernoulli latent variables. A neural variational inference framework is proposed for training where gradients are directly backpropagated through the discrete latent variable to optimize the hash function. We also draw connections between proposed method and rate-distortion theory which provides a theoretical foundation for the effectiveness of the proposed framework. Experimental results on three public datasets demonstrate that our method significantly outperforms several state-of-the-art models on both unsupervised and supervised scenarios.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/raginsky2024locality/">Locality-sensitive Binary Codes From Shift-invariant Kernels</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Locality-sensitive Binary Codes From Shift-invariant Kernels' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Locality-sensitive Binary Codes From Shift-invariant Kernels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Locality-sensitive%20Binary%20Codes%20From%20Shift-invariant%20Kernels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Raginsky M., Lazebnik</td>
	<td>Arxiv</td>
	<td><p>This paper addresses the problem of designing binary codes for high-dimensional data such that vectors that are similar in the original space map to similar binary strings. We introduce a simple distribution-free encoding scheme based on random projections such that the expected Hamming distance between the binary codes of two vectors is related to the value of a shift-invariant kernel (e.g. a Gaussian kernel) between the vectors. We present a full theoretical analysis of the convergence properties of the proposed scheme and report favorable experimental performance as compared to a recent state-of-the-art method spectral hashing.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/qiu2024deep/">Deep Semantic Hashing With Generative Adversarial Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Semantic Hashing With Generative Adversarial Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Semantic Hashing With Generative Adversarial Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Semantic%20Hashing%20With%20Generative%20Adversarial%20Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Qiu Zhaofan, Pan, Yao, Mei</td>
	<td>Arxiv</td>
	<td><p>Hashing has been a widely-adopted technique for nearest neighbor search in large-scale image retrieval tasks. Recent research has shown that leveraging supervised information can lead to high quality hashing. However the cost of annotating data is often an obstacle when applying supervised hashing to a new domain. Moreover the results can suffer from the robustness problem as the data at training and test stage may come from different distributions. This paper studies the exploration of generating synthetic data through semisupervised generative adversarial networks (GANs) which leverages largely unlabeled and limited labeled training data to produce highly compelling data with intrinsic invariance and global coherence for better understanding statistical structures of natural data. We demonstrate that the above two limitations can be well mitigated by applying the synthetic data for hashing. Specifically a novel deep semantic hashing with GANs (DSH-GANs) is presented which mainly consists of four components a deep convolution neural networks (CNN) for learning image representations an adversary stream to distinguish synthetic images from real ones a hash stream for encoding image representations to hash codes and a classification stream. The whole architecture is trained endto-end by jointly optimizing three losses i.e. adversarial loss to correct label of synthetic or real for each sample triplet ranking loss to preserve the relative similarity ordering in the input real-synthetic triplets and classification loss to classify each sample accurately. Extensive experiments conducted on both CIFAR-10 and NUS-WIDE image benchmarks validate the capability of exploiting synthetic images for hashing. Our framework also achieves superior results when compared to state-of-the-art deep hash models.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/petrovic2024streaming/">Streaming First Story Detection With Application To Twitter</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Streaming First Story Detection With Application To Twitter' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Streaming First Story Detection With Application To Twitter' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Streaming%20First%20Story%20Detection%20With%20Application%20To%20Twitter' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Petrovic S., Osborne, Lavrenko</td>
	<td>Arxiv</td>
	<td><p>With the recent rise in popularity and size of social media there is a growing need for systems that can extract useful information from this amount of data. We address the problem of detecting new events from a stream of Twitter posts. To make event detection feasible on web-scale corpora we present an algorithm based on locality-sensitive hashing which is able overcome the limitations of traditional approaches while maintaining competitive results. In particular a comparison with a stateof-the-art system on the first story detection task shows that we achieve over an order of magnitude speedup in processing time while retaining comparable performance. Event detection experiments on a collection of 160 million Twitter posts show that celebrity deaths are the fastest spreading news on Twitter.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/pauleve2024locality/">Locality Sensitive Hashing A Comparison Of Hash Function Types And Querying Mechanisms</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Locality Sensitive Hashing A Comparison Of Hash Function Types And Querying Mechanisms' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Locality Sensitive Hashing A Comparison Of Hash Function Types And Querying Mechanisms' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Locality%20Sensitive%20Hashing%20A%20Comparison%20Of%20Hash%20Function%20Types%20And%20Querying%20Mechanisms' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Pauleve Loic, Jegou, Amsaleg</td>
	<td>Arxiv</td>
	<td><p>It is well known that high-dimensional nearest-neighbor retrieval is very expensive. Dramatic performance gains are obtained using approximate search schemes such as the popular Locality-Sensitive Hashing (LSH). Several extensions have been proposed to address the limitations of this algorithm in particular by choosing more appropriate hash functions to better partition the vector space. All the proposed extensions however rely on a structured quantizer for hashing poorly fitting real data sets limiting its performance in practice. In this paper we compare several families of space hashing functions in a real setup namely when searching for high-dimension SIFT descriptors. The comparison of random projections lattice quantizers k-means and hierarchical k-means reveal that unstructured quantizer significantly improves the accuracy of LSH as it closely fits the data in the feature space. We then compare two querying mechanisms introduced in the literature with the one originally proposed in LSH and discuss their respective merits and limitations.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/petrovic2024using/">Using Paraphrases For Improving First Story Detection In News And Twitter</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Using Paraphrases For Improving First Story Detection In News And Twitter' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Using Paraphrases For Improving First Story Detection In News And Twitter' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Using%20Paraphrases%20For%20Improving%20First%20Story%20Detection%20In%20News%20And%20Twitter' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Petrovic S., Osborne, Lavrenko</td>
	<td>Arxiv</td>
	<td><p>First story detection (FSD) involves identifying first stories about events from a continuous stream of documents. A major problem in this task is the high degree of lexical variation in documents which makes it very difficult to detect stories that talk about the same event but expressed using different words. We suggest using paraphrases to alleviate this problem making this the first work to use paraphrases for FSD. We show a novel way of integrating paraphrases with locality sensitive hashing (LSH) in order to obtain an efficient FSD system that can scale to very large datasets. Our system achieves state-of-the-art results on the first story detection task beating both the best supervised and unsupervised systems. To test our approach on large data we construct a corpus of events for Twitter consisting of 50 million documents and show that paraphrasing is also beneficial in this domain.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/ramos2024blockboost/">Blockboost Scalable And Efficient Blocking Through Boosting</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Blockboost Scalable And Efficient Blocking Through Boosting' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Blockboost Scalable And Efficient Blocking Through Boosting' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Blockboost%20Scalable%20And%20Efficient%20Blocking%20Through%20Boosting' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Ramos Thiago, Schuller, Okuno, Nissenbaum, Oliveira, Orenstein</td>
	<td>Arxiv</td>
	<td><p>As datasets grow larger matching and merging entries from different databases has become a costly task in modern data pipelines. To avoid expensive comparisons between entries blocking similar items is a popular preprocessing step. In this paper we introduce BlockBoost a novel boosting-based method that generates compact binary hash codes for database entries through which blocking can be performed efficiently. The algorithm is fast and scalable resulting in computational costs that are orders of magnitude lower than current benchmarks. Unlike existing alternatives BlockBoost comes with associated feature importance measures for interpretability and possesses strong theoretical guarantees including lower bounds on critical performance metrics like recall and reduction ratio. Finally we show that BlockBoost delivers great empirical results outperforming state-of-the-art blocking benchmarks in terms of both performance metrics and computational cost.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/ou2024comparing/">Comparing Apples To Oranges A Scalable Solution With Heterogeneous Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Comparing Apples To Oranges A Scalable Solution With Heterogeneous Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Comparing Apples To Oranges A Scalable Solution With Heterogeneous Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Comparing%20Apples%20To%20Oranges%20A%20Scalable%20Solution%20With%20Heterogeneous%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Ou M., Cui, Wang, Wang, Zhu, Yang</td>
	<td>Arxiv</td>
	<td><p>Although hashing techniques have been popular for the large scale similarity search problem most of the existing methods for designing optimal hash functions focus on homogeneous similarity assessment i.e. the data entities to be indexed are of the same type. Realizing that heterogeneous entities and relationships are also ubiquitous in the real world applications there is an emerging need to retrieve and search similar or relevant data entities from multiple heterogeneous domains e.g. recommending relevant posts and images to a certain Facebook user. In this paper we address the problem of comparing apples to oranges under the large scale setting. Specifically we propose a novel Relation-aware Heterogeneous Hashing (RaHH) which provides a general framework for generating hash codes of data entities sitting in multiple heterogeneous domains. Unlike some existing hashing methods that map heterogeneous data in a common Hamming space the RaHH approach constructs a Hamming space for each type of data entities and learns optimal mappings between them simultaneously. This makes the learned hash codes flexibly cope with the characteristics of different data domains. Moreover the RaHH framework encodes both homogeneous and heterogeneous relationships between the data entities to design hash functions with improved accuracy. To validate the proposed RaHH method we conduct extensive evaluations on two large datasets; one is crawled from a popular social media sites Tencent Weibo and the other is an open dataset of Flickr(NUS-WIDE). The experimental results clearly demonstrate that the RaHH outperforms several state-of-the-art hashing methods with significant performance gains.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/norouzi2024hamming/">Hamming Distance Metric Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Hamming Distance Metric Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Hamming Distance Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Hamming%20Distance%20Metric%20Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Norouzi M., Fleet, Salakhutdinov</td>
	<td>Arxiv</td>
	<td><p>Motivated by large-scale multimedia applications we propose to learn mappings from high-dimensional data to binary codes that preserve semantic similarity. Binary codes are well suited to large-scale applications as they are storage efficient and permit exact sub-linear kNN search. The framework is applicable to broad families of mappings and uses a flexible form of triplet ranking loss. We overcome discontinuous optimization of the discrete mappings by minimizing a piecewise-smooth upper bound on empirical loss inspired by latent structural SVMs. We develop a new loss-augmented inference algorithm that is quadratic in the code length. We show strong retrieval performance on CIFAR-10 and MNIST with promising classification results using no more than kNN on the binary codes.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/norouzi2024minimal/">Minimal Loss Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Minimal Loss Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Minimal Loss Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Minimal%20Loss%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Norouzi M., Fleet</td>
	<td>Arxiv</td>
	<td><p>We propose a method for learning similaritypreserving hash functions that map highdimensional data onto binary codes. The formulation is based on structured prediction with latent variables and a hinge-like loss function. It is efficient to train for large datasets scales well to large code lengths and outperforms state-of-the-art methods.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/neyshabur2024power/">The Power Of Asymmetry In Binary Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=The Power Of Asymmetry In Binary Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=The Power Of Asymmetry In Binary Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=The%20Power%20Of%20Asymmetry%20In%20Binary%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Neyshabur B., Salakhutdinov, Srebro</td>
	<td>Arxiv</td>
	<td><p>When approximating binary similarity using the hamming distance between short binary hashes we show that even if the similarity is symmetric we can have shorter and more accurate hashes by using two distinct code maps. I.e. by approximating the similarity between x and x 0 as the hamming distance between f(x) and g(x0) for two distinct binary codes f g rather than as the hamming distance between f(x) and f(x0).</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/ng2024unsupervised/">Unsupervised Hashing With Similarity Distribution Calibration</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Hashing With Similarity Distribution Calibration' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Unsupervised Hashing With Similarity Distribution Calibration' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Unsupervised%20Hashing%20With%20Similarity%20Distribution%20Calibration' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Ng Kam, Zhu, Hoe, Chan, Zhang, Song, Xiang</td>
	<td>Arxiv</td>
	<td><p>Unsupervised hashing methods typically aim to preserve the similarity between data points in a feature space by mapping them to binary hash codes. However these methods often overlook the fact that the similarity between data points in the continuous feature space may not be preserved in the discrete hash code space due to the limited similarity range of hash codes. The similarity range is bounded by the code length and can lead to a problem known as similarity collapse. That is the positive and negative pairs of data points become less distinguishable from each other in the hash space. To alleviate this problem in this paper a novel Simialrity Distribution Calibration (SDC) method is introduced. SDC aligns the hash code similarity distribution towards a calibration distribution (e.g. beta distribution) with sufficient spread across the entire similarity range thus alleviating the similarity collapse problem. Extensive experiments show that our SDC outperforms significantly the state-of-the-art alternatives on coarse category-level and instance-level image retrieval.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/mukherjee2024nmf/">An NMF Perspective On Binary Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=An NMF Perspective On Binary Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=An NMF Perspective On Binary Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=An%20NMF%20Perspective%20On%20Binary%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Mukherjee Lopamudra, Ravi, Ithapu, Singh</td>
	<td>Arxiv</td>
	<td><p>The pervasiveness of massive data repositories has led to much interest in efficient methods for indexing search and retrieval. For image data a rapidly developing body of work for these applications shows impressive performance with methods that broadly fall under the umbrella term of Binary Hashing. Given a distance matrix a binary hashing algorithm solves for a binary code for the given set of examples whose Hamming distance nicely approximates the original distances. The formulation is non-convex â€” so existing solutions adopt spectral relaxations or perform coordinate descent (or quantization) on a surrogate objective that is numerically more tractable. In this paper we first derive an Augmented Lagrangian approach to optimize the standard binary Hashing objective (i.e. maintain fidelity with a given distance matrix). With appropriate step sizes we find that this scheme already yields results that match or substantially outperform state of the art methods on most benchmarks used in the literature. Then to allow the model to scale to large datasets we obtain an interesting reformulation of the binary hashing objective as a non-negative matrix factorization. Later this leads to a simple multiplicative updates algorithm â€” whose parallelization properties are exploited to obtain a fast GPU based implementation. We give a probabilistic analysis of our initialization scheme and present a range of experiments to show that the method is simple to implement and competes favorably with available methods (both for optimization and generalization).</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/morgado2024deep/">Deep Hashing With Hash-consistent Large Margin Proxy Embeddings</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Hashing With Hash-consistent Large Margin Proxy Embeddings' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Hashing With Hash-consistent Large Margin Proxy Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Hashing%20With%20Hash-consistent%20Large%20Margin%20Proxy%20Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Morgado Pedro, Li, Pereira, Saberian, Vasconcelos</td>
	<td>Arxiv</td>
	<td><p>Image hash codes are produced by binarizing the embeddings of convolutional neural networks (CNN) trained for either classification or retrieval. While proxy embeddings achieve good performance on both tasks they are non-trivial to binarize due to a rotational ambiguity that encourages non-binary embeddings. The use of a fixed set of proxies (weights of the CNN classification layer) is proposed to eliminate this ambiguity and a procedure to design proxy sets that are nearly optimal for both classification and hashing is introduced. The resulting hash-consistent large margin (HCLM) proxies are shown to encourage saturation of hashing units thus guaranteeing a small binarization error while producing highly discriminative hash-codes. A semantic extension (sHCLM) aimed to improve hashing performance in a transfer scenario is also proposed. Extensive experiments show that sHCLM embeddings achieve significant improvements over state-of-the-art hashing procedures on several small and large datasets both within and beyond the set of training classes.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/yandex2024yandex/">Yandex DEEP-1B</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Yandex DEEP-1B' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Yandex DEEP-1B' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Yandex%20DEEP-1B' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Yandex Yandex</td>
	<td>Arxiv</td>
	<td><p>Yandex DEEP-1B image descriptor dataset consisting of the projected and normalized outputs from the last fully-connected layer of the GoogLeNet model which was pretrained on the Imagenet classification task.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/yan2024deep/">Deep Hashing By Discriminating Hard Examples</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Hashing By Discriminating Hard Examples' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Hashing By Discriminating Hard Examples' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Hashing%20By%20Discriminating%20Hard%20Examples' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Yan Cheng, Pang, Bai, Shen, Zhou, Hancock</td>
	<td>Arxiv</td>
	<td><p>This paper tackles a rarely explored but critical problem within learning to hash i.e. to learn hash codes that effectively discriminate hard similar and dissimilar examples to empower large-scale image retrieval. Hard similar examples refer to image pairs from the same semantic class that demonstrate some shared appearance but have different fine-grained appearance. Hard dissimilar examples are image pairs that come from different semantic classes but exhibit similar appearance. These hard examples generally have a small distance due to the shared appearance. Therefore effective encoding of the hard examples can well discriminate the relevant images within a small Hamming distance enabling more accurate retrieval in the top-ranked returned images. However most existing hashing methods cannot capture this key information as their optimization is dominated byeasy examples i.e. distant similar/dissimilar pairs that share no or limited appearance. To address this problem we introduce a novel Gamma distribution-enabled and symmetric Kullback-Leibler divergence-based loss which is dubbed dual hinge loss because it works similarly as imposing two smoothed hinge losses on the respective similar and dissimilar pairs. Specifically the loss enforces exponentially variant penalization on the hard similar (dissimilar) examples to emphasize and learn their fine-grained difference. It meanwhile imposes a bounding penalization on easy similar (dissimilar) examples to prevent the dominance of the easy examples in the optimization while preserving the high-level similarity (dissimilarity). This enables our model to well encode the key information carried by both easy and hard examples. Extensive empirical results on three widely-used image retrieval datasets show that (i) our method consistently and substantially outperforms state-of-the-art competing methods using hash codes of the same length and (ii) our method can use significantly (e.g. 5037;-7537;) shorter hash codes to perform substantially better than or comparably well to the competing methods.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/xiong2024adaptive/">Adaptive Quantization For Hashing An Information-based Approach To Learning Binary Codes</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Adaptive Quantization For Hashing An Information-based Approach To Learning Binary Codes' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Adaptive Quantization For Hashing An Information-based Approach To Learning Binary Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Adaptive%20Quantization%20For%20Hashing%20An%20Information-based%20Approach%20To%20Learning%20Binary%20Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Xiong C., Chen, Chen, Johnson, Corso</td>
	<td>Arxiv</td>
	<td><p>Large-scale data mining and retrieval applications have increasingly turned to compact binary data representations as a way to achieve both fast queries and efficient data storage; many algorithms have been proposed for learning effective binary encodings. Most of these algorithms focus on learning a set of projection hyperplanes for the data and simply binarizing the result from each hyperplane but this neglects the fact that informativeness may not be uniformly distributed across the projections. In this paper we address this issue by proposing a novel adaptive quantization (AQ) strategy that adaptively assigns varying numbers of bits to different hyperplanes based on their information content. Our method provides an information-based schema that preserves the neighborhood structure of data points and we jointly find the globally optimal bit-allocation for all hyperplanes. In our experiments we compare with state-of-the-art methods on four large-scale datasets and find that our adaptive quantization approach significantly improves on traditional hashing methods.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/xu2024convolutional/">Convolutional Neural Networks For Text Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Convolutional Neural Networks For Text Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Convolutional Neural Networks For Text Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Convolutional%20Neural%20Networks%20For%20Text%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Xu Jiaming, Pengwang, Tian, Xu, Zhao, Wang, Hao</td>
	<td>Arxiv</td>
	<td><p>Hashing as a popular approximate nearest neighbor search has been widely used for large-scale similarity search. Recently a spectrum of machine learning methods are utilized to learn similarity-preserving binary codes. However most of them directly encode the explicit features keywords which fail to preserve the accurate semantic similarities in binary code beyond keyword matching especially on short texts. Here we propose a novel text hashing framework with convolutional neural networks. In particular we first embed the keyword features into compact binary code with a locality preserving constraint. Meanwhile word features and position features are together fed into a convolutional network to learn the implicit features which are further incorporated with the explicit features to fit the pre-trained binary code. Such base method can be successfully accomplished without any external tags/labels and other three model variations are designed to integrate tags/labels. Experimental results show the superiority of our proposed approach over several state-of-the-art hashing methods when tested on one short text dataset as well as one normal text dataset.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/wu2024deep/">Deep Incremental Hashing Network For Efficient Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Incremental Hashing Network For Efficient Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Incremental Hashing Network For Efficient Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Incremental%20Hashing%20Network%20For%20Efficient%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Wu Dayan, Dai, Liu, Li, Wang</td>
	<td>Arxiv</td>
	<td><p>Hashing has shown great potential in large-scale image retrieval due to its storage and computation efficiency especially the recent deep supervised hashing methods. To achieve promising performance deep supervised hashing methods require a large amount of training data from different classes. However when images of new categories emerge existing deep hashing methods have to retrain the CNN model and generate hash codes for all the database images again which is impractical for large-scale retrieval system. In this paper we propose a novel deep hashing framework called Deep Incremental Hashing Network (DIHN) for learning hash codes in an incremental manner. DIHN learns the hash codes for the new coming images directly while keeping the old ones unchanged. Simultaneously a deep hash function for query set is learned by preserving the similarities between training points. Extensive experiments on two widely used image retrieval benchmarks demonstrate that the proposed DIHN framework can significantly decrease the training time while keeping the state-of-the-art retrieval accuracy.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/weng2024online/">Online Hashing With Efficient Updating Of Binary Codes</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Online Hashing With Efficient Updating Of Binary Codes' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Online Hashing With Efficient Updating Of Binary Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Online%20Hashing%20With%20Efficient%20Updating%20Of%20Binary%20Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Weng Zhenyu, Zhu</td>
	<td>Arxiv</td>
	<td><p>Online hashing methods are efficient in learning the hash functions from the streaming data. However when the hash functions change the binary codes for the database have to be recomputed to guarantee the retrieval accuracy. Recomputing the binary codes by accumulating the whole database brings a timeliness challenge to the online retrieval process. In this paper we propose a novel online hashing framework to update the binary codes efficiently without accumulating the whole database. In our framework the hash functions are fixed and the projection functions are introduced to learn online from the streaming data. Therefore inefficient updating of the binary codes by accumulating the whole database can be transformed to efficient updating of the binary codes by projecting the binary codes into another binary space. The queries and the binary code database are projected asymmetrically to further improve the retrieval accuracy. The experiments on two multi-label image databases demonstrate the effectiveness and the efficiency of our method for multi-label image retrieval.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/xia2024supervised/">Supervised Hashing Via Image Representation Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Hashing Via Image Representation Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Supervised Hashing Via Image Representation Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Supervised%20Hashing%20Via%20Image%20Representation%20Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Xia R., Pan, Lai, Liu, Yan.</td>
	<td>Arxiv</td>
	<td><p>Hashing is a popular approximate nearest neighbor search approach for large-scale image retrieval. Supervised hashing which incorporates similarity/dissimilarity information on entity pairs to improve the quality of hashing function learning has recently received increasing attention. However in the existing supervised hashing methods for images an input image is usually encoded by a vector of hand-crafted visual features. Such hand-crafted feature vectors do not necessarily preserve the accurate semantic similarities of images pairs which may often degrade the performance of hashing function learning. In this paper we propose a supervised hashing method for image retrieval in which we automatically learn a good image representation tailored to hashing as well as a set of hash functions. The proposed method has two stages. In the first stage given the pairwise similarity matrix S over training images we propose a scalable coordinate descent method to decompose S into a product of HHT where H is a matrix with each of its rows being the approximate hash code associated to a training image. In the second stage we propose to simultaneously learn a good feature representation for the input images as well as a set of hash functions via a deep convolutional network tailored to the learned hash codes in H and optionally the discrete class labels of the images. Extensive empirical evaluations on three benchmark datasets with different kinds of images show that the proposed method has superior performance gains over several state-of-the-art supervised and unsupervised hashing methods.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/xu2024harmonious/">Harmonious Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Harmonious Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Harmonious Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Harmonious%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Xu B., Bu, Chen, He, Cai</td>
	<td>Arxiv</td>
	<td><p>Hashing-based fast nearest neighbor search technique has attracted great attention in both research and industry areas recently. Many existing hashing approaches encode data with projection-based hash functions and represent each projected dimension by 1-bit. However the dimensions with high variance hold large energy or information of data but treated equivalently as dimensions with low variance which leads to a serious information loss. In this paper we introduce a novel hashing algorithm called Harmonious Hashing which aims at learning hash functions with low information loss. Specifically we learn a set of optimized projections to preserve the maximum cumulative energy and meet the constraint of equivalent variance on each dimension as much as possible. In this way we could minimize the information loss after binarization. Despite the extreme simplicity our method outperforms superiorly to many state-of-the-art hashing methods in large-scale and high-dimensional nearest neighbor search experiments.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/weiss2024spectral/">Spectral Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Spectral Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Spectral Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Spectral%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Weiss Y., Torralba, Fergus</td>
	<td>Arxiv</td>
	<td><p>Semantic hashing seeks compact binary codes of data-points so that the Hamming distance between codewords correlates with semantic similarity. In this paper we show that the problem of finding a best code for a given dataset is closely related to the problem of graph partitioning and can be shown to be NP hard. By relaxing the original problem we obtain a spectral method whose solutions are simply a subset of thresholded eigenvectors of the graph Laplacian. By utilizing recent results on convergence of graph Laplacian eigenvectors to the Laplace-Beltrami eigenfunctions of manifolds we show how to efficiently calculate the code of a novel datapoint. Taken together both learning the code and applying it to a novel point are extremely simple. Our experiments show that our codes outperform the state-of-the art.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/weiss2024multidimensional/">Multidimensional Spectral Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Multidimensional Spectral Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Multidimensional Spectral Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Multidimensional%20Spectral%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Weiss Y., Fergus, Torralba</td>
	<td>Arxiv</td>
	<td><p>en a surge of interest in methods based on semantic hashing i.e. compact binary codes of data-points so that the Hamming distance between codewords correlates with similarity. In reviewing and comparing existing methods we show that their relative performance can change drastically depending on the definition of ground-truth neighbors. Motivated by this finding we propose a new formulation for learning binary codes which seeks to reconstruct the affinity between datapoints rather than their distances. We show that this criterion is intractable to solve exactly but a spectral relaxation gives an algorithm where the bits correspond to thresholded eigenvectors of the affinity matrix and as the number of datapoints goes to infinity these eigenvectors converge to eigenfunctions of Laplace-Beltrami operators similar to the recently proposed Spectral Hashing (SH) method. Unlike SH whose performance may degrade as the number of bits increases the optimal code using our formulation is guaranteed to faithfully reproduce the affinities as the number of bits increases. We show that the number of eigenfunctions needed may increase exponentially with dimension but introduce a kernel trick to allow us to compute with an exponentially large number of bits but using only memory and computation that grows linearly with dimension. Experiments shows that MDSH outperforms the state-of-the art especially in the challenging regime of small distance thresholds.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/wei2024learning/">A-net Learning Attribute-aware Hash Codes For Large-scale Fine-grained Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A-net Learning Attribute-aware Hash Codes For Large-scale Fine-grained Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A-net Learning Attribute-aware Hash Codes For Large-scale Fine-grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A-net%20Learning%20Attribute-aware%20Hash%20Codes%20For%20Large-scale%20Fine-grained%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Wei Xiu-shen, Xiu-shen_wei, Shen Yang, Sun, Ye, Yang</td>
	<td>Arxiv</td>
	<td><p>Our work focuses on tackling large-scale fine-grained image retrieval as ranking the images depicting the concept of interests (i.e. the same sub-category labels) highest based on the fine-grained details in the query. It is desirable to alleviate the challenges of both fine-grained nature of small inter-class variations with large intra-class variations and explosive growth of fine-grained data for such a practical task. In this paper we propose an Attribute-Aware hashing Network (A-Net) for generating attribute-aware hash codes to not only make the retrieval process efficient but also establish explicit correspondences between hash codes and visual attributes. Specifically based on the captured visual representations by attention we develop an encoder-decoder structure network of a reconstruction task to unsupervisedly distill high-level attribute-specific vectors from the appearance-specific visual representations without attribute annotations. A-Net is also equipped with a feature decorrelation constraint upon these attribute vectors to enhance their representation abilities. Finally the required hash codes are generated by the attribute vectors driven by preserving original similarities. Qualitative experiments on five benchmark fine-grained datasets show our superiority over competing methods. More importantly quantitative results demonstrate the obtained hash codes can strongly correspond to certain kinds of crucial properties of fine-grained objects.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/wang2024uncertainty/">Uncertainty-aware Unsupervised Video Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Uncertainty-aware Unsupervised Video Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Uncertainty-aware Unsupervised Video Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Uncertainty-aware%20Unsupervised%20Video%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Wang Yucheng, Zhou, Sun, Qian</td>
	<td>Arxiv</td>
	<td><p>Learning to hash has become popular for video retrieval due to its fast speed and low storage consumption. Previous efforts formulate video hashing as training a binary auto-encoder for which noncontinuous latent representations are optimized by the biased straight-through (ST) back-propagation heuristic. We propose to formulate video hashing as learning a discrete variational auto-encoder with the factorized Bernoulli latent distribution termed as Bernoulli variational auto-encoder (BerVAE). The corresponding evidence lower bound (ELBO) in our BerVAE implementation leads to closed-form gradient expression which can be applied to achieve principled training along with some other unbiased gradient estimators. BerVAE enables uncertainty-aware video hashing by predicting the probability distribution of video hash code-words thus providing reliable uncertainty quantification. Experiments on both simulated and real-world large-scale video data demonstrate that our BerVAE trained with unbiased gradient estimators can achieve the state-of-the-art retrieval performance. Furthermore we show that quantified uncertainty is highly correlated to video retrieval performance which can be leveraged to further improve the retrieval accuracy. Our code is available at https://github.com/wangyucheng1234/BerVAE</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/wang2024semi/">Semi-supervised Deep Quantization For Cross-modal Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Semi-supervised Deep Quantization For Cross-modal Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Semi-supervised Deep Quantization For Cross-modal Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Semi-supervised%20Deep%20Quantization%20For%20Cross-modal%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Wang Xin, Zhu, Liu</td>
	<td>Arxiv</td>
	<td><p>The problem of cross-modal similarity search which aims at making efficient and accurate queries across multiple domains has become a significant and important research topic. Composite quantization a compact coding solution superior to hashing techniques has shown its effectiveness for similarity search. However most existing works utilizing composite quantization to search multi-domain content only consider either pairwise similarity information or class label information across different domains which fails to tackle the semi-supervised problem in composite quantization. In this paper we address the semi-supervised quantization problem by considering (i) pairwise similarity information (without class label information) across different domains which captures the intra-document relation (ii) cross-domain data with class label which can help capture inter-document relation and (iii) cross-domain data with neither pairwise similarity nor class label which enables the full use of abundant unlabelled information. To the best of our knowledge we are the first to consider both supervised information (pairwise similarity + class label) and unsupervised information (neither pairwise similarity nor class label) simultaneously in composite quantization. A challenging problem arises how can we jointly handle these three sorts of information across multiple domains in an efficient way To tackle this challenge we propose a novel semi-supervised deep quantization (SSDQ) model that takes both supervised and unsupervised information into account. The proposed SSDQ model is capable of incorporating the above three kinds of information into one single framework when utilizing composite quantization for accurate and efficient queries across different domains. More specifically we employ a modified deep autoencoder for better latent representation and formulate pairwise similarity loss supervised quantization loss as well as unsupervised distribution match loss to handle all three types of information. The extensive experiments demonstrate the significant improvement of SSDQ over several state-of-the-art methods on various datasets.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/wang2024prototype/">Prototype-supervised Adversarial Network For Targeted Attack Of Deep Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Prototype-supervised Adversarial Network For Targeted Attack Of Deep Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Prototype-supervised Adversarial Network For Targeted Attack Of Deep Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Prototype-supervised%20Adversarial%20Network%20For%20Targeted%20Attack%20Of%20Deep%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Wang Xunguang, Zhang, Wu, Shen, Lu</td>
	<td>Arxiv</td>
	<td><p>Due to its powerful capability of representation learning and high-efficiency computation deep hashing has made significant progress in large-scale image retrieval. However deep hashing networks are vulnerable to adversarial examples which is a practical secure problem but seldom studied in hashing-based retrieval field. In this paper we propose a novel prototype-supervised adversarial network (ProS-GAN) which formulates a flexible generative architecture for efficient and effective targeted hashing attack. To the best of our knowledge this is the first generation-based method to attack deep hashing networks. Generally our proposed framework consists of three parts i.e. a PrototypeNet a generator and a discriminator. Specifically the designed PrototypeNet embeds the target label into the semantic representation and learns the prototype code as the category-level representative of the target label. Moreover the semantic representation and the original image are jointly fed into the generator for flexible targeted attack. Particularly the prototype code is adopted to supervise the generator to construct the targeted adversarial example by minimizing the Hamming distance between the hash code of the adversarial example and the prototype code. Furthermore the generator is against the discriminator to simultaneously encourage the adversarial examples visually realistic and the semantic representation informative. Extensive experiments verify that the proposed framework can efficiently produce adversarial examples with better targeted attack performance and transferability over state-of-the-art targeted attack methods of deep hashing.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/wang2024online/">Online Collective Matrix Factorization Hashing For Large-scale Cross-media Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Online Collective Matrix Factorization Hashing For Large-scale Cross-media Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Online Collective Matrix Factorization Hashing For Large-scale Cross-media Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Online%20Collective%20Matrix%20Factorization%20Hashing%20For%20Large-scale%20Cross-media%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Wang Di, Wang, An, Gao, Tian</td>
	<td>Arxiv</td>
	<td><p>Cross-modal hashing has been widely investigated recently for its efficiency in large-scale cross-media retrieval. However most existing cross-modal hashing methods learn hash functions in a batch-based learning mode. Such mode is not suitable for large-scale data sets due to the large memory consumption and loses its efficiency when training streaming data. Online cross-modal hashing can deal with the above problems by learning hash model in an online learning process. However existing online cross-modal hashing methods cannot update hash codes of old data by the newly learned model. In this paper we propose Online Collective Matrix Factorization Hashing (OCMFH) based on collective matrix factorization hashing (CMFH) which can adaptively update hash codes of old data according to dynamic changes of hash model without accessing to old data. Specifically it learns discriminative hash codes for streaming data by collective matrix factorization in an online optimization scheme. Unlike conventional CMFH which needs to load the entire data points into memory the proposed OCMFH retrains hash functions only by newly arriving data points. Meanwhile it generates hash codes of new data and updates hash codes of old data by the latest updated hash model. In such way hash codes of new data and old data are well-matched. Furthermore a zero mean strategy is developed to solve the mean-varying problem in the online hash learning process. Extensive experiments on three benchmark data sets demonstrate the effectiveness and efficiency of OCMFH on online cross-media retrieval.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/wang2024semantic/">Semantic Topic Multimodal Hashing For Cross-media Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Semantic Topic Multimodal Hashing For Cross-media Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Semantic Topic Multimodal Hashing For Cross-media Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Semantic%20Topic%20Multimodal%20Hashing%20For%20Cross-media%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Wang Di, Gao, He</td>
	<td>Arxiv</td>
	<td><p>Multimodal hashing is essential to cross-media similarity search for its low storage cost and fast query speed. Most existing multimodal hashing methods embedded heterogeneous data into a common low-dimensional Hamming space and then rounded the continuous embeddings to obtain the binary codes. Yet they usually neglect the inherent discrete nature of hashing for relaxing the discrete constraints which will cause degraded retrieval performance especially for long codes. For this purpose a novel Semantic Topic Multimodal Hashing (STMH) is developed by considering latent semantic information in coding procedure. It first discovers clustering patterns of texts and robust factorizes the matrix of images to obtain multiple semantic topics of texts and concepts of images. Then the learned multimodal semantic features are transformed into a common subspace by their correlations. Finally each bit of unified hash code can be generated directly by figuring out whether a topic or concept is contained in a text or an image. Therefore the obtained model by STMH is more suitable for hashing scheme as it directly learns discrete hash codes in the coding process. Experimental results demonstrate that the proposed method outperforms several state-of-the-art methods.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/wang2024sequential/">Sequential Projection Learning For Hashing With Compact Codes</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Sequential Projection Learning For Hashing With Compact Codes' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Sequential Projection Learning For Hashing With Compact Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Sequential%20Projection%20Learning%20For%20Hashing%20With%20Compact%20Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Wang J., Kumar, Chang</td>
	<td>Arxiv</td>
	<td><p>Hashing based Approximate Nearest Neighbor (ANN) search has attracted much attention due to its fast query time and drastically reduced storage. However most of the hashing methods either use random projections or extract principal directions from the data to derive hash functions. The resulting embedding suffers from poor discrimination when compact codes are used. In this paper we propose a novel data-dependent projection learning method such that each hash function is designed to correct the errors made by the previous one sequentially. The proposed method easily adapts to both unsupervised and semi-supervised scenarios and shows significant performance gains over the state-ofthe-art methods on two large datasets containing up to 1 million points.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/wang2024deep/">Deep Collaborative Discrete Hashing With Semantic-invariant Structure</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Collaborative Discrete Hashing With Semantic-invariant Structure' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Collaborative Discrete Hashing With Semantic-invariant Structure' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Collaborative%20Discrete%20Hashing%20With%20Semantic-invariant%20Structure' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Wang Zijian, Zhang, Huang</td>
	<td>Arxiv</td>
	<td><p>Existing deep hashing approaches fail to fully explore semantic correlations and neglect the effect of linguistic context on visual attention learning leading to inferior performance. This paper proposes a dual-stream learning framework dubbed Deep Collaborative Discrete Hashing (DCDH) which constructs a discriminative common discrete space by collaboratively incorporating the shared and individual semantics deduced from visual features and semantic labels. Specifically the context-aware representations are generated by employing the outer product of visual embeddings and semantic encodings. Moreover we reconstruct the labels and introduce the focal loss to take advantage of frequent and rare concepts. The common binary code space is built on the joint learning of the visual representations attended by language the semantic-invariant structure construction and the label distribution correction. Extensive experiments demonstrate the superiority of our method.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/wang2024affinity/">Affinity Preserving Quantization For Hashing A Vector Quantization Approach To Learning Compact Binary Codes</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Affinity Preserving Quantization For Hashing A Vector Quantization Approach To Learning Compact Binary Codes' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Affinity Preserving Quantization For Hashing A Vector Quantization Approach To Learning Compact Binary Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Affinity%20Preserving%20Quantization%20For%20Hashing%20A%20Vector%20Quantization%20Approach%20To%20Learning%20Compact%20Binary%20Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Wang Z., Duan, Huang, Gao</td>
	<td>Arxiv</td>
	<td><p>Hashing techniques are powerful for approximate nearest neighbour (ANN) search. Existing quantization methods in hashing are all focused on scalar quantization (SQ) which is inferior in utilizing the inherent data distribution. In this paper we propose a novel vector quantization (VQ) method named affinity preserving quantization (APQ) to improve the quantization quality of projection values which has significantly boosted the performance of state-of-the-art hashing techniques. In particular our method incorporates the neighbourhood structure in the pre- and post-projection data space into vector quantization. APQ minimizes the quantization errors of projection values as well as the loss of affinity property of original space. An effective algorithm has been proposed to solve the joint optimization problem in APQ and the extension to larger binary codes has been resolved by applying product quantization to APQ. Extensive experiments have shown that APQ consistently outperforms the state-of-the-art quantization methods and has significantly improved the performance of various hashing techniques.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/wang2024hamming/">Hamming Compatible Quantization For Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Hamming Compatible Quantization For Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Hamming Compatible Quantization For Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Hamming%20Compatible%20Quantization%20For%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Wang Z., Duan, Lin, Wang, Gao</td>
	<td>Arxiv</td>
	<td><p>Hashing is one of the effective techniques for fast Approximate Nearest Neighbour (ANN) search. Traditional single-bit quantization (SBQ) in most hashing methods incurs lots of quantization error which seriously degrades the search performance. To address the limitation of SBQ researchers have proposed promising multi-bit quantization (MBQ) methods to quantize each projection dimension with multiple bits. However some MBQ methods need to adopt specific distance for binary code matching instead of the original Hamming distance which would significantly decrease the retrieval speed. Two typical MBQ methods Hierarchical Quantization and Double Bit Quantization retain the Hamming distance but both of them only consider the projection dimensions during quantization ignoring the neighborhood structure of raw data inherent in Euclidean space. In this paper we propose a multi-bit quantization method named Hamming Compatible Quantization (HCQ) to preserve the capability of similarity metric between Euclidean space and Hamming space by utilizing the neighborhood structure of raw data. Extensive experiment results have shown our approach significantly improves the performance of various stateof-the-art hashing methods while maintaining fast retrieval speed.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/wang2024idea/">IDEA An Invariant Perspective For Efficient Domain Adaptive Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=IDEA An Invariant Perspective For Efficient Domain Adaptive Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=IDEA An Invariant Perspective For Efficient Domain Adaptive Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=IDEA%20An%20Invariant%20Perspective%20For%20Efficient%20Domain%20Adaptive%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Wang Haixin, Wu, Sun, Zhang, Chen, Hua, Luo</td>
	<td>Arxiv</td>
	<td><p>In this paper we investigate the problem of unsupervised domain adaptive hashing which leverage knowledge from a label-rich source domain to expedite learning to hash on a label-scarce target domain. Although numerous existing approaches attempt to incorporate transfer learning techniques into deep hashing frameworks they often neglect the essential invariance for adequate alignment between these two domains. Worse yet these methods fail to distinguish between causal and non-causal effects embedded in images rendering cross-domain retrieval ineffective. To address these challenges we propose an Invariance-acquired Domain AdaptivE HAshing (IDEA) model. Our IDEA first decomposes each image into a causal feature representing label information and a non-causal feature indicating domain information. Subsequently we generate discriminative hash codes using causal features with consistency learning on both source and target domains. More importantly we employ a generative model for synthetic samples to simulate the intervention of various non-causal effects ultimately minimizing their impact on hash codes for domain invariance. Comprehensive experiments conducted on benchmark datasets validate the superior performance of our IDEA compared to a variety of competitive baselines.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/ye2024unsupervised/">Unsupervised Few-bits Semantic Hashing With Implicit Topics Modeling</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Few-bits Semantic Hashing With Implicit Topics Modeling' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Unsupervised Few-bits Semantic Hashing With Implicit Topics Modeling' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Unsupervised%20Few-bits%20Semantic%20Hashing%20With%20Implicit%20Topics%20Modeling' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Ye Fanghua, Manotumruksa, Yilmaz</td>
	<td>Arxiv</td>
	<td><p>Semantic hashing is a powerful paradigm for representing texts as compact binary hash codes. The explosion of short text data has spurred the demand of few-bits hashing. However the performance of existing semantic hashing methods cannot be guaranteed when applied to few-bits hashing because of severe information loss. In this paper we present a simple but effective unsupervised neural generative semantic hashing method with a focus on few-bits hashing. Our model is built upon variational autoencoder and represents each hash bit as a Bernoulli variable which allows the model to be end-to-end trainable. To address the issue of information loss we introduce a set of auxiliary implicit topic vectors. With the aid of these topic vectors the generated hash codes are not only low-dimensional representations of the original texts but also capture their implicit topics. We conduct comprehensive experiments on four datasets. The results demonstrate that our approach achieves significant improvements over state-of-the-art semantic hashing methods in few-bits hashing.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/yang2024nonlinear/">Nonlinear Robust Discrete Hashing For Cross-modal Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Nonlinear Robust Discrete Hashing For Cross-modal Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Nonlinear Robust Discrete Hashing For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Nonlinear%20Robust%20Discrete%20Hashing%20For%20Cross-modal%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Yang Zhan, Long, Zhu, Huang</td>
	<td>Arxiv</td>
	<td><p>Hashing techniques have recently been successfully applied to solve similarity search problems in the information retrieval field because of their significantly reduced storage and high-speed search capabilities. However the hash codes learned from most recent cross-modal hashing methods lack the ability to comprehensively preserve adequate information resulting in a less than desirable performance. To solve this limitation we propose a novel method termed Nonlinear Robust Discrete Hashing (NRDH) for cross-modal retrieval. The main idea behind NRDH is motivated by the success of neural networks i.e. nonlinear descriptors in the field of representation learning and the use of nonlinear descriptors instead of simple linear transformations is more in line with the complex relationships that exist between common latent representation and heterogeneous multimedia data in the real world. In NRDH we first learn a common latent representation through nonlinear descriptors to encode complementary and consistent information from the features of the heterogeneous multimedia data. Moreover an asymmetric learning scheme is proposed to correlate the learned hash codes with the common latent representation. Empirically we demonstrate that NRDH is able to successfully generate a comprehensive common latent representation that significantly improves the quality of the learned hash codes. Then NRDH adopts a linear learning strategy to fast learn the hash function with the learned hash codes. Extensive experiments performed on two benchmark datasets highlight the superiority of NRDH over several state-of-the-art methods.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/yang2024adaptive/">Adaptive Labeling For Deep Learning To Hash</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Adaptive Labeling For Deep Learning To Hash' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Adaptive Labeling For Deep Learning To Hash' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Adaptive%20Labeling%20For%20Deep%20Learning%20To%20Hash' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Yang Huei-fang, Tu, Chen</td>
	<td>Arxiv</td>
	<td><p>Hash function learning has been widely used for largescale image retrieval because of the efficiency of computation and storage. We introduce AdaLabelHash a binary hash function learning approach via deep neural networks in this paper. In AdaLabelHash class label representations are variables that are adapted during the backward network training procedure. We express the labels as hypercube vertices in a K-dimensional space and the class label representations together with the network weights are updated in the learning process. As the label representations (or referred to as codewords in this work) are learned from data semantically similar classes will be assigned with the codewords that are close to each other in terms of Hamming distance in the label space. The codewords then serve as the desired output of the hash function learning and yield compact and discriminating binary hash representations. AdaLabelHash is easy to implement which can jointly learn label representations and infer compact binary codes from data. It is applicable to both supervised and semi-supervised hash. Experimental results on standard benchmarks demonstrate the satisfactory performance of AdaLabelHash.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/yang2024distillhash/">Distillhash Unsupervised Deep Hashing By Distilling Data Pairs</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Distillhash Unsupervised Deep Hashing By Distilling Data Pairs' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Distillhash Unsupervised Deep Hashing By Distilling Data Pairs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Distillhash%20Unsupervised%20Deep%20Hashing%20By%20Distilling%20Data%20Pairs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Yang Erkun, Liu, Deng, Liu, Tao</td>
	<td>Arxiv</td>
	<td><p>Due to the high storage and search efficiency hashing has become prevalent for large-scale similarity search. Particularly deep hashing methods have greatly improved the search performance under supervised scenarios. In contrast unsupervised deep hashing models can hardly achieve satisfactory performance due to the lack of reliable supervisory similarity signals. To address this issue we propose a novel deep unsupervised hashing model dubbed DistillHash which can learn a distilled data set consisted of data pairs which have confidence similarity signals. Specifically we investigate the relationship between the initial noisy similarity signals learned from local structures and the semantic similarity labels assigned by a Bayes optimal classifier. We show that under a mild assumption some data pairs of which labels are consistent with those assigned by the Bayes optimal classifier can be potentially distilled. Inspired by this fact we design a simple yet effective strategy to distill data pairs automatically and further adopt a Bayesian learning framework to learn hash functions from the distilled data set. Extensive experimental results on three widely used benchmark datasets show that the proposed DistillHash consistently accomplishes the stateof-the-art search performance.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/yuan2024towards/">Towards Optimal Deep Hashing Via Policy Gradient</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Optimal Deep Hashing Via Policy Gradient' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Optimal Deep Hashing Via Policy Gradient' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Towards%20Optimal%20Deep%20Hashing%20Via%20Policy%20Gradient' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Yuan Xin, Ren, Lu, Zhou</td>
	<td>Arxiv</td>
	<td><p>In this paper we propose a simple yet effective relaxation free method to learn more effective binary codes via policy gradient for scalable image search. While a variety of deep hashing methods have been proposed in recent years most of them are confronted by the dilemma to obtain optimal binary codes in a truly end-to-end manner with nonsmooth sign activations. Unlike existing methods which usually employ a general relaxation framework to adapt to the gradient-based algorithms our approach formulates the non-smooth part of the hashing network as sampling with a stochastic policy so that the retrieval performance degradation caused by the relaxation can be avoided. Specifically our method directly generates the binary codes and maximizes the expectation of rewards for similarity preservation where the network can be trained directly via policy gradient. Hence the differentiation challenge for discrete optimization can be naturally addressed which leads to effective gradients and binary codes. Extensive experimental results on three benchmark datasets validate the effectiveness of the proposed method.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/yu2024deep/">Deep Graph-neighbor Coherence Preserving Network For Unsupervised Cross-modal Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Graph-neighbor Coherence Preserving Network For Unsupervised Cross-modal Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Graph-neighbor Coherence Preserving Network For Unsupervised Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Graph-neighbor%20Coherence%20Preserving%20Network%20For%20Unsupervised%20Cross-modal%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Yu Jun, Zhou, Zhan, Tao</td>
	<td>Arxiv</td>
	<td><p>Unsupervised cross-modal hashing (UCMH) has become a hot topic recently. Current UCMH focuses on exploring data similarities. However current UCMH methods calculate the similarity between two data mainly relying on the two datas cross-modal features. These methods suffer from inaccurate similarity problems that result in a suboptimal retrieval Hamming space because the cross-modal features between the data are not sufficient to describe the complex data relationships such as situations where two data have different feature representations but share the inherent concepts. In this paper we devise a deep graph-neighbor coherence preserving network (DGCPN). Specifically DGCPN stems from graph models and explores graph-neighbor coherence by consolidating the information between data and their neighbors. DGCPN regulates comprehensive similarity preserving losses by exploiting three types of data similarities (i.e. the graph-neighbor coherence the coexistent similarity and the intra- and inter-modality consistency) and designs a half-real and half-binary optimization strategy to reduce the quantization errors during hashing. Essentially DGCPN addresses the inaccurate similarity problem by exploring and exploiting the datas intrinsic relationships in a graph. We conduct extensive experiments on three public UCMH datasets. The experimental results demonstrate the superiority of DGCPN e.g. by improving the mean average precision from 0.722 to 0.751 on MIRFlickr-25K using 64-bit hashing codes to retrieval texts from images. We will release the source code package and the trained model on https://github.com/Atmegal/DGCPN.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/yuan2024central/">Central Similarity Hashing For Efficient Image And Video Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Central Similarity Hashing For Efficient Image And Video Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Central Similarity Hashing For Efficient Image And Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Central%20Similarity%20Hashing%20For%20Efficient%20Image%20And%20Video%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Yuan Li, Wang, Zhang, Jie, Tay, Feng</td>
	<td>Arxiv</td>
	<td><p>Existing data-dependent hashing methods usually learn hash functions from the pairwise or triplet data relationships which only capture the data similarity locally and often suffer low learning efficiency and low collision rate. In this work we propose a new global similarity metric termed as central similarity with which the hash codes for similar data pairs are encouraged to approach a common center and those for dissimilar pairs to converge to different centers to improve hash learning efficiency and retrieval accuracy. We principally formulate the computation of the proposed central similarity metric by introducing a new concept i.e. hash center that refers to a set of data points scattered in the Hamming space with sufficient mutual distance between each other. We then provide an efficient method to construct well separated hash centers by leveraging the Hadamard matrix and Bernoulli distributions. Finally we propose the Central Similarity Hashing (CSH) that optimizes the central similarity between data points w.r.t. their hash centers instead of optimizing the local similarity. The CSH is generic and applicable to both image and video hashing. Extensive experiments on large-scale image and video retrieval demonstrate CSH can generate cohesive hash codes for similar data pairs and dispersed hash codes for dissimilar pairs and achieve noticeable boost in retrieval performance i.e. 337;-2037; in mAP over the previous state-of-the-art. The codes are in https://github.com/yuanli2333/ Hadamard-Matrix-for-hashing</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/yu2024circulant/">Circulant Binary Embedding</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Circulant Binary Embedding' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Circulant Binary Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Circulant%20Binary%20Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Yu F., Kumar, Gong, Chang</td>
	<td>Arxiv</td>
	<td><p>Binary embedding of high-dimensional data requires long codes to preserve the discriminative power of the input space. Traditional binary coding methods often suffer from very high computation and storage costs in such a scenario. To address this problem we propose Circulant Binary Embedding (CBE) which generates binary codes by projecting the data with a circulant matrix. The circulant structure enables the use of Fast Fourier Transformation to speed up the computation. Compared to methods that use unstructured matrices the proposed method improves the time complexity from O(d^2 ) to O(d log d) and the space complexity from O(d^2) to O(d) where d is the input dimensionality. We also propose a novel time-frequency alternating optimization to learn data-dependent circulant projections which alternatively minimizes the objective in original and Fourier domains. We show by extensive experiments that the proposed approach gives much better performance than the state-of-the-art approaches for fixed time and provides much faster computation with no performance degradation for fixed number of bits.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/wang2024survey/">A Survey On Learning To Hash</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Survey On Learning To Hash' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Survey On Learning To Hash' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20Survey%20On%20Learning%20To%20Hash' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Wang Jingdong, Zhang, Song, Sebe, Shen</td>
	<td>Arxiv</td>
	<td><p>Nearest neighbor search is a problem of finding the data points from the database such that the distances from them to the query point are the smallest. Learning to hash is one of the major solutions to this problem and has been widely studied recently. In this paper we present a comprehensive survey of the learning to hash algorithms categorize them according to the manners of preserving the similarities into pairwise similarity preserving multiwise similarity preserving implicit similarity preserving as well as quantization and discuss their relations. We separate quantization from pairwise similarity preserving as the objective function is very different though quantization as we show can be derived from preserving the pairwise similarities. In addition we present the evaluation protocols and the general performance analysis and point out that the quantization algori</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/shen2024unsupervised/">Unsupervised Deep Hashing With Similarity-adaptive And Discrete Optimization</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Deep Hashing With Similarity-adaptive And Discrete Optimization' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Unsupervised Deep Hashing With Similarity-adaptive And Discrete Optimization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Unsupervised%20Deep%20Hashing%20With%20Similarity-adaptive%20And%20Discrete%20Optimization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Shen Fumin, Xu, Liu, Yang, Huang, Shen</td>
	<td>Arxiv</td>
	<td><p>Recent vision and learning studies show that learning compact hash codes can facilitate massive data processing with significantly reduced storage and computation. Particularly learning deep hash functions has greatly improved the retrieval performance typically under the semantic supervision. In contrast current unsupervised deep hashing algorithms can hardly achieve satisfactory performance due to either the relaxed optimization or absence of similarity-sensitive objective. In this work we propose a simple yet effective unsupervised hashing framework named Similarity-Adaptive Deep Hashing (SADH) which alternatingly proceeds over three training modules deep hash model training similarity graph updating and binary code optimization. The key difference from the widely-used two-step hashing method is that the output representations of the learned deep model help update the similarity graph matrix which is then used to improve the subsequent code optimization. In addition for producing high-quality binary codes we devise an effective discrete optimization algorithm which can directly handle the binary constraints with a general hashing loss. Extensive experiments validate the efficacy of SADH which consistently outperforms the state-of-the-arts by large gaps.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/chaidaroon2024variational/">Variational Deep Semantic Hashing For Text Documents</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Variational Deep Semantic Hashing For Text Documents' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Variational Deep Semantic Hashing For Text Documents' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Variational%20Deep%20Semantic%20Hashing%20For%20Text%20Documents' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Chaidaroon Suthee, Fang</td>
	<td>Arxiv</td>
	<td><p>As the amount of textual data has been rapidly increasing over the past decade efficient similarity search methods have become a crucial component of large-scale information retrieval systems. A popular strategy is to represent original data samples by compact binary codes through hashing. A spectrum of machine learning methods have been utilized but they often lack expressiveness and flexibility in modeling to learn effective representations. The recent advances of deep learning in a wide range of applications has demonstrated its capability to learn robust and powerful feature representations for complex data. Especially deep generative models naturally combine the expressiveness of probabilistic generative models with the high capacity of deep neural networks which is very suitable for text modeling. However little work has leveraged the recent progress in deep learning for text hashing. In this paper we propose a series of novel deep document generative models for text hashing. The first proposed model is unsupervised while the second one is supervised by utilizing document labels/tags for hashing. The third model further considers document-specific factors that affect the generation of words. The probabilistic generative formulation of the proposed models provides a principled framework for model extension uncertainty estimation simulation and interpretability. Based on variational inference and reparameterization the proposed models can be interpreted as encoder-decoder deep neural networks and thus they are capable of learning complex nonlinear distributed representations of the original documents. We conduct a comprehensive set of experiments on four public testbeds. The experimental results have demonstrated the effectiveness of the proposed supervised learning models for text hashing.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/chaidaroon2024deep/">Deep Semantic Text Hashing With Weak Supervision</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Semantic Text Hashing With Weak Supervision' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Semantic Text Hashing With Weak Supervision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Semantic%20Text%20Hashing%20With%20Weak%20Supervision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Chaidaroon Suthee, Ebesu, Fang</td>
	<td>Arxiv</td>
	<td><p>With an ever increasing amount of data available on the web fast similarity search has become the critical component for large-scale information retrieval systems. One solution is semantic hashing which designs binary codes to accelerate similarity search. Recently deep learning has been successfully applied to the semantic hashing problem and produces high-quality compact binary codes compared to traditional methods. However most state-of-the-art semantic hashing approaches require large amounts of hand-labeled training data which are often expensive and time consuming to collect. The cost of getting labeled data is the key bottleneck in deploying these hashing methods. Motivated by the recent success in machine learning that makes use of weak supervision we employ unsupervised ranking methods such as BM25 to extract weak signals from training data. We further introduce two deep generative semantic hashing models to leverage weak signals for text hashing. The experimental results on four public datasets show that our models can generate high-quality binary codes without using hand-labeled training data and significantly outperform the competitive unsupervised semantic hashing baselines.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/carreiraperpinan2024hashing/">Hashing With Binary Autoencoders</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Hashing With Binary Autoencoders' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Hashing With Binary Autoencoders' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Hashing%20With%20Binary%20Autoencoders' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Carreira-perpinan M., Raziperchikolaei</td>
	<td>Arxiv</td>
	<td><p>An attractive approach for fast search in image databases is binary hashing where each high-dimensional real-valued image is mapped onto a low-dimensional binary vector and the search is done in this binary space. Finding the optimal hash function is difficult because it involves binary constraints and most approaches approximate the optimization by relaxing the constraints and then binarizing the result. Here we focus on the binary autoencoder model which seeks to reconstruct an image from the binary code produced by the hash function. We show that the optimization can be simplified with the method of auxiliary coordinates. This reformulates the optimization as alternating two easier steps one that learns the encoder and decoder separately and one that optimizes the code for each image. Image retrieval experiments show the resulting hash function outperforms or is competitive with state-ofthe-art methods for binary hashing.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/chen2024long/">Long-tail Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Long-tail Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Long-tail Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Long-tail%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Chen Yong, Hou, Leng, Hu, Lin, Zhang</td>
	<td>Arxiv</td>
	<td><p>Hashing which represents data items as compact binary codes has been becoming a more and more popular technique e.g. for large-scale image retrieval owing to its super fast search speed as well as its extremely economical memory consumption. However existing hashing methods all try to learn binary codes from artificially balanced datasets which are not commonly available in real-world scenarios. In this paper we propose Long-Tail Hashing Network (LTHNet) a novel two-stage deep hashing approach that addresses the problem of learning to hash for more realistic datasets where the data labels roughly exhibit a long-tail distribution. Specifically the first stage is to learn relaxed embeddings of the given dataset with its long-tail characteristic taken into account via an end-to-end deep neural network; the second stage is to binarize those obtained embeddings. A critical part of LTHNet is its extended dynamic meta-embedding module which can adaptively realize visual knowledge transfer between head and tail classes and thus enrich image representations for hashing. Our experiments have shown that LTHNet achieves dramatic performance improvements over all state-of-the-art competitors on long-tail datasets with no or little sacrifice on balanced datasets. Further analyses reveal that while to our surprise directly manipulating class weights in the loss function has little effect the extended dynamic meta-embedding module the usage of cross-entropy loss instead of square loss and the relatively small batch-size for training all contribute to LTHNetâ€™s success.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/cao2024correlation/">Correlation Autoencoder Hashing For Supervised Cross-modal Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Correlation Autoencoder Hashing For Supervised Cross-modal Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Correlation Autoencoder Hashing For Supervised Cross-modal Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Correlation%20Autoencoder%20Hashing%20For%20Supervised%20Cross-modal%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Cao Yue, Long, Wang, Zhu</td>
	<td>Arxiv</td>
	<td><p>Due to its storage and query efficiency hashing has been widely applied to approximate nearest neighbor search from large-scale datasets. While there is increasing interest in cross-modal hashing which facilitates cross-media retrieval by embedding data from different modalities into a common Hamming space how to distill the cross-modal correlation structure effectively remains a challenging problem. In this paper we propose a novel supervised cross-modal hashing method Correlation Autoencoder Hashing (CAH) to learn discriminative and compact binary codes based on deep autoencoders. Specifically CAH jointly maximizes the feature correlation revealed by bimodal data and the semantic correlation conveyed in similarity labels while embeds them into hash codes by nonlinear deep autoencoders. Extensive experiments clearly show the superior effectiveness and efficiency of CAH against the state-of-the-art hashing methods on standard cross-modal retrieval benchmarks.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/cao2024collective/">Collective Deep Quantization For Efficient Cross-modal Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Collective Deep Quantization For Efficient Cross-modal Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Collective Deep Quantization For Efficient Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Collective%20Deep%20Quantization%20For%20Efficient%20Cross-modal%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Cao Yue, Long, Wang, Liu</td>
	<td>Arxiv</td>
	<td><p>Cross-modal similarity retrieval is a problem about designing a retrieval system that supports querying across content modalities e.g. using an image to retrieve for texts. This paper presents a compact coding solution for efficient cross-modal retrieval with a focus on the quantization approach which has already shown the superior performance over the hashing solutions in single-modal similarity retrieval. We propose a collective deep quantization (CDQ) approach which is the first attempt to introduce quantization in end-to-end deep architecture for cross-modal retrieval. The major contribution lies in jointly learning deep representations and the quantizers for both modalities using carefully-crafted hybrid networks and well-specified loss functions. In addition our approach simultaneously learns the common quantizer codebook for both modalities through which the crossmodal correlation can be substantially enhanced. CDQ enables efficient and effective cross-modal retrieval using inner product distance computed based on the common codebook with fast distance table lookup. Extensive experiments show that CDQ yields state of the art cross-modal retrieval results on standard benchmarks.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/cao2024deep/">Deep Cauchy Hashing For Hamming Space Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Cauchy Hashing For Hamming Space Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Cauchy Hashing For Hamming Space Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Cauchy%20Hashing%20For%20Hamming%20Space%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Cao Yue, Long, Liu, Wang</td>
	<td>Arxiv</td>
	<td><p>Due to its computation efficiency and retrieval quality hashing has been widely applied to approximate nearest neighbor search for large-scale image retrieval while deep hashing further improves the retrieval quality by end-toend representation learning and hash coding. With compact hash codes Hamming space retrieval enables the most efficient constant-time search that returns data points within a given Hamming radius to each query by hash table lookups instead of linear scan. However subject to the weak capability of concentrating relevant images to be within a small Hamming ball due to mis-specified loss functions existing deep hashing methods may underperform for Hamming space retrieval. This work presents Deep Cauchy Hashing (DCH) a novel deep hashing model that generates compact and concentrated binary hash codes to enable efficient and effective Hamming space retrieval. The main idea is to design a pairwise cross-entropy loss based on Cauchy distribution which penalizes significantly on similar image pairs with Hamming distance larger than the given Hamming radius threshold. Comprehensive experiments demonstrate that DCH can generate highly concentrated hash codes and yield state-of-the-art Hamming space retrieval performance on three datasets NUS-WIDE CIFAR-10 and MS-COCO.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/cakir2024hashing/">Hashing With Binary Matrix Pursuit</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Hashing With Binary Matrix Pursuit' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Hashing With Binary Matrix Pursuit' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Hashing%20With%20Binary%20Matrix%20Pursuit' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Cakir F., He, Sclaroff</td>
	<td>Arxiv</td>
	<td><p>We propose theoretical and empirical improvements for two-stage hashing methods. We first provide a theoretical analysis on the quality of the binary codes and show that under mild assumptions a residual learning scheme can construct binary codes that fit any neighborhood structure with arbitrary accuracy. Secondly we show that with high-capacity hash functions such as CNNs binary code inference can be greatly simplified for many standard neighborhood definitions yielding smaller optimization problems and more robust codes. Incorporating our findings we propose a novel two-stage hashing method that significantly outperforms previous hashing studies on widely used image retrieval benchmarks.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/cakir2024adaptive/">Adaptive Hashing For Fast Similarity Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Adaptive Hashing For Fast Similarity Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Adaptive Hashing For Fast Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Adaptive%20Hashing%20For%20Fast%20Similarity%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Cakir F., Sclaroff</td>
	<td>Arxiv</td>
	<td><p>With the staggering growth in image and video datasets algorithms that provide fast similarity search and compact storage are crucial. Hashing methods that map the data into Hamming space have shown promise; however many of these methods employ a batch-learning strategy in which the computational cost and memory requirements may become intractable and infeasible with larger and larger datasets. To overcome these challenges we propose an online learning algorithm based on stochastic gradient descent in which the hash functions are updated iteratively with streaming data. In experiments with three image retrieval benchmarks our online algorithm attains retrieval accuracy that is comparable to competing state-of-the-art batch-learning solutions while our formulation is orders of magnitude faster and being online it is adaptable to the variations of the data. Moreover our formulation yields improved retrieval performance over a recently reported online hashing technique Online Kernel Hashing.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/cakir2024mihash/">Mihash Online Hashing With Mutual Information</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Mihash Online Hashing With Mutual Information' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Mihash Online Hashing With Mutual Information' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Mihash%20Online%20Hashing%20With%20Mutual%20Information' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Cakir F., He, Bargal, Sclaroff</td>
	<td>Arxiv</td>
	<td><p>Learning-based hashing methods are widely used for nearest neighbor retrieval and recently online hashing methods have demonstrated good performance-complexity trade-offs by learning hash functions from streaming data. In this paper we first address a key challenge for online hashing the binary codes for indexed data must be recomputed to keep pace with updates to the hash functions. We propose an efficient quality measure for hash functions based on an information-theoretic quantity mutual information and use it successfully as a criterion to eliminate unnecessary hash table updates. Next we also show how to optimize the mutual information objective using stochastic gradient descent. We thus develop a novel hashing method MIHash that can be used in both online and batch settings. Experiments on image retrieval benchmarks (including a 2.5M image dataset) confirm the effectiveness of our formulation both in reducing hash table recomputations and in learning high-quality hash functions.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/cao2024hashgan/">Hashgan Deep Learning To Hash With Pair Conditional Wasserstein GAN</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Hashgan Deep Learning To Hash With Pair Conditional Wasserstein GAN' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Hashgan Deep Learning To Hash With Pair Conditional Wasserstein GAN' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Hashgan%20Deep%20Learning%20To%20Hash%20With%20Pair%20Conditional%20Wasserstein%20GAN' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Cao Yue, Long, Liu, Wang</td>
	<td>Arxiv</td>
	<td><p>Deep learning to hash improves image retrieval performance by end-to-end representation learning and hash coding from training data with pairwise similarity information. Subject to the scarcity of similarity information that is often expensive to collect for many application domains existing deep learning to hash methods may overfit the training data and result in substantial loss of retrieval quality. This paper presents HashGAN a novel architecture for deep learning to hash which learns compact binary hash codes from both real images and diverse images synthesized by generative models. The main idea is to augment the training data with nearly real images synthesized from a new Pair Conditional Wasserstein GAN (PC-WGAN) conditioned on the pairwise similarity information. Extensive experiments demonstrate that HashGAN can generate high-quality binary hash codes and yield state-of-the-art image retrieval performance on three benchmarks NUS-WIDE CIFAR-10 and MS-COCO.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/broder2024min/">Min-wise Independent Permutations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Min-wise Independent Permutations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Min-wise Independent Permutations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Min-wise%20Independent%20Permutations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Broder Andrei, Charikar Moses, Frieze Alan, Mitzenmacher Michael</td>
	<td>Arxiv</td>
	<td><p>We define and study the notion of min-wise independent families of permutations. Our research was motivated by the fact that such a family (under some relaxations) is essential to the algorithm used in practice by the AltaVista web index software to detect and filter near-duplicate documents. However in the course of our investigation we have discovered interesting and challenging theoretical questions related to this concept we present the solutions to some of them and we list the rest as open problems.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/berriche2024leveraging/">Leveraging High-resolution Features For Improved Deep Hashing-based Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Leveraging High-resolution Features For Improved Deep Hashing-based Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Leveraging High-resolution Features For Improved Deep Hashing-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Leveraging%20High-resolution%20Features%20For%20Improved%20Deep%20Hashing-based%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Berriche Aymene, Zakaria Mehdi Adjal, Baghdadi Riyadh</td>
	<td>Arxiv</td>
	<td><p>Deep hashing techniques have emerged as the predominant approach for efficient image retrieval. Traditionally these methods utilize pre-trained convolutional neural networks (CNNs) such as AlexNet and VGG-16 as feature extractors. However the increasing complexity of datasets poses challenges for these backbone architectures in capturing meaningful features essential for effective image retrieval. In this study we explore the efficacy of employing high-resolution features learned through state-of-the-art techniques for image retrieval tasks. Specifically we propose a novel methodology that utilizes High-Resolution Networks (HRNets) as the backbone for the deep hashing task termed High-Resolution Hashing Network (HHNet). Our approach demonstrates superior performance compared to existing methods across all tested benchmark datasets including CIFAR-10 NUS-WIDE MS COCO and ImageNet. This performance improvement is more pronounced for complex datasets which highlights the need to learn high-resolution features for intricate image retrieval tasks. Furthermore we conduct a comprehensive analysis of different HRNet configurations and provide insights into the optimal architecture for the deep hashing task</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/bawa2024lsh/">LSH Forest Self-tuning Indexes For Similarity Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=LSH Forest Self-tuning Indexes For Similarity Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=LSH Forest Self-tuning Indexes For Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=LSH%20Forest%20Self-tuning%20Indexes%20For%20Similarity%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Bawa M., Condie, Ganesan</td>
	<td>Arxiv</td>
	<td><p>We consider the problem of indexing high-dimensional data for answering (approximate) similarity-search queries. Similarity indexes prove to be important in a wide variety of settings Web search engines desire fast parallel main-memory-based indexes for similarity search on text data; database systems desire disk-based similarity indexes for high-dimensional data including text and images; peer-to-peer systems desire distributed similarity indexes with low communication cost. We propose an indexing scheme called LSH Forest which is applicable in all the above contexts. Our index uses the well-known technique of locality-sensitive hashing (LSH) but improves upon previous designs by (a) eliminating the different data-dependent parameters for which LSH must be constantly hand-tuned and (b) improving on LSHâ€™s performance guarantees for skewed data distributions while retaining the same storage and query overhead. We show how to construct this index in main memory on disk in parallel systems and in peer-to-peer systems. We evaluate the design with experiments on multiple text corpora and demonstrate both the self-tuning nature and the superior performance of LSH Forest.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/bai2024targeted/">Targeted Attack For Deep Hashing Based Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Targeted Attack For Deep Hashing Based Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Targeted Attack For Deep Hashing Based Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Targeted%20Attack%20For%20Deep%20Hashing%20Based%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Bai Jiawang, Chen, Li, Wu, Guo, Xia, Yang</td>
	<td>Arxiv</td>
	<td><p>The deep hashing based retrieval method is widely adopted in large-scale image and video retrieval. However there is little investigation on its security. In this paper we propose a novel method dubbed deep hashing targeted attack (DHTA) to study the targeted attack on such retrieval. Specifically we first formulate the targeted attack as a point-to-set optimization which minimizes the average distance between the hash code of an adversarial example and those of a set of objects with the target label. Then we design a novel component-voting scheme to obtain an anchor code as the representative of the set of hash codes of objects with the target label whose optimality guarantee is also theoretically derived. To balance the performance and perceptibility we propose to minimize the Hamming distance between the hash code of the adversarial example and the anchor code under the â„“âˆž restriction on the perturbation. Extensive experiments verify that DHTA is effective in attacking both deep hashing based image retrieval and video retrieval.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/andoni2024practical/">Practical And Optimal LSH For Angular Distance</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Practical And Optimal LSH For Angular Distance' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Practical And Optimal LSH For Angular Distance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Practical%20And%20Optimal%20LSH%20For%20Angular%20Distance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Andoni A., Indyk, Laarhoven</td>
	<td>Arxiv</td>
	<td><p>We show the existence of a Locality-Sensitive Hashing (LSH) family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent. Unlike earlier algorithms with this property (e.g. Spherical LSH 1 2) our algorithm is also practical improving upon the well-studied hyperplane LSH 3 in practice. We also introduce a multiprobe version of this algorithm and conduct an experimental evaluation on real and synthetic data sets. We complement the above positive results with a fine-grained lower bound for the quality of any LSH family for angular distance. Our lower bound implies that the above LSH family exhibits a trade-off between evaluation time and quality that is close to optimal for a natural class of LSH functions.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/andoni2024learning/">Learning To Hash Robustly With Guarantees</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning To Hash Robustly With Guarantees' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning To Hash Robustly With Guarantees' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20To%20Hash%20Robustly%20With%20Guarantees' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Andoni Alexandr, Beaglehole</td>
	<td>Arxiv</td>
	<td><p>The indexing algorithms for the high-dimensional nearest neighbor search (NNS) with the best worst-case guarantees are based on the randomized Locality Sensitive Hashing (LSH) and its derivatives. In practice many heuristic approaches exist to learn the best indexing method in order to speed-up NNS crucially adapting to the structure of the given dataset. Oftentimes these heuristics outperform the LSH-based algorithms on real datasets but almost always come at the cost of losing the guarantees of either correctness or robust performance on adversarial queries or apply to datasets with an assumed extra structure/model. In this paper we design an NNS algorithm for the Hamming space that has worst-case guarantees essentially matching that of theoretical algorithms while optimizing the hashing to the structure of the dataset (think instance-optimal algorithms) for performance on the minimum-performing query. We evaluate the algorithms ability to optimize for a given dataset both theoretically and practically. On the theoretical side we exhibit a natural setting (dataset model) where our algorithm is much better than the standard theoretical one. On the practical side we run experiments that show that our algorithm has a 1.8x and 2.1x better recall on the worst-performing queries to the MNIST and ImageNet datasets.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/2024scratch/">SCRATCH A Scalable Discrete Matrix Factorization Hashing For Cross-modal Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=SCRATCH A Scalable Discrete Matrix Factorization Hashing For Cross-modal Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=SCRATCH A Scalable Discrete Matrix Factorization Hashing For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=SCRATCH%20A%20Scalable%20Discrete%20Matrix%20Factorization%20Hashing%20For%20Cross-modal%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Chuan-xiang, Chen, Zhang, Luo, Nie, Zhang, Xu</td>
	<td>Arxiv</td>
	<td><p>In recent years many hashing methods have been proposed for the cross-modal retrieval task. However there are still some issues that need to be further explored. For example some of them relax the binary constraints to generate the hash codes which may generate large quantization error. Although some discrete schemes have been proposed most of them are time-consuming. In addition most of the existing supervised hashing methods use an n x n similarity matrix during the optimization making them unscalable. To address these issues in this paper we present a novel supervised cross-modal hashing methodâ€”Scalable disCRete mATrix faCtorization Hashing SCRATCH for short. It leverages the collective matrix factorization on the kernelized features and the semantic embedding with labels to find a latent semantic space to preserve the intra- and inter-modality similarities. In addition it incorporates the label matrix instead of the similarity matrix into the loss function. Based on the proposed loss function and the iterative optimization algorithm it can learn the hash functions and binary codes simultaneously. Moreover the binary codes can be generated discretely reducing the quantization error generated by the relaxation scheme. Its time complexity is linear to the size of the dataset making it scalable to large-scale datasets. Extensive experiments on three benchmark datasets namely Wiki MIRFlickr-25K and NUS-WIDE have verified that our proposed SCRATCH model outperforms several state-of-the-art unsupervised and supervised hashing methods for cross-modal retrieval.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/andoni2024near/">Near-optimal Hashing Algorithms For Approximate Nearest Neighbor In High Dimensions</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Near-optimal Hashing Algorithms For Approximate Nearest Neighbor In High Dimensions' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Near-optimal Hashing Algorithms For Approximate Nearest Neighbor In High Dimensions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Near-optimal%20Hashing%20Algorithms%20For%20Approximate%20Nearest%20Neighbor%20In%20High%20Dimensions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Andoni A., Indyk</td>
	<td>Arxiv</td>
	<td><p>We present an algorithm for the c-approximate nearest neighbor problem in a d-dimensional Euclidean space achieving query time of O(dn 1c2/+o(1)) and space O(dn + n1+1c2/+o(1)). This almost matches the lower bound for hashing-based algorithm recently obtained in (R. Motwani et al. 2006). We also obtain a space-efficient version of the algorithm which uses dn+n logO(1) n space with a query time of dnO(1/c2). Finally we discuss practical variants of the algorithms that utilize fast bounded-distance decoders for the Leech lattice</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/cao2024hashnet/">Hashnet Deep Learning To Hash By Continuation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Hashnet Deep Learning To Hash By Continuation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Hashnet Deep Learning To Hash By Continuation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Hashnet%20Deep%20Learning%20To%20Hash%20By%20Continuation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Cao Zhangjie, Long, Wang, Yu</td>
	<td>Arxiv</td>
	<td><p>Learning to hash has been widely applied to approximate nearest neighbor search for large-scale multimedia retrieval due to its computation efficiency and retrieval quality. Deep learning to hash which improves retrieval quality by end-to-end representation learning and hash encoding has received increasing attention recently. Subject to the illposed gradient difficulty in the optimization with sign activations existing deep learning to hash methods need to first learn continuous representations and then generate binary hash codes in a separated binarization step which suffer from substantial loss of retrieval quality. This work presents HashNet a novel deep architecture for deep learning to hash by continuation method with convergence guarantees which learns exactly binary hash codes from imbalanced similarity data. The key idea is to attack the ill-posed gradient problem in optimizing deep networks with non-smooth binary activations by continuation method in which we begin from learning an easier network with smoothed activation function and let it evolve during the training until it eventually goes back to being the original difficult to optimize deep network with the sign activation function. Comprehensive empirical evidence shows that HashNet can generate exactly binary hash codes and yield state-of-the-art multimedia retrieval performance on standard benchmarks.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/he2024hashing/">Hashing As Tie-aware Learning To Rank</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Hashing As Tie-aware Learning To Rank' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Hashing As Tie-aware Learning To Rank' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Hashing%20As%20Tie-aware%20Learning%20To%20Rank' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>He K., Cakir, Bargal, Sclaroff</td>
	<td>Arxiv</td>
	<td><p>Hashing or learning binary embeddings of data is frequently used in nearest neighbor retrieval. In this paper we develop learning to rank formulations for hashing aimed at directly optimizing ranking-based evaluation metrics such as Average Precision (AP) and Normalized Discounted Cumulative Gain (NDCG). We first observe that the integer-valued Hamming distance often leads to tied rankings and propose to use tie-aware versions of AP and NDCG to evaluate hashing for retrieval. Then to optimize tie-aware ranking metrics we derive their continuous relaxations and perform gradient-based optimization with deep neural networks. Our results establish the new state-of-the-art for image retrieval by Hamming ranking in common benchmarks.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/krizhevsky2024learning/">Learning Multiple Layers Of Features From Tiny Images</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning Multiple Layers Of Features From Tiny Images' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning Multiple Layers Of Features From Tiny Images' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20Multiple%20Layers%20Of%20Features%20From%20Tiny%20Images' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Krizhevsky A.</td>
	<td>Arxiv</td>
	<td><p>Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is in principle an excellent dataset for unsupervised training of deep generative models but previous researchers who have tried this have found it difficult to learn a good set of filters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels we show that object recognition is significantly improved by pre-training a layer of features on a large set of unlabeled tiny images.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/moran2024variable/">Variable Bit Quantisation For LSH</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Variable Bit Quantisation For LSH' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Variable Bit Quantisation For LSH' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Variable%20Bit%20Quantisation%20For%20LSH' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Moran S., Lavrenko, Osborne</td>
	<td>Arxiv</td>
	<td><p>We introduce a scheme for optimally allocating a variable number of bits per LSH hyperplane. Previous approaches assign a constant number of bits per hyperplane. This neglects the fact that a subset of hyperplanes may be more informative than others. Our method dubbed Variable Bit Quantisation (VBQ) provides a datadriven non-uniform bit allocation across hyperplanes. Despite only using a fraction of the available hyperplanes VBQ outperforms uniform quantisation by up to 16837; for retrieval across standard text and image datasets.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/zhao2024deep/">Deep Semantic Ranking Based Hashing For Multi-label Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Semantic Ranking Based Hashing For Multi-label Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Semantic Ranking Based Hashing For Multi-label Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Semantic%20Ranking%20Based%20Hashing%20For%20Multi-label%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhao F., Huang, Wang, Tan</td>
	<td>Arxiv</td>
	<td><p>With the rapid growth of web images hashing has received increasing interests in large scale image retrieval. Research efforts have been devoted to learning compact binary codes that preserve semantic similarity based on labels. However most of these hashing methods are designed to handle simple binary similarity. The complex multilevel semantic structure of images associated with multiple labels have not yet been well explored. Here we propose a deep semantic ranking based method for learning hash functions that preserve multilevel semantic similarity between multilabel images. In our approach deep convolutional neural network is incorporated into hash functions to jointly learn feature representations and mappings from them to hash codes which avoids the limitation of semantic representation power of hand-crafted features. Meanwhile a ranking list that encodes the multilevel similarity information is employed to guide the learning of such deep hash functions. An effective scheme based on surrogate loss is used to solve the intractable optimization problem of nonsmooth and multivariate ranking measures involved in the learning procedure. Experimental results show the superiority of our proposed approach over several state-of-theart hashing methods in term of ranking evaluation metrics when tested on multi-label image datasets.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/zhang2024self/">Self-taught Hashing For Fast Similarity Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Self-taught Hashing For Fast Similarity Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Self-taught Hashing For Fast Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Self-taught%20Hashing%20For%20Fast%20Similarity%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhang D., Wang, Cai, Lu</td>
	<td>Arxiv</td>
	<td><p>The ability of fast similarity search at large scale is of great importance to many Information Retrieval (IR) applications. A promising way to accelerate similarity search is semantic hashing which designs compact binary codes for a large number of documents so that semantically similar documents are mapped to similar codes (within a short Hamming distance). Although some recently proposed techniques are able to generate high-quality codes for documents known in advance obtaining the codes for previously unseen documents remains to be a very challenging problem. In this paper we emphasise this issue and propose a novel SelfTaught Hashing (STH) approach to semantic hashing we first find the optimal l-bit binary codes for all documents in the given corpus via unsupervised learning and then train l classifiers via supervised learning to predict the l-bit code for any query document unseen before. Our experiments on three real-world text datasets show that the proposed approach using binarised Laplacian Eigenmap (LapEig) and linear Support Vector Machine (SVM) outperforms stateof-the-art techniques significantly.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/zhang2024high/">High-order Nonlocal Hashing For Unsupervised Cross-modal Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=High-order Nonlocal Hashing For Unsupervised Cross-modal Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=High-order Nonlocal Hashing For Unsupervised Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=High-order%20Nonlocal%20Hashing%20For%20Unsupervised%20Cross-modal%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhang Peng-fei, Luo, Huang, Xu, Song</td>
	<td>Arxiv</td>
	<td><p>In light of the ability to enable efficient storage and fast query for big data hashing techniques for cross-modal search have aroused extensive attention. Despite the great success achieved unsupervised cross-modal hashing still suffers from lacking reliable similarity supervision and struggles with handling the heterogeneity issue between different modalities. To cope with these in this paper we devise a new deep hashing model termed as High-order Nonlocal Hashing (HNH) to facilitate cross-modal retrieval with the following advantages. First different from existing methods that mainly leverage low-level local-view similarity as the guidance for hashing learning we propose a high-order affinity measure that considers the multi-modal neighbourhood structures from a nonlocal perspective thereby comprehensively capturing the similarity relationships between data items. Second a common representation is introduced to correlate different modalities. By enforcing the modal-specific descriptors and the common representation to be aligned with each other the proposed HNH significantly bridges the modality gap and maintains the intra-consistency. Third an effective affinity preserving objective function is delicately designed to generate high-quality binary codes. Extensive experiments evidence the superiority of the proposed HNH in unsupervised cross-modal retrieval tasks over the state-of-the-art baselines.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/zhang2024hierarchical/">Hierarchical Deep Hashing For Fast Large Scale Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Hierarchical Deep Hashing For Fast Large Scale Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Hierarchical Deep Hashing For Fast Large Scale Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Hierarchical%20Deep%20Hashing%20For%20Fast%20Large%20Scale%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhang Yongfei, Peng, Jingtao, Liu, Pu, Chen</td>
	<td>Arxiv</td>
	<td><p>Fast image retrieval is of great importance in many computer vision tasks and especially practical applications. Deep hashing the state-of-the-art fast image retrieval scheme introduces deep learning to learn the hash functions and generate binary hash codes and outperforms the other image retrieval methods in terms of accuracy. However all the existing deep hashing methods could only generate one level hash codes and require a linear traversal of all the hash codes to figure out the closest one when a new query arrives which is very time-consuming and even intractable for large scale applications. In this work we propose a Hierarchical Deep Hashing(HDHash) scheme to speed up the state-of-the-art deep hashing methods. More specifically hierarchical deep hash codes of multiple levels can be generated and indexed with tree structures rather than linear ones and pruning irrelevant branches can sharply decrease the retrieval time. To our best knowledge this is the first work to introduce hierarchical indexed deep hashing for fast large scale image retrieval. Extensive experimental results on three benchmark datasets demonstrate that the proposed HDHash scheme achieves better or comparable accuracy with significantly improved efficiency and reduced memory as compared to state-of- the-art fast image retrieval schemes.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/zhang2024large/">Large-scale Supervised Multimodal Hashing With Semantic Correlation Maximization</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Large-scale Supervised Multimodal Hashing With Semantic Correlation Maximization' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Large-scale Supervised Multimodal Hashing With Semantic Correlation Maximization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Large-scale%20Supervised%20Multimodal%20Hashing%20With%20Semantic%20Correlation%20Maximization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhang D., Li</td>
	<td>Arxiv</td>
	<td><p>Due to its low storage cost and fast query speed hashing has been widely adopted for similarity search in multimedia data. In particular more and more attentions have been payed to multimodal hashing for search in multimedia data with multiple modalities such as images with tags. Typically supervised information of semantic labels is also available for the data points in many real applications. Hence many supervised multimodal hashing (SMH) methods have been proposed to utilize such semantic labels to further improve the search accuracy. However the training time complexity of most existing SMH methods is too high which makes them unscalable to large-scale datasets. In this paper a novel SMH method called semantic correlation maximization (SCM) is proposed to seamlessly integrate semantic labels into the hashing learning procedure for large-scale data modeling. Experimental results on two real-world datasets show that SCM can signifi- cantly outperform the state-of-the-art SMH methods in terms of both accuracy and scalability.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/zhang2024supervised/">Supervised Hashing With Latent Factor Models</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Hashing With Latent Factor Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Supervised Hashing With Latent Factor Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Supervised%20Hashing%20With%20Latent%20Factor%20Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhang P., Zhang, Li, Guo</td>
	<td>Arxiv</td>
	<td><p>Due to its low storage cost and fast query speed hashing has been widely adopted for approximate nearest neighbor search in large-scale datasets. Traditional hashing methods try to learn the hash codes in an unsupervised way where the metric (Euclidean) structure of the training data is preserved. Very recently supervised hashing methods which try to preserve the semantic structure constructed from the semantic labels of the training points have exhibited higher accuracy than unsupervised methods. In this paper we propose a novel supervised hashing method called latent factor hashing (LFH) to learn similarity-preserving binary codes based on latent factor models. An algorithm with convergence guarantee is proposed to learn the parameters of LFH. Furthermore a linear-time variant with stochastic learning is proposed for training LFH on large-scale datasets. Experimental results on two large datasets with semantic labels show that LFH can achieve superior accuracy than state-of-the-art methods with comparable training time.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/zhang2024deep/">Deep Center-based Dual-constrained Hashing For Discriminative Face Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Center-based Dual-constrained Hashing For Discriminative Face Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Center-based Dual-constrained Hashing For Discriminative Face Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Center-based%20Dual-constrained%20Hashing%20For%20Discriminative%20Face%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhang Ming, Zhe, Yan</td>
	<td>Arxiv</td>
	<td><p>With the advantages of low storage cost and extremely fast retrieval speed deep hashing methods have attracted much attention for image retrieval recently. However large-scale face image retrieval with significant intra-class variations is still challenging. Neither existing pairwise/triplet labels-based nor softmax classification loss-based deep hashing works can generate compact and discriminative binary codes. Considering these issues we propose a center-based framework integrating end-to-end hashing learning and class centers learning simultaneously. The framework minimizes the intra-class variance by clustering intra-class samples into a learnable class center. To strengthen inter-class separability it additionally imposes a novel regularization term to enlarge the Hamming distance between pairwise class centers. Moreover a simple yet effective regression matrix is introduced to encourage intra-class samples to generate the same binary codes which further enhances the hashing codes compactness. Experiments on four large-scale datasets show the proposed method outperforms state-of-the-art baselines under various code lengths and commonly-used evaluation metrics.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/zhang2024bit/">Bit-scalable Deep Hashing With Regularized Similarity Learning For Image Retrieval And Person Re-identification</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Bit-scalable Deep Hashing With Regularized Similarity Learning For Image Retrieval And Person Re-identification' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Bit-scalable Deep Hashing With Regularized Similarity Learning For Image Retrieval And Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Bit-scalable%20Deep%20Hashing%20With%20Regularized%20Similarity%20Learning%20For%20Image%20Retrieval%20And%20Person%20Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhang R., Lin, Zhang, Zuo, Zhang</td>
	<td>Arxiv</td>
	<td><p>Extracting informative image features and learning effective approximate hashing functions are two crucial steps in image retrieval . Conventional methods often study these two steps separately e.g. learning hash functions from a predefined hand-crafted feature space. Meanwhile the bit lengths of output hashing codes are preset in most previous methods neglecting the significance level of different bits and restricting their practical flexibility. To address these issues we propose a supervised learning framework to generate compact and bit-scalable hashing codes directly from raw images. We pose hashing learning as a problem of regularized similarity learning. Specifically we organize the training images into a batch of triplet samples each sample containing two images with the same label and one with a different label. With these triplet samples we maximize the margin between matched pairs and mismatched pairs in the Hamming space. In addition a regularization term is introduced to enforce the adjacency consistency i.e. images of similar appearances should have similar codes. The deep convolutional neural network is utilized to train the model in an end-to-end fashion where discriminative image features and hash functions are simultaneously optimized. Furthermore each bit of our hashing codes is unequally weighted so that we can manipulate the code lengths by truncating the insignificant bits. Our framework outperforms state-of-the-arts on public benchmarks of similar image search and also achieves promising results in the application of person re-identification in surveillance. It is also shown that the generated bit-scalable hashing codes well preserve the discriminative powers with shorter code lengths.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/zhang2024binary/">Binary Code Ranking With Weighted Hamming Distance</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Binary Code Ranking With Weighted Hamming Distance' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Binary Code Ranking With Weighted Hamming Distance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Binary%20Code%20Ranking%20With%20Weighted%20Hamming%20Distance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhang Lei, Zhang, Tang, Lu, Tian</td>
	<td>Arxiv</td>
	<td><p>Binary hashing has been widely used for efficient similarity search due to its query and storage efficiency. In most existing binary hashing methods the high-dimensional data are embedded into Hamming space and the distance or similarity of two points are approximated by the Hamming distance between their binary codes. The Hamming distance calculation is efficient however in practice there are often lots of results sharing the same Hamming distance to a query which makes this distance measure ambiguous and poses a critical issue for similarity search where ranking is important. In this paper we propose a weighted Hamming distance ranking algorithm (WhRank) to rank the binary codes of hashing methods. By assigning different bit-level weights to different hash bits the returned binary codes are ranked at a finer-grained binary code level. We give an algorithm to learn the data-adaptive and query-sensitive weight for each hash bit. Evaluations on two large-scale image data sets demonstrate the efficacy of our weighted Hamming distance for binary code ranking.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/zhang2024composite/">Composite Hashing With Multiple Information Sources</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Composite Hashing With Multiple Information Sources' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Composite Hashing With Multiple Information Sources' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Composite%20Hashing%20With%20Multiple%20Information%20Sources' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhang D., Wang, Si</td>
	<td>Arxiv</td>
	<td><p>Similarity search applications with a large amount of text and image data demands an efficient and effective solution. One useful strategy is to represent the examples in databases as compact binary codes through semantic hashing which has attracted much attention due to its fast query/search speed and drastically reduced storage requirement. All of the current semantic hashing methods only deal with the case when each example is represented by one type of features. However examples are often described from several different information sources in many real world applications. For example the characteristics of a webpage can be derived from both its content part and its associated links. To address the problem of learning good hashing codes in this scenario we propose a novel research problem â€“ Composite Hashing with Multiple Information Sources (CHMIS). The focus of the new research problem is to design an algorithm for incorporating the features from different information sources into the binary hashing codes efficiently and effectively. In particular we propose an algorithm CHMISAW (CHMIS with Adjusted Weights) for learning the codes. The proposed algorithm integrates information from several different sources into the binary hashing codes by adjusting the weights on each individual source for maximizing the coding performance and enables fast conversion from query examples to their binary hashing codes. Experimental results on five different datasets demonstrate the superior performance of the proposed method against several other state-of-the-art semantic hashing techniques.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/zhang2024efficient/">Efficient Training Of Very Deep Neural Networks For Supervised Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Training Of Very Deep Neural Networks For Supervised Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Efficient Training Of Very Deep Neural Networks For Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Efficient%20Training%20Of%20Very%20Deep%20Neural%20Networks%20For%20Supervised%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhang Ziming, Chen, Saligrama</td>
	<td>Arxiv</td>
	<td><p>In this paper we propose training very deep neural networks (DNNs) for supervised learning of hash codes. Existing methods in this context train relatively shallow networks limited by the issues arising in back propagation (e.e. vanishing gradients) as well as computational efficiency. We propose a novel and efficient training algorithm inspired by alternating direction method of multipliers (ADMM) that overcomes some of these limitations. Our method decomposes the training process into independent layer-wise local updates through auxiliary variables. Empirically we observe that our training algorithm always converges and its computational complexity is linearly proportional to the number of edges in the networks. Empirically we manage to train DNNs with 64 hidden layers and 1024 nodes per layer for supervised hashing in about 3 hours using a single GPU. Our proposed very deep supervised hashing (VDSH) method significantly outperforms the state-of-the-art on several benchmark datasets.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/zhang2024fast/">Fast Discrete Cross-modal Hashing Based On Label Relaxation And Matrix Factorization</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Fast Discrete Cross-modal Hashing Based On Label Relaxation And Matrix Factorization' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Fast Discrete Cross-modal Hashing Based On Label Relaxation And Matrix Factorization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Fast%20Discrete%20Cross-modal%20Hashing%20Based%20On%20Label%20Relaxation%20And%20Matrix%20Factorization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhang Donglin, Wu, Liu, Yu, Kittler</td>
	<td>Arxiv</td>
	<td><p>In recent years cross-media retrieval has drawn considerable attention due to the exponential growth of multimedia data. Many hashing approaches have been proposed for the cross-media search task. However there are still open problems that warrant investigation. For example most existing supervised hashing approaches employ a binary label matrix which achieves small margins between wrong labels (0) and true labels (1). This may affect the retrieval performance by generating many false negatives and false positives. In addition some methods adopt a relaxation scheme to solve the binary constraints which may cause large quantization errors. There are also some discrete hashing methods that have been presented but most of them are time-consuming. To conquer these problems we present a label relaxation and discrete matrix factorization method (LRMF) for cross-modal retrieval. It offers a number of innovations. First of all the proposed approach employs a novel label relaxation scheme to control the margins adaptively which has the benefit of reducing the quantization error. Second by virtue of the proposed discrete matrix factorization method designed to learn the binary codes large quantization errors caused by relaxation can be avoided. The experimental results obtained on two widely-used databases demonstrate that LRMF outperforms state-of-the-art cross-media methods.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/zhen2024co/">Co-regularized Hashing For Multimodal Data</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Co-regularized Hashing For Multimodal Data' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Co-regularized Hashing For Multimodal Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Co-regularized%20Hashing%20For%20Multimodal%20Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhen Y., Yeung</td>
	<td>Arxiv</td>
	<td><p>Hashing-based methods provide a very promising approach to large-scale similarity search. To obtain compact hash codes a recent trend seeks to learn the hash functions from data automatically. In this paper we study hash function learning in the context of multimodal data. We propose a novel multimodal hash function learning method called Co-Regularized Hashing (CRH) based on a boosted coregularization framework. The hash functions for each bit of the hash codes are learned by solving DC (difference of convex functions) programs while the learning for multiple bits proceeds via a boosting procedure so that the bias introduced by the hash functions can be sequentially minimized. We empirically compare CRH with two state-of-the-art multimodal hash function learning methods on two publicly available data sets.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/zhen2024cross/">Cross-modal Similarity Learning Via Pairs Preferences And Active Supervision</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Cross-modal Similarity Learning Via Pairs Preferences And Active Supervision' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Cross-modal Similarity Learning Via Pairs Preferences And Active Supervision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Cross-modal%20Similarity%20Learning%20Via%20Pairs%20Preferences%20And%20Active%20Supervision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhen Yi, Rai, Zha, Carin</td>
	<td>Arxiv</td>
	<td><p>We present a probabilistic framework for learning pairwise similarities between objects belonging to different modalities such as drugs and proteins or text and images. Our framework is based on learning a binary code based representation for objects in each modality and has the following key properties (i) it can leverage both pairwise as well as easy-to-obtain relative preference based cross-modal constraints (ii) the probabilistic framework naturally allows querying for the most useful/informative constraints facilitating an active learning setting (existing methods for cross-modal similarity learning do not have such a mechanism) and (iii) the binary code length is learned from the data. We demonstrate the effectiveness of the proposed approach on two problems that require computing pairwise similarities between cross-modal object pairs cross-modal link prediction in bipartite graphs and hashing based cross-modal similarity search.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/zhu2024deep/">Deep Hashing Network For Efficient Similarity Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Hashing Network For Efficient Similarity Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Hashing Network For Efficient Similarity Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Hashing%20Network%20For%20Efficient%20Similarity%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhu Han, Long, Wang, Cao</td>
	<td>Arxiv</td>
	<td><p>Due to the storage and retrieval efficiency hashing has been widely deployed to approximate nearest neighbor search for large-scale multimedia retrieval. Supervised hashing which improves the quality of hash coding by exploiting the semantic similarity on data pairs has received increasing attention recently. For most existing supervised hashing methods for image retrieval an image is first represented as a vector of hand-crafted or machine-learned features followed by another separate quantization step that generates binary codes. However suboptimal hash coding may be produced because the quantization error is not statistically minimized and the feature representation is not optimally compatible with the binary coding. In this paper we propose a novel Deep Hashing Network (DHN) architecture for supervised hashing in which we jointly learn good image representation tailored to hash coding and formally control the quantization error. The DHN model constitutes four key components (1) a sub-network with multiple convolution-pooling layers to capture image representations; (2) a fully-connected hashing layer to generate compact binary hash codes; (3) a pairwise cross-entropy loss layer for similarity-preserving learning; and (4) a pairwise quantization loss for controlling hashing quality. Extensive experiments on standard image retrieval datasets show the proposed DHN model yields substantial boosts over latest state-of-the-art hashing methods.</p>
</td>
</tr>

<tr>
	<td>2024</td>
	<td><a href="/publications/zhu2024linear/">Linear Cross-modal Hashing For Efficient Multimedia Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Linear Cross-modal Hashing For Efficient Multimedia Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Linear Cross-modal Hashing For Efficient Multimedia Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Linear%20Cross-modal%20Hashing%20For%20Efficient%20Multimedia%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhu Xiaofeng, Huang, Shen, Zhao</td>
	<td>Arxiv</td>
	<td><p>Most existing cross-modal hashing methods suffer from the scalability issue in the training phase. In this paper we propose a novel cross-modal hashing approach with a linear time complexity to the training data size to enable scalable indexing for multimedia search across multiple modals. Taking both the intra-similarity in each modal and the inter-similarity across different modals into consideration the proposed approach aims at effectively learning hash functions from large-scale training datasets. More specifically for each modal we first partition the training data into (k) clusters and then represent each training data point with its distances to (k) centroids of the clusters. Interestingly such a k-dimensional data representation can reduce the time complexity of the training phase from traditional O(n2) or higher to O(n) where (n) is the training data size leading to practical learning on large-scale datasets. We further prove that this new representation preserves the intra-similarity in each modal. To preserve the inter-similarity among data points across different modals we transform the derived data representations into a common binary subspace in which binary codes from all the modals are consistent and comparable. The transformation simultaneously outputs the hash functions for all modals which are used to convert unseen data into binary codes. Given a query of one modal it is first mapped into the binary codes using the modals hash functions followed by matching the database binary codes of any other modals. Experimental results on two benchmark datasets confirm the scalability and the effectiveness of the proposed approach in comparison with the state of the art.</p>
</td>
</tr>



<tr>
	<td>2023</td>
	<td><a href="/publications/schwengber2023deep/">Deep Hashing Via Householder Quantization</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Hashing Via Householder Quantization' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Hashing Via Householder Quantization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Hashing%20Via%20Householder%20Quantization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Schwengber Lucas R., Resende Lucas, Orenstein Paulo, Oliveira Roberto I.</td>
	<td>Arxiv</td>
	<td><p>Hashing is at the heart of large-scale image similarity search and recent methods have been substantially improved through deep learning techniques. Such algorithms typically learn continuous embeddings of the data. To avoid a subsequent costly binarization step a common solution is to employ loss functions that combine a similarity learning term (to ensure similar images are grouped to nearby embeddings) and a quantization penalty term (to ensure that the embedding entries are close to binarized entries e.g. -1 or 1). Still the interaction between these two terms can make learning harder and the embeddings worse. We propose an alternative quantization strategy that decomposes the learning problem in two stages first perform similarity learning over the embedding space with no quantization; second find an optimal orthogonal transformation of the embeddings so each coordinate of the embedding is close to its sign and then quantize the transformed embedding through the sign function. In the second step we parametrize orthogonal transformations using Householder matrices to efficiently leverage stochastic gradient descent. Since similarity measures are usually invariant under orthogonal transformations this quantization strategy comes at no cost in terms of performance. The resulting algorithm is unsupervised fast hyperparameter-free and can be run on top of any existing deep hashing or metric learning algorithm. We provide extensive experimental results showing that this approach leads to state-of-the-art performance on widely used image datasets and unlike other quantization strategies brings consistent improvements in performance to existing deep hashing algorithms.</p>
</td>
</tr>

<tr>
	<td>2023</td>
	<td><a href="/publications/houen2023sparse/">A Sparse Johnson-lindenstrauss Transform Using Fast Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Sparse Johnson-lindenstrauss Transform Using Fast Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Sparse Johnson-lindenstrauss Transform Using Fast Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20Sparse%20Johnson-lindenstrauss%20Transform%20Using%20Fast%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Houen Jakob BÃ¦k Tejs, Thorup Mikkel</td>
	<td>Arxiv</td>
	<td><p>The emphSparse Johnson-Lindenstrauss Transform of Kane and Nelson (SODA 2012) provides a linear dimensionality-reducing map A in mathbbR^m times u( in )ell_2( that preserves distances up to distortion of )1 + varepsilon with probability (1 - delta) where (m = O(varepsilon^-2 log 1/delta)) and each column of (A) has (O(varepsilon m)) non-zero entries. The previous analyses of the Sparse Johnson-Lindenstrauss Transform all assumed access to a (Omega(log 1/delta))-wise independent hash function. The main contribution of this paper is a more general analysis of the Sparse Johnson-Lindenstrauss Transform with less assumptions on the hash function. We also show that the emphMixed Tabulation hash function of Dahlgaard Knudsen Rotenberg and Thorup (FOCS 2015) satisfies the conditions of our analysis thus giving us the first analysis of a Sparse Johnson-Lindenstrauss Transform that works with a practical hash function.</p>
</td>
</tr>

<tr>
	<td>2023</td>
	<td><a href="/publications/holden2023identifying/">Identifying Reducible K-tuples Of Vectors With Subspace-proximity Sensitive Hashing/filtering</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Identifying Reducible K-tuples Of Vectors With Subspace-proximity Sensitive Hashing/filtering' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Identifying Reducible K-tuples Of Vectors With Subspace-proximity Sensitive Hashing/filtering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Identifying%20Reducible%20K-tuples%20Of%20Vectors%20With%20Subspace-proximity%20Sensitive%20Hashing/filtering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Holden Gabriella, Shiu Daniel, Strutt Lauren</td>
	<td>Arxiv</td>
	<td><p>We introduce and analyse a family of hash and predicate functions that are more likely to produce collisions for small reducible configurations of vectors. These may offer practical improvements to lattice sieving for short vectors. In particular in one asymptotic regime the family exhibits significantly different convergent behaviour than existing hash functions and predicates.</p>
</td>
</tr>

<tr>
	<td>2023</td>
	<td><a href="/publications/liu2023can/">Can LSH (locality-sensitive Hashing) Be Replaced By Neural Network</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Can LSH (locality-sensitive Hashing) Be Replaced By Neural Network' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Can LSH (locality-sensitive Hashing) Be Replaced By Neural Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Can%20LSH%20(locality-sensitive%20Hashing)%20Be%20Replaced%20By%20Neural%20Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Liu Renyang, Zhao Jun, Chu Xing, Liang Yu, Zhou Wei, He Jing</td>
	<td>Arxiv</td>
	<td><p>With the rapid development of GPU (Graphics Processing Unit) technologies and neural networks we can explore more appropriate data structures and algorithms. Recent progress shows that neural networks can partly replace traditional data structures. In this paper we proposed a novel DNN (Deep Neural Network)-based learned locality-sensitive hashing called LLSH to efficiently and flexibly map high-dimensional data to low-dimensional space. LLSH replaces the traditional LSH (Locality-sensitive Hashing) function families with parallel multi-layer neural networks which reduces the time and memory consumption and guarantees query accuracy simultaneously. The proposed LLSH demonstrate the feasibility of replacing the hash index with learning-based neural networks and open a new door for developers to design and configure data organization more accurately to improve information-searching performance. Extensive experiments on different types of datasets show the superiority of the proposed method in query accuracy time consumption and memory usage.</p>
</td>
</tr>

<tr>
	<td>2023</td>
	<td><a href="/publications/lecroq2023optimal/">Optimal-hash Exact String Matching Algorithms</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Optimal-hash Exact String Matching Algorithms' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Optimal-hash Exact String Matching Algorithms' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Optimal-hash%20Exact%20String%20Matching%20Algorithms' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Lecroq Thierry</td>
	<td>Arxiv</td>
	<td><p>String matching is the problem of finding all the occurrences of a pattern in a text. We propose improved versions of the fast family of string matching algorithms based on hashing (q)-grams. The improvement consists of considering minimal values (q) such that each (q)-grams of the pattern has a unique hash value. The new algorithms are fastest than algorithm of the HASH family for short patterns on large size alphabets.</p>
</td>
</tr>

<tr>
	<td>2023</td>
	<td><a href="/publications/lu2023attributes/">Attributes Grouping And Mining Hashing For Fine-grained Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Attributes Grouping And Mining Hashing For Fine-grained Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Attributes Grouping And Mining Hashing For Fine-grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Attributes%20Grouping%20And%20Mining%20Hashing%20For%20Fine-grained%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Lu Xin, Chen Shikun, Cao Yichao, Zhou Xin, Lu Xiaobo</td>
	<td>Proceedings of the</td>
	<td><p>In recent years hashing methods have been popular in the large-scale media search for low storage and strong representation capabilities. To describe objects with similar overall appearance but subtle differences more and more studies focus on hashing-based fine-grained image retrieval. Existing hashing networks usually generate both local and global features through attention guidance on the same deep activation tensor which limits the diversity of feature representations. To handle this limitation we substitute convolutional descriptors for attention-guided features and propose an Attributes Grouping and Mining Hashing (AGMH) which groups and embeds the category-specific visual attributes in multiple descriptors to generate a comprehensive feature representation for efficient fine-grained image retrieval. Specifically an Attention Dispersion Loss (ADL) is designed to force the descriptors to attend to various local regions and capture diverse subtle details. Moreover we propose a Stepwise Interactive External Attention (SIEA) to mine critical attributes in each descriptor and construct correlations between fine-grained attributes and objects. The attention mechanism is dedicated to learning discrete attributes which will not cost additional computations in hash codes generation. Finally the compact binary codes are learned by preserving pairwise similarities. Experimental results demonstrate that AGMH consistently yields the best performance against state-of-the-art methods on fine-grained benchmark datasets.</p>
</td>
</tr>

<tr>
	<td>2023</td>
	<td><a href="/publications/liu2023hs/">HS-GCN Hamming Spatial Graph Convolutional Networks For Recommendation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=HS-GCN Hamming Spatial Graph Convolutional Networks For Recommendation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=HS-GCN Hamming Spatial Graph Convolutional Networks For Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=HS-GCN%20Hamming%20Spatial%20Graph%20Convolutional%20Networks%20For%20Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Liu Han, Wei Yinwei, Yin Jianhua, Nie Liqiang</td>
	<td>Arxiv</td>
	<td><p>An efficient solution to the large-scale recommender system is to represent users and items as binary hash codes in the Hamming space. Towards this end existing methods tend to code users by modeling their Hamming similarities with the items they historically interact with which are termed as the first-order similarities in this work. Despite their efficiency these methods suffer from the suboptimal representative capacity since they forgo the correlation established by connecting multiple first-order similarities i.e. the relation among the indirect instances which could be defined as the high-order similarity. To tackle this drawback we propose to model both the first- and the high-order similarities in the Hamming space through the user-item bipartite graph. Therefore we develop a novel learning to hash framework namely Hamming Spatial Graph Convolutional Networks (HS-GCN) which explicitly models the Hamming similarity and embeds it into the codes of users and items. Extensive experiments on three public benchmark datasets demonstrate that our proposed model significantly outperforms several state-of-the-art hashing models and obtains performance comparable with the real-valued recommendation models.</p>
</td>
</tr>

<tr>
	<td>2023</td>
	<td><a href="/publications/korfhage2023elastichash/">Elastichash Semantic Image Similarity Search By Deep Hashing With Elasticsearch</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Elastichash Semantic Image Similarity Search By Deep Hashing With Elasticsearch' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Elastichash Semantic Image Similarity Search By Deep Hashing With Elasticsearch' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Elastichash%20Semantic%20Image%20Similarity%20Search%20By%20Deep%20Hashing%20With%20Elasticsearch' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Korfhage Nikolaus, MÃ¼hling Markus, Freisleben Bernd</td>
	<td>The</td>
	<td><p>We present ElasticHash a novel approach for high-quality efficient and large-scale semantic image similarity search. It is based on a deep hashing model to learn hash codes for fine-grained image similarity search in natural images and a two-stage method for efficiently searching binary hash codes using Elasticsearch (ES). In the first stage a coarse search based on short hash codes is performed using multi-index hashing and ES terms lookup of neighboring hash codes. In the second stage the list of results is re-ranked by computing the Hamming distance on long hash codes. We evaluate the retrieval performance of textitElasticHash for more than 120000 query images on about 6.9 million database images of the OpenImages data set. The results show that our approach achieves high-quality retrieval results and low search latencies.</p>
</td>
</tr>

<tr>
	<td>2023</td>
	<td><a href="/publications/tan2023fast/">Fast Locality Sensitive Hashing With Theoretical Guarantee</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Fast Locality Sensitive Hashing With Theoretical Guarantee' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Fast Locality Sensitive Hashing With Theoretical Guarantee' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Fast%20Locality%20Sensitive%20Hashing%20With%20Theoretical%20Guarantee' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Tan Zongyuan, Wang Hongya, Xu Bo, Luo Minjie, Du Ming</td>
	<td>Arxiv</td>
	<td><p>Locality-sensitive hashing (LSH) is an effective randomized technique widely used in many machine learning tasks. The cost of hashing is proportional to data dimensions and thus often the performance bottleneck when dimensionality is high and the number of hash functions involved is large. Surprisingly however little work has been done to improve the efficiency of LSH computation. In this paper we design a simple yet efficient LSH scheme named FastLSH under l2 norm. By combining random sampling and random projection FastLSH reduces the time complexity from O(n) to O(m) (m&lt;n) where n is the data dimensionality and m is the number of sampled dimensions. Moreover FastLSH has provable LSH property which distinguishes it from the non-LSH fast sketches. We conduct comprehensive experiments over a collection of real and synthetic datasets for the nearest neighbor search task. Experimental results demonstrate that FastLSH is on par with the state-of-the-arts in terms of answer quality space occupation and query efficiency while enjoying up to 80x speedup in hash function evaluation. We believe that FastLSH is a promising alternative to the classic LSH scheme.</p>
</td>
</tr>

<tr>
	<td>2023</td>
	<td><a href="/publications/ng2023unsupervised/">Unsupervised Hashing With Similarity Distribution Calibration</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Hashing With Similarity Distribution Calibration' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Unsupervised Hashing With Similarity Distribution Calibration' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Unsupervised%20Hashing%20With%20Similarity%20Distribution%20Calibration' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Ng Kam Woh, Zhu Xiatian, Hoe Jiun Tian, Chan Chee Seng, Zhang Tianyu, Song Yi-zhe, Xiang Tao</td>
	<td>Arxiv</td>
	<td><p>Unsupervised hashing methods typically aim to preserve the similarity between data points in a feature space by mapping them to binary hash codes. However these methods often overlook the fact that the similarity between data points in the continuous feature space may not be preserved in the discrete hash code space due to the limited similarity range of hash codes. The similarity range is bounded by the code length and can lead to a problem known as similarity collapse. That is the positive and negative pairs of data points become less distinguishable from each other in the hash space. To alleviate this problem in this paper a novel Similarity Distribution Calibration (SDC) method is introduced. SDC aligns the hash code similarity distribution towards a calibration distribution (e.g. beta distribution) with sufficient spread across the entire similarity range thus alleviating the similarity collapse problem. Extensive experiments show that our SDC outperforms significantly the state-of-the-art alternatives on coarse category-level and instance-level image retrieval. Code is available at https://github.com/kamwoh/sdc.</p>
</td>
</tr>

<tr>
	<td>2023</td>
	<td><a href="/publications/chen2023supervised/">Supervised Auto-encoding Twin-bottleneck Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Auto-encoding Twin-bottleneck Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Supervised Auto-encoding Twin-bottleneck Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Supervised%20Auto-encoding%20Twin-bottleneck%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Chen Yuan, Marchand-maillet StÃ©phane</td>
	<td>Arxiv</td>
	<td><p>Deep hashing has shown to be a complexity-efficient solution for the Approximate Nearest Neighbor search problem in high dimensional space. Many methods usually build the loss function from pairwise or triplet data points to capture the local similarity structure. Other existing methods construct the similarity graph and consider all points simultaneously. Auto-encoding Twin-bottleneck Hashing is one such method that dynamically builds the graph. Specifically each input data is encoded into a binary code and a continuous variable or the so-called twin bottlenecks. The similarity graph is then computed from these binary codes which get updated consistently during the training. In this work we generalize the original model into a supervised deep hashing network by incorporating the label information. In addition we examine the differences of codes structure between these two networks and consider the class imbalance problem especially in multi-labeled datasets. Experiments on three datasets yield statistically significant improvement against the original model. Results are also comparable and competitive to other supervised methods.</p>
</td>
</tr>

<tr>
	<td>2023</td>
	<td><a href="/publications/zhu2023central/">Central Similarity Multi-view Hashing For Multimedia Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Central Similarity Multi-view Hashing For Multimedia Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Central Similarity Multi-view Hashing For Multimedia Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Central%20Similarity%20Multi-view%20Hashing%20For%20Multimedia%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhu Jian, Cheng Wen, Cui Yu, Tang Chang, Dai Yuyang, Li Yong, Zeng Lingfang</td>
	<td>Arxiv</td>
	<td><p>Hash representation learning of multi-view heterogeneous data is the key to improving the accuracy of multimedia retrieval. However existing methods utilize local similarity and fall short of deeply fusing the multi-view features resulting in poor retrieval accuracy. Current methods only use local similarity to train their model. These methods ignore global similarity. Furthermore most recent works fuse the multi-view features via a weighted sum or concatenation. We contend that these fusion methods are insufficient for capturing the interaction between various views. We present a novel Central Similarity Multi-View Hashing (CSMVH) method to address the mentioned problems. Central similarity learning is used for solving the local similarity problem which can utilize the global similarity between the hash center and samples. We present copious empirical data demonstrating the superiority of gate-based fusion over conventional approaches. On the MS COCO and NUS-WIDE the proposed CSMVH performs better than the state-of-the-art methods by a large margin (up to 11.4137; mean Average Precision (mAP) improvement).</p>
</td>
</tr>



<tr>
	<td>2022</td>
	<td><a href="/publications/wei2022hyperbolic/">Hyperbolic Hierarchical Contrastive Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Hyperbolic Hierarchical Contrastive Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Hyperbolic Hierarchical Contrastive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Hyperbolic%20Hierarchical%20Contrastive%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Wei Rukai, Liu Yu, Song Jingkuan, Xie Yanzhao, Zhou Ke</td>
	<td>Transaction on Image Processing</td>
	<td><p>Hierarchical semantic structures naturally existing in real-world datasets can assist in capturing the latent distribution of data to learn robust hash codes for retrieval systems. Although hierarchical semantic structures can be simply expressed by integrating semantically relevant data into a high-level taxon with coarser-grained semantics the construction embedding and exploitation of the structures remain tricky for unsupervised hash learning. To tackle these problems we propose a novel unsupervised hashing method named Hyperbolic Hierarchical Contrastive Hashing (HHCH). We propose to embed continuous hash codes into hyperbolic space for accurate semantic expression since embedding hierarchies in hyperbolic space generates less distortion than in hyper-sphere space and Euclidean space. In addition we extend the K-Means algorithm to hyperbolic space and perform the proposed hierarchical hyperbolic K-Means algorithm to construct hierarchical semantic structures adaptively. To exploit the hierarchical semantic structures in hyperbolic space we designed the hierarchical contrastive learning algorithm including hierarchical instance-wise and hierarchical prototype-wise contrastive learning. Extensive experiments on four benchmark datasets demonstrate that the proposed method outperforms the state-of-the-art unsupervised hashing methods. Codes will be released.</p>
</td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/yu2022learning/">Learning To Hash Naturally Sorts</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning To Hash Naturally Sorts' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning To Hash Naturally Sorts' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20To%20Hash%20Naturally%20Sorts' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Yu Jiaguo, Shen Yuming, Wang Menghan, Zhang Haofeng, Torr Philip H. S.</td>
	<td>Arxiv</td>
	<td><p>Learning to hash pictures a list-wise sorting problem. Its testing metrics e.g. mean-average precision count on a sorted candidate list ordered by pair-wise code similarity. However scarcely does one train a deep hashing model with the sorted results end-to-end because of the non-differentiable nature of the sorting operation. This inconsistency in the objectives of training and test may lead to sub-optimal performance since the training loss often fails to reflect the actual retrieval metric. In this paper we tackle this problem by introducing Naturally-Sorted Hashing (NSH). We sort the Hamming distances of samples hash codes and accordingly gather their latent representations for self-supervised training. Thanks to the recent advances in differentiable sorting approximations the hash head receives gradients from the sorter so that the hash encoder can be optimized along with the training procedure. Additionally we describe a novel Sorted Noise-Contrastive Estimation (SortedNCE) loss that selectively picks positive and negative samples for contrastive learning which allows NSH to mine data semantic relations during training in an unsupervised manner. Our extensive experiments show the proposed NSH model significantly outperforms the existing unsupervised hashing methods on three benchmarked datasets.</p>
</td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/shao2022johnson/">Johnson-lindenstrauss Embeddings For Noisy Vectors -- Taking Advantage Of The Noise</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Johnson-lindenstrauss Embeddings For Noisy Vectors -- Taking Advantage Of The Noise' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Johnson-lindenstrauss Embeddings For Noisy Vectors -- Taking Advantage Of The Noise' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Johnson-lindenstrauss%20Embeddings%20For%20Noisy%20Vectors%20--%20Taking%20Advantage%20Of%20The%20Noise' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Shao Zhen</td>
	<td>Arxiv</td>
	<td><p>This paper investigates theoretical properties of subsampling and hashing as tools for approximate Euclidean norm-preserving embeddings for vectors with (unknown) additive Gaussian noises. Such embeddings are sometimes called Johnson-lindenstrauss embeddings due to their celebrated lemma. Previous work shows that as sparse embeddings the success of subsampling and hashing closely depends on the (l_infty) to (l_2) ratios of the vector to be mapped. This paper shows that the presence of noise removes such constrain in high-dimensions in other words sparse embeddings such as subsampling and hashing with comparable embedding dimensions to dense embeddings have similar approximate norm-preserving dimensionality-reduction properties. The key is that the noise should be treated as an information to be exploited not simply something to be removed. Theoretical bounds for subsampling and hashing to recover the approximate norm of a high dimension vector in the presence of noise are derived with numerical illustrations showing better performances are achieved in the presence of noise.</p>
</td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/mckeown2022hamming/">Hamming Distributions Of Popular Perceptual Hashing Techniques</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Hamming Distributions Of Popular Perceptual Hashing Techniques' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Hamming Distributions Of Popular Perceptual Hashing Techniques' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Hamming%20Distributions%20Of%20Popular%20Perceptual%20Hashing%20Techniques' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Mckeown Sean, Buchanan William J</td>
	<td>DFRWS</td>
	<td><p>Content-based file matching has been widely deployed for decades largely for the detection of sources of copyright infringement extremist materials and abusive sexual media. Perceptual hashes such as Microsofts PhotoDNA are one automated mechanism for facilitating detection allowing for machines to approximately match visual features of an image or video in a robust manner. However there does not appear to be much public evaluation of such approaches particularly when it comes to how effective they are against content-preserving modifications to media files. In this paper we present a million-image scale evaluation of several perceptual hashing archetypes for popular algorithms (including Facebooks PDQ Apples Neuralhash and the popular pHash library) against seven image variants. The focal point is the distribution of Hamming distance scores between both unrelated images and image variants to better understand the problems faced by each approach.</p>
</td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/jia2022fast/">Fast Online Hashing With Multi-label Projection</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Fast Online Hashing With Multi-label Projection' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Fast Online Hashing With Multi-label Projection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Fast%20Online%20Hashing%20With%20Multi-label%20Projection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Jia Wenzhe, Cao Yuan, Liu Junwei, Gui Jie</td>
	<td>Arxiv</td>
	<td><p>Hashing has been widely researched to solve the large-scale approximate nearest neighbor search problem owing to its time and storage superiority. In recent years a number of online hashing methods have emerged which can update the hash functions to adapt to the new stream data and realize dynamic retrieval. However existing online hashing methods are required to update the whole database with the latest hash functions when a query arrives which leads to low retrieval efficiency with the continuous increase of the stream data. On the other hand these methods ignore the supervision relationship among the examples especially in the multi-label case. In this paper we propose a novel Fast Online Hashing (FOH) method which only updates the binary codes of a small part of the database. To be specific we first build a query pool in which the nearest neighbors of each central point are recorded. When a new query arrives only the binary codes of the corresponding potential neighbors are updated. In addition we create a similarity matrix which takes the multi-label supervision information into account and bring in the multi-label projection loss to further preserve the similarity among the multi-label data. The experimental results on two common benchmarks show that the proposed FOH can achieve dramatic superiority on query time up to 6.28 seconds less than state-of-the-art baselines with competitive retrieval accuracy.</p>
</td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/yu2022weighted/">Weighted Contrastive Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Weighted Contrastive Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Weighted Contrastive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Weighted%20Contrastive%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Yu Jiaguo, Qiu Huming, Chen Dubing, Zhang Haofeng</td>
	<td>Arxiv</td>
	<td><p>The development of unsupervised hashing is advanced by the recent popular contrastive learning paradigm. However previous contrastive learning-based works have been hampered by (1) insufficient data similarity mining based on global-only image representations and (2) the hash code semantic loss caused by the data augmentation. In this paper we propose a novel method namely Weighted Contrative Hashing (WCH) to take a step towards solving these two problems. We introduce a novel mutual attention module to alleviate the problem of information asymmetry in network features caused by the missing image structure during contrative augmentation. Furthermore we explore the fine-grained semantic relations between images i.e. we divide the images into multiple patches and calculate similarities between patches. The aggregated weighted similarities which reflect the deep image relations are distilled to facilitate the hash codes learning with a distillation loss so as to obtain better retrieval performance. Extensive experiments show that the proposed WCH significantly outperforms existing unsupervised hashing methods on three benchmark datasets.</p>
</td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/yang2022fedhap/">Fedhap Federated Hashing With Global Prototypes For Cross-silo Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Fedhap Federated Hashing With Global Prototypes For Cross-silo Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Fedhap Federated Hashing With Global Prototypes For Cross-silo Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Fedhap%20Federated%20Hashing%20With%20Global%20Prototypes%20For%20Cross-silo%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Yang Meilin, Xu Jian, Liu Yang, Ding Wenbo</td>
	<td>Arxiv</td>
	<td><p>Deep hashing has been widely applied in large-scale data retrieval due to its superior retrieval efficiency and low storage cost. However data are often scattered in data silos with privacy concerns so performing centralized data storage and retrieval is not always possible. Leveraging the concept of federated learning (FL) to perform deep hashing is a recent research trend. However existing frameworks mostly rely on the aggregation of the local deep hashing models which are trained by performing similarity learning with local skewed data only. Therefore they cannot work well for non-IID clients in a real federated environment. To overcome these challenges we propose a novel federated hashing framework that enables participating clients to jointly train the shared deep hashing model by leveraging the prototypical hash codes for each class. Globally the transmission of global prototypes with only one prototypical hash code per class will minimize the impact of communication cost and privacy risk. Locally the use of global prototypes are maximized by jointly training a discriminator network and the local hashing network. Extensive experiments on benchmark datasets are conducted to demonstrate that our method can significantly improve the performance of the deep hashing model in the federated environments with non-IID data distributions.</p>
</td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/doan2022one/">One Loss For Quantization Deep Hashing With Discrete Wasserstein Distributional Matching</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=One Loss For Quantization Deep Hashing With Discrete Wasserstein Distributional Matching' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=One Loss For Quantization Deep Hashing With Discrete Wasserstein Distributional Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=One%20Loss%20For%20Quantization%20Deep%20Hashing%20With%20Discrete%20Wasserstein%20Distributional%20Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Doan Khoa D., Yang Peng, Li Ping</td>
	<td>Arxiv</td>
	<td><p>Image hashing is a principled approximate nearest neighbor approach to find similar items to a query in a large collection of images. Hashing aims to learn a binary-output function that maps an image to a binary vector. For optimal retrieval performance producing balanced hash codes with low-quantization error to bridge the gap between the learning stages continuous relaxation and the inference stages discrete quantization is important. However in the existing deep supervised hashing methods coding balance and low-quantization error are difficult to achieve and involve several losses. We argue that this is because the existing quantization approaches in these methods are heuristically constructed and not effective to achieve these objectives. This paper considers an alternative approach to learning the quantization constraints. The task of learning balanced codes with low quantization error is re-formulated as matching the learned distribution of the continuous codes to a pre-defined discrete uniform distribution. This is equivalent to minimizing the distance between two distributions. We then propose a computationally efficient distributional distance by leveraging the discrete property of the hash functions. This distributional distance is a valid distance and enjoys lower time and sample complexities. The proposed single-loss quantization objective can be integrated into any existing supervised hashing method to improve code balance and quantization error. Experiments confirm that the proposed approach substantially improves the performance of several representative hashing~methods.</p>
</td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/shi2022efficient/">Efficient Cross-modal Retrieval Via Deep Binary Hashing And Quantization</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Cross-modal Retrieval Via Deep Binary Hashing And Quantization' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Efficient Cross-modal Retrieval Via Deep Binary Hashing And Quantization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Efficient%20Cross-modal%20Retrieval%20Via%20Deep%20Binary%20Hashing%20And%20Quantization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Shi Yang, Chung Young-joo</td>
	<td>BMVC</td>
	<td><p>Cross-modal retrieval aims to search for data with similar semantic meanings across different content modalities. However cross-modal retrieval requires huge amounts of storage and retrieval time since it needs to process data in multiple modalities. Existing works focused on learning single-source compact features such as binary hash codes that preserve similarities between different modalities. In this work we propose a jointly learned deep hashing and quantization network (HQ) for cross-modal retrieval. We simultaneously learn binary hash codes and quantization codes to preserve semantic information in multiple modalities by an end-to-end deep learning architecture. At the retrieval step binary hashing is used to retrieve a subset of items from the search space then quantization is used to re-rank the retrieved items. We theoretically and empirically show that this two-stage retrieval approach provides faster retrieval results while preserving accuracy. Experimental results on the NUS-WIDE MIR-Flickr and Amazon datasets demonstrate that HQ achieves boosts of more than 737; in precision compared to supervised neural network-based compact coding models.</p>
</td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/zhu2022lower/">A Lower Bound Of Hash Codes Performance</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Lower Bound Of Hash Codes Performance' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Lower Bound Of Hash Codes Performance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20Lower%20Bound%20Of%20Hash%20Codes%20Performance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Xiaosu Zhu, Jingkuan Song, Yu Lei, Lianli Gao, Hengtao Shen</td>
	<td>Neural Information Processing Systems</td>
	<td><p>As a crucial approach for compact representation learning hashing has achieved great success in effectiveness and efficiency. Numerous heuristic Hamming space metric learning objectives are designed to obtain high-quality hash codes. Nevertheless a theoretical analysis of criteria for learning good hash codes remains largely unexploited. In this paper we prove that inter-class distinctiveness and intra-class compactness among hash codes determine the lower bound of hash codes performance. Promoting these two characteristics could lift the bound and improve hash learning. We then propose a surrogate model to fully exploit the above objective by estimating the posterior of hash codes and controlling it which results in a low-bias optimization. Extensive experiments reveal the effectiveness of the proposed method. By testing on a series of hash-models we obtain performance improvements among all of them with an up to (26.5) increase in mean Average Precision and an up to (20.5) increase in accuracy. Our code is publicly available at https://github.com/VL-Group/LBHash.</p>
</td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/zhang2022supervised/">Supervised Deep Hashing For High-dimensional And Heterogeneous Case-based Reasoning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Deep Hashing For High-dimensional And Heterogeneous Case-based Reasoning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Supervised Deep Hashing For High-dimensional And Heterogeneous Case-based Reasoning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Supervised%20Deep%20Hashing%20For%20High-dimensional%20And%20Heterogeneous%20Case-based%20Reasoning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhang Qi, Hu Liang, Shi Chongyang, Liu Ke, Cao Longbing</td>
	<td>Arxiv</td>
	<td><p>Case-based Reasoning (CBR) on high-dimensional and heterogeneous data is a trending yet challenging and computationally expensive task in the real world. A promising approach is to obtain low-dimensional hash codes representing cases and perform a similarity retrieval of cases in Hamming space. However previous methods based on data-independent hashing rely on random projections or manual construction inapplicable to address specific data issues (e.g. high-dimensionality and heterogeneity) due to their insensitivity to data characteristics. To address these issues this work introduces a novel deep hashing network to learn similarity-preserving compact hash codes for efficient case retrieval and proposes a deep-hashing-enabled CBR model HeCBR. Specifically we introduce position embedding to represent heterogeneous features and utilize a multilinear interaction layer to obtain case embeddings which effectively filtrates zero-valued features to tackle high-dimensionality and sparsity and captures inter-feature couplings. Then we feed the case embeddings into fully-connected layers and subsequently a hash layer generates hash codes with a quantization regularizer to control the quantization loss during relaxation. To cater to incremental learning of CBR we further propose an adaptive learning strategy to update the hash function. Extensive experiments on public datasets show that HeCBR greatly reduces storage and significantly accelerates case retrieval. HeCBR achieves desirable performance compared with the state-of-the-art CBR methods and performs significantly better than hashing-based CBR methods in classification.</p>
</td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/wang2022binary/">Binary Representation Via Jointly Personalized Sparse Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Binary Representation Via Jointly Personalized Sparse Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Binary Representation Via Jointly Personalized Sparse Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Binary%20Representation%20Via%20Jointly%20Personalized%20Sparse%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Wang Xiaoqin, Chen Chen, Lan Rushi, Liu Licheng, Liu Zhenbing, Zhou Huiyu, Luo Xiaonan</td>
	<td>Arxiv</td>
	<td><p>Unsupervised hashing has attracted much attention for binary representation learning due to the requirement of economical storage and efficiency of binary codes. It aims to encode high-dimensional features in the Hamming space with similarity preservation between instances. However most existing methods learn hash functions in manifold-based approaches. Those methods capture the local geometric structures (i.e. pairwise relationships) of data and lack satisfactory performance in dealing with real-world scenarios that produce similar features (e.g. color and shape) with different semantic information. To address this challenge in this work we propose an effective unsupervised method namely Jointly Personalized Sparse Hashing (JPSH) for binary representation learning. To be specific firstly we propose a novel personalized hashing module i.e. Personalized Sparse Hashing (PSH). Different personalized subspaces are constructed to reflect category-specific attributes for different clusters adaptively mapping instances within the same cluster to the same Hamming space. In addition we deploy sparse constraints for different personalized subspaces to select important features. We also collect the strengths of the other clusters to build the PSH module with avoiding over-fitting. Then to simultaneously preserve semantic and pairwise similarities in our JPSH we incorporate the PSH and manifold-based hash learning into the seamless formulation. As such JPSH not only distinguishes the instances from different clusters but also preserves local neighborhood structures within the cluster. Finally an alternating optimization algorithm is adopted to iteratively capture analytical solutions of the JPSH model. Extensive experiments on four benchmark datasets verify that the JPSH outperforms several hashing algorithms on the similarity search task.</p>
</td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/pham2022locality/">Falconn++ A Locality-sensitive Filtering Approach For Approximate Nearest Neighbor Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Falconn++ A Locality-sensitive Filtering Approach For Approximate Nearest Neighbor Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Falconn++ A Locality-sensitive Filtering Approach For Approximate Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Falconn++%20A%20Locality-sensitive%20Filtering%20Approach%20For%20Approximate%20Nearest%20Neighbor%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Ninh Pham, Tao Liu</td>
	<td>Neural Information Processing Systems</td>
	<td><p>We present Falconn++ a novel locality-sensitive filtering (LSF) approach for approximate nearest neighbor search on angular distance. Falconn++ can filter out potential far away points in any hash bucket before querying which results in higher quality candidates compared to other hashing-based solutions. Theoretically Falconn++ asymptotically achieves lower query time complexity than Falconn an optimal locality-sensitive hashing scheme on angular distance. Empirically Falconn++ achieves a higher recall-speed tradeoff than Falconn on many real-world data sets. Falconn++ is also competitive with HNSW an efficient representative of graph-based solutions on high search recall regimes.</p>
</td>
</tr>



<tr>
	<td>2021</td>
	<td><a href="/publications/shen2021re/">Re-ranking For Image Retrieval And Transductive Few-shot Classification</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Re-ranking For Image Retrieval And Transductive Few-shot Classification' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Re-ranking For Image Retrieval And Transductive Few-shot Classification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Re-ranking%20For%20Image%20Retrieval%20And%20Transductive%20Few-shot%20Classification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Xi Shen, Yang Xiao, Shell Hu, Othman Sbai, Mathieu Aubry</td>
	<td>Neural Information Processing Systems</td>
	<td><p>In the problems of image retrieval and few-shot classification the mainstream approaches focus on learning a better feature representation. However directly tackling the distance or similarity measure between images could also be efficient. To this end we revisit the idea of re-ranking the top-k retrieved images in the context of image retrieval (e.g. the k-reciprocal nearest neighbors) and generalize this idea to transductive few-shot learning. We propose to meta-learn the re-ranking updates such that the similarity graph converges towards the target similarity graph induced by the image labels. Specifically the re-ranking module takes as input an initial similarity graph between the query image and the contextual images using a pre-trained feature extractor and predicts an improved similarity graph by leveraging the structure among the involved images. We show that our re-ranking approach can be applied to unseen images and can further boost existing approaches for both image retrieval and few-shot learning problems. Our approach operates either independently or in conjunction with classical re-ranking approaches yielding clear and consistent improvements on image retrieval (CUB Cars SOP rOxford5K and rParis6K) and transductive few-shot classification (Mini-ImageNet tiered-ImageNet and CIFAR-FS) benchmarks. Our code is available at https://imagine.enpc.fr/~shenx/SSR/.</p>
</td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/jang2021similarity/">Similarity Guided Deep Face Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Similarity Guided Deep Face Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Similarity Guided Deep Face Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Similarity%20Guided%20Deep%20Face%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Jang Young Kyun, Cho Nam Ik</td>
	<td>Arxiv</td>
	<td><p>Face image retrieval which searches for images of the same identity from the query input face image is drawing more attention as the size of the image database increases rapidly. In order to conduct fast and accurate retrieval a compact hash code-based methods have been proposed and recently deep face image hashing methods with supervised classification training have shown outstanding performance. However classification-based scheme has a disadvantage in that it cannot reveal complex similarities between face images into the hash code learning. In this paper we attempt to improve the face image retrieval quality by proposing a Similarity Guided Hashing (SGH) method which gently considers self and pairwise-similarity simultaneously. SGH employs various data augmentations designed to explore elaborate similarities between face images solving both intra and inter identity-wise difficulties. Extensive experimental results on the protocols with existing benchmarks and an additionally proposed large scale higher resolution face image dataset demonstrate that our SGH delivers state-of-the-art retrieval performance.</p>
</td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/jang2021self/">Self-supervised Product Quantization For Deep Unsupervised Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Self-supervised Product Quantization For Deep Unsupervised Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Self-supervised Product Quantization For Deep Unsupervised Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Self-supervised%20Product%20Quantization%20For%20Deep%20Unsupervised%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Jang Young Kyun, Cho Nam Ik</td>
	<td>Arxiv</td>
	<td><p>Supervised deep learning-based hash and vector quantization are enabling fast and large-scale image retrieval systems. By fully exploiting label annotations they are achieving outstanding retrieval performances compared to the conventional methods. However it is painstaking to assign labels precisely for a vast amount of training data and also the annotation process is error-prone. To tackle these issues we propose the first deep unsupervised image retrieval method dubbed Self-supervised Product Quantization (SPQ) network which is label-free and trained in a self-supervised manner. We design a Cross Quantized Contrastive learning strategy that jointly learns codewords and deep visual descriptors by comparing individually transformed images (views). Our method analyzes the image contents to extract descriptive features allowing us to understand image representations for accurate retrieval. By conducting extensive experiments on benchmarks we demonstrate that the proposed method yields state-of-the-art results even without supervised pretraining.</p>
</td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/jang2021deep/">Deep Hash Distillation For Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Hash Distillation For Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Hash Distillation For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Hash%20Distillation%20For%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Jang Young Kyun, Gu Geonmo, Ko Byungsoo, Kang Isaac, Cho Nam Ik</td>
	<td>Arxiv</td>
	<td><p>In hash-based image retrieval systems degraded or transformed inputs usually generate different codes from the original deteriorating the retrieval accuracy. To mitigate this issue data augmentation can be applied during training. However even if augmented samples of an image are similar in real feature space the quantization can scatter them far away in Hamming space. This results in representation discrepancies that can impede training and degrade performance. In this work we propose a novel self-distilled hashing scheme to minimize the discrepancy while exploiting the potential of augmented data. By transferring the hash knowledge of the weakly-transformed samples to the strong ones we make the hash code insensitive to various transformations. We also introduce hash proxy-based similarity learning and binary cross entropy-based quantization loss to provide fine quality hash codes. Ultimately we construct a deep hashing framework that not only improves the existing deep hashing approaches but also achieves the state-of-the-art retrieval results. Extensive experiments are conducted and confirm the effectiveness of our work.</p>
</td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/zeng2021phpq/">PHPQ Pyramid Hybrid Pooling Quantization For Efficient Fine-grained Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=PHPQ Pyramid Hybrid Pooling Quantization For Efficient Fine-grained Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=PHPQ Pyramid Hybrid Pooling Quantization For Efficient Fine-grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=PHPQ%20Pyramid%20Hybrid%20Pooling%20Quantization%20For%20Efficient%20Fine-grained%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zeng Ziyun, Wang Jinpeng, Chen Bin, Dai Tao, Xia Shu-tao, Wang Zhi</td>
	<td>Pattern Recognition Letters Volume</td>
	<td><p>Deep hashing approaches including deep quantization and deep binary hashing have become a common solution to large-scale image retrieval due to their high computation and storage efficiency. Most existing hashing methods cannot produce satisfactory results for fine-grained retrieval because they usually adopt the outputs of the last CNN layer to generate binary codes. Since deeper layers tend to summarize visual clues e.g. texture into abstract semantics e.g. dogs and cats the feature produced by the last CNN layer is less effective in capturing subtle but discriminative visual details that mostly exist in shallow layers. To improve fine-grained image hashing we propose Pyramid Hybrid Pooling Quantization (PHPQ). Specifically we propose a Pyramid Hybrid Pooling (PHP) module to capture and preserve fine-grained semantic information from multi-level features which emphasizes the subtle discrimination of different sub-categories. Besides we propose a learnable quantization module with a partial codebook attention mechanism which helps to optimize the most relevant codewords and improves the quantization. Comprehensive experiments on two widely-used public benchmarks i.e. CUB-200-2011 and Stanford Dogs demonstrate that PHPQ outperforms state-of-the-art methods.</p>
</td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/hoe2021one/">One Loss For All Deep Hashing With A Single Cosine Similarity Based Learning Objective</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=One Loss For All Deep Hashing With A Single Cosine Similarity Based Learning Objective' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=One Loss For All Deep Hashing With A Single Cosine Similarity Based Learning Objective' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=One%20Loss%20For%20All%20Deep%20Hashing%20With%20A%20Single%20Cosine%20Similarity%20Based%20Learning%20Objective' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Hoe Jiun Tian, Ng Kam Woh, Zhang Tianyu, Chan Chee Seng, Song Yi-zhe, Xiang Tao</td>
	<td>Arxiv</td>
	<td><p>A deep hashing model typically has two main learning objectives to make the learned binary hash codes discriminative and to minimize a quantization error. With further constraints such as bit balance and code orthogonality it is not uncommon for existing models to employ a large number (4) of losses. This leads to difficulties in model training and subsequently impedes their effectiveness. In this work we propose a novel deep hashing model with only a single learning objective. Specifically we show that maximizing the cosine similarity between the continuous codes and their corresponding binary orthogonal codes can ensure both hash code discriminativeness and quantization error minimization. Further with this learning objective code balancing can be achieved by simply using a Batch Normalization (BN) layer and multi-label classification is also straightforward with label smoothing. The result is an one-loss deep hashing model that removes all the hassles of tuning the weights of various losses. Importantly extensive experiments show that our model is highly effective outperforming the state-of-the-art multi-loss hashing models on three large-scale instance retrieval benchmarks often by significant margins. Code is available at https://github.com/kamwoh/orthohash</p>
</td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/ma2021rank/">Rank-consistency Deep Hashing For Scalable Multi-label Image Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Rank-consistency Deep Hashing For Scalable Multi-label Image Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Rank-consistency Deep Hashing For Scalable Multi-label Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Rank-consistency%20Deep%20Hashing%20For%20Scalable%20Multi-label%20Image%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Ma Cheng, Lu Jiwen, Zhou Jie</td>
	<td>IEEE Transactions on Multimedia</td>
	<td><p>As hashing becomes an increasingly appealing technique for large-scale image retrieval multi-label hashing is also attracting more attention for the ability to exploit multi-level semantic contents. In this paper we propose a novel deep hashing method for scalable multi-label image search. Unlike existing approaches with conventional objectives such as contrast and triplet losses we employ a rank list rather than pairs or triplets to provide sufficient global supervision information for all the samples. Specifically a new rank-consistency objective is applied to align the similarity orders from two spaces the original space and the hamming space. A powerful loss function is designed to penalize the samples whose semantic similarity and hamming distance are mismatched in two spaces. Besides a multi-label softmax cross-entropy loss is presented to enhance the discriminative power with a concise formulation of the derivative function. In order to manipulate the neighborhood structure of the samples with different labels we design a multi-label clustering loss to cluster the hashing vectors of the samples with the same labels by reducing the distances between the samples and their multiple corresponding class centers. The state-of-the-art experimental results achieved on three public multi-label datasets MIRFLICKR-25K IAPRTC12 and NUS-WIDE demonstrate the effectiveness of the proposed method.</p>
</td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/hansen2021unsupervised/">Unsupervised Multi-index Semantic Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Multi-index Semantic Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Unsupervised Multi-index Semantic Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Unsupervised%20Multi-index%20Semantic%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Hansen Christian, Hansen Casper, Simonsen Jakob Grue, Alstrup Stephen, Lioma Christina</td>
	<td>Arxiv</td>
	<td><p>Semantic hashing represents documents as compact binary vectors (hash codes) and allows both efficient and effective similarity search in large-scale information retrieval. The state of the art has primarily focused on learning hash codes that improve similarity search effectiveness while assuming a brute-force linear scan strategy for searching over all the hash codes even though much faster alternatives exist. One such alternative is multi-index hashing an approach that constructs a smaller candidate set to search over which depending on the distribution of the hash codes can lead to sub-linear search time. In this work we propose Multi-Index Semantic Hashing (MISH) an unsupervised hashing model that learns hash codes that are both effective and highly efficient by being optimized for multi-index hashing. We derive novel training objectives which enable to learn hash codes that reduce the candidate sets produced by multi-index hashing while being end-to-end trainable. In fact our proposed training objectives are model agnostic i.e. not tied to how the hash codes are generated specifically in MISH and are straight-forward to include in existing and future semantic hashing models. We experimentally compare MISH to state-of-the-art semantic hashing baselines in the task of document similarity search. We find that even though multi-index hashing also improves the efficiency of the baselines compared to a linear scan they are still upwards of 3337; slower than MISH while MISH is still able to obtain state-of-the-art effectiveness.</p>
</td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/dubey2021vision/">Vision Transformer Hashing For Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Vision Transformer Hashing For Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Vision Transformer Hashing For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Vision%20Transformer%20Hashing%20For%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Dubey Shiv Ram, Singh Satish Kumar, Chu Wei-ta</td>
	<td>Arxiv</td>
	<td><p>Deep learning has shown a tremendous growth in hashing techniques for image retrieval. Recently Transformer has emerged as a new architecture by utilizing self-attention without convolution. Transformer is also extended to Vision Transformer (ViT) for the visual recognition with a promising performance on ImageNet. In this paper we propose a Vision Transformer based Hashing (VTS) for image retrieval. We utilize the pre-trained ViT on ImageNet as the backbone network and add the hashing head. The proposed VTS model is fine tuned for hashing under six different image retrieval frameworks including Deep Supervised Hashing (DSH) HashNet GreedyHash Improved Deep Hashing Network (IDHN) Deep Polarized Network (DPN) and Central Similarity Quantization (CSQ) with their objective functions. We perform the extensive experiments on CIFAR10 ImageNet NUS-Wide and COCO datasets. The proposed VTS based image retrieval outperforms the recent state-of-the-art hashing techniques with a great margin. We also find the proposed VTS model as the backbone network is better than the existing networks such as AlexNet and ResNet. The code is released at urlhttps://github.com/shivram1987/VisionTransformerHashing}.</p>
</td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/engels2021practical/">Practical Near Neighbor Search Via Group Testing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Practical Near Neighbor Search Via Group Testing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Practical Near Neighbor Search Via Group Testing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Practical%20Near%20Neighbor%20Search%20Via%20Group%20Testing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Joshua Engels, Benjamin Coleman, Anshumali Shrivastava</td>
	<td>Neural Information Processing Systems</td>
	<td><p>We present a new algorithm for the approximate near neighbor problem that combines classical ideas from group testing with locality-sensitive hashing (LSH). We reduce the near neighbor search problem to a group testing problem by designating neighbors as positives non-neighbors as negatives and approximate membership queries as group tests. We instantiate this framework using distance-sensitive Bloom Filters to Identify Near-Neighbor Groups (FLINNG). We prove that FLINNG has sub-linear query time and show that our algorithm comes with a variety of practical advantages. For example FLINNG can be constructed in a single pass through the data consists entirely of efficient integer operations and does not require any distance computations. We conduct large-scale experiments on high-dimensional search tasks such as genome search URL similarity search and embedding search over the massive YFCC100M dataset. In our comparison with leading algorithms such as HNSW and FAISS we find that FLINNG can provide up to a 10x query speedup with substantially smaller indexing time and memory.</p>
</td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/weng2021online/">Online Hashing With Similarity Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Online Hashing With Similarity Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Online Hashing With Similarity Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Online%20Hashing%20With%20Similarity%20Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Weng Zhenyu, Zhu Yuesheng</td>
	<td>Arxiv</td>
	<td><p>Online hashing methods usually learn the hash functions online aiming to efficiently adapt to the data variations in the streaming environment. However when the hash functions are updated the binary codes for the whole database have to be updated to be consistent with the hash functions resulting in the inefficiency in the online image retrieval process. In this paper we propose a novel online hashing framework without updating binary codes. In the proposed framework the hash functions are fixed and a parametric similarity function for the binary codes is learnt online to adapt to the streaming data. Specifically a parametric similarity function that has a bilinear form is adopted and a metric learning algorithm is proposed to learn the similarity function online based on the characteristics of the hashing methods. The experiments on two multi-label image datasets show that our method is competitive or outperforms the state-of-the-art online hashing methods in terms of both accuracy and efficiency for multi-label image retrieval.</p>
</td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/zhang2021improved/">Improved Deep Classwise Hashing With Centers Similarity Learning For Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Improved Deep Classwise Hashing With Centers Similarity Learning For Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Improved Deep Classwise Hashing With Centers Similarity Learning For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Improved%20Deep%20Classwise%20Hashing%20With%20Centers%20Similarity%20Learning%20For%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhang Ming, Yan Hong</td>
	<td>Arxiv</td>
	<td><p>Deep supervised hashing for image retrieval has attracted researchers attention due to its high efficiency and superior retrieval performance. Most existing deep supervised hashing works which are based on pairwise/triplet labels suffer from the expensive computational cost and insufficient utilization of the semantics information. Recently deep classwise hashing introduced a classwise loss supervised by class labels information alternatively; however we find it still has its drawback. In this paper we propose an improved deep classwise hashing which enables hashing learning and class centers learning simultaneously. Specifically we design a two-step strategy on center similarity learning. It interacts with the classwise loss to attract the class center to concentrate on the intra-class samples while pushing other class centers as far as possible. The centers similarity learning contributes to generating more compact and discriminative hashing codes. We conduct experiments on three benchmark datasets. It shows that the proposed method effectively surpasses the original method and outperforms state-of-the-art baselines under various commonly-used evaluation metrics for image retrieval.</p>
</td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/chen2021transhash/">Transhash Transformer-based Hamming Hashing For Efficient Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Transhash Transformer-based Hamming Hashing For Efficient Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Transhash Transformer-based Hamming Hashing For Efficient Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Transhash%20Transformer-based%20Hamming%20Hashing%20For%20Efficient%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Chen Yongbiao, Zhang Sheng, Liu Fangxin, Chang Zhigang, Ye Mang, Qi Zhengwei</td>
	<td>Arxiv</td>
	<td><p>Deep hamming hashing has gained growing popularity in approximate nearest neighbour search for large-scale image retrieval. Until now the deep hashing for the image retrieval community has been dominated by convolutional neural network architectures e.g. textttResnetcitehe2016deep. In this paper inspired by the recent advancements of vision transformers we present textbfTranshash a pure transformer-based framework for deep hashing learning. Concretely our framework is composed of two major modules (1) Based on textitVision Transformer (ViT) we design a siamese vision transformer backbone for image feature extraction. To learn fine-grained features we innovate a dual-stream feature learning on top of the transformer to learn discriminative global and local features. (2) Besides we adopt a Bayesian learning scheme with a dynamically constructed similarity matrix to learn compact binary hash codes. The entire framework is jointly trained in an end-to-end manner.~To the best of our knowledge this is the first work to tackle deep hashing learning problems without convolutional neural networks (textitCNNs). We perform comprehensive experiments on three widely-studied datasets textbfCIFAR-10 textbfNUSWIDE and textbfIMAGENET. The experiments have evidenced our superiority against the existing state-of-the-art deep hashing methods. Specifically we achieve 8.237; 2.637; 12.737; performance gains in terms of average textitmAP for different hash bit lengths on three public datasets respectively.</p>
</td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/zhang2021instance/">Instance-weighted Central Similarity For Multi-label Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Instance-weighted Central Similarity For Multi-label Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Instance-weighted Central Similarity For Multi-label Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Instance-weighted%20Central%20Similarity%20For%20Multi-label%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhang Zhiwei, Peng Hanyu</td>
	<td>Arxiv</td>
	<td><p>Deep hashing has been widely applied to large-scale image retrieval by encoding high-dimensional data points into binary codes for efficient retrieval. Compared with pairwise/triplet similarity based hash learning central similarity based hashing can more efficiently capture the global data distribution. For multi-label image retrieval however previous methods only use multiple hash centers with equal weights to generate one centroid as the learning target which ignores the relationship between the weights of hash centers and the proportion of instance regions in the image. To address the above issue we propose a two-step alternative optimization approach Instance-weighted Central Similarity (ICS) to automatically learn the center weight corresponding to a hash code. Firstly we apply the maximum entropy regularizer to prevent one hash center from dominating the loss function and compute the center weights via projection gradient descent. Secondly we update neural network parameters by standard back-propagation with fixed center weights. More importantly the learned center weights can well reflect the proportion of foreground instances in the image. Our method achieves the state-of-the-art performance on the image retrieval benchmarks and especially improves the mAP by 1.637;-6.437; on the MS COCO dataset.</p>
</td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/tian2021one/">One Loss For All Deep Hashing With A Single Cosine Similarity Based Learning Objective</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=One Loss For All Deep Hashing With A Single Cosine Similarity Based Learning Objective' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=One Loss For All Deep Hashing With A Single Cosine Similarity Based Learning Objective' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=One%20Loss%20For%20All%20Deep%20Hashing%20With%20A%20Single%20Cosine%20Similarity%20Based%20Learning%20Objective' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Jiun Tian Hoe, Kam Woh Ng, Tianyu Zhang, Chee Seng Chan, Yi-zhe Song, Tao Xiang</td>
	<td>Neural Information Processing Systems</td>
	<td><p>A deep hashing model typically has two main learning objectives to make the learned binary hash codes discriminative and to minimize a quantization error. With further constraints such as bit balance and code orthogonality it is not uncommon for existing models to employ a large number (4) of losses. This leads to difficulties in model training and subsequently impedes their effectiveness. In this work we propose a novel deep hashing model with only (). Specifically we show that maximizing the cosine similarity between the continuous codes and their corresponding () can ensure both hash code discriminativeness and quantization error minimization. Further with this learning objective code balancing can be achieved by simply using a Batch Normalization (BN) layer and multi-label classification is also straightforward with label smoothing. The result is a one-loss deep hashing model that removes all the hassles of tuning the weights of various losses. Importantly extensive experiments show that our model is highly effective outperforming the state-of-the-art multi-loss hashing models on three large-scale instance retrieval benchmarks often by significant margins.</p>
</td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/biswas2021state/">State Of The Art Image Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=State Of The Art Image Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=State Of The Art Image Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=State%20Of%20The%20Art%20Image%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Biswas Rubel, Blanco-medina Pablo</td>
	<td>Arxiv</td>
	<td><p>Perceptual image hashing methods are often applied in various objectives such as image retrieval finding duplicate or near-duplicate images and finding similar images from large-scale image content. The main challenge in image hashing techniques is robust feature extraction which generates the same or similar hashes in images that are visually identical. In this article we present a short review of the state-of-the-art traditional perceptual hashing and deep learning-based perceptual hashing methods identifying the best approaches.</p>
</td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/andoni2021from/">From Average Embeddings To Nearest Neighbor Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=From Average Embeddings To Nearest Neighbor Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=From Average Embeddings To Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=From%20Average%20Embeddings%20To%20Nearest%20Neighbor%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Andoni Alexandr, Cheikhi David</td>
	<td>Arxiv</td>
	<td><p>In this note we show that one can use average embeddings introduced recently in Naor20 arXiv1905.01280 to obtain efficient algorithms for approximate nearest neighbor search. In particular a metric (X) embeds into (ell_2) on average with distortion (D) if for any distribution (mu) on (X) the embedding is (D) Lipschitz and the (square of) distance does not decrease on average (wrt (mu)). In particular existence of such an embedding (assuming it is efficient) implies a (O(D^3)) approximate nearest neighbor search under (X). This can be seen as a strengthening of the classic (bi-Lipschitz) embedding approach to nearest neighbor search and is another application of data-dependent hashing paradigm.</p>
</td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/qiu2021unsupervised/">Unsupervised Hashing With Contrastive Information Bottleneck</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Hashing With Contrastive Information Bottleneck' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Unsupervised Hashing With Contrastive Information Bottleneck' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Unsupervised%20Hashing%20With%20Contrastive%20Information%20Bottleneck' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Qiu Zexuan, Su Qinliang, Ou Zijing, Yu Jianxing, Chen Changyou</td>
	<td>Arxiv</td>
	<td><p>Many unsupervised hashing methods are implicitly established on the idea of reconstructing the input data which basically encourages the hashing codes to retain as much information of original data as possible. However this requirement may force the models spending lots of their effort on reconstructing the unuseful background information while ignoring to preserve the discriminative semantic information that is more important for the hashing task. To tackle this problem inspired by the recent success of contrastive learning in learning continuous representations we propose to adapt this framework to learn binary hashing codes. Specifically we first propose to modify the objective function to meet the specific requirement of hashing and then introduce a probabilistic binary representation layer into the model to facilitate end-to-end training of the entire model. We further prove the strong connection between the proposed contrastive-learning-based hashing method and the mutual information and show that the proposed model can be considered under the broader framework of the information bottleneck (IB). Under this perspective a more general hashing model is naturally obtained. Extensive experimental results on three benchmark image datasets demonstrate that the proposed hashing method significantly outperforms existing baselines.</p>
</td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/lin2021deep/">Deep Self-adaptive Hashing For Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Self-adaptive Hashing For Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Self-adaptive Hashing For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Self-adaptive%20Hashing%20For%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Lin Qinghong, Chen Xiaojun, Zhang Qin, Tian Shangxuan, Chen Yudong</td>
	<td>Arxiv</td>
	<td><p>Hashing technology has been widely used in image retrieval due to its computational and storage efficiency. Recently deep unsupervised hashing methods have attracted increasing attention due to the high cost of human annotations in the real world and the superiority of deep learning technology. However most deep unsupervised hashing methods usually pre-compute a similarity matrix to model the pairwise relationship in the pre-trained feature space. Then this similarity matrix would be used to guide hash learning in which most of the data pairs are treated equivalently. The above process is confronted with the following defects 1) The pre-computed similarity matrix is inalterable and disconnected from the hash learning process which cannot explore the underlying semantic information. 2) The informative data pairs may be buried by the large number of less-informative data pairs. To solve the aforementioned problems we propose a Deep Self-Adaptive Hashing (DSAH) model to adaptively capture the semantic information with two special designs Adaptive Neighbor Discovery (AND) and Pairwise Information Content (PIC). Firstly we adopt the AND to initially construct a neighborhood-based similarity matrix and then refine this initial similarity matrix with a novel update strategy to further investigate the semantic structure behind the learned representation. Secondly we measure the priorities of data pairs with PIC and assign adaptive weights to them which is relies on the assumption that more dissimilar data pairs contain more discriminative information for hash learning. Extensive experiments on several datasets demonstrate that the above two technologies facilitate the deep hashing model to achieve superior performance.</p>
</td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/lu2021slosh/">SLOSH Set Locality Sensitive Hashing Via Sliced-wasserstein Embeddings</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=SLOSH Set Locality Sensitive Hashing Via Sliced-wasserstein Embeddings' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=SLOSH Set Locality Sensitive Hashing Via Sliced-wasserstein Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=SLOSH%20Set%20Locality%20Sensitive%20Hashing%20Via%20Sliced-wasserstein%20Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Lu Yuzhe, Liu Xinran, Soltoggio Andrea, Kolouri Soheil</td>
	<td>Arxiv</td>
	<td><p>Learning from set-structured data is an essential problem with many applications in machine learning and computer vision. This paper focuses on non-parametric and data-independent learning from set-structured data using approximate nearest neighbor (ANN) solutions particularly locality-sensitive hashing. We consider the problem of set retrieval from an input set query. Such retrieval problem requires 1) an efficient mechanism to calculate the distances/dissimilarities between sets and 2) an appropriate data structure for fast nearest neighbor search. To that end we propose Sliced-Wasserstein set embedding as a computationally efficient set-2-vector mechanism that enables downstream ANN with theoretical guarantees. The set elements are treated as samples from an unknown underlying distribution and the Sliced-Wasserstein distance is used to compare sets. We demonstrate the effectiveness of our algorithm denoted as Set-LOcality Sensitive Hashing (SLOSH) on various set retrieval datasets and compare our proposed embedding with standard set embedding approaches including Generalized Mean (GeM) embedding/pooling Featurewise Sort Pooling (FSPool) and Covariance Pooling and show consistent improvement in retrieval results. The code for replicating our results is available here hrefhttps://github.com/mint-vu/SLOSH}{https://github.com/mint-vu/SLOSH}.</p>
</td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/ouyang2021contextual/">Contextual Similarity Aggregation With Self-attention For Visual Re-ranking</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Contextual Similarity Aggregation With Self-attention For Visual Re-ranking' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Contextual Similarity Aggregation With Self-attention For Visual Re-ranking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Contextual%20Similarity%20Aggregation%20With%20Self-attention%20For%20Visual%20Re-ranking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Jianbo Ouyang, Hui Wu, Min Wang, Wengang Zhou, Houqiang Li</td>
	<td>Neural Information Processing Systems</td>
	<td><p>In content-based image retrieval the first-round retrieval result by simple visual feature comparison may be unsatisfactory which can be refined by visual re-ranking techniques. In image retrieval it is observed that the contextual similarity among the top-ranked images is an important clue to distinguish the semantic relevance. Inspired by this observation in this paper we propose a visual re-ranking method by contextual similarity aggregation with self-attention. In our approach for each image in the top-K ranking list we represent it into an affinity feature vector by comparing it with a set of anchor images. Then the affinity features of the top-K images are refined by aggregating the contextual information with a transformer encoder. Finally the affinity features are used to recalculate the similarity scores between the query and the top-K images for re-ranking of the latter. To further improve the robustness of our re-ranking model and enhance the performance of our method a new data augmentation scheme is designed. Since our re-ranking model is not directly involved with the visual feature used in the initial retrieval it is ready to be applied to retrieval result lists obtained from various retrieval algorithms. We conduct comprehensive experiments on four benchmark datasets to demonstrate the generality and effectiveness of our proposed visual re-ranking method.</p>
</td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/wei2021learning/">A(^2)-net Learning Attribute-aware Hash Codes For Large-scale Fine-grained Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A(^2)-net Learning Attribute-aware Hash Codes For Large-scale Fine-grained Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A(^2)-net Learning Attribute-aware Hash Codes For Large-scale Fine-grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A(%5E2)-net%20Learning%20Attribute-aware%20Hash%20Codes%20For%20Large-scale%20Fine-grained%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Xiu-shen Wei, Yang Shen, Xuhao Sun, Han-jia Ye, Jian Yang</td>
	<td>Neural Information Processing Systems</td>
	<td><p>Our work focuses on tackling large-scale fine-grained image retrieval as ranking the images depicting the concept of interests (i.e. the same sub-category labels) highest based on the fine-grained details in the query. It is desirable to alleviate the challenges of both fine-grained nature of small inter-class variations with large intra-class variations and explosive growth of fine-grained data for such a practical task. In this paper we propose an Attribute-Aware hashing Network (A(^2)-Net) for generating attribute-aware hash codes to not only make the retrieval process efficient but also establish explicit correspondences between hash codes and visual attributes. Specifically based on the captured visual representations by attention we develop an encoder-decoder structure network of a reconstruction task to unsupervisedly distill high-level attribute-specific vectors from the appearance-specific visual representations without attribute annotations. A(^2)-Net is also equipped with a feature decorrelation constraint upon these attribute vectors to enhance their representation abilities. Finally the required hash codes are generated by the attribute vectors driven by preserving original similarities. Qualitative experiments on five benchmark fine-grained datasets show our superiority over competing methods. More importantly quantitative results demonstrate the obtained hash codes can strongly correspond to certain kinds of crucial properties of fine-grained objects.</p>
</td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/luo2021deep/">Deep Unsupervised Hashing By Distilled Smooth Guidance</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Unsupervised Hashing By Distilled Smooth Guidance' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Unsupervised Hashing By Distilled Smooth Guidance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Unsupervised%20Hashing%20By%20Distilled%20Smooth%20Guidance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Luo Xiao, Ma Zeyu, Wu Daqing, Zhong Huasong, Chen Chong, Ma Jinwen, Deng Minghua</td>
	<td>ICME</td>
	<td><p>Hashing has been widely used in approximate nearest neighbor search for its storage and computational efficiency. Deep supervised hashing methods are not widely used because of the lack of labeled data especially when the domain is transferred. Meanwhile unsupervised deep hashing models can hardly achieve satisfactory performance due to the lack of reliable similarity signals. To tackle this problem we propose a novel deep unsupervised hashing method namely Distilled Smooth Guidance (DSG) which can learn a distilled dataset consisting of similarity signals as well as smooth confidence signals. To be specific we obtain the similarity confidence weights based on the initial noisy similarity signals learned from local structures and construct a priority loss function for smooth similarity-preserving learning. Besides global information based on clustering is utilized to distill the image pairs by removing contradictory similarity signals. Extensive experiments on three widely used benchmark datasets show that the proposed DSG consistently outperforms the state-of-the-art search methods.</p>
</td>
</tr>



<tr>
	<td>2020</td>
	<td><a href="/publications/kaplan2020locality/">Locality Sensitive Hashing For Set-queries Motivated By Group Recommendations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Locality Sensitive Hashing For Set-queries Motivated By Group Recommendations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Locality Sensitive Hashing For Set-queries Motivated By Group Recommendations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Locality%20Sensitive%20Hashing%20For%20Set-queries%20Motivated%20By%20Group%20Recommendations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Kaplan Haim, Tenenbaum Jay</td>
	<td>Arxiv</td>
	<td><p>Locality Sensitive Hashing (LSH) is an effective method to index a set of points such that we can efficiently find the nearest neighbors of a query point. We extend this method to our novel Set-query LSH (SLSH) such that it can find the nearest neighbors of a set of points given as a query. Let ( s(xy) ) be the similarity between two points ( x ) and ( y ). We define a similarity between a set ( Q) and a point ( x ) by aggregating the similarities ( s(px) ) for all ( pin Q ). For example we can take ( s(px) ) to be the angular similarity between ( p ) and ( x ) (i.e. 1-angle (xp)/pi) and aggregate by arithmetic or geometric averaging or taking the lowest similarity. We develop locality sensitive hash families and data structures for a large set of such arithmetic and geometric averaging similarities and analyze their collision probabilities. We also establish an analogous framework and hash families for distance functions. Specifically we give a structure for the euclidean distance aggregated by either averaging or taking the maximum. We leverage SLSH to solve a geometric extension of the approximate near neighbors problem. In this version we consider a metric for which the unit ball is an ellipsoid and its orientation is specified with the query. An important application that motivates our work is group recommendation systems. Such a system embeds movies and users in the same feature space and the task of recommending a movie for a group to watch together translates to a set-query ( Q ) using an appropriate similarity.</p>
</td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/zhan2020weakly/">Weakly-supervised Online Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Weakly-supervised Online Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Weakly-supervised Online Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Weakly-supervised%20Online%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhan Yu-wei, Luo Xin, Sun Yu, Wang Yongxin, Chen Zhen-duo, Xu Xin-shun</td>
	<td>Arxiv</td>
	<td><p>With the rapid development of social websites recent years have witnessed an explosive growth of social images with user-provided tags which continuously arrive in a streaming fashion. Due to the fast query speed and low storage cost hashing-based methods for image search have attracted increasing attention. However existing hashing methods for social image retrieval are based on batch mode which violates the nature of social images i.e. social images are usually generated periodically or collected in a stream fashion. Although there exist many online image hashing methods they either adopt unsupervised learning which ignore the relevant tags or are designed in the supervised manner which needs high-quality labels. In this paper to overcome the above limitations we propose a new method named Weakly-supervised Online Hashing (WOH). In order to learn high-quality hash codes WOH exploits the weak supervision by considering the semantics of tags and removing the noise. Besides We develop a discrete online optimization algorithm for WOH which is efficient and scalable. Extensive experiments conducted on two real-world datasets demonstrate the superiority of WOH compared with several state-of-the-art hashing baselines.</p>
</td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/shand2020locality/">Locality-sensitive Hashing In Function Spaces</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Locality-sensitive Hashing In Function Spaces' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Locality-sensitive Hashing In Function Spaces' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Locality-sensitive%20Hashing%20In%20Function%20Spaces' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Shand Will, Becker Stephen</td>
	<td>Arxiv</td>
	<td><p>We discuss the problem of performing similarity search over function spaces. To perform search over such spaces in a reasonable amount of time we use it locality-sensitive hashing (LSH). We present two methods that allow LSH functions on (^N) to be extended to (L^p) spaces one using function approximation in an orthonormal basis and another using (quasi-)Monte Carlo-style techniques. We use the presented hashing schemes to construct an LSH family for Wasserstein distance over one-dimensional continuous probability distributions.</p>
</td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/kanda2020dynamic/">Dynamic Similarity Search On Integer Sketches</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Dynamic Similarity Search On Integer Sketches' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Dynamic Similarity Search On Integer Sketches' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Dynamic%20Similarity%20Search%20On%20Integer%20Sketches' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Kanda Shunsuke, Tabei Yasuo</td>
	<td>Arxiv</td>
	<td><p>Similarity-preserving hashing is a core technique for fast similarity searches and it randomly maps data points in a metric space to strings of discrete symbols (i.e. sketches) in the Hamming space. While traditional hashing techniques produce binary sketches recent ones produce integer sketches for preserving various similarity measures. However most similarity search methods are designed for binary sketches and inefficient for integer sketches. Moreover most methods are either inapplicable or inefficient for dynamic datasets although modern real-world datasets are updated over time. We propose dynamic filter trie (DyFT) a dynamic similarity search method for both binary and integer sketches. An extensive experimental analysis using large real-world datasets shows that DyFT performs superiorly with respect to scalability time performance and memory efficiency. For example on a huge dataset of 216 million data points DyFT performs a similarity search 6000 times faster than a state-of-the-art method while reducing to one-thirteenth in memory.</p>
</td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/morgado2020deep/">Deep Hashing With Hash-consistent Large Margin Proxy Embeddings</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Hashing With Hash-consistent Large Margin Proxy Embeddings' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Hashing With Hash-consistent Large Margin Proxy Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Hashing%20With%20Hash-consistent%20Large%20Margin%20Proxy%20Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Morgado Pedro, Li Yunsheng, Pereira Jose Costa, Saberian Mohammad, Vasconcelos Nuno</td>
	<td>Arxiv</td>
	<td><p>Image hash codes are produced by binarizing the embeddings of convolutional neural networks (CNN) trained for either classification or retrieval. While proxy embeddings achieve good performance on both tasks they are non-trivial to binarize due to a rotational ambiguity that encourages non-binary embeddings. The use of a fixed set of proxies (weights of the CNN classification layer) is proposed to eliminate this ambiguity and a procedure to design proxy sets that are nearly optimal for both classification and hashing is introduced. The resulting hash-consistent large margin (HCLM) proxies are shown to encourage saturation of hashing units thus guaranteeing a small binarization error while producing highly discriminative hash-codes. A semantic extension (sHCLM) aimed to improve hashing performance in a transfer scenario is also proposed. Extensive experiments show that sHCLM embeddings achieve significant improvements over state-of-the-art hashing procedures on several small and large datasets both within and beyond the set of training classes.</p>
</td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/song2020deep/">Deep Robust Multilevel Semantic Cross-modal Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Robust Multilevel Semantic Cross-modal Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Robust Multilevel Semantic Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Robust%20Multilevel%20Semantic%20Cross-modal%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Song Ge, Zhao Jun, Tan Xiaoyang</td>
	<td>Arxiv</td>
	<td><p>Hashing based cross-modal retrieval has recently made significant progress. But straightforward embedding data from different modalities into a joint Hamming space will inevitably produce false codes due to the intrinsic modality discrepancy and noises. We present a novel Robust Multilevel Semantic Hashing (RMSH) for more accurate cross-modal retrieval. It seeks to preserve fine-grained similarity among data with rich semantics while explicitly require distances between dissimilar points to be larger than a specific value for strong robustness. For this we give an effective bound of this value based on the information coding-theoretic analysis and the above goals are embodied into a margin-adaptive triplet loss. Furthermore we introduce pseudo-codes via fusing multiple hash codes to explore seldom-seen semantics alleviating the sparsity problem of similarity information. Experiments on three benchmarks show the validity of the derived bounds and our method achieves state-of-the-art performance.</p>
</td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/yu2020self/">Self-supervised Asymmetric Deep Hashing With Margin-scalable Constraint</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Self-supervised Asymmetric Deep Hashing With Margin-scalable Constraint' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Self-supervised Asymmetric Deep Hashing With Margin-scalable Constraint' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Self-supervised%20Asymmetric%20Deep%20Hashing%20With%20Margin-scalable%20Constraint' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Yu Zhengyang, Wu Song, Dou Zhihao, Bakker Erwin M.</td>
	<td>Arxiv</td>
	<td><p>Due to its effectivity and efficiency deep hashing approaches are widely used for large-scale visual search. However it is still challenging to produce compact and discriminative hash codes for images associated with multiple semantics for two main reasons 1) similarity constraints designed in most of the existing methods are based upon an oversimplified similarity assignment(i.e. 0 for instance pairs sharing no label 1 for instance pairs sharing at least 1 label) 2) the exploration in multi-semantic relevance are insufficient or even neglected in many of the existing methods. These problems significantly limit the discrimination of generated hash codes. In this paper we propose a novel self-supervised asymmetric deep hashing method with a margin-scalable constraint(SADH) approach to cope with these problems. SADH implements a self-supervised network to sufficiently preserve semantic information in a semantic feature dictionary and a semantic code dictionary for the semantics of the given dataset which efficiently and precisely guides a feature learning network to preserve multilabel semantic information using an asymmetric learning strategy. By further exploiting semantic dictionaries a new margin-scalable constraint is employed for both precise similarity searching and robust hash code generation. Extensive empirical research on four popular benchmarks validates the proposed method and shows it outperforms several state-of-the-art approaches.</p>
</td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/hoang2020unsupervised/">Unsupervised Deep Cross-modality Spectral Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Deep Cross-modality Spectral Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Unsupervised Deep Cross-modality Spectral Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Unsupervised%20Deep%20Cross-modality%20Spectral%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Hoang Tuan, Do Thanh-toan, Nguyen Tam V., Cheung Ngai-man</td>
	<td>Arxiv</td>
	<td><p>This paper presents a novel framework namely Deep Cross-modality Spectral Hashing (DCSH) to tackle the unsupervised learning problem of binary hash codes for efficient cross-modal retrieval. The framework is a two-step hashing approach which decouples the optimization into (1) binary optimization and (2) hashing function learning. In the first step we propose a novel spectral embedding-based algorithm to simultaneously learn single-modality and binary cross-modality representations. While the former is capable of well preserving the local structure of each modality the latter reveals the hidden patterns from all modalities. In the second step to learn mapping functions from informative data inputs (images and word embeddings) to binary codes obtained from the first step we leverage the powerful CNN for images and propose a CNN-based deep architecture to learn text modality. Quantitative evaluations on three standard benchmark datasets demonstrate that the proposed DCSH method consistently outperforms other state-of-the-art methods.</p>
</td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/hansen2020unsupervised/">Unsupervised Semantic Hashing With Pairwise Reconstruction</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Semantic Hashing With Pairwise Reconstruction' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Unsupervised Semantic Hashing With Pairwise Reconstruction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Unsupervised%20Semantic%20Hashing%20With%20Pairwise%20Reconstruction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Hansen Casper, Hansen Christian, Simonsen Jakob Grue, Alstrup Stephen, Lioma Christina</td>
	<td>Arxiv</td>
	<td><p>Semantic Hashing is a popular family of methods for efficient similarity search in large-scale datasets. In Semantic Hashing documents are encoded as short binary vectors (i.e. hash codes) such that semantic similarity can be efficiently computed using the Hamming distance. Recent state-of-the-art approaches have utilized weak supervision to train better performing hashing models. Inspired by this we present Semantic Hashing with Pairwise Reconstruction (PairRec) which is a discrete variational autoencoder based hashing model. PairRec first encodes weakly supervised training pairs (a query document and a semantically similar document) into two hash codes and then learns to reconstruct the same query document from both of these hash codes (i.e. pairwise reconstruction). This pairwise reconstruction enables our model to encode local neighbourhood structures within the hash code directly through the decoder. We experimentally compare PairRec to traditional and state-of-the-art approaches and obtain significant performance improvements in the task of document similarity search.</p>
</td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/doan2020image/">Image Hashing By Minimizing Discrete Component-wise Wasserstein Distance</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Image Hashing By Minimizing Discrete Component-wise Wasserstein Distance' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Image Hashing By Minimizing Discrete Component-wise Wasserstein Distance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Image%20Hashing%20By%20Minimizing%20Discrete%20Component-wise%20Wasserstein%20Distance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Doan Khoa D., Manchanda Saurav, Badirli Sarkhan, Reddy Chandan K.</td>
	<td>Arxiv</td>
	<td><p>Image hashing is one of the fundamental problems that demand both efficient and effective solutions for various practical scenarios. Adversarial autoencoders are shown to be able to implicitly learn a robust locality-preserving hash function that generates balanced and high-quality hash codes. However the existing adversarial hashing methods are inefficient to be employed for large-scale image retrieval applications. Specifically they require an exponential number of samples to be able to generate optimal hash codes and a significantly high computational cost to train. In this paper we show that the high sample-complexity requirement often results in sub-optimal retrieval performance of the adversarial hashing methods. To address this challenge we propose a new adversarial-autoencoder hashing approach that has a much lower sample requirement and computational cost. Specifically by exploiting the desired properties of the hash function in the low-dimensional discrete space our method efficiently estimates a better variant of Wasserstein distance by averaging a set of easy-to-compute one-dimensional Wasserstein distances. The resulting hashing approach has an order-of-magnitude better sample complexity thus better generalization property compared to the other adversarial hashing methods. In addition the computational cost is significantly reduced using our approach. We conduct experiments on several real-world datasets and show that the proposed method outperforms the competing hashing methods achieving up to 1037; improvement over the current state-of-the-art image hashing methods. The code accompanying this paper is available on Github (https://github.com/khoadoan/adversarial-hashing).</p>
</td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/christiani2020dartminhash/">Dartminhash Fast Sketching For Weighted Sets</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Dartminhash Fast Sketching For Weighted Sets' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Dartminhash Fast Sketching For Weighted Sets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Dartminhash%20Fast%20Sketching%20For%20Weighted%20Sets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Christiani Tobias</td>
	<td>Arxiv</td>
	<td><p>Weighted minwise hashing is a standard dimensionality reduction technique with applications to similarity search and large-scale kernel machines. We introduce a simple algorithm that takes a weighted set x in mathbbR_geq 0^d( and computes )k( independent minhashes in expected time )O(k log k + Vert x Vert_0log( Vert x Vert_1 + 1/Vert x Vert_1)) improving upon the state-of-the-art BagMinHash algorithm (KDD 18) and representing the fastest weighted minhash algorithm for sparse data. Our experiments show running times that scale better with (k) and (Vert x Vert_0) compared to ICWS (ICDM 10) and BagMinhash obtaining (10)x speedups in common use cases. Our approach also gives rise to a technique for computing fully independent locality-sensitive hash values for ((L K))-parameterized approximate near neighbor search under weighted Jaccard similarity in optimal expected time (O(LK + Vert x Vert_0)) improving on prior work even in the case of unweighted sets.</p>
</td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/yan2020deep/">Deep Multi-view Enhancement Hashing For Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Multi-view Enhancement Hashing For Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Multi-view Enhancement Hashing For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Multi-view%20Enhancement%20Hashing%20For%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Yan Chenggang, Gong Biao, Wei Yuxuan, Gao Yue</td>
	<td>Arxiv</td>
	<td><p>Hashing is an efficient method for nearest neighbor search in large-scale data space by embedding high-dimensional feature descriptors into a similarity preserving Hamming space with a low dimension. However large-scale high-speed retrieval through binary code has a certain degree of reduction in retrieval accuracy compared to traditional retrieval methods. We have noticed that multi-view methods can well preserve the diverse characteristics of data. Therefore we try to introduce the multi-view deep neural network into the hash learning field and design an efficient and innovative retrieval model which has achieved a significant improvement in retrieval performance. In this paper we propose a supervised multi-view hash model which can enhance the multi-view information through neural networks. This is a completely new hash learning method that combines multi-view and deep learning methods. The proposed method utilizes an effective view stability evaluation method to actively explore the relationship among views which will affect the optimization direction of the entire network. We have also designed a variety of multi-data fusion methods in the Hamming space to preserve the advantages of both convolution and multi-view. In order to avoid excessive computing resources on the enhancement procedure during retrieval we set up a separate structure called memory network which participates in training together. The proposed method is systematically evaluated on the CIFAR-10 NUS-WIDE and MS-COCO datasets and the results show that our method significantly outperforms the state-of-the-art single-view and multi-view hashing methods.</p>
</td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/zhang2020survey/">A Survey On Deep Hashing For Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Survey On Deep Hashing For Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Survey On Deep Hashing For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20Survey%20On%20Deep%20Hashing%20For%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhang Xiaopeng</td>
	<td>Arxiv</td>
	<td><p>Hashing has been widely used in approximate nearest search for large-scale database retrieval for its computation and storage efficiency. Deep hashing which devises convolutional neural network architecture to exploit and extract the semantic information or feature of images has received increasing attention recently. In this survey several deep supervised hashing methods for image retrieval are evaluated and I conclude three main different directions for deep supervised hashing methods. Several comments are made at the end. Moreover to break through the bottleneck of the existing hashing methods I propose a Shadow Recurrent Hashing(SRH) method as a try. Specifically I devise a CNN architecture to extract the semantic features of images and design a loss function to encourage similar images projected close. To this end I propose a concept shadow of the CNN output. During optimization process the CNN output and its shadow are guiding each other so as to achieve the optimal solution as much as possible. Several experiments on dataset CIFAR-10 show the satisfying performance of SRH.</p>
</td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/tepper2020procrustean/">Procrustean Orthogonal Sparse Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Procrustean Orthogonal Sparse Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Procrustean Orthogonal Sparse Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Procrustean%20Orthogonal%20Sparse%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Tepper Mariano, Sengupta Dipanjan, Willke Ted</td>
	<td>Arxiv</td>
	<td><p>Hashing is one of the most popular methods for similarity search because of its speed and efficiency. Dense binary hashing is prevalent in the literature. Recently insect olfaction was shown to be structurally and functionally analogous to sparse hashing 6. Here we prove that this biological mechanism is the solution to a well-posed optimization problem. Furthermore we show that orthogonality increases the accuracy of sparse hashing. Next we present a novel method Procrustean Orthogonal Sparse Hashing (POSH) that unifies these findings learning an orthogonal transform from training data compatible with the sparse hashing mechanism. We provide theoretical evidence of the shortcomings of Optimal Sparse Lifting (OSL) 22 and BioHash 30 two related olfaction-inspired methods and propose two new methods Binary OSL and SphericalHash to address these deficiencies. We compare POSH Binary OSL and SphericalHash to several state-of-the-art hashing methods and provide empirical results for the superiority of the proposed methods across a wide range of standard benchmarks and parameter settings.</p>
</td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/arponen2020learning/">Learning To Hash With Semantic Similarity Metrics And Empirical KL Divergence</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning To Hash With Semantic Similarity Metrics And Empirical KL Divergence' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning To Hash With Semantic Similarity Metrics And Empirical KL Divergence' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20To%20Hash%20With%20Semantic%20Similarity%20Metrics%20And%20Empirical%20KL%20Divergence' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Arponen Heikki, Bishop Tom E.</td>
	<td>Arxiv</td>
	<td><p>Learning to hash is an efficient paradigm for exact and approximate nearest neighbor search from massive databases. Binary hash codes are typically extracted from an image by rounding output features from a CNN which is trained on a supervised binary similar/ dissimilar task. Drawbacks of this approach are (i) resulting codes do not necessarily capture semantic similarity of the input data (ii) rounding results in information loss manifesting as decreased retrieval performance and (iii) Using only class-wise similarity as a target can lead to trivial solutions simply encoding classifier outputs rather than learning more intricate relations which is not detected by most performance metrics. We overcome (i) via a novel loss function encouraging the relative hash code distances of learned features to match those derived from their targets. We address (ii) via a differentiable estimate of the KL divergence between network outputs and a binary target distribution resulting in minimal information loss when the features are rounded to binary. Finally we resolve (iii) by focusing on a hierarchical precision metric. Efficiency of the methods is demonstrated with semantic image retrieval on the CIFAR-100 ImageNet and Conceptual Captions datasets using similarities inferred from the WordNet label hierarchy or sentence embeddings.</p>
</td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/dadaneh2020pairwise/">Pairwise Supervised Hashing With Bernoulli Variational Auto-encoder And Self-control Gradient Estimator</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Pairwise Supervised Hashing With Bernoulli Variational Auto-encoder And Self-control Gradient Estimator' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Pairwise Supervised Hashing With Bernoulli Variational Auto-encoder And Self-control Gradient Estimator' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Pairwise%20Supervised%20Hashing%20With%20Bernoulli%20Variational%20Auto-encoder%20And%20Self-control%20Gradient%20Estimator' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Dadaneh Siamak Zamani, Boluki Shahin, Yin Mingzhang, Zhou Mingyuan, Qian Xiaoning</td>
	<td>Uncertainty in Artificial Intelligence Conference</td>
	<td><p>Semantic hashing has become a crucial component of fast similarity search in many large-scale information retrieval systems in particular for text data. Variational auto-encoders (VAEs) with binary latent variables as hashing codes provide state-of-the-art performance in terms of precision for document retrieval. We propose a pairwise loss function with discrete latent VAE to reward within-class similarity and between-class dissimilarity for supervised hashing. Instead of solving the optimization relying on existing biased gradient estimators an unbiased low-variance gradient estimator is adopted to optimize the hashing function by evaluating the non-differentiable loss function over two correlated sets of binary hashing codes to control the variance of gradient estimates. This new semantic hashing framework achieves superior performance compared to the state-of-the-arts as demonstrated by our comprehensive experiments.</p>
</td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/weng2020random/">Random VLAD Based Deep Hashing For Efficient Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Random VLAD Based Deep Hashing For Efficient Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Random VLAD Based Deep Hashing For Efficient Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Random%20VLAD%20Based%20Deep%20Hashing%20For%20Efficient%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Weng Li, Ye Lingzhi, Tian Jiangmin, Cao Jiuwen, Wang Jianzhong</td>
	<td>Arxiv</td>
	<td><p>Image hash algorithms generate compact binary representations that can be quickly matched by Hamming distance thus become an efficient solution for large-scale image retrieval. This paper proposes RV-SSDH a deep image hash algorithm that incorporates the classical VLAD (vector of locally aggregated descriptors) architecture into neural networks. Specifically a novel neural network component is formed by coupling a random VLAD layer with a latent hash layer through a transform layer. This component can be combined with convolutional layers to realize a hash algorithm. We implement RV-SSDH as a point-wise algorithm that can be efficiently trained by minimizing classification error and quantization loss. Comprehensive experiments show this new architecture significantly outperforms baselines such as NetVLAD and SSDH and offers a cost-effective trade-off in the state-of-the-art. In addition the proposed random VLAD layer leads to satisfactory accuracy with low complexity thus shows promising potentials as an alternative to NetVLAD.</p>
</td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/ren2020hm/">HM-ANN Efficient Billion-point Nearest Neighbor Search On Heterogeneous Memory</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=HM-ANN Efficient Billion-point Nearest Neighbor Search On Heterogeneous Memory' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=HM-ANN Efficient Billion-point Nearest Neighbor Search On Heterogeneous Memory' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=HM-ANN%20Efficient%20Billion-point%20Nearest%20Neighbor%20Search%20On%20Heterogeneous%20Memory' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Jie Ren, Minjia Zhang, Dong Li</td>
	<td>Neural Information Processing Systems</td>
	<td><p>The state-of-the-art approximate nearest neighbor search (ANNS) algorithms face a fundamental tradeoff between query latency and accuracy because of small main memory capacity To store indices in main memory for short query latency the ANNS algorithms have to limit dataset size or use a quantization scheme which hurts search accuracy. The emergence of heterogeneous memory (HM) brings a solution to significantly increase memory capacity and break the above tradeoff Using HM billions of data points can be placed in the main memory on a single machine without using any data compression. However HM consists of both fast (but small) memory and slow (but large) memory and using HM inappropriately slows down query significantly. In this work we present a novel graph-based similarity search algorithm called HM-ANN which takes both memory and data heterogeneity into consideration and enables billion-scale similarity search on a single node without using compression. On two billion-sized datasets BIGANN and DEEP1B HM-ANN outperforms state-of-the-art compression-based solutions such as Lamp;C and IMI+OPQ in recall-vs-latency by a large margin obtaining 4637; higher recall under the same search latency. We also extend existing graph-based methods such as HNSW and NSG with two strong baseline implementations on HM. At billion-point scale HM-ANN is 2X and 5.8X faster than our HNSWand NSG baselines respectively to reach the same accuracy.</p>
</td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/ryali2020bio/">Bio-inspired Hashing For Unsupervised Similarity Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Bio-inspired Hashing For Unsupervised Similarity Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Bio-inspired Hashing For Unsupervised Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Bio-inspired%20Hashing%20For%20Unsupervised%20Similarity%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Ryali Chaitanya K., Hopfield John J., Grinberg Leopold, Krotov Dmitry</td>
	<td>Proceedings of the International Conference on Machine Learning</td>
	<td><p>The fruit fly Drosophilas olfactory circuit has inspired a new locality sensitive hashing (LSH) algorithm FlyHash. In contrast with classical LSH algorithms that produce low dimensional hash codes FlyHash produces sparse high-dimensional hash codes and has also been shown to have superior empirical performance compared to classical LSH algorithms in similarity search. However FlyHash uses random projections and cannot learn from data. Building on inspiration from FlyHash and the ubiquity of sparse expansive representations in neurobiology our work proposes a novel hashing algorithm BioHash that produces sparse high dimensional hash codes in a data-driven manner. We show that BioHash outperforms previously published benchmarks for various hashing methods. Since our learning algorithm is based on a local and biologically plausible synaptic plasticity rule our work provides evidence for the proposal that LSH might be a computational reason for the abundance of sparse expansive motifs in a variety of biological systems. We also propose a convolutional variant BioConvHash that further improves performance. From the perspective of computer science BioHash and BioConvHash are fast scalable and yield compressed binary representations that are useful for similarity search.</p>
</td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/li2020deep/">Deep Unsupervised Image Hashing By Maximizing Bit Entropy</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Unsupervised Image Hashing By Maximizing Bit Entropy' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Unsupervised Image Hashing By Maximizing Bit Entropy' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Unsupervised%20Image%20Hashing%20By%20Maximizing%20Bit%20Entropy' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Li Yunqiang, Van Gemert Jan</td>
	<td>Arxiv</td>
	<td><p>Unsupervised hashing is important for indexing huge image or video collections without having expensive annotations available. Hashing aims to learn short binary codes for compact storage and efficient semantic retrieval. We propose an unsupervised deep hashing layer called Bi-half Net that maximizes entropy of the binary codes. Entropy is maximal when both possible values of the bit are uniformly (half-half) distributed. To maximize bit entropy we do not add a term to the loss function as this is difficult to optimize and tune. Instead we design a new parameter-free network layer to explicitly force continuous image features to approximate the optimal half-half bit distribution. This layer is shown to minimize a penalized term of the Wasserstein distance between the learned continuous image features and the optimal half-half bit distribution. Experimental results on the image datasets Flickr25k Nus-wide Cifar-10 Mscoco Mnist and the video datasets Ucf-101 and Hmdb-51 show that our approach leads to compact codes and compares favorably to the current state-of-the-art.</p>
</td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/li2020multiple/">Multiple Code Hashing For Efficient Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Multiple Code Hashing For Efficient Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Multiple Code Hashing For Efficient Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Multiple%20Code%20Hashing%20For%20Efficient%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Li Ming-wei, Jiang Qing-yuan, Li Wu-jun</td>
	<td>Arxiv</td>
	<td><p>Due to its low storage cost and fast query speed hashing has been widely used in large-scale image retrieval tasks. Hash bucket search returns data points within a given Hamming radius to each query which can enable search at a constant or sub-linear time cost. However existing hashing methods cannot achieve satisfactory retrieval performance for hash bucket search in complex scenarios since they learn only one hash code for each image. More specifically by using one hash code to represent one image existing methods might fail to put similar image pairs to the buckets with a small Hamming distance to the query when the semantic information of images is complex. As a result a large number of hash buckets need to be visited for retrieving similar images based on the learned codes. This will deteriorate the efficiency of hash bucket search. In this paper we propose a novel hashing framework called multiple code hashing (MCH) to improve the performance of hash bucket search. The main idea of MCH is to learn multiple hash codes for each image with each code representing a different region of the image. Furthermore we propose a deep reinforcement learning algorithm to learn the parameters in MCH. To the best of our knowledge this is the first work that proposes to learn multiple hash codes for each image in image retrieval. Experiments demonstrate that MCH can achieve a significant improvement in hash bucket search compared with existing methods that learn only one hash code for each image.</p>
</td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/luo2020cimon/">CIMON Towards High-quality Hash Codes</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=CIMON Towards High-quality Hash Codes' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=CIMON Towards High-quality Hash Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=CIMON%20Towards%20High-quality%20Hash%20Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Luo Xiao, Wu Daqing, Ma Zeyu, Chen Chong, Deng Minghua, Ma Jinwen, Jin Zhongming, Huang Jianqiang, Hua Xian-sheng</td>
	<td>Arxiv</td>
	<td><p>Recently hashing is widely used in approximate nearest neighbor search for its storage and computational efficiency. Most of the unsupervised hashing methods learn to map images into semantic similarity-preserving hash codes by constructing local semantic similarity structure from the pre-trained model as the guiding information i.e. treating each point pair similar if their distance is small in feature space. However due to the inefficient representation ability of the pre-trained model many false positives and negatives in local semantic similarity will be introduced and lead to error propagation during the hash code learning. Moreover few of the methods consider the robustness of models which will cause instability of hash codes to disturbance. In this paper we propose a new method named textbfComprehensive stextbfImilarity textbfMining and ctextbfOnsistency leartextbfNing (CIMON). First we use global refinement and similarity statistical distribution to obtain reliable and smooth guidance. Second both semantic and contrastive consistency learning are introduced to derive both disturb-invariant and discriminative hash codes. Extensive experiments on several benchmark datasets show that the proposed method outperforms a wide range of state-of-the-art methods in both retrieval performance and robustness.</p>
</td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/luo2020survey/">A Survey On Deep Hashing Methods</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Survey On Deep Hashing Methods' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Survey On Deep Hashing Methods' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20Survey%20On%20Deep%20Hashing%20Methods' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Luo Xiao, Wang Haixin, Wu Daqing, Chen Chong, Deng Minghua, Huang Jianqiang, Hua Xian-sheng</td>
	<td>Arxiv</td>
	<td><p>Nearest neighbor search aims to obtain the samples in the database with the smallest distances from them to the queries which is a basic task in a range of fields including computer vision and data mining. Hashing is one of the most widely used methods for its computational and storage efficiency. With the development of deep learning deep hashing methods show more advantages than traditional methods. In this survey we detailedly investigate current deep hashing algorithms including deep supervised hashing and deep unsupervised hashing. Specifically we categorize deep supervised hashing methods into pairwise methods ranking-based methods pointwise methods as well as quantization according to how measuring the similarities of the learned hash codes. Moreover deep unsupervised hashing is categorized into similarity reconstruction-based methods pseudo-label-based methods and prediction-free self-supervised learning-based methods based on their semantic learning manners. We also introduce three related important topics including semi-supervised deep hashing domain adaption deep hashing and multi-modal deep hashing. Meanwhile we present some commonly used public datasets and the scheme to measure the performance of deep hashing algorithms. Finally we discuss some potential research directions in conclusion.</p>
</td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/liu2020reinforcing/">Reinforcing Short-length Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Reinforcing Short-length Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Reinforcing Short-length Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Reinforcing%20Short-length%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Liu Xingbo, Nie Xiushan, Dai Qi, Huang Yupan, Yin Yilong</td>
	<td>Arxiv</td>
	<td><p>Due to the compelling efficiency in retrieval and storage similarity-preserving hashing has been widely applied to approximate nearest neighbor search in large-scale image retrieval. However existing methods have poor performance in retrieval using an extremely short-length hash code due to weak ability of classification and poor distribution of hash bit. To address this issue in this study we propose a novel reinforcing short-length hashing (RSLH). In this proposed RSLH mutual reconstruction between the hash representation and semantic labels is performed to preserve the semantic information. Furthermore to enhance the accuracy of hash representation a pairwise similarity matrix is designed to make a balance between accuracy and training expenditure on memory. In addition a parameter boosting strategy is integrated to reinforce the precision with hash bits fusion. Extensive experiments on three large-scale image benchmarks demonstrate the superior performance of RSLH under various short-length hashing scenarios.</p>
</td>
</tr>



<tr>
	<td>2019</td>
	<td><a href="/publications/weng2019online/">Online Hashing With Efficient Updating Of Binary Codes</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Online Hashing With Efficient Updating Of Binary Codes' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Online Hashing With Efficient Updating Of Binary Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Online%20Hashing%20With%20Efficient%20Updating%20Of%20Binary%20Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Weng Zhenyu, Zhu Yuesheng</td>
	<td>Arxiv</td>
	<td><p>Online hashing methods are efficient in learning the hash functions from the streaming data. However when the hash functions change the binary codes for the database have to be recomputed to guarantee the retrieval accuracy. Recomputing the binary codes by accumulating the whole database brings a timeliness challenge to the online retrieval process. In this paper we propose a novel online hashing framework to update the binary codes efficiently without accumulating the whole database. In our framework the hash functions are fixed and the projection functions are introduced to learn online from the streaming data. Therefore inefficient updating of the binary codes by accumulating the whole database can be transformed to efficient updating of the binary codes by projecting the binary codes into another binary space. The queries and the binary code database are projected asymmetrically to further improve the retrieval accuracy. The experiments on two multi-label image databases demonstrate the effectiveness and the efficiency of our method for multi-label image retrieval.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/kaga2019pdh/">PDH Probabilistic Deep Hashing Based On MAP Estimation Of Hamming Distance</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=PDH Probabilistic Deep Hashing Based On MAP Estimation Of Hamming Distance' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=PDH Probabilistic Deep Hashing Based On MAP Estimation Of Hamming Distance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=PDH%20Probabilistic%20Deep%20Hashing%20Based%20On%20MAP%20Estimation%20Of%20Hamming%20Distance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Kaga Yosuke, Fujio Masakazu, Takahashi Kenta, Ohki Tetsushi, Nishigaki Masakatsu</td>
	<td></td>
	<td><p>With the growth of image on the web research on hashing which enables high-speed image retrieval has been actively studied. In recent years various hashing methods based on deep neural networks have been proposed and achieved higher precision than the other hashing methods. In these methods multiple losses for hash codes and the parameters of neural networks are defined. They generate hash codes that minimize the weighted sum of the losses. Therefore an expert has to tune the weights for the losses heuristically and the probabilistic optimality of the loss function cannot be explained. In order to generate explainable hash codes without weight tuning we theoretically derive a single loss function with no hyperparameters for the hash code from the probability distribution of the images. By generating hash codes that minimize this loss function highly accurate image retrieval with probabilistic optimality is performed. We evaluate the performance of hashing using MNIST CIFAR-10 SVHN and show that the proposed method outperforms the state-of-the-art hashing methods.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/kanda2019sketch/">(b)-bit Sketch Trie Scalable Similarity Search On Integer Sketches</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=(b)-bit Sketch Trie Scalable Similarity Search On Integer Sketches' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=(b)-bit Sketch Trie Scalable Similarity Search On Integer Sketches' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=(b)-bit%20Sketch%20Trie%20Scalable%20Similarity%20Search%20On%20Integer%20Sketches' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Kanda Shunsuke, Tabei Yasuo</td>
	<td>Arxiv</td>
	<td><p>Recently randomly mapping vectorial data to strings of discrete symbols (i.e. sketches) for fast and space-efficient similarity searches has become popular. Such random mapping is called similarity-preserving hashing and approximates a similarity metric by using the Hamming distance. Although many efficient similarity searches have been proposed most of them are designed for binary sketches. Similarity searches on integer sketches are in their infancy. In this paper we present a novel space-efficient trie named (b)-bit sketch trie on integer sketches for scalable similarity searches by leveraging the idea behind succinct data structures (i.e. space-efficient data structures while supporting various data operations in the compressed format) and a favorable property of integer sketches as fixed-length strings. Our experimental results obtained using real-world datasets show that a trie-based index is built from integer sketches and efficiently performs similarity searches on the index by pruning useless portions of the search space which greatly improves the search time and space-efficiency of the similarity search. The experimental results show that our similarity search is at most one order of magnitude faster than state-of-the-art similarity searches. Besides our method needs only 10 GiB of memory on a billion-scale database while state-of-the-art similarity searches need 29 GiB of memory.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/ma2019hierarchy/">Hierarchy Neighborhood Discriminative Hashing For An Unified View Of Single-label And Multi-label Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Hierarchy Neighborhood Discriminative Hashing For An Unified View Of Single-label And Multi-label Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Hierarchy Neighborhood Discriminative Hashing For An Unified View Of Single-label And Multi-label Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Hierarchy%20Neighborhood%20Discriminative%20Hashing%20For%20An%20Unified%20View%20Of%20Single-label%20And%20Multi-label%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Ma Lei, Li Hongliang, Wu Qingbo, Meng Fanman, Ngan King Ngi</td>
	<td>Arxiv</td>
	<td><p>Recently deep supervised hashing methods have become popular for large-scale image retrieval task. To preserve the semantic similarity notion between examples they typically utilize the pairwise supervision or the triplet supervised information for hash learning. However these methods usually ignore the semantic class information which can help the improvement of the semantic discriminative ability of hash codes. In this paper we propose a novel hierarchy neighborhood discriminative hashing method. Specifically we construct a bipartite graph to build coarse semantic neighbourhood relationship between the sub-class feature centers and the embeddings features. Moreover we utilize the pairwise supervised information to construct the fined semantic neighbourhood relationship between embeddings features. Finally we propose a hierarchy neighborhood discriminative hashing loss to unify the single-label and multilabel image retrieval problem with a one-stream deep neural network architecture. Experimental results on two largescale datasets demonstrate that the proposed method can outperform the state-of-the-art hashing methods.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/james2019deephashing/">Deephashing Using Tripletloss</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deephashing Using Tripletloss' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deephashing Using Tripletloss' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deephashing%20Using%20Tripletloss' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>James Jithin</td>
	<td>Arxiv</td>
	<td><p>Hashing is one of the most efficient techniques for approximate nearest neighbour search for large scale image retrieval. Most of the techniques are based on hand-engineered features and do not give optimal results all the time. Deep Convolutional Neural Networks have proven to generate very effective representation of images that are used for various computer vision tasks and inspired by this there have been several Deep Hashing models like Wang et al. (2016) have been proposed. These models train on the triplet loss function which can be used to train models with superior representation capabilities. Taking the latest advancements in training using the triplet loss I propose new techniques that help the Deep Hash-ing models train more faster and efficiently. Experiment result1show that using the more efficient techniques for training on the triplet loss we have obtained a 537;percent improvement in our model compared to the original work of Wang et al.(2016). Using a larger model and more training data we can drastically improve the performance using the techniques we propose</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/tan2019drill/">Drill-down Interactive Retrieval Of Complex Scenes Using Natural Language Queries</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Drill-down Interactive Retrieval Of Complex Scenes Using Natural Language Queries' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Drill-down Interactive Retrieval Of Complex Scenes Using Natural Language Queries' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Drill-down%20Interactive%20Retrieval%20Of%20Complex%20Scenes%20Using%20Natural%20Language%20Queries' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Fuwen Tan, Paola Cascante-bonilla, Xiaoxiao Guo, Hui Wu, Song Feng, Vicente Ordonez</td>
	<td>Neural Information Processing Systems</td>
	<td><p>This paper explores the task of interactive image retrieval using natural language queries where a user progressively provides input queries to refine a set of retrieval results. Moreover our work explores this problem in the context of complex image scenes containing multiple objects. We propose Drill-down an effective framework for encoding multiple queries with an efficient compact state representation that significantly extends current methods for single-round image retrieval. We show that using multiple rounds of natural language queries as input can be surprisingly effective to find arbitrarily specific images of complex scenes. Furthermore we find that existing image datasets with textual captions can provide a surprisingly effective form of weak supervision for this task. We compare our method with existing sequential encoding and embedding networks demonstrating superior performance on two proposed benchmarks automatic image retrieval on a simulated scenario that uses region captions as queries and interactive image retrieval using real queries from human evaluators.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/jafari2019efficient/">Efficient Bitmap-based Indexing And Retrieval Of Similarity Search Image Queries</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Bitmap-based Indexing And Retrieval Of Similarity Search Image Queries' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Efficient Bitmap-based Indexing And Retrieval Of Similarity Search Image Queries' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Efficient%20Bitmap-based%20Indexing%20And%20Retrieval%20Of%20Similarity%20Search%20Image%20Queries' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Jafari Omid, Nagarkar Parth, MontaÃ±o Jonathan</td>
	<td></td>
	<td><p>Finding similar images is a necessary operation in many multimedia applications. Images are often represented and stored as a set of high-dimensional features which are extracted using localized feature extraction algorithms. Locality Sensitive Hashing is one of the most popular approximate processing techniques for finding similar points in high-dimensional spaces. Locality Sensitive Hashing (LSH) and its variants are designed to find similar points but they are not designed to find objects (such as images which are made up of a collection of points) efficiently. In this paper we propose an index structure Bitmap-Image LSH (bImageLSH) for efficient processing of high-dimensional images. Using a real dataset we experimentally show the performance benefit of our novel design while keeping the accuracy of the image results high.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/jagadeesan2019understanding/">Understanding Sparse JL For Feature Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Understanding Sparse JL For Feature Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Understanding Sparse JL For Feature Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Understanding%20Sparse%20JL%20For%20Feature%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Meena Jagadeesan</td>
	<td>Neural Information Processing Systems</td>
	<td><p>Feature hashing and other random projection schemes are commonly used to reduce the dimensionality of feature vectors. The goal is to efficiently project a high-dimensional feature vector living in R^n into a much lower-dimensional space R^m while approximately preserving Euclidean norm. These schemes can be constructed using sparse random projections for example using a sparse Johnson-Lindenstrauss (JL) transform. A line of work introduced by Weinberger et. al (ICML 09) analyzes the accuracy of sparse JL with sparsity 1 on feature vectors with small linfinity-to-l2 norm ratio. Recently Freksen Kamma and Larsen (NeurIPS 18) closed this line of work by proving a tight tradeoff between linfinity-to-l2 norm ratio and accuracy for sparse JL with sparsity 1. In this paper we demonstrate the benefits of using sparsity s greater than 1 in sparse JL on feature vectors. Our main result is a tight tradeoff between linfinity-to-l2 norm ratio and accuracy for a general sparsity s that significantly generalizes the result of Freksen et. al. Our result theoretically demonstrates that sparse JL with s 1 can have significantly better norm-preservation properties on feature vectors than sparse JL with s = 1; we also empirically demonstrate this finding.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/yang2019feature/">Feature Pyramid Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Feature Pyramid Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Feature Pyramid Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Feature%20Pyramid%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Yang Yifan, Geng Libing, Lai Hanjiang, Pan Yan, Yin Jian</td>
	<td>Arxiv</td>
	<td><p>In recent years deep-networks-based hashing has become a leading approach for large-scale image retrieval. Most deep hashing approaches use the high layer to extract the powerful semantic representations. However these methods have limited ability for fine-grained image retrieval because the semantic features extracted from the high layer are difficult in capturing the subtle differences. To this end we propose a novel two-pyramid hashing architecture to learn both the semantic information and the subtle appearance details for fine-grained image search. Inspired by the feature pyramids of convolutional neural network a vertical pyramid is proposed to capture the high-layer features and a horizontal pyramid combines multiple low-layer features with structural information to capture the subtle differences. To fuse the low-level features a novel combination strategy called consensus fusion is proposed to capture all subtle information from several low-layers for finer retrieval. Extensive evaluation on two fine-grained datasets CUB-200-2011 and Stanford Dogs demonstrate that the proposed method achieves significant performance compared with the state-of-art baselines.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/wang2019deep/">Deep Policy Hashing Network With Listwise Supervision</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Policy Hashing Network With Listwise Supervision' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Policy Hashing Network With Listwise Supervision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Policy%20Hashing%20Network%20With%20Listwise%20Supervision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Wang Shaoying, Lai Haijiang, Yang Yifan, Yin Jian</td>
	<td>Arxiv</td>
	<td><p>Deep-networks-based hashing has become a leading approach for large-scale image retrieval which learns a similarity-preserving network to map similar images to nearby hash codes. The pairwise and triplet losses are two widely used similarity preserving manners for deep hashing. These manners ignore the fact that hashing is a prediction task on the list of binary codes. However learning deep hashing with listwise supervision is challenging in 1) how to obtain the rank list of whole training set when the batch size of the deep network is always small and 2) how to utilize the listwise supervision. In this paper we present a novel deep policy hashing architecture with two systems are learned in parallel a query network and a shared and slowly changing database network. The following three steps are repeated until convergence 1) the database network encodes all training samples into binary codes to obtain a whole rank list 2) the query network is trained based on policy learning to maximize a reward that indicates the performance of the whole ranking list of binary codes e.g. mean average precision (MAP) and 3) the database network is updated as the query network. Extensive evaluations on several benchmark datasets show that the proposed method brings substantial improvements over state-of-the-art hashing methods.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/morozov2019unsupervised/">Unsupervised Neural Quantization For Compressed-domain Similarity Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Neural Quantization For Compressed-domain Similarity Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Unsupervised Neural Quantization For Compressed-domain Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Unsupervised%20Neural%20Quantization%20For%20Compressed-domain%20Similarity%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Morozov Stanislav, Babenko Artem</td>
	<td>Arxiv</td>
	<td><p>We tackle the problem of unsupervised visual descriptors compression which is a key ingredient of large-scale image retrieval systems. While the deep learning machinery has benefited literally all computer vision pipelines the existing state-of-the-art compression methods employ shallow architectures and we aim to close this gap by our paper. In more detail we introduce a DNN architecture for the unsupervised compressed-domain retrieval based on multi-codebook quantization. The proposed architecture is designed to incorporate both fast data encoding and efficient distances computation via lookup tables. We demonstrate the exceptional advantage of our scheme over existing quantization approaches on several datasets of visual descriptors via outperforming the previous state-of-the-art by a large margin.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/yuan2019central/">Central Similarity Quantization For Efficient Image And Video Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Central Similarity Quantization For Efficient Image And Video Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Central Similarity Quantization For Efficient Image And Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Central%20Similarity%20Quantization%20For%20Efficient%20Image%20And%20Video%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Yuan Li, Wang Tao, Zhang Xiaopeng, Tay Francis Eh, Jie Zequn, Liu Wei, Feng Jiashi</td>
	<td>Arxiv</td>
	<td><p>Existing data-dependent hashing methods usually learn hash functions from pairwise or triplet data relationships which only capture the data similarity locally and often suffer from low learning efficiency and low collision rate. In this work we propose a new emphglobal similarity metric termed as emphcentral similarity with which the hash codes of similar data pairs are encouraged to approach a common center and those for dissimilar pairs to converge to different centers to improve hash learning efficiency and retrieval accuracy. We principally formulate the computation of the proposed central similarity metric by introducing a new concept i.e. emphhash center that refers to a set of data points scattered in the Hamming space with a sufficient mutual distance between each other. We then provide an efficient method to construct well separated hash centers by leveraging the Hadamard matrix and Bernoulli distributions. Finally we propose the Central Similarity Quantization (CSQ) that optimizes the central similarity between data points w.r.t. their hash centers instead of optimizing the local similarity. CSQ is generic and applicable to both image and video hashing scenarios. Extensive experiments on large-scale image and video retrieval tasks demonstrate that CSQ can generate cohesive hash codes for similar data pairs and dispersed hash codes for dissimilar pairs achieving a noticeable boost in retrieval performance i.e. 337;-2037; in mAP over the previous state-of-the-arts. The code is at urlhttps://github.com/yuanli2333/Hadamard-Matrix-for-hashing}</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/lin2019supervised/">Supervised Online Hashing Via Similarity Distribution Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Online Hashing Via Similarity Distribution Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Supervised Online Hashing Via Similarity Distribution Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Supervised%20Online%20Hashing%20Via%20Similarity%20Distribution%20Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Lin Mingbao, Ji Rongrong, Chen Shen, Zheng Feng, Sun Xiaoshuai, Zhang Baochang, Cao Liujuan, Guo Guodong, Huang Feiyue</td>
	<td>Arxiv</td>
	<td><p>Online hashing has attracted extensive research attention when facing streaming data. Most online hashing methods learning binary codes based on pairwise similarities of training instances fail to capture the semantic relationship and suffer from a poor generalization in large-scale applications due to large variations. In this paper we propose to model the similarity distributions between the input data and the hashing codes upon which a novel supervised online hashing method dubbed as Similarity Distribution based Online Hashing (SDOH) is proposed to keep the intrinsic semantic relationship in the produced Hamming space. Specifically we first transform the discrete similarity matrix into a probability matrix via a Gaussian-based normalization to address the extremely imbalanced distribution issue. And then we introduce a scaling Student t-distribution to solve the challenging initialization problem and efficiently bridge the gap between the known and unknown distributions. Lastly we align the two distributions via minimizing the Kullback-Leibler divergence (KL-diverence) with stochastic gradient descent (SGD) by which an intuitive similarity constraint is imposed to update hashing model on the new streaming data with a powerful generalizing ability to the past data. Extensive experiments on three widely-used benchmarks validate the superiority of the proposed SDOH over the state-of-the-art methods in the online retrieval task.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/ertl2019probminhash/">Probminhash -- A Class Of Locality-sensitive Hash Algorithms For The (probability) Jaccard Similarity</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Probminhash -- A Class Of Locality-sensitive Hash Algorithms For The (probability) Jaccard Similarity' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Probminhash -- A Class Of Locality-sensitive Hash Algorithms For The (probability) Jaccard Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Probminhash%20--%20A%20Class%20Of%20Locality-sensitive%20Hash%20Algorithms%20For%20The%20(probability)%20Jaccard%20Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Ertl Otmar</td>
	<td>Arxiv</td>
	<td><p>The probability Jaccard similarity was recently proposed as a natural generalization of the Jaccard similarity to measure the proximity of sets whose elements are associated with relative frequencies or probabilities. In combination with a hash algorithm that maps those weighted sets to compact signatures which allow fast estimation of pairwise similarities it constitutes a valuable method for big data applications such as near-duplicate detection nearest neighbor search or clustering. This paper introduces a class of one-pass locality-sensitive hash algorithms that are orders of magnitude faster than the original approach. The performance gain is achieved by calculating signature components not independently but collectively. Four different algorithms are proposed based on this idea. Two of them are statistically equivalent to the original approach and can be used as drop-in replacements. The other two may even improve the estimation error by introducing statistical dependence between signature components. Moreover the presented techniques can be specialized for the conventional Jaccard similarity resulting in highly efficient algorithms that outperform traditional minwise hashing and that are able to compete with the state of the art.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/garg2019nearly/">Nearly-unsupervised Hashcode Representations For Relation Extraction</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Nearly-unsupervised Hashcode Representations For Relation Extraction' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Nearly-unsupervised Hashcode Representations For Relation Extraction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Nearly-unsupervised%20Hashcode%20Representations%20For%20Relation%20Extraction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Garg Sahil, Galstyan Aram, Steeg Greg Ver, Cecchi Guillermo</td>
	<td>Arxiv</td>
	<td><p>Recently kernelized locality sensitive hashcodes have been successfully employed as representations of natural language text especially showing high relevance to biomedical relation extraction tasks. In this paper we propose to optimize the hashcode representations in a nearly unsupervised manner in which we only use data points but not their class labels for learning. The optimized hashcode representations are then fed to a supervised classifier following the prior work. This nearly unsupervised approach allows fine-grained optimization of each hash function which is particularly suitable for building hashcode representations generalizing from a training set to a test set. We empirically evaluate the proposed approach for biomedical relation extraction tasks obtaining significant accuracy improvements w.r.t. state-of-the-art supervised and semi-supervised approaches.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/wang2019fusion/">Fusion-supervised Deep Cross-modal Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Fusion-supervised Deep Cross-modal Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Fusion-supervised Deep Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Fusion-supervised%20Deep%20Cross-modal%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Wang Li, Zhu Lei, Yu En, Sun Jiande, Zhang Huaxiang</td>
	<td>Arxiv</td>
	<td><p>Deep hashing has recently received attention in cross-modal retrieval for its impressive advantages. However existing hashing methods for cross-modal retrieval cannot fully capture the heterogeneous multi-modal correlation and exploit the semantic information. In this paper we propose a novel emphFusion-supervised Deep Cross-modal Hashing (FDCH) approach. Firstly FDCH learns unified binary codes through a fusion hash network with paired samples as input which effectively enhances the modeling of the correlation of heterogeneous multi-modal data. Then these high-quality unified hash codes further supervise the training of the modality-specific hash networks for encoding out-of-sample queries. Meanwhile both pair-wise similarity information and classification information are embedded in the hash networks under one stream framework which simultaneously preserves cross-modal similarity and keeps semantic consistency. Experimental results on two benchmark datasets demonstrate the state-of-the-art performance of FDCH.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/do2019simultaneous/">Simultaneous Feature Aggregating And Hashing For Compact Binary Code Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Simultaneous Feature Aggregating And Hashing For Compact Binary Code Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Simultaneous Feature Aggregating And Hashing For Compact Binary Code Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Simultaneous%20Feature%20Aggregating%20And%20Hashing%20For%20Compact%20Binary%20Code%20Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Do Thanh-toan, Le Khoa, Hoang Tuan, Le Huu, Nguyen Tam V., Cheung Ngai-man</td>
	<td>Arxiv</td>
	<td><p>Representing images by compact hash codes is an attractive approach for large-scale content-based image retrieval. In most state-of-the-art hashing-based image retrieval systems for each image local descriptors are first aggregated as a global representation vector. This global vector is then subjected to a hashing function to generate a binary hash code. In previous works the aggregating and the hashing processes are designed independently. Hence these frameworks may generate suboptimal hash codes. In this paper we first propose a novel unsupervised hashing framework in which feature aggregating and hashing are designed simultaneously and optimized jointly. Specifically our joint optimization generates aggregated representations that can be better reconstructed by some binary codes. This leads to more discriminative binary hash codes and improved retrieval accuracy. In addition the proposed method is flexible. It can be extended for supervised hashing. When the data label is available the framework can be adapted to learn binary codes which minimize the reconstruction loss w.r.t. label vectors. Furthermore we also propose a fast version of the state-of-the-art hashing method Binary Autoencoder to be used in our proposed frameworks. Extensive experiments on benchmark datasets under various settings show that the proposed methods outperform state-of-the-art unsupervised and supervised hashing methods.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/markchit2019effective/">Effective And Efficient Indexing In Cross-modal Hashing-based Datasets</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Effective And Efficient Indexing In Cross-modal Hashing-based Datasets' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Effective And Efficient Indexing In Cross-modal Hashing-based Datasets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Effective%20And%20Efficient%20Indexing%20In%20Cross-modal%20Hashing-based%20Datasets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Markchit Sarawut, Chiu Chih-yi</td>
	<td>Arxiv</td>
	<td><p>To overcome the barrier of storage and computation the hashing technique has been widely used for nearest neighbor search in multimedia retrieval applications recently. Particularly cross-modal retrieval that searches across different modalities becomes an active but challenging problem. Although dozens of cross-modal hashing algorithms are proposed to yield compact binary codes the exhaustive search is impractical for the real-time purpose and Hamming distance computation suffers inaccurate results. In this paper we propose a novel search method that utilizes a probability-based index scheme over binary hash codes in cross-modal retrieval. The proposed hash code indexing scheme exploits a few binary bits of the hash code as the index code. We construct an inverted index table based on index codes and train a neural network to improve the indexing accuracy and efficiency. Experiments are performed on two benchmark datasets for retrieval across image and text modalities where hash codes are generated by three cross-modal hashing methods. Results show the proposed method effectively boost the performance on these hash methods.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/wang2019cluster/">Cluster-wise Unsupervised Hashing For Cross-modal Similarity Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Cluster-wise Unsupervised Hashing For Cross-modal Similarity Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Cluster-wise Unsupervised Hashing For Cross-modal Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Cluster-wise%20Unsupervised%20Hashing%20For%20Cross-modal%20Similarity%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Wang Lu, Yang Jie</td>
	<td>Arxiv</td>
	<td><p>Large-scale cross-modal hashing similarity retrieval has attracted more and more attention in modern search applications such as search engines and autopilot showing great superiority in computation and storage. However current unsupervised cross-modal hashing methods still have some limitations (1)many methods relax the discrete constraints to solve the optimization objective which may significantly degrade the retrieval performance;(2)most existing hashing model project heterogenous data into a common latent space which may always lose sight of diversity in heterogenous data;(3)transforming real-valued data point to binary codes always results in abundant loss of information producing the suboptimal continuous latent space. To overcome above problems in this paper a novel Cluster-wise Unsupervised Hashing (CUH) method is proposed. Specifically CUH jointly performs the multi-view clustering that projects the original data points from different modalities into its own low-dimensional latent semantic space and finds the cluster centroid points and the common clustering indicators in its own low-dimensional space and learns the compact hash codes and the corresponding linear hash functions. An discrete optimization framework is developed to learn the unified binary codes across modalities under the guidance cluster-wise code-prototypes. The reasonableness and effectiveness of CUH is well demonstrated by comprehensive experiments on diverse benchmark datasets.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/deng2019triplet/">Triplet-based Deep Hashing Network For Cross-modal Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Triplet-based Deep Hashing Network For Cross-modal Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Triplet-based Deep Hashing Network For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Triplet-based%20Deep%20Hashing%20Network%20For%20Cross-modal%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Deng Cheng, Chen Zhaojia, Liu Xianglong, Gao Xinbo, Tao Dacheng</td>
	<td>Arxiv</td>
	<td><p>Given the benefits of its low storage requirements and high retrieval efficiency hashing has recently received increasing attention. In particularcross-modal hashing has been widely and successfully used in multimedia similarity search applications. However almost all existing methods employing cross-modal hashing cannot obtain powerful hash codes due to their ignoring the relative similarity between heterogeneous data that contains richer semantic information leading to unsatisfactory retrieval performance. In this paper we propose a triplet-based deep hashing (TDH) network for cross-modal retrieval. First we utilize the triplet labels which describes the relative relationships among three instances as supervision in order to capture more general semantic correlations between cross-modal instances. We then establish a loss function from the inter-modal view and the intra-modal view to boost the discriminative abilities of the hash codes. Finally graph regularization is introduced into our proposed TDH method to preserve the original semantic similarity between hash codes in Hamming space. Experimental results show that our proposed method outperforms several state-of-the-art approaches on two popular cross-modal datasets.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/valsesia2019analysis/">Analysis Of Sparsehash An Efficient Embedding Of Set-similarity Via Sparse Projections</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Analysis Of Sparsehash An Efficient Embedding Of Set-similarity Via Sparse Projections' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Analysis Of Sparsehash An Efficient Embedding Of Set-similarity Via Sparse Projections' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Analysis%20Of%20Sparsehash%20An%20Efficient%20Embedding%20Of%20Set-similarity%20Via%20Sparse%20Projections' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Valsesia Diego, Fosson Sophie Marie, Ravazzi Chiara, Bianchi Tiziano, Magli Enrico</td>
	<td>Arxiv</td>
	<td><p>Embeddings provide compact representations of signals in order to perform efficient inference in a wide variety of tasks. In particular random projections are common tools to construct Euclidean distance-preserving embeddings while hashing techniques are extensively used to embed set-similarity metrics such as the Jaccard coefficient. In this letter we theoretically prove that a class of random projections based on sparse matrices called SparseHash can preserve the Jaccard coefficient between the supports of sparse signals which can be used to estimate set similarities. Moreover besides the analysis we provide an efficient implementation and we test the performance in several numerical experiments both on synthetic and real datasets.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/tian2019global/">Global Hashing System For Fast Image Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Global Hashing System For Fast Image Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Global Hashing System For Fast Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Global%20Hashing%20System%20For%20Fast%20Image%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Tian Dayong, Tao Dacheng</td>
	<td>Arxiv</td>
	<td><p>Hashing methods have been widely investigated for fast approximate nearest neighbor searching in large data sets. Most existing methods use binary vectors in lower dimensional spaces to represent data points that are usually real vectors of higher dimensionality. We divide the hashing process into two steps. Data points are first embedded in a low-dimensional space and the global positioning system method is subsequently introduced but modified for binary embedding. We devise dataindependent and data-dependent methods to distribute the satellites at appropriate locations. Our methods are based on finding the tradeoff between the information losses in these two steps. Experiments show that our data-dependent method outperforms other methods in different-sized data sets from 100k to 10M. By incorporating the orthogonality of the code matrix both our data-independent and data-dependent methods are particularly impressive in experiments on longer bits.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/zhang2019joint/">Joint Cluster Unary Loss For Efficient Cross-modal Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Joint Cluster Unary Loss For Efficient Cross-modal Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Joint Cluster Unary Loss For Efficient Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Joint%20Cluster%20Unary%20Loss%20For%20Efficient%20Cross-modal%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhang Shifeng, Li Jianmin, Zhang Bo</td>
	<td>Arxiv</td>
	<td><p>With the rapid growth of various types of multimodal data cross-modal deep hashing has received broad attention for solving cross-modal retrieval problems efficiently. Most cross-modal hashing methods follow the traditional supervised hashing framework in which the (O(n^2)) data pairs and (O(n^3)) data triplets are generated for training but the training procedure is less efficient because the complexity is high for large-scale dataset. To address these issues we propose a novel and efficient cross-modal hashing algorithm in which the unary loss is introduced. First of all We introduce the Cross-Modal Unary Loss (CMUL) with (O(n)) complexity to bridge the traditional triplet loss and classification-based unary loss. A more accurate bound of the triplet loss for structured multilabel data is also proposed in CMUL. Second we propose the novel Joint Cluster Cross-Modal Hashing (JCCH) algorithm for efficient hash learning in which the CMUL is involved. The resultant hashcodes form several clusters in which the hashcodes in the same cluster share similar semantic information and the heterogeneity gap on different modalities is diminished by sharing the clusters. The proposed algorithm is able to be applied to various types of data and experiments on large-scale datasets show that the proposed method is superior over or comparable with state-of-the-art cross-modal hashing methods and training with the proposed method is more efficient than others.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/yu2019unsupervised/">Unsupervised Multi-modal Hashing For Cross-modal Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Multi-modal Hashing For Cross-modal Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Unsupervised Multi-modal Hashing For Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Unsupervised%20Multi-modal%20Hashing%20For%20Cross-modal%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Yu Jun, Wu Xiao-jun</td>
	<td>Arxiv</td>
	<td><p>With the advantage of low storage cost and high efficiency hashing learning has received much attention in the domain of Big Data. In this paper we propose a novel unsupervised hashing learning method to cope with this open problem to directly preserve the manifold structure by hashing. To address this problem both the semantic correlation in textual space and the locally geometric structure in the visual space are explored simultaneously in our framework. Besides the 2;1-norm constraint is imposed on the projection matrices to learn the discriminative hash function for each modality. Extensive experiments are performed to evaluate the proposed method on the three publicly available datasets and the experimental results show that our method can achieve superior performance over the state-of-the-art methods.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/liu2019deep/">Deep Triplet Quantization</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Triplet Quantization' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Triplet Quantization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Triplet%20Quantization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Liu Bin, Cao Yue, Long Mingsheng, Wang Jianmin, Wang Jingdong</td>
	<td>Arxiv</td>
	<td><p>Deep hashing establishes efficient and effective image retrieval by end-to-end learning of deep representations and hash codes from similarity data. We present a compact coding solution focusing on deep learning to quantization approach that has shown superior performance over hashing solutions for similarity retrieval. We propose Deep Triplet Quantization (DTQ) a novel approach to learning deep quantization models from the similarity triplets. To enable more effective triplet training we design a new triplet selection approach Group Hard that randomly selects hard triplets in each image group. To generate compact binary codes we further apply a triplet quantization with weak orthogonality during triplet training. The quantization loss reduces the codebook redundancy and enhances the quantizability of deep representations through back-propagation. Extensive experiments demonstrate that DTQ can generate high-quality and compact binary codes which yields state-of-the-art image retrieval performance on three benchmark datasets NUS-WIDE CIFAR-10 and MS-COCO.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/zhang2019sadih/">SADIH Semantic-aware Discrete Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=SADIH Semantic-aware Discrete Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=SADIH Semantic-aware Discrete Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=SADIH%20Semantic-aware%20Discrete%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhang Zheng, Xie Guo-sen, Li Yang, Li Sheng, Huang Zi</td>
	<td>Arxiv</td>
	<td><p>Due to its low storage cost and fast query speed hashing has been recognized to accomplish similarity search in large-scale multimedia retrieval applications. Particularly supervised hashing has recently received considerable research attention by leveraging the label information to preserve the pairwise similarities of data points in the Hamming space. However there still remain two crucial bottlenecks 1) the learning process of the full pairwise similarity preservation is computationally unaffordable and unscalable to deal with big data; 2) the available category information of data are not well-explored to learn discriminative hash functions. To overcome these challenges we propose a unified Semantic-Aware DIscrete Hashing (SADIH) framework which aims to directly embed the transformed semantic information into the asymmetric similarity approximation and discriminative hashing function learning. Specifically a semantic-aware latent embedding is introduced to asymmetrically preserve the full pairwise similarities while skillfully handle the cumbersome n times n pairwise similarity matrix. Meanwhile a semantic-aware autoencoder is developed to jointly preserve the data structures in the discriminative latent semantic space and perform data reconstruction. Moreover an efficient alternating optimization algorithm is proposed to solve the resulting discrete optimization problem. Extensive experimental results on multiple large-scale datasets demonstrate that our SADIH can clearly outperform the state-of-the-art baselines with the additional benefit of lower computational costs.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/zhang2019pairwise/">Pairwise Teacher-student Network For Semi-supervised Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Pairwise Teacher-student Network For Semi-supervised Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Pairwise Teacher-student Network For Semi-supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Pairwise%20Teacher-student%20Network%20For%20Semi-supervised%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhang Shifeng, Li Jianmin, Zhang Bo</td>
	<td>Arxiv</td>
	<td><p>Hashing method maps similar high-dimensional data to binary hashcodes with smaller hamming distance and it has received broad attention due to its low storage cost and fast retrieval speed. Pairwise similarity is easily obtained and widely used for retrieval and most supervised hashing algorithms are carefully designed for the pairwise supervisions. As labeling all data pairs is difficult semi-supervised hashing is proposed which aims at learning efficient codes with limited labeled pairs and abundant unlabeled ones. Existing methods build graphs to capture the structure of dataset but they are not working well for complex data as the graph is built based on the data representations and determining the representations of complex data is difficult. In this paper we propose a novel teacher-student semi-supervised hashing framework in which the student is trained with the pairwise information produced by the teacher network. The network follows the smoothness assumption which achieves consistent distances for similar data pairs so that the retrieval results are similar for neighborhood queries. Experiments on large-scale datasets show that the proposed method reaches impressive gain over the supervised baselines and is superior to state-of-the-art semi-supervised hashing methods.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/chen2019hadamard/">Hadamard Codebook Based Deep Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Hadamard Codebook Based Deep Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Hadamard Codebook Based Deep Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Hadamard%20Codebook%20Based%20Deep%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Chen Shen, Cao Liujuan, Lin Mingbao, Wang Yan, Sun Xiaoshuai, Wu Chenglin, Qiu Jingfei, Ji Rongrong</td>
	<td>Arxiv</td>
	<td><p>As an approximate nearest neighbor search technique hashing has been widely applied in large-scale image retrieval due to its excellent efficiency. Most supervised deep hashing methods have similar loss designs with embedding learning while quantizing the continuous high-dim feature into compact binary space. We argue that the existing deep hashing schemes are defective in two issues that seriously affect the performance i.e. bit independence and bit balance. The former refers to hash codes of different classes should be independent of each other while the latter means each bit should have a balanced distribution of +1s and -1s. In this paper we propose a novel supervised deep hashing method termed Hadamard Codebook based Deep Hashing (HCDH) which solves the above two problems in a unified formulation. Specifically we utilize an off-the-shelf algorithm to generate a binary Hadamard codebook to satisfy the requirement of bit independence and bit balance which subsequently serves as the desired outputs of the hash functions learning. We also introduce a projection matrix to solve the inconsistency between the order of Hadamard matrix and the number of classes. Besides the proposed HCDH further exploits the supervised labels by constructing a classifier on top of the outputs of hash functions. Extensive experiments demonstrate that HCDH can yield discriminative and balanced binary codes which well outperforms many state-of-the-arts on three widely-used benchmarks.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/zhang2019semantic/">Semantic Hierarchy Preserving Deep Hashing For Large-scale Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Semantic Hierarchy Preserving Deep Hashing For Large-scale Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Semantic Hierarchy Preserving Deep Hashing For Large-scale Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Semantic%20Hierarchy%20Preserving%20Deep%20Hashing%20For%20Large-scale%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhang Ming, Zhe Xuefei, Ou-yang Le, Chen Shifeng, Yan Hong</td>
	<td>Arxiv</td>
	<td><p>Deep hashing models have been proposed as an efficient method for large-scale similarity search. However most existing deep hashing methods only utilize fine-level labels for training while ignoring the natural semantic hierarchy structure. This paper presents an effective method that preserves the classwise similarity of full-level semantic hierarchy for large-scale image retrieval. Experiments on two benchmark datasets show that our method helps improve the fine-level retrieval performance. Moreover with the help of the semantic hierarchy it can produce significantly better binary codes for hierarchical retrieval which indicates its potential of providing more user-desired retrieval results.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/chen2019locality/">Locality-sensitive Hashing For F-divergences Mutual Information Loss And Beyond</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Locality-sensitive Hashing For F-divergences Mutual Information Loss And Beyond' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Locality-sensitive Hashing For F-divergences Mutual Information Loss And Beyond' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Locality-sensitive%20Hashing%20For%20F-divergences%20Mutual%20Information%20Loss%20And%20Beyond' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Lin Chen, Hossein Esfandiari, Gang Fu, Vahab Mirrokni</td>
	<td>Neural Information Processing Systems</td>
	<td><p>Computing approximate nearest neighbors in high dimensional spaces is a central problem in large-scale data mining with a wide range of applications in machine learning and data science. A popular and effective technique in computing nearest neighbors approximately is the locality-sensitive hashing (LSH) scheme. In this paper we aim to develop LSH schemes for distance functions that measure the distance between two probability distributions particularly for f-divergences as well as a generalization to capture mutual information loss. First we provide a general framework to design LHS schemes for f-divergence distance functions and develop LSH schemes for the generalized Jensen-Shannon divergence and triangular discrimination in this framework. We show a two-sided approximation result for approximation of the generalized Jensen-Shannon divergence by the Hellinger distance which may be of independent interest. Next we show a general method of reducing the problem of designing an LSH scheme for a Krein kernel (which can be expressed as the difference of two positive definite kernels) to the problem of maximum inner product search. We exemplify this method by applying it to the mutual information loss due to its several important applications such as model compression.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/liu2019cross/">Cross-modal Zero-shot Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Cross-modal Zero-shot Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Cross-modal Zero-shot Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Cross-modal%20Zero-shot%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Liu Xuanwu, Li Zhao, Wang Jun, Yu Guoxian, Domeniconi Carlotta, Zhang Xiangliang</td>
	<td>Arxiv</td>
	<td><p>Hashing has been widely studied for big data retrieval due to its low storage cost and fast query speed. Zero-shot hashing (ZSH) aims to learn a hashing model that is trained using only samples from seen categories but can generalize well to samples of unseen categories. ZSH generally uses category attributes to seek a semantic embedding space to transfer knowledge from seen categories to unseen ones. As a result it may perform poorly when labeled data are insufficient. ZSH methods are mainly designed for single-modality data which prevents their application to the widely spread multi-modal data. On the other hand existing cross-modal hashing solutions assume that all the modalities share the same category labels while in practice the labels of different data modalities may be different. To address these issues we propose a general Cross-modal Zero-shot Hashing (CZHash) solution to effectively leverage unlabeled and labeled multi-modality data with different label spaces. CZHash first quantifies the composite similarity between instances using label and feature information. It then defines an objective function to achieve deep feature learning compatible with the composite similarity preserving category attribute space learning and hashing coding function learning. CZHash further introduces an alternative optimization procedure to jointly optimize these learning objectives. Experiments on benchmark multi-modal datasets show that CZHash significantly outperforms related representative hashing approaches both on effectiveness and adaptability.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/zou2019transductive/">Transductive Zero-shot Hashing For Multilabel Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Transductive Zero-shot Hashing For Multilabel Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Transductive Zero-shot Hashing For Multilabel Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Transductive%20Zero-shot%20Hashing%20For%20Multilabel%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zou Qin, Zhang Zheng, Cao Ling, Chen Long, Wang Song</td>
	<td>IEEE Transactions on Neural Networks and Learning Systems</td>
	<td><p>Hash coding has been widely used in approximate nearest neighbor search for large-scale image retrieval. Given semantic annotations such as class labels and pairwise similarities of the training data hashing methods can learn and generate effective and compact binary codes. While some newly introduced images may contain undefined semantic labels which we call unseen images zeor-shot hashing techniques have been studied. However existing zeor-shot hashing methods focus on the retrieval of single-label images and cannot handle multi-label images. In this paper for the first time a novel transductive zero-shot hashing method is proposed for multi-label unseen image retrieval. In order to predict the labels of the unseen/target data a visual-semantic bridge is built via instance-concept coherence ranking on the seen/source data. Then pairwise similarity loss and focal quantization loss are constructed for training a hashing model using both the seen/source and unseen/target data. Extensive evaluations on three popular multi-label datasets demonstrate that the proposed hashing method achieves significantly better results than the competing methods.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/zhu2019exploring/">Exploring Auxiliary Context Discrete Semantic Transfer Hashing For Scalable Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Exploring Auxiliary Context Discrete Semantic Transfer Hashing For Scalable Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Exploring Auxiliary Context Discrete Semantic Transfer Hashing For Scalable Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Exploring%20Auxiliary%20Context%20Discrete%20Semantic%20Transfer%20Hashing%20For%20Scalable%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhu Lei, Huang Zi, Li Zhihui, Xie Liang, Shen Heng Tao</td>
	<td>Arxiv</td>
	<td><p>Unsupervised hashing can desirably support scalable content-based image retrieval (SCBIR) for its appealing advantages of semantic label independence memory and search efficiency. However the learned hash codes are embedded with limited discriminative semantics due to the intrinsic limitation of image representation. To address the problem in this paper we propose a novel hashing approach dubbed as emphDiscrete Semantic Transfer Hashing (DSTH). The key idea is to emphdirectly augment the semantics of discrete image hash codes by exploring auxiliary contextual modalities. To this end a unified hashing framework is formulated to simultaneously preserve visual similarities of images and perform semantic transfer from contextual modalities. Further to guarantee direct semantic transfer and avoid information loss we explicitly impose the discrete constraint bitâ€“uncorrelation constraint and bit-balance constraint on hash codes. A novel and effective discrete optimization method based on augmented Lagrangian multiplier is developed to iteratively solve the optimization problem. The whole learning process has linear computation complexity and desirable scalability. Experiments on three benchmark datasets demonstrate the superiority of DSTH compared with several state-of-the-art approaches.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/arponen2019shrewd/">SHREWD Semantic Hierarchy-based Relational Embeddings For Weakly-supervised Deep Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=SHREWD Semantic Hierarchy-based Relational Embeddings For Weakly-supervised Deep Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=SHREWD Semantic Hierarchy-based Relational Embeddings For Weakly-supervised Deep Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=SHREWD%20Semantic%20Hierarchy-based%20Relational%20Embeddings%20For%20Weakly-supervised%20Deep%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Arponen Heikki, Bishop Tom E</td>
	<td>Arxiv</td>
	<td><p>Using class labels to represent class similarity is a typical approach to training deep hashing systems for retrieval; samples from the same or different classes take binary 1 or 0 similarity values. This similarity does not model the full rich knowledge of semantic relations that may be present between data points. In this work we build upon the idea of using semantic hierarchies to form distance metrics between all available sample labels; for example cat to dog has a smaller distance than cat to guitar. We combine this type of semantic distance into a loss function to promote similar distances between the deep neural network embeddings. We also introduce an empirical Kullback-Leibler divergence loss term to promote binarization and uniformity of the embeddings. We test the resulting SHREWD method and demonstrate improvements in hierarchical retrieval scores using compact binary hash codes instead of real valued ones and show that in a weakly supervised hashing setting we are able to learn competitively without explicitly relying on class labels but instead on similarities between labels.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/liu2019ranking/">Ranking-based Deep Cross-modal Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Ranking-based Deep Cross-modal Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Ranking-based Deep Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Ranking-based%20Deep%20Cross-modal%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Liu Xuanwu, Yu Guoxian, Domeniconi Carlotta, Wang Jun, Ren Yazhou, Guo Maozu</td>
	<td>Arxiv</td>
	<td><p>Cross-modal hashing has been receiving increasing interests for its low storage cost and fast query speed in multi-modal data retrievals. However most existing hashing methods are based on hand-crafted or raw level features of objects which may not be optimally compatible with the coding process. Besides these hashing methods are mainly designed to handle simple pairwise similarity. The complex multilevel ranking semantic structure of instances associated with multiple labels has not been well explored yet. In this paper we propose a ranking-based deep cross-modal hashing approach (RDCMH). RDCMH firstly uses the feature and label information of data to derive a semi-supervised semantic ranking list. Next to expand the semantic representation power of hand-crafted features RDCMH integrates the semantic ranking information into deep cross-modal hashing and jointly optimizes the compatible parameters of deep feature representations and of hashing functions. Experiments on real multi-modal datasets show that RDCMH outperforms other competitive baselines and achieves the state-of-the-art performance in cross-modal retrieval applications.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/yao2019efficient/">Efficient Discrete Supervised Hashing For Large-scale Cross-modal Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Discrete Supervised Hashing For Large-scale Cross-modal Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Efficient Discrete Supervised Hashing For Large-scale Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Efficient%20Discrete%20Supervised%20Hashing%20For%20Large-scale%20Cross-modal%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Yao Tao, Kong Xiangwei, Yan Lianshan, Tang Wenjing, Tian Qi</td>
	<td>Arxiv</td>
	<td><p>Supervised cross-modal hashing has gained increasing research interest on large-scale retrieval task owning to its satisfactory performance and efficiency. However it still has some challenging issues to be further studied 1) most of them fail to well preserve the semantic correlations in hash codes because of the large heterogenous gap; 2) most of them relax the discrete constraint on hash codes leading to large quantization error and consequent low performance; 3) most of them suffer from relatively high memory cost and computational complexity during training procedure which makes them unscalable. In this paper to address above issues we propose a supervised cross-modal hashing method based on matrix factorization dubbed Efficient Discrete Supervised Hashing (EDSH). Specifically collective matrix factorization on heterogenous features and semantic embedding with class labels are seamlessly integrated to learn hash codes. Therefore the feature based similarities and semantic correlations can be both preserved in hash codes which makes the learned hash codes more discriminative. Then an efficient discrete optimal algorithm is proposed to handle the scalable issue. Instead of learning hash codes bit-by-bit hash codes matrix can be obtained directly which is more efficient. Extensive experimental results on three public real-world datasets demonstrate that EDSH produces a superior performance in both accuracy and scalability over some existing cross-modal hashing methods.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/liu2019guided/">Guided Similarity Separation For Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Guided Similarity Separation For Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Guided Similarity Separation For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Guided%20Similarity%20Separation%20For%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Chundi Liu, Guangwei Yu, Maksims Volkovs, Cheng Chang, Himanshu Rai, Junwei Ma, Satya Krishna Gorti</td>
	<td>Neural Information Processing Systems</td>
	<td><p>Despite recent progress in computer vision image retrieval remains a challenging open problem. Numerous variations such as view angle lighting and occlusion make it difficult to design models that are both robust and efficient. Many leading methods traverse the nearest neighbor graph to exploit higher order neighbor information and uncover the highly complex underlying manifold. In this work we propose a different approach where we leverage graph convolutional networks to directly encode neighbor information into image descriptors. We further leverage ideas from clustering and manifold learning and introduce an unsupervised loss based on pairwise separation of image similarities. Empirically we demonstrate that our model is able to successfully learn a new descriptor space that significantly improves retrieval accuracy while still allowing efficient inner product inference. Experiments on five public benchmarks show highly competitive performance with up to 2437; relative improvement in mAP over leading baselines. Full code for this work is available here https://github.com/layer6ai-labs/GSS.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/roy2019metric/">Metric-learning Based Deep Hashing Network For Content Based Retrieval Of Remote Sensing Images</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Metric-learning Based Deep Hashing Network For Content Based Retrieval Of Remote Sensing Images' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Metric-learning Based Deep Hashing Network For Content Based Retrieval Of Remote Sensing Images' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Metric-learning%20Based%20Deep%20Hashing%20Network%20For%20Content%20Based%20Retrieval%20Of%20Remote%20Sensing%20Images' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Roy Subhankar, Sangineto Enver, Demir BegÃ¼m, Sebe Nicu</td>
	<td>Arxiv</td>
	<td><p>Hashing methods have been recently found very effective in retrieval of remote sensing (RS) images due to their computational efficiency and fast search speed. The traditional hashing methods in RS usually exploit hand-crafted features to learn hash functions to obtain binary codes which can be insufficient to optimally represent the information content of RS images. To overcome this problem in this paper we introduce a metric-learning based hashing network which learns 1) a semantic-based metric space for effective feature representation; and 2) compact binary hash codes for fast archive search. Our network considers an interplay of multiple loss functions that allows to jointly learn a metric based semantic space facilitating similar images to be clustered together in that target space and at the same time producing compact final activations that lose negligible information when binarized. Experiments carried out on two benchmark RS archives point out that the proposed network significantly improves the retrieval performance under the same retrieval time when compared to the state-of-the-art hashing methods in RS.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/li2019re/">Re-randomized Densification For One Permutation Hashing And Bin-wise Consistent Weighted Sampling</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Re-randomized Densification For One Permutation Hashing And Bin-wise Consistent Weighted Sampling' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Re-randomized Densification For One Permutation Hashing And Bin-wise Consistent Weighted Sampling' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Re-randomized%20Densification%20For%20One%20Permutation%20Hashing%20And%20Bin-wise%20Consistent%20Weighted%20Sampling' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Ping Li, Xiaoyun Li, Cun-hui Zhang</td>
	<td>Neural Information Processing Systems</td>
	<td><p>Jaccard similarity is widely used as a distance measure in many machine learning and search applications. Typically hashing methods are essential for the use of Jaccard similarity to be practical in large-scale settings. For hashing binary (0/1) data the idea of one permutation hashing (OPH) with densification significantly accelerates traditional minwise hashing algorithms while providing unbiased and accurate estimates. In this paper we propose a strategy named re-randomization in the process of densification that could achieve the smallest variance among all densification schemes. The success of this idea naturally inspires us to generalize one permutation hashing to weighted (non-binary) data which results in the socalled bin-wise consistent weighted sampling (BCWS) algorithm. We analyze the behavior of BCWS and compare it with a recent alternative. Extensive experiments on various datasets illustrates the effectiveness of our proposed methods.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/schlemper2019deep/">Deep Hashing Using Entropy Regularised Product Quantisation Network</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Hashing Using Entropy Regularised Product Quantisation Network' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Hashing Using Entropy Regularised Product Quantisation Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Hashing%20Using%20Entropy%20Regularised%20Product%20Quantisation%20Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Schlemper Jo, Caballero Jose, Aitken Andy, Van Amersfoort Joost</td>
	<td>Arxiv</td>
	<td><p>In large scale systems approximate nearest neighbour search is a crucial algorithm to enable efficient data retrievals. Recently deep learning-based hashing algorithms have been proposed as a promising paradigm to enable data dependent schemes. Often their efficacy is only demonstrated on data sets with fixed limited numbers of classes. In practical scenarios those labels are not always available or one requires a method that can handle a higher input variability as well as a higher granularity. To fulfil those requirements we look at more flexible similarity measures. In this work we present a novel flexible end-to-end trainable network for large-scale data hashing. Our method works by transforming the data distribution to behave as a uniform distribution on a product of spheres. The transformed data is subsequently hashed to a binary form in a way that maximises entropy of the output (i.e. to fully utilise the available bit-rate capacity) while maintaining the correctness (i.e. close items hash to the same key in the map). We show that the method outperforms baseline approaches such as locality-sensitive hashing and product quantisation in the limited capacity regime.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/zeng2019simultaneous/">Simultaneous Region Localization And Hash Coding For Fine-grained Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Simultaneous Region Localization And Hash Coding For Fine-grained Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Simultaneous Region Localization And Hash Coding For Fine-grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Simultaneous%20Region%20Localization%20And%20Hash%20Coding%20For%20Fine-grained%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zeng Haien, Lai Hanjiang, Yin Jian</td>
	<td>Arxiv</td>
	<td><p>Fine-grained image hashing is a challenging problem due to the difficulties of discriminative region localization and hash code generation. Most existing deep hashing approaches solve the two tasks independently. While these two tasks are correlated and can reinforce each other. In this paper we propose a deep fine-grained hashing to simultaneously localize the discriminative regions and generate the efficient binary codes. The proposed approach consists of a region localization module and a hash coding module. The region localization module aims to provide informative regions to the hash coding module. The hash coding module aims to generate effective binary codes and give feedback for learning better localizer. Moreover to better capture subtle differences multi-scale regions at different layers are learned without the need of bounding-box/part annotations. Extensive experiments are conducted on two public benchmark fine-grained datasets. The results demonstrate significant improvements in the performance of our method relative to other fine-grained hashing algorithms.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/passalis2019deep/">Deep Supervised Hashing Leveraging Quadratic Spherical Mutual Information For Content-based Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Supervised Hashing Leveraging Quadratic Spherical Mutual Information For Content-based Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Supervised Hashing Leveraging Quadratic Spherical Mutual Information For Content-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Supervised%20Hashing%20Leveraging%20Quadratic%20Spherical%20Mutual%20Information%20For%20Content-based%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Passalis Nikolaos, Tefas Anastasios</td>
	<td>Arxiv</td>
	<td><p>Several deep supervised hashing techniques have been proposed to allow for efficiently querying large image databases. However deep supervised image hashing techniques are developed to a great extent heuristically often leading to suboptimal results. Contrary to this we propose an efficient deep supervised hashing algorithm that optimizes the learned codes using an information-theoretic measure the Quadratic Mutual Information (QMI). The proposed method is adapted to the needs of large-scale hashing and information retrieval leading to a novel information-theoretic measure the Quadratic Spherical Mutual Information (QSMI). Apart from demonstrating the effectiveness of the proposed method under different scenarios and outperforming existing state-of-the-art image hashing techniques this paper provides a structured way to model the process of information retrieval and develop novel methods adapted to the needs of each application.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/liu2019weakly/">Weakly-paired Cross-modal Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Weakly-paired Cross-modal Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Weakly-paired Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Weakly-paired%20Cross-modal%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Liu Xuanwu, Wang Jun, Yu Guoxian, Domeniconi Carlotta, Zhang Xiangliang</td>
	<td>Arxiv</td>
	<td><p>Hashing has been widely adopted for large-scale data retrieval in many domains due to its low storage cost and high retrieval speed. Existing cross-modal hashing methods optimistically assume that the correspondence between training samples across modalities are readily available. This assumption is unrealistic in practical applications. In addition these methods generally require the same number of samples across different modalities which restricts their flexibility. We propose a flexible cross-modal hashing approach (Flex-CMH) to learn effective hashing codes from weakly-paired data whose correspondence across modalities are partially (or even totally) unknown. FlexCMH first introduces a clustering-based matching strategy to explore the local structure of each cluster and thus to find the potential correspondence between clusters (and samples therein) across modalities. To reduce the impact of an incomplete correspondence it jointly optimizes in a unified objective function the potential correspondence the cross-modal hashing functions derived from the correspondence and a hashing quantitative loss. An alternative optimization technique is also proposed to coordinate the correspondence and hash functions and to reinforce the reciprocal effects of the two objectives. Experiments on publicly multi-modal datasets show that FlexCMH achieves significantly better results than state-of-the-art methods and it indeed offers a high degree of flexibility for practical cross-modal hashing tasks.</p>
</td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/liu2019query/">Query-adaptive Hash Code Ranking For Large-scale Multi-view Visual Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Query-adaptive Hash Code Ranking For Large-scale Multi-view Visual Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Query-adaptive Hash Code Ranking For Large-scale Multi-view Visual Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Query-adaptive%20Hash%20Code%20Ranking%20For%20Large-scale%20Multi-view%20Visual%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Liu Xianglong, Huang Lei, Deng Cheng, Lang Bo, Tao Dacheng</td>
	<td>Arxiv</td>
	<td><p>Hash based nearest neighbor search has become attractive in many applications. However the quantization in hashing usually degenerates the discriminative power when using Hamming distance ranking. Besides for large-scale visual search existing hashing methods cannot directly support the efficient search over the data with multiple sources and while the literature has shown that adaptively incorporating complementary information from diverse sources or views can significantly boost the search performance. To address the problems this paper proposes a novel and generic approach to building multiple hash tables with multiple views and generating fine-grained ranking results at bitwise and tablewise levels. For each hash table a query-adaptive bitwise weighting is introduced to alleviate the quantization loss by simultaneously exploiting the quality of hash functions and their complement for nearest neighbor search. From the tablewise aspect multiple hash tables are built for different data views as a joint index over which a query-specific rank fusion is proposed to rerank all results from the bitwise ranking by diffusing in a graph. Comprehensive experiments on image search over three well-known benchmarks show that the proposed method achieves up to 17.1137; and 20.2837; performance gains on single and multiple table search over state-of-the-art methods.</p>
</td>
</tr>



<tr>
	<td>2018</td>
	<td><a href="/publications/long2018filter/">A Filter Of Minhash For Image Similarity Measures</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Filter Of Minhash For Image Similarity Measures' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Filter Of Minhash For Image Similarity Measures' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20Filter%20Of%20Minhash%20For%20Image%20Similarity%20Measures' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Long Jun, Liu Qunfeng, Yuan Xinpan, Zhang Chengyuan, Liu Junfeng</td>
	<td>Arxiv</td>
	<td><p>Image similarity measures play an important role in nearest neighbor search and duplicate detection for large-scale image datasets. Recently Minwise Hashing (or Minhash) and its related hashing algorithms have achieved great performances in large-scale image retrieval systems. However there are a large number of comparisons for image pairs in these applications which may spend a lot of computation time and affect the performance. In order to quickly obtain the pairwise images that theirs similarities are higher than the specific threshold T (e.g. 0.5) we propose a dynamic threshold filter of Minwise Hashing for image similarity measures. It greatly reduces the calculation time by terminating the unnecessary comparisons in advance. We also find that the filter can be extended to other hashing algorithms on when the estimator satisfies the binomial distribution such as b-Bit Minwise Hashing One Permutation Hashing etc. In this pager we use the Bag-of-Visual-Words (BoVW) model based on the Scale Invariant Feature Transform (SIFT) to represent the image features. We have proved that the filter is correct and effective through the experiment on real image datasets.</p>
</td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/tu2018object/">Object Detection Based Deep Unsupervised Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Object Detection Based Deep Unsupervised Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Object Detection Based Deep Unsupervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Object%20Detection%20Based%20Deep%20Unsupervised%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Tu Rong-cheng, Mao Xian-ling, Feng Bo-si, Bian Bing-bing, Ying Yu-shu</td>
	<td>Arxiv</td>
	<td><p>Recently similarity-preserving hashing methods have been extensively studied for large-scale image retrieval. Compared with unsupervised hashing supervised hashing methods for labeled data have usually better performance by utilizing semantic label information. Intuitively for unlabeled data it will improve the performance of unsupervised hashing methods if we can first mine some supervised semantic label information from unlabeled data and then incorporate the label information into the training process. Thus in this paper we propose a novel Object Detection based Deep Unsupervised Hashing method (ODDUH). Specifically a pre-trained object detection model is utilized to mining supervised label information which is used to guide the learning process to generate high-quality hash codes.Extensive experiments on two public datasets demonstrate that the proposed method outperforms the state-of-the-art unsupervised hashing methods in the image retrieval task.</p>
</td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/tian2018learning/">Learning Decorrelated Hashing Codes For Multimodal Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning Decorrelated Hashing Codes For Multimodal Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning Decorrelated Hashing Codes For Multimodal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20Decorrelated%20Hashing%20Codes%20For%20Multimodal%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Tian Dayong</td>
	<td>Arxiv</td>
	<td><p>In social networks heterogeneous multimedia data correlate to each other such as videos and their corresponding tags in YouTube and image-text pairs in Facebook. Nearest neighbor retrieval across multiple modalities on large data sets becomes a hot yet challenging problem. Hashing is expected to be an efficient solution since it represents data as binary codes. As the bit-wise XOR operations can be fast handled the retrieval time is greatly reduced. Few existing multimodal hashing methods consider the correlation among hashing bits. The correlation has negative impact on hashing codes. When the hashing code length becomes longer the retrieval performance improvement becomes slower. In this paper we propose a minimum correlation regularization (MCR) for multimodal hashing. First the sigmoid function is used to embed the data matrices. Then the MCR is applied on the output of sigmoid function. As the output of sigmoid function approximates a binary code matrix the proposed MCR can efficiently decorrelate the hashing codes. Experiments show the superiority of the proposed method becomes greater as the code length increases.</p>
</td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/loncaric2018learning/">Learning Hash Codes Via Hamming Distance Targets</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning Hash Codes Via Hamming Distance Targets' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning Hash Codes Via Hamming Distance Targets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20Hash%20Codes%20Via%20Hamming%20Distance%20Targets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Loncaric Martin, Liu Bowei, Weber Ryan</td>
	<td>Arxiv</td>
	<td><p>We present a powerful new loss function and training scheme for learning binary hash codes with any differentiable model and similarity function. Our loss function improves over prior methods by using log likelihood loss on top of an accurate approximation for the probability that two inputs fall within a Hamming distance target. Our novel training scheme obtains a good estimate of the true gradient by better sampling inputs and evaluating loss terms between all pairs of inputs in each minibatch. To fully leverage the resulting hashes we use multi-indexing. We demonstrate that these techniques provide large improvements to a similarity search tasks. We report the best results to date on competitive information retrieval tasks for ImageNet and SIFT 1M improving MAP from 7337; to 8437; and reducing query cost by a factor of 2-8 respectively.</p>
</td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/gattupalli2018weakly/">Weakly Supervised Deep Image Hashing Through Tag Embeddings</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Weakly Supervised Deep Image Hashing Through Tag Embeddings' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Weakly Supervised Deep Image Hashing Through Tag Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Weakly%20Supervised%20Deep%20Image%20Hashing%20Through%20Tag%20Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Gattupalli Vijetha, Zhuo Yaoxin, Li Baoxin</td>
	<td>Arxiv</td>
	<td><p>Many approaches to semantic image hashing have been formulated as supervised learning problems that utilize images and label information to learn the binary hash codes. However large-scale labeled image data is expensive to obtain thus imposing a restriction on the usage of such algorithms. On the other hand unlabelled image data is abundant due to the existence of many Web image repositories. Such Web images may often come with images tags that contain useful information although raw tags in general do not readily lead to semantic labels. Motivated by this scenario we formulate the problem of semantic image hashing as a weakly-supervised learning problem. We utilize the information contained in the user-generated tags associated with the images to learn the hash codes. More specifically we extract the word2vec semantic embeddings of the tags and use the information contained in them for constraining the learning. Accordingly we name our model Weakly Supervised Deep Hashing using Tag Embeddings (WDHT). WDHT is tested for the task of semantic image retrieval and is compared against several state-of-art models. Results show that our approach sets a new state-of-art in the area of weekly supervised image hashing.</p>
</td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/do2018binary/">Binary Constrained Deep Hashing Network For Image Retrieval Without Manual Annotation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Binary Constrained Deep Hashing Network For Image Retrieval Without Manual Annotation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Binary Constrained Deep Hashing Network For Image Retrieval Without Manual Annotation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Binary%20Constrained%20Deep%20Hashing%20Network%20For%20Image%20Retrieval%20Without%20Manual%20Annotation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Do Thanh-toan, Hoang Tuan, Tan Dang-khoa Le, Pham Trung, Le Huu, Cheung Ngai-man, Reid Ian</td>
	<td>Arxiv</td>
	<td><p>Learning compact binary codes for image retrieval task using deep neural networks has attracted increasing attention recently. However training deep hashing networks for the task is challenging due to the binary constraints on the hash codes the similarity preserving property and the requirement for a vast amount of labelled images. To the best of our knowledge none of the existing methods has tackled all of these challenges completely in a unified framework. In this work we propose a novel end-to-end deep learning approach for the task in which the network is trained to produce binary codes directly from image pixels without the need of manual annotation. In particular to deal with the non-smoothness of binary constraints we propose a novel pairwise constrained loss function which simultaneously encodes the distances between pairs of hash codes and the binary quantization error. In order to train the network with the proposed loss function we propose an efficient parameter learning algorithm. In addition to provide similar / dissimilar training images to train the network we exploit 3D models reconstructed from unlabelled images for automatic generation of enormous training image pairs. The extensive experiments on image retrieval benchmark datasets demonstrate the improvements of the proposed method over the state-of-the-art compact representation methods on the image retrieval problem.</p>
</td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/loncaric2018convolutional/">Convolutional Hashing For Automated Scene Matching</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Convolutional Hashing For Automated Scene Matching' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Convolutional Hashing For Automated Scene Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Convolutional%20Hashing%20For%20Automated%20Scene%20Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Loncaric Martin, Liu Bowei, Weber Ryan</td>
	<td>Arxiv</td>
	<td><p>We present a powerful new loss function and training scheme for learning binary hash functions. In particular we demonstrate our method by creating for the first time a neural network that outperforms state-of-the-art Haar wavelets and color layout descriptors at the task of automated scene matching. By accurately relating distance on the manifold of network outputs to distance in Hamming space we achieve a 100-fold reduction in nontrivial false positive rate and significantly higher true positive rate. We expect our insights to provide large wins for hashing models applied to other information retrieval hashing tasks as well.</p>
</td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/zhang2018semantic/">Semantic Cluster Unary Loss For Efficient Deep Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Semantic Cluster Unary Loss For Efficient Deep Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Semantic Cluster Unary Loss For Efficient Deep Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Semantic%20Cluster%20Unary%20Loss%20For%20Efficient%20Deep%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhang Shifeng, Li Jianmin, Zhang Bo</td>
	<td>IEEE Transactions on Image Processing</td>
	<td><p>Hashing method maps similar data to binary hashcodes with smaller hamming distance which has received a broad attention due to its low storage cost and fast retrieval speed. With the rapid development of deep learning deep hashing methods have achieved promising results in efficient information retrieval. Most of the existing deep hashing methods adopt pairwise or triplet losses to deal with similarities underlying the data but the training is difficult and less efficient because (O(n^2)) data pairs and (O(n^3)) triplets are involved. To address these issues we propose a novel deep hashing algorithm with unary loss which can be trained very efficiently. We first of all introduce a Unary Upper Bound of the traditional triplet loss thus reducing the complexity to (O(n)) and bridging the classification-based unary loss and the triplet loss. Second we propose a novel Semantic Cluster Deep Hashing (SCDH) algorithm by introducing a modified Unary Upper Bound loss named Semantic Cluster Unary Loss (SCUL). The resultant hashcodes form several compact clusters which means hashcodes in the same cluster have similar semantic information. We also demonstrate that the proposed SCDH is easy to be extended to semi-supervised settings by incorporating the state-of-the-art semi-supervised learning algorithms. Experiments on large-scale datasets show that the proposed method is superior to state-of-the-art hashing algorithms.</p>
</td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/luo2018collaborative/">Collaborative Learning For Extremely Low Bit Asymmetric Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Collaborative Learning For Extremely Low Bit Asymmetric Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Collaborative Learning For Extremely Low Bit Asymmetric Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Collaborative%20Learning%20For%20Extremely%20Low%20Bit%20Asymmetric%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Luo Yadan, Huang Zi, Li Yang, Shen Fumin, Yang Yang, Cui Peng</td>
	<td>Arxiv</td>
	<td><p>Hashing techniques are in great demand for a wide range of real-world applications such as image retrieval and network compression. Nevertheless existing approaches could hardly guarantee a satisfactory performance with the extremely low-bit (e.g. 4-bit) hash codes due to the severe information loss and the shrink of the discrete solution space. In this paper we propose a novel textitCollaborative Learning strategy that is tailored for generating high-quality low-bit hash codes. The core idea is to jointly distill bit-specific and informative representations for a group of pre-defined code lengths. The learning of short hash codes among the group can benefit from the manifold shared with other long codes where multiple views from different hash codes provide the supplementary guidance and regularization making the convergence faster and more stable. To achieve that an asymmetric hashing framework with two variants of multi-head embedding structures is derived termed as Multi-head Asymmetric Hashing (MAH) leading to great efficiency of training and querying. Extensive experiments on three benchmark datasets have been conducted to verify the superiority of the proposed MAH and have shown that the 8-bit hash codes generated by MAH achieve (94.3) of the MAP (Mean Average Precision (MAP)) score on the CIFAR-10 dataset which significantly surpasses the performance of the 48-bit codes by the state-of-the-arts in image retrieval tasks.</p>
</td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/liu2018discriminative/">Discriminative Cross-view Binary Representation Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Discriminative Cross-view Binary Representation Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Discriminative Cross-view Binary Representation Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Discriminative%20Cross-view%20Binary%20Representation%20Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Liu Liu, Qi Hairong</td>
	<td>WACV</td>
	<td><p>Learning compact representation is vital and challenging for large scale multimedia data. Cross-view/cross-modal hashing for effective binary representation learning has received significant attention with exponentially growing availability of multimedia content. Most existing cross-view hashing algorithms emphasize the similarities in individual views which are then connected via cross-view similarities. In this work we focus on the exploitation of the discriminative information from different views and propose an end-to-end method to learn semantic-preserving and discriminative binary representation dubbed Discriminative Cross-View Hashing (DCVH) in light of learning multitasking binary representation for various tasks including cross-view retrieval image-to-image retrieval and image annotation/tagging. The proposed DCVH has the following key components. First it uses convolutional neural network (CNN) based nonlinear hashing functions and multilabel classification for both images and texts simultaneously. Such hashing functions achieve effective continuous relaxation during training without explicit quantization loss by using Direct Binary Embedding (DBE) layers. Second we propose an effective view alignment via Hamming distance minimization which is efficiently accomplished by bit-wise XOR operation. Extensive experiments on two image-text benchmark datasets demonstrate that DCVH outperforms state-of-the-art cross-view hashing algorithms as well as single-view image hashing algorithms. In addition DCVH can provide competitive performance for image annotation/tagging.</p>
</td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/li2018dual/">Dual Asymmetric Deep Hashing Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Dual Asymmetric Deep Hashing Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Dual Asymmetric Deep Hashing Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Dual%20Asymmetric%20Deep%20Hashing%20Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Li Jinxing, Zhang Bob, Lu Guangming, Zhang David</td>
	<td>Arxiv</td>
	<td><p>Due to the impressive learning power deep learning has achieved a remarkable performance in supervised hash function learning. In this paper we propose a novel asymmetric supervised deep hashing method to preserve the semantic structure among different categories and generate the binary codes simultaneously. Specifically two asymmetric deep networks are constructed to reveal the similarity between each pair of images according to their semantic labels. The deep hash functions are then learned through two networks by minimizing the gap between the learned features and discrete codes. Furthermore since the binary codes in the Hamming space also should keep the semantic affinity existing in the original space another asymmetric pairwise loss is introduced to capture the similarity between the binary codes and real-value features. This asymmetric loss not only improves the retrieval performance but also contributes to a quick convergence at the training phase. By taking advantage of the two-stream deep structures and two types of asymmetric pairwise functions an alternating algorithm is designed to optimize the deep features and high-quality binary codes efficiently. Experimental results on three real-world datasets substantiate the effectiveness and superiority of our approach as compared with state-of-the-art.</p>
</td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/ceccarello2018fresh/">FRESH Frechet Similarity With Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=FRESH Frechet Similarity With Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=FRESH Frechet Similarity With Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=FRESH%20Frechet%20Similarity%20With%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Ceccarello Matteo, Driemel Anne, Silvestri Francesco</td>
	<td>Proc. of Algorithms and Data Structures Symposium</td>
	<td><p>This paper studies the (r)-range search problem for curves under the continuous Frechet distance given a dataset (S) of (n) polygonal curves and a threshold (r0) construct a data structure that for any query curve (q) efficiently returns all entries in (S) with distance at most (r) from (q). We propose FRESH an approximate and randomized approach for (r)-range search that leverages on a locality sensitive hashing scheme for detecting candidate near neighbors of the query curve and on a subsequent pruning step based on a cascade of curve simplifications. We experimentally compare fresh to exact and deterministic solutions and we show that high performance can be reached by suitably relaxing precision and recall.</p>
</td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/misra2018bernoulli/">Bernoulli Embeddings For Graphs</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Bernoulli Embeddings For Graphs' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Bernoulli Embeddings For Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Bernoulli%20Embeddings%20For%20Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Misra Vinith, Bhatia Sumit</td>
	<td>Arxiv</td>
	<td><p>Just as semantic hashing can accelerate information retrieval binary valued embeddings can significantly reduce latency in the retrieval of graphical data. We introduce a simple but effective model for learning such binary vectors for nodes in a graph. By imagining the embeddings as independent coin flips of varying bias continuous optimization techniques can be applied to the approximate expected loss. Embeddings optimized in this fashion consistently outperform the quantization of both spectral graph embeddings and various learned real-valued embeddings on both ranking and pre-ranking tasks for a variety of datasets.</p>
</td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/cao2018deep/">Deep Priority Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Priority Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Priority Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Priority%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Cao Zhangjie, Sun Ziping, Long Mingsheng, Wang Jianmin, Yu Philip S.</td>
	<td>Arxiv</td>
	<td><p>Deep hashing enables image retrieval by end-to-end learning of deep representations and hash codes from training data with pairwise similarity information. Subject to the distribution skewness underlying the similarity information most existing deep hashing methods may underperform for imbalanced data due to misspecified loss functions. This paper presents Deep Priority Hashing (DPH) an end-to-end architecture that generates compact and balanced hash codes in a Bayesian learning framework. The main idea is to reshape the standard cross-entropy loss for similarity-preserving learning such that it down-weighs the loss associated to highly-confident pairs. This idea leads to a novel priority cross-entropy loss which prioritizes the training on uncertain pairs over confident pairs. Also we propose another priority quantization loss which prioritizes hard-to-quantize examples for generation of nearly lossless hash codes. Extensive experiments demonstrate that DPH can generate high-quality hash codes and yield state-of-the-art image retrieval results on three datasets ImageNet NUS-WIDE and MS-COCO.</p>
</td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/yan2018norm/">Norm-ranging LSH For Maximum Inner Product Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Norm-ranging LSH For Maximum Inner Product Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Norm-ranging LSH For Maximum Inner Product Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Norm-ranging%20LSH%20For%20Maximum%20Inner%20Product%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Xiao Yan, Jinfeng Li, Xinyan Dai, Hongzhi Chen, James Cheng</td>
	<td>Neural Information Processing Systems</td>
	<td><p>Neyshabur and Srebro proposed SIMPLE-LSH which is the state-of-the-art hashing based algorithm for maximum inner product search (MIPS). We found that the performance of SIMPLE-LSH in both theory and practice suffers from long tails in the 2-norm distribution of real datasets. We propose NORM-RANGING LSH which addresses the excessive normalization problem caused by long tails by partitioning a dataset into sub-datasets and building a hash index for each sub-dataset independently. We prove that NORM-RANGING LSH achieves lower query time complexity than SIMPLE-LSH under mild conditions. We also show that the idea of dataset partitioning can improve another hashing based MIPS algorithm. Experiments show that NORM-RANGING LSH probes much less items than SIMPLE-LSH at the same recall thus significantly benefiting MIPS based applications.</p>
</td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/cakir2018hashing/">Hashing With Binary Matrix Pursuit</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Hashing With Binary Matrix Pursuit' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Hashing With Binary Matrix Pursuit' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Hashing%20With%20Binary%20Matrix%20Pursuit' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Cakir Fatih, He Kun, Sclaroff Stan</td>
	<td>Arxiv</td>
	<td><p>We propose theoretical and empirical improvements for two-stage hashing methods. We first provide a theoretical analysis on the quality of the binary codes and show that under mild assumptions a residual learning scheme can construct binary codes that fit any neighborhood structure with arbitrary accuracy. Secondly we show that with high-capacity hash functions such as CNNs binary code inference can be greatly simplified for many standard neighborhood definitions yielding smaller optimization problems and more robust codes. Incorporating our findings we propose a novel two-stage hashing method that significantly outperforms previous hashing studies on widely used image retrieval benchmarks.</p>
</td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/zhe2018deep/">Deep Class-wise Hashing Semantics-preserving Hashing Via Class-wise Loss</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Class-wise Hashing Semantics-preserving Hashing Via Class-wise Loss' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Class-wise Hashing Semantics-preserving Hashing Via Class-wise Loss' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Class-wise%20Hashing%20Semantics-preserving%20Hashing%20Via%20Class-wise%20Loss' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhe Xuefei, Chen Shifeng, Yan Hong</td>
	<td>Arxiv</td>
	<td><p>Deep supervised hashing has emerged as an influential solution to large-scale semantic image retrieval problems in computer vision. In the light of recent progress convolutional neural network based hashing methods typically seek pair-wise or triplet labels to conduct the similarity preserving learning. However complex semantic concepts of visual contents are hard to capture by similar/dissimilar labels which limits the retrieval performance. Generally pair-wise or triplet losses not only suffer from expensive training costs but also lack in extracting sufficient semantic information. In this regard we propose a novel deep supervised hashing model to learn more compact class-level similarity preserving binary codes. Our deep learning based model is motivated by deep metric learning that directly takes semantic labels as supervised information in training and generates corresponding discriminant hashing code. Specifically a novel cubic constraint loss function based on Gaussian distribution is proposed which preserves semantic variations while penalizes the overlap part of different classes in the embedding space. To address the discrete optimization problem introduced by binary codes a two-step optimization strategy is proposed to provide efficient training and avoid the problem of gradient vanishing. Extensive experiments on four large-scale benchmark databases show that our model can achieve the state-of-the-art retrieval performance. Moreover when training samples are limited our method surpasses other supervised deep hashing methods with non-negligible margins.</p>
</td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/zhang2018improved/">Improved Deep Hashing With Soft Pairwise Similarity For Multi-label Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Improved Deep Hashing With Soft Pairwise Similarity For Multi-label Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Improved Deep Hashing With Soft Pairwise Similarity For Multi-label Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Improved%20Deep%20Hashing%20With%20Soft%20Pairwise%20Similarity%20For%20Multi-label%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhang Zheng, Zou Qin, Lin Yuewei, Chen Long, Wang Song</td>
	<td>Arxiv</td>
	<td><p>Hash coding has been widely used in the approximate nearest neighbor search for large-scale image retrieval. Recently many deep hashing methods have been proposed and shown largely improved performance over traditional feature-learning-based methods. Most of these methods examine the pairwise similarity on the semantic-level labels where the pairwise similarity is generally defined in a hard-assignment way. That is the pairwise similarity is 1 if they share no less than one class label and 0 if they do not share any. However such similarity definition cannot reflect the similarity ranking for pairwise images that hold multiple labels. In this paper a new deep hashing method is proposed for multi-label image retrieval by re-defining the pairwise similarity into an instance similarity where the instance similarity is quantified into a percentage based on the normalized semantic labels. Based on the instance similarity a weighted cross-entropy loss and a minimum mean square error loss are tailored for loss-function construction and are efficiently used for simultaneous feature learning and hash coding. Experiments on three popular datasets demonstrate that the proposed method outperforms the competing methods and achieves the state-of-the-art performance in multi-label image retrieval.</p>
</td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/b2018fully/">Fully Understanding The Hashing Trick</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Fully Understanding The Hashing Trick' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Fully Understanding The Hashing Trick' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Fully%20Understanding%20The%20Hashing%20Trick' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Casper B. Freksen, Lior Kamma, Kasper Green Larsen</td>
	<td>Neural Information Processing Systems</td>
	<td><p>Feature hashing also known as em the hashing trick introduced by Weinberger et al. (2009) is one of the key techniques used in scaling-up machine learning algorithms. Loosely speaking feature hashing uses a random sparse projection matrix (A ^n to ^m) (where (m ll n)) in order to reduce the dimension of the data from (n) to (m) while approximately preserving the Euclidean norm. Every column of (A) contains exactly one non-zero entry equals to either (-1) or (1). Weinberger et al. showed tail bounds on (Ax_2^2). Specifically they showed that for every (varepsilon delta) if (x_infty / x_2) is sufficiently small and (m) is sufficiently large then beginequationPr ; ;Ax_2^2 - x_2^2; &lt; varepsilon x_2^2 ; ge 1 - delta ;.endequation These bounds were later extended by Dasgupta et al. (2010) and most recently refined by Dahlgaard et al. (2017) however the true nature of the performance of this key technique and specifically the correct tradeoff between the pivotal parameters (x_infty / x_2 m varepsilon delta) remained an open question. We settle this question by giving tight asymptotic bounds on the exact tradeoff between the central parameters thus providing a complete understanding of the performance of feature hashing. We complement the asymptotic bound with empirical data which shows that the constants hiding in the asymptotic notation are in fact very close to (1) thus further illustrating the tightness of the presented bounds in practice.</p>
</td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/zieba2018bingan/">Bingan Learning Compact Binary Descriptors With A Regularized GAN</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Bingan Learning Compact Binary Descriptors With A Regularized GAN' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Bingan Learning Compact Binary Descriptors With A Regularized GAN' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Bingan%20Learning%20Compact%20Binary%20Descriptors%20With%20A%20Regularized%20GAN' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Maciej Zieba, Piotr Semberecki, Tarek El-gaaly, Tomasz Trzcinski</td>
	<td>Neural Information Processing Systems</td>
	<td><p>In this paper we propose a novel regularization method for Generative Adversarial Networks that allows the model to learn discriminative yet compact binary representations of image patches (image descriptors). We exploit the dimensionality reduction that takes place in the intermediate layers of the discriminator network and train the binarized penultimate layers low-dimensional representation to mimic the distribution of the higher-dimensional preceding layers. To achieve this we introduce two loss terms that aim at (i) reducing the correlation between the dimensions of the binarized penultimate layers low-dimensional representation (i.e. maximizing joint entropy) and (ii) propagating the relations between the dimensions in the high-dimensional space to the low-dimensional space. We evaluate the resulting binary image descriptors on two challenging applications image matching and retrieval where they achieve state-of-the-art results.</p>
</td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/liu2018fusion/">Fusion Hashing A General Framework For Self-improvement Of Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Fusion Hashing A General Framework For Self-improvement Of Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Fusion Hashing A General Framework For Self-improvement Of Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Fusion%20Hashing%20A%20General%20Framework%20For%20Self-improvement%20Of%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Liu Xingbo, Nie Xiushan, Yin Yilong</td>
	<td>Arxiv</td>
	<td><p>Hashing has been widely used for efficient similarity search based on its query and storage efficiency. To obtain better precision most studies focus on designing different objective functions with different constraints or penalty terms that consider neighborhood information. In this paper in contrast to existing hashing methods we propose a novel generalized framework called fusion hashing (FH) to improve the precision of existing hashing methods without adding new constraints or penalty terms. In the proposed FH given an existing hashing method we first execute it several times to get several different hash codes for a set of training samples. We then propose two novel fusion strategies that combine these different hash codes into one set of final hash codes. Based on the final hash codes we learn a simple linear hash function for the samples that can significantly improve model precision. In general the proposed FH can be adopted in existing hashing method and achieve more precise and stable performance compared to the original hashing method with little extra expenditure in terms of time and space. Extensive experiments were performed based on three benchmark datasets and the results demonstrate the superior performance of the proposed framework</p>
</td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/schlegel2018adding/">Adding Cues To Binary Feature Descriptors For Visual Place Recognition</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Adding Cues To Binary Feature Descriptors For Visual Place Recognition' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Adding Cues To Binary Feature Descriptors For Visual Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Adding%20Cues%20To%20Binary%20Feature%20Descriptors%20For%20Visual%20Place%20Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Schlegel Dominik, Grisetti Giorgio</td>
	<td>Arxiv</td>
	<td><p>In this paper we propose an approach to embed continuous and selector cues in binary feature descriptors used for visual place recognition. The embedding is achieved by extending each feature descriptor with a binary string that encodes a cue and supports the Hamming distance metric. Augmenting the descriptors in such a way has the advantage of being transparent to the procedure used to compare them. We present two concrete applications of our methodology demonstrating the two considered types of cues. In addition to that we conducted on these applications a broad quantitative and comparative evaluation covering five benchmark datasets and several state-of-the-art image retrieval approaches in combination with various binary descriptor types.</p>
</td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/lessley2018data/">Data-parallel Hashing Techniques For GPU Architectures</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Data-parallel Hashing Techniques For GPU Architectures' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Data-parallel Hashing Techniques For GPU Architectures' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Data-parallel%20Hashing%20Techniques%20For%20GPU%20Architectures' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Lessley Brenton</td>
	<td>Arxiv</td>
	<td><p>Hash tables are one of the most fundamental data structures for effectively storing and accessing sparse data with widespread usage in domains ranging from computer graphics to machine learning. This study surveys the state-of-the-art research on data-parallel hashing techniques for emerging massively-parallel many-core GPU architectures. Key factors affecting the performance of different hashing schemes are discovered and used to suggest best practices and pinpoint areas for further research.</p>
</td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/magliani2018efficient/">Efficient Nearest Neighbors Search For Large-scale Landmark Recognition</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Nearest Neighbors Search For Large-scale Landmark Recognition' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Efficient Nearest Neighbors Search For Large-scale Landmark Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Efficient%20Nearest%20Neighbors%20Search%20For%20Large-scale%20Landmark%20Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Magliani Federico, Fontanini Tomaso, Prati Andrea</td>
	<td>Arxiv</td>
	<td><p>The problem of landmark recognition has achieved excellent results in small-scale datasets. When dealing with large-scale retrieval issues that were irrelevant with small amount of data quickly become fundamental for an efficient retrieval phase. In particular computational time needs to be kept as low as possible whilst the retrieval accuracy has to be preserved as much as possible. In this paper we propose a novel multi-index hashing method called Bag of Indexes (BoI) for Approximate Nearest Neighbors (ANN) search. It allows to drastically reduce the query time and outperforms the accuracy results compared to the state-of-the-art methods for large-scale landmark recognition. It has been demonstrated that this family of algorithms can be applied on different embedding techniques like VLAD and R-MAC obtaining excellent results in very short times on different public datasets Holidays+Flickr1M Oxford105k and Paris106k.</p>
</td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/yu2018learning/">Learning Discriminative Hashing Codes For Cross-modal Retrieval Based On Multi-view Features</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning Discriminative Hashing Codes For Cross-modal Retrieval Based On Multi-view Features' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning Discriminative Hashing Codes For Cross-modal Retrieval Based On Multi-view Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20Discriminative%20Hashing%20Codes%20For%20Cross-modal%20Retrieval%20Based%20On%20Multi-view%20Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Yu Jun, Wu Xiao-jun, Kittler Josef</td>
	<td>Arxiv</td>
	<td><p>Hashing techniques have been applied broadly in retrieval tasks due to their low storage requirements and high speed of processing. Many hashing methods based on a single view have been extensively studied for information retrieval. However the representation capacity of a single view is insufficient and some discriminative information is not captured which results in limited improvement. In this paper we employ multiple views to represent images and texts for enriching the feature information. Our framework exploits the complementary information among multiple views to better learn the discriminative compact hash codes. A discrete hashing learning framework that jointly performs classifier learning and subspace learning is proposed to complete multiple search tasks simultaneously. Our framework includes two stages namely a kernelization process and a quantization process. Kernelization aims to find a common subspace where multi-view features can be fused. The quantization stage is designed to learn discriminative unified hashing codes. Extensive experiments are performed on single-label datasets (WiKi and MMED) and multi-label datasets (MIRFlickr and NUS-WIDE) and the experimental results indicate the superiority of our method compared with the state-of-the-art methods.</p>
</td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/su2018greedy/">Greedy Hash Towards Fast Optimization For Accurate Hash Coding In CNN</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Greedy Hash Towards Fast Optimization For Accurate Hash Coding In CNN' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Greedy Hash Towards Fast Optimization For Accurate Hash Coding In CNN' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Greedy%20Hash%20Towards%20Fast%20Optimization%20For%20Accurate%20Hash%20Coding%20In%20CNN' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Shupeng Su, Chao Zhang, Kai Han, Yonghong Tian</td>
	<td>Neural Information Processing Systems</td>
	<td><p>To convert the input into binary code hashing algorithm has been widely used for approximate nearest neighbor search on large-scale image sets due to its computation and storage efficiency. Deep hashing further improves the retrieval quality by combining the hash coding with deep neural network. However a major difficulty in deep hashing lies in the discrete constraints imposed on the network output which generally makes the optimization NP hard. In this work we adopt the greedy principle to tackle this NP hard problem by iteratively updating the network toward the probable optimal discrete solution in each iteration. A hash coding layer is designed to implement our approach which strictly uses the sign function in forward propagation to maintain the discrete constraints while in back propagation the gradients are transmitted intactly to the front layer to avoid the vanishing gradients. In addition to the theoretical derivation we provide a new perspective to visualize and understand the effectiveness and efficiency of our algorithm. Experiments on benchmark datasets show that our scheme outperforms state-of-the-art hashing methods in both supervised and unsupervised tasks.</p>
</td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/sharma2018improving/">Improving Similarity Search With High-dimensional Locality-sensitive Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Improving Similarity Search With High-dimensional Locality-sensitive Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Improving Similarity Search With High-dimensional Locality-sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Improving%20Similarity%20Search%20With%20High-dimensional%20Locality-sensitive%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Sharma Jaiyam, Navlakha Saket</td>
	<td>Arxiv</td>
	<td><p>We propose a new class of data-independent locality-sensitive hashing (LSH) algorithms based on the fruit fly olfactory circuit. The fundamental difference of this approach is that instead of assigning hashes as dense points in a low dimensional space hashes are assigned in a high dimensional space which enhances their separability. We show theoretically and empirically that this new family of hash functions is locality-sensitive and preserves rank similarity for inputs in any p space. We then analyze different variations on this strategy and show empirically that they outperform existing LSH methods for nearest-neighbors search on six benchmark datasets. Finally we propose a multi-probe version of our algorithm that achieves higher performance for the same query time or conversely that maintains performance of prior approaches while taking significantly less indexing time and memory. Overall our approach leverages the advantages of separability provided by high-dimensional spaces while still remaining computationally efficient</p>
</td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/jin2018deep/">Deep Ordinal Hashing With Spatial Attention</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Ordinal Hashing With Spatial Attention' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Ordinal Hashing With Spatial Attention' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Ordinal%20Hashing%20With%20Spatial%20Attention' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Jin Lu, Shu Xiangbo, Li Kai, Li Zechao, Qi Guo-jun, Tang Jinhui</td>
	<td>Arxiv</td>
	<td><p>Hashing has attracted increasing research attentions in recent years due to its high efficiency of computation and storage in image retrieval. Recent works have demonstrated the superiority of simultaneous feature representations and hash functions learning with deep neural networks. However most existing deep hashing methods directly learn the hash functions by encoding the global semantic information while ignoring the local spatial information of images. The loss of local spatial structure makes the performance bottleneck of hash functions therefore limiting its application for accurate similarity retrieval. In this work we propose a novel Deep Ordinal Hashing (DOH) method which learns ordinal representations by leveraging the ranking structure of feature space from both local and global views. In particular to effectively build the ranking structure we propose to learn the rank correlation space by exploiting the local spatial information from Fully Convolutional Network (FCN) and the global semantic information from the Convolutional Neural Network (CNN) simultaneously. More specifically an effective spatial attention model is designed to capture the local spatial information by selectively learning well-specified locations closely related to target objects. In such hashing frameworkthe local spatial and global semantic nature of images are captured in an end-to-end ranking-to-hashing manner. Experimental results conducted on three widely-used datasets demonstrate that the proposed DOH method significantly outperforms the state-of-the-art hashing methods.</p>
</td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/shen2018zero/">Zero-shot Sketch-image Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Zero-shot Sketch-image Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Zero-shot Sketch-image Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Zero-shot%20Sketch-image%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Shen Yuming, Liu Li, Shen Fumin, Shao Ling</td>
	<td>Arxiv</td>
	<td><p>Recent studies show that large-scale sketch-based image retrieval (SBIR) can be efficiently tackled by cross-modal binary representation learning methods where Hamming distance matching significantly speeds up the process of similarity search. Providing training and test data subjected to a fixed set of pre-defined categories the cutting-edge SBIR and cross-modal hashing works obtain acceptable retrieval performance. However most of the existing methods fail when the categories of query sketches have never been seen during training. In this paper the above problem is briefed as a novel but realistic zero-shot SBIR hashing task. We elaborate the challenges of this special task and accordingly propose a zero-shot sketch-image hashing (ZSIH) model. An end-to-end three-network architecture is built two of which are treated as the binary encoders. The third network mitigates the sketch-image heterogeneity and enhances the semantic relations among data by utilizing the Kronecker fusion layer and graph convolution respectively. As an important part of ZSIH we formulate a generative hashing scheme in reconstructing semantic knowledge representations for zero-shot retrieval. To the best of our knowledge ZSIH is the first zero-shot hashing work suitable for SBIR and cross-modal search. Comprehensive experiments are conducted on two extended datasets i.e. Sketchy and TU-Berlin with a novel zero-shot train-test split. The proposed model remarkably outperforms related works.</p>
</td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/wu2018learning/">Learning Effective Binary Visual Representations With Deep Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning Effective Binary Visual Representations With Deep Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning Effective Binary Visual Representations With Deep Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20Effective%20Binary%20Visual%20Representations%20With%20Deep%20Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Wu Jianxin, Luo Jian-hao</td>
	<td>Arxiv</td>
	<td><p>Although traditionally binary visual representations are mainly designed to reduce computational and storage costs in the image retrieval research this paper argues that binary visual representations can be applied to large scale recognition and detection problems in addition to hashing in retrieval. Furthermore the binary nature may make it generalize better than its real-valued counterparts. Existing binary hashing methods are either two-stage or hinging on loss term regularization or saturated functions hence converge slowly and only emit soft binary values. This paper proposes Approximately Binary Clamping (ABC) which is non-saturating end-to-end trainable with fast convergence and can output true binary visual representations. ABC achieves comparable accuracy in ImageNet classification as its real-valued counterpart and even generalizes better in object detection. On benchmark image retrieval datasets ABC also outperforms existing hashing methods.</p>
</td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/yu2018discriminative/">Discriminative Supervised Hashing For Cross-modal Similarity Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Discriminative Supervised Hashing For Cross-modal Similarity Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Discriminative Supervised Hashing For Cross-modal Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Discriminative%20Supervised%20Hashing%20For%20Cross-modal%20Similarity%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Yu Jun, Wu Xiao-jun, Kittler Josef</td>
	<td>Arxiv</td>
	<td><p>With the advantage of low storage cost and high retrieval efficiency hashing techniques have recently been an emerging topic in cross-modal similarity search. As multiple modal data reflect similar semantic content many researches aim at learning unified binary codes. However discriminative hashing features learned by these methods are not adequate. This results in lower accuracy and robustness. We propose a novel hashing learning framework which jointly performs classifier learning subspace learning and matrix factorization to preserve class-specific semantic content termed Discriminative Supervised Hashing (DSH) to learn the discrimative unified binary codes for multi-modal data. Besides reducing the loss of information and preserving the non-linear structure of data DSH non-linearly projects different modalities into the common space in which the similarity among heterogeneous data points can be measured. Extensive experiments conducted on the three publicly available datasets demonstrate that the framework proposed in this paper outperforms several state-of -the-art methods.</p>
</td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/jin2018unsupervised/">Unsupervised Semantic Deep Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Semantic Deep Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Unsupervised Semantic Deep Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Unsupervised%20Semantic%20Deep%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Jin Sheng</td>
	<td>Arxiv</td>
	<td><p>In recent years deep hashing methods have been proved to be efficient since it employs convolutional neural network to learn features and hashing codes simultaneously. However these methods are mostly supervised. In real-world application it is a time-consuming and overloaded task for annotating a large number of images. In this paper we propose a novel unsupervised deep hashing method for large-scale image retrieval. Our method namely unsupervised semantic deep hashing (textbfUSDH) uses semantic information preserved in the CNN feature layer to guide the training of network. We enforce four criteria on hashing codes learning based on VGG-19 model 1) preserving relevant information of feature space in hashing space; 2) minimizing quantization loss between binary-like codes and hashing codes; 3) improving the usage of each bit in hashing codes by using maximum information entropy and 4) invariant to image rotation. Extensive experiments on CIFAR-10 NUSWIDE have demonstrated that textbfUSDH outperforms several state-of-the-art unsupervised hashing methods for image retrieval. We also conduct experiments on Oxford 17 datasets for fine-grained classification to verify its efficiency for other computer vision tasks.</p>
</td>
</tr>



<tr>
	<td>2017</td>
	<td><a href="/publications/mishchuk2017working/">Working Hard To Know Your Neighbors Margins Local Descriptor Learning Loss</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Working Hard To Know Your Neighbors Margins Local Descriptor Learning Loss' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Working Hard To Know Your Neighbors Margins Local Descriptor Learning Loss' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Working%20Hard%20To%20Know%20Your%20Neighbors%20Margins%20Local%20Descriptor%20Learning%20Loss' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Anastasiia Mishchuk, Dmytro Mishkin, Filip Radenovic, Jiri Matas</td>
	<td>Neural Information Processing Systems</td>
	<td><p>We introduce a loss for metric learning which is inspired by the Lowes matching criterion for SIFT. We show that the proposed loss that maximizes the distance between the closest positive and closest negative example in the batch is better than complex regularization methods; it works well for both shallow and deep convolution network architectures. Applying the novel loss to the L2Net CNN architecture results in a compact descriptor named HardNet. It has the same dimensionality as SIFT (128) and shows state-of-art performance in wide baseline stereo patch verification and instance retrieval benchmarks.</p>
</td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/lai2017transductive/">Transductive Zero-shot Hashing Via Coarse-to-fine Similarity Mining</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Transductive Zero-shot Hashing Via Coarse-to-fine Similarity Mining' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Transductive Zero-shot Hashing Via Coarse-to-fine Similarity Mining' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Transductive%20Zero-shot%20Hashing%20Via%20Coarse-to-fine%20Similarity%20Mining' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Lai Hanjiang, Pan Yan</td>
	<td>Arxiv</td>
	<td><p>Zero-shot Hashing (ZSH) is to learn hashing models for novel/target classes without training data which is an important and challenging problem. Most existing ZSH approaches exploit transfer learning via an intermediate shared semantic representations between the seen/source classes and novel/target classes. However due to having disjoint the hash functions learned from the source dataset are biased when applied directly to the target classes. In this paper we study the transductive ZSH i.e. we have unlabeled data for novel classes. We put forward a simple yet efficient joint learning approach via coarse-to-fine similarity mining which transfers knowledges from source data to target data. It mainly consists of two building blocks in the proposed deep architecture 1) a shared two-streams network which the first stream operates on the source data and the second stream operates on the unlabeled data to learn the effective common image representations and 2) a coarse-to-fine module which begins with finding the most representative images from target classes and then further detect similarities among these images to transfer the similarities of the source data to the target data in a greedy fashion. Extensive evaluation results on several benchmark datasets demonstrate that the proposed hashing method achieves significant improvement over the state-of-the-art methods.</p>
</td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/wang2017supervised/">Supervised Deep Hashing For Hierarchical Labeled Data</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Deep Hashing For Hierarchical Labeled Data' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Supervised Deep Hashing For Hierarchical Labeled Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Supervised%20Deep%20Hashing%20For%20Hierarchical%20Labeled%20Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Wang Dan, Huang Heyan, Lu Chi, Feng Bo-si, Nie Liqiang, Wen Guihua, Mao Xian-ling</td>
	<td>Arxiv</td>
	<td><p>Recently hashing methods have been widely used in large-scale image retrieval. However most existing hashing methods did not consider the hierarchical relation of labels which means that they ignored the rich information stored in the hierarchy. Moreover most of previous works treat each bit in a hash code equally which does not meet the scenario of hierarchical labeled data. In this paper we propose a novel deep hashing method called supervised hierarchical deep hashing (SHDH) to perform hash code learning for hierarchical labeled data. Specifically we define a novel similarity formula for hierarchical labeled data by weighting each layer and design a deep convolutional neural network to obtain a hash code for each data point. Extensive experiments on several real-world public datasets show that the proposed method outperforms the state-of-the-art baselines in the image retrieval task.</p>
</td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/tito2017hash/">Hash Embeddings For Efficient Word Representations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Hash Embeddings For Efficient Word Representations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Hash Embeddings For Efficient Word Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Hash%20Embeddings%20For%20Efficient%20Word%20Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Dan Tito Svenstrup, Jonas Hansen, Ole Winther</td>
	<td>Neural Information Processing Systems</td>
	<td><p>We present hash embeddings an efficient method for representing words in a continuous vector form. A hash embedding may be seen as an interpolation between a standard word embedding and a word embedding created using a random hash function (the hashing trick). In hash embeddings each token is represented by (k) (d)-dimensional embeddings vectors and one (k) dimensional weight vector. The final (d) dimensional representation of the token is the product of the two. Rather than fitting the embedding vectors for each token these are selected by the hashing trick from a shared pool of (B) embedding vectors. Our experiments show that hash embeddings can easily deal with huge vocabularies consisting of millions tokens. When using a hash embedding there is no need to create a dictionary before training nor to perform any kind of vocabulary pruning after training. We show that models trained using hash embeddings exhibit at least the same level of performance as models trained using regular embeddings across a wide range of tasks. Furthermore the number of parameters needed by such an embedding is only a fraction of what is required by a regular embedding. Since standard embeddings and embeddings constructed using the hashing trick are actually just special cases of a hash embedding hash embeddings can be considered an extension and improvement over the existing regular embedding types.</p>
</td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/tian2017semi/">Semi-supervised Multimodal Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Semi-supervised Multimodal Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Semi-supervised Multimodal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Semi-supervised%20Multimodal%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Tian Dayong, Gong Maoguo, Zhou Deyun, Shi Jiao, Lei Yu</td>
	<td>Arxiv</td>
	<td><p>Retrieving nearest neighbors across correlated data in multiple modalities such as image-text pairs on Facebook and video-tag pairs on YouTube has become a challenging task due to the huge amount of data. Multimodal hashing methods that embed data into binary codes can boost the retrieving speed and reduce storage requirement. As unsupervised multimodal hashing methods are usually inferior to supervised ones while the supervised ones requires too much manually labeled data the proposed method in this paper utilizes a part of labels to design a semi-supervised multimodal hashing method. It first computes the transformation matrices for data matrices and label matrix. Then with these transformation matrices fuzzy logic is introduced to estimate a label matrix for unlabeled data. Finally it uses the estimated label matrix to learn hashing functions for data in each modality to generate a unified binary code matrix. Experiments show that the proposed semi-supervised method with 5037; labels can get a medium performance among the compared supervised ones and achieve an approximate performance to the best supervised method with 9037; labels. With only 1037; labels the proposed method can still compete with the worst compared supervised one.</p>
</td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/jhuo2017set/">Set-to-set Hashing With Applications In Visual Recognition</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Set-to-set Hashing With Applications In Visual Recognition' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Set-to-set Hashing With Applications In Visual Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Set-to-set%20Hashing%20With%20Applications%20In%20Visual%20Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Jhuo I-hong, Wang Jun</td>
	<td>Arxiv</td>
	<td><p>Visual data such as an image or a sequence of video frames is often naturally represented as a point set. In this paper we consider the fundamental problem of finding a nearest set from a collection of sets to a query set. This problem has obvious applications in large-scale visual retrieval and recognition and also in applied fields beyond computer vision. One challenge stands out in solving the problemâ€”set representation and measure of similarity. Particularly the query set and the sets in dataset collection can have varying cardinalities. The training collection is large enough such that linear scan is impractical. We propose a simple representation scheme that encodes both statistical and structural information of the sets. The derived representations are integrated in a kernel framework for flexible similarity measurement. For the query set process we adopt a learning-to-hash pipeline that turns the kernel representations into hash bits based on simple learners using multiple kernel learning. Experiments on two visual retrieval datasets show unambiguously that our set-to-set hashing framework outperforms prior methods that do not take the set-to-set search setting.</p>
</td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/komorowski2017evaluation/">Evaluation Of Hashing Methods Performance On Binary Feature Descriptors</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Evaluation Of Hashing Methods Performance On Binary Feature Descriptors' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Evaluation Of Hashing Methods Performance On Binary Feature Descriptors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Evaluation%20Of%20Hashing%20Methods%20Performance%20On%20Binary%20Feature%20Descriptors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Komorowski Jacek, Trzcinski Tomasz</td>
	<td>Arxiv</td>
	<td><p>In this paper we evaluate performance of data-dependent hashing methods on binary data. The goal is to find a hashing method that can effectively produce lower dimensional binary representation of 512-bit FREAK descriptors. A representative sample of recent unsupervised semi-supervised and supervised hashing methods was experimentally evaluated on large datasets of labelled binary FREAK feature descriptors.</p>
</td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/ertl2017superminhash/">Superminhash - A New Minwise Hashing Algorithm For Jaccard Similarity Estimation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Superminhash - A New Minwise Hashing Algorithm For Jaccard Similarity Estimation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Superminhash - A New Minwise Hashing Algorithm For Jaccard Similarity Estimation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Superminhash%20-%20A%20New%20Minwise%20Hashing%20Algorithm%20For%20Jaccard%20Similarity%20Estimation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Ertl Otmar</td>
	<td>Arxiv</td>
	<td><p>This paper presents a new algorithm for calculating hash signatures of sets which can be directly used for Jaccard similarity estimation. The new approach is an improvement over the MinHash algorithm because it has a better runtime behavior and the resulting signatures allow a more precise estimation of the Jaccard index.</p>
</td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/sivertsen2017fast/">Fast Nearest Neighbor Preserving Embeddings</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Fast Nearest Neighbor Preserving Embeddings' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Fast Nearest Neighbor Preserving Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Fast%20Nearest%20Neighbor%20Preserving%20Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Sivertsen Johan</td>
	<td>Arxiv</td>
	<td><p>We show an analog to the Fast Johnson-Lindenstrauss Transform for Nearest Neighbor Preserving Embeddings in (ell_2). These are sparse randomized embeddings that preserve the (approximate) nearest neighbors. The dimensionality of the embedding space is bounded not by the size of the embedded set n but by its doubling dimension lambda. For most large real-world datasets this will mean a considerably lower-dimensional embedding space than possible when preserving all distances. The resulting embeddings can be used with existing approximate nearest neighbor data structures to yield speed improvements.</p>
</td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/he2017hashing/">Hashing As Tie-aware Learning To Rank</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Hashing As Tie-aware Learning To Rank' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Hashing As Tie-aware Learning To Rank' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Hashing%20As%20Tie-aware%20Learning%20To%20Rank' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>He Kun, Cakir Fatih, Bargal Sarah Adel, Sclaroff Stan</td>
	<td>Arxiv</td>
	<td><p>Hashing or learning binary embeddings of data is frequently used in nearest neighbor retrieval. In this paper we develop learning to rank formulations for hashing aimed at directly optimizing ranking-based evaluation metrics such as Average Precision (AP) and Normalized Discounted Cumulative Gain (NDCG). We first observe that the integer-valued Hamming distance often leads to tied rankings and propose to use tie-aware versions of AP and NDCG to evaluate hashing for retrieval. Then to optimize tie-aware ranking metrics we derive their continuous relaxations and perform gradient-based optimization with deep neural networks. Our results establish the new state-of-the-art for image retrieval by Hamming ranking in common benchmarks.</p>
</td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/do2017simultaneous/">Simultaneous Feature Aggregating And Hashing For Large-scale Image Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Simultaneous Feature Aggregating And Hashing For Large-scale Image Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Simultaneous Feature Aggregating And Hashing For Large-scale Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Simultaneous%20Feature%20Aggregating%20And%20Hashing%20For%20Large-scale%20Image%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Do Thanh-toan, Tan Dang-khoa Le, Pham Trung T., Cheung Ngai-man</td>
	<td>Arxiv</td>
	<td><p>In most state-of-the-art hashing-based visual search systems local image descriptors of an image are first aggregated as a single feature vector. This feature vector is then subjected to a hashing function that produces a binary hash code. In previous work the aggregating and the hashing processes are designed independently. In this paper we propose a novel framework where feature aggregating and hashing are designed simultaneously and optimized jointly. Specifically our joint optimization produces aggregated representations that can be better reconstructed by some binary codes. This leads to more discriminative binary hash codes and improved retrieval accuracy. In addition we also propose a fast version of the recently-proposed Binary Autoencoder to be used in our proposed framework. We perform extensive retrieval experiments on several benchmark datasets with both SIFT and convolutional features. Our results suggest that the proposed framework achieves significant improvements over the state of the art.</p>
</td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/dahlgaard2017practical/">Practical Hash Functions For Similarity Estimation And Dimensionality Reduction</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Practical Hash Functions For Similarity Estimation And Dimensionality Reduction' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Practical Hash Functions For Similarity Estimation And Dimensionality Reduction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Practical%20Hash%20Functions%20For%20Similarity%20Estimation%20And%20Dimensionality%20Reduction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>SÃ¸ren Dahlgaard, Mathias Knudsen, Mikkel Thorup</td>
	<td>Neural Information Processing Systems</td>
	<td><p>Hashing is a basic tool for dimensionality reduction employed in several aspects of machine learning. However the perfomance analysis is often carried out under the abstract assumption that a truly random unit cost hash function is used without concern for which concrete hash function is employed. The concrete hash function may work fine on sufficiently random input. The question is if it can be trusted in the real world when faced with more structured input. In this paper we focus on two prominent applications of hashing namely similarity estimation with the one permutation hashing (OPH) scheme of Li et al. NIPS12 and feature hashing (FH) of Weinberger et al. ICML09 both of which have found numerous applications i.e. in approximate near-neighbour search with LSH and large-scale classification with SVM. We consider the recent mixed tabulation hash function of Dahlgaard et al. FOCS15 which was proved theoretically to perform like a truly random hash function in many applications including the above OPH. Here we first show improved concentration bounds for FH with truly random hashing and then argue that mixed tabulation performs similar when the input vectors are sparse. Our main contribution however is an experimental comparison of different hashing schemes when used inside FH OPH and LSH. We find that mixed tabulation hashing is almost as fast as the classic multiply-mod-prime scheme ax+b mod p. Mutiply-mod-prime is guaranteed to work well on sufficiently random data but we demonstrate that in the above applications it can lead to bias and poor concentration on both real-world and synthetic data. We also compare with the very popular MurmurHash3 which has no proven guarantees. Mixed tabulation and MurmurHash3 both perform similar to truly random hashing in our experiments. However mixed tabulation was 4037; faster than MurmurHash3 and it has the proven guarantee of good performance on all possible input making it more reliable.</p>
</td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/qiu2017foresthash/">Foresthash Semantic Hashing With Shallow Random Forests And Tiny Convolutional Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Foresthash Semantic Hashing With Shallow Random Forests And Tiny Convolutional Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Foresthash Semantic Hashing With Shallow Random Forests And Tiny Convolutional Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Foresthash%20Semantic%20Hashing%20With%20Shallow%20Random%20Forests%20And%20Tiny%20Convolutional%20Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Qiu Qiang, Lezama Jose, Bronstein Alex, Sapiro Guillermo</td>
	<td>Arxiv</td>
	<td><p>Hash codes are efficient data representations for coping with the ever growing amounts of data. In this paper we introduce a random forest semantic hashing scheme that embeds tiny convolutional neural networks (CNN) into shallow random forests with near-optimal information-theoretic code aggregation among trees. We start with a simple hashing scheme where random trees in a forest act as hashing functions by setting 1 for the visited tree leaf and 0 for the rest. We show that traditional random forests fail to generate hashes that preserve the underlying similarity between the trees rendering the random forests approach to hashing challenging. To address this we propose to first randomly group arriving classes at each tree split node into two groups obtaining a significantly simplified two-class classification problem which can be handled using a light-weight CNN weak learner. Such random class grouping scheme enables code uniqueness by enforcing each class to share its code with different classes in different trees. A non-conventional low-rank loss is further adopted for the CNN weak learners to encourage code consistency by minimizing intra-class variations and maximizing inter-class distance for the two random class groups. Finally we introduce an information-theoretic approach for aggregating codes of individual trees into a single hash code producing a near-optimal unique hash for each class. The proposed approach significantly outperforms state-of-the-art hashing methods for image retrieval tasks on large-scale public datasets while performing at the level of other state-of-the-art image classification techniques while utilizing a more compact and efficient scalable representation. This work proposes a principled and robust procedure to train and deploy in parallel an ensemble of light-weight CNNs instead of simply going deeper.</p>
</td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/svenstrup2017hash/">Hash Embeddings For Efficient Word Representations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Hash Embeddings For Efficient Word Representations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Hash Embeddings For Efficient Word Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Hash%20Embeddings%20For%20Efficient%20Word%20Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Svenstrup Dan, Hansen Jonas Meinertz, Winther Ole</td>
	<td>Arxiv</td>
	<td><p>We present hash embeddings an efficient method for representing words in a continuous vector form. A hash embedding may be seen as an interpolation between a standard word embedding and a word embedding created using a random hash function (the hashing trick). In hash embeddings each token is represented by (k) (d)-dimensional embeddings vectors and one (k) dimensional weight vector. The final (d) dimensional representation of the token is the product of the two. Rather than fitting the embedding vectors for each token these are selected by the hashing trick from a shared pool of (B) embedding vectors. Our experiments show that hash embeddings can easily deal with huge vocabularies consisting of millions of tokens. When using a hash embedding there is no need to create a dictionary before training nor to perform any kind of vocabulary pruning after training. We show that models trained using hash embeddings exhibit at least the same level of performance as models trained using regular embeddings across a wide range of tasks. Furthermore the number of parameters needed by such an embedding is only a fraction of what is required by a regular embedding. Since standard embeddings and embeddings constructed using the hashing trick are actually just special cases of a hash embedding hash embeddings can be considered an extension and improvement over the existing regular embedding types.</p>
</td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/lai2017improved/">Improved Search In Hamming Space Using Deep Multi-index Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Improved Search In Hamming Space Using Deep Multi-index Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Improved Search In Hamming Space Using Deep Multi-index Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Improved%20Search%20In%20Hamming%20Space%20Using%20Deep%20Multi-index%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Lai Hanjiang, Pan Yan</td>
	<td>Arxiv</td>
	<td><p>Similarity-preserving hashing is a widely-used method for nearest neighbour search in large-scale image retrieval tasks. There has been considerable research on generating efficient image representation via the deep-network-based hashing methods. However the issue of efficient searching in the deep representation space remains largely unsolved. To this end we propose a simple yet efficient deep-network-based multi-index hashing method for simultaneously learning the powerful image representation and the efficient searching. To achieve these two goals we introduce the multi-index hashing (MIH) mechanism into the proposed deep architecture which divides the binary codes into multiple substrings. Due to the non-uniformly distributed codes will result in inefficiency searching we add the two balanced constraints at feature-level and instance-level respectively. Extensive evaluations on several benchmark image retrieval datasets show that the learned balanced binary codes bring dramatic speedups and achieve comparable performance over the existing baselines.</p>
</td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/li2017deep/">Deep Supervised Discrete Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Supervised Discrete Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Supervised Discrete Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Supervised%20Discrete%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Qi Li, Zhenan Sun, Ran He, Tieniu Tan</td>
	<td>Neural Information Processing Systems</td>
	<td><p>With the rapid growth of image and video data on the web hashing has been extensively studied for image or video search in recent years. Benefiting from recent advances in deep learning deep hashing methods have achieved promising results for image retrieval. However there are some limitations of previous deep hashing methods (e.g. the semantic information is not fully exploited). In this paper we develop a deep supervised discrete hashing algorithm based on the assumption that the learned binary codes should be ideal for classification. Both the pairwise label information and the classification information are used to learn the hash codes within one stream framework. We constrain the outputs of the last layer to be binary codes directly which is rarely investigated in deep hashing algorithm. Because of the discrete nature of hash codes an alternating minimization method is used to optimize the objective function. Experimental results have shown that our method outperforms current state-of-the-art methods on benchmark datasets.</p>
</td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/cakir2017mihash/">Mihash Online Hashing With Mutual Information</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Mihash Online Hashing With Mutual Information' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Mihash Online Hashing With Mutual Information' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Mihash%20Online%20Hashing%20With%20Mutual%20Information' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Cakir Fatih, He Kun, Bargal Sarah Adel, Sclaroff Stan</td>
	<td>Arxiv</td>
	<td><p>Learning-based hashing methods are widely used for nearest neighbor retrieval and recently online hashing methods have demonstrated good performance-complexity trade-offs by learning hash functions from streaming data. In this paper we first address a key challenge for online hashing the binary codes for indexed data must be recomputed to keep pace with updates to the hash functions. We propose an efficient quality measure for hash functions based on an information-theoretic quantity mutual information and use it successfully as a criterion to eliminate unnecessary hash table updates. Next we also show how to optimize the mutual information objective using stochastic gradient descent. We thus develop a novel hashing method MIHash that can be used in both online and batch settings. Experiments on image retrieval benchmarks (including a 2.5M image dataset) confirm the effectiveness of our formulation both in reducing hash table recomputations and in learning high-quality hash functions.</p>
</td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/cai2017revisit/">A Revisit On Deep Hashings For Large-scale Content Based Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Revisit On Deep Hashings For Large-scale Content Based Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Revisit On Deep Hashings For Large-scale Content Based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20Revisit%20On%20Deep%20Hashings%20For%20Large-scale%20Content%20Based%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Cai Deng, Gu Xiuye, Wang Chaoqi</td>
	<td>Arxiv</td>
	<td><p>There is a growing trend in studying deep hashing methods for content-based image retrieval (CBIR) where hash functions and binary codes are learnt using deep convolutional neural networks and then the binary codes can be used to do approximate nearest neighbor (ANN) search. All the existing deep hashing papers report their methods superior performance over the traditional hashing methods according to their experimental results. However there are serious flaws in the evaluations of existing deep hashing papers (1) The datasets they used are too small and simple to simulate the real CBIR situation. (2) They did not correctly include the search time in their evaluation criteria while the search time is crucial in real CBIR systems. (3) The performance of some unsupervised hashing algorithms (e.g. LSH) can easily be boosted if one uses multiple hash tables which is an important factor should be considered in the evaluation while most of the deep hashing papers failed to do so. We re-evaluate several state-of-the-art deep hashing methods with a carefully designed experimental setting. Empirical results reveal that the performance of these deep hashing methods are inferior to multi-table IsoH a very simple unsupervised hashing method. Thus the conclusions in all the deep hashing papers should be carefully re-examined.</p>
</td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/liu2017deep/">Deep Hashing With Category Mask For Fast Video Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Hashing With Category Mask For Fast Video Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Hashing With Category Mask For Fast Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Hashing%20With%20Category%20Mask%20For%20Fast%20Video%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Liu Xu, Zhao Lili, Ding Dajun, Dong Yajiao</td>
	<td>Arxiv</td>
	<td><p>This paper proposes an end-to-end deep hashing framework with category mask for fast video retrieval. We train our network in a supervised way by fully exploiting inter-class diversity and intra-class identity. Classification loss is optimized to maximize inter-class diversity while intra-pair is introduced to learn representative intra-class identity. We investigate the binary bits distribution related to categories and find out that the effectiveness of binary bits is highly correlated with data categories and some bits may degrade classification performance of some categories. We then design hash code generation scheme with category mask to filter out bits with negative contribution. Experimental results demonstrate the proposed method outperforms several state-of-the-arts under various evaluation metrics on public datasets.</p>
</td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/zhou2017deep/">Deep Hashing With Triplet Quantization Loss</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Hashing With Triplet Quantization Loss' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Hashing With Triplet Quantization Loss' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Hashing%20With%20Triplet%20Quantization%20Loss' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhou Yuefu, Huang Shanshan, Zhang Ya, Wang Yanfeng</td>
	<td>Arxiv</td>
	<td><p>With the explosive growth of image databases deep hashing which learns compact binary descriptors for images has become critical for fast image retrieval. Many existing deep hashing methods leverage quantization loss defined as distance between the features before and after quantization to reduce the error from binarizing features. While minimizing the quantization loss guarantees that quantization has minimal effect on retrieval accuracy it unfortunately significantly reduces the expressiveness of features even before the quantization. In this paper we show that the above definition of quantization loss is too restricted and in fact not necessary for maintaining high retrieval accuracy. We therefore propose a new form of quantization loss measured in triplets. The core idea of the triplet quantization loss is to learn discriminative real-valued descriptors which lead to minimal loss on retrieval accuracy after quantization. Extensive experiments on two widely used benchmark data sets of different scales CIFAR-10 and In-shop demonstrate that the proposed method outperforms the state-of-the-art deep hashing methods. Moreover we show that the compact binary descriptors obtained with triplet quantization loss lead to very small performance drop after quantization.</p>
</td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/zhu2017part/">Part-based Deep Hashing For Large-scale Person Re-identification</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Part-based Deep Hashing For Large-scale Person Re-identification' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Part-based Deep Hashing For Large-scale Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Part-based%20Deep%20Hashing%20For%20Large-scale%20Person%20Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhu Fuqing, Kong Xiangwei, Zheng Liang, Fu Haiyan, Tian Qi</td>
	<td>Arxiv</td>
	<td><p>Large-scale is a trend in person re-identification (re-id). It is important that real-time search be performed in a large gallery. While previous methods mostly focus on discriminative learning this paper makes the attempt in integrating deep learning and hashing into one framework to evaluate the efficiency and accuracy for large-scale person re-id. We integrate spatial information for discriminative visual representation by partitioning the pedestrian image into horizontal parts. Specifically Part-based Deep Hashing (PDH) is proposed in which batches of triplet samples are employed as the input of the deep hashing architecture. Each triplet sample contains two pedestrian images (or parts) with the same identity and one pedestrian image (or part) of the different identity. A triplet loss function is employed with a constraint that the Hamming distance of pedestrian images (or parts) with the same identity is smaller than ones with the different identity. In the experiment we show that the proposed Part-based Deep Hashing method yields very competitive re-id accuracy on the large-scale Market-1501 and Market-1501+500K datasets.</p>
</td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/argerich2017generic/">Generic LSH Families For The Angular Distance Based On Johnson-lindenstrauss Projections And Feature Hashing LSH</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Generic LSH Families For The Angular Distance Based On Johnson-lindenstrauss Projections And Feature Hashing LSH' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Generic LSH Families For The Angular Distance Based On Johnson-lindenstrauss Projections And Feature Hashing LSH' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Generic%20LSH%20Families%20For%20The%20Angular%20Distance%20Based%20On%20Johnson-lindenstrauss%20Projections%20And%20Feature%20Hashing%20LSH' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Argerich Luis, Golmar Natalia</td>
	<td>Arxiv</td>
	<td><p>In this paper we propose the creation of generic LSH families for the angular distance based on Johnson-Lindenstrauss projections. We show that feature hashing is a valid J-L projection and propose two new LSH families based on feature hashing. These new LSH families are tested on both synthetic and real datasets with very good results and a considerable performance improvement over other LSH families. While the theoretical analysis is done for the angular distance these families can also be used in practice for the euclidean distance with excellent results 2. Our tests using real datasets show that the proposed LSH functions work well for the euclidean distance.</p>
</td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/komorowski2017random/">Random Binary Trees For Approximate Nearest Neighbour Search In Binary Space</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Random Binary Trees For Approximate Nearest Neighbour Search In Binary Space' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Random Binary Trees For Approximate Nearest Neighbour Search In Binary Space' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Random%20Binary%20Trees%20For%20Approximate%20Nearest%20Neighbour%20Search%20In%20Binary%20Space' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Komorowski Michal, Trzcinski Tomasz</td>
	<td>Arxiv</td>
	<td><p>Approximate nearest neighbour (ANN) search is one of the most important problems in computer science fields such as data mining or computer vision. In this paper we focus on ANN for high-dimensional binary vectors and we propose a simple yet powerful search method that uses Random Binary Search Trees (RBST). We apply our method to a dataset of 1.25M binary local feature descriptors obtained from a real-life image-based localisation system provided by Google as a part of Project Tango. An extensive evaluation of our method against the state-of-the-art variations of Locality Sensitive Hashing (LSH) namely Uniform LSH and Multi-probe LSH shows the superiority of our method in terms of retrieval precision with performance boost of over 2037;</p>
</td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/brooks2017multi/">Multi-level Spherical Locality Sensitive Hashing For Approximate Near Neighbors</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Multi-level Spherical Locality Sensitive Hashing For Approximate Near Neighbors' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Multi-level Spherical Locality Sensitive Hashing For Approximate Near Neighbors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Multi-level%20Spherical%20Locality%20Sensitive%20Hashing%20For%20Approximate%20Near%20Neighbors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Brooks Teresa Nicole, Almajalid Rania</td>
	<td>Arxiv</td>
	<td><p>This paper introduces Multi-Level Spherical LSH parameter-free a multi-level data-dependant Locality Sensitive Hashing data structure for solving the Approximate Near Neighbors Problem (ANN). This data structure uses a modified version of a multi-probe adaptive querying algorithm with the potential of achieving a (O(n^p + t)) query run time for all inputs n where t &lt;= n.</p>
</td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/jiang2017asymmetric/">Asymmetric Deep Supervised Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Asymmetric Deep Supervised Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Asymmetric Deep Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Asymmetric%20Deep%20Supervised%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Jiang Qing-yuan, Li Wu-jun</td>
	<td>Arxiv</td>
	<td><p>Hashing has been widely used for large-scale approximate nearest neighbor search because of its storage and search efficiency. Recent work has found that deep supervised hashing can significantly outperform non-deep supervised hashing in many applications. However most existing deep supervised hashing methods adopt a symmetric strategy to learn one deep hash function for both query points and database (retrieval) points. The training of these symmetric deep supervised hashing methods is typically time-consuming which makes them hard to effectively utilize the supervised information for cases with large-scale database. In this paper we propose a novel deep supervised hashing method called asymmetric deep supervised hashing (ADSH) for large-scale nearest neighbor search. ADSH treats the query points and database points in an asymmetric way. More specifically ADSH learns a deep hash function only for query points while the hash codes for database points are directly learned. The training of ADSH is much more efficient than that of traditional symmetric deep supervised hashing methods. Experiments show that ADSH can achieve state-of-the-art performance in real applications.</p>
</td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/huang2017unsupervised/">Unsupervised Triplet Hashing For Fast Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Triplet Hashing For Fast Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Unsupervised Triplet Hashing For Fast Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Unsupervised%20Triplet%20Hashing%20For%20Fast%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Huang Shanshan, Xiong Yichao, Zhang Ya, Wang Jia</td>
	<td>Arxiv</td>
	<td><p>Hashing has played a pivotal role in large-scale image retrieval. With the development of Convolutional Neural Network (CNN) hashing learning has shown great promise. But existing methods are mostly tuned for classification which are not optimized for retrieval tasks especially for instance-level retrieval. In this study we propose a novel hashing method for large-scale image retrieval. Considering the difficulty in obtaining labeled datasets for image retrieval task in large scale we propose a novel CNN-based unsupervised hashing method namely Unsupervised Triplet Hashing (UTH). The unsupervised hashing network is designed under the following three principles 1) more discriminative representations for image retrieval; 2) minimum quantization loss between the original real-valued feature descriptors and the learned hash codes; 3) maximum information entropy for the learned hash codes. Extensive experiments on CIFAR-10 MNIST and In-shop datasets have shown that UTH outperforms several state-of-the-art unsupervised hashing methods in terms of retrieval accuracy.</p>
</td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/tan2017supervised/">Supervised Hashing With End-to-end Binary Deep Neural Network</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Hashing With End-to-end Binary Deep Neural Network' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Supervised Hashing With End-to-end Binary Deep Neural Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Supervised%20Hashing%20With%20End-to-end%20Binary%20Deep%20Neural%20Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Tan Dang-khoa Le, Do Thanh-toan, Cheung Ngai-man</td>
	<td>Arxiv</td>
	<td><p>Image hashing is a popular technique applied to large scale content-based visual retrieval due to its compact and efficient binary codes. Our work proposes a new end-to-end deep network architecture for supervised hashing which directly learns binary codes from input images and maintains good properties over binary codes such as similarity preservation independence and balancing. Furthermore we also propose a new learning scheme that can cope with the binary constrained loss function. The proposed algorithm not only is scalable for learning over large-scale datasets but also outperforms state-of-the-art supervised hashing methods which are illustrated throughout extensive experiments from various image retrieval benchmarks.</p>
</td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/hoang2017enhance/">Enhance Feature Discrimination For Unsupervised Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Enhance Feature Discrimination For Unsupervised Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Enhance Feature Discrimination For Unsupervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Enhance%20Feature%20Discrimination%20For%20Unsupervised%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Hoang Tuan, Do Thanh-toan, Tan Dang-khoa Le, Cheung Ngai-man</td>
	<td>Arxiv</td>
	<td><p>We introduce a novel approach to improve unsupervised hashing. Specifically we propose a very efficient embedding method Gaussian Mixture Model embedding (Gemb). The proposed method using Gaussian Mixture Model embeds feature vector into a low-dimensional vector and simultaneously enhances the discriminative property of features before passing them into hashing. Our experiment shows that the proposed method boosts the hashing performance of many state-of-the-art e.g. Binary Autoencoder (BA) 1 Iterative Quantization (ITQ) 2 in standard evaluation metrics for the three main benchmark datasets.</p>
</td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/wu2017improved/">Improved Consistent Weighted Sampling Revisited</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Improved Consistent Weighted Sampling Revisited' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Improved Consistent Weighted Sampling Revisited' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Improved%20Consistent%20Weighted%20Sampling%20Revisited' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Wu Wei, Li Bin, Chen Ling, Zhang Chengqi, Yu Philip S.</td>
	<td>Arxiv</td>
	<td><p>Min-Hash is a popular technique for efficiently estimating the Jaccard similarity of binary sets. Consistent Weighted Sampling (CWS) generalizes the Min-Hash scheme to sketch weighted sets and has drawn increasing interest from the community. Due to its constant-time complexity independent of the values of the weights Improved CWS (ICWS) is considered as the state-of-the-art CWS algorithm. In this paper we revisit ICWS and analyze its underlying mechanism to show that there actually exists dependence between the two components of the hash-code produced by ICWS which violates the condition of independence. To remedy the problem we propose an Improved ICWS (I(^2)CWS) algorithm which not only shares the same theoretical computational complexity as ICWS but also abides by the required conditions of the CWS scheme. The experimental results on a number of synthetic data sets and real-world text data sets demonstrate that our I(^2)CWS algorithm can estimate the Jaccard similarity more accurately and also compete with or outperform the compared methods including ICWS in classification and top-(K) retrieval after relieving the underlying dependence.</p>
</td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/song2017deep/">Deep Discrete Hashing With Self-supervised Pairwise Labels</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Discrete Hashing With Self-supervised Pairwise Labels' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Discrete Hashing With Self-supervised Pairwise Labels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Discrete%20Hashing%20With%20Self-supervised%20Pairwise%20Labels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Song Jingkuan, He Tao, Fan Hangbo, Gao Lianli</td>
	<td>Arxiv</td>
	<td><p>Hashing methods have been widely used for applications of large-scale image retrieval and classification. Non-deep hashing methods using handcrafted features have been significantly outperformed by deep hashing methods due to their better feature representation and end-to-end learning framework. However the most striking successes in deep hashing have mostly involved discriminative models which require labels. In this paper we propose a novel unsupervised deep hashing method named Deep Discrete Hashing (DDH) for large-scale image retrieval and classification. In the proposed framework we address two main problems 1) how to directly learn discrete binary codes 2) how to equip the binary representation with the ability of accurate image retrieval and classification in an unsupervised way We resolve these problems by introducing an intermediate variable and a loss function steering the learning process which is based on the neighborhood structure in the original space. Experimental results on standard datasets (CIFAR-10 NUS-WIDE and Oxford-17) demonstrate that our DDH significantly outperforms existing hashing methods by large margin in terms of~mAP for image retrieval and object recognition. Code is available at urlhttps://github.com/htconquer/ddh}.</p>
</td>
</tr>



<tr>
	<td>2016</td>
	<td><a href="/publications/conjeti2016deep/">Deep Residual Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Residual Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Residual Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Residual%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Conjeti Sailesh, Roy Abhijit Guha, Katouzian Amin, Navab Nassir</td>
	<td>Arxiv</td>
	<td><p>Hashing aims at generating highly compact similarity preserving code words which are well suited for large-scale image retrieval tasks. Most existing hashing methods first encode the images as a vector of hand-crafted features followed by a separate binarization step to generate hash codes. This two-stage process may produce sub-optimal encoding. In this paper for the first time we propose a deep architecture for supervised hashing through residual learning termed Deep Residual Hashing (DRH) for an end-to-end simultaneous representation learning and hash coding. The DRH model constitutes four key elements (1) a sub-network with multiple stacked residual blocks; (2) hashing layer for binarization; (3) supervised retrieval loss function based on neighbourhood component analysis for similarity preserving embedding; and (4) hashing related losses and regularisation to control the quantization error and improve the quality of hash coding. We present results of extensive experiments on a large public chest x-ray image database with co-morbidities and discuss the outcome showing substantial improvements over the latest state-of-the art methods.</p>
</td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/yu2016variable/">Variable-length Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Variable-length Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Variable-length Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Variable-length%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Yu Honghai, Moulin Pierre, Ng Hong Wei, Li Xiaoli</td>
	<td>Arxiv</td>
	<td><p>Hashing has emerged as a popular technique for large-scale similarity search. Most learning-based hashing methods generate compact yet correlated hash codes. However this redundancy is storage-inefficient. Hence we propose a lossless variable-length hashing (VLH) method that is both storage- and search-efficient. Storage efficiency is achieved by converting the fixed-length hash code into a variable-length code. Search efficiency is obtained by using a multiple hash table structure. With VLH we are able to deliberately add redundancy into hash codes to improve retrieval performance with little sacrifice in storage efficiency or search complexity. In particular we propose a block K-means hashing (B-KMH) method to obtain significantly improved retrieval performance with no increase in storage and marginal increase in computational cost.</p>
</td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/raziperchikolaei2016optimizing/">Optimizing Affinity-based Binary Hashing Using Auxiliary Coordinates</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Optimizing Affinity-based Binary Hashing Using Auxiliary Coordinates' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Optimizing Affinity-based Binary Hashing Using Auxiliary Coordinates' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Optimizing%20Affinity-based%20Binary%20Hashing%20Using%20Auxiliary%20Coordinates' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Ramin Raziperchikolaei, Miguel A. Carreira-perpinan</td>
	<td>Neural Information Processing Systems</td>
	<td><p>In supervised binary hashing one wants to learn a function that maps a high-dimensional feature vector to a vector of binary codes for application to fast image retrieval. This typically results in a difficult optimization problem nonconvex and nonsmooth because of the discrete variables involved. Much work has simply relaxed the problem during training solving a continuous optimization and truncating the codes a posteriori. This gives reasonable results but is quite suboptimal. Recent work has tried to optimize the objective directly over the binary codes and achieved better results but the hash function was still learned a posteriori which remains suboptimal. We propose a general framework for learning hash functions using affinity-based loss functions that uses auxiliary coordinates. This closes the loop and optimizes jointly over the hash functions and the binary codes so that they gradually match each other. The resulting algorithm can be seen as an iterated version of the procedure of optimizing first over the codes and then learning the hash function. Compared to this our optimization is guaranteed to obtain better hash functions while being not much slower as demonstrated experimentally in various supervised datasets. In addition our framework facilitates the design of optimization algorithms for arbitrary types of loss and hash functions.</p>
</td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/curtin2016fast/">Fast Approximate Furthest Neighbors With Data-dependent Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Fast Approximate Furthest Neighbors With Data-dependent Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Fast Approximate Furthest Neighbors With Data-dependent Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Fast%20Approximate%20Furthest%20Neighbors%20With%20Data-dependent%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Curtin Ryan R., Gardner Andrew B.</td>
	<td>Arxiv</td>
	<td><p>We present a novel hashing strategy for approximate furthest neighbor search that selects projection bases using the data distribution. This strategy leads to an algorithm which we call DrusillaHash that is able to outperform existing approximate furthest neighbor strategies. Our strategy is motivated by an empirical study of the behavior of the furthest neighbor search problem which lends intuition for where our algorithm is most useful. We also present a variant of the algorithm that gives an absolute approximation guarantee; to our knowledge this is the first such approximate furthest neighbor hashing approach to give such a guarantee. Performance studies indicate that DrusillaHash can achieve comparable levels of approximation to other algorithms while giving up to an order of magnitude speedup. An implementation is available in the mlpack machine learning library (found at http://www.mlpack.org).</p>
</td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/carreiraperpi%C3%B1%C3%A1n2016ensemble/">An Ensemble Diversity Approach To Supervised Binary Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=An Ensemble Diversity Approach To Supervised Binary Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=An Ensemble Diversity Approach To Supervised Binary Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=An%20Ensemble%20Diversity%20Approach%20To%20Supervised%20Binary%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Carreira-perpiÃ±Ã¡n Miguel Ã., Raziperchikolaei Ramin</td>
	<td>Arxiv</td>
	<td><p>Binary hashing is a well-known approach for fast approximate nearest-neighbor search in information retrieval. Much work has focused on affinity-based objective functions involving the hash functions or binary codes. These objective functions encode neighborhood information between data points and are often inspired by manifold learning algorithms. They ensure that the hash functions differ from each other through constraints or penalty terms that encourage codes to be orthogonal or dissimilar across bits but this couples the binary variables and complicates the already difficult optimization. We propose a much simpler approach we train each hash function (or bit) independently from each other but introduce diversity among them using techniques from classifier ensembles. Surprisingly we find that not only is this faster and trivially parallelizable but it also improves over the more complex coupled objective function and achieves state-of-the-art precision and recall in experiments with image retrieval.</p>
</td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/yang2016zero/">Zero-shot Hashing Via Transferring Supervised Knowledge</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Zero-shot Hashing Via Transferring Supervised Knowledge' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Zero-shot Hashing Via Transferring Supervised Knowledge' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Zero-shot%20Hashing%20Via%20Transferring%20Supervised%20Knowledge' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Yang Yang, Chen Weilun, Luo Yadan, Shen Fumin, Shao Jie, Shen Heng Tao</td>
	<td>Arxiv</td>
	<td><p>Hashing has shown its efficiency and effectiveness in facilitating large-scale multimedia applications. Supervised knowledge e.g. semantic labels or pair-wise relationship) associated to data is capable of significantly improving the quality of hash codes and hash functions. However confronted with the rapid growth of newly-emerging concepts and multimedia data on the Web existing supervised hashing approaches may easily suffer from the scarcity and validity of supervised information due to the expensive cost of manual labelling. In this paper we propose a novel hashing scheme termed emphzero-shot hashing (ZSH) which compresses images of unseen categories to binary codes with hash functions learned from limited training data of seen categories. Specifically we project independent data labels i.e. 0/1-form label vectors) into semantic embedding space where semantic relationships among all the labels can be precisely characterized and thus seen supervised knowledge can be transferred to unseen classes. Moreover in order to cope with the semantic shift problem we rotate the embedded space to more suitably align the embedded semantics with the low-level visual feature space thereby alleviating the influence of semantic gap. In the meantime to exert positive effects on learning high-quality hash functions we further propose to preserve local structural property and discrete nature in binary codes. Besides we develop an efficient alternating algorithm to solve the ZSH model. Extensive experiments conducted on various real-life datasets show the superior zero-shot image retrieval performance of ZSH as compared to several state-of-the-art hashing methods.</p>
</td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/zhang2016query/">Query-adaptive Image Retrieval By Deep Weighted Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Query-adaptive Image Retrieval By Deep Weighted Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Query-adaptive Image Retrieval By Deep Weighted Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Query-adaptive%20Image%20Retrieval%20By%20Deep%20Weighted%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhang Jian, Peng Yuxin</td>
	<td>Arxiv</td>
	<td><p>Hashing methods have attracted much attention for large scale image retrieval. Some deep hashing methods have achieved promising results by taking advantage of the strong representation power of deep networks recently. However existing deep hashing methods treat all hash bits equally. On one hand a large number of images share the same distance to a query image due to the discrete Hamming distance which raises a critical issue of image retrieval where fine-grained rankings are very important. On the other hand different hash bits actually contribute to the image retrieval differently and treating them equally greatly affects the retrieval accuracy of image. To address the above two problems we propose the query-adaptive deep weighted hashing (QaDWH) approach which can perform fine-grained ranking for different queries by weighted Hamming distance. First a novel deep hashing network is proposed to learn the hash codes and corresponding class-wise weights jointly so that the learned weights can reflect the importance of different hash bits for different image classes. Second a query-adaptive image retrieval method is proposed which rapidly generates hash bit weights for different query images by fusing its semantic probability and the learned class-wise weights. Fine-grained image retrieval is then performed by the weighted Hamming distance which can provide more accurate ranking than the traditional Hamming distance. Experiments on four widely used datasets show that the proposed approach outperforms eight state-of-the-art hashing methods.</p>
</td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/cao2016transitive/">Transitive Hashing Network For Heterogeneous Multimedia Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Transitive Hashing Network For Heterogeneous Multimedia Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Transitive Hashing Network For Heterogeneous Multimedia Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Transitive%20Hashing%20Network%20For%20Heterogeneous%20Multimedia%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Cao Zhangjie, Long Mingsheng, Yang Qiang</td>
	<td>Arxiv</td>
	<td><p>Hashing has been widely applied to large-scale multimedia retrieval due to the storage and retrieval efficiency. Cross-modal hashing enables efficient retrieval from database of one modality in response to a query of another modality. Existing work on cross-modal hashing assumes heterogeneous relationship across modalities for hash function learning. In this paper we relax the strong assumption by only requiring such heterogeneous relationship in an auxiliary dataset different from the query/database domain. We craft a hybrid deep architecture to simultaneously learn the cross-modal correlation from the auxiliary dataset and align the dataset distributions between the auxiliary dataset and the query/database domain which generates transitive hash codes for heterogeneous multimedia retrieval. Extensive experiments exhibit that the proposed approach yields state of the art multimedia retrieval performance on public datasets i.e. NUS-WIDE ImageNet-YahooQA.</p>
</td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/pronobis2016sharing/">Sharing Hash Codes For Multiple Purposes</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Sharing Hash Codes For Multiple Purposes' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Sharing Hash Codes For Multiple Purposes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Sharing%20Hash%20Codes%20For%20Multiple%20Purposes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Pronobis Wikor, Panknin Danny, Kirschnick Johannes, Srinivasan Vignesh, Samek Wojciech, Markl Volker, Kaul Manohar, Mueller Klaus-robert, Nakajima Shinichi</td>
	<td>Arxiv</td>
	<td><p>Locality sensitive hashing (LSH) is a powerful tool for sublinear-time approximate nearest neighbor search and a variety of hashing schemes have been proposed for different dissimilarity measures. However hash codes significantly depend on the dissimilarity which prohibits users from adjusting the dissimilarity at query time. In this paper we propose multiple purpose LSH (mp-LSH) which shares the hash codes for different dissimilarities. mp-LSH supports L2 cosine and inner product dissimilarities and their corresponding weighted sums where the weights can be adjusted at query time. It also allows us to modify the importance of pre-defined groups of features. Thus mp-LSH enables us for example to retrieve similar items to a query with the user preference taken into account to find a similar material to a query with some properties (stability utility etc.) optimized and to turn on or off a part of multi-modal information (brightness color audio text etc.) in image/video retrieval. We theoretically and empirically analyze the performance of three variants of mp-LSH and demonstrate their usefulness on real-world data sets.</p>
</td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/pham2016scalability/">Scalability And Total Recall With Fast Coveringlsh</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Scalability And Total Recall With Fast Coveringlsh' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Scalability And Total Recall With Fast Coveringlsh' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Scalability%20And%20Total%20Recall%20With%20Fast%20Coveringlsh' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Pham Ninh, Pagh Rasmus</td>
	<td>Arxiv</td>
	<td><p>Locality-sensitive hashing (LSH) has emerged as the dominant algorithmic technique for similarity search with strong performance guarantees in high-dimensional spaces. A drawback of traditional LSH schemes is that they may have emphfalse negatives i.e. the recall is less than 10037;. This limits the applicability of LSH in settings requiring precise performance guarantees. Building on the recent theoretical CoveringLSH construction that eliminates false negatives we propose a fast and practical covering LSH scheme for Hamming space called emphFast CoveringLSH (fcLSH). Inheriting the design benefits of CoveringLSH our method avoids false negatives and always reports all near neighbors. Compared to CoveringLSH we achieve an asymptotic improvement to the hash function computation time from ((dL)) to ((d + L)) where (d) is the dimensionality of data and (L) is the number of hash tables. Our experiments on synthetic and real-world data sets demonstrate that emphfcLSH is comparable (and often superior) to traditional hashing-based approaches for search radius up to 20 in high-dimensional Hamming space.</p>
</td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/cao2016correlation/">Correlation Hashing Network For Efficient Cross-modal Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Correlation Hashing Network For Efficient Cross-modal Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Correlation Hashing Network For Efficient Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Correlation%20Hashing%20Network%20For%20Efficient%20Cross-modal%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Cao Yue, Long Mingsheng, Wang Jianmin, Yu Philip S.</td>
	<td>Arxiv</td>
	<td><p>Hashing is widely applied to approximate nearest neighbor search for large-scale multimodal retrieval with storage and computation efficiency. Cross-modal hashing improves the quality of hash coding by exploiting semantic correlations across different modalities. Existing cross-modal hashing methods first transform data into low-dimensional feature vectors and then generate binary codes by another separate quantization step. However suboptimal hash codes may be generated since the quantization error is not explicitly minimized and the feature representation is not jointly optimized with the binary codes. This paper presents a Correlation Hashing Network (CHN) approach to cross-modal hashing which jointly learns good data representation tailored to hash coding and formally controls the quantization error. The proposed CHN is a hybrid deep architecture that constitutes a convolutional neural network for learning good image representations a multilayer perception for learning good text representations two hashing layers for generating compact binary codes and a structured max-margin loss that integrates all things together to enable learning similarity-preserving and high-quality hash codes. Extensive empirical study shows that CHN yields state of the art cross-modal retrieval performance on standard benchmarks.</p>
</td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/kalantidis2016loh/">LOH And Behold Web-scale Visual Search Recommendation And Clustering Using Locally Optimized Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=LOH And Behold Web-scale Visual Search Recommendation And Clustering Using Locally Optimized Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=LOH And Behold Web-scale Visual Search Recommendation And Clustering Using Locally Optimized Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=LOH%20And%20Behold%20Web-scale%20Visual%20Search%20Recommendation%20And%20Clustering%20Using%20Locally%20Optimized%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Kalantidis Yannis, Kennedy Lyndon, Nguyen Huy, Mellina Clayton, Shamma David A.</td>
	<td>Arxiv</td>
	<td><p>We propose a novel hashing-based matching scheme called Locally Optimized Hashing (LOH) based on a state-of-the-art quantization algorithm that can be used for efficient large-scale search recommendation clustering and deduplication. We show that matching with LOH only requires set intersections and summations to compute and so is easily implemented in generic distributed computing systems. We further show application of LOH to a) large-scale search tasks where performance is on par with other state-of-the-art hashing approaches; b) large-scale recommendation where queries consisting of thousands of images can be used to generate accurate recommendations from collections of hundreds of millions of images; and c) efficient clustering with a graph-based algorithm that can be scaled to massive collections in a distributed environment or can be used for deduplication for small collections like search results performing better than traditional hashing approaches while only requiring a few milliseconds to run. In this paper we experiment on datasets of up to 100 million images but in practice our system can scale to larger collections and can be used for other types of data that have a vector representation in a Euclidean space.</p>
</td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/cai2016revisit/">A Revisit Of Hashing Algorithms For Approximate Nearest Neighbor Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Revisit Of Hashing Algorithms For Approximate Nearest Neighbor Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Revisit Of Hashing Algorithms For Approximate Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20Revisit%20Of%20Hashing%20Algorithms%20For%20Approximate%20Nearest%20Neighbor%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Cai Deng</td>
	<td>Arxiv</td>
	<td><p>Approximate Nearest Neighbor Search (ANNS) is a fundamental problem in many areas of machine learning and data mining. During the past decade numerous hashing algorithms are proposed to solve this problem. Every proposed algorithm claims outperform other state-of-the-art hashing methods. However the evaluation of these hashing papers was not thorough enough and those claims should be re-examined. The ultimate goal of an ANNS method is returning the most accurate answers (nearest neighbors) in the shortest time. If implemented correctly almost all the hashing methods will have their performance improved as the code length increases. However many existing hashing papers only report the performance with the code length shorter than 128. In this paper we carefully revisit the problem of search with a hash index and analyze the pros and cons of two popular hash index search procedures. Then we proposed a very simple but effective two level index structures and make a thorough comparison of eleven popular hashing algorithms. Surprisingly the random-projection-based Locality Sensitive Hashing (LSH) is the best performed algorithm which is in contradiction to the claims in all the other ten hashing papers. Despite the extreme simplicity of random-projection-based LSH our results show that the capability of this algorithm has been far underestimated. For the sake of reproducibility all the codes used in the paper are released on GitHub which can be used as a testing platform for a fair comparison between various hashing algorithms.</p>
</td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/shrivastava2016simple/">Simple And Efficient Weighted Minwise Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Simple And Efficient Weighted Minwise Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Simple And Efficient Weighted Minwise Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Simple%20And%20Efficient%20Weighted%20Minwise%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Anshumali Shrivastava</td>
	<td>Neural Information Processing Systems</td>
	<td><p>Weighted minwise hashing (WMH) is one of the fundamental subroutine required by many celebrated approximation algorithms commonly adopted in industrial practice for large -scale search and learning. The resource bottleneck with WMH is the computation of multiple (typically a few hundreds to thousands) independent hashes of the data. We propose a simple rejection type sampling scheme based on a carefully designed red-green map where we show that the number of rejected sample has exactly the same distribution as weighted minwise sampling. The running time of our method for many practical datasets is an order of magnitude smaller than existing methods. Experimental evaluations on real datasets show that for computing 500 WMH our proposal can be 60000x faster than the Ioffes method without losing any accuracy. Our method is also around 100x faster than approximate heuristics capitalizing on the efficient densified one permutation hashing schemes~citeProcOneHashLSHICML14ProcShrivastavaUAI14. Given the simplicity of our approach and its significant advantages we hope that it will replace existing implementations in practice.</p>
</td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/mor%C3%A8re2016nested/">Nested Invariance Pooling And RBM Hashing For Image Instance Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Nested Invariance Pooling And RBM Hashing For Image Instance Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Nested Invariance Pooling And RBM Hashing For Image Instance Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Nested%20Invariance%20Pooling%20And%20RBM%20Hashing%20For%20Image%20Instance%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>MorÃ¨re Olivier, Lin Jie, Veillard Antoine, Chandrasekhar Vijay, Poggio Tomaso</td>
	<td>Arxiv</td>
	<td><p>The goal of this work is the computation of very compact binary hashes for image instance retrieval. Our approach has two novel contributions. The first one is Nested Invariance Pooling (NIP) a method inspired from i-theory a mathematical theory for computing group invariant transformations with feed-forward neural networks. NIP is able to produce compact and well-performing descriptors with visual representations extracted from convolutional neural networks. We specifically incorporate scale translation and rotation invariances but the scheme can be extended to any arbitrary sets of transformations. We also show that using moments of increasing order throughout nesting is important. The NIP descriptors are then hashed to the target code size (32-256 bits) with a Restricted Boltzmann Machine with a novel batch-level regularization scheme specifically designed for the purpose of hashing (RBMH). A thorough empirical evaluation with state-of-the-art shows that the results obtained both with the NIP descriptors and the NIP+RBMH hashes are consistently outstanding across a wide range of datasets.</p>
</td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/argerich2016feature/">Hash2vec Feature Hashing For Word Embeddings</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Hash2vec Feature Hashing For Word Embeddings' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Hash2vec Feature Hashing For Word Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Hash2vec%20Feature%20Hashing%20For%20Word%20Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Argerich Luis, Zaffaroni JoaquÃ­n TorrÃ©, Cano MatÃ­as J</td>
	<td></td>
	<td><p>In this paper we propose the application of feature hashing to create word embeddings for natural language processing. Feature hashing has been used successfully to create document vectors in related tasks like document classification. In this work we show that feature hashing can be applied to obtain word embeddings in linear time with the size of the data. The results show that this algorithm that does not need training is able to capture the semantic meaning of words. We compare the results against GloVe showing that they are similar. As far as we know this is the first application of feature hashing to the word embeddings problem and the results indicate this is a scalable technique with practical results for NLP applications.</p>
</td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/zhang2016ssdh/">SSDH Semi-supervised Deep Hashing For Large Scale Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=SSDH Semi-supervised Deep Hashing For Large Scale Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=SSDH Semi-supervised Deep Hashing For Large Scale Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=SSDH%20Semi-supervised%20Deep%20Hashing%20For%20Large%20Scale%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhang Jian, Peng Yuxin</td>
	<td>Arxiv</td>
	<td><p>Hashing methods have been widely used for efficient similarity retrieval on large scale image database. Traditional hashing methods learn hash functions to generate binary codes from hand-crafted features which achieve limited accuracy since the hand-crafted features cannot optimally represent the image content and preserve the semantic similarity. Recently several deep hashing methods have shown better performance because the deep architectures generate more discriminative feature representations. However these deep hashing methods are mainly designed for supervised scenarios which only exploit the semantic similarity information but ignore the underlying data structures. In this paper we propose the semi-supervised deep hashing (SSDH) approach to perform more effective hash function learning by simultaneously preserving semantic similarity and underlying data structures. The main contributions are as follows (1) We propose a semi-supervised loss to jointly minimize the empirical error on labeled data as well as the embedding error on both labeled and unlabeled data which can preserve the semantic similarity and capture the meaningful neighbors on the underlying data structures for effective hashing. (2) A semi-supervised deep hashing network is designed to extensively exploit both labeled and unlabeled data in which we propose an online graph construction method to benefit from the evolving deep features during training to better capture semantic neighbors. To the best of our knowledge the proposed deep network is the first deep hashing method that can perform hash code learning and feature learning simultaneously in a semi-supervised fashion. Experimental results on 5 widely-used datasets show that our proposed approach outperforms the state-of-the-art hashing methods.</p>
</td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/a2016ensemble/">An Ensemble Diversity Approach To Supervised Binary Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=An Ensemble Diversity Approach To Supervised Binary Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=An Ensemble Diversity Approach To Supervised Binary Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=An%20Ensemble%20Diversity%20Approach%20To%20Supervised%20Binary%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Miguel A. Carreira-perpinan, Ramin Raziperchikolaei</td>
	<td>Neural Information Processing Systems</td>
	<td><p>Binary hashing is a well-known approach for fast approximate nearest-neighbor search in information retrieval. Much work has focused on affinity-based objective functions involving the hash functions or binary codes. These objective functions encode neighborhood information between data points and are often inspired by manifold learning algorithms. They ensure that the hash functions differ from each other through constraints or penalty terms that encourage codes to be orthogonal or dissimilar across bits but this couples the binary variables and complicates the already difficult optimization. We propose a much simpler approach we train each hash function (or bit) independently from each other but introduce diversity among them using techniques from classifier ensembles. Surprisingly we find that not only is this faster and trivially parallelizable but it also improves over the more complex coupled objective function and achieves state-of-the-art precision and recall in experiments with image retrieval.</p>
</td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/ercoli2016compact/">Compact Hash Codes For Efficient Visual Descriptors Retrieval In Large Scale Databases</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Compact Hash Codes For Efficient Visual Descriptors Retrieval In Large Scale Databases' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Compact Hash Codes For Efficient Visual Descriptors Retrieval In Large Scale Databases' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Compact%20Hash%20Codes%20For%20Efficient%20Visual%20Descriptors%20Retrieval%20In%20Large%20Scale%20Databases' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Ercoli Simone, Bertini Marco, Del Bimbo Alberto</td>
	<td>Arxiv</td>
	<td><p>In this paper we present an efficient method for visual descriptors retrieval based on compact hash codes computed using a multiple k-means assignment. The method has been applied to the problem of approximate nearest neighbor (ANN) search of local and global visual content descriptors and it has been tested on different datasets three large scale public datasets of up to one billion descriptors (BIGANN) and supported by recent progress in convolutional neural networks (CNNs) also on the CIFAR-10 and MNIST datasets. Experimental results show that despite its simplicity the proposed method obtains a very high performance that makes it superior to more complex state-of-the-art methods.</p>
</td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/lai2016instance/">Instance-aware Hashing For Multi-label Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Instance-aware Hashing For Multi-label Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Instance-aware Hashing For Multi-label Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Instance-aware%20Hashing%20For%20Multi-label%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Lai Hanjiang, Yan Pan, Shu Xiangbo, Wei Yunchao, Yan Shuicheng</td>
	<td>Arxiv</td>
	<td><p>Similarity-preserving hashing is a commonly used method for nearest neighbour search in large-scale image retrieval. For image retrieval deep-networks-based hashing methods are appealing since they can simultaneously learn effective image representations and compact hash codes. This paper focuses on deep-networks-based hashing for multi-label images each of which may contain objects of multiple categories. In most existing hashing methods each image is represented by one piece of hash code which is referred to as semantic hashing. This setting may be suboptimal for multi-label image retrieval. To solve this problem we propose a deep architecture that learns textbfinstance-aware image representations for multi-label image data which are organized in multiple groups with each group containing the features for one category. The instance-aware representations not only bring advantages to semantic hashing but also can be used in category-aware hashing in which an image is represented by multiple pieces of hash codes and each piece of code corresponds to a category. Extensive evaluations conducted on several benchmark datasets demonstrate that for both semantic hashing and category-aware hashing the proposed method shows substantial improvement over the state-of-the-art supervised and unsupervised hashing methods.</p>
</td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/liu2016dual/">Dual Purpose Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Dual Purpose Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Dual Purpose Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Dual%20Purpose%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Liu Haomiao, Wang Ruiping, Shan Shiguang, Chen Xilin</td>
	<td>Arxiv</td>
	<td><p>Recent years have seen more and more demand for a unified framework to address multiple realistic image retrieval tasks concerning both category and attributes. Considering the scale of modern datasets hashing is favorable for its low complexity. However most existing hashing methods are designed to preserve one single kind of similarity thus improper for dealing with the different tasks simultaneously. To overcome this limitation we propose a new hashing method named Dual Purpose Hashing (DPH) which jointly preserves the category and attribute similarities by exploiting the Convolutional Neural Network (CNN) models to hierarchically capture the correlations between category and attributes. Since images with both category and attribute labels are scarce our method is designed to take the abundant partially labelled images on the Internet as training inputs. With such a framework the binary codes of new-coming images can be readily obtained by quantizing the network outputs of a binary-like layer and the attributes can be recovered from the codes easily. Experiments on two large-scale datasets show that our dual purpose hash codes can achieve comparable or even better performance than those state-of-the-art methods specifically designed for each individual retrieval task while being more compact than the compared methods.</p>
</td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/wang2016deep/">Deep Supervised Hashing With Triplet Labels</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Supervised Hashing With Triplet Labels' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Supervised Hashing With Triplet Labels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Supervised%20Hashing%20With%20Triplet%20Labels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Wang Xiaofang, Shi Yi, Kitani Kris M.</td>
	<td>Arxiv</td>
	<td><p>Hashing is one of the most popular and powerful approximate nearest neighbor search techniques for large-scale image retrieval. Most traditional hashing methods first represent images as off-the-shelf visual features and then produce hashing codes in a separate stage. However off-the-shelf visual features may not be optimally compatible with the hash code learning procedure which may result in sub-optimal hash codes. Recently deep hashing methods have been proposed to simultaneously learn image features and hash codes using deep neural networks and have shown superior performance over traditional hashing methods. Most deep hashing methods are given supervised information in the form of pairwise labels or triplet labels. The current state-of-the-art deep hashing method DPSH~citeli2015feature which is based on pairwise labels performs image feature learning and hash code learning simultaneously by maximizing the likelihood of pairwise similarities. Inspired by DPSH~citeli2015feature we propose a triplet label based deep hashing method which aims to maximize the likelihood of the given triplet labels. Experimental results show that our method outperforms all the baselines on CIFAR-10 and NUS-WIDE datasets including the state-of-the-art method DPSH~citeli2015feature and all the previous triplet label based deep hashing methods.</p>
</td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/ozdemir2016scalable/">Scalable Gaussian Processes For Supervised Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Scalable Gaussian Processes For Supervised Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Scalable Gaussian Processes For Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Scalable%20Gaussian%20Processes%20For%20Supervised%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Ozdemir Bahadir, Davis Larry S.</td>
	<td>Arxiv</td>
	<td><p>We propose a flexible procedure for large-scale image search by hash functions with kernels. Our method treats binary codes and pairwise semantic similarity as latent and observed variables respectively in a probabilistic model based on Gaussian processes for binary classification. We present an efficient inference algorithm with the sparse pseudo-input Gaussian process (SPGP) model and parallelization. Experiments on three large-scale image dataset demonstrate the effectiveness of the proposed hashing method Gaussian Process Hashing (GPH) for short binary codes and the datasets without predefined classes in comparison to the state-of-the-art supervised hashing methods.</p>
</td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/salvi2016bloom/">Bloom Filters And Compact Hash Codes For Efficient And Distributed Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Bloom Filters And Compact Hash Codes For Efficient And Distributed Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Bloom Filters And Compact Hash Codes For Efficient And Distributed Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Bloom%20Filters%20And%20Compact%20Hash%20Codes%20For%20Efficient%20And%20Distributed%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Salvi Andrea, Ercoli Simone, Bertini Marco, Del Bimbo Alberto</td>
	<td>Arxiv</td>
	<td><p>This paper presents a novel method for efficient image retrieval based on a simple and effective hashing of CNN features and the use of an indexing structure based on Bloom filters. These filters are used as gatekeepers for the database of image features allowing to avoid to perform a query if the query features are not stored in the database and speeding up the query process without affecting retrieval performance. Thanks to the limited memory requirements the system is suitable for mobile applications and distributed databases associating each filter to a distributed portion of the database. Experimental validation has been performed on three standard image retrieval datasets outperforming state-of-the-art hashing methods in terms of precision while the proposed indexing method obtains a (2times) speedup.</p>
</td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/sablayrolles2016how/">How Should We Evaluate Supervised Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=How Should We Evaluate Supervised Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=How Should We Evaluate Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=How%20Should%20We%20Evaluate%20Supervised%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Sablayrolles Alexandre, Douze Matthijs, JÃ©gou HervÃ©, Usunier Nicolas</td>
	<td>Arxiv</td>
	<td><p>Hashing produces compact representations for documents to perform tasks like classification or retrieval based on these short codes. When hashing is supervised the codes are trained using labels on the training data. This paper first shows that the evaluation protocols used in the literature for supervised hashing are not satisfactory we show that a trivial solution that encodes the output of a classifier significantly outperforms existing supervised or semi-supervised methods while using much shorter codes. We then propose two alternative protocols for supervised hashing one based on retrieval on a disjoint set of classes and another based on transfer learning to new classes. We provide two baseline methods for image-related tasks to assess the performance of (semi-)supervised hashing without coding and with unsupervised codes. These baselines give a lower- and upper-bound on the performance of a supervised hashing scheme.</p>
</td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/jiang2016deep/">Deep Cross-modal Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Cross-modal Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Cross-modal%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Jiang Qing-yuan, Li Wu-jun</td>
	<td>Arxiv</td>
	<td><p>Due to its low storage cost and fast query speed cross-modal hashing (CMH) has been widely used for similarity search in multimedia retrieval applications. However almost all existing CMH methods are based on hand-crafted features which might not be optimally compatible with the hash-code learning procedure. As a result existing CMH methods with handcrafted features may not achieve satisfactory performance. In this paper we propose a novel cross-modal hashing method called deep crossmodal hashing (DCMH) by integrating feature learning and hash-code learning into the same framework. DCMH is an end-to-end learning framework with deep neural networks one for each modality to perform feature learning from scratch. Experiments on two real datasets with text-image modalities show that DCMH can outperform other baselines to achieve the state-of-the-art performance in cross-modal retrieval applications.</p>
</td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/wang2016algorithm/">An Algorithm For L1 Nearest Neighbor Search Via Monotonic Embedding</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=An Algorithm For L1 Nearest Neighbor Search Via Monotonic Embedding' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=An Algorithm For L1 Nearest Neighbor Search Via Monotonic Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=An%20Algorithm%20For%20L1%20Nearest%20Neighbor%20Search%20Via%20Monotonic%20Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Xinan Wang, Sanjoy Dasgupta</td>
	<td>Neural Information Processing Systems</td>
	<td><p>Fast algorithms for nearest neighbor (NN) search have in large part focused on L2 distance. Here we develop an approach for L1 distance that begins with an explicit and exact embedding of the points into L2. We show how this embedding can efficiently be combined with random projection methods for L2 NN search such as locality-sensitive hashing or random projection trees. We rigorously establish the correctness of the methodology and show by experimentation that it is competitive in practice with available alternatives.</p>
</td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/zhang2016scalable/">Scalable Discrete Supervised Hash Learning With Asymmetric Matrix Factorization</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Scalable Discrete Supervised Hash Learning With Asymmetric Matrix Factorization' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Scalable Discrete Supervised Hash Learning With Asymmetric Matrix Factorization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Scalable%20Discrete%20Supervised%20Hash%20Learning%20With%20Asymmetric%20Matrix%20Factorization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhang Shifeng, Li Jianmin, Guo Jinma, Zhang Bo</td>
	<td>Arxiv</td>
	<td><p>Hashing method maps similar data to binary hashcodes with smaller hamming distance and it has received a broad attention due to its low storage cost and fast retrieval speed. However the existing limitations make the present algorithms difficult to deal with large-scale datasets (1) discrete constraints are involved in the learning of the hash function; (2) pairwise or triplet similarity is adopted to generate efficient hashcodes resulting both time and space complexity are greater than O(n^2). To address these issues we propose a novel discrete supervised hash learning framework which can be scalable to large-scale datasets. First the discrete learning procedure is decomposed into a binary classifier learning scheme and binary codes learning scheme which makes the learning procedure more efficient. Second we adopt the Asymmetric Low-rank Matrix Factorization and propose the Fast Clustering-based Batch Coordinate Descent method such that the time and space complexity is reduced to O(n). The proposed framework also provides a flexible paradigm to incorporate with arbitrary hash function including deep neural networks and kernel methods. Experiments on large-scale datasets demonstrate that the proposed method is superior or comparable with state-of-the-art hashing algorithms.</p>
</td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/aghazadeh2016near/">Near-isometric Binary Hashing For Large-scale Datasets</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Near-isometric Binary Hashing For Large-scale Datasets' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Near-isometric Binary Hashing For Large-scale Datasets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Near-isometric%20Binary%20Hashing%20For%20Large-scale%20Datasets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Aghazadeh Amirali, Lan Andrew, Shrivastava Anshumali, Baraniuk Richard</td>
	<td>Arxiv</td>
	<td><p>We develop a scalable algorithm to learn binary hash codes for indexing large-scale datasets. Near-isometric binary hashing (NIBH) is a data-dependent hashing scheme that quantizes the output of a learned low-dimensional embedding to obtain a binary hash code. In contrast to conventional hashing schemes which typically rely on an (ell_2)-norm (i.e. average distortion) minimization NIBH is based on a (ell_infty)-norm (i.e. worst-case distortion) minimization that provides several benefits including superior distance ranking and near-neighbor preservation performance. We develop a practical and efficient algorithm for NIBH based on column generation that scales well to large datasets. A range of experimental evaluations demonstrate the superiority of NIBH over ten state-of-the-art binary hashing schemes.</p>
</td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/wang2016unsupervised/">Unsupervised Cross-media Hashing With Structure Preservation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Cross-media Hashing With Structure Preservation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Unsupervised Cross-media Hashing With Structure Preservation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Unsupervised%20Cross-media%20Hashing%20With%20Structure%20Preservation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Wang Xiangyu, Chia Alex Yong-sang</td>
	<td>Arxiv</td>
	<td><p>Recent years have seen the exponential growth of heterogeneous multimedia data. The need for effective and accurate data retrieval from heterogeneous data sources has attracted much research interest in cross-media retrieval. Here given a query of any media type cross-media retrieval seeks to find relevant results of different media types from heterogeneous data sources. To facilitate large-scale cross-media retrieval we propose a novel unsupervised cross-media hashing method. Our method incorporates local affinity and distance repulsion constraints into a matrix factorization framework. Correspondingly the proposed method learns hash functions that generates unified hash codes from different media types while ensuring intrinsic geometric structure of the data distribution is preserved. These hash codes empower the similarity between data of different media types to be evaluated directly. Experimental results on two large-scale multimedia datasets demonstrate the effectiveness of the proposed method where we outperform the state-of-the-art methods.</p>
</td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/ozdemir2016supervised/">Supervised Incremental Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Incremental Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Supervised Incremental Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Supervised%20Incremental%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Ozdemir Bahadir, Najibi Mahyar, Davis Larry S.</td>
	<td>Arxiv</td>
	<td><p>We propose an incremental strategy for learning hash functions with kernels for large-scale image search. Our method is based on a two-stage classification framework that treats binary codes as intermediate variables between the feature space and the semantic space. In the first stage of classification binary codes are considered as class labels by a set of binary SVMs; each corresponds to one bit. In the second stage binary codes become the input space of a multi-class SVM. Hash functions are learned by an efficient algorithm where the NP-hard problem of finding optimal binary codes is solved via cyclic coordinate descent and SVMs are trained in a parallelized incremental manner. For modifications like adding images from a previously unseen class we describe an incremental procedure for effective and efficient updates to the previous hash functions. Experiments on three large-scale image datasets demonstrate the effectiveness of the proposed hashing method Supervised Incremental Hashing (SIH) over the state-of-the-art supervised hashing methods.</p>
</td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/huang2016local/">Local Similarity-aware Deep Feature Embedding</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Local Similarity-aware Deep Feature Embedding' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Local Similarity-aware Deep Feature Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Local%20Similarity-aware%20Deep%20Feature%20Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Chen Huang, Chen Change Loy, Xiaoou Tang</td>
	<td>Neural Information Processing Systems</td>
	<td><p>Existing deep embedding methods in vision tasks are capable of learning a compact Euclidean space from images where Euclidean distances correspond to a similarity metric. To make learning more effective and efficient hard sample mining is usually employed with samples identified through computing the Euclidean feature distance. However the global Euclidean distance cannot faithfully characterize the true feature similarity in a complex visual feature space where the intraclass distance in a high-density region may be larger than the interclass distance in low-density regions. In this paper we introduce a Position-Dependent Deep Metric (PDDM) unit which is capable of learning a similarity metric adaptive to local feature structure. The metric can be used to select genuinely hard samples in a local neighborhood to guide the deep embedding learning in an online and robust manner. The new layer is appealing in that it is pluggable to any convolutional networks and is trained end-to-end. Our local similarity-aware feature embedding not only demonstrates faster convergence and boosted performance on two complex image retrieval datasets its large margin nature also leads to superior generalization results under the large and open set scenarios of transfer learning and zero-shot learning on ImageNet 2010 and ImageNet-10K datasets.</p>
</td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/mu2016deep/">Deep Hashing A Joint Approach For Image Signature Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Hashing A Joint Approach For Image Signature Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Hashing A Joint Approach For Image Signature Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Hashing%20A%20Joint%20Approach%20For%20Image%20Signature%20Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Mu Yadong, Liu Zhu</td>
	<td>Arxiv</td>
	<td><p>Similarity-based image hashing represents crucial technique for visual data storage reduction and expedited image search. Conventional hashing schemes typically feed hand-crafted features into hash functions which separates the procedures of feature extraction and hash function learning. In this paper we propose a novel algorithm that concurrently performs feature engineering and non-linear supervised hashing function learning. Our technical contributions in this paper are two-folds 1) deep network optimization is often achieved by gradient propagation which critically requires a smooth objective function. The discrete nature of hash codes makes them not amenable for gradient-based optimization. To address this issue we propose an exponentiated hashing loss function and its bilinear smooth approximation. Effective gradient calculation and propagation are thereby enabled; 2) pre-training is an important trick in supervised deep learning. The impact of pre-training on the hash code quality has never been discussed in current deep hashing literature. We propose a pre-training scheme inspired by recent advance in deep network based image classification and experimentally demonstrate its effectiveness. Comprehensive quantitative evaluations are conducted on several widely-used image benchmarks. On all benchmarks our proposed deep hashing algorithm outperforms all state-of-the-art competitors by significant margins. In particular our algorithm achieves a near-perfect 0.99 in terms of Hamming ranking accuracy with only 12 bits on MNIST and a new record of 0.74 on the CIFAR10 dataset. In comparison the best accuracies obtained on CIFAR10 by existing hashing algorithms without or with deep networks are known to be 0.36 and 0.58 respectively.</p>
</td>
</tr>



<tr>
	<td>2015</td>
	<td><a href="/publications/li2015rank/">Rank Subspace Learning For Compact Hash Codes</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Rank Subspace Learning For Compact Hash Codes' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Rank Subspace Learning For Compact Hash Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Rank%20Subspace%20Learning%20For%20Compact%20Hash%20Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Li Kai, Qi Guojun, Ye Jun, Hua Kien A.</td>
	<td>Arxiv</td>
	<td><p>The era of Big Data has spawned unprecedented interests in developing hashing algorithms for efficient storage and fast nearest neighbor search. Most existing work learn hash functions that are numeric quantizations of feature values in projected feature space. In this work we propose a novel hash learning framework that encodes features rank orders instead of numeric values in a number of optimal low-dimensional ranking subspaces. We formulate the ranking subspace learning problem as the optimization of a piece-wise linear convex-concave function and present two versions of our algorithm one with independent optimization of each hash bit and the other exploiting a sequential learning framework. Our work is a generalization of the Winner-Take-All (WTA) hash family and naturally enjoys all the numeric stability benefits of rank correlation measures while being optimized to achieve high precision at very short code length. We compare with several state-of-the-art hashing algorithms in both supervised and unsupervised domain showing superior performance in a number of data sets.</p>
</td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/zhang2015bit/">Bit-scalable Deep Hashing With Regularized Similarity Learning For Image Retrieval And Person Re-identification</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Bit-scalable Deep Hashing With Regularized Similarity Learning For Image Retrieval And Person Re-identification' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Bit-scalable Deep Hashing With Regularized Similarity Learning For Image Retrieval And Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Bit-scalable%20Deep%20Hashing%20With%20Regularized%20Similarity%20Learning%20For%20Image%20Retrieval%20And%20Person%20Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhang Ruimao, Lin Liang, Zhang Rui, Zuo Wangmeng, Zhang Lei</td>
	<td>Arxiv</td>
	<td><p>Extracting informative image features and learning effective approximate hashing functions are two crucial steps in image retrieval . Conventional methods often study these two steps separately e.g. learning hash functions from a predefined hand-crafted feature space. Meanwhile the bit lengths of output hashing codes are preset in most previous methods neglecting the significance level of different bits and restricting their practical flexibility. To address these issues we propose a supervised learning framework to generate compact and bit-scalable hashing codes directly from raw images. We pose hashing learning as a problem of regularized similarity learning. Specifically we organize the training images into a batch of triplet samples each sample containing two images with the same label and one with a different label. With these triplet samples we maximize the margin between matched pairs and mismatched pairs in the Hamming space. In addition a regularization term is introduced to enforce the adjacency consistency i.e. images of similar appearances should have similar codes. The deep convolutional neural network is utilized to train the model in an end-to-end fashion where discriminative image features and hash functions are simultaneously optimized. Furthermore each bit of our hashing codes is unequally weighted so that we can manipulate the code lengths by truncating the insignificant bits. Our framework outperforms state-of-the-arts on public benchmarks of similar image search and also achieves promising results in the application of person re-identification in surveillance. It is also shown that the generated bit-scalable hashing codes well preserve the discriminative powers with shorter code lengths.</p>
</td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/rao2015diverse/">Diverse Yet Efficient Retrieval Using Hash Functions</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Diverse Yet Efficient Retrieval Using Hash Functions' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Diverse Yet Efficient Retrieval Using Hash Functions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Diverse%20Yet%20Efficient%20Retrieval%20Using%20Hash%20Functions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Rao Vidyadhar, Jain Prateek, Jawahar C. V</td>
	<td>Arxiv</td>
	<td><p>Typical retrieval systems have three requirements a) Accurate retrieval i.e. the method should have high precision b) Diverse retrieval i.e. the obtained set of points should be diverse c) Retrieval time should be small. However most of the existing methods address only one or two of the above mentioned requirements. In this work we present a method based on randomized locality sensitive hashing which tries to address all of the above requirements simultaneously. While earlier hashing approaches considered approximate retrieval to be acceptable only for the sake of efficiency we argue that one can further exploit approximate retrieval to provide impressive trade-offs between accuracy and diversity. We extend our method to the problem of multi-label prediction where the goal is to output a diverse and accurate set of labels for a given document in real-time. Moreover we introduce a new notion to simultaneously evaluate a methods performance for both the precision and diversity measures. Finally we present empirical results on several different retrieval tasks and show that our method retrieves diverse and accurate images/labels while ensuring (100x)-speed-up over the existing diverse retrieval approaches.</p>
</td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/zhong2015deep/">A Deep Hashing Learning Network</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Deep Hashing Learning Network' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Deep Hashing Learning Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20Deep%20Hashing%20Learning%20Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhong Guoqiang, Yang Pan, Wang Sijiang, Dong Junyu</td>
	<td>Arxiv</td>
	<td><p>Hashing-based methods seek compact and efficient binary codes that preserve the neighborhood structure in the original data space. For most existing hashing methods an image is first encoded as a vector of hand-crafted visual feature followed by a hash projection and quantization step to get the compact binary vector. Most of the hand-crafted features just encode the low-level information of the input the feature may not preserve the semantic similarities of images pairs. Meanwhile the hashing function learning process is independent with the feature representation so the feature may not be optimal for the hashing projection. In this paper we propose a supervised hashing method based on a well designed deep convolutional neural network which tries to learn hashing code and compact representations of data simultaneously. The proposed model learn the binary codes by adding a compact sigmoid layer before the loss layer. Experiments on several image data sets show that the proposed model outperforms other state-of-the-art methods.</p>
</td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/huang2015hash/">Hash Function Learning Via Codewords</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Hash Function Learning Via Codewords' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Hash Function Learning Via Codewords' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Hash%20Function%20Learning%20Via%20Codewords' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Huang Yinjie, Georgiopoulos Michael, Anagnostopoulos Georgios C.</td>
	<td>Arxiv</td>
	<td><p>In this paper we introduce a novel hash learning framework that has two main distinguishing features when compared to past approaches. First it utilizes codewords in the Hamming space as ancillary means to accomplish its hash learning task. These codewords which are inferred from the data attempt to capture similarity aspects of the datas hash codes. Secondly and more importantly the same framework is capable of addressing supervised unsupervised and even semi-supervised hash learning tasks in a natural manner. A series of comparative experiments focused on content-based image retrieval highlights its performance advantages.</p>
</td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/cakir2015online/">Online Supervised Hashing For Ever-growing Datasets</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Online Supervised Hashing For Ever-growing Datasets' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Online Supervised Hashing For Ever-growing Datasets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Online%20Supervised%20Hashing%20For%20Ever-growing%20Datasets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Cakir Fatih, Bargal Sarah Adel, Sclaroff Stan</td>
	<td>Arxiv</td>
	<td><p>Supervised hashing methods are widely-used for nearest neighbor search in computer vision applications. Most state-of-the-art supervised hashing approaches employ batch-learners. Unfortunately batch-learning strategies can be inefficient when confronted with large training datasets. Moreover with batch-learners it is unclear how to adapt the hash functions as a dataset continues to grow and diversify over time. Yet in many practical scenarios the dataset grows and diversifies; thus both the hash functions and the indexing must swiftly accommodate these changes. To address these issues we propose an online hashing method that is amenable to changes and expansions of the datasets. Since it is an online algorithm our approach offers linear complexity with the dataset size. Our solution is supervised in that we incorporate available label information to preserve the semantic neighborhood. Such an adaptive hashing method is attractive; but it requires recomputing the hash table as the hash functions are updated. If the frequency of update is high then recomputing the hash table entries may cause inefficiencies in the system especially for large indexes. Thus we also propose a framework to reduce hash table updates. We compare our method to state-of-the-art solutions on two benchmarks and demonstrate significant improvements over previous work.</p>
</td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/yoshikawa2015cross/">Cross-domain Matching For Bag-of-words Data Via Kernel Embeddings Of Latent Distributions</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Cross-domain Matching For Bag-of-words Data Via Kernel Embeddings Of Latent Distributions' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Cross-domain Matching For Bag-of-words Data Via Kernel Embeddings Of Latent Distributions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Cross-domain%20Matching%20For%20Bag-of-words%20Data%20Via%20Kernel%20Embeddings%20Of%20Latent%20Distributions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Yuya Yoshikawa, Tomoharu Iwata, Hiroshi Sawada, Takeshi Yamada</td>
	<td>Neural Information Processing Systems</td>
	<td><p>We propose a kernel-based method for finding matching between instances across different domains such as multilingual documents and images with annotations. Each instance is assumed to be represented as a multiset of features e.g. a bag-of-words representation for documents. The major difficulty in finding cross-domain relationships is that the similarity between instances in different domains cannot be directly measured. To overcome this difficulty the proposed method embeds all the features of different domains in a shared latent space and regards each instance as a distribution of its own features in the shared latent space. To represent the distributions efficiently and nonparametrically we employ the framework of the kernel embeddings of distributions. The embedding is estimated so as to minimize the difference between distributions of paired instances while keeping unpaired instances apart. In our experiments we show that the proposed method can achieve high performance on finding correspondence between multi-lingual Wikipedia articles between documents and tags and between images and tags.</p>
</td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/carreiraperpi%C3%B1%C3%A1n2015hashing/">Hashing With Binary Autoencoders</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Hashing With Binary Autoencoders' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Hashing With Binary Autoencoders' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Hashing%20With%20Binary%20Autoencoders' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Carreira-perpiÃ±Ã¡n Miguel Ã., Raziperchikolaei Ramin</td>
	<td>Arxiv</td>
	<td><p>An attractive approach for fast search in image databases is binary hashing where each high-dimensional real-valued image is mapped onto a low-dimensional binary vector and the search is done in this binary space. Finding the optimal hash function is difficult because it involves binary constraints and most approaches approximate the optimization by relaxing the constraints and then binarizing the result. Here we focus on the binary autoencoder model which seeks to reconstruct an image from the binary code produced by the hash function. We show that the optimization can be simplified with the method of auxiliary coordinates. This reformulates the optimization as alternating two easier steps one that learns the encoder and decoder separately and one that optimizes the code for each image. Image retrieval experiments using precision/recall and a measure of code utilization show the resulting hash function outperforms or is competitive with state-of-the-art methods for binary hashing.</p>
</td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/lin2015deephash/">Deephash Getting Regularization Depth And Fine-tuning Right</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deephash Getting Regularization Depth And Fine-tuning Right' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deephash Getting Regularization Depth And Fine-tuning Right' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deephash%20Getting%20Regularization%20Depth%20And%20Fine-tuning%20Right' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Lin Jie, Morere Olivier, Chandrasekhar Vijay, Veillard Antoine, Goh Hanlin</td>
	<td>Arxiv</td>
	<td><p>This work focuses on representing very high-dimensional global image descriptors using very compact 64-1024 bit binary hashes for instance retrieval. We propose DeepHash a hashing scheme based on deep networks. Key to making DeepHash work at extremely low bitrates are three important considerations â€“ regularization depth and fine-tuning â€“ each requiring solutions specific to the hashing problem. In-depth evaluation shows that our scheme consistently outperforms state-of-the-art methods across all data sets for both Fisher Vectors and Deep Convolutional Neural Network features by up to 20 percent over other schemes. The retrieval performance with 256-bit hashes is close to that of the uncompressed floating point features â€“ a remarkable 512 times compression.</p>
</td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/lin2015implicit/">Implicit Sparse Code Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Implicit Sparse Code Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Implicit Sparse Code Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Implicit%20Sparse%20Code%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Lin Tsung-yu, Ke Tsung-wei, Liu Tyng-luh</td>
	<td>Arxiv</td>
	<td><p>We address the problem of converting large-scale high-dimensional image data into binary codes so that approximate nearest-neighbor search over them can be efficiently performed. Different from most of the existing unsupervised approaches for yielding binary codes our method is based on a dimensionality-reduction criterion that its resulting mapping is designed to preserve the image relationships entailed by the inner products of sparse codes rather than those implied by the Euclidean distances in the ambient space. While the proposed formulation does not require computing any sparse codes the underlying computation model still inevitably involves solving an unmanageable eigenproblem when extremely high-dimensional descriptors are used. To overcome the difficulty we consider the column-sampling technique and presume a special form of rotation matrix to facilitate subproblem decomposition. We test our method on several challenging image datasets and demonstrate its effectiveness by comparing with state-of-the-art binary coding techniques.</p>
</td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/lin2015tiny/">Tiny Descriptors For Image Retrieval With Unsupervised Triplet Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Tiny Descriptors For Image Retrieval With Unsupervised Triplet Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Tiny Descriptors For Image Retrieval With Unsupervised Triplet Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Tiny%20Descriptors%20For%20Image%20Retrieval%20With%20Unsupervised%20Triplet%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Lin Jie, MorÃ¨re Olivier, Petta Julie, Chandrasekhar Vijay, Veillard Antoine</td>
	<td>Arxiv</td>
	<td><p>A typical image retrieval pipeline starts with the comparison of global descriptors from a large database to find a short list of candidate matches. A good image descriptor is key to the retrieval pipeline and should reconcile two contradictory requirements providing recall rates as high as possible and being as compact as possible for fast matching. Following the recent successes of Deep Convolutional Neural Networks (DCNN) for large scale image classification descriptors extracted from DCNNs are increasingly used in place of the traditional hand crafted descriptors such as Fisher Vectors (FV) with better retrieval performances. Nevertheless the dimensionality of a typical DCNN descriptor â€“extracted either from the visual feature pyramid or the fully-connected layersâ€“ remains quite high at several thousands of scalar values. In this paper we propose Unsupervised Triplet Hashing (UTH) a fully unsupervised method to compute extremely compact binary hashes â€“in the 32-256 bits rangeâ€“ from high-dimensional global descriptors. UTH consists of two successive deep learning steps. First Stacked Restricted Boltzmann Machines (SRBM) a type of unsupervised deep neural nets are used to learn binary embedding functions able to bring the descriptor size down to the desired bitrate. SRBMs are typically able to ensure a very high compression rate at the expense of loosing some desirable metric properties of the original DCNN descriptor space. Then triplet networks a rank learning scheme based on weight sharing nets is used to fine-tune the binary embedding functions to retain as much as possible of the useful metric properties of the original space. A thorough empirical evaluation conducted on multiple publicly available dataset using DCNN descriptors shows that our method is able to significantly outperform state-of-the-art unsupervised schemes in the target bit range.</p>
</td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/choromanski2015efficient/">Efficient Data Hashing With Structured Binary Embeddings</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Data Hashing With Structured Binary Embeddings' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Efficient Data Hashing With Structured Binary Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Efficient%20Data%20Hashing%20With%20Structured%20Binary%20Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Choromanski Krzysztof</td>
	<td>Arxiv</td>
	<td><p>We present here new mechanisms for hashing data via binary embeddings. Contrary to most of the techniques presented before the embedding matrix of our mechanism is highly structured. That enables us to perform hashing more efficiently and use less memory. What is crucial and nonintuitive is the fact that imposing structured mechanism does not affect the quality of the produced hash. To the best of our knowledge we are the first to give strong theoretical guarantees of the proposed binary hashing method by proving the efficiency of the mechanism for several classes of structured projection matrices. As a corollary we obtain binary hashing mechanisms with strong concentration results for circulant and Topelitz matrices. Our approach is however much more general.</p>
</td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/yang2015supervised/">Supervised Learning Of Semantics-preserving Hash Via Deep Convolutional Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Learning Of Semantics-preserving Hash Via Deep Convolutional Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Supervised Learning Of Semantics-preserving Hash Via Deep Convolutional Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Supervised%20Learning%20Of%20Semantics-preserving%20Hash%20Via%20Deep%20Convolutional%20Neural%20Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Yang Huei-fang, Lin Kevin, Chen Chu-song</td>
	<td>Arxiv</td>
	<td><p>This paper presents a simple yet effective supervised deep hash approach that constructs binary hash codes from labeled data for large-scale image search. We assume that the semantic labels are governed by several latent attributes with each attribute on or off and classification relies on these attributes. Based on this assumption our approach dubbed supervised semantics-preserving deep hashing (SSDH) constructs hash functions as a latent layer in a deep network and the binary codes are learned by minimizing an objective function defined over classification error and other desirable hash codes properties. With this design SSDH has a nice characteristic that classification and retrieval are unified in a single learning model. Moreover SSDH performs joint learning of image representations hash codes and classification in a point-wised manner and thus is scalable to large-scale datasets. SSDH is simple and can be realized by a slight enhancement of an existing deep architecture for classification; yet it is effective and outperforms other hashing approaches on several benchmarks and large datasets. Compared with state-of-the-art approaches SSDH achieves higher retrieval accuracy while the classification performance is not sacrificed.</p>
</td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/lai2015simultaneous/">Simultaneous Feature Learning And Hash Coding With Deep Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Simultaneous Feature Learning And Hash Coding With Deep Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Simultaneous Feature Learning And Hash Coding With Deep Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Simultaneous%20Feature%20Learning%20And%20Hash%20Coding%20With%20Deep%20Neural%20Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Lai Hanjiang, Pan Yan, Liu Ye, Yan Shuicheng</td>
	<td>Arxiv</td>
	<td><p>Similarity-preserving hashing is a widely-used method for nearest neighbour search in large-scale image retrieval tasks. For most existing hashing methods an image is first encoded as a vector of hand-engineering visual features followed by another separate projection or quantization step that generates binary codes. However such visual feature vectors may not be optimally compatible with the coding process thus producing sub-optimal hashing codes. In this paper we propose a deep architecture for supervised hashing in which images are mapped into binary codes via carefully designed deep neural networks. The pipeline of the proposed deep architecture consists of three building blocks 1) a sub-network with a stack of convolution layers to produce the effective intermediate image features; 2) a divide-and-encode module to divide the intermediate image features into multiple branches each encoded into one hash bit; and 3) a triplet ranking loss designed to characterize that one image is more similar to the second image than to the third one. Extensive evaluations on several benchmark image datasets show that the proposed simultaneous feature learning and hash coding pipeline brings substantial improvements over other state-of-the-art supervised or unsupervised hashing methods.</p>
</td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/zhao2015deep/">Deep Semantic Ranking Based Hashing For Multi-label Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Semantic Ranking Based Hashing For Multi-label Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Semantic Ranking Based Hashing For Multi-label Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Semantic%20Ranking%20Based%20Hashing%20For%20Multi-label%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhao Fang, Huang Yongzhen, Wang Liang, Tan Tieniu</td>
	<td>Arxiv</td>
	<td><p>With the rapid growth of web images hashing has received increasing interests in large scale image retrieval. Research efforts have been devoted to learning compact binary codes that preserve semantic similarity based on labels. However most of these hashing methods are designed to handle simple binary similarity. The complex multilevel semantic structure of images associated with multiple labels have not yet been well explored. Here we propose a deep semantic ranking based method for learning hash functions that preserve multilevel semantic similarity between multi-label images. In our approach deep convolutional neural network is incorporated into hash functions to jointly learn feature representations and mappings from them to hash codes which avoids the limitation of semantic representation power of hand-crafted features. Meanwhile a ranking list that encodes the multilevel similarity information is employed to guide the learning of such deep hash functions. An effective scheme based on surrogate loss is used to solve the intractable optimization problem of nonsmooth and multivariate ranking measures involved in the learning procedure. Experimental results show the superiority of our proposed approach over several state-of-the-art hashing methods in term of ranking evaluation metrics when tested on multi-label image datasets.</p>
</td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/zhong2015efficient/">Efficient Similarity Indexing And Searching In High Dimensions</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Similarity Indexing And Searching In High Dimensions' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Efficient Similarity Indexing And Searching In High Dimensions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Efficient%20Similarity%20Indexing%20And%20Searching%20In%20High%20Dimensions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhong Yu</td>
	<td>Arxiv</td>
	<td><p>Efficient indexing and searching of high dimensional data has been an area of active research due to the growing exploitation of high dimensional data and the vulnerability of traditional search methods to the curse of dimensionality. This paper presents a new approach for fast and effective searching and indexing of high dimensional features using random partitions of the feature space. Experiments on both handwritten digits and 3-D shape descriptors have shown the proposed algorithm to be highly effective and efficient in indexing and searching real data sets of several hundred dimensions. We also compare its performance to that of the state-of-the-art locality sensitive hashing algorithm.</p>
</td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/andoni2015practical/">Practical And Optimal LSH For Angular Distance</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Practical And Optimal LSH For Angular Distance' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Practical And Optimal LSH For Angular Distance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Practical%20And%20Optimal%20LSH%20For%20Angular%20Distance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, Ludwig Schmidt</td>
	<td>Neural Information Processing Systems</td>
	<td><p>We show the existence of a Locality-Sensitive Hashing (LSH) family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent. Unlike earlier algorithms with this property (e.g. Spherical LSH (Andoni-Indyk-Nguyen-Razenshteyn 2014) (Andoni-Razenshteyn 2015)) our algorithm is also practical improving upon the well-studied hyperplane LSH (Charikar 2002) in practice. We also introduce a multiprobe version of this algorithm and conduct an experimental evaluation on real and synthetic data sets.We complement the above positive results with a fine-grained lower bound for the quality of any LSH family for angular distance. Our lower bound implies that the above LSH family exhibits a trade-off between evaluation time and quality that is close to optimal for a natural class of LSH functions.</p>
</td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/pagh2015coveringlsh/">Coveringlsh Locality-sensitive Hashing Without False Negatives</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Coveringlsh Locality-sensitive Hashing Without False Negatives' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Coveringlsh Locality-sensitive Hashing Without False Negatives' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Coveringlsh%20Locality-sensitive%20Hashing%20Without%20False%20Negatives' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Pagh Rasmus</td>
	<td>Arxiv</td>
	<td><p>We consider a new construction of locality-sensitive hash functions for Hamming space that is emphcovering in the sense that is it guaranteed to produce a collision for every pair of vectors within a given radius (r). The construction is emphefficient in the sense that the expected number of hash collisions between vectors at distance~(cr) for a given (c1) comes close to that of the best possible data independent LSH without the covering guarantee namely the seminal LSH construction of Indyk and Motwani (STOC 98). The efficiency of the new construction essentially emphmatches their bound when the search radius is not too large â€” e.g. when (cr = o(log(n)/loglog n)) where (n) is the number of points in the data set and when (cr = log(n)/k) where (k) is an integer constant. In general it differs by at most a factor (ln(4)) in the exponent of the time bounds. As a consequence LSH-based similarity search in Hamming space can avoid the problem of false negatives at little or no cost in efficiency.</p>
</td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/andoni2015optimal/">Optimal Data-dependent Hashing For Approximate Near Neighbors</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Optimal Data-dependent Hashing For Approximate Near Neighbors' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Optimal Data-dependent Hashing For Approximate Near Neighbors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Optimal%20Data-dependent%20Hashing%20For%20Approximate%20Near%20Neighbors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Andoni Alexandr, Razenshteyn Ilya</td>
	<td>Arxiv</td>
	<td><p>We show an optimal data-dependent hashing scheme for the approximate near neighbor problem. For an (n)-point data set in a (d)-dimensional space our data structure achieves query time (O(d n^rho+o(1))) and space O(n^1+rho+o(1) + dn)( where )rho=tfrac12c^2-1 for the Euclidean space and approximation (c1). For the Hamming space we obtain an exponent of (rho=2c-1). Our result completes the direction set forth in AINR14 who gave a proof-of-concept that data-dependent hashing can outperform classical Locality Sensitive Hashing (LSH). In contrast to AINR14 the new bound is not only optimal but in fact improves over the best (optimal) LSH data structures IM98AI06 for all approximation factors (c1). From the technical perspective we proceed by decomposing an arbitrary dataset into several subsets that are in a certain sense pseudo-random.</p>
</td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/najibi2015large/">On Large-scale Retrieval Binary Or N-ary Coding</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=On Large-scale Retrieval Binary Or N-ary Coding' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=On Large-scale Retrieval Binary Or N-ary Coding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=On%20Large-scale%20Retrieval%20Binary%20Or%20N-ary%20Coding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Najibi Mahyar, Rastegari Mohammad, Davis Larry S.</td>
	<td>Arxiv</td>
	<td><p>The growing amount of data available in modern-day datasets makes the need to efficiently search and retrieve information. To make large-scale search feasible Distance Estimation and Subset Indexing are the main approaches. Although binary coding has been popular for implementing both techniques n-ary coding (known as Product Quantization) is also very effective for Distance Estimation. However their relative performance has not been studied for Subset Indexing. We investigate whether binary or n-ary coding works better under different retrieval strategies. This leads to the design of a new n-ary coding method Linear Subspace Quantization (LSQ) which unlike other n-ary encoders can be used as a similarity-preserving embedding. Experiments on image retrieval show that when Distance Estimation is used n-ary LSQ outperforms other methods. However when Subset Indexing is applied interestingly binary codings are more effective and binary LSQ achieves the best accuracy.</p>
</td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/ishikawa2015pairwise/">Pairwise Rotation Hashing For High-dimensional Features</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Pairwise Rotation Hashing For High-dimensional Features' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Pairwise Rotation Hashing For High-dimensional Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Pairwise%20Rotation%20Hashing%20For%20High-dimensional%20Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Ishikawa Kohta, Sato Ikuro, Ambai Mitsuru</td>
	<td>Arxiv</td>
	<td><p>Binary Hashing is widely used for effective approximate nearest neighbors search. Even though various binary hashing methods have been proposed very few methods are feasible for extremely high-dimensional features often used in visual tasks today. We propose a novel highly sparse linear hashing method based on pairwise rotations. The encoding cost of the proposed algorithm is ((n log n)) for n-dimensional features whereas that of the existing state-of-the-art method is typically ((n^2)). The proposed method is also remarkably faster in the learning phase. Along with the efficiency the retrieval accuracy is comparable to or slightly outperforming the state-of-the-art. Pairwise rotations used in our method are formulated from an analytical study of the trade-off relationship between quantization error and entropy of binary codes. Although these hashing criteria are widely used in previous researches its analytical behavior is rarely studied. All building blocks of our algorithm are based on the analytical solution and it thus provides a fairly simple and efficient procedure.</p>
</td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/guo2015cnn/">CNN Based Hashing For Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=CNN Based Hashing For Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=CNN Based Hashing For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=CNN%20Based%20Hashing%20For%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Guo Jinma, Li Jianmin</td>
	<td>Arxiv</td>
	<td><p>Along with data on the web increasing dramatically hashing is becoming more and more popular as a method of approximate nearest neighbor search. Previous supervised hashing methods utilized similarity/dissimilarity matrix to get semantic information. But the matrix is not easy to construct for a new dataset. Rather than to reconstruct the matrix we proposed a straightforward CNN-based hashing method i.e. binarilizing the activations of a fully connected layer with threshold 0 and taking the binary result as hash codes. This method achieved the best performance on CIFAR-10 and was comparable with the state-of-the-art on MNIST. And our experiments on CIFAR-10 suggested that the signs of activations may carry more information than the relative values of activations between samples and that the co-adaption between feature extractor and hash functions is important for hashing.</p>
</td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/hyv%C3%B6nen2015fast/">Fast K-nn Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Fast K-nn Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Fast K-nn Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Fast%20K-nn%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>HyvÃ¶nen Ville, PitkÃ¤nen Teemu, Tasoulis Sotiris, JÃ¤Ã¤saari Elias, Tuomainen Risto, Wang Liang, Corander Jukka, Roos Teemu</td>
	<td>IEEE International Conference on Big Data</td>
	<td><p>Efficient index structures for fast approximate nearest neighbor queries are required in many applications such as recommendation systems. In high-dimensional spaces many conventional methods suffer from excessive usage of memory and slow response times. We propose a method where multiple random projection trees are combined by a novel voting scheme. The key idea is to exploit the redundancy in a large number of candidate sets obtained by independently generated random projections in order to reduce the number of expensive exact distance evaluations. The method is straightforward to implement using sparse projections which leads to a reduced memory footprint and fast index construction. Furthermore it enables grouping of the required computations into big matrix multiplications which leads to additional savings due to cache effects and low-level parallelization. We demonstrate by extensive experiments on a wide variety of data sets that the method is faster than existing partitioning tree or hashing based approaches making it the fastest available technique on high accuracy levels.</p>
</td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/gu2015cross/">Cross-modality Hashing With Partial Correspondence</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Cross-modality Hashing With Partial Correspondence' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Cross-modality Hashing With Partial Correspondence' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Cross-modality%20Hashing%20With%20Partial%20Correspondence' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Gu Yun, Xue Haoyang, Yang Jie</td>
	<td>Arxiv</td>
	<td><p>Learning a hashing function for cross-media search is very desirable due to its low storage cost and fast query speed. However the data crawled from Internet cannot always guarantee good correspondence among different modalities which affects the learning for hashing function. In this paper we focus on cross-modal hashing with partially corresponded data. The data without full correspondence are made in use to enhance the hashing performance. The experiments on Wiki and NUS-WIDE datasets demonstrates that the proposed method outperforms some state-of-the-art hashing approaches with fewer correspondence information.</p>
</td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/li2015feature/">Feature Learning Based Deep Supervised Hashing With Pairwise Labels</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Feature Learning Based Deep Supervised Hashing With Pairwise Labels' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Feature Learning Based Deep Supervised Hashing With Pairwise Labels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Feature%20Learning%20Based%20Deep%20Supervised%20Hashing%20With%20Pairwise%20Labels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Li Wu-jun, Wang Sheng, Kang Wang-cheng</td>
	<td>Arxiv</td>
	<td><p>Recent years have witnessed wide application of hashing for large-scale image retrieval. However most existing hashing methods are based on hand-crafted features which might not be optimally compatible with the hashing procedure. Recently deep hashing methods have been proposed to perform simultaneous feature learning and hash-code learning with deep neural networks which have shown better performance than traditional hashing methods with hand-crafted features. Most of these deep hashing methods are supervised whose supervised information is given with triplet labels. For another common application scenario with pairwise labels there have not existed methods for simultaneous feature learning and hash-code learning. In this paper we propose a novel deep hashing method called deep pairwise-supervised hashing(DPSH) to perform simultaneous feature learning and hash-code learning for applications with pairwise labels. Experiments on real datasets show that our DPSH method can outperform other methods to achieve the state-of-the-art performance in image retrieval applications.</p>
</td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/li2015fast/">Fast K-nearest Neighbour Search Via Dynamic Continuous Indexing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Fast K-nearest Neighbour Search Via Dynamic Continuous Indexing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Fast K-nearest Neighbour Search Via Dynamic Continuous Indexing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Fast%20K-nearest%20Neighbour%20Search%20Via%20Dynamic%20Continuous%20Indexing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Li Ke, Malik Jitendra</td>
	<td>Arxiv</td>
	<td><p>Existing methods for retrieving k-nearest neighbours suffer from the curse of dimensionality. We argue this is caused in part by inherent deficiencies of space partitioning which is the underlying strategy used by most existing methods. We devise a new strategy that avoids partitioning the vector space and present a novel randomized algorithm that runs in time linear in dimensionality of the space and sub-linear in the intrinsic dimensionality and the size of the dataset and takes space constant in dimensionality of the space and linear in the size of the dataset. The proposed algorithm allows fine-grained control over accuracy and speed on a per-query basis automatically adapts to variations in data density supports dynamic updates to the dataset and is easy-to-implement. We show appealing theoretical properties and demonstrate empirically that the proposed algorithm outperforms locality-sensitivity hashing (LSH) in terms of approximation quality speed and space efficiency.</p>
</td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/wang2015learning/">Learning To Hash For Indexing Big Data - A Survey</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning To Hash For Indexing Big Data - A Survey' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning To Hash For Indexing Big Data - A Survey' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20To%20Hash%20For%20Indexing%20Big%20Data%20-%20A%20Survey' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Wang Jun, Liu Wei, Kumar Sanjiv, Chang Shih-fu</td>
	<td>Arxiv</td>
	<td><p>The explosive growth in big data has attracted much attention in designing efficient indexing and search methods recently. In many critical applications such as large-scale search and pattern matching finding the nearest neighbors to a query is a fundamental research problem. However the straightforward solution using exhaustive comparison is infeasible due to the prohibitive computational complexity and memory requirement. In response Approximate Nearest Neighbor (ANN) search based on hashing techniques has become popular due to its promising performance in both efficiency and accuracy. Prior randomized hashing methods e.g. Locality-Sensitive Hashing (LSH) explore data-independent hash functions with random projections or permutations. Although having elegant theoretic guarantees on the search quality in certain metric spaces performance of randomized hashing has been shown insufficient in many real-world applications. As a remedy new approaches incorporating data-driven learning methods in development of advanced hash functions have emerged. Such learning to hash methods exploit information such as data distributions or class labels when optimizing the hash codes or functions. Importantly the learned hash codes are able to preserve the proximity of neighboring data in the original feature spaces in the hash code spaces. The goal of this paper is to provide readers with systematic understanding of insights pros and cons of the emerging techniques. We provide a comprehensive survey of the learning to hash framework and representative techniques of various types including unsupervised semi-supervised and supervised. In addition we also summarize recent hashing approaches utilizing the deep learning models. Finally we discuss the future direction and trends of research in this area.</p>
</td>
</tr>



<tr>
	<td>2014</td>
	<td><a href="/publications/qiu2014random/">Random Forests Can Hash</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Random Forests Can Hash' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Random Forests Can Hash' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Random%20Forests%20Can%20Hash' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Qiu Qiang, Sapiro Guillermo, Bronstein Alex</td>
	<td>Arxiv</td>
	<td><p>Hash codes are a very efficient data representation needed to be able to cope with the ever growing amounts of data. We introduce a random forest semantic hashing scheme with information-theoretic code aggregation showing for the first time how random forest a technique that together with deep learning have shown spectacular results in classification can also be extended to large-scale retrieval. Traditional random forest fails to enforce the consistency of hashes generated from each tree for the same class data i.e. to preserve the underlying similarity and it also lacks a principled way for code aggregation across trees. We start with a simple hashing scheme where independently trained random trees in a forest are acting as hashing functions. We the propose a subspace model as the splitting function and show that it enforces the hash consistency in a tree for data from the same class. We also introduce an information-theoretic approach for aggregating codes of individual trees into a single hash code producing a near-optimal unique hash for each class. Experiments on large-scale public datasets are presented showing that the proposed approach significantly outperforms state-of-the-art hashing methods for retrieval tasks.</p>
</td>
</tr>

<tr>
	<td>2014</td>
	<td><a href="/publications/noma2014eclipse/">Eclipse Hashing Alexandrov Compactification And Hashing With Hyperspheres For Fast Similarity Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Eclipse Hashing Alexandrov Compactification And Hashing With Hyperspheres For Fast Similarity Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Eclipse Hashing Alexandrov Compactification And Hashing With Hyperspheres For Fast Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Eclipse%20Hashing%20Alexandrov%20Compactification%20And%20Hashing%20With%20Hyperspheres%20For%20Fast%20Similarity%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Noma Yui, Konoshima Makiko</td>
	<td>Arxiv</td>
	<td><p>The similarity searches that use high-dimensional feature vectors consisting of a vast amount of data have a wide range of application. One way of conducting a fast similarity search is to transform the feature vectors into binary vectors and perform the similarity search by using the Hamming distance. Such a transformation is a hashing method and the choice of hashing function is important. Hashing methods using hyperplanes or hyperspheres are proposed. One study reported here is inspired by Spherical LSH and we use hypersperes to hash the feature vectors. Our method called Eclipse-hashing performs a compactification of R^n by using the inverse stereographic projection which is a kind of Alexandrov compactification. By using Eclipse-hashing one can obtain the hypersphere-hash function without explicitly using hyperspheres. Hence the number of nonlinear operations is reduced and the processing time of hashing becomes shorter. Furthermore we also show that as a result of improving the approximation accuracy Eclipse-hashing is more accurate than hyperplane-hashing.</p>
</td>
</tr>

<tr>
	<td>2014</td>
	<td><a href="/publications/jiang2014revisiting/">Revisiting Kernelized Locality-sensitive Hashing For Improved Large-scale Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Revisiting Kernelized Locality-sensitive Hashing For Improved Large-scale Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Revisiting Kernelized Locality-sensitive Hashing For Improved Large-scale Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Revisiting%20Kernelized%20Locality-sensitive%20Hashing%20For%20Improved%20Large-scale%20Image%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Jiang Ke, Que Qichao, Kulis Brian</td>
	<td>Arxiv</td>
	<td><p>We present a simple but powerful reinterpretation of kernelized locality-sensitive hashing (KLSH) a general and popular method developed in the vision community for performing approximate nearest-neighbor searches in an arbitrary reproducing kernel Hilbert space (RKHS). Our new perspective is based on viewing the steps of the KLSH algorithm in an appropriately projected space and has several key theoretical and practical benefits. First it eliminates the problematic conceptual difficulties that are present in the existing motivation of KLSH. Second it yields the first formal retrieval performance bounds for KLSH. Third our analysis reveals two techniques for boosting the empirical performance of KLSH. We evaluate these extensions on several large-scale benchmark image retrieval data sets and show that our analysis leads to improved recall performance of at least 1237; and sometimes much higher over the standard KLSH method.</p>
</td>
</tr>

<tr>
	<td>2014</td>
	<td><a href="/publications/neyshabur2014symmetric/">On Symmetric And Asymmetric Lshs For Inner Product Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=On Symmetric And Asymmetric Lshs For Inner Product Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=On Symmetric And Asymmetric Lshs For Inner Product Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=On%20Symmetric%20And%20Asymmetric%20Lshs%20For%20Inner%20Product%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Neyshabur Behnam, Srebro Nathan</td>
	<td>Arxiv</td>
	<td><p>We consider the problem of designing locality sensitive hashes (LSH) for inner product similarity and of the power of asymmetric hashes in this context. Shrivastava and Li argue that there is no symmetric LSH for the problem and propose an asymmetric LSH based on different mappings for query and database points. However we show there does exist a simple symmetric LSH that enjoys stronger guarantees and better empirical performance than the asymmetric LSH they suggest. We also show a variant of the settings where asymmetry is in-fact needed but there a different asymmetric LSH is required.</p>
</td>
</tr>

<tr>
	<td>2014</td>
	<td><a href="/publications/gottlieb2014near/">Near-optimal Sample Compression For Nearest Neighbors</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Near-optimal Sample Compression For Nearest Neighbors' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Near-optimal Sample Compression For Nearest Neighbors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Near-optimal%20Sample%20Compression%20For%20Nearest%20Neighbors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Lee-ad Gottlieb, Aryeh Kontorovich, Pinhas Nisnevitch</td>
	<td>Neural Information Processing Systems</td>
	<td><p>We present the first sample compression algorithm for nearest neighbors with non-trivial performance guarantees. We complement these guarantees by demonstrating almost matching hardness lower bounds which show that our bound is nearly optimal. Our result yields new insight into margin-based nearest neighbor classification in metric spaces and allows us to significantly sharpen and simplify existing bounds. Some encouraging empirical results are also presented.</p>
</td>
</tr>

<tr>
	<td>2014</td>
	<td><a href="/publications/liu2014discrete/">Discrete Graph Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Discrete Graph Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Discrete Graph Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Discrete%20Graph%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Wei Liu, Cun Mu, Sanjiv Kumar, Shih-fu Chang</td>
	<td>Neural Information Processing Systems</td>
	<td><p>Hashing has emerged as a popular technique for fast nearest neighbor search in gigantic databases. In particular learning based hashing has received considerable attention due to its appealing storage and search efficiency. However the performance of most unsupervised learning based hashing methods deteriorates rapidly as the hash code length increases. We argue that the degraded performance is due to inferior optimization procedures used to achieve discrete binary codes. This paper presents a graph-based unsupervised hashing model to preserve the neighborhood structure of massive data in a discrete code space. We cast the graph hashing problem into a discrete optimization framework which directly learns the binary codes. A tractable alternating maximization algorithm is then proposed to explicitly deal with the discrete constraints yielding high-quality codes to well capture the local neighborhoods. Extensive experiments performed on four large datasets with up to one million samples show that our discrete optimization based graph hashing method obtains superior search accuracy over state-of-the-art unsupervised hashing methods especially for longer codes.</p>
</td>
</tr>

<tr>
	<td>2014</td>
	<td><a href="/publications/wang2014hashing/">Hashing For Similarity Search A Survey</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Hashing For Similarity Search A Survey' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Hashing For Similarity Search A Survey' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Hashing%20For%20Similarity%20Search%20A%20Survey' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Wang Jingdong, Shen Heng Tao, Song Jingkuan, Ji Jianqiu</td>
	<td>Arxiv</td>
	<td><p>Similarity search (nearest neighbor search) is a problem of pursuing the data items whose distances to a query item are the smallest from a large database. Various methods have been developed to address this problem and recently a lot of efforts have been devoted to approximate search. In this paper we present a survey on one of the main solutions hashing which has been widely studied since the pioneering work locality sensitive hashing. We divide the hashing algorithms two main categories locality sensitive hashing which designs hash functions without exploring the data distribution and learning to hash which learns hash functions according the data distribution and review them from various aspects including hash function design and distance measure and search scheme in the hash coding space.</p>
</td>
</tr>

<tr>
	<td>2014</td>
	<td><a href="/publications/shen2014hashing/">Hashing On Nonlinear Manifolds</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Hashing On Nonlinear Manifolds' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Hashing On Nonlinear Manifolds' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Hashing%20On%20Nonlinear%20Manifolds' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Shen Fumin, Shen Chunhua, Shi Qinfeng, Hengel Anton Van Den, Tang Zhenmin, Shen Heng Tao</td>
	<td>Arxiv</td>
	<td><p>Learning based hashing methods have attracted considerable attention due to their ability to greatly increase the scale at which existing algorithms may operate. Most of these methods are designed to generate binary codes preserving the Euclidean similarity in the original space. Manifold learning techniques in contrast are better able to model the intrinsic structure embedded in the original high-dimensional data. The complexities of these models and the problems with out-of-sample data have previously rendered them unsuitable for application to large-scale embedding however. In this work how to learn compact binary embeddings on their intrinsic manifolds is considered. In order to address the above-mentioned difficulties an efficient inductive solution to the out-of-sample data problem and a process by which non-parametric manifold learning may be used as the basis of a hashing method is proposed. The proposed approach thus allows the development of a range of new hashing techniques exploiting the flexibility of the wide variety of manifold learning approaches available. It is particularly shown that hashing on the basis of t-SNE outperforms state-of-the-art hashing methods on large-scale benchmark datasets and is very effective for image classification with very short code lengths. The proposed hashing framework is shown to be easily improved for example by minimizing the quantization error with learned orthogonal rotations. In addition a supervised inductive manifold hashing framework is developed by incorporating the label information which is shown to greatly advance the semantic retrieval performance.</p>
</td>
</tr>

<tr>
	<td>2014</td>
	<td><a href="/publications/shrivastava2014asymmetric/">Asymmetric LSH (ALSH) For Sublinear Time Maximum Inner Product Search (MIPS)</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Asymmetric LSH (ALSH) For Sublinear Time Maximum Inner Product Search (MIPS)' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Asymmetric LSH (ALSH) For Sublinear Time Maximum Inner Product Search (MIPS)' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Asymmetric%20LSH%20(ALSH)%20For%20Sublinear%20Time%20Maximum%20Inner%20Product%20Search%20(MIPS)' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Anshumali Shrivastava, Ping Li</td>
	<td>Neural Information Processing Systems</td>
	<td><p>We present the first provably sublinear time hashing algorithm for approximate emphMaximum Inner Product Search (MIPS). Searching with (un-normalized) inner product as the underlying similarity measure is a known difficult problem and finding hashing schemes for MIPS was considered hard. While the existing Locality Sensitive Hashing (LSH) framework is insufficient for solving MIPS in this paper we extend the LSH framework to allow asymmetric hashing schemes. Our proposal is based on a key observation that the problem of finding maximum inner products after independent asymmetric transformations can be converted into the problem of approximate near neighbor search in classical settings. This key observation makes efficient sublinear hashing scheme for MIPS possible. Under the extended asymmetric LSH (ALSH) framework this paper provides an example of explicit construction of provably fast hashing scheme for MIPS. Our proposed algorithm is simple and easy to implement. The proposed hashing scheme leads to significant computational savings over the two popular conventional LSH schemes (i) Sign Random Projection (SRP) and (ii) hashing based on (p)-stable distributions for (L_2) norm (L2LSH) in the collaborative filtering task of item recommendations on Netflix and Movielens (10M) datasets.</p>
</td>
</tr>

<tr>
	<td>2014</td>
	<td><a href="/publications/shrivastava2014improved/">Improved Densification Of One Permutation Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Improved Densification Of One Permutation Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Improved Densification Of One Permutation Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Improved%20Densification%20Of%20One%20Permutation%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Shrivastava Anshumali, Li Ping</td>
	<td>Arxiv</td>
	<td><p>The existing work on densification of one permutation hashing reduces the query processing cost of the ((KL))-parameterized Locality Sensitive Hashing (LSH) algorithm with minwise hashing from (O(dKL)) to merely (O(d + KL)) where (d) is the number of nonzeros of the data vector (K) is the number of hashes in each hash table and (L) is the number of hash tables. While that is a substantial improvement our analysis reveals that the existing densification scheme is sub-optimal. In particular there is no enough randomness in that procedure which affects its accuracy on very sparse datasets. In this paper we provide a new densification procedure which is provably better than the existing scheme. This improvement is more significant for very sparse datasets which are common over the web. The improved technique has the same cost of (O(d + KL)) for query processing thereby making it strictly preferable over the existing procedure. Experimental evaluations on public datasets in the task of hashing based near neighbor search support our theoretical findings.</p>
</td>
</tr>



<tr>
	<td>2013</td>
	<td><a href="/publications/boytsov2013learning/">Learning To Prune In Metric And Non-metric Spaces</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning To Prune In Metric And Non-metric Spaces' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning To Prune In Metric And Non-metric Spaces' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20To%20Prune%20In%20Metric%20And%20Non-metric%20Spaces' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Leonid Boytsov, Bilegsaikhan Naidan</td>
	<td>Neural Information Processing Systems</td>
	<td><p>Our focus is on approximate nearest neighbor retrieval in metric and non-metric spaces. We employ a VP-tree and explore two simple yet effective learning-to prune approaches density estimation through sampling and stretching of the triangle inequality. Both methods are evaluated using data sets with metric (Euclidean) and non-metric (KL-divergence and Itakura-Saito) distance functions. Conditions on spaces where the VP-tree is applicable are discussed. The VP-tree with a learned pruner is compared against the recently proposed state-of-the-art approaches the bbtree the multi-probe locality sensitive hashing (LSH) and permutation methods. Our method was competitive against state-of-the-art methods and in most cases was more efficient for the same rank approximation quality.</p>
</td>
</tr>

<tr>
	<td>2013</td>
	<td><a href="/publications/nguyen2013approximate/">Approximate Nearest Neighbor Search In (ell_p)</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Approximate Nearest Neighbor Search In (ell_p)' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Approximate Nearest Neighbor Search In (ell_p)' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Approximate%20Nearest%20Neighbor%20Search%20In%20(ell_p)' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Nguyen Huy L.</td>
	<td>Arxiv</td>
	<td><p>We present a new locality sensitive hashing (LSH) algorithm for (c)-approximate nearest neighbor search in (ell_p) with (1&lt;p&lt;2). For a database of (n) points in (ell_p) we achieve (O(dn^rho)) query time and (O(dn+n^1+rho)) space where (rho le O((ln c)^2/c^p)). This improves upon the previous best upper bound (rhole 1/c) by Datar et al. (SOCG 2004) and is close to the lower bound (rho ge 1/c^p) by ODonnell Wu and Zhou (ITCS 2011). The proof is a simple generalization of the LSH scheme for (ell_2) by Andoni and Indyk (FOCS 2006).</p>
</td>
</tr>

<tr>
	<td>2013</td>
	<td><a href="/publications/neyshabur2013power/">The Power Of Asymmetry In Binary Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=The Power Of Asymmetry In Binary Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=The Power Of Asymmetry In Binary Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=The%20Power%20Of%20Asymmetry%20In%20Binary%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Behnam Neyshabur, Nati Srebro, Russ R. Salakhutdinov, Yury Makarychev, Payman Yadollahpour</td>
	<td>Neural Information Processing Systems</td>
	<td><p>When approximating binary similarity using the hamming distance between short binary hashes we shown that even if the similarity is symmetric we can have shorter and more accurate hashes by using two distinct code maps. I.e.~by approximating the similarity between (x) and (x) as the hamming distance between (f(x)) and (g(x)) for two distinct binary codes (fg) rather than as the hamming distance between (f(x)) and (f(x)).</p>
</td>
</tr>

<tr>
	<td>2013</td>
	<td><a href="/publications/xie2013kernelized/">Kernelized Locality-sensitive Hashing For Semi-supervised Agglomerative Clustering</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Kernelized Locality-sensitive Hashing For Semi-supervised Agglomerative Clustering' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Kernelized Locality-sensitive Hashing For Semi-supervised Agglomerative Clustering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Kernelized%20Locality-sensitive%20Hashing%20For%20Semi-supervised%20Agglomerative%20Clustering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Xie Boyi, Zheng Shuheng</td>
	<td>Arxiv</td>
	<td><p>Large scale agglomerative clustering is hindered by computational burdens. We propose a novel scheme where exact inter-instance distance calculation is replaced by the Hamming distance between Kernelized Locality-Sensitive Hashing (KLSH) hashed values. This results in a method that drastically decreases computation time. Additionally we take advantage of certain labeled data points via distance metric learning to achieve a competitive precision and recall comparing to K-Means but in much less computation time.</p>
</td>
</tr>

<tr>
	<td>2013</td>
	<td><a href="/publications/ermon2013embed/">Embed And Project Discrete Sampling With Universal Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Embed And Project Discrete Sampling With Universal Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Embed And Project Discrete Sampling With Universal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Embed%20And%20Project%20Discrete%20Sampling%20With%20Universal%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Stefano Ermon, Carla P. Gomes, Ashish Sabharwal, Bart Selman</td>
	<td>Neural Information Processing Systems</td>
	<td><p>We consider the problem of sampling from a probability distribution defined over a high-dimensional discrete set specified for instance by a graphical model. We propose a sampling algorithm called PAWS based on embedding the set into a higher-dimensional space which is then randomly projected using universal hash functions to a lower-dimensional subspace and explored using combinatorial search methods. Our scheme can leverage fast combinatorial optimization tools as a blackbox and unlike MCMC methods samples produced are guaranteed to be within an (arbitrarily small) constant factor of the true probability distribution. We demonstrate that by using state-of-the-art combinatorial search tools PAWS can efficiently sample from Ising grids with strong interactions and from software verification instances while MCMC and variational methods fail in both cases.</p>
</td>
</tr>

<tr>
	<td>2013</td>
	<td><a href="/publications/shrivastava2013beyond/">Beyond Pairwise Provably Fast Algorithms For Approximate (k)-way Similarity Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Beyond Pairwise Provably Fast Algorithms For Approximate (k)-way Similarity Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Beyond Pairwise Provably Fast Algorithms For Approximate (k)-way Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Beyond%20Pairwise%20Provably%20Fast%20Algorithms%20For%20Approximate%20(k)-way%20Similarity%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Anshumali Shrivastava, Ping Li</td>
	<td>Neural Information Processing Systems</td>
	<td><p>We go beyond the notion of pairwise similarity and look into search problems with (k)-way similarity functions. In this paper we focus on problems related to emph3-way Jaccard similarity (^3way= S_1 cup S_2 cup S_3) (S_1 S_2 S_3 in ) where () is a size (n) collection of sets (or binary vectors). We show that approximate (^3way) similarity search problems admit fast algorithms with provable guarantees analogous to the pairwise case. Our analysis and speedup guarantees naturally extend to (k)-way resemblance. In the process we extend traditional framework of emphlocality sensitive hashing (LSH) to handle higher order similarities which could be of independent theoretical interest. The applicability of (^3way) search is shown on the Google sets application. In addition we demonstrate the advantage of (^3way) resemblance over the pairwise case in improving retrieval quality.</p>
</td>
</tr>

<tr>
	<td>2013</td>
	<td><a href="/publications/norouzi2013fast/">Fast Exact Search In Hamming Space With Multi-index Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Fast Exact Search In Hamming Space With Multi-index Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Fast Exact Search In Hamming Space With Multi-index Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Fast%20Exact%20Search%20In%20Hamming%20Space%20With%20Multi-index%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Norouzi Mohammad, Punjani Ali, Fleet David J.</td>
	<td>Arxiv</td>
	<td><p>There is growing interest in representing image data and feature descriptors using compact binary codes for fast near neighbor search. Although binary codes are motivated by their use as direct indices (addresses) into a hash table codes longer than 32 bits are not being used as such as it was thought to be ineffective. We introduce a rigorous way to build multiple hash tables on binary code substrings that enables exact k-nearest neighbor search in Hamming space. The approach is storage efficient and straightforward to implement. Theoretical analysis shows that the algorithm exhibits sub-linear run-time behavior for uniformly distributed codes. Empirical results show dramatic speedups over a linear scan baseline for datasets of up to one billion codes of 64 128 or 256 bits.</p>
</td>
</tr>

<tr>
	<td>2013</td>
	<td><a href="/publications/masci2013sparse/">Sparse Similarity-preserving Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Sparse Similarity-preserving Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Sparse Similarity-preserving Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Sparse%20Similarity-preserving%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Masci Jonathan, Bronstein Alex M., Bronstein Michael M., Sprechmann Pablo, Sapiro Guillermo</td>
	<td>Arxiv</td>
	<td><p>In recent years a lot of attention has been devoted to efficient nearest neighbor search by means of similarity-preserving hashing. One of the plights of existing hashing techniques is the intrinsic trade-off between performance and computational complexity while longer hash codes allow for lower false positive rates it is very difficult to increase the embedding dimensionality without incurring in very high false negatives rates or prohibiting computational costs. In this paper we propose a way to overcome this limitation by enforcing the hash codes to be sparse. Sparse high-dimensional codes enjoy from the low false positive rates typical of long hashes while keeping the false negative rates similar to those of a shorter dense hashing scheme with equal number of degrees of freedom. We use a tailored feed-forward neural network for the hashing function. Extensive experimental evaluation involving visual and multi-modal data shows the benefits of the proposed method.</p>
</td>
</tr>

<tr>
	<td>2013</td>
	<td><a href="/publications/andoni2013beyond/">Beyond Locality-sensitive Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Beyond Locality-sensitive Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Beyond Locality-sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Beyond%20Locality-sensitive%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Andoni Alexandr, Indyk Piotr, Nguyen Huy L., Razenshteyn Ilya</td>
	<td>Arxiv</td>
	<td><p>We present a new data structure for the c-approximate near neighbor problem (ANN) in the Euclidean space. For n points in R^d our algorithm achieves O(n^rho + d log n) query time and O(n^1 + rho + d log n) space where rho &lt;= 7/(8c^2) + O(1 / c^3) + o(1). This is the first improvement over the result by Andoni and Indyk (FOCS 2006) and the first data structure that bypasses a locality-sensitive hashing lower bound proved by ODonnell Wu and Zhou (ICS 2011). By a standard reduction we obtain a data structure for the Hamming space and ell_1 norm with rho &lt;= 7/(8c) + O(1/c^3/2) + o(1) which is the first improvement over the result of Indyk and Motwani (STOC 1998).</p>
</td>
</tr>

<tr>
	<td>2013</td>
	<td><a href="/publications/teixeira2013scalable/">Scalable Locality-sensitive Hashing For Similarity Search In High-dimensional Large-scale Multimedia Datasets</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Scalable Locality-sensitive Hashing For Similarity Search In High-dimensional Large-scale Multimedia Datasets' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Scalable Locality-sensitive Hashing For Similarity Search In High-dimensional Large-scale Multimedia Datasets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Scalable%20Locality-sensitive%20Hashing%20For%20Similarity%20Search%20In%20High-dimensional%20Large-scale%20Multimedia%20Datasets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Teixeira Thiago S. F. X., Teodoro George, Valle Eduardo, Saltz Joel H.</td>
	<td>Arxiv</td>
	<td><p>Similarity search is critical for many database applications including the increasingly popular online services for Content-Based Multimedia Retrieval (CBMR). These services which include image search engines must handle an overwhelming volume of data while keeping low response times. Thus scalability is imperative for similarity search in Web-scale applications but most existing methods are sequential and target shared-memory machines. Here we address these issues with a distributed efficient and scalable index based on Locality-Sensitive Hashing (LSH). LSH is one of the most efficient and popular techniques for similarity search but its poor referential locality properties has made its implementation a challenging problem. Our solution is based on a widely asynchronous dataflow parallelization with a number of optimizations that include a hierarchical parallelization to decouple indexing and data storage locality-aware data partition strategies to reduce message passing and multi-probing to limit memory usage. The proposed parallelization attained an efficiency of 9037; in a distributed system with about 800 CPU cores. In particular the original locality-aware data partition reduced the number of messages exchanged in 3037;. Our parallel LSH was evaluated using the largest public dataset for similarity search (to the best of our knowledge) with (10^9) 128-d SIFT descriptors extracted from Web images. This is two orders of magnitude larger than datasets that previous LSH parallelizations could handle.</p>
</td>
</tr>



<tr>
	<td>2012</td>
	<td><a href="/publications/norouzi2012hamming/">Hamming Distance Metric Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Hamming Distance Metric Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Hamming Distance Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Hamming%20Distance%20Metric%20Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Mohammad Norouzi, David J. Fleet, Russ R. Salakhutdinov</td>
	<td>Neural Information Processing Systems</td>
	<td><p>Motivated by large-scale multimedia applications we propose to learn mappings from high-dimensional data to binary codes that preserve semantic similarity. Binary codes are well suited to large-scale applications as they are storage efficient and permit exact sub-linear kNN search. The framework is applicable to broad families of mappings and uses a flexible form of triplet ranking loss. We overcome discontinuous optimization of the discrete mappings by minimizing a piecewise-smooth upper bound on empirical loss inspired by latent structural SVMs. We develop a new loss-augmented inference algorithm that is quadratic in the code length. We show strong retrieval performance on CIFAR-10 and MNIST with promising classification results using no more than kNN on the binary codes.</p>
</td>
</tr>

<tr>
	<td>2012</td>
	<td><a href="/publications/ji2012super/">Super-bit Locality-sensitive Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Super-bit Locality-sensitive Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Super-bit Locality-sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Super-bit%20Locality-sensitive%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Jianqiu Ji, Jianmin Li, Shuicheng Yan, Bo Zhang, Qi Tian</td>
	<td>Neural Information Processing Systems</td>
	<td><p>Sign-random-projection locality-sensitive hashing (SRP-LSH) is a probabilistic dimension reduction method which provides an unbiased estimate of angular similarity yet suffers from the large variance of its estimation. In this work we propose the Super-Bit locality-sensitive hashing (SBLSH). It is easy to implement which orthogonalizes the random projection vectors in batches and it is theoretically guaranteed that SBLSH also provides an unbiased estimate of angular similarity yet with a smaller variance when the angle to estimate is within ((0pi/2). The extensive experiments on real data well validate that given the same length of binary code SBLSH may achieve significant mean squared error reduction in estimating pairwise angular similarity. Moreover SBLSH shows the superiority over SRP-LSH in approximate nearest neighbor (ANN) retrieval experiments.</p>
</td>
</tr>

<tr>
	<td>2012</td>
	<td><a href="/publications/li2012one/">One Permutation Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=One Permutation Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=One Permutation Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=One%20Permutation%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Ping Li, Art Owen, Cun-hui Zhang</td>
	<td>Neural Information Processing Systems</td>
	<td><p>While minwise hashing is promising for large-scale learning in massive binary data the preprocessing cost is prohibitive as it requires applying (e.g.) (k=500) permutations on the data. The testing time is also expensive if a new data point (e.g. a new document or a new image) has not been processed. In this paper we develop a simple textbfone permutation hashing scheme to address this important issue. While it is true that the preprocessing step can be parallelized it comes at the cost of additional hardware and implementation. Also reducing (k) permutations to just one would be much more textbfenergy-efficient which might be an important perspective as minwise hashing is commonly deployed in the search industry. While the theoretical probability analysis is interesting our experiments on similarity estimation and SVM amp; logistic regression also confirm the theoretical results.</p>
</td>
</tr>

<tr>
	<td>2012</td>
	<td><a href="/publications/lin2012density/">Density Sensitive Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Density Sensitive Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Density Sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Density%20Sensitive%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Lin Yue, Cai Deng, Li Cheng</td>
	<td>Arxiv</td>
	<td><p>Nearest neighbors search is a fundamental problem in various research fields like machine learning data mining and pattern recognition. Recently hashing-based approaches e.g. Locality Sensitive Hashing (LSH) are proved to be effective for scalable high dimensional nearest neighbors search. Many hashing algorithms found their theoretic root in random projection. Since these algorithms generate the hash tables (projections) randomly a large number of hash tables (i.e. long codewords) are required in order to achieve both high precision and recall. To address this limitation we propose a novel hashing algorithm called em Density Sensitive Hashing (DSH) in this paper. DSH can be regarded as an extension of LSH. By exploring the geometric structure of the data DSH avoids the purely random projections selection and uses those projective functions which best agree with the distribution of the data. Extensive experimental results on real-world data sets have shown that the proposed method achieves better performance compared to the state-of-the-art hashing approaches.</p>
</td>
</tr>

<tr>
	<td>2012</td>
	<td><a href="/publications/masci2012multimodal/">Multimodal Similarity-preserving Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Multimodal Similarity-preserving Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Multimodal Similarity-preserving Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Multimodal%20Similarity-preserving%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Masci Jonathan, Bronstein Michael M., Bronstein Alexander A., Schmidhuber JÃ¼rgen</td>
	<td>Arxiv</td>
	<td><p>We introduce an efficient computational framework for hashing data belonging to multiple modalities into a single representation space where they become mutually comparable. The proposed approach is based on a novel coupled siamese neural network architecture and allows unified treatment of intra- and inter-modality similarity learning. Unlike existing cross-modality similarity learning approaches our hashing functions are not limited to binarized linear projections and can assume arbitrarily complex forms. We show experimentally that our method significantly outperforms state-of-the-art hashing approaches on multimedia retrieval tasks.</p>
</td>
</tr>

<tr>
	<td>2012</td>
	<td><a href="/publications/gong2012angular/">Angular Quantization-based Binary Codes For Fast Similarity Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Angular Quantization-based Binary Codes For Fast Similarity Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Angular Quantization-based Binary Codes For Fast Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Angular%20Quantization-based%20Binary%20Codes%20For%20Fast%20Similarity%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Yunchao Gong, Sanjiv Kumar, Vishal Verma, Svetlana Lazebnik</td>
	<td>Neural Information Processing Systems</td>
	<td><p>This paper focuses on the problem of learning binary embeddings for efficient retrieval of high-dimensional non-negative data. Such data typically arises in a large number of vision and text applications where counts or frequencies are used as features. Also cosine distance is commonly used as a measure of dissimilarity between such vectors. In this work we introduce a novel spherical quantization scheme to generate binary embedding of such data and analyze its properties. The number of quantization landmarks in this scheme grows exponentially with data dimensionality resulting in low-distortion quantization. We propose a very efficient method for computing the binary embedding using such large number of landmarks. Further a linear transformation is learned to minimize the quantization error by adapting the method to the input data resulting in improved embedding. Experiments on image and text retrieval applications show superior performance of the proposed method over other existing state-of-the-art methods.</p>
</td>
</tr>

<tr>
	<td>2012</td>
	<td><a href="/publications/ziegler2012locally/">Locally Uniform Comparison Image Descriptor</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Locally Uniform Comparison Image Descriptor' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Locally Uniform Comparison Image Descriptor' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Locally%20Uniform%20Comparison%20Image%20Descriptor' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Andrew Ziegler, Eric Christiansen, David Kriegman, Serge Belongie</td>
	<td>Neural Information Processing Systems</td>
	<td><p>Keypoint matching between pairs of images using popular descriptors like SIFT or a faster variant called SURF is at the heart of many computer vision algorithms including recognition mosaicing and structure from motion. For real-time mobile applications very fast but less accurate descriptors like BRIEF and related methods use a random sampling of pairwise comparisons of pixel intensities in an image patch. Here we introduce Locally Uniform Comparison Image Descriptor (LUCID) a simple description method based on permutation distances between the ordering of intensities of RGB values between two patches. LUCID is computable in linear time with respect to patch size and does not require floating point computation. An analysis reveals an underlying issue that limits the potential of BRIEF and related approaches compared to LUCID. Experiments demonstrate that LUCID is faster than BRIEF and its accuracy is directly comparable to SURF while being more than an order of magnitude faster.</p>
</td>
</tr>

<tr>
	<td>2012</td>
	<td><a href="/publications/zhen2012co/">Co-regularized Hashing For Multimodal Data</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Co-regularized Hashing For Multimodal Data' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Co-regularized Hashing For Multimodal Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Co-regularized%20Hashing%20For%20Multimodal%20Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Yi Zhen, Dit-yan Yeung</td>
	<td>Neural Information Processing Systems</td>
	<td><p>Hashing-based methods provide a very promising approach to large-scale similarity search. To obtain compact hash codes a recent trend seeks to learn the hash functions from data automatically. In this paper we study hash function learning in the context of multimodal data. We propose a novel multimodal hash function learning method called Co-Regularized Hashing (CRH) based on a boosted co-regularization framework. The hash functions for each bit of the hash codes are learned by solving DC (difference of convex functions) programs while the learning for multiple bits proceeds via a boosting procedure so that the bias introduced by the hash functions can be sequentially minimized. We empirically compare CRH with two state-of-the-art multimodal hash function learning methods on two publicly available data sets.</p>
</td>
</tr>

<tr>
	<td>2012</td>
	<td><a href="/publications/kong2012isotropic/">Isotropic Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Isotropic Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Isotropic Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Isotropic%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Weihao Kong, Wu-jun Li</td>
	<td>Neural Information Processing Systems</td>
	<td><p>Most existing hashing methods adopt some projection functions to project the original data into several dimensions of real values and then each of these projected dimensions is quantized into one bit (zero or one) by thresholding. Typically the variances of different projected dimensions are different for existing projection functions such as principal component analysis (PCA). Using the same number of bits for different projected dimensions is unreasonable because larger-variance dimensions will carry more information. Although this viewpoint has been widely accepted by many researchers it is still not verified by either theory or experiment because no methods have been proposed to find a projection with equal variances for different dimensions. In this paper we propose a novel method called isotropic hashing (IsoHash) to learn projection functions which can produce projected dimensions with isotropic variances (equal variances). Experimental results on real data sets show that IsoHash can outperform its counterpart with different variances for different dimensions which verifies the viewpoint that projections with isotropic variances will be better than those with anisotropic variances.</p>
</td>
</tr>



<tr>
	<td>2011</td>
	<td><a href="/publications/li2011learning/">Learning To Search Efficiently In High Dimensions</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning To Search Efficiently In High Dimensions' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning To Search Efficiently In High Dimensions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20To%20Search%20Efficiently%20In%20High%20Dimensions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhen Li, Huazhong Ning, Liangliang Cao, Tong Zhang, Yihong Gong, Thomas S. Huang</td>
	<td>Neural Information Processing Systems</td>
	<td><p>High dimensional similarity search in large scale databases becomes an important challenge due to the advent of Internet. For such applications specialized data structures are required to achieve computational efficiency. Traditional approaches relied on algorithmic constructions that are often data independent (such as Locality Sensitive Hashing) or weakly dependent (such as kd-trees k-means trees). While supervised learning algorithms have been applied to related problems those proposed in the literature mainly focused on learning hash codes optimized for compact embedding of the data rather than search efficiency. Consequently such an embedding has to be used with linear scan or another search algorithm. Hence learning to hash does not directly address the search efficiency issue. This paper considers a new framework that applies supervised learning to directly optimize a data structure that supports efficient large scale search. Our approach takes both search quality and computational cost into consideration. Specifically we learn a boosted search forest that is optimized using pair-wise similarity labeled examples. The output of this search forest can be efficiently converted into an inverted indexing data structure which can leverage modern text search infrastructure to achieve both scalability and efficiency. Experimental results show that our approach significantly outperforms the start-of-the-art learning to hash methods (such as spectral hashing) as well as state-of-the-art high dimensional search algorithms (such as LSH and k-means trees).</p>
</td>
</tr>

<tr>
	<td>2011</td>
	<td><a href="/publications/satuluri2011bayesian/">Bayesian Locality Sensitive Hashing For Fast Similarity Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Bayesian Locality Sensitive Hashing For Fast Similarity Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Bayesian Locality Sensitive Hashing For Fast Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Bayesian%20Locality%20Sensitive%20Hashing%20For%20Fast%20Similarity%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Satuluri Venu, Parthasarathy Srinivasan</td>
	<td>PVLDB</td>
	<td><p>Given a collection of objects and an associated similarity measure the all-pairs similarity search problem asks us to find all pairs of objects with similarity greater than a certain user-specified threshold. Locality-sensitive hashing (LSH) based methods have become a very popular approach for this problem. However most such methods only use LSH for the first phase of similarity search - i.e. efficient indexing for candidate generation. In this paper we present BayesLSH a principled Bayesian algorithm for the subsequent phase of similarity search - performing candidate pruning and similarity estimation using LSH. A simpler variant BayesLSH-Lite which calculates similarities exactly is also presented. BayesLSH is able to quickly prune away a large majority of the false positive candidate pairs leading to significant speedups over baseline approaches. For BayesLSH we also provide probabilistic guarantees on the quality of the output both in terms of accuracy and recall. Finally the quality of BayesLSHs output can be easily tuned and does not require any manual setting of the number of hashes to use for similarity estimation unlike standard approaches. For two state-of-the-art candidate generation algorithms AllPairs and LSH BayesLSH enables significant speedups typically in the range 2x-20x for a wide variety of datasets.</p>
</td>
</tr>

<tr>
	<td>2011</td>
	<td><a href="/publications/li2011hashing/">Hashing Algorithms For Large-scale Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Hashing Algorithms For Large-scale Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Hashing Algorithms For Large-scale Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Hashing%20Algorithms%20For%20Large-scale%20Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Ping Li, Anshumali Shrivastava, Joshua Moore, Arnd KÃ¶nig</td>
	<td>Neural Information Processing Systems</td>
	<td><p>Minwise hashing is a standard technique in the context of search for efficiently computing set similarities. The recent development of b-bit minwise hashing provides a substantial improvement by storing only the lowest b bits of each hashed value. In this paper we demonstrate that b-bit minwise hashing can be naturally integrated with linear learning algorithms such as linear SVM and logistic regression to solve large-scale and high-dimensional statistical learning tasks especially when the data do not fit in memory. We compare (b)-bit minwise hashing with the Count-Min (CM) and Vowpal Wabbit (VW) algorithms which have essentially the same variances as random projections. Our theoretical and empirical comparisons illustrate that b-bit minwise hashing is significantly more accurate (at the same storage cost) than VW (and random projections) for binary data.</p>
</td>
</tr>

<tr>
	<td>2011</td>
	<td><a href="/publications/j%C3%A9gou2011anti/">Anti-sparse Coding For Approximate Nearest Neighbor Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Anti-sparse Coding For Approximate Nearest Neighbor Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Anti-sparse Coding For Approximate Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Anti-sparse%20Coding%20For%20Approximate%20Nearest%20Neighbor%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>JÃ©gou HervÃ© Inria - Irisa, Furon Teddy Inria - Irisa, Fuchs Jean-jacques Inria - Irisa</td>
	<td>Arxiv</td>
	<td><p>This paper proposes a binarization scheme for vectors of high dimension based on the recent concept of anti-sparse coding and shows its excellent performance for approximate nearest neighbor search. Unlike other binarization schemes this framework allows up to a scaling factor the explicit reconstruction from the binary representation of the original vector. The paper also shows that random projections which are used in Locality Sensitive Hashing algorithms are significantly outperformed by regular frames for both synthetic and real data if the number of bits exceeds the vector dimensionality i.e. when high precision is required.</p>
</td>
</tr>

<tr>
	<td>2011</td>
	<td><a href="/publications/bronstein2011kernel/">Kernel Diff-hash</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Kernel Diff-hash' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Kernel Diff-hash' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Kernel%20Diff-hash' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Bronstein Michael M</td>
	<td>Arxiv</td>
	<td><p>This paper presents a kernel formulation of the recently introduced diff-hash algorithm for the construction of similarity-sensitive hash functions. Our kernel diff-hash algorithm that shows superior performance on the problem of image feature descriptor matching.</p>
</td>
</tr>

<tr>
	<td>2011</td>
	<td><a href="/publications/li2011accurate/">Accurate Estimators For Improving Minwise Hashing And B-bit Minwise Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Accurate Estimators For Improving Minwise Hashing And B-bit Minwise Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Accurate Estimators For Improving Minwise Hashing And B-bit Minwise Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Accurate%20Estimators%20For%20Improving%20Minwise%20Hashing%20And%20B-bit%20Minwise%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Li Ping, Konig Christian</td>
	<td>Arxiv</td>
	<td><p>Minwise hashing is the standard technique in the context of search and databases for efficiently estimating set (e.g. high-dimensional 0/1 vector) similarities. Recently b-bit minwise hashing was proposed which significantly improves upon the original minwise hashing in practice by storing only the lowest b bits of each hashed value as opposed to using 64 bits. b-bit hashing is particularly effective in applications which mainly concern sets of high similarities (e.g. the resemblance 0.5). However there are other important applications in which not just pairs of high similarities matter. For example many learning algorithms require all pairwise similarities and it is expected that only a small fraction of the pairs are similar. Furthermore many applications care more about containment (e.g. how much one object is contained by another object) than the resemblance. In this paper we show that the estimators for minwise hashing and b-bit minwise hashing used in the current practice can be systematically improved and the improvements are most significant for set pairs of low resemblance and high containment.</p>
</td>
</tr>

<tr>
	<td>2011</td>
	<td><a href="/publications/bronstein2011multimodal/">Multimodal Diff-hash</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Multimodal Diff-hash' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Multimodal Diff-hash' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Multimodal%20Diff-hash' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Bronstein Michael M.</td>
	<td>Arxiv</td>
	<td><p>Many applications require comparing multimodal data with different structure and dimensionality that cannot be compared directly. Recently there has been increasing interest in methods for learning and efficiently representing such multimodal similarity. In this paper we present a simple algorithm for multimodal similarity-preserving hashing trying to map multimodal data into the Hamming space while preserving the intra- and inter-modal similarities. We show that our method significantly outperforms the state-of-the-art method in the field.</p>
</td>
</tr>



<tr>
	<td>2010</td>
	<td><a href="/publications/zhang2010self/">Self-taught Hashing For Fast Similarity Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Self-taught Hashing For Fast Similarity Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Self-taught Hashing For Fast Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Self-taught%20Hashing%20For%20Fast%20Similarity%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Zhang Dell, Wang Jun, Cai Deng, Lu Jinsong</td>
	<td>Arxiv</td>
	<td><p>The ability of fast similarity search at large scale is of great importance to many Information Retrieval (IR) applications. A promising way to accelerate similarity search is semantic hashing which designs compact binary codes for a large number of documents so that semantically similar documents are mapped to similar codes (within a short Hamming distance). Although some recently proposed techniques are able to generate high-quality codes for documents known in advance obtaining the codes for previously unseen documents remains to be a very challenging problem. In this paper we emphasise this issue and propose a novel Self-Taught Hashing (STH) approach to semantic hashing we first find the optimal (l)-bit binary codes for all documents in the given corpus via unsupervised learning and then train (l) classifiers via supervised learning to predict the (l)-bit code for any query document unseen before. Our experiments on three real-world text datasets show that the proposed approach using binarised Laplacian Eigenmap (LapEig) and linear Support Vector Machine (SVM) outperforms state-of-the-art techniques significantly.</p>
</td>
</tr>

<tr>
	<td>2010</td>
	<td><a href="/publications/li2010b/">B-bit Minwise Hashing For Estimating Three-way Similarities</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=B-bit Minwise Hashing For Estimating Three-way Similarities' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=B-bit Minwise Hashing For Estimating Three-way Similarities' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=B-bit%20Minwise%20Hashing%20For%20Estimating%20Three-way%20Similarities' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Ping Li, Arnd Konig, Wenhao Gui</td>
	<td>Neural Information Processing Systems</td>
	<td><p>Computing two-way and multi-way set similarities is a fundamental problem. This study focuses on estimating 3-way resemblance (Jaccard similarity) using b-bit minwise hashing. While traditional minwise hashing methods store each hashed value using 64 bits b-bit minwise hashing only stores the lowest b bits (where b= 2 for 3-way). The extension to 3-way similarity from the prior work on 2-way similarity is technically non-trivial. We develop the precise estimator which is accurate and very complicated; and we recommend a much simplified estimator suitable for sparse data. Our analysis shows that (b)-bit minwise hashing can normally achieve a 10 to 25-fold improvement in the storage space required for a given estimator accuracy of the 3-way resemblance.</p>
</td>
</tr>

<tr>
	<td>2010</td>
	<td><a href="/publications/jain2010hashing/">Hashing Hyperplane Queries To Near Points With Applications To Large-scale Active Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Hashing Hyperplane Queries To Near Points With Applications To Large-scale Active Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Hashing Hyperplane Queries To Near Points With Applications To Large-scale Active Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Hashing%20Hyperplane%20Queries%20To%20Near%20Points%20With%20Applications%20To%20Large-scale%20Active%20Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Prateek Jain, Sudheendra Vijayanarasimhan, Kristen Grauman</td>
	<td>Neural Information Processing Systems</td>
	<td><p>We consider the problem of retrieving the database points nearest to a given em hyperplane query without exhaustively scanning the database. We propose two hashing-based solutions. Our first approach maps the data to two-bit binary keys that are locality-sensitive for the angle between the hyperplane normal and a database point. Our second approach embeds the data into a vector space where the Euclidean norm reflects the desired distance between the original points and hyperplane query. Both use hashing to retrieve near points in sub-linear time. Our first methods preprocessing stage is more efficient while the second has stronger accuracy guarantees. We apply both to pool-based active learning taking the current hyperplane classifier as a query our algorithm identifies those points (approximately) satisfying the well-known minimal distance-to-hyperplane selection criterion. We empirically demonstrate our methods tradeoffs and show that they make it practical to perform active selection with millions of unlabeled points.</p>
</td>
</tr>



<tr>
	<td>2009</td>
	<td><a href="/publications/li2009b/">B-bit Minwise Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=B-bit Minwise Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=B-bit Minwise Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=B-bit%20Minwise%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Li Ping, Konig Arnd Christian</td>
	<td>Arxiv</td>
	<td><p>This paper establishes the theoretical framework of b-bit minwise hashing. The original minwise hashing method has become a standard technique for estimating set similarity (e.g. resemblance) with applications in information retrieval data management social networks and computational advertising. By only storing the lowest (b) bits of each (minwise) hashed value (e.g. b=1 or 2) one can gain substantial advantages in terms of computational efficiency and storage space. We prove the basic theoretical results and provide an unbiased estimator of the resemblance for any b. We demonstrate that even in the least favorable scenario using b=1 may reduce the storage space at least by a factor of 21.3 (or 10.7) compared to using b=64 (or b=32) if one is interested in resemblance 0.5.</p>
</td>
</tr>

<tr>
	<td>2009</td>
	<td><a href="/publications/bengio2009group/">Group Sparse Coding</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Group Sparse Coding' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Group Sparse Coding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Group%20Sparse%20Coding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Samy Bengio, Fernando Pereira, Yoram Singer, Dennis Strelow</td>
	<td>Neural Information Processing Systems</td>
	<td><p>Bag-of-words document representations are often used in text image and video processing. While it is relatively easy to determine a suitable word dictionary for text documents there is no simple mapping from raw images or videos to dictionary terms. The classical approach builds a dictionary using vector quantization over a large set of useful visual descriptors extracted from a training set and uses a nearest-neighbor algorithm to count the number of occurrences of each dictionary word in documents to be encoded. More robust approaches have been proposed recently that represent each visual descriptor as a sparse weighted combination of dictionary words. While favoring a sparse representation at the level of visual descriptors those methods however do not ensure that images have sparse representation. In this work we use mixed-norm regularization to achieve sparsity at the image level as well as a small overall dictionary. This approach can also be used to encourage using the same dictionary words for all the images in a class providing a discriminative signal in the construction of image representations. Experimental results on a benchmark image classification dataset show that when compact image or dictionary representations are needed for computational efficiency the proposed approach yields better mean average precision in classification.</p>
</td>
</tr>

<tr>
	<td>2009</td>
	<td><a href="/publications/kulis2009learning/">Learning To Hash With Binary Reconstructive Embeddings</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning To Hash With Binary Reconstructive Embeddings' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning To Hash With Binary Reconstructive Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20To%20Hash%20With%20Binary%20Reconstructive%20Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Brian Kulis, Trevor Darrell</td>
	<td>Neural Information Processing Systems</td>
	<td><p>Fast retrieval methods are increasingly critical for many large-scale analysis tasks and there have been several recent methods that attempt to learn hash functions for fast and accurate nearest neighbor searches. In this paper we develop an algorithm for learning hash functions based on explicitly minimizing the reconstruction error between the original distances and the Hamming distances of the corresponding binary embeddings. We develop a scalable coordinate-descent algorithm for our proposed hashing objective that is able to efficiently learn hash functions in a variety of settings. Unlike existing methods such as semantic hashing and spectral hashing our method is easily kernelized and does not require restrictive assumptions about the underlying distribution of the data. We present results over several domains to demonstrate that our method outperforms existing state-of-the-art techniques.</p>
</td>
</tr>

<tr>
	<td>2009</td>
	<td><a href="/publications/raginsky2009locality/">Locality-sensitive Binary Codes From Shift-invariant Kernels</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Locality-sensitive Binary Codes From Shift-invariant Kernels' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Locality-sensitive Binary Codes From Shift-invariant Kernels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Locality-sensitive%20Binary%20Codes%20From%20Shift-invariant%20Kernels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Maxim Raginsky, Svetlana Lazebnik</td>
	<td>Neural Information Processing Systems</td>
	<td><p>This paper addresses the problem of designing binary codes for high-dimensional data such that vectors that are similar in the original space map to similar binary strings. We introduce a simple distribution-free encoding scheme based on random projections such that the expected Hamming distance between the binary codes of two vectors is related to the value of a shift-invariant kernel (e.g. a Gaussian kernel) between the vectors. We present a full theoretical analysis of the convergence properties of the proposed scheme and report favorable experimental performance as compared to a recent state-of-the-art method spectral hashing.</p>
</td>
</tr>

<tr>
	<td>2009</td>
	<td><a href="/publications/ram2009rank/">Rank-approximate Nearest Neighbor Search Retaining Meaning And Speed In High Dimensions</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Rank-approximate Nearest Neighbor Search Retaining Meaning And Speed In High Dimensions' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Rank-approximate Nearest Neighbor Search Retaining Meaning And Speed In High Dimensions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Rank-approximate%20Nearest%20Neighbor%20Search%20Retaining%20Meaning%20And%20Speed%20In%20High%20Dimensions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Parikshit Ram, Dongryeol Lee, Hua Ouyang, Alexander Gray</td>
	<td>Neural Information Processing Systems</td>
	<td><p>The long-standing problem of efficient nearest-neighbor (NN) search has ubiquitous applications ranging from astrophysics to MP3 fingerprinting to bioinformatics to movie recommendations. As the dimensionality of the dataset increases exact NN search becomes computationally prohibitive; (1+eps)-distance-approximate NN search can provide large speedups but risks losing the meaning of NN search present in the ranks (ordering) of the distances. This paper presents a simple practical algorithm allowing the user to for the first time directly control the true accuracy of NN search (in terms of ranks) while still achieving the large speedups over exact NN. Experiments with high-dimensional datasets show that it often achieves faster and more accurate results than the best-known distance-approximate method with much more stable behavior.</p>
</td>
</tr>



<tr>
	<td>2008</td>
	<td><a href="/publications/jain2008online/">Online Metric Learning And Fast Similarity Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Online Metric Learning And Fast Similarity Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Online Metric Learning And Fast Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Online%20Metric%20Learning%20And%20Fast%20Similarity%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Prateek Jain, Brian Kulis, Inderjit Dhillon, Kristen Grauman</td>
	<td>Neural Information Processing Systems</td>
	<td><p>Metric learning algorithms can provide useful distance functions for a variety of domains and recent work has shown good accuracy for problems where the learner can access all distance constraints at once. However in many real applications constraints are only available incrementally thus necessitating methods that can perform online updates to the learned metric. Existing online algorithms offer bounds on worst-case performance but typically do not perform well in practice as compared to their offline counterparts. We present a new online metric learning algorithm that updates a learned Mahalanobis metric based on LogDet regularization and gradient descent. We prove theoretical worst-case performance bounds and empirically compare the proposed method against existing online metric learning algorithms. To further boost the practicality of our approach we develop an online locality-sensitive hashing scheme which leads to efficient updates for approximate similarity search data structures. We demonstrate our algorithm on multiple datasets and show that it outperforms relevant baselines.</p>
</td>
</tr>

<tr>
	<td>2008</td>
	<td><a href="/publications/gordon2008optimal/">Optimal Hash Functions For Approximate Closest Pairs On The N-cube</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Optimal Hash Functions For Approximate Closest Pairs On The N-cube' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Optimal Hash Functions For Approximate Closest Pairs On The N-cube' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Optimal%20Hash%20Functions%20For%20Approximate%20Closest%20Pairs%20On%20The%20N-cube' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Gordon Daniel M., Miller Victor, Ostapenko Peter</td>
	<td>Arxiv</td>
	<td><p>One way to find closest pairs in large datasets is to use hash functions. In recent years locality-sensitive hash functions for various metrics have been given projecting an n-cube onto k bits is simple hash function that performs well. In this paper we investigate alternatives to projection. For various parameters hash functions given by complete decoding algorithms for codes work better and asymptotically random codes perform better than projection.</p>
</td>
</tr>

<tr>
	<td>2008</td>
	<td><a href="/publications/weiss2008spectral/">Spectral Hashing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Spectral Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Spectral Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Spectral%20Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Yair Weiss, Antonio Torralba, Rob Fergus</td>
	<td>Neural Information Processing Systems</td>
	<td><p>Semantic hashing seeks compact binary codes of datapoints so that the Hamming distance between codewords correlates with semantic similarity. Hinton et al. used a clever implementation of autoencoders to find such codes. In this paper we show that the problem of finding a best code for a given dataset is closely related to the problem of graph partitioning and can be shown to be NP hard. By relaxing the original problem we obtain a spectral method whose solutions are simply a subset of thresh- olded eigenvectors of the graph Laplacian. By utilizing recent results on convergence of graph Laplacian eigenvectors to the Laplace-Beltrami eigen- functions of manifolds we show how to efficiently calculate the code of a novel datapoint. Taken together both learning the code and applying it to a novel point are extremely simple. Our experiments show that our codes significantly outperform the state-of-the art.</p>
</td>
</tr>



<tr>
	<td>2007</td>
	<td><a href="/publications/parsana2007kernels/">Kernels On Attributed Pointsets With Applications</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Kernels On Attributed Pointsets With Applications' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Kernels On Attributed Pointsets With Applications' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Kernels%20On%20Attributed%20Pointsets%20With%20Applications' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Mehul Parsana, Sourangshu Bhattacharya, Chiru Bhattacharya, K. Ramakrishnan</td>
	<td>Neural Information Processing Systems</td>
	<td><p>This paper introduces kernels on attributed pointsets which are sets of vectors embedded in an euclidean space. The embedding gives the notion of neighborhood which is used to define positive semidefinite kernels on pointsets. Two novel kernels on neighborhoods are proposed one evaluating the attribute similarity and the other evaluating shape similarity. Shape similarity function is motivated from spectral graph matching techniques. The kernels are tested on three real life applications face recognition photo album tagging and shot annotation in video sequences with encouraging results.</p>
</td>
</tr>

<tr>
	<td>2007</td>
	<td><a href="/publications/lemire2007recursive/">Recursive N-gram Hashing Is Pairwise Independent At Best</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Recursive N-gram Hashing Is Pairwise Independent At Best' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Recursive N-gram Hashing Is Pairwise Independent At Best' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Recursive%20N-gram%20Hashing%20Is%20Pairwise%20Independent%20At%20Best' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Lemire Daniel, Kaser Owen</td>
	<td>Computer Speech Language</td>
	<td><p>Many applications use sequences of n consecutive symbols (n-grams). Hashing these n-grams can be a performance bottleneck. For more speed recursive hash families compute hash values by updating previous values. We prove that recursive hash families cannot be more than pairwise independent. While hashing by irreducible polynomials is pairwise independent our implementations either run in time O(n) or use an exponential amount of memory. As a more scalable alternative we make hashing by cyclic polynomials pairwise independent by ignoring n-1 bits. Experimentally we show that hashing by cyclic polynomials is is twice as fast as hashing by irreducible polynomials. We also show that randomized Karp-Rabin hash families are not pairwise independent.</p>
</td>
</tr>

<tr>
	<td>2007</td>
	<td><a href="/publications/cayton2007learning/">A Learning Framework For Nearest Neighbor Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Learning Framework For Nearest Neighbor Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Learning Framework For Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20Learning%20Framework%20For%20Nearest%20Neighbor%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Lawrence Cayton, Sanjoy Dasgupta</td>
	<td>Neural Information Processing Systems</td>
	<td><p>Can we leverage learning techniques to build a fast nearest-neighbor (NN) retrieval data structure We present a general learning framework for the NN problem in which sample queries are used to learn the parameters of a data structure that minimize the retrieval time and/or the miss rate. We explore the potential of this novel framework through two popular NN data structures KD-trees and the rectilinear structures employed by locality sensitive hashing. We derive a generalization theory for these data structure classes and present simple learning algorithms for both. Experimental results reveal that learning often improves on the already strong performance of these data structures.</p>
</td>
</tr>



<tr>
	<td>2006</td>
	<td><a href="/publications/frome2006image/">Image Retrieval And Classification Using Local Distance Functions</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Image Retrieval And Classification Using Local Distance Functions' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Image Retrieval And Classification Using Local Distance Functions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Image%20Retrieval%20And%20Classification%20Using%20Local%20Distance%20Functions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Andrea Frome, Yoram Singer, Jitendra Malik</td>
	<td>Neural Information Processing Systems</td>
	<td><p>In this paper we introduce and experiment with a framework for learning local perceptual distance functions for visual recognition. We learn a distance function for each training image as a combination of elementary distances between patch-based visual features. We apply these combined local distance functions to the tasks of image retrieval and classification of novel images. On the Caltech 101 object recognition benchmark we achieve 60.337; mean recognition across classes using 15 training images per class which is better than the best published performance by Zhang et al.</p>
</td>
</tr>



<tr>
	<td>2003</td>
	<td><a href="/publications/platt2003fast/">Fast Embedding Of Sparse Similarity Graphs</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Fast Embedding Of Sparse Similarity Graphs' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Fast Embedding Of Sparse Similarity Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Fast%20Embedding%20Of%20Sparse%20Similarity%20Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>John Platt</td>
	<td>Neural Information Processing Systems</td>
	<td><p>This paper applies fast sparse multidimensional scaling (MDS) to a large graph of music similarity with 267K vertices that represent artists al- bums and tracks; and 3.22M edges that represent similarity between those entities. Once vertices are assigned locations in a Euclidean space the locations can be used to browse music and to generate playlists. MDS on very large sparse graphs can be effectively performed by a family of algorithms called Rectangular Dijsktra (RD) MDS algorithms. These RD algorithms operate on a dense rectangular slice of the distance matrix created by calling Dijsktra a constant number of times. Two RD algorithms are compared Landmark MDS which uses the NystrÃ¶m ap- proximation to perform MDS; and a new algorithm called Fast Sparse Embedding which uses FastMap. These algorithms compare favorably to Laplacian Eigenmaps both in terms of speed and embedding quality.</p>
</td>
</tr>



<tr>
	<td>1998</td>
	<td><a href="/publications/ioffe1998learning/">Learning To Find Pictures Of People</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning To Find Pictures Of People' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning To Find Pictures Of People' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20To%20Find%20Pictures%20Of%20People' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Sergey Ioffe, David Forsyth</td>
	<td>Neural Information Processing Systems</td>
	<td><p>Finding articulated objects like people in pictures present.s a par- ticularly difficult object. recognition problem. We show how t.o find people by finding putative body segments and then construct.- ing assemblies of those segments that are consist.ent with the con- straints on the appearance of a person that result from kinematic properties. Since a reasonable model of a person requires at. least nine segments it is not possible to present every group to a classi- fier. Instead the search can be pruned by using projected versions of a classifier that accepts groups corresponding to people. We describe an efficient projection algorithm for one popular classi- fier and demonstrate that our approach can be used to determine whether images of real scenes contain people.</p>
</td>
</tr>


</tbody></table>

<script>
var datatable;
function searchTable() {
    var hash = decodeURIComponent(window.location.hash.substr(1));
    datatable.search(hash).draw();
}
$(document).ready( function () {
    datatable = $('#allPapers').DataTable({
		paging: false,
		"order": [[ 0, 'desc' ], [ 1, 'asc' ]],
		columnDefs: [
			{
				targets: [3, 4],
				visible: false,
				searchable: true
			}]
		});
    searchTable();
});
$(window).on('hashchange', function() {
  searchTable();
});
</script>

    </div>

  </body>
</html>
