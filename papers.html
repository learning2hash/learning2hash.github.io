<!DOCTYPE html>
<html lang="en-us">

  <head>
<!-- Begin Web-Stat code v 7.0 -->
<span id="wts2185304"></span>
<script>var wts=document.createElement('script');wts.async=true;
wts.src='https://app.ardalio.com/log7.js';document.head.appendChild(wts);
wts.onload = function(){ wtslog7(2185304,4); };
</script><noscript><a href="https://www.web-stat.com">
<img src="https://app.ardalio.com/7/4/2185304.png" 
alt="Web-Stat web statistics"></a></noscript>
<!-- End Web-Stat code v 7.0 -->
  <!-- Hotjar Tracking Code for https://learning2hash.github.io/ -->
<script>
    (function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:1843243,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109544763-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-109544763-1');
</script>
<script>
    window.MathJax = {
      tex: {
        inlineMath: [["\\(","\\)"]],
        displayMath: [["\\[","\\]"]],
      },
      options: {
        processHtmlClass: "mathjax-content",
        processEscapes: true,
      }
    };
  </script>
  <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="keywords" content="machine learning, hashing, approximate nearest neighbour search, lsh, learning-to-hash">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Search all Publications on Machine Learning for Hashing | Awesome Learning to Hash</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Search all Publications on Machine Learning for Hashing" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A list of research papers for machine learning models for hashing." />
<meta property="og:description" content="A list of research papers for machine learning models for hashing." />
<link rel="canonical" href="https://learning2hash.github.io/papers.html" />
<meta property="og:url" content="https://learning2hash.github.io/papers.html" />
<meta property="og:site_name" content="Awesome Learning to Hash" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Search all Publications on Machine Learning for Hashing" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"A list of research papers for machine learning models for hashing.","headline":"Search all Publications on Machine Learning for Hashing","url":"https://learning2hash.github.io/papers.html"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link rel="search" href="/public/opensearchdescription.xml" 
      type="application/opensearchdescription+xml" 
      title="learning2hash" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
  integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
  crossorigin="anonymous"></script>
  
  <link rel="stylesheet" type="text/css" href="//cdn.datatables.net/1.10.16/css/jquery.dataTables.min.css">
  <script type="text/javascript" charset="utf8" src="//cdn.datatables.net/1.10.16/js/jquery.dataTables.min.js"></script>
</head>


  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <body class="theme-base-0c layout-reverse">

    <a href='/contributing.html' class='ribbon'>Add your paper to Learning2Hash</a>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Awesome Learning to Hash
        </a>
      </h1>
      <p class="lead">A Webpage dedicated to the latest research on Hash Function Learning. Maintained by <a href="http://sjmoran.github.io/">Sean Moran</a>.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="sidebar-item">
        <p style="font-size: 12px">
          Search related work 
          <input type='text' id='searchTarget' size="16"/> 
          <button onClick="search();">Go</button>
        </p>
      </div>
      <a class="sidebar-nav-item" href="/papers.html">All Papers</a>
      <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
      <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>
      <a class="sidebar-nav-item" href="/topic-viz.html">Topic-based Explorer</a>
      <a class="sidebar-nav-item" href="/tutorial.html">Tutorial</a>
      <a class="sidebar-nav-item" href="/resources.html">Resources, Courses &#38; Events</a>
      <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
    </nav>

    <div class="sidebar-item">
      <p style="font-size: 12px">
        Contact <a href="http://www.seanjmoran.com">Sean Moran</a> about this survey or website.
        <span style="font-size: 9px">
          Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
        </span>
      </p>
    </div>
  </div>
</div>

<script>
$("#searchTarget").keydown(function (e) {	
  if (e.keyCode == 13) {
    search();
  }
});

function search() {
  try {
    ga('send', 'event', 'search', 'search', $("#searchTarget").val());
  } finally {
    window.location = "/papers.html#" + $("#searchTarget").val();
  }
}
</script>


    <div class="content container">
      Search across all paper titles, abstracts, and authors by using the search field.
Please consider <a href="/contributing.html">contributing</a> by updating
the information of existing papers or adding new work.

<!-- Loading Indicator -->
<div id="loading">
  <p>Loading...</p>
</div>

<!-- Data Table -->
<table id="allPapers">
<thead>
  <tr>
    <th>Year</th>
    <th>Title</th>
    <th>Authors</th>
    <th>Venue</th>
    <th>Citations</th>
    <th>Abstract</th>
    <th>Tags</th>
  </tr>
  </thead>
  <tbody>
    
    
      
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/pham2024composing/">Composing Object Relations and Attributes for Image-Text Matching</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Composing Object Relations and Attributes for Image-Text Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Composing Object Relations and Attributes for Image-Text Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Pham Khoi, Huynh Chuong, Lim Ser-nam, Shrivastava Abhinav</td> <!-- 🔧 You were missing this -->
    <td>2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>11</td>
    <td><p>We study the visual semantic embedding problem for image-text matching. Most
existing work utilizes a tailored cross-attention mechanism to perform local
alignment across the two image and text modalities. This is computationally
expensive, even though it is more powerful than the unimodal dual-encoder
approach. This work introduces a dual-encoder image-text matching model,
leveraging a scene graph to represent captions with nodes for objects and
attributes interconnected by relational edges. Utilizing a graph attention
network, our model efficiently encodes object-attribute and object-object
semantic relations, resulting in a robust and fast-performing system.
Representing caption as a scene graph offers the ability to utilize the strong
relational inductive bias of graph neural networks to learn object-attribute
and object-object relations effectively. To train the model, we propose losses
that align the image and caption both at the holistic level (image-caption) and
the local level (image-object entity), which we show is key to the success of
the model. Our model is termed Composition model for Object Relations and
Attributes, CORA. Experimental results on two prominent image-text retrieval
benchmarks, Flickr30K and MSCOCO, demonstrate that CORA outperforms existing
state-of-the-art computationally expensive cross-attention methods regarding
recall score while achieving fast computation speed of the dual encoder.</p>
</td>
    <td>
      
        CVPR 
      
        Text-Retrieval 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/ma2023direction/">Direction-Oriented Visual-semantic Embedding Model for Remote Sensing Image-text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Direction-Oriented Visual-semantic Embedding Model for Remote Sensing Image-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Direction-Oriented Visual-semantic Embedding Model for Remote Sensing Image-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ma Qing, Pan Jiancheng, Bai Cong</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Geoscience and Remote Sensing</td>
    <td>11</td>
    <td><p>Image-text retrieval has developed rapidly in recent years. However, it is
still a challenge in remote sensing due to visual-semantic imbalance, which
leads to incorrect matching of non-semantic visual and textual features. To
solve this problem, we propose a novel Direction-Oriented Visual-semantic
Embedding Model (DOVE) to mine the relationship between vision and language.
Our highlight is to conduct visual and textual representations in latent space,
directing them as close as possible to a redundancy-free regional visual
representation. Concretely, a Regional-Oriented Attention Module (ROAM)
adaptively adjusts the distance between the final visual and textual embeddings
in the latent semantic space, oriented by regional visual features. Meanwhile,
a lightweight Digging Text Genome Assistant (DTGA) is designed to expand the
range of tractable textual representation and enhance global word-level
semantic connections using less attention operations. Ultimately, we exploit a
global visual-semantic constraint to reduce single visual dependency and serve
as an external constraint for the final visual and textual representations. The
effectiveness and superiority of our method are verified by extensive
experiments including parameter evaluation, quantitative comparison, ablation
studies and visual analysis, on two benchmark datasets, RSICD and RSITMD.</p>
</td>
    <td>
      
        Datasets 
      
        Text-Retrieval 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/liu2023bi/">Bi-directional Training for Composed Image Retrieval via Text Prompt Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Bi-directional Training for Composed Image Retrieval via Text Prompt Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Bi-directional Training for Composed Image Retrieval via Text Prompt Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Zheyuan, Sun Weixuan, Hong Yicong, Teney Damien, Gould Stephen</td> <!-- 🔧 You were missing this -->
    <td>2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>13</td>
    <td><p>Composed image retrieval searches for a target image based on a multi-modal
user query comprised of a reference image and modification text describing the
desired changes. Existing approaches to solving this challenging task learn a
mapping from the (reference image, modification text)-pair to an image
embedding that is then matched against a large image corpus. One area that has
not yet been explored is the reverse direction, which asks the question, what
reference image when modified as described by the text would produce the given
target image? In this work we propose a bi-directional training scheme that
leverages such reversed queries and can be applied to existing composed image
retrieval architectures with minimum changes, which improves the performance of
the model. To encode the bi-directional query we prepend a learnable token to
the modification text that designates the direction of the query and then
finetune the parameters of the text embedding module. We make no other changes
to the network architecture. Experiments on two standard datasets show that our
novel approach achieves improved performance over a baseline BLIP-based model
that itself already achieves competitive performance. Our code is released at
https://github.com/Cuberick-Orion/Bi-Blip4CIR.</p>
</td>
    <td>
      
        Datasets 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/jush2023medical/">Medical Image Retrieval Using Pretrained Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Medical Image Retrieval Using Pretrained Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Medical Image Retrieval Using Pretrained Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jush Farnaz Khun, Truong Tuan, Vogler Steffen, Lenga Matthias</td> <!-- 🔧 You were missing this -->
    <td>2024 IEEE International Symposium on Biomedical Imaging (ISBI)</td>
    <td>8</td>
    <td><p>A wide range of imaging techniques and data formats available for medical
images make accurate retrieval from image databases challenging.
  Efficient retrieval systems are crucial in advancing medical research,
enabling large-scale studies and innovative diagnostic tools. Thus, addressing
the challenges of medical image retrieval is essential for the continued
enhancement of healthcare and research.
  In this study, we evaluated the feasibility of employing four
state-of-the-art pretrained models for medical image retrieval at modality,
body region, and organ levels and compared the results of two similarity
indexing approaches. Since the employed networks take 2D images, we analyzed
the impacts of weighting and sampling strategies to incorporate 3D information
during retrieval of 3D volumes. We showed that medical image retrieval is
feasible using pretrained networks without any additional training or
fine-tuning steps. Using pretrained embeddings, we achieved a recall of 1 for
various tasks at modality, body region, and organ level.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Scalability 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/ji2023hierarchical/">Hierarchical Matching and Reasoning for Multi-Query Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hierarchical Matching and Reasoning for Multi-Query Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hierarchical Matching and Reasoning for Multi-Query Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ji Zhong, Li Zhihao, Zhang Yan, Wang Haoran, Pang Yanwei, Li Xuelong</td> <!-- 🔧 You were missing this -->
    <td>Neural Networks</td>
    <td>8</td>
    <td><p>As a promising field, Multi-Query Image Retrieval (MQIR) aims at searching
for the semantically relevant image given multiple region-specific text
queries. Existing works mainly focus on a single-level similarity between image
regions and text queries, which neglects the hierarchical guidance of
multi-level similarities and results in incomplete alignments. Besides, the
high-level semantic correlations that intrinsically connect different
region-query pairs are rarely considered. To address above limitations, we
propose a novel Hierarchical Matching and Reasoning Network (HMRN) for MQIR. It
disentangles MQIR into three hierarchical semantic representations, which is
responsible to capture fine-grained local details, contextual global scopes,
and high-level inherent correlations. HMRN comprises two modules: Scalar-based
Matching (SM) module and Vector-based Reasoning (VR) module. Specifically, the
SM module characterizes the multi-level alignment similarity, which consists of
a fine-grained local-level similarity and a context-aware global-level
similarity. Afterwards, the VR module is developed to excavate the potential
semantic correlations among multiple region-query pairs, which further explores
the high-level reasoning similarity. Finally, these three-level similarities
are aggregated into a joint similarity space to form the ultimate similarity.
Extensive experiments on the benchmark dataset demonstrate that our HMRN
substantially surpasses the current state-of-the-art methods. For instance,
compared with the existing best method Drill-down, the metric R@1 in the last
round is improved by 23.4%. Our source codes will be released at
https://github.com/LZH-053/HMRN.</p>
</td>
    <td>
      
        Evaluation 
      
        Datasets 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/hou2024bridging/">Bridging Language and Items for Retrieval and Recommendation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Bridging Language and Items for Retrieval and Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Bridging Language and Items for Retrieval and Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hou Yupeng, Li Jiacheng, He Zhankui, Yan An, Chen Xiusi, Mcauley Julian</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>11</td>
    <td><p>This paper introduces BLaIR, a series of pretrained sentence embedding models
specialized for recommendation scenarios. BLaIR is trained to learn
correlations between item metadata and potential natural language context,
which is useful for retrieving and recommending items. To pretrain BLaIR, we
collect Amazon Reviews 2023, a new dataset comprising over 570 million reviews
and 48 million items from 33 categories, significantly expanding beyond the
scope of previous versions. We evaluate the generalization ability of BLaIR
across multiple domains and tasks, including a new task named complex product
search, referring to retrieving relevant items given long, complex natural
language contexts. Leveraging large language models like ChatGPT, we
correspondingly construct a semi-synthetic evaluation set, Amazon-C4. Empirical
results on the new task, as well as conventional retrieval and recommendation
tasks, demonstrate that BLaIR exhibit strong text and item representation
capacity. Our datasets, code, and checkpoints are available at:
https://github.com/hyp1231/AmazonReviews2023.</p>
</td>
    <td>
      
        Datasets 
      
        Recommender-Systems 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/huang2024cross/">Cross-Modal and Uni-Modal Soft-Label Alignment for Image-Text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-Modal and Uni-Modal Soft-Label Alignment for Image-Text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-Modal and Uni-Modal Soft-Label Alignment for Image-Text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Huang Hailang, Nie Zhijie, Wang Ziqiao, Shang Ziyu</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>9</td>
    <td><p>Current image-text retrieval methods have demonstrated impressive performance
in recent years. However, they still face two problems: the inter-modal
matching missing problem and the intra-modal semantic loss problem. These
problems can significantly affect the accuracy of image-text retrieval. To
address these challenges, we propose a novel method called Cross-modal and
Uni-modal Soft-label Alignment (CUSA). Our method leverages the power of
uni-modal pre-trained models to provide soft-label supervision signals for the
image-text retrieval model. Additionally, we introduce two alignment
techniques, Cross-modal Soft-label Alignment (CSA) and Uni-modal Soft-label
Alignment (USA), to overcome false negatives and enhance similarity recognition
between uni-modal samples. Our method is designed to be plug-and-play, meaning
it can be easily applied to existing image-text retrieval models without
changing their original architectures. Extensive experiments on various
image-text retrieval models and datasets, we demonstrate that our method can
consistently improve the performance of image-text retrieval and achieve new
state-of-the-art results. Furthermore, our method can also boost the uni-modal
retrieval performance of image-text retrieval models, enabling it to achieve
universal retrieval. The code and supplementary files can be found at
https://github.com/lerogo/aaai24_itr_cusa.</p>
</td>
    <td>
      
        AAAI 
      
        Datasets 
      
        Text-Retrieval 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/douze2024faiss/">The Faiss library</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=The Faiss library' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=The Faiss library' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Douze Matthijs, Guzhva Alexandr, Deng Chengqi, Johnson Jeff, Szilvasy Gergely, Mazaré Pierre-emmanuel, Lomeli Maria, Hosseini Lucas, Jégou Hervé</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>17</td>
    <td><p>Vector databases typically manage large collections of embedding vectors.
Currently, AI applications are growing rapidly, and so is the number of
embeddings that need to be stored and indexed. The Faiss library is dedicated
to vector similarity search, a core functionality of vector databases. Faiss is
a toolkit of indexing methods and related primitives used to search, cluster,
compress and transform vectors. This paper describes the trade-off space of
vector search and the design principles of Faiss in terms of structure,
approach to optimization and interfacing. We benchmark key features of the
library and discuss a few selected applications to highlight its broad
applicability.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Evaluation 
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2024</td>
    <td>
      <a href="/publications/bruch2024efficient/">Efficient Inverted Indexes for Approximate Retrieval over Learned Sparse Representations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Inverted Indexes for Approximate Retrieval over Learned Sparse Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient Inverted Indexes for Approximate Retrieval over Learned Sparse Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Bruch Sebastian, Nardini Franco Maria, Rulli Cosimo, Venturini Rossano</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>14</td>
    <td><p>Learned sparse representations form an attractive class of contextual
embeddings for text retrieval. That is so because they are effective models of
relevance and are interpretable by design. Despite their apparent compatibility
with inverted indexes, however, retrieval over sparse embeddings remains
challenging. That is due to the distributional differences between learned
embeddings and term frequency-based lexical models of relevance such as BM25.
Recognizing this challenge, a great deal of research has gone into, among other
things, designing retrieval algorithms tailored to the properties of learned
sparse representations, including approximate retrieval systems. In fact, this
task featured prominently in the latest BigANN Challenge at NeurIPS 2023, where
approximate algorithms were evaluated on a large benchmark dataset by
throughput and recall. In this work, we propose a novel organization of the
inverted index that enables fast yet effective approximate retrieval over
learned sparse embeddings. Our approach organizes inverted lists into
geometrically-cohesive blocks, each equipped with a summary vector. During
query processing, we quickly determine if a block must be evaluated using the
summaries. As we show experimentally, single-threaded query processing using
our method, Seismic, reaches sub-millisecond per-query latency on various
sparse embeddings of the MS MARCO dataset while maintaining high recall. Our
results indicate that Seismic is one to two orders of magnitude faster than
state-of-the-art inverted index-based solutions and further outperforms the
winning (graph-based) submissions to the BigANN Challenge by a significant
margin.</p>
</td>
    <td>
      
        Text-Retrieval 
      
        Graph-Based-ANN 
      
        Datasets 
      
        SIGIR 
      
        Evaluation 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/tan2022multilingual/">Multilingual Representation Distillation with Contrastive Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multilingual Representation Distillation with Contrastive Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multilingual Representation Distillation with Contrastive Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tan Weiting, Heffernan Kevin, Schwenk Holger, Koehn Philipp</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics</td>
    <td>7</td>
    <td><p>Multilingual sentence representations from large models encode semantic
information from two or more languages and can be used for different
cross-lingual information retrieval and matching tasks. In this paper, we
integrate contrastive learning into multilingual representation distillation
and use it for quality estimation of parallel sentences (i.e., find
semantically similar sentences that can be used as translations of each other).
We validate our approach with multilingual similarity search and corpus
filtering tasks. Experiments across different low-resource languages show that
our method greatly outperforms previous sentence encoders such as LASER,
LASER3, and LaBSE.</p>
</td>
    <td>
      
        Self-Supervised 
      
        Similarity-Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/song2022boosting/">Boosting vision transformers for image retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Boosting vision transformers for image retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Boosting vision transformers for image retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Song Chull Hwan, Yoon Jooyoung, Choi Shunghyun, Avrithis Yannis</td> <!-- 🔧 You were missing this -->
    <td>2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>28</td>
    <td><p>Vision transformers have achieved remarkable progress in vision tasks such as
image classification and detection. However, in instance-level image retrieval,
transformers have not yet shown good performance compared to convolutional
networks. We propose a number of improvements that make transformers outperform
the state of the art for the first time. (1) We show that a hybrid architecture
is more effective than plain transformers, by a large margin. (2) We introduce
two branches collecting global (classification token) and local (patch tokens)
information, from which we form a global image representation. (3) In each
branch, we collect multi-layer features from the transformer encoder,
corresponding to skip connections across distant layers. (4) We enhance
locality of interactions at the deeper layers of the encoder, which is the
relative weakness of vision transformers. We train our model on all commonly
used training sets and, for the first time, we make fair comparisons separately
per training set. In all cases, we outperform previous models based on global
representation. Public code is available at
https://github.com/dealicious-inc/DToP.</p>
</td>
    <td>
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/shao2023global/">Global Features are All You Need for Image Retrieval and Reranking</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Global Features are All You Need for Image Retrieval and Reranking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Global Features are All You Need for Image Retrieval and Reranking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shao Shihao, Chen Kaifeng, Karpur Arjun, Cui Qinghua, Araujo Andre, Cao Bingyi</td> <!-- 🔧 You were missing this -->
    <td>2023 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>21</td>
    <td><p>Image retrieval systems conventionally use a two-stage paradigm, leveraging
global features for initial retrieval and local features for reranking.
However, the scalability of this method is often limited due to the significant
storage and computation cost incurred by local feature matching in the
reranking stage. In this paper, we present SuperGlobal, a novel approach that
exclusively employs global features for both stages, improving efficiency
without sacrificing accuracy. SuperGlobal introduces key enhancements to the
retrieval system, specifically focusing on the global feature extraction and
reranking processes. For extraction, we identify sub-optimal performance when
the widely-used ArcFace loss and Generalized Mean (GeM) pooling methods are
combined and propose several new modules to improve GeM pooling. In the
reranking stage, we introduce a novel method to update the global features of
the query and top-ranked images by only considering feature refinement with a
small set of images, thus being very compute and memory efficient. Our
experiments demonstrate substantial improvements compared to the state of the
art in standard benchmarks. Notably, on the Revisited Oxford+1M Hard dataset,
our single-stage results improve by 7.1%, while our two-stage gain reaches 3.7%
with a strong 64,865x speedup. Our two-stage system surpasses the current
single-stage state-of-the-art by 16.3%, offering a scalable, accurate
alternative for high-performing image retrieval systems with minimal time
overhead. Code: https://github.com/ShihaoShao-GH/SuperGlobal.</p>
</td>
    <td>
      
        ICCV 
      
        Image-Retrieval 
      
        Datasets 
      
        Scalability 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/rajput2023recommender/">Recommender Systems with Generative Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Recommender Systems with Generative Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Recommender Systems with Generative Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Rajput Shashank, Mehta Nikhil, Singh Anima, Keshavan Raghunandan H., Vu Trung, Heldt Lukasz, Hong Lichan, Tay Yi, Tran Vinh Q., Samost Jonah, Kula Maciej, Chi Ed H., Sathiamoorthy Maheswaran</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>17</td>
    <td><p>Modern recommender systems perform large-scale retrieval by first embedding
queries and item candidates in the same unified space, followed by approximate
nearest neighbor search to select top candidates given a query embedding. In
this paper, we propose a novel generative retrieval approach, where the
retrieval model autoregressively decodes the identifiers of the target
candidates. To that end, we create semantically meaningful tuple of codewords
to serve as a Semantic ID for each item. Given Semantic IDs for items in a user
session, a Transformer-based sequence-to-sequence model is trained to predict
the Semantic ID of the next item that the user will interact with. To the best
of our knowledge, this is the first Semantic ID-based generative model for
recommendation tasks. We show that recommender systems trained with the
proposed paradigm significantly outperform the current SOTA models on various
datasets. In addition, we show that incorporating Semantic IDs into the
sequence-to-sequence model enhances its ability to generalize, as evidenced by
the improved retrieval performance observed for items with no prior interaction
history.</p>
</td>
    <td>
      
        Recommender-Systems 
      
        Datasets 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/qu2023learnable/">Learnable Pillar-based Re-ranking for Image-Text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learnable Pillar-based Re-ranking for Image-Text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learnable Pillar-based Re-ranking for Image-Text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Qu Leigang, Liu Meng, Wang Wenjie, Zheng Zhedong, Nie Liqiang, Chua Tat-seng</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>12</td>
    <td><p>Image-text retrieval aims to bridge the modality gap and retrieve cross-modal
content based on semantic similarities. Prior work usually focuses on the
pairwise relations (i.e., whether a data sample matches another) but ignores
the higher-order neighbor relations (i.e., a matching structure among multiple
data samples). Re-ranking, a popular post-processing practice, has revealed the
superiority of capturing neighbor relations in single-modality retrieval tasks.
However, it is ineffective to directly extend existing re-ranking algorithms to
image-text retrieval. In this paper, we analyze the reason from four
perspectives, i.e., generalization, flexibility, sparsity, and asymmetry, and
propose a novel learnable pillar-based re-ranking paradigm. Concretely, we
first select top-ranked intra- and inter-modal neighbors as pillars, and then
reconstruct data samples with the neighbor relations between them and the
pillars. In this way, each sample can be mapped into a multimodal pillar space
only using similarities, ensuring generalization. After that, we design a
neighbor-aware graph reasoning module to flexibly exploit the relations and
excavate the sparse positive items within a neighborhood. We also present a
structure alignment constraint to promote cross-modal collaboration and align
the asymmetric modalities. On top of various base backbones, we carry out
extensive experiments on two benchmark datasets, i.e., Flickr30K and MS-COCO,
demonstrating the effectiveness, superiority, generalization, and
transferability of our proposed re-ranking paradigm.</p>
</td>
    <td>
      
        Text-Retrieval 
      
        Datasets 
      
        SIGIR 
      
        Hybrid-ANN-Methods 
      
        Re-Ranking 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/peng2023embedding/">Embedding-based Retrieval with LLM for Effective Agriculture Information Extracting from Unstructured Data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Embedding-based Retrieval with LLM for Effective Agriculture Information Extracting from Unstructured Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Embedding-based Retrieval with LLM for Effective Agriculture Information Extracting from Unstructured Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Peng Ruoling, Liu Kang, Yang Po, Yuan Zhipeng, Li Shunbao</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>14</td>
    <td><p>Pest identification is a crucial aspect of pest control in agriculture.
However, most farmers are not capable of accurately identifying pests in the
field, and there is a limited number of structured data sources available for
rapid querying. In this work, we explored using domain-agnostic general
pre-trained large language model(LLM) to extract structured data from
agricultural documents with minimal or no human intervention. We propose a
methodology that involves text retrieval and filtering using embedding-based
retrieval, followed by LLM question-answering to automatically extract entities
and attributes from the documents, and transform them into structured data. In
comparison to existing methods, our approach achieves consistently better
accuracy in the benchmark while maintaining efficiency.</p>
</td>
    <td>
      
        Text-Retrieval 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/peer2023towards/">Towards Writer Retrieval for Historical Datasets</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Towards Writer Retrieval for Historical Datasets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Towards Writer Retrieval for Historical Datasets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Peer Marco, Kleber Florian, Sablatnig Robert</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>7</td>
    <td><p>This paper presents an unsupervised approach for writer retrieval based on
clustering SIFT descriptors detected at keypoint locations resulting in
pseudo-cluster labels. With those cluster labels, a residual network followed
by our proposed NetRVLAD, an encoding layer with reduced complexity compared to
NetVLAD, is trained on 32x32 patches at keypoint locations. Additionally, we
suggest a graph-based reranking algorithm called SGR to exploit similarities of
the page embeddings to boost the retrieval performance. Our approach is
evaluated on two historical datasets (Historical-WI and HisIR19). We include an
evaluation of different backbones and NetRVLAD. It competes with related work
on historical datasets without using explicit encodings. We set a new
State-of-the-art on both datasets by applying our reranking scheme and show
that our approach achieves comparable performance on a modern dataset as well.</p>
</td>
    <td>
      
        Unsupervised 
      
        Datasets 
      
        Evaluation 
      
        Graph-Based-ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/salemi2023symmetric/">A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Salemi Alireza, Pizzorno Juan Altmayer, Zamani Hamed</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>13</td>
    <td><p>Knowledge-Intensive Visual Question Answering (KI-VQA) refers to answering a
question about an image whose answer does not lie in the image. This paper
presents a new pipeline for KI-VQA tasks, consisting of a retriever and a
reader. First, we introduce DEDR, a symmetric dual encoding dense retrieval
framework in which documents and queries are encoded into a shared embedding
space using uni-modal (textual) and multi-modal encoders. We introduce an
iterative knowledge distillation approach that bridges the gap between the
representation spaces in these two encoders. Extensive evaluation on two
well-established KI-VQA datasets, i.e., OK-VQA and FVQA, suggests that DEDR
outperforms state-of-the-art baselines by 11.6% and 30.9% on OK-VQA and FVQA,
respectively. Utilizing the passages retrieved by DEDR, we further introduce
MM-FiD, an encoder-decoder multi-modal fusion-in-decoder model, for generating
a textual answer for KI-VQA tasks. MM-FiD encodes the question, the image, and
each retrieved passage separately and uses all passages jointly in its decoder.
Compared to competitive baselines in the literature, this approach leads to
5.5% and 8.5% improvements in terms of question answering accuracy on OK-VQA
and FVQA, respectively.</p>
</td>
    <td>
      
        Datasets 
      
        SIGIR 
      
        Evaluation 
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/zhang2023graph/">Graph Convolution Based Efficient Re-Ranking for Visual Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Graph Convolution Based Efficient Re-Ranking for Visual Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Graph Convolution Based Efficient Re-Ranking for Visual Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Yuqi, Qian Qi, Wang Hongsong, Liu Chong, Chen Weihua, Wang Fan</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>9</td>
    <td><p>Visual retrieval tasks such as image retrieval and person re-identification
(Re-ID) aim at effectively and thoroughly searching images with similar content
or the same identity. After obtaining retrieved examples, re-ranking is a
widely adopted post-processing step to reorder and improve the initial
retrieval results by making use of the contextual information from semantically
neighboring samples. Prevailing re-ranking approaches update distance metrics
and mostly rely on inefficient crosscheck set comparison operations while
computing expanded neighbors based distances. In this work, we present an
efficient re-ranking method which refines initial retrieval results by updating
features. Specifically, we reformulate re-ranking based on Graph Convolution
Networks (GCN) and propose a novel Graph Convolution based Re-ranking (GCR) for
visual retrieval tasks via feature propagation. To accelerate computation for
large-scale retrieval, a decentralized and synchronous feature propagation
algorithm which supports parallel or distributed computing is introduced. In
particular, the plain GCR is extended for cross-camera retrieval and an
improved feature propagation formulation is presented to leverage affinity
relationships across different cameras. It is also extended for video-based
retrieval, and Graph Convolution based Re-ranking for Video (GCRV) is proposed
by mathematically deriving a novel profile vector generation method for the
tracklet. Without bells and whistles, the proposed approaches achieve
state-of-the-art performances on seven benchmark datasets from three different
tasks, i.e., image retrieval, person Re-ID and video-based person Re-ID.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Hybrid-ANN-Methods 
      
        Re-Ranking 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/mckeown2022hamming/">Hamming Distributions of Popular Perceptual Hashing Techniques</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hamming Distributions of Popular Perceptual Hashing Techniques' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hamming Distributions of Popular Perceptual Hashing Techniques' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mckeown Sean, Buchanan William J</td> <!-- 🔧 You were missing this -->
    <td>Forensic Science International: Digital Investigation</td>
    <td>9</td>
    <td><p>Content-based file matching has been widely deployed for decades, largely for
the detection of sources of copyright infringement, extremist materials, and
abusive sexual media. Perceptual hashes, such as Microsoft’s PhotoDNA, are one
automated mechanism for facilitating detection, allowing for machines to
approximately match visual features of an image or video in a robust manner.
However, there does not appear to be much public evaluation of such approaches,
particularly when it comes to how effective they are against content-preserving
modifications to media files. In this paper, we present a million-image scale
evaluation of several perceptual hashing archetypes for popular algorithms
(including Facebook’s PDQ, Apple’s Neuralhash, and the popular pHash library)
against seven image variants. The focal point is the distribution of Hamming
distance scores between both unrelated images and image variants to better
understand the problems faced by each approach.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Evaluation 
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/lu2023attributes/">Attributes Grouping and Mining Hashing for Fine-Grained Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Attributes Grouping and Mining Hashing for Fine-Grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Attributes Grouping and Mining Hashing for Fine-Grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lu Xin, Chen Shikun, Cao Yichao, Zhou Xin, Lu Xiaobo</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 31st ACM International Conference on Multimedia</td>
    <td>10</td>
    <td><p>In recent years, hashing methods have been popular in the large-scale media
search for low storage and strong representation capabilities. To describe
objects with similar overall appearance but subtle differences, more and more
studies focus on hashing-based fine-grained image retrieval. Existing hashing
networks usually generate both local and global features through attention
guidance on the same deep activation tensor, which limits the diversity of
feature representations. To handle this limitation, we substitute convolutional
descriptors for attention-guided features and propose an Attributes Grouping
and Mining Hashing (AGMH), which groups and embeds the category-specific visual
attributes in multiple descriptors to generate a comprehensive feature
representation for efficient fine-grained image retrieval. Specifically, an
Attention Dispersion Loss (ADL) is designed to force the descriptors to attend
to various local regions and capture diverse subtle details. Moreover, we
propose a Stepwise Interactive External Attention (SIEA) to mine critical
attributes in each descriptor and construct correlations between fine-grained
attributes and objects. The attention mechanism is dedicated to learning
discrete attributes, which will not cost additional computations in hash codes
generation. Finally, the compact binary codes are learned by preserving
pairwise similarities. Experimental results demonstrate that AGMH consistently
yields the best performance against state-of-the-art methods on fine-grained
benchmark datasets.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Compact-Codes 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/lu2022asymmetric/">Asymmetric Transfer Hashing with Adaptive Bipartite Graph Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Asymmetric Transfer Hashing with Adaptive Bipartite Graph Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Asymmetric Transfer Hashing with Adaptive Bipartite Graph Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lu Jianglin, Zhou Jie, Chen Yudong, Pedrycz Witold, Hung Kwok-wai</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Cybernetics</td>
    <td>8</td>
    <td><p>Thanks to the efficient retrieval speed and low storage consumption, learning
to hash has been widely used in visual retrieval tasks. However, existing
hashing methods assume that the query and retrieval samples lie in homogeneous
feature space within the same domain. As a result, they cannot be directly
applied to heterogeneous cross-domain retrieval. In this paper, we propose a
Generalized Image Transfer Retrieval (GITR) problem, which encounters two
crucial bottlenecks: 1) the query and retrieval samples may come from different
domains, leading to an inevitable {domain distribution gap}; 2) the features of
the two domains may be heterogeneous or misaligned, bringing up an additional
{feature gap}. To address the GITR problem, we propose an Asymmetric Transfer
Hashing (ATH) framework with its unsupervised/semi-supervised/supervised
realizations. Specifically, ATH characterizes the domain distribution gap by
the discrepancy between two asymmetric hash functions, and minimizes the
feature gap with the help of a novel adaptive bipartite graph constructed on
cross-domain data. By jointly optimizing asymmetric hash functions and the
bipartite graph, not only can knowledge transfer be achieved but information
loss caused by feature alignment can also be avoided. Meanwhile, to alleviate
negative transfer, the intrinsic geometrical structure of single-domain data is
preserved by involving a domain affinity graph. Extensive experiments on both
single-domain and cross-domain benchmarks under different GITR subtasks
indicate the superiority of our ATH method in comparison with the
state-of-the-art hashing methods.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Supervised 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Unsupervised 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/tu2022unsupervised/">Unsupervised Hashing with Semantic Concept Mining</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Hashing with Semantic Concept Mining' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Hashing with Semantic Concept Mining' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tu Rong-cheng, Mao Xian-ling, Lin Kevin Qinghong, Cai Chengfei, Qin Weize, Wang Hongfa, Wei Wei, Huang Heyan</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the ACM on Management of Data</td>
    <td>6</td>
    <td><p>Recently, to improve the unsupervised image retrieval performance, plenty of
unsupervised hashing methods have been proposed by designing a semantic
similarity matrix, which is based on the similarities between image features
extracted by a pre-trained CNN model. However, most of these methods tend to
ignore high-level abstract semantic concepts contained in images. Intuitively,
concepts play an important role in calculating the similarity among images. In
real-world scenarios, each image is associated with some concepts, and the
similarity between two images will be larger if they share more identical
concepts. Inspired by the above intuition, in this work, we propose a novel
Unsupervised Hashing with Semantic Concept Mining, called UHSCM, which
leverages a VLP model to construct a high-quality similarity matrix.
Specifically, a set of randomly chosen concepts is first collected. Then, by
employing a vision-language pretraining (VLP) model with the prompt engineering
which has shown strong power in visual representation learning, the set of
concepts is denoised according to the training images. Next, the proposed
method UHSCM applies the VLP model with prompting again to mine the concept
distribution of each image and construct a high-quality semantic similarity
matrix based on the mined concept distributions. Finally, with the semantic
similarity matrix as guiding information, a novel hashing loss with a modified
contrastive loss based regularization item is proposed to optimize the hashing
network. Extensive experiments on three benchmark datasets show that the
proposed method outperforms the state-of-the-art baselines in the image
retrieval task.</p>
</td>
    <td>
      
        Supervised 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Neural-Hashing 
      
        Unsupervised 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/lin2022dense/">A Dense Representation Framework for Lexical and Semantic Matching</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Dense Representation Framework for Lexical and Semantic Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Dense Representation Framework for Lexical and Semantic Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin Sheng-chieh, Lin Jimmy</td> <!-- 🔧 You were missing this -->
    <td>ACM Transactions on Information Systems</td>
    <td>8</td>
    <td><p>Lexical and semantic matching capture different successful approaches to text
retrieval and the fusion of their results has proven to be more effective and
robust than either alone. Prior work performs hybrid retrieval by conducting
lexical and semantic matching using different systems (e.g., Lucene and Faiss,
respectively) and then fusing their model outputs. In contrast, our work
integrates lexical representations with dense semantic representations by
densifying high-dimensional lexical representations into what we call
low-dimensional dense lexical representations (DLRs). Our experiments show that
DLRs can effectively approximate the original lexical representations,
preserving effectiveness while improving query latency. Furthermore, we can
combine dense lexical and semantic representations to generate dense hybrid
representations (DHRs) that are more flexible and yield faster retrieval
compared to existing hybrid techniques. In addition, we explore it jointly
training lexical and semantic representations in a single model and empirically
show that the resulting DHRs are able to combine the advantages of the
individual components. Our best DHR model is competitive with state-of-the-art
single-vector and multi-vector dense retrievers in both in-domain and zero-shot
evaluation settings. Furthermore, our model is both faster and requires smaller
indexes, making our dense representation framework an attractive approach to
text retrieval. Our code is available at https://github.com/castorini/dhr.</p>
</td>
    <td>
      
        Few-Shot-&-Zero-Shot 
      
        Evaluation 
      
        Text-Retrieval 
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/li2023style/">The style transformer with common knowledge optimization for image-text retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=The style transformer with common knowledge optimization for image-text retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=The style transformer with common knowledge optimization for image-text retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Wenrui, Ma Zhengyu, Shi Jinqiao, Fan Xiaopeng</td> <!-- 🔧 You were missing this -->
    <td>IEEE Signal Processing Letters</td>
    <td>5</td>
    <td><p>Image-text retrieval which associates different modalities has drawn broad
attention due to its excellent research value and broad real-world application.
However, most of the existing methods haven’t taken the high-level semantic
relationships (“style embedding”) and common knowledge from multi-modalities
into full consideration. To this end, we introduce a novel style transformer
network with common knowledge optimization (CKSTN) for image-text retrieval.
The main module is the common knowledge adaptor (CKA) with both the style
embedding extractor (SEE) and the common knowledge optimization (CKO) modules.
Specifically, the SEE uses the sequential update strategy to effectively
connect the features of different stages in SEE. The CKO module is introduced
to dynamically capture the latent concepts of common knowledge from different
modalities. Besides, to get generalized temporal common knowledge, we propose a
sequential update strategy to effectively integrate the features of different
layers in SEE with previous common feature units. CKSTN demonstrates the
superiorities of the state-of-the-art methods in image-text retrieval on MSCOCO
and Flickr30K datasets. Moreover, CKSTN is constructed based on the lightweight
transformer which is more convenient and practical for the application of real
scenes, due to the better performance and lower parameters.</p>
</td>
    <td>
      
        Datasets 
      
        Text-Retrieval 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/li2023constructing/">Constructing Tree-based Index for Efficient and Effective Dense Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Constructing Tree-based Index for Efficient and Effective Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Constructing Tree-based Index for Efficient and Effective Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Haitao, Ai Qingyao, Zhan Jingtao, Mao Jiaxin, Liu Yiqun, Liu Zheng, Cao Zhao</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>10</td>
    <td><p>Recent studies have shown that Dense Retrieval (DR) techniques can
significantly improve the performance of first-stage retrieval in IR systems.
Despite its empirical effectiveness, the application of DR is still limited. In
contrast to statistic retrieval models that rely on highly efficient inverted
index solutions, DR models build dense embeddings that are difficult to be
pre-processed with most existing search indexing systems. To avoid the
expensive cost of brute-force search, the Approximate Nearest Neighbor (ANN)
algorithm and corresponding indexes are widely applied to speed up the
inference process of DR models. Unfortunately, while ANN can improve the
efficiency of DR models, it usually comes with a significant price on retrieval
performance.
  To solve this issue, we propose JTR, which stands for Joint optimization of
TRee-based index and query encoding. Specifically, we design a new unified
contrastive learning loss to train tree-based index and query encoder in an
end-to-end manner. The tree-based negative sampling strategy is applied to make
the tree have the maximum heap property, which supports the effectiveness of
beam search well. Moreover, we treat the cluster assignment as an optimization
problem to update the tree-based index that allows overlapped clustering. We
evaluate JTR on numerous popular retrieval benchmarks. Experimental results
show that JTR achieves better retrieval performance while retaining high system
efficiency compared with widely-adopted baselines. It provides a potential
solution to balance efficiency and effectiveness in neural retrieval system
designs.</p>
</td>
    <td>
      
        Evaluation 
      
        SIGIR 
      
        Self-Supervised 
      
        Tree-Based-ANN 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/leyvavallina2023data/">Data-efficient Large Scale Place Recognition with Graded Similarity Supervision</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Data-efficient Large Scale Place Recognition with Graded Similarity Supervision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Data-efficient Large Scale Place Recognition with Graded Similarity Supervision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Leyva-vallina Maria, Strisciuglio Nicola, Petkov Nicolai</td> <!-- 🔧 You were missing this -->
    <td>2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>24</td>
    <td><p>Visual place recognition (VPR) is a fundamental task of computer vision for
visual localization. Existing methods are trained using image pairs that either
depict the same place or not. Such a binary indication does not consider
continuous relations of similarity between images of the same place taken from
different positions, determined by the continuous nature of camera pose. The
binary similarity induces a noisy supervision signal into the training of VPR
methods, which stall in local minima and require expensive hard mining
algorithms to guarantee convergence. Motivated by the fact that two images of
the same place only partially share visual cues due to camera pose differences,
we deploy an automatic re-annotation strategy to re-label VPR datasets. We
compute graded similarity labels for image pairs based on available
localization metadata. Furthermore, we propose a new Generalized Contrastive
Loss (GCL) that uses graded similarity labels for training contrastive
networks. We demonstrate that the use of the new labels and GCL allow to
dispense from hard-pair mining, and to train image descriptors that perform
better in VPR by nearest neighbor search, obtaining superior or comparable
results than methods that require expensive hard-pair mining and re-ranking
techniques. Code and models available at:
https://github.com/marialeyvallina/generalized_contrastive_loss</p>
</td>
    <td>
      
        Datasets 
      
        CVPR 
      
        Hybrid-ANN-Methods 
      
        Re-Ranking 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/kobayashi2023sketch/">Sketch-based Medical Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Sketch-based Medical Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Sketch-based Medical Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kobayashi Kazuma, Gu Lin, Hataya Ryuichiro, Mizuno Takaaki, Miyake Mototaka, Watanabe Hirokazu, Takahashi Masamichi, Takamizawa Yasuyuki, Yoshida Yukihiro, Nakamura Satoshi, Kouno Nobuji, Bolatkan Amina, Kurose Yusuke, Harada Tatsuya, Hamamoto Ryuji</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>6</td>
    <td><p>The amount of medical images stored in hospitals is increasing faster than
ever; however, utilizing the accumulated medical images has been limited. This
is because existing content-based medical image retrieval (CBMIR) systems
usually require example images to construct query vectors; nevertheless,
example images cannot always be prepared. Besides, there can be images with
rare characteristics that make it difficult to find similar example images,
which we call isolated samples. Here, we introduce a novel sketch-based medical
image retrieval (SBMIR) system that enables users to find images of interest
without example images. The key idea lies in feature decomposition of medical
images, whereby the entire feature of a medical image can be decomposed into
and reconstructed from normal and abnormal features. By extending this idea,
our SBMIR system provides an easy-to-use two-step graphical user interface:
users first select a template image to specify a normal feature and then draw a
semantic sketch of the disease on the template image to represent an abnormal
feature. Subsequently, it integrates the two kinds of input to construct a
query vector and retrieves reference images with the closest reference vectors.
Using two datasets, ten healthcare professionals with various clinical
backgrounds participated in the user test for evaluation. As a result, our
SBMIR system enabled users to overcome previous challenges, including image
retrieval based on fine-grained image characteristics, image retrieval without
example images, and image retrieval for isolated samples. Our SBMIR system
achieves flexible medical image retrieval on demand, thereby expanding the
utility of medical image databases.</p>
</td>
    <td>
      
        Datasets 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/kim2023exposing/">Exposing and Mitigating Spurious Correlations for Cross-Modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Exposing and Mitigating Spurious Correlations for Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Exposing and Mitigating Spurious Correlations for Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kim Jae Myung, Koepke A. Sophia, Schmid Cordelia, Akata Zeynep</td> <!-- 🔧 You were missing this -->
    <td>2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>15</td>
    <td><p>Cross-modal retrieval methods are the preferred tool to search databases for
the text that best matches a query image and vice versa. However, image-text
retrieval models commonly learn to memorize spurious correlations in the
training data, such as frequent object co-occurrence, instead of looking at the
actual underlying reasons for the prediction in the image. For image-text
retrieval, this manifests in retrieved sentences that mention objects that are
not present in the query image. In this work, we introduce ODmAP@k, an object
decorrelation metric that measures a model’s robustness to spurious
correlations in the training data. We use automatic image and text
manipulations to control the presence of such object correlations in designated
test data. Additionally, our data synthesis technique is used to tackle model
biases due to spurious correlations of semantically unrelated objects in the
training data. We apply our proposed pipeline, which involves the finetuning of
image-text retrieval frameworks on carefully designed synthetic data, to three
state-of-the-art models for image-text retrieval. This results in significant
improvements for all three models, both in terms of the standard retrieval
performance and in terms of our object decorrelation metric. The code is
available at https://github.com/ExplainableML/Spurious_CM_Retrieval.</p>
</td>
    <td>
      
        Multimodal-Retrieval 
      
        Text-Retrieval 
      
        CVPR 
      
        Evaluation 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/kim2022improving/">Improving Cross-Modal Retrieval with Set of Diverse Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Improving Cross-Modal Retrieval with Set of Diverse Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Improving Cross-Modal Retrieval with Set of Diverse Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kim Dongwon, Kim Namyup, Kwak Suha</td> <!-- 🔧 You were missing this -->
    <td>2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>28</td>
    <td><p>Cross-modal retrieval across image and text modalities is a challenging task
due to its inherent ambiguity: An image often exhibits various situations, and
a caption can be coupled with diverse images. Set-based embedding has been
studied as a solution to this problem. It seeks to encode a sample into a set
of different embedding vectors that capture different semantics of the sample.
In this paper, we present a novel set-based embedding method, which is distinct
from previous work in two aspects. First, we present a new similarity function
called smooth-Chamfer similarity, which is designed to alleviate the side
effects of existing similarity functions for set-based embedding. Second, we
propose a novel set prediction module to produce a set of embedding vectors
that effectively captures diverse semantics of input by the slot attention
mechanism. Our method is evaluated on the COCO and Flickr30K datasets across
different visual backbones, where it outperforms existing methods including
ones that demand substantially larger computation at inference.</p>
</td>
    <td>
      
        Datasets 
      
        CVPR 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/kulkarni2023lexically/">Lexically-Accelerated Dense Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Lexically-Accelerated Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Lexically-Accelerated Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kulkarni Hrishikesh, Macavaney Sean, Goharian Nazli, Frieder Ophir</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>19</td>
    <td><p>Retrieval approaches that score documents based on learned dense vectors
(i.e., dense retrieval) rather than lexical signals (i.e., conventional
retrieval) are increasingly popular. Their ability to identify related
documents that do not necessarily contain the same terms as those appearing in
the user’s query (thereby improving recall) is one of their key advantages.
However, to actually achieve these gains, dense retrieval approaches typically
require an exhaustive search over the document collection, making them
considerably more expensive at query-time than conventional lexical approaches.
Several techniques aim to reduce this computational overhead by approximating
the results of a full dense retriever. Although these approaches reasonably
approximate the top results, they suffer in terms of recall – one of the key
advantages of dense retrieval. We introduce ‘LADR’ (Lexically-Accelerated Dense
Retrieval), a simple-yet-effective approach that improves the efficiency of
existing dense retrieval models without compromising on retrieval
effectiveness. LADR uses lexical retrieval techniques to seed a dense retrieval
exploration that uses a document proximity graph. We explore two variants of
LADR: a proactive approach that expands the search space to the neighbors of
all seed documents, and an adaptive approach that selectively searches the
documents with the highest estimated relevance in an iterative fashion. Through
extensive experiments across a variety of dense retrieval models, we find that
LADR establishes a new dense retrieval effectiveness-efficiency Pareto frontier
among approximate k nearest neighbor techniques. Further, we find that when
tuned to take around 8ms per query in retrieval latency on our hardware, LADR
consistently achieves both precision and recall that are on par with an
exhaustive search on standard benchmarks.</p>
</td>
    <td>
      
        Graph-Based-ANN 
      
        SIGIR 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/hemati2022learning/">Learning Binary and Sparse Permutation-Invariant Representations for Fast and Memory Efficient Whole Slide Image Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Binary and Sparse Permutation-Invariant Representations for Fast and Memory Efficient Whole Slide Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Binary and Sparse Permutation-Invariant Representations for Fast and Memory Efficient Whole Slide Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hemati Sobhan, Kalra Shivam, Babaie Morteza, Tizhoosh H. R.</td> <!-- 🔧 You were missing this -->
    <td>Computers in Biology and Medicine</td>
    <td>9</td>
    <td><p>Learning suitable Whole slide images (WSIs) representations for efficient
retrieval systems is a non-trivial task. The WSI embeddings obtained from
current methods are in Euclidean space not ideal for efficient WSI retrieval.
Furthermore, most of the current methods require high GPU memory due to the
simultaneous processing of multiple sets of patches. To address these
challenges, we propose a novel framework for learning binary and sparse WSI
representations utilizing a deep generative modelling and the Fisher Vector. We
introduce new loss functions for learning sparse and binary
permutation-invariant WSI representations that employ instance-based training
achieving better memory efficiency. The learned WSI representations are
validated on The Cancer Genomic Atlas (TCGA) and Liver-Kidney-Stomach (LKS)
datasets. The proposed method outperforms Yottixel (a recent search engine for
histopathology images) both in terms of retrieval accuracy and speed. Further,
we achieve competitive performance against SOTA on the public benchmark LKS
dataset for WSI classification.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
        Datasets 
      
        Memory-Efficiency 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/hashimoto2021case/">Case-based Similar Image Retrieval for Weakly Annotated Large Histopathological Images of Malignant Lymphoma Using Deep Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Case-based Similar Image Retrieval for Weakly Annotated Large Histopathological Images of Malignant Lymphoma Using Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Case-based Similar Image Retrieval for Weakly Annotated Large Histopathological Images of Malignant Lymphoma Using Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hashimoto Noriaki, Takagi Yusuke, Masuda Hiroki, Miyoshi Hiroaki, Kohno Kei, Nagaishi Miharu, Sato Kensaku, Takeuchi Mai, Furuta Takuya, Kawamoto Keisuke, Yamada Kyohei, Moritsubo Mayuko, Inoue Kanako, Shimasaki Yasumasa, Ogura Yusuke, Imamoto Teppei, Mishina Tatsuzo, Tanaka Ken, Kawaguchi Yoshino, Nakamura Shigeo, Ohshima Koichi, Hontani Hidekata, Takeuchi Ichiro</td> <!-- 🔧 You were missing this -->
    <td>Medical Image Analysis</td>
    <td>20</td>
    <td><p>In the present study, we propose a novel case-based similar image retrieval
(SIR) method for hematoxylin and eosin (H&amp;E)-stained histopathological images
of malignant lymphoma. When a whole slide image (WSI) is used as an input
query, it is desirable to be able to retrieve similar cases by focusing on
image patches in pathologically important regions such as tumor cells. To
address this problem, we employ attention-based multiple instance learning,
which enables us to focus on tumor-specific regions when the similarity between
cases is computed. Moreover, we employ contrastive distance metric learning to
incorporate immunohistochemical (IHC) staining patterns as useful supervised
information for defining appropriate similarity between heterogeneous malignant
lymphoma cases. In the experiment with 249 malignant lymphoma patients, we
confirmed that the proposed method exhibited higher evaluation measures than
the baseline case-based SIR methods. Furthermore, the subjective evaluation by
pathologists revealed that our similarity measure using IHC staining patterns
is appropriate for representing the similarity of H&amp;E-stained tissue images for
malignant lymphoma.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Supervised 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/gupta2022medical/">Medical Image Retrieval via Nearest Neighbor Search on Pre-trained Image Features</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Medical Image Retrieval via Nearest Neighbor Search on Pre-trained Image Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Medical Image Retrieval via Nearest Neighbor Search on Pre-trained Image Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gupta Deepak, Loane Russell, Gayen Soumya, Demner-fushman Dina</td> <!-- 🔧 You were missing this -->
    <td>Knowledge-Based Systems</td>
    <td>7</td>
    <td><p>Nearest neighbor search (NNS) aims to locate the points in high-dimensional
space that is closest to the query point. The brute-force approach for finding
the nearest neighbor becomes computationally infeasible when the number of
points is large. The NNS has multiple applications in medicine, such as
searching large medical imaging databases, disease classification, diagnosis,
etc. With a focus on medical imaging, this paper proposes DenseLinkSearch an
effective and efficient algorithm that searches and retrieves the relevant
images from heterogeneous sources of medical images. Towards this, given a
medical database, the proposed algorithm builds the index that consists of
pre-computed links of each point in the database. The search algorithm utilizes
the index to efficiently traverse the database in search of the nearest
neighbor. We extensively tested the proposed NNS approach and compared the
performance with state-of-the-art NNS approaches on benchmark datasets and our
created medical image datasets. The proposed approach outperformed the existing
approach in terms of retrieving accurate neighbors and retrieval speed. We also
explore the role of medical image feature representation in content-based
medical image retrieval tasks. We propose a Transformer-based feature
representation technique that outperformed the existing pre-trained Transformer
approach on CLEF 2011 medical image retrieval task. The source code of our
experiments are available at https://github.com/deepaknlp/DLS.</p>
</td>
    <td>
      
        Datasets 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/ge2022cross/">Cross-modal Semantic Enhanced Interaction for Image-Sentence Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-modal Semantic Enhanced Interaction for Image-Sentence Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-modal Semantic Enhanced Interaction for Image-Sentence Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ge Xuri, Chen Fuhai, Xu Songpei, Tao Fuxiang, Jose Joemon M.</td> <!-- 🔧 You were missing this -->
    <td>2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>29</td>
    <td><p>Image-sentence retrieval has attracted extensive research attention in
multimedia and computer vision due to its promising application. The key issue
lies in jointly learning the visual and textual representation to accurately
estimate their similarity. To this end, the mainstream schema adopts an
object-word based attention to calculate their relevance scores and refine
their interactive representations with the attention features, which, however,
neglects the context of the object representation on the inter-object
relationship that matches the predicates in sentences. In this paper, we
propose a Cross-modal Semantic Enhanced Interaction method, termed CMSEI for
image-sentence retrieval, which correlates the intra- and inter-modal semantics
between objects and words. In particular, we first design the intra-modal
spatial and semantic graphs based reasoning to enhance the semantic
representations of objects guided by the explicit relationships of the objects’
spatial positions and their scene graph. Then the visual and textual semantic
representations are refined jointly via the inter-modal interactive attention
and the cross-modal alignment. To correlate the context of objects with the
textual context, we further refine the visual semantic representation via the
cross-level object-sentence and word-image based interactive attention.
Experimental results on seven standard evaluation metrics show that the
proposed CMSEI outperforms the state-of-the-art and the alternative approaches
on MS-COCO and Flickr30K benchmarks.</p>
</td>
    <td>
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/gao2022precise/">Precise Zero-Shot Dense Retrieval without Relevance Labels</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Precise Zero-Shot Dense Retrieval without Relevance Labels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Precise Zero-Shot Dense Retrieval without Relevance Labels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gao Luyu, Ma Xueguang, Lin Jimmy, Callan Jamie</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</td>
    <td>81</td>
    <td><p>While dense retrieval has been shown effective and efficient across tasks and
languages, it remains difficult to create effective fully zero-shot dense
retrieval systems when no relevance label is available. In this paper, we
recognize the difficulty of zero-shot learning and encoding relevance. Instead,
we propose to pivot through Hypothetical Document Embeddings~(HyDE). Given a
query, HyDE first zero-shot instructs an instruction-following language model
(e.g. InstructGPT) to generate a hypothetical document. The document captures
relevance patterns but is unreal and may contain false details. Then, an
unsupervised contrastively learned encoder~(e.g. Contriever) encodes the
document into an embedding vector. This vector identifies a neighborhood in the
corpus embedding space, where similar real documents are retrieved based on
vector similarity. This second step ground the generated document to the actual
corpus, with the encoder’s dense bottleneck filtering out the incorrect
details. Our experiments show that HyDE significantly outperforms the
state-of-the-art unsupervised dense retriever Contriever and shows strong
performance comparable to fine-tuned retrievers, across various tasks (e.g. web
search, QA, fact verification) and languages~(e.g. sw, ko, ja).</p>
</td>
    <td>
      
        Unsupervised 
      
        Few-Shot-&-Zero-Shot 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/gao2022long/">Long-tail Cross Modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Long-tail Cross Modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Long-tail Cross Modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gao Zijun, Wang Jun, Yu Guoxian, Yan Zhongmin, Domeniconi Carlotta, Zhang Jinglin</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>5</td>
    <td><p>Existing Cross Modal Hashing (CMH) methods are mainly designed for balanced
data, while imbalanced data with long-tail distribution is more general in
real-world. Several long-tail hashing methods have been proposed but they can
not adapt for multi-modal data, due to the complex interplay between labels and
individuality and commonality information of multi-modal data. Furthermore, CMH
methods mostly mine the commonality of multi-modal data to learn hash codes,
which may override tail labels encoded by the individuality of respective
modalities. In this paper, we propose LtCMH (Long-tail CMH) to handle
imbalanced multi-modal data. LtCMH firstly adopts auto-encoders to mine the
individuality and commonality of different modalities by minimizing the
dependency between the individuality of respective modalities and by enhancing
the commonality of these modalities. Then it dynamically combines the
individuality and commonality with direct features extracted from respective
modalities to create meta features that enrich the representation of tail
labels, and binaries meta features to generate hash codes. LtCMH significantly
outperforms state-of-the-art baselines on long-tail datasets and holds a better
(or comparable) performance on datasets with balanced labels.</p>
</td>
    <td>
      
        AAAI 
      
        Datasets 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/galanopoulos2023are/">Are All Combinations Equal? Combining Textual and Visual Features with Multiple Space Learning for Text-Based Video Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Are All Combinations Equal? Combining Textual and Visual Features with Multiple Space Learning for Text-Based Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Are All Combinations Equal? Combining Textual and Visual Features with Multiple Space Learning for Text-Based Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Galanopoulos Damianos, Mezaris Vasileios</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>11</td>
    <td><p>In this paper we tackle the cross-modal video retrieval problem and, more
specifically, we focus on text-to-video retrieval. We investigate how to
optimally combine multiple diverse textual and visual features into feature
pairs that lead to generating multiple joint feature spaces, which encode
text-video pairs into comparable representations. To learn these
representations our proposed network architecture is trained by following a
multiple space learning procedure. Moreover, at the retrieval stage, we
introduce additional softmax operations for revising the inferred query-video
similarities. Extensive experiments in several setups based on three
large-scale datasets (IACC.3, V3C1, and MSR-VTT) lead to conclusions on how to
best combine text-visual features and document the performance of the proposed
network. Source code is made publicly available at:
https://github.com/bmezaris/TextToVideoRetrieval-TtimesV</p>
</td>
    <td>
      
        Datasets 
      
        Scalability 
      
        Video-Retrieval 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/galanopoulos2022are/">Are All Combinations Equal? Combining Textual and Visual Features with Multiple Space Learning for Text-Based Video Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Are All Combinations Equal? Combining Textual and Visual Features with Multiple Space Learning for Text-Based Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Are All Combinations Equal? Combining Textual and Visual Features with Multiple Space Learning for Text-Based Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Galanopoulos Damianos, Mezaris Vasileios</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>11</td>
    <td><p>In this paper we tackle the cross-modal video retrieval problem and, more
specifically, we focus on text-to-video retrieval. We investigate how to
optimally combine multiple diverse textual and visual features into feature
pairs that lead to generating multiple joint feature spaces, which encode
text-video pairs into comparable representations. To learn these
representations our proposed network architecture is trained by following a
multiple space learning procedure. Moreover, at the retrieval stage, we
introduce additional softmax operations for revising the inferred query-video
similarities. Extensive experiments in several setups based on three
large-scale datasets (IACC.3, V3C1, and MSR-VTT) lead to conclusions on how to
best combine text-visual features and document the performance of the proposed
network. Source code is made publicly available at:
https://github.com/bmezaris/TextToVideoRetrieval-TtimesV</p>
</td>
    <td>
      
        Datasets 
      
        Scalability 
      
        Video-Retrieval 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/gan2023binary/">Binary Embedding-based Retrieval at Tencent</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Binary Embedding-based Retrieval at Tencent' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Binary Embedding-based Retrieval at Tencent' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gan Yukang, Ge Yixiao, Zhou Chang, Su Shupeng, Xu Zhouchuan, Xu Xuyuan, Hui Quanchao, Chen Xiang, Wang Yexin, Shan Ying</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</td>
    <td>7</td>
    <td><p>Large-scale embedding-based retrieval (EBR) is the cornerstone of
search-related industrial applications. Given a user query, the system of EBR
aims to identify relevant information from a large corpus of documents that may
be tens or hundreds of billions in size. The storage and computation turn out
to be expensive and inefficient with massive documents and high concurrent
queries, making it difficult to further scale up. To tackle the challenge, we
propose a binary embedding-based retrieval (BEBR) engine equipped with a
recurrent binarization algorithm that enables customized bits per dimension.
Specifically, we compress the full-precision query and document embeddings,
formulated as float vectors in general, into a composition of multiple binary
vectors using a lightweight transformation model with residual multilayer
perception (MLP) blocks. We can therefore tailor the number of bits for
different applications to trade off accuracy loss and cost savings.
Importantly, we enable task-agnostic efficient training of the binarization
model using a new embedding-to-embedding strategy. We also exploit the
compatible training of binary embeddings so that the BEBR engine can support
indexing among multiple embedding versions within a unified system. To further
realize efficient search, we propose Symmetric Distance Calculation (SDC) to
achieve lower response time than Hamming codes. We successfully employed the
introduced BEBR to Tencent products, including Sogou, Tencent Video, QQ World,
etc. The binarization algorithm can be seamlessly generalized to various tasks
with multiple modalities. Extensive experiments on offline benchmarks and
online A/B tests demonstrate the efficiency and effectiveness of our method,
significantly saving 30%~50% index costs with almost no loss of accuracy at the
system level.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        KDD 
      
        Scalability 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/chen2023bipartite/">Bipartite Graph Convolutional Hashing for Effective and Efficient Top-N Search in Hamming Space</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Bipartite Graph Convolutional Hashing for Effective and Efficient Top-N Search in Hamming Space' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Bipartite Graph Convolutional Hashing for Effective and Efficient Top-N Search in Hamming Space' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen Yankai, Fang Yixiang, Zhang Yifei, King Irwin</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the ACM Web Conference 2023</td>
    <td>15</td>
    <td><p>Searching on bipartite graphs is basal and versatile to many real-world Web
applications, e.g., online recommendation, database retrieval, and
query-document searching. Given a query node, the conventional approaches rely
on the similarity matching with the vectorized node embeddings in the
continuous Euclidean space. To efficiently manage intensive similarity
computation, developing hashing techniques for graph structured data has
recently become an emerging research direction. Despite the retrieval
efficiency in Hamming space, prior work is however confronted with catastrophic
performance decay. In this work, we investigate the problem of hashing with
Graph Convolutional Network on bipartite graphs for effective Top-N search. We
propose an end-to-end Bipartite Graph Convolutional Hashing approach, namely
BGCH, which consists of three novel and effective modules: (1) adaptive graph
convolutional hashing, (2) latent feature dispersion, and (3) Fourier
serialized gradient estimation. Specifically, the former two modules achieve
the substantial retention of the structural information against the inevitable
information loss in hash encoding; the last module develops Fourier Series
decomposition to the hashing function in the frequency domain mainly for more
accurate gradient estimation. The extensive experiments on six real-world
datasets not only show the performance superiority over the competing
hashing-based counterparts, but also demonstrate the effectiveness of all
proposed model components contained therein.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        Recommender-Systems 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/bin2023unifying/">Unifying Two-Stream Encoders with Transformers for Cross-Modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unifying Two-Stream Encoders with Transformers for Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unifying Two-Stream Encoders with Transformers for Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Bin Yi, Li Haoxuan, Xu Yahui, Xu Xing, Yang Yang, Shen Heng Tao</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 31st ACM International Conference on Multimedia</td>
    <td>17</td>
    <td><p>Most existing cross-modal retrieval methods employ two-stream encoders with
different architectures for images and texts, \textit{e.g.}, CNN for images and
RNN/Transformer for texts. Such discrepancy in architectures may induce
different semantic distribution spaces and limit the interactions between
images and texts, and further result in inferior alignment between images and
texts. To fill this research gap, inspired by recent advances of Transformers
in vision tasks, we propose to unify the encoder architectures with
Transformers for both modalities. Specifically, we design a cross-modal
retrieval framework purely based on two-stream Transformers, dubbed
\textbf{Hierarchical Alignment Transformers (HAT)}, which consists of an image
Transformer, a text Transformer, and a hierarchical alignment module. With such
identical architectures, the encoders could produce representations with more
similar characteristics for images and texts, and make the interactions and
alignments between them much easier. Besides, to leverage the rich semantics,
we devise a hierarchical alignment scheme to explore multi-level
correspondences of different layers between images and texts. To evaluate the
effectiveness of the proposed HAT, we conduct extensive experiments on two
benchmark datasets, MSCOCO and Flickr30K. Experimental results demonstrate that
HAT outperforms SOTA baselines by a large margin. Specifically, on two key
tasks, \textit{i.e.}, image-to-text and text-to-image retrieval, HAT achieves
7.6% and 16.7% relative score improvement of Recall@1 on MSCOCO, and 4.4%
and 11.6% on Flickr30k respectively. The code is available at
https://github.com/LuminosityX/HAT.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
        Datasets 
      
        Evaluation 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/bruch2023approximate/">An Approximate Algorithm for Maximum Inner Product Search over Streaming Sparse Vectors</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=An Approximate Algorithm for Maximum Inner Product Search over Streaming Sparse Vectors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=An Approximate Algorithm for Maximum Inner Product Search over Streaming Sparse Vectors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Bruch Sebastian, Nardini Franco Maria, Ingber Amir, Liberty Edo</td> <!-- 🔧 You were missing this -->
    <td>ACM Transactions on Information Systems</td>
    <td>6</td>
    <td><p>Maximum Inner Product Search or top-k retrieval on sparse vectors is
well-understood in information retrieval, with a number of mature algorithms
that solve it exactly. However, all existing algorithms are tailored to text
and frequency-based similarity measures. To achieve optimal memory footprint
and query latency, they rely on the near stationarity of documents and on laws
governing natural languages. We consider, instead, a setup in which collections
are streaming – necessitating dynamic indexing – and where indexing and
retrieval must work with arbitrarily distributed real-valued vectors. As we
show, existing algorithms are no longer competitive in this setup, even against
naive solutions. We investigate this gap and present a novel approximate
solution, called Sinnamon, that can efficiently retrieve the top-k results for
sparse real valued vectors drawn from arbitrary distributions. Notably,
Sinnamon offers levers to trade-off memory consumption, latency, and accuracy,
making the algorithm suitable for constrained applications and systems. We give
theoretical results on the error introduced by the approximate nature of the
algorithm, and present an empirical evaluation of its performance on two
hardware platforms and synthetic and real-valued datasets. We conclude by
laying out concrete directions for future research on this general top-k
retrieval problem over sparse vectors.</p>
</td>
    <td>
      
        Memory-Efficiency 
      
        Datasets 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/baldrati2023composed/">Composed Image Retrieval using Contrastive Learning and Task-oriented CLIP-based Features</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Composed Image Retrieval using Contrastive Learning and Task-oriented CLIP-based Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Composed Image Retrieval using Contrastive Learning and Task-oriented CLIP-based Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Baldrati Alberto, Bertini Marco, Uricchio Tiberio, del Bimbo Alberto</td> <!-- 🔧 You were missing this -->
    <td>ACM Transactions on Multimedia Computing, Communications, and Applications</td>
    <td>13</td>
    <td><p>Given a query composed of a reference image and a relative caption, the
Composed Image Retrieval goal is to retrieve images visually similar to the
reference one that integrates the modifications expressed by the caption. Given
that recent research has demonstrated the efficacy of large-scale vision and
language pre-trained (VLP) models in various tasks, we rely on features from
the OpenAI CLIP model to tackle the considered task. We initially perform a
task-oriented fine-tuning of both CLIP encoders using the element-wise sum of
visual and textual features. Then, in the second stage, we train a Combiner
network that learns to combine the image-text features integrating the bimodal
information and providing combined features used to perform the retrieval. We
use contrastive learning in both stages of training. Starting from the bare
CLIP features as a baseline, experimental results show that the task-oriented
fine-tuning and the carefully crafted Combiner network are highly effective and
outperform more complex state-of-the-art approaches on FashionIQ and CIRR, two
popular and challenging datasets for composed image retrieval. Code and
pre-trained models are available at https://github.com/ABaldrati/CLIP4Cir</p>
</td>
    <td>
      
        Self-Supervised 
      
        Scalability 
      
        Datasets 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/aguerrebere2023similarity/">Similarity search in the blink of an eye with compressed indices</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Similarity search in the blink of an eye with compressed indices' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Similarity search in the blink of an eye with compressed indices' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Aguerrebere Cecilia, Bhati Ishwar, Hildebrand Mark, Tepper Mariano, Willke Ted</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the VLDB Endowment</td>
    <td>15</td>
    <td><p>Nowadays, data is represented by vectors. Retrieving those vectors, among
millions and billions, that are similar to a given query is a ubiquitous
problem, known as similarity search, of relevance for a wide range of
applications. Graph-based indices are currently the best performing techniques
for billion-scale similarity search. However, their random-access memory
pattern presents challenges to realize their full potential. In this work, we
present new techniques and systems for creating faster and smaller graph-based
indices. To this end, we introduce a novel vector compression method,
Locally-adaptive Vector Quantization (LVQ), that uses per-vector scaling and
scalar quantization to improve search performance with fast similarity
computations and a reduced effective bandwidth, while decreasing memory
footprint and barely impacting accuracy. LVQ, when combined with a new
high-performance computing system for graph-based similarity search,
establishes the new state of the art in terms of performance and memory
footprint. For billions of vectors, LVQ outcompetes the second-best
alternatives: (1) in the low-memory regime, by up to 20.7x in throughput with
up to a 3x memory footprint reduction, and (2) in the high-throughput regime by
5.8x with 1.4x less memory.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Graph-Based-ANN 
      
        Quantization 
      
        Memory-Efficiency 
      
        Scalability 
      
        Large-Scale-Search 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/yuan2023semantic/">Semantic-Aware Adversarial Training for Reliable Deep Hashing Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semantic-Aware Adversarial Training for Reliable Deep Hashing Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semantic-Aware Adversarial Training for Reliable Deep Hashing Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yuan Xu, Zhang Zheng, Wang Xunguang, Wu Lin</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Information Forensics and Security</td>
    <td>13</td>
    <td><p>Deep hashing has been intensively studied and successfully applied in
large-scale image retrieval systems due to its efficiency and effectiveness.
Recent studies have recognized that the existence of adversarial examples poses
a security threat to deep hashing models, that is, adversarial vulnerability.
Notably, it is challenging to efficiently distill reliable semantic
representatives for deep hashing to guide adversarial learning, and thereby it
hinders the enhancement of adversarial robustness of deep hashing-based
retrieval models. Moreover, current researches on adversarial training for deep
hashing are hard to be formalized into a unified minimax structure. In this
paper, we explore Semantic-Aware Adversarial Training (SAAT) for improving the
adversarial robustness of deep hashing models. Specifically, we conceive a
discriminative mainstay features learning (DMFL) scheme to construct semantic
representatives for guiding adversarial learning in deep hashing. Particularly,
our DMFL with the strict theoretical guarantee is adaptively optimized in a
discriminative learning manner, where both discriminative and semantic
properties are jointly considered. Moreover, adversarial examples are
fabricated by maximizing the Hamming distance between the hash codes of
adversarial samples and mainstay features, the efficacy of which is validated
in the adversarial attack trials. Further, we, for the first time, formulate
the formalized adversarial training of deep hashing into a unified minimax
optimization under the guidance of the generated mainstay codes. Extensive
experiments on benchmark datasets show superb attack performance against the
state-of-the-art algorithms, meanwhile, the proposed adversarial training can
effectively eliminate adversarial perturbations for trustworthy deep
hashing-based retrieval. Our code is available at
https://github.com/xandery-geek/SAAT.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Efficiency 
      
        Scalability 
      
        Evaluation 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/yu2022live/">Live Laparoscopic Video Retrieval with Compressed Uncertainty</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Live Laparoscopic Video Retrieval with Compressed Uncertainty' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Live Laparoscopic Video Retrieval with Compressed Uncertainty' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yu Tong, Mascagni Pietro, Verde Juan, Marescaux Jacques, Mutter Didier, Padoy Nicolas</td> <!-- 🔧 You were missing this -->
    <td>Medical Image Analysis</td>
    <td>5</td>
    <td><p>Searching through large volumes of medical data to retrieve relevant
information is a challenging yet crucial task for clinical care. However the
primitive and most common approach to retrieval, involving text in the form of
keywords, is severely limited when dealing with complex media formats.
Content-based retrieval offers a way to overcome this limitation, by using rich
media as the query itself. Surgical video-to-video retrieval in particular is a
new and largely unexplored research problem with high clinical value,
especially in the real-time case: using real-time video hashing, search can be
achieved directly inside of the operating room. Indeed, the process of hashing
converts large data entries into compact binary arrays or hashes, enabling
large-scale search operations at a very fast rate. However, due to fluctuations
over the course of a video, not all bits in a given hash are equally reliable.
In this work, we propose a method capable of mitigating this uncertainty while
maintaining a light computational footprint. We present superior retrieval
results (3-4 % top 10 mean average precision) on a multi-task evaluation
protocol for surgery, using cholecystectomy phases, bypass phases, and coming
from an entirely new dataset introduced here, critical events across six
different surgery types. Success on this multi-task benchmark shows the
generalizability of our approach for surgical video retrieval.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        Video-Retrieval 
      
        Scalability 
      
        Large-Scale-Search 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/yang2022transformer/">Transformer-based Cross-Modal Recipe Embeddings with Large Batch Training</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Transformer-based Cross-Modal Recipe Embeddings with Large Batch Training' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Transformer-based Cross-Modal Recipe Embeddings with Large Batch Training' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yang Jing, Chen Junwen, Yanai Keiji</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>6</td>
    <td><p>In this paper, we present a cross-modal recipe retrieval framework,
Transformer-based Network for Large Batch Training (TNLBT), which is inspired
by ACME~(Adversarial Cross-Modal Embedding) and H-T~(Hierarchical Transformer).
TNLBT aims to accomplish retrieval tasks while generating images from recipe
embeddings. We apply the Hierarchical Transformer-based recipe text encoder,
the Vision Transformer~(ViT)-based recipe image encoder, and an adversarial
network architecture to enable better cross-modal embedding learning for recipe
texts and images. In addition, we use self-supervised learning to exploit the
rich information in the recipe texts having no corresponding images. Since
contrastive learning could benefit from a larger batch size according to the
recent literature on self-supervised learning, we adopt a large batch size
during training and have validated its effectiveness. In the experiments, the
proposed framework significantly outperformed the current state-of-the-art
frameworks in both cross-modal recipe retrieval and image generation tasks on
the benchmark Recipe1M. This is the first work which confirmed the
effectiveness of large batch training on cross-modal recipe embeddings.</p>
</td>
    <td>
      
        Supervised 
      
        Tools-&-Libraries 
      
        Self-Supervised 
      
        Evaluation 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/wei2023attribute/">Attribute-Aware Deep Hashing with Self-Consistency for Large-Scale Fine-Grained Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Attribute-Aware Deep Hashing with Self-Consistency for Large-Scale Fine-Grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Attribute-Aware Deep Hashing with Self-Consistency for Large-Scale Fine-Grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wei Xiu-shen, Shen Yang, Sun Xuhao, Wang Peng, Peng Yuxin</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>16</td>
    <td><p>Our work focuses on tackling large-scale fine-grained image retrieval as
ranking the images depicting the concept of interests (i.e., the same
sub-category labels) highest based on the fine-grained details in the query. It
is desirable to alleviate the challenges of both fine-grained nature of small
inter-class variations with large intra-class variations and explosive growth
of fine-grained data for such a practical task. In this paper, we propose
attribute-aware hashing networks with self-consistency for generating
attribute-aware hash codes to not only make the retrieval process efficient,
but also establish explicit correspondences between hash codes and visual
attributes. Specifically, based on the captured visual representations by
attention, we develop an encoder-decoder structure network of a reconstruction
task to unsupervisedly distill high-level attribute-specific vectors from the
appearance-specific visual representations without attribute annotations. Our
models are also equipped with a feature decorrelation constraint upon these
attribute vectors to strengthen their representative abilities. Then, driven by
preserving original entities’ similarity, the required hash codes can be
generated from these attribute-specific vectors and thus become
attribute-aware. Furthermore, to combat simplicity bias in deep hashing, we
consider the model design from the perspective of the self-consistency
principle and propose to further enhance models’ self-consistency by equipping
an additional image reconstruction path. Comprehensive quantitative experiments
under diverse empirical settings on six fine-grained retrieval datasets and two
generic retrieval datasets show the superiority of our models over competing
methods.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Scalability 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/wang2023unified/">Unified Coarse-to-Fine Alignment for Video-Text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unified Coarse-to-Fine Alignment for Video-Text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unified Coarse-to-Fine Alignment for Video-Text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Ziyang, Sung Yi-lin, Cheng Feng, Bertasius Gedas, Bansal Mohit</td> <!-- 🔧 You were missing this -->
    <td>2023 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>20</td>
    <td><p>The canonical approach to video-text retrieval leverages a coarse-grained or
fine-grained alignment between visual and textual information. However,
retrieving the correct video according to the text query is often challenging
as it requires the ability to reason about both high-level (scene) and
low-level (object) visual clues and how they relate to the text query. To this
end, we propose a Unified Coarse-to-fine Alignment model, dubbed UCoFiA.
Specifically, our model captures the cross-modal similarity information at
different granularity levels. To alleviate the effect of irrelevant visual
clues, we also apply an Interactive Similarity Aggregation module (ISA) to
consider the importance of different visual features while aggregating the
cross-modal similarity to obtain a similarity score for each granularity.
Finally, we apply the Sinkhorn-Knopp algorithm to normalize the similarities of
each level before summing them, alleviating over- and under-representation
issues at different levels. By jointly considering the crossmodal similarity of
different granularity, UCoFiA allows the effective unification of multi-grained
alignments. Empirically, UCoFiA outperforms previous state-of-the-art
CLIP-based methods on multiple video-text retrieval benchmarks, achieving 2.4%,
1.4% and 1.3% improvements in text-to-video retrieval R@1 on MSR-VTT,
Activity-Net, and DiDeMo, respectively. Our code is publicly available at
https://github.com/Ziyang412/UCoFiA.</p>
</td>
    <td>
      
        ICCV 
      
        Video-Retrieval 
      
        Text-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/wang2023correspondence/">Correspondence-Free Domain Alignment for Unsupervised Cross-Domain Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Correspondence-Free Domain Alignment for Unsupervised Cross-Domain Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Correspondence-Free Domain Alignment for Unsupervised Cross-Domain Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Xu, Peng Dezhong, Yan Ming, Hu Peng</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>7</td>
    <td><p>Cross-domain image retrieval aims at retrieving images across different
domains to excavate cross-domain classificatory or correspondence
relationships. This paper studies a less-touched problem of cross-domain image
retrieval, i.e., unsupervised cross-domain image retrieval, considering the
following practical assumptions: (i) no correspondence relationship, and (ii)
no category annotations. It is challenging to align and bridge distinct domains
without cross-domain correspondence. To tackle the challenge, we present a
novel Correspondence-free Domain Alignment (CoDA) method to effectively
eliminate the cross-domain gap through In-domain Self-matching Supervision
(ISS) and Cross-domain Classifier Alignment (CCA). To be specific, ISS is
presented to encapsulate discriminative information into the latent common
space by elaborating a novel self-matching supervision mechanism. To alleviate
the cross-domain discrepancy, CCA is proposed to align distinct domain-specific
classifiers. Thanks to the ISS and CCA, our method could encode the
discrimination into the domain-invariant embedding space for unsupervised
cross-domain image retrieval. To verify the effectiveness of the proposed
method, extensive experiments are conducted on four benchmark datasets compared
with six state-of-the-art methods.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Datasets 
      
        AAAI 
      
        Unsupervised 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2023</td>
    <td>
      <a href="/publications/zhang2021orthonormal/">Orthonormal Product Quantization Network for Scalable Face Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Orthonormal Product Quantization Network for Scalable Face Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Orthonormal Product Quantization Network for Scalable Face Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Ming, Zhe Xuefei, Yan Hong</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition</td>
    <td>9</td>
    <td><p>Existing deep quantization methods provided an efficient solution for
large-scale image retrieval. However, the significant intra-class variations
like pose, illumination, and expressions in face images, still pose a challenge
for face image retrieval. In light of this, face image retrieval requires
sufficiently powerful learning metrics, which are absent in current deep
quantization works. Moreover, to tackle the growing unseen identities in the
query stage, face image retrieval drives more demands regarding model
generalization and system scalability than general image retrieval tasks. This
paper integrates product quantization with orthonormal constraints into an
end-to-end deep learning framework to effectively retrieve face images.
Specifically, a novel scheme that uses predefined orthonormal vectors as
codewords is proposed to enhance the quantization informativeness and reduce
codewords’ redundancy. A tailored loss function maximizes discriminability
among identities in each quantization subspace for both the quantized and
original features. An entropy-based regularization term is imposed to reduce
the quantization error. Experiments are conducted on four commonly-used face
datasets under both seen and unseen identities retrieval settings. Our method
outperforms all the compared deep hashing/quantization state-of-the-arts under
both settings. Results validate the effectiveness of the proposed orthonormal
codewords in improving models’ standard retrieval performance and
generalization ability. Combing with further experiments on two general image
datasets, it demonstrates the broad superiority of our method for scalable
image retrieval.</p>
</td>
    <td>
      
        Scalability 
      
        Quantization 
      
        Evaluation 
      
        Datasets 
      
        Hashing-Methods 
      
        Tools-&-Libraries 
      
        Neural-Hashing 
      
        Image-Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/tian2022learned/">A Learned Index for Exact Similarity Search in Metric Spaces</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Learned Index for Exact Similarity Search in Metric Spaces' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Learned Index for Exact Similarity Search in Metric Spaces' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tian Yao, Yan Tingyun, Zhao Xi, Huang Kai, Zhou Xiaofang</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Knowledge and Data Engineering</td>
    <td>12</td>
    <td><p>Indexing is an effective way to support efficient query processing in large
databases. Recently the concept of learned index, which replaces or complements
traditional index structures with machine learning models, has been actively
explored to reduce storage and search costs. However, accurate and efficient
similarity query processing in high-dimensional metric spaces remains to be an
open challenge. In this paper, we propose a novel indexing approach called LIMS
that uses data clustering, pivot-based data transformation techniques and
learned indexes to support efficient similarity query processing in metric
spaces. In LIMS, the underlying data is partitioned into clusters such that
each cluster follows a relatively uniform data distribution. Data
redistribution is achieved by utilizing a small number of pivots for each
cluster. Similar data are mapped into compact regions and the mapped values are
totally ordinal. Machine learning models are developed to approximate the
position of each data record on disk. Efficient algorithms are designed for
processing range queries and nearest neighbor queries based on LIMS, and for
index maintenance with dynamic updates. Extensive experiments on real-world and
synthetic datasets demonstrate the superiority of LIMS compared with
traditional indexes and state-of-the-art learned indexes.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Datasets 
      
        Vector-Indexing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/thakur2022injecting/">Injecting Domain Adaptation with Learning-to-hash for Effective and Efficient Zero-shot Dense Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Injecting Domain Adaptation with Learning-to-hash for Effective and Efficient Zero-shot Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Injecting Domain Adaptation with Learning-to-hash for Effective and Efficient Zero-shot Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Thakur Nandan, Reimers Nils, Lin Jimmy</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>6</td>
    <td><p>Dense retrieval overcome the lexical gap and has shown great success in
ad-hoc information retrieval (IR). Despite their success, dense retrievers are
expensive to serve across practical use cases. For use cases requiring to
search from millions of documents, the dense index becomes bulky and requires
high memory usage for storing the index. More recently, learning-to-hash (LTH)
techniques, for e.g., BPR and JPQ, produce binary document vectors, thereby
reducing the memory requirement to efficiently store the dense index. LTH
techniques are supervised and finetune the retriever using a ranking loss. They
outperform their counterparts, i.e., traditional out-of-the-box vector
compression techniques such as PCA or PQ. A missing piece from prior work is
that existing techniques have been evaluated only in-domain, i.e., on a single
dataset such as MS MARCO. In our work, we evaluate LTH and vector compression
techniques for improving the downstream zero-shot retrieval accuracy of the
TAS-B dense retriever while maintaining efficiency at inference. Our results
demonstrate that, unlike prior work, LTH strategies when applied naively can
underperform the zero-shot TAS-B dense retriever on average by up to 14%
nDCG@10 on the BEIR benchmark. To solve this limitation, in our work, we
propose an easy yet effective solution of injecting domain adaptation with
existing supervised LTH techniques. We experiment with two well-known
unsupervised domain adaptation techniques: GenQ and GPL. Our domain adaptation
injection technique can improve the downstream zero-shot retrieval
effectiveness for both BPR and JPQ variants of the TAS-B model by on average
11.5% and 8.2% nDCG@10 while both maintaining 32\(\times\) memory efficiency and
14\(\times\) and 2\(\times\) speedup respectively in CPU retrieval latency on BEIR.
All our code, models, and data are publicly available at
https://github.com/thakur-nandan/income.</p>
</td>
    <td>
      
        Few-Shot-&-Zero-Shot 
      
        Supervised 
      
        Datasets 
      
        Memory-Efficiency 
      
        Unsupervised 
      
        Quantization 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/tan2021fast/">A Fast Partial Video Copy Detection Using KNN and Global Feature Database</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Fast Partial Video Copy Detection Using KNN and Global Feature Database' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Fast Partial Video Copy Detection Using KNN and Global Feature Database' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tan Weijun, Guo Hongwei, Liu Rushuai</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>12</td>
    <td><p>We propose a fast partial video copy detection framework in this paper. In
this framework all frame features of the reference videos are organized in a
KNN searchable database. Instead of scanning all reference videos, the query
video segment does a fast KNN search in the global feature database. The
returned results are used to generate a short list of candidate videos. A
modified temporal network is then used to localize the copy segment in the
candidate videos. We evaluate different choice of CNN features on the VCDB
dataset. Our benchmark F1 score exceeds the state of the art by a big margin.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Datasets 
      
        Evaluation 
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/sun2022deep/">Deep Normalized Cross-Modal Hashing With Bi-Direction Relation Reasoning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Normalized Cross-Modal Hashing With Bi-Direction Relation Reasoning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Normalized Cross-Modal Hashing With Bi-Direction Relation Reasoning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sun Changchang, Latapie, Liu, Yan</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>26</td>
    <td><p>Due to the continuous growth of large-scale multi-modal data and increasing requirements for retrieval speed, deep cross-modal hashing has gained increasing attention recently. Most of existing studies take a similarity matrix as supervision to optimize their models, and the inner product between continuous surrogates of hash codes is utilized to depict the similarity in the Hamming space. However, all of them merely consider the relevant information to build the similarity matrix, ignoring the contribution of the irrelevant one, i.e., the categories that samples do not belong to. Therefore, they cannot effectively alleviate the effect of dissimilar samples. Moreover, due to the modality distribution difference, directly utilizing continuous surrogates of hash codes to calculate similarity may induce suboptimal retrieval performance. To tackle these issues, in this paper, we propose a novel deep normalized cross-modal hashing scheme with bi-direction relation reasoning, named Bi_NCMH. Specifically, we build the multi-level semantic similarity matrix by considering bi-direction relation, i.e., consistent and inconsistent relation. It hence can holistically characterize relations among instances. Besides, we execute feature normalization on continuous surrogates of hash codes to eliminate the deviation caused by modality gap, which further reduces the negative impact of binarization on retrieval performance. Extensive experiments on two cross-modal benchmark datasets demonstrate the superiority of our model over several state-of-the-art baselines.</p>
</td>
    <td>
      
        Scalability 
      
        Datasets 
      
        CVPR 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/sun2025deep/">Deep Normalized Cross-Modal Hashing With Bi-Direction Relation Reasoning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Normalized Cross-Modal Hashing With Bi-Direction Relation Reasoning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Normalized Cross-Modal Hashing With Bi-Direction Relation Reasoning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sun Changchang, Latapie, Liu, Yan</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>26</td>
    <td><p>Due to the continuous growth of large-scale multi-modal data and increasing requirements for retrieval speed, deep cross-modal hashing has gained increasing attention recently. Most of existing studies take a similarity matrix as supervision to optimize their models, and the inner product between continuous surrogates of hash codes is utilized to depict the similarity in the Hamming space. However, all of them merely consider the relevant information to build the similarity matrix, ignoring the contribution of the irrelevant one, i.e., the categories that samples do not belong to. Therefore, they cannot effectively alleviate the effect of dissimilar samples. Moreover, due to the modality distribution difference, directly utilizing continuous surrogates of hash codes to calculate similarity may induce suboptimal retrieval performance. To tackle these issues, in this paper, we propose a novel deep normalized cross-modal hashing scheme with bi-direction relation reasoning, named Bi_NCMH. Specifically, we build the multi-level semantic similarity matrix by considering bi-direction relation, i.e., consistent and inconsistent relation. It hence can holistically characterize relations among instances. Besides, we execute feature normalization on continuous surrogates of hash codes to eliminate the deviation caused by modality gap, which further reduces the negative impact of binarization on retrieval performance. Extensive experiments on two cross-modal benchmark datasets demonstrate the superiority of our model over several state-of-the-art baselines.</p>
</td>
    <td>
      
        Scalability 
      
        Datasets 
      
        CVPR 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/song2022asymmetric/">Asymmetric Hash Code Learning for Remote Sensing Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Asymmetric Hash Code Learning for Remote Sensing Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Asymmetric Hash Code Learning for Remote Sensing Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Song Weiwei, Gao Zhi, Dian Renwei, Ghamisi Pedram, Zhang Yongjun, Benediktsson Jón Atli</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Geoscience and Remote Sensing</td>
    <td>44</td>
    <td><p>Remote sensing image retrieval (RSIR), aiming at searching for a set of
similar items to a given query image, is a very important task in remote
sensing applications. Deep hashing learning as the current mainstream method
has achieved satisfactory retrieval performance. On one hand, various deep
neural networks are used to extract semantic features of remote sensing images.
On the other hand, the hashing techniques are subsequently adopted to map the
high-dimensional deep features to the low-dimensional binary codes. This kind
of methods attempts to learn one hash function for both the query and database
samples in a symmetric way. However, with the number of database samples
increasing, it is typically time-consuming to generate the hash codes of
large-scale database images. In this paper, we propose a novel deep hashing
method, named asymmetric hash code learning (AHCL), for RSIR. The proposed AHCL
generates the hash codes of query and database images in an asymmetric way. In
more detail, the hash codes of query images are obtained by binarizing the
output of the network, while the hash codes of database images are directly
learned by solving the designed objective function. In addition, we combine the
semantic information of each image and the similarity information of pairs of
images as supervised information to train a deep hashing network, which
improves the representation ability of deep features and hash codes. The
experimental results on three public datasets demonstrate that the proposed
method outperforms symmetric methods in terms of retrieval accuracy and
efficiency. The source code is available at
https://github.com/weiweisong415/Demo AHCL for TGRS2022.</p>
</td>
    <td>
      
        Supervised 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Scalability 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/shvetsova2021everything/">Everything at Once -- Multi-modal Fusion Transformer for Video Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Everything at Once -- Multi-modal Fusion Transformer for Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Everything at Once -- Multi-modal Fusion Transformer for Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shvetsova Nina, Chen Brian, Rouditchenko Andrew, Thomas Samuel, Kingsbury Brian, Feris Rogerio, Harwath David, Glass James, Kuehne Hilde</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>98</td>
    <td><p>Multi-modal learning from video data has seen increased attention recently as
it allows to train semantically meaningful embeddings without human annotation
enabling tasks like zero-shot retrieval and classification. In this work, we
present a multi-modal, modality agnostic fusion transformer approach that
learns to exchange information between multiple modalities, such as video,
audio, and text, and integrate them into a joined multi-modal representation to
obtain an embedding that aggregates multi-modal temporal information. We
propose to train the system with a combinatorial loss on everything at once,
single modalities as well as pairs of modalities, explicitly leaving out any
add-ons such as position or modality encoding. At test time, the resulting
model can process and fuse any number of input modalities. Moreover, the
implicit properties of the transformer allow to process inputs of different
lengths. To evaluate the proposed approach, we train the model on the large
scale HowTo100M dataset and evaluate the resulting embedding space on four
challenging benchmark datasets obtaining state-of-the-art results in zero-shot
video retrieval and zero-shot video action localization.</p>
</td>
    <td>
      
        Few-Shot-&-Zero-Shot 
      
        Datasets 
      
        Video-Retrieval 
      
        CVPR 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/sami2022benchmarking/">Benchmarking Human Face Similarity Using Identical Twins</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Benchmarking Human Face Similarity Using Identical Twins' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Benchmarking Human Face Similarity Using Identical Twins' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sami Shoaib Meraj, Mccauley John, Soleymani Sobhan, Nasrabadi Nasser, Dawson Jeremy</td> <!-- 🔧 You were missing this -->
    <td>IET Biometrics</td>
    <td>5</td>
    <td><p>The problem of distinguishing identical twins and non-twin look-alikes in
automated facial recognition (FR) applications has become increasingly
important with the widespread adoption of facial biometrics. Due to the high
facial similarity of both identical twins and look-alikes, these face pairs
represent the hardest cases presented to facial recognition tools. This work
presents an application of one of the largest twin datasets compiled to date to
address two FR challenges: 1) determining a baseline measure of facial
similarity between identical twins and 2) applying this similarity measure to
determine the impact of doppelgangers, or look-alikes, on FR performance for
large face datasets. The facial similarity measure is determined via a deep
convolutional neural network. This network is trained on a tailored
verification task designed to encourage the network to group together highly
similar face pairs in the embedding space and achieves a test AUC of 0.9799.
The proposed network provides a quantitative similarity score for any two given
faces and has been applied to large-scale face datasets to identify similar
face pairs. An additional analysis which correlates the comparison score
returned by a facial recognition tool and the similarity score returned by the
proposed network has also been performed.</p>
</td>
    <td>
      
        Datasets 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/shukor2022transformer/">Transformer Decoders with MultiModal Regularization for Cross-Modal Food Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Transformer Decoders with MultiModal Regularization for Cross-Modal Food Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Transformer Decoders with MultiModal Regularization for Cross-Modal Food Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shukor Mustafa, Couairon Guillaume, Grechka Asya, Cord Matthieu</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>23</td>
    <td><p>Cross-modal image-recipe retrieval has gained significant attention in recent
years. Most work focuses on improving cross-modal embeddings using unimodal
encoders, that allow for efficient retrieval in large-scale databases, leaving
aside cross-attention between modalities which is more computationally
expensive. We propose a new retrieval framework, T-Food (Transformer Decoders
with MultiModal Regularization for Cross-Modal Food Retrieval) that exploits
the interaction between modalities in a novel regularization scheme, while
using only unimodal encoders at test time for efficient retrieval. We also
capture the intra-dependencies between recipe entities with a dedicated recipe
encoder, and propose new variants of triplet losses with dynamic margins that
adapt to the difficulty of the task. Finally, we leverage the power of the
recent Vision and Language Pretraining (VLP) models such as CLIP for the image
encoder. Our approach outperforms existing approaches by a large margin on the
Recipe1M dataset. Specifically, we achieve absolute improvements of 8.1 % (72.6
R@1) and +10.9 % (44.6 R@1) on the 1k and 10k test sets respectively. The code
is available here:https://github.com/mshukor/TFood</p>
</td>
    <td>
      
        Similarity-Search 
      
        Tools-&-Libraries 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        CVPR 
      
        Scalability 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/qiu2022pre/">Pre-training Tasks for User Intent Detection and Embedding Retrieval in E-commerce Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Pre-training Tasks for User Intent Detection and Embedding Retrieval in E-commerce Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Pre-training Tasks for User Intent Detection and Embedding Retrieval in E-commerce Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Qiu Yiming, Zhao Chenyu, Zhang Han, Zhuo Jingwei, Li Tianhao, Zhang Xiaowei, Wang Songlin, Xu Sulong, Long Bo, Yang Wen-yun</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</td>
    <td>13</td>
    <td><p>BERT-style models pre-trained on the general corpus (e.g., Wikipedia) and
fine-tuned on specific task corpus, have recently emerged as breakthrough
techniques in many NLP tasks: question answering, text classification, sequence
labeling and so on. However, this technique may not always work, especially for
two scenarios: a corpus that contains very different text from the general
corpus Wikipedia, or a task that learns embedding spacial distribution for a
specific purpose (e.g., approximate nearest neighbor search). In this paper, to
tackle the above two scenarios that we have encountered in an industrial
e-commerce search system, we propose customized and novel pre-training tasks
for two critical modules: user intent detection and semantic embedding
retrieval. The customized pre-trained models after fine-tuning, being less than
10% of BERT-base’s size in order to be feasible for cost-efficient CPU serving,
significantly improve the other baseline models: 1) no pre-training model and
2) fine-tuned model from the official pre-trained BERT using general corpus, on
both offline datasets and online system. We have open sourced our datasets for
the sake of reproducibility and future works.</p>
</td>
    <td>
      
        Datasets 
      
        CIKM 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/paolicelli2022learning/">Learning Semantics for Visual Place Recognition through Multi-Scale Attention</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Semantics for Visual Place Recognition through Multi-Scale Attention' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Semantics for Visual Place Recognition through Multi-Scale Attention' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Paolicelli Valerio, Tavera Antonio, Masone Carlo, Berton Gabriele, Caputo Barbara</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>10</td>
    <td><p>In this paper we address the task of visual place recognition (VPR), where
the goal is to retrieve the correct GPS coordinates of a given query image
against a huge geotagged gallery. While recent works have shown that building
descriptors incorporating semantic and appearance information is beneficial,
current state-of-the-art methods opt for a top down definition of the
significant semantic content. Here we present the first VPR algorithm that
learns robust global embeddings from both visual appearance and semantic
content of the data, with the segmentation process being dynamically guided by
the recognition of places through a multi-scale attention module. Experiments
on various scenarios validate this new approach and demonstrate its performance
against state-of-the-art methods. Finally, we propose the first synthetic-world
dataset suited for both place recognition and segmentation tasks.</p>
</td>
    <td>
      
        Datasets 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/patel2021recall/">Recall@k Surrogate Loss with Large Batches and Similarity Mixup</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Recall@k Surrogate Loss with Large Batches and Similarity Mixup' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Recall@k Surrogate Loss with Large Batches and Similarity Mixup' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Patel Yash, Tolias Giorgos, Matas Jiri</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>28</td>
    <td><p>This work focuses on learning deep visual representation models for retrieval
by exploring the interplay between a new loss function, the batch size, and a
new regularization approach. Direct optimization, by gradient descent, of an
evaluation metric, is not possible when it is non-differentiable, which is the
case for recall in retrieval. A differentiable surrogate loss for the recall is
proposed in this work. Using an implementation that sidesteps the hardware
constraints of the GPU memory, the method trains with a very large batch size,
which is essential for metrics computed on the entire retrieval database. It is
assisted by an efficient mixup regularization approach that operates on
pairwise scalar similarities and virtually increases the batch size further.
The suggested method achieves state-of-the-art performance in several image
retrieval benchmarks when used for deep metric learning. For instance-level
recognition, the method outperforms similar approaches that train using an
approximation of average precision.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        CVPR 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/ortega2022unconventional/">Unconventional application of k-means for distributed approximate similarity search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unconventional application of k-means for distributed approximate similarity search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unconventional application of k-means for distributed approximate similarity search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ortega Felipe, Algar Maria Jesus, de Diego Isaac Martín, Moguerza Javier M.</td> <!-- 🔧 You were missing this -->
    <td>Information Sciences</td>
    <td>5</td>
    <td><p>Similarity search based on a distance function in metric spaces is a
fundamental problem for many applications. Queries for similar objects lead to
the well-known machine learning task of nearest-neighbours identification. Many
data indexing strategies, collectively known as Metric Access Methods (MAM),
have been proposed to speed up queries for similar elements in this context.
Moreover, since exact approaches to solve similarity queries can be complex and
time-consuming, alternative options have appeared to reduce query execution
time, such as returning approximate results or resorting to distributed
computing platforms. In this paper, we introduce MASK (Multilevel Approximate
Similarity search with \(k\)-means), an unconventional application of the
\(k\)-means algorithm as the foundation of a multilevel index structure for
approximate similarity search, suitable for metric spaces. We show that
inherent properties of \(k\)-means, like representing high-density data areas
with fewer prototypes, can be leveraged for this purpose. An implementation of
this new indexing method is evaluated, using a synthetic dataset and a
real-world dataset in a high-dimensional and high-sparsity space. Results are
promising and underpin the applicability of this novel indexing method in
multiple domains.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Datasets 
      
        Vector-Indexing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/neelakantan2022text/">Text and Code Embeddings by Contrastive Pre-Training</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Text and Code Embeddings by Contrastive Pre-Training' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Text and Code Embeddings by Contrastive Pre-Training' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Neelakantan Arvind, Xu Tao, Puri Raul, Radford Alec, Han Jesse Michael, Tworek Jerry, Yuan Qiming, Tezak Nikolas, Kim Jong Wook, Hallacy Chris, Heidecke Johannes, Shyam Pranav, Power Boris, Nekoul Tyna Eloundou, Sastry Girish, Krueger Gretchen, Schnurr David, Such Felipe Petroski, Hsu Kenny, Thompson Madeleine, Khan Tabarak, Sherbakov Toki, Jang Joanne, Welinder Peter, Weng Lilian</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>110</td>
    <td><p>Text embeddings are useful features in many applications such as semantic
search and computing text similarity. Previous work typically trains models
customized for different use cases, varying in dataset choice, training
objective and model architecture. In this work, we show that contrastive
pre-training on unsupervised data at scale leads to high quality vector
representations of text and code. The same unsupervised text embeddings that
achieve new state-of-the-art results in linear-probe classification also
display impressive semantic search capabilities and sometimes even perform
competitively with fine-tuned models. On linear-probe classification accuracy
averaging over 7 tasks, our best unsupervised model achieves a relative
improvement of 4% and 1.8% over previous best unsupervised and supervised text
embedding models respectively. The same text embeddings when evaluated on
large-scale semantic search attains a relative improvement of 23.4%, 14.7%, and
10.6% over previous best unsupervised methods on MSMARCO, Natural Questions and
TriviaQA benchmarks, respectively. Similarly to text embeddings, we train code
embedding models on (text, code) pairs, obtaining a 20.8% relative improvement
over prior best work on code search.</p>
</td>
    <td>
      
        Unsupervised 
      
        Scalability 
      
        Supervised 
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/mikriukov2022deep/">Deep Unsupervised Contrastive Hashing for Large-Scale Cross-Modal Text-Image Retrieval in Remote Sensing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Unsupervised Contrastive Hashing for Large-Scale Cross-Modal Text-Image Retrieval in Remote Sensing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Unsupervised Contrastive Hashing for Large-Scale Cross-Modal Text-Image Retrieval in Remote Sensing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mikriukov Georgii, Ravanbakhsh Mahdyar, Demir Begüm</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>19</td>
    <td><p>Due to the availability of large-scale multi-modal data (e.g., satellite
images acquired by different sensors, text sentences, etc) archives, the
development of cross-modal retrieval systems that can search and retrieve
semantically relevant data across different modalities based on a query in any
modality has attracted great attention in RS. In this paper, we focus our
attention on cross-modal text-image retrieval, where queries from one modality
(e.g., text) can be matched to archive entries from another (e.g., image). Most
of the existing cross-modal text-image retrieval systems require a high number
of labeled training samples and also do not allow fast and memory-efficient
retrieval due to their intrinsic characteristics. These issues limit the
applicability of the existing cross-modal retrieval systems for large-scale
applications in RS. To address this problem, in this paper we introduce a novel
deep unsupervised cross-modal contrastive hashing (DUCH) method for RS
text-image retrieval. The proposed DUCH is made up of two main modules: 1)
feature extraction module (which extracts deep representations of the
text-image modalities); and 2) hashing module (which learns to generate
cross-modal binary hash codes from the extracted representations). Within the
hashing module, we introduce a novel multi-objective loss function including:
i) contrastive objectives that enable similarity preservation in both intra-
and inter-modal similarities; ii) an adversarial objective that is enforced
across two modalities for cross-modal representation consistency; iii)
binarization objectives for generating representative hash codes. Experimental
results show that the proposed DUCH outperforms state-of-the-art unsupervised
cross-modal hashing methods on two multi-modal (image and text) benchmark
archives in RS. Our code is publicly available at
https://git.tu-berlin.de/rsim/duch.</p>
</td>
    <td>
      
        Multimodal-Retrieval 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Neural-Hashing 
      
        Unsupervised 
      
        Scalability 
      
        Evaluation 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/mikriukov2022unsupervised/">Unsupervised Contrastive Hashing for Cross-Modal Retrieval in Remote Sensing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Contrastive Hashing for Cross-Modal Retrieval in Remote Sensing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Contrastive Hashing for Cross-Modal Retrieval in Remote Sensing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mikriukov Georgii, Ravanbakhsh Mahdyar, Demir Begüm</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE International Conference on Image Processing (ICIP)</td>
    <td>5</td>
    <td><p>The development of cross-modal retrieval systems that can search and retrieve
semantically relevant data across different modalities based on a query in any
modality has attracted great attention in remote sensing (RS). In this paper,
we focus our attention on cross-modal text-image retrieval, where queries from
one modality (e.g., text) can be matched to archive entries from another (e.g.,
image). Most of the existing cross-modal text-image retrieval systems in RS
require a high number of labeled training samples and also do not allow fast
and memory-efficient retrieval. These issues limit the applicability of the
existing cross-modal retrieval systems for large-scale applications in RS. To
address this problem, in this paper we introduce a novel unsupervised
cross-modal contrastive hashing (DUCH) method for text-image retrieval in RS.
To this end, the proposed DUCH is made up of two main modules: 1) feature
extraction module, which extracts deep representations of two modalities; 2)
hashing module that learns to generate cross-modal binary hash codes from the
extracted representations. We introduce a novel multi-objective loss function
including: i) contrastive objectives that enable similarity preservation in
intra- and inter-modal similarities; ii) an adversarial objective that is
enforced across two modalities for cross-modal representation consistency; and
iii) binarization objectives for generating hash codes. Experimental results
show that the proposed DUCH outperforms state-of-the-art methods. Our code is
publicly available at https://git.tu-berlin.de/rsim/duch.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Multimodal-Retrieval 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Neural-Hashing 
      
        Unsupervised 
      
        Scalability 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/neculai2022probabilistic/">Probabilistic Compositional Embeddings for Multimodal Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Probabilistic Compositional Embeddings for Multimodal Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Probabilistic Compositional Embeddings for Multimodal Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Neculai Andrei, Chen Yanbei, Akata Zeynep</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>21</td>
    <td><p>Existing works in image retrieval often consider retrieving images with one
or two query inputs, which do not generalize to multiple queries. In this work,
we investigate a more challenging scenario for composing multiple multimodal
queries in image retrieval. Given an arbitrary number of query images and (or)
texts, our goal is to retrieve target images containing the semantic concepts
specified in multiple multimodal queries. To learn an informative embedding
that can flexibly encode the semantics of various queries, we propose a novel
multimodal probabilistic composer (MPC). Specifically, we model input images
and texts as probabilistic embeddings, which can be further composed by a
probabilistic composition rule to facilitate image retrieval with multiple
multimodal queries. We propose a new benchmark based on the MS-COCO dataset and
evaluate our model on various setups that compose multiple images and (or) text
queries for multimodal image retrieval. Without bells and whistles, we show
that our probabilistic model formulation significantly outperforms existing
related methods on multimodal image retrieval while generalizing well to query
with different amounts of inputs given in arbitrary visual and (or) textual
modalities. Code is available here: https://github.com/andreineculai/MPC.</p>
</td>
    <td>
      
        Datasets 
      
        CVPR 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/ma2022pre/">Pre-train a Discriminative Text Encoder for Dense Retrieval via Contrastive Span Prediction</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Pre-train a Discriminative Text Encoder for Dense Retrieval via Contrastive Span Prediction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Pre-train a Discriminative Text Encoder for Dense Retrieval via Contrastive Span Prediction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ma Xinyu, Guo Jiafeng, Zhang Ruqing, Fan Yixing, Cheng Xueqi</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>30</td>
    <td><p>Dense retrieval has shown promising results in many information retrieval
(IR) related tasks, whose foundation is high-quality text representation
learning for effective search. Some recent studies have shown that
autoencoder-based language models are able to boost the dense retrieval
performance using a weak decoder. However, we argue that 1) it is not
discriminative to decode all the input texts and, 2) even a weak decoder has
the bypass effect on the encoder. Therefore, in this work, we introduce a novel
contrastive span prediction task to pre-train the encoder alone, but still
retain the bottleneck ability of the autoencoder. % Therefore, in this work, we
propose to drop out the decoder and introduce a novel contrastive span
prediction task to pre-train the encoder alone. The key idea is to force the
encoder to generate the text representation close to its own random spans while
far away from others using a group-wise contrastive loss. In this way, we can
1) learn discriminative text representations efficiently with the group-wise
contrastive learning over spans and, 2) avoid the bypass effect of the decoder
thoroughly. Comprehensive experiments over publicly available retrieval
benchmark datasets show that our approach can outperform existing pre-training
methods for dense retrieval significantly.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Datasets 
      
        SIGIR 
      
        Self-Supervised 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/luo2022survey/">A Survey on Deep Hashing Methods</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Survey on Deep Hashing Methods' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Survey on Deep Hashing Methods' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Luo Xiao, Wang, Wu, Chen, Deng, Huang, Hua</td> <!-- 🔧 You were missing this -->
    <td>ACM Transactions on Knowledge Discovery from Data</td>
    <td>123</td>
    <td><p>Nearest neighbor search aims at obtaining the samples in the database with the smallest distances from them to the queries, which is a basic task in a range of fields, including computer vision and data mining. Hashing is one of the most widely used methods for its computational and storage efficiency. With the development of deep learning, deep hashing methods show more advantages than traditional methods. In this survey, we detailedly investigate current deep hashing algorithms including deep supervised hashing and deep unsupervised hashing. Specifically, we categorize deep supervised hashing methods into pairwise methods, ranking-based methods, pointwise methods as well as quantization according to how measuring the similarities of the learned hash codes. Moreover, deep unsupervised hashing is categorized into similarity reconstruction-based methods, pseudo-label-based methods, and prediction-free self-supervised learning-based methods based on their semantic learning manners. We also introduce three related important topics including semi-supervised deep hashing, domain adaption deep hashing, and multi-modal deep hashing. Meanwhile, we present some commonly used public datasets and the scheme to measure the performance of deep hashing algorithms. Finally, we discuss some potential research directions in conclusion.</p>
</td>
    <td>
      
        Efficiency 
      
        Unsupervised 
      
        Survey-Paper 
      
        Datasets 
      
        Neural-Hashing 
      
        Self-Supervised 
      
        Evaluation 
      
        Quantization 
      
        Hashing-Methods 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/luo2020survey/">A Survey on Deep Hashing Methods</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Survey on Deep Hashing Methods' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Survey on Deep Hashing Methods' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Luo Xiao, Wang Haixin, Wu Daqing, Chen Chong, Deng Minghua, Huang Jianqiang, Hua Xian-sheng</td> <!-- 🔧 You were missing this -->
    <td>ACM Transactions on Knowledge Discovery from Data</td>
    <td>123</td>
    <td><p>Nearest neighbor search aims to obtain the samples in the database with the
smallest distances from them to the queries, which is a basic task in a range
of fields, including computer vision and data mining. Hashing is one of the
most widely used methods for its computational and storage efficiency. With the
development of deep learning, deep hashing methods show more advantages than
traditional methods. In this survey, we detailedly investigate current deep
hashing algorithms including deep supervised hashing and deep unsupervised
hashing. Specifically, we categorize deep supervised hashing methods into
pairwise methods, ranking-based methods, pointwise methods as well as
quantization according to how measuring the similarities of the learned hash
codes. Moreover, deep unsupervised hashing is categorized into similarity
reconstruction-based methods, pseudo-label-based methods and prediction-free
self-supervised learning-based methods based on their semantic learning
manners. We also introduce three related important topics including
semi-supervised deep hashing, domain adaption deep hashing and multi-modal deep
hashing. Meanwhile, we present some commonly used public datasets and the
scheme to measure the performance of deep hashing algorithms. Finally, we
discuss some potential research directions in conclusion.</p>
</td>
    <td>
      
        Unsupervised 
      
        Supervised 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Self-Supervised 
      
        Quantization 
      
        Survey-Paper 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/luo2025survey/">A Survey on Deep Hashing Methods</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Survey on Deep Hashing Methods' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Survey on Deep Hashing Methods' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Luo Xiao, Wang, Wu, Chen, Deng, Huang, Hua</td> <!-- 🔧 You were missing this -->
    <td>ACM Transactions on Knowledge Discovery from Data</td>
    <td>123</td>
    <td><p>Nearest neighbor search aims at obtaining the samples in the database with the smallest distances from them to the queries, which is a basic task in a range of fields, including computer vision and data mining. Hashing is one of the most widely used methods for its computational and storage efficiency. With the development of deep learning, deep hashing methods show more advantages than traditional methods. In this survey, we detailedly investigate current deep hashing algorithms including deep supervised hashing and deep unsupervised hashing. Specifically, we categorize deep supervised hashing methods into pairwise methods, ranking-based methods, pointwise methods as well as quantization according to how measuring the similarities of the learned hash codes. Moreover, deep unsupervised hashing is categorized into similarity reconstruction-based methods, pseudo-label-based methods, and prediction-free self-supervised learning-based methods based on their semantic learning manners. We also introduce three related important topics including semi-supervised deep hashing, domain adaption deep hashing, and multi-modal deep hashing. Meanwhile, we present some commonly used public datasets and the scheme to measure the performance of deep hashing algorithms. Finally, we discuss some potential research directions in conclusion.</p>
</td>
    <td>
      
        Efficiency 
      
        Unsupervised 
      
        Survey-Paper 
      
        Datasets 
      
        Neural-Hashing 
      
        Self-Supervised 
      
        Evaluation 
      
        Quantization 
      
        Hashing-Methods 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/zhao2022progressive/">Progressive Learning for Image Retrieval with Hybrid-Modality Queries</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Progressive Learning for Image Retrieval with Hybrid-Modality Queries' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Progressive Learning for Image Retrieval with Hybrid-Modality Queries' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhao Yida, Song Yuqing, Jin Qin</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>21</td>
    <td><p>Image retrieval with hybrid-modality queries, also known as composing text
and image for image retrieval (CTI-IR), is a retrieval task where the search
intention is expressed in a more complex query format, involving both vision
and text modalities. For example, a target product image is searched using a
reference product image along with text about changing certain attributes of
the reference image as the query. It is a more challenging image retrieval task
that requires both semantic space learning and cross-modal fusion. Previous
approaches that attempt to deal with both aspects achieve unsatisfactory
performance. In this paper, we decompose the CTI-IR task into a three-stage
learning problem to progressively learn the complex knowledge for image
retrieval with hybrid-modality queries. We first leverage the semantic
embedding space for open-domain image-text retrieval, and then transfer the
learned knowledge to the fashion-domain with fashion-related pre-training
tasks. Finally, we enhance the pre-trained model from single-query to
hybrid-modality query for the CTI-IR task. Furthermore, as the contribution of
individual modality in the hybrid-modality query varies for different retrieval
scenarios, we propose a self-supervised adaptive weighting strategy to
dynamically determine the importance of image and text in the hybrid-modality
query for better retrieval. Extensive experiments show that our proposed model
significantly outperforms state-of-the-art methods in the mean of Recall@K by
24.9% and 9.5% on the Fashion-IQ and Shoes benchmark datasets respectively.</p>
</td>
    <td>
      
        Supervised 
      
        Text-Retrieval 
      
        Image-Retrieval 
      
        Datasets 
      
        SIGIR 
      
        Self-Supervised 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/liu2022towards/">Towards Fast and Accurate Federated Learning with non-IID Data for Cloud-Based IoT Applications</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Towards Fast and Accurate Federated Learning with non-IID Data for Cloud-Based IoT Applications' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Towards Fast and Accurate Federated Learning with non-IID Data for Cloud-Based IoT Applications' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Tian, Ding Jiahao, Wang Ting, Pan Miao, Chen Mingsong</td> <!-- 🔧 You were missing this -->
    <td>Journal of Circuits, Systems and Computers</td>
    <td>7</td>
    <td><p>As a promising method of central model training on decentralized device data
while securing user privacy, Federated Learning (FL)is becoming popular in
Internet of Things (IoT) design. However, when the data collected by IoT
devices are highly skewed in a non-independent and identically distributed
(non-IID) manner, the accuracy of vanilla FL method cannot be guaranteed.
Although there exist various solutions that try to address the bottleneck of FL
with non-IID data, most of them suffer from extra intolerable communication
overhead and low model accuracy. To enable fast and accurate FL, this paper
proposes a novel data-based device grouping approach that can effectively
reduce the disadvantages of weight divergence during the training of non-IID
data. However, since our grouping method is based on the similarity of
extracted feature maps from IoT devices, it may incur additional risks of
privacy exposure. To solve this problem, we propose an improved version by
exploiting similarity information using the Locality-Sensitive Hashing (LSH)
algorithm without exposing extracted feature maps. Comprehensive experimental
results on well-known benchmarks show that our approach can not only accelerate
the convergence rate, but also improve the prediction accuracy for FL with
non-IID data.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Locality-Sensitive-Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/lu2021learnable/">Learnable Locality-Sensitive Hashing for Video Anomaly Detection</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learnable Locality-Sensitive Hashing for Video Anomaly Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learnable Locality-Sensitive Hashing for Video Anomaly Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lu Yue, Cao Congqi, Zhang Yanning</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Circuits and Systems for Video Technology</td>
    <td>23</td>
    <td><p>Video anomaly detection (VAD) mainly refers to identifying anomalous events
that have not occurred in the training set where only normal samples are
available. Existing works usually formulate VAD as a reconstruction or
prediction problem. However, the adaptability and scalability of these methods
are limited. In this paper, we propose a novel distance-based VAD method to
take advantage of all the available normal data efficiently and flexibly. In
our method, the smaller the distance between a testing sample and normal
samples, the higher the probability that the testing sample is normal.
Specifically, we propose to use locality-sensitive hashing (LSH) to map samples
whose similarity exceeds a certain threshold into the same bucket in advance.
In this manner, the complexity of near neighbor search is cut down
significantly. To make the samples that are semantically similar get closer and
samples not similar get further apart, we propose a novel learnable version of
LSH that embeds LSH into a neural network and optimizes the hash functions with
contrastive learning strategy. The proposed method is robust to data imbalance
and can handle the large intra-class variations in normal data flexibly.
Besides, it has a good ability of scalability. Extensive experiments
demonstrate the superiority of our method, which achieves new state-of-the-art
results on VAD benchmarks.</p>
</td>
    <td>
      
        Locality-Sensitive-Hashing 
      
        Hashing-Methods 
      
        Self-Supervised 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/liu2020neuromorphic/">Neuromorphic Computing for Content-based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Neuromorphic Computing for Content-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Neuromorphic Computing for Content-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Te-yuan, Mahjoubfar Ata, Prusinski Daniel, Stevens Luis</td> <!-- 🔧 You were missing this -->
    <td>PLOS ONE</td>
    <td>15</td>
    <td><p>Neuromorphic computing mimics the neural activity of the brain through
emulating spiking neural networks. In numerous machine learning tasks,
neuromorphic chips are expected to provide superior solutions in terms of cost
and power efficiency. Here, we explore the application of Loihi, a neuromorphic
computing chip developed by Intel, for the computer vision task of image
retrieval. We evaluated the functionalities and the performance metrics that
are critical in content-based visual search and recommender systems using
deep-learning embeddings. Our results show that the neuromorphic solution is
about 2.5 times more energy-efficient compared with an ARM Cortex-A72 CPU and
12.5 times more energy-efficient compared with NVIDIA T4 GPU for inference by a
lightweight convolutional neural network without batching while maintaining the
same level of matching accuracy. The study validates the potential of
neuromorphic computing in low-power image retrieval, as a complementary
paradigm to the existing von Neumann architectures.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Recommender-Systems 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/zhong2022evaluating/">Evaluating Token-Level and Passage-Level Dense Retrieval Models for Math Information Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Evaluating Token-Level and Passage-Level Dense Retrieval Models for Math Information Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Evaluating Token-Level and Passage-Level Dense Retrieval Models for Math Information Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhong Wei, Yang Jheng-hong, Xie Yuqing, Lin Jimmy</td> <!-- 🔧 You were missing this -->
    <td>Findings of the Association for Computational Linguistics: EMNLP 2022</td>
    <td>12</td>
    <td><p>With the recent success of dense retrieval methods based on bi-encoders,
studies have applied this approach to various interesting downstream retrieval
tasks with good efficiency and in-domain effectiveness. Recently, we have also
seen the presence of dense retrieval models in Math Information Retrieval (MIR)
tasks, but the most effective systems remain classic retrieval methods that
consider hand-crafted structure features. In this work, we try to combine the
best of both worlds:\ a well-defined structure search method for effective
formula search and efficient bi-encoder dense retrieval models to capture
contextual similarities. Specifically, we have evaluated two representative
bi-encoder models for token-level and passage-level dense retrieval on recent
MIR tasks. Our results show that bi-encoder models are highly complementary to
existing structure search methods, and we are able to advance the
state-of-the-art on MIR datasets.</p>
</td>
    <td>
      
        Datasets 
      
        EMNLP 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/lin2022deep/">Deep Unsupervised Hashing with Latent Semantic Components</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Unsupervised Hashing with Latent Semantic Components' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Unsupervised Hashing with Latent Semantic Components' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin Qinghong, Chen Xiaojun, Zhang Qin, Cai Shaotian, Zhao Wenzhe, Wang Hongfa</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>17</td>
    <td><p>Deep unsupervised hashing has been appreciated in the regime of image
retrieval. However, most prior arts failed to detect the semantic components
and their relationships behind the images, which makes them lack discriminative
power. To make up the defect, we propose a novel Deep Semantic Components
Hashing (DSCH), which involves a common sense that an image normally contains a
bunch of semantic components with homology and co-occurrence relationships.
Based on this prior, DSCH regards the semantic components as latent variables
under the Expectation-Maximization framework and designs a two-step iterative
algorithm with the objective of maximum likelihood of training data. Firstly,
DSCH constructs a semantic component structure by uncovering the fine-grained
semantics components of images with a Gaussian Mixture Modal~(GMM), where an
image is represented as a mixture of multiple components, and the semantics
co-occurrence are exploited. Besides, coarse-grained semantics components, are
discovered by considering the homology relationships between fine-grained
components, and the hierarchy organization is then constructed. Secondly, DSCH
makes the images close to their semantic component centers at both fine-grained
and coarse-grained levels, and also makes the images share similar semantic
components close to each other. Extensive experiments on three benchmark
datasets demonstrate that the proposed hierarchical semantic components indeed
facilitate the hashing model to achieve superior performance.</p>
</td>
    <td>
      
        Supervised 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        AAAI 
      
        Unsupervised 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/li2023dual/">Dual-Stream Knowledge-Preserving Hashing for Unsupervised Video Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Dual-Stream Knowledge-Preserving Hashing for Unsupervised Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Dual-Stream Knowledge-Preserving Hashing for Unsupervised Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Pandeng, Xie Hongtao, Ge Jiannan, Zhang Lei, Min Shaobo, Zhang Yongdong</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>16</td>
    <td><p>Unsupervised video hashing usually optimizes binary codes by learning to
reconstruct input videos. Such reconstruction constraint spends much effort on
frame-level temporal context changes without focusing on video-level global
semantics that are more useful for retrieval. Hence, we address this problem by
decomposing video information into reconstruction-dependent and
semantic-dependent information, which disentangles the semantic extraction from
reconstruction constraint. Specifically, we first design a simple dual-stream
structure, including a temporal layer and a hash layer. Then, with the help of
semantic similarity knowledge obtained from self-supervision, the hash layer
learns to capture information for semantic retrieval, while the temporal layer
learns to capture the information for reconstruction. In this way, the model
naturally preserves the disentangled semantics into binary codes. Validated by
comprehensive experiments, our method consistently outperforms the
state-of-the-arts on three video benchmarks.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Unsupervised 
      
        Compact-Codes 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/li2022adaptive/">Adaptive Structural Similarity Preserving for Unsupervised Cross Modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Adaptive Structural Similarity Preserving for Unsupervised Cross Modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Adaptive Structural Similarity Preserving for Unsupervised Cross Modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Liang, Zheng Baihua, Sun Weiwei</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 30th ACM International Conference on Multimedia</td>
    <td>23</td>
    <td><p>Cross-modal hashing is an important approach for multimodal data management
and application. Existing unsupervised cross-modal hashing algorithms mainly
rely on data features in pre-trained models to mine their similarity
relationships. However, their optimization objectives are based on the static
metric between the original uni-modal features, without further exploring data
correlations during the training. In addition, most of them mainly focus on
association mining and alignment among pairwise instances in continuous space
but ignore the latent structural correlations contained in the semantic hashing
space. In this paper, we propose an unsupervised hash learning framework,
namely Adaptive Structural Similarity Preservation Hashing (ASSPH), to solve
the above problems. Firstly, we propose an adaptive learning scheme, with
limited data and training batches, to enrich semantic correlations of unlabeled
instances during the training process and meanwhile to ensure a smooth
convergence of the training process. Secondly, we present an asymmetric
structural semantic representation learning scheme. We introduce structural
semantic metrics based on graph adjacency relations during the semantic
reconstruction and correlation mining stage and meanwhile align the structure
semantics in the hash space with an asymmetric binary optimization process.
Finally, we conduct extensive experiments to validate the enhancements of our
work in comparison with existing works.</p>
</td>
    <td>
      
        Text-Retrieval 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Unsupervised 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/li2021learning/">Learning Semantic-Aligned Feature Representation for Text-based Person Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Semantic-Aligned Feature Representation for Text-based Person Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Semantic-Aligned Feature Representation for Text-based Person Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Shiping, Cao Min, Zhang Min</td> <!-- 🔧 You were missing this -->
    <td>ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</td>
    <td>47</td>
    <td><p>Text-based person search aims to retrieve images of a certain pedestrian by a
textual description. The key challenge of this task is to eliminate the
inter-modality gap and achieve the feature alignment across modalities. In this
paper, we propose a semantic-aligned embedding method for text-based person
search, in which the feature alignment across modalities is achieved by
automatically learning the semantic-aligned visual features and textual
features. First, we introduce two Transformer-based backbones to encode robust
feature representations of the images and texts. Second, we design a
semantic-aligned feature aggregation network to adaptively select and aggregate
features with the same semantics into part-aware features, which is achieved by
a multi-head attention module constrained by a cross-modality part alignment
loss and a diversity loss. Experimental results on the CUHK-PEDES and Flickr30K
datasets show that our method achieves state-of-the-art performances.</p>
</td>
    <td>
      
        Datasets 
      
        ICASSP 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/zhang2022multi/">Multi-View Document Representation Learning for Open-Domain Dense Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multi-View Document Representation Learning for Open-Domain Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multi-View Document Representation Learning for Open-Domain Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Shunyu, Liang Yaobo, Gong Ming, Jiang Daxin, Duan Nan</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>38</td>
    <td><p>Dense retrieval has achieved impressive advances in first-stage retrieval
from a large-scale document collection, which is built on bi-encoder
architecture to produce single vector representation of query and document.
However, a document can usually answer multiple potential queries from
different views. So the single vector representation of a document is hard to
match with multi-view queries, and faces a semantic mismatch problem. This
paper proposes a multi-view document representation learning framework, aiming
to produce multi-view embeddings to represent documents and enforce them to
align with different queries. First, we propose a simple yet effective method
of generating multiple embeddings through viewers. Second, to prevent
multi-view embeddings from collapsing to the same one, we further propose a
global-local loss with annealed temperature to encourage the multiple viewers
to better align with different potential queries. Experiments show our method
outperforms recent works and achieves state-of-the-art results.</p>
</td>
    <td>
      
        Scalability 
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/leonhardt2021efficient/">Efficient Neural Ranking using Forward Indexes</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Neural Ranking using Forward Indexes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient Neural Ranking using Forward Indexes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Leonhardt Jurek, Rudra Koustav, Khosla Megha, Anand Abhijit, Anand Avishek</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the ACM Web Conference 2022</td>
    <td>12</td>
    <td><p>Neural document ranking approaches, specifically transformer models, have
achieved impressive gains in ranking performance. However, query processing
using such over-parameterized models is both resource and time intensive. In
this paper, we propose the Fast-Forward index – a simple vector forward index
that facilitates ranking documents using interpolation of lexical and semantic
scores – as a replacement for contextual re-rankers and dense indexes based on
nearest neighbor search. Fast-Forward indexes rely on efficient sparse models
for retrieval and merely look up pre-computed dense transformer-based vector
representations of documents and passages in constant time for fast CPU-based
semantic similarity computation during query processing. We propose index
pruning and theoretically grounded early stopping techniques to improve the
query processing throughput. We conduct extensive large-scale experiments on
TREC-DL datasets and show improvements over hybrid indexes in performance and
query processing efficiency using only CPUs. Fast-Forward indexes can provide
superior ranking performance using interpolation due to the complementary
benefits of lexical and semantic similarities.</p>
</td>
    <td>
      
        Datasets 
      
        Hybrid-ANN-Methods 
      
        Scalability 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/lee2022correlation/">Correlation Verification for Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Correlation Verification for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Correlation Verification for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lee Seongwon, Seong Hongje, Lee Suhyeon, Kim Euntai</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>59</td>
    <td><p>Geometric verification is considered a de facto solution for the re-ranking
task in image retrieval. In this study, we propose a novel image retrieval
re-ranking network named Correlation Verification Networks (CVNet). Our
proposed network, comprising deeply stacked 4D convolutional layers, gradually
compresses dense feature correlation into image similarity while learning
diverse geometric matching patterns from various image pairs. To enable
cross-scale matching, it builds feature pyramids and constructs cross-scale
feature correlations within a single inference, replacing costly multi-scale
inferences. In addition, we use curriculum learning with the hard negative
mining and Hide-and-Seek strategy to handle hard samples without losing
generality. Our proposed re-ranking network shows state-of-the-art performance
on several retrieval benchmarks with a significant margin (+12.6% in mAP on
ROxford-Hard+1M set) over state-of-the-art methods. The source code and models
are available online: https://github.com/sungonce/CVNet.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        CVPR 
      
        Hybrid-ANN-Methods 
      
        Re-Ranking 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/zhao2021feature/">A Feature Consistency Driven Attention Erasing Network for Fine-Grained Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Feature Consistency Driven Attention Erasing Network for Fine-Grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Feature Consistency Driven Attention Erasing Network for Fine-Grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhao Qi, Wang Xu, Lyu Shuchang, Liu Binghao, Yang Yifan</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition</td>
    <td>20</td>
    <td><p>Large-scale fine-grained image retrieval has two main problems. First, low
dimensional feature embedding can fasten the retrieval process but bring
accuracy reduce due to overlooking the feature of significant attention regions
of images in fine-grained datasets. Second, fine-grained images lead to the
same category query hash codes mapping into the different cluster in database
hash latent space. To handle these two issues, we propose a feature consistency
driven attention erasing network (FCAENet) for fine-grained image retrieval.
For the first issue, we propose an adaptive augmentation module in FCAENet,
which is selective region erasing module (SREM). SREM makes the network more
robust on subtle differences of fine-grained task by adaptively covering some
regions of raw images. The feature extractor and hash layer can learn more
representative hash code for fine-grained images by SREM. With regard to the
second issue, we fully exploit the pair-wise similarity information and add the
enhancing space relation loss (ESRL) in FCAENet to make the vulnerable relation
stabler between the query hash code and database hash code. We conduct
extensive experiments on five fine-grained benchmark datasets (CUB2011,
Aircraft, NABirds, VegFru, Food101) for 12bits, 24bits, 32bits, 48bits hash
code. The results show that FCAENet achieves the state-of-the-art (SOTA)
fine-grained retrieval performance compared with other methods.</p>
</td>
    <td>
      
        Scalability 
      
        Evaluation 
      
        Datasets 
      
        Hashing-Methods 
      
        Image-Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/kim2022accelerating/">Accelerating Large-Scale Graph-based Nearest Neighbor Search on a Computational Storage Platform</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Accelerating Large-Scale Graph-based Nearest Neighbor Search on a Computational Storage Platform' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Accelerating Large-Scale Graph-based Nearest Neighbor Search on a Computational Storage Platform' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kim Ji-hoon, Park Yeo-reum, Do Jaeyoung, Ji Soo-young, Kim Joo-young</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Computers</td>
    <td>20</td>
    <td><p>K-nearest neighbor search is one of the fundamental tasks in various
applications and the hierarchical navigable small world (HNSW) has recently
drawn attention in large-scale cloud services, as it easily scales up the
database while offering fast search. On the other hand, a computational storage
device (CSD) that combines programmable logic and storage modules on a single
board becomes popular to address the data bandwidth bottleneck of modern
computing systems. In this paper, we propose a computational storage platform
that can accelerate a large-scale graph-based nearest neighbor search algorithm
based on SmartSSD CSD. To this end, we modify the algorithm more amenable on
the hardware and implement two types of accelerators using HLS- and RTL-based
methodology with various optimization methods. In addition, we scale up the
proposed platform to have 4 SmartSSDs and apply graph parallelism to boost the
system performance further. As a result, the proposed computational storage
platform achieves 75.59 query per second throughput for the SIFT1B dataset at
258.66W power dissipation, which is 12.83x and 17.91x faster and 10.43x and
24.33x more energy efficient than the conventional CPU-based and GPU-based
server platform, respectively. With multi-terabyte storage and custom
acceleration capability, we believe that the proposed computational storage
platform is a promising solution for cost-sensitive cloud datacenters.</p>
</td>
    <td>
      
        Datasets 
      
        Scalability 
      
        Evaluation 
      
        Graph-Based-ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/jeong2022augmenting/">Augmenting Document Representations for Dense Retrieval with Interpolation and Perturbation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Augmenting Document Representations for Dense Retrieval with Interpolation and Perturbation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Augmenting Document Representations for Dense Retrieval with Interpolation and Perturbation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jeong Soyeong, Baek Jinheon, Cho Sukmin, Hwang Sung Ju, Park Jong C.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</td>
    <td>7</td>
    <td><p>Dense retrieval models, which aim at retrieving the most relevant document
for an input query on a dense representation space, have gained considerable
attention for their remarkable success. Yet, dense models require a vast amount
of labeled training data for notable performance, whereas it is often
challenging to acquire query-document pairs annotated by humans. To tackle this
problem, we propose a simple but effective Document Augmentation for dense
Retrieval (DAR) framework, which augments the representations of documents with
their interpolation and perturbation. We validate the performance of DAR on
retrieval tasks with two benchmark datasets, showing that the proposed DAR
significantly outperforms relevant baselines on the dense retrieval of both the
labeled and unlabeled documents.</p>
</td>
    <td>
      
        Datasets 
      
        Evaluation 
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/jang2021deep/">Deep Hash Distillation for Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Hash Distillation for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Hash Distillation for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jang Young Kyun, Gu Geonmo, Ko Byungsoo, Kang Isaac, Cho Nam Ik</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>23</td>
    <td><p>In hash-based image retrieval systems, degraded or transformed inputs usually
generate different codes from the original, deteriorating the retrieval
accuracy. To mitigate this issue, data augmentation can be applied during
training. However, even if augmented samples of an image are similar in real
feature space, the quantization can scatter them far away in Hamming space.
This results in representation discrepancies that can impede training and
degrade performance. In this work, we propose a novel self-distilled hashing
scheme to minimize the discrepancy while exploiting the potential of augmented
data. By transferring the hash knowledge of the weakly-transformed samples to
the strong ones, we make the hash code insensitive to various transformations.
We also introduce hash proxy-based similarity learning and binary cross
entropy-based quantization loss to provide fine quality hash codes. Ultimately,
we construct a deep hashing framework that not only improves the existing deep
hashing approaches, but also achieves the state-of-the-art retrieval results.
Extensive experiments are conducted and confirm the effectiveness of our work.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Neural-Hashing 
      
        Quantization 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/hui2022efficient/">Efficient 3D Point Cloud Feature Learning for Large-Scale Place Recognition</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient 3D Point Cloud Feature Learning for Large-Scale Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient 3D Point Cloud Feature Learning for Large-Scale Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hui Le, Cheng Mingmei, Xie Jin, Yang Jian</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>15</td>
    <td><p>Point cloud based retrieval for place recognition is still a challenging
problem due to drastic appearance and illumination changes of scenes in
changing environments. Existing deep learning based global descriptors for the
retrieval task usually consume a large amount of computation resources (e.g.,
memory), which may not be suitable for the cases of limited hardware resources.
In this paper, we develop an efficient point cloud learning network (EPC-Net)
to form a global descriptor for visual place recognition, which can obtain good
performance and reduce computation memory and inference time. First, we propose
a lightweight but effective neural network module, called ProxyConv, to
aggregate the local geometric features of point clouds. We leverage the spatial
adjacent matrix and proxy points to simplify the original edge convolution for
lower memory consumption. Then, we design a lightweight grouped VLAD network
(G-VLAD) to form global descriptors for retrieval. Compared with the original
VLAD network, we propose a grouped fully connected (GFC) layer to decompose the
high-dimensional vectors into a group of low-dimensional vectors, which can
reduce the number of parameters of the network and maintain the discrimination
of the feature vector. Finally, to further reduce the inference time, we
develop a simple version of EPC-Net, called EPC-Net-L, which consists of two
ProxyConv modules and one max pooling layer to aggregate global descriptors. By
distilling the knowledge from EPC-Net, EPC-Net-L can obtain discriminative
global descriptors for retrieval. Extensive experiments on the Oxford dataset
and three in-house datasets demonstrate that our proposed method can achieve
state-of-the-art performance with lower parameters, FLOPs, and runtime per
frame.</p>
</td>
    <td>
      
        Datasets 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/hui2021efficient/">Efficient 3D Point Cloud Feature Learning for Large-Scale Place Recognition</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient 3D Point Cloud Feature Learning for Large-Scale Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient 3D Point Cloud Feature Learning for Large-Scale Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hui Le, Cheng Mingmei, Xie Jin, Yang Jian</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>15</td>
    <td><p>Point cloud based retrieval for place recognition is still a challenging
problem due to drastic appearance and illumination changes of scenes in
changing environments. Existing deep learning based global descriptors for the
retrieval task usually consume a large amount of computation resources (e.g.,
memory), which may not be suitable for the cases of limited hardware resources.
In this paper, we develop an efficient point cloud learning network (EPC-Net)
to form a global descriptor for visual place recognition, which can obtain good
performance and reduce computation memory and inference time. First, we propose
a lightweight but effective neural network module, called ProxyConv, to
aggregate the local geometric features of point clouds. We leverage the spatial
adjacent matrix and proxy points to simplify the original edge convolution for
lower memory consumption. Then, we design a lightweight grouped VLAD network
(G-VLAD) to form global descriptors for retrieval. Compared with the original
VLAD network, we propose a grouped fully connected (GFC) layer to decompose the
high-dimensional vectors into a group of low-dimensional vectors, which can
reduce the number of parameters of the network and maintain the discrimination
of the feature vector. Finally, to further reduce the inference time, we
develop a simple version of EPC-Net, called EPC-Net-L, which consists of two
ProxyConv modules and one max pooling layer to aggregate global descriptors. By
distilling the knowledge from EPC-Net, EPC-Net-L can obtain discriminative
global descriptors for retrieval. Extensive experiments on the Oxford dataset
and three in-house datasets demonstrate that our proposed method can achieve
state-of-the-art performance with lower parameters, FLOPs, and runtime per
frame.</p>
</td>
    <td>
      
        Datasets 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/hu2022feature/">Feature Representation Learning for Unsupervised Cross-domain Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Feature Representation Learning for Unsupervised Cross-domain Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Feature Representation Learning for Unsupervised Cross-domain Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hu Conghui, Lee Gim Hee</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>12</td>
    <td><p>Current supervised cross-domain image retrieval methods can achieve excellent
performance. However, the cost of data collection and labeling imposes an
intractable barrier to practical deployment in real applications. In this
paper, we investigate the unsupervised cross-domain image retrieval task, where
class labels and pairing annotations are no longer a prerequisite for training.
This is an extremely challenging task because there is no supervision for both
in-domain feature representation learning and cross-domain alignment. We
address both challenges by introducing: 1) a new cluster-wise contrastive
learning mechanism to help extract class semantic-aware features, and 2) a
novel distance-of-distance loss to effectively measure and minimize the domain
discrepancy without any external supervision. Experiments on the Office-Home
and DomainNet datasets consistently show the superior image retrieval
accuracies of our framework over state-of-the-art approaches. Our source code
can be found at https://github.com/conghuihu/UCDIR.</p>
</td>
    <td>
      
        Supervised 
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
        Datasets 
      
        Unsupervised 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/hendriksen2021extending/">Extending CLIP for Category-to-image Retrieval in E-commerce</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Extending CLIP for Category-to-image Retrieval in E-commerce' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Extending CLIP for Category-to-image Retrieval in E-commerce' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hendriksen Mariya, Bleeker Maurits, Vakulenko Svitlana, van Noord Nanne, Kuiper Ernst, de Rijke Maarten</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>14</td>
    <td><p>E-commerce provides rich multimodal data that is barely leveraged in
practice. One aspect of this data is a category tree that is being used in
search and recommendation. However, in practice, during a user’s session there
is often a mismatch between a textual and a visual representation of a given
category. Motivated by the problem, we introduce the task of category-to-image
retrieval in e-commerce and propose a model for the task, CLIP-ITA. The model
leverages information from multiple modalities (textual, visual, and attribute
modality) to create product representations. We explore how adding information
from multiple modalities (textual, visual, and attribute modality) impacts the
model’s performance. In particular, we observe that CLIP-ITA significantly
outperforms a comparable model that leverages only the visual modality and a
comparable model that leverages the visual and attribute modality.</p>
</td>
    <td>
      
        Recommender-Systems 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/gui2022cross/">Cross-Language Binary-Source Code Matching with Intermediate Representations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-Language Binary-Source Code Matching with Intermediate Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-Language Binary-Source Code Matching with Intermediate Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gui Yi, Wan Yao, Zhang Hongyu, Huang Huifang, Sui Yulei, Xu Guandong, Shao Zhiyuan, Jin Hai</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)</td>
    <td>21</td>
    <td><p>Binary-source code matching plays an important role in many security and
software engineering related tasks such as malware detection, reverse
engineering and vulnerability assessment. Currently, several approaches have
been proposed for binary-source code matching by jointly learning the
embeddings of binary code and source code in a common vector space. Despite
much effort, existing approaches target on matching the binary code and source
code written in a single programming language. However, in practice, software
applications are often written in different programming languages to cater for
different requirements and computing platforms. Matching binary and source code
across programming languages introduces additional challenges when maintaining
multi-language and multi-platform applications. To this end, this paper
formulates the problem of cross-language binary-source code matching, and
develops a new dataset for this new problem. We present a novel approach XLIR,
which is a Transformer-based neural network by learning the intermediate
representations for both binary and source code. To validate the effectiveness
of XLIR, comprehensive experiments are conducted on two tasks of cross-language
binary-source code matching, and cross-language source-source code matching, on
top of our curated dataset. Experimental results and analysis show that our
proposed XLIR with intermediate representations significantly outperforms other
state-of-the-art models in both of the two tasks.</p>
</td>
    <td>
      
        Datasets 
      
        Compact-Codes 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/gu2022accelerating/">Accelerating Code Search with Deep Hashing and Code Classification</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Accelerating Code Search with Deep Hashing and Code Classification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Accelerating Code Search with Deep Hashing and Code Classification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gu Wenchao, Wang Yanlin, Du Lun, Zhang Hongyu, Han Shi, Zhang Dongmei, Lyu Michael R.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</td>
    <td>10</td>
    <td><p>Code search is to search reusable code snippets from source code corpus based
on natural languages queries. Deep learning-based methods of code search have
shown promising results. However, previous methods focus on retrieval accuracy
but lacked attention to the efficiency of the retrieval process. We propose a
novel method CoSHC to accelerate code search with deep hashing and code
classification, aiming to perform an efficient code search without sacrificing
too much accuracy. To evaluate the effectiveness of CoSHC, we apply our method
to five code search models. Extensive experimental results indicate that
compared with previous code search baselines, CoSHC can save more than 90% of
retrieval time meanwhile preserving at least 99% of retrieval accuracy.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Neural-Hashing 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/gu2021local/">Local Citation Recommendation with Hierarchical-Attention Text Encoder and SciBERT-based Reranking</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Local Citation Recommendation with Hierarchical-Attention Text Encoder and SciBERT-based Reranking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Local Citation Recommendation with Hierarchical-Attention Text Encoder and SciBERT-based Reranking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gu Nianlong, Gao Yingqiang, Hahnloser Richard H. R.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>14</td>
    <td><p>The goal of local citation recommendation is to recommend a missing reference
from the local citation context and optionally also from the global context. To
balance the tradeoff between speed and accuracy of citation recommendation in
the context of a large-scale paper database, a viable approach is to first
prefetch a limited number of relevant documents using efficient ranking methods
and then to perform a fine-grained reranking using more sophisticated models.
In that vein, BM25 has been found to be a tough-to-beat approach to
prefetching, which is why recent work has focused mainly on the reranking step.
Even so, we explore prefetching with nearest neighbor search among text
embeddings constructed by a hierarchical attention network. When coupled with a
SciBERT reranker fine-tuned on local citation recommendation tasks, our
hierarchical Attention encoder (HAtten) achieves high prefetch recall for a
given number of candidates to be reranked. Consequently, our reranker requires
fewer prefetch candidates to rerank, yet still achieves state-of-the-art
performance on various local citation recommendation datasets such as ACL-200,
FullTextPeerRead, RefSeer, and arXiv.</p>
</td>
    <td>
      
        Datasets 
      
        Recommender-Systems 
      
        Re-Ranking 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/gu2021cross/">Cross-modal Image Retrieval with Deep Mutual Information Maximization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-modal Image Retrieval with Deep Mutual Information Maximization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-modal Image Retrieval with Deep Mutual Information Maximization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gu Chunbin, Bu Jiajun, Zhou Xixi, Yao Chengwei, Ma Dongfang, Yu Zhi, Yan Xifeng</td> <!-- 🔧 You were missing this -->
    <td>Neurocomputing</td>
    <td>13</td>
    <td><p>In this paper, we study the cross-modal image retrieval, where the inputs
contain a source image plus some text that describes certain modifications to
this image and the desired image. Prior work usually uses a three-stage
strategy to tackle this task: 1) extract the features of the inputs; 2) fuse
the feature of the source image and its modified text to obtain fusion feature;
3) learn a similarity metric between the desired image and the source image +
modified text by using deep metric learning. Since classical image/text
encoders can learn the useful representation and common pair-based loss
functions of distance metric learning are enough for cross-modal retrieval,
people usually improve retrieval accuracy by designing new fusion networks.
However, these methods do not successfully handle the modality gap caused by
the inconsistent distribution and representation of the features of different
modalities, which greatly influences the feature fusion and similarity
learning. To alleviate this problem, we adopt the contrastive self-supervised
learning method Deep InforMax (DIM) to our approach to bridge this gap by
enhancing the dependence between the text, the image, and their fusion.
Specifically, our method narrows the modality gap between the text modality and
the image modality by maximizing mutual information between their not exactly
semantically identical representation. Moreover, we seek an effective common
subspace for the semantically same fusion feature and desired image’s feature
by utilizing Deep InforMax between the low-level layer of the image encoder and
the high-level layer of the fusion network. Extensive experiments on three
large-scale benchmark datasets show that we have bridged the modality gap
between different modalities and achieve state-of-the-art retrieval
performance.</p>
</td>
    <td>
      
        Supervised 
      
        Image-Retrieval 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Self-Supervised 
      
        Scalability 
      
        Evaluation 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/gong2022improving/">Improving Visual-Semantic Embeddings by Learning Semantically-Enhanced Hard Negatives for Cross-modal Information Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Improving Visual-Semantic Embeddings by Learning Semantically-Enhanced Hard Negatives for Cross-modal Information Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Improving Visual-Semantic Embeddings by Learning Semantically-Enhanced Hard Negatives for Cross-modal Information Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gong Yan, Cosma Georgina</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition</td>
    <td>13</td>
    <td><p>Visual Semantic Embedding (VSE) aims to extract the semantics of images and
their descriptions, and embed them into the same latent space for cross-modal
information retrieval. Most existing VSE networks are trained by adopting a
hard negatives loss function which learns an objective margin between the
similarity of relevant and irrelevant image-description embedding pairs.
However, the objective margin in the hard negatives loss function is set as a
fixed hyperparameter that ignores the semantic differences of the irrelevant
image-description pairs. To address the challenge of measuring the optimal
similarities between image-description pairs before obtaining the trained VSE
networks, this paper presents a novel approach that comprises two main parts:
(1) finds the underlying semantics of image descriptions; and (2) proposes a
novel semantically enhanced hard negatives loss function, where the learning
objective is dynamically determined based on the optimal similarity scores
between irrelevant image-description pairs. Extensive experiments were carried
out by integrating the proposed methods into five state-of-the-art VSE networks
that were applied to three benchmark datasets for cross-modal information
retrieval tasks. The results revealed that the proposed methods achieved the
best performance and can also be adopted by existing and future VSE networks.</p>
</td>
    <td>
      
        Evaluation 
      
        Datasets 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/dubey2021vision/">Vision Transformer Hashing for Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Vision Transformer Hashing for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Vision Transformer Hashing for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dubey Shiv Ram, Singh Satish Kumar, Chu Wei-ta</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE International Conference on Multimedia and Expo (ICME)</td>
    <td>47</td>
    <td><p>Deep learning has shown a tremendous growth in hashing techniques for image
retrieval. Recently, Transformer has emerged as a new architecture by utilizing
self-attention without convolution. Transformer is also extended to Vision
Transformer (ViT) for the visual recognition with a promising performance on
ImageNet. In this paper, we propose a Vision Transformer based Hashing (VTS)
for image retrieval. We utilize the pre-trained ViT on ImageNet as the backbone
network and add the hashing head. The proposed VTS model is fine tuned for
hashing under six different image retrieval frameworks, including Deep
Supervised Hashing (DSH), HashNet, GreedyHash, Improved Deep Hashing Network
(IDHN), Deep Polarized Network (DPN) and Central Similarity Quantization (CSQ)
with their objective functions. We perform the extensive experiments on
CIFAR10, ImageNet, NUS-Wide, and COCO datasets. The proposed VTS based image
retrieval outperforms the recent state-of-the-art hashing techniques with a
great margin. We also find the proposed VTS model as the backbone network is
better than the existing networks, such as AlexNet and ResNet. The code is
released at https://github.com/shivram1987/VisionTransformerHashing.</p>
</td>
    <td>
      
        Supervised 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Quantization 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/couairon2021embedding/">Embedding Arithmetic of Multimodal Queries for Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Embedding Arithmetic of Multimodal Queries for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Embedding Arithmetic of Multimodal Queries for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Couairon Guillaume, Cord Matthieu, Douze Matthijs, Schwenk Holger</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>13</td>
    <td><p>Latent text representations exhibit geometric regularities, such as the
famous analogy: queen is to king what woman is to man. Such structured semantic
relations were not demonstrated on image representations. Recent works aiming
at bridging this semantic gap embed images and text into a multimodal space,
enabling the transfer of text-defined transformations to the image modality. We
introduce the SIMAT dataset to evaluate the task of Image Retrieval with
Multimodal queries. SIMAT contains 6k images and 18k textual transformation
queries that aim at either replacing scene elements or changing pairwise
relationships between scene elements. The goal is to retrieve an image
consistent with the (source image, text transformation) query. We use an
image/text matching oracle (OSCAR) to assess whether the image transformation
is successful. The SIMAT dataset will be publicly available. We use SIMAT to
evaluate the geometric properties of multimodal embedding spaces trained with
an image/text matching objective, like CLIP. We show that vanilla CLIP
embeddings are not very well suited to transform images with delta vectors, but
that a simple finetuning on the COCO dataset can bring dramatic improvements.
We also study whether it is beneficial to leverage pretrained universal
sentence encoders (FastText, LASER and LaBSE).</p>
</td>
    <td>
      
        Datasets 
      
        CVPR 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/coleman2020similarity/">Similarity Search for Efficient Active Learning and Search of Rare Concepts</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Similarity Search for Efficient Active Learning and Search of Rare Concepts' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Similarity Search for Efficient Active Learning and Search of Rare Concepts' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Coleman Cody, Chou Edward, Katz-samuels Julian, Culatana Sean, Bailis Peter, Berg Alexander C., Nowak Robert, Sumbaly Roshan, Zaharia Matei, Yalniz I. Zeki</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>16</td>
    <td><p>Many active learning and search approaches are intractable for large-scale
industrial settings with billions of unlabeled examples. Existing approaches
search globally for the optimal examples to label, scaling linearly or even
quadratically with the unlabeled data. In this paper, we improve the
computational efficiency of active learning and search methods by restricting
the candidate pool for labeling to the nearest neighbors of the currently
labeled set instead of scanning over all of the unlabeled data. We evaluate
several selection strategies in this setting on three large-scale computer
vision datasets: ImageNet, OpenImages, and a de-identified and aggregated
dataset of 10 billion images provided by a large internet company. Our approach
achieved similar mean average precision and recall as the traditional global
approach while reducing the computational cost of selection by up to three
orders of magnitude, thus enabling web-scale active learning.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Datasets 
      
        AAAI 
      
        Scalability 
      
        Large-Scale-Search 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/chen2022learning/">Learning Binarized Graph Representations with Multi-faceted Quantization Reinforcement for Top-K Recommendation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Binarized Graph Representations with Multi-faceted Quantization Reinforcement for Top-K Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Binarized Graph Representations with Multi-faceted Quantization Reinforcement for Top-K Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen Yankai, Guo Huifeng, Zhang Yingxue, Ma Chen, Tang Ruiming, Li Jingjie, King Irwin</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</td>
    <td>24</td>
    <td><p>Learning vectorized embeddings is at the core of various recommender systems
for user-item matching. To perform efficient online inference, representation
quantization, aiming to embed the latent features by a compact sequence of
discrete numbers, recently shows the promising potentiality in optimizing both
memory and computation overheads. However, existing work merely focuses on
numerical quantization whilst ignoring the concomitant information loss issue,
which, consequently, leads to conspicuous performance degradation. In this
paper, we propose a novel quantization framework to learn Binarized Graph
Representations for Top-K Recommendation (BiGeaR). BiGeaR introduces
multi-faceted quantization reinforcement at the pre-, mid-, and post-stage of
binarized representation learning, which substantially retains the
representation informativeness against embedding binarization. In addition to
saving the memory footprint, BiGeaR further develops solid online inference
acceleration with bitwise operations, providing alternative flexibility for the
realistic deployment. The empirical results over five large real-world
benchmarks show that BiGeaR achieves about 22%~40% performance improvement over
the state-of-the-art quantization-based recommender system, and recovers about
95%~102% of the performance capability of the best full-precision counterpart
with over 8x time and space reduction.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Recommender-Systems 
      
        KDD 
      
        Memory-Efficiency 
      
        Quantization 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/chen2022intra/">Intra-Modal Constraint Loss For Image-Text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Intra-Modal Constraint Loss For Image-Text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Intra-Modal Constraint Loss For Image-Text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen Jianan, Zhang Lu, Wang Qiong, Bai Cong, Kpalma Kidiyo</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE International Conference on Image Processing (ICIP)</td>
    <td>6</td>
    <td><p>Cross-modal retrieval has drawn much attention in both computer vision and
natural language processing domains. With the development of convolutional and
recurrent neural networks, the bottleneck of retrieval across image-text
modalities is no longer the extraction of image and text features but an
efficient loss function learning in embedding space. Many loss functions try to
closer pairwise features from heterogeneous modalities. This paper proposes a
method for learning joint embedding of images and texts using an intra-modal
constraint loss function to reduce the violation of negative pairs from the
same homogeneous modality. Experimental results show that our approach
outperforms state-of-the-art bi-directional image-text retrieval methods on
Flickr30K and Microsoft COCO datasets. Our code is publicly available:
https://github.com/CanonChen/IMC.</p>
</td>
    <td>
      
        Datasets 
      
        Text-Retrieval 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/chen2022multi/">Multi-Level Visual Similarity Based Personalized Tourist Attraction Recommendation Using Geo-Tagged Photos</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multi-Level Visual Similarity Based Personalized Tourist Attraction Recommendation Using Geo-Tagged Photos' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multi-Level Visual Similarity Based Personalized Tourist Attraction Recommendation Using Geo-Tagged Photos' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen Ling, Lyu Dandan, Yu Shanshan, Chen Gencai</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>19</td>
    <td><p>Geo-tagged photo based tourist attraction recommendation can discover users’
travel preferences from their taken photos, so as to recommend suitable tourist
attractions to them. However, existing visual content based methods cannot
fully exploit the user and tourist attraction information of photos to extract
visual features, and do not differentiate the significances of different
photos. In this paper, we propose multi-level visual similarity based
personalized tourist attraction recommendation using geo-tagged photos (MEAL).
MEAL utilizes the visual contents of photos and interaction behavior data to
obtain the final embeddings of users and tourist attractions, which are then
used to predict the visit probabilities. Specifically, by crossing the user and
tourist attraction information of photos, we define four visual similarity
levels and introduce a corresponding quintuplet loss to embed the visual
contents of photos. In addition, to capture the significances of different
photos, we exploit the self-attention mechanism to obtain the visual
representations of users and tourist attractions. We conducted experiments on a
dataset crawled from Flickr, and the experimental results proved the advantage
of this method.</p>
</td>
    <td>
      
        Datasets 
      
        Recommender-Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/chen2022approximate/">Approximate Nearest Neighbor Search under Neural Similarity Metric for Large-Scale Recommendation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Approximate Nearest Neighbor Search under Neural Similarity Metric for Large-Scale Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Approximate Nearest Neighbor Search under Neural Similarity Metric for Large-Scale Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen Rihan, Liu Bin, Zhu Han, Wang Yaoxuan, Li Qi, Ma Buting, Hua Qingbo, Jiang Jun, Xu Yunlong, Deng Hongbo, Zheng Bo</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</td>
    <td>12</td>
    <td><p>Model-based methods for recommender systems have been studied extensively for
years. Modern recommender systems usually resort to 1) representation learning
models which define user-item preference as the distance between their
embedding representations, and 2) embedding-based Approximate Nearest Neighbor
(ANN) search to tackle the efficiency problem introduced by large-scale corpus.
While providing efficient retrieval, the embedding-based retrieval pattern also
limits the model capacity since the form of user-item preference measure is
restricted to the distance between their embedding representations. However,
for other more precise user-item preference measures, e.g., preference scores
directly derived from a deep neural network, they are computationally
intractable because of the lack of an efficient retrieval method, and an
exhaustive search for all user-item pairs is impractical. In this paper, we
propose a novel method to extend ANN search to arbitrary matching functions,
e.g., a deep neural network. Our main idea is to perform a greedy walk with a
matching function in a similarity graph constructed from all items. To solve
the problem that the similarity measures of graph construction and user-item
matching function are heterogeneous, we propose a pluggable adversarial
training task to ensure the graph search with arbitrary matching function can
achieve fairly high precision. Experimental results in both open source and
industry datasets demonstrate the effectiveness of our method. The proposed
method has been fully deployed in the Taobao display advertising platform and
brings a considerable advertising revenue increase. We also summarize our
detailed experiences in deployment in this paper.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Scalability 
      
        Efficiency 
      
        Evaluation 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        CIKM 
      
        Recommender-Systems 
      
        Graph-Based-ANN 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/biten2021is/">Is An Image Worth Five Sentences? A New Look into Semantics for Image-Text Matching</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Is An Image Worth Five Sentences? A New Look into Semantics for Image-Text Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Is An Image Worth Five Sentences? A New Look into Semantics for Image-Text Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Biten Ali Furkan, Mafla Andres, Gomez Lluis, Karatzas Dimosthenis</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>17</td>
    <td><p>The task of image-text matching aims to map representations from different
modalities into a common joint visual-textual embedding. However, the most
widely used datasets for this task, MSCOCO and Flickr30K, are actually image
captioning datasets that offer a very limited set of relationships between
images and sentences in their ground-truth annotations. This limited ground
truth information forces us to use evaluation metrics based on binary
relevance: given a sentence query we consider only one image as relevant.
However, many other relevant images or captions may be present in the dataset.
In this work, we propose two metrics that evaluate the degree of semantic
relevance of retrieved items, independently of their annotated binary
relevance. Additionally, we incorporate a novel strategy that uses an image
captioning metric, CIDEr, to define a Semantic Adaptive Margin (SAM) to be
optimized in a standard triplet loss. By incorporating our formulation to
existing models, a <em>large</em> improvement is obtained in scenarios where
available training data is limited. We also demonstrate that the performance on
the annotated image-caption pairs is maintained while improving on other
non-annotated relevant items when employing the full training set. Code with
our metrics and adaptive margin formulation will be made public.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/bogolin2021cross/">Cross Modal Retrieval with Querybank Normalisation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross Modal Retrieval with Querybank Normalisation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross Modal Retrieval with Querybank Normalisation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Bogolin Simion-vlad, Croitoru Ioana, Jin Hailin, Liu Yang, Albanie Samuel</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>58</td>
    <td><p>Profiting from large-scale training datasets, advances in neural architecture
design and efficient inference, joint embeddings have become the dominant
approach for tackling cross-modal retrieval. In this work we first show that,
despite their effectiveness, state-of-the-art joint embeddings suffer
significantly from the longstanding “hubness problem” in which a small number
of gallery embeddings form the nearest neighbours of many queries. Drawing
inspiration from the NLP literature, we formulate a simple but effective
framework called Querybank Normalisation (QB-Norm) that re-normalises query
similarities to account for hubs in the embedding space. QB-Norm improves
retrieval performance without requiring retraining. Differently from prior
work, we show that QB-Norm works effectively without concurrent access to any
test set queries. Within the QB-Norm framework, we also propose a novel
similarity normalisation method, the Dynamic Inverted Softmax, that is
significantly more robust than existing approaches. We showcase QB-Norm across
a range of cross modal retrieval models and benchmarks where it consistently
enhances strong baselines beyond the state of the art. Code is available at
https://vladbogo.github.io/QB-Norm/.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Datasets 
      
        CVPR 
      
        Scalability 
      
        Evaluation 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/bhunia2022adaptive/">Adaptive Fine-Grained Sketch-Based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Adaptive Fine-Grained Sketch-Based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Adaptive Fine-Grained Sketch-Based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Bhunia Ayan Kumar, Sain Aneeshan, Shah Parth, Gupta Animesh, Chowdhury Pinaki Nath, Xiang Tao, Song Yi-zhe</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>20</td>
    <td><p>The recent focus on Fine-Grained Sketch-Based Image Retrieval (FG-SBIR) has
shifted towards generalising a model to new categories without any training
data from them. In real-world applications, however, a trained FG-SBIR model is
often applied to both new categories and different human sketchers, i.e.,
different drawing styles. Although this complicates the generalisation problem,
fortunately, a handful of examples are typically available, enabling the model
to adapt to the new category/style. In this paper, we offer a novel perspective
– instead of asking for a model that generalises, we advocate for one that
quickly adapts, with just very few samples during testing (in a few-shot
manner). To solve this new problem, we introduce a novel model-agnostic
meta-learning (MAML) based framework with several key modifications: (1) As a
retrieval task with a margin-based contrastive loss, we simplify the MAML
training in the inner loop to make it more stable and tractable. (2) The margin
in our contrastive loss is also meta-learned with the rest of the model. (3)
Three additional regularisation losses are introduced in the outer loop, to
make the meta-learned FG-SBIR model more effective for category/style
adaptation. Extensive experiments on public datasets suggest a large gain over
generalisation and zero-shot based approaches, and a few strong few-shot
baselines.</p>
</td>
    <td>
      
        Few-Shot-&-Zero-Shot 
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
        Distance-Metric-Learning 
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/an2020fast/">Fast and Incremental Loop Closure Detection with Deep Features and Proximity Graphs</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast and Incremental Loop Closure Detection with Deep Features and Proximity Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast and Incremental Loop Closure Detection with Deep Features and Proximity Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>An Shan, Zhu Haogang, Wei Dong, Tsintotas Konstantinos A., Gasteratos Antonios</td> <!-- 🔧 You were missing this -->
    <td>Journal of Field Robotics</td>
    <td>44</td>
    <td><p>In recent years, the robotics community has extensively examined methods
concerning the place recognition task within the scope of simultaneous
localization and mapping applications.This article proposes an appearance-based
loop closure detection pipeline named ``FILD++” (Fast and Incremental Loop
closure Detection).First, the system is fed by consecutive images and, via
passing them twice through a single convolutional neural network, global and
local deep features are extracted.Subsequently, a hierarchical navigable
small-world graph incrementally constructs a visual database representing the
robot’s traversed path based on the computed global features.Finally, a query
image, grabbed each time step, is set to retrieve similar locations on the
traversed route.An image-to-image pairing follows, which exploits local
features to evaluate the spatial information. Thus, in the proposed article, we
propose a single network for global and local feature extraction in contrast to
our previous work (FILD), while an exhaustive search for the verification
process is adopted over the generated deep local features avoiding the
utilization of hash codes. Exhaustive experiments on eleven publicly available
datasets exhibit the system’s high performance (achieving the highest recall
score on eight of them) and low execution times (22.05 ms on average in New
College, which is the largest one containing 52480 images) compared to other
state-of-the-art approaches.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        Evaluation 
      
        Graph-Based-ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/aksoy2022satellite/">Satellite Image Search in AgoraEO</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Satellite Image Search in AgoraEO' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Satellite Image Search in AgoraEO' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Aksoy Ahmet Kerem, Dushev Pavel, Zacharatou Eleni Tzirita, Hemsen Holmer, Charfuelan Marcela, Quiané-ruiz Jorge-arnulfo, Demir Begüm, Markl Volker</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the VLDB Endowment</td>
    <td>8</td>
    <td><p>The growing operational capability of global Earth Observation (EO) creates
new opportunities for data-driven approaches to understand and protect our
planet. However, the current use of EO archives is very restricted due to the
huge archive sizes and the limited exploration capabilities provided by EO
platforms. To address this limitation, we have recently proposed MiLaN, a
content-based image retrieval approach for fast similarity search in satellite
image archives. MiLaN is a deep hashing network based on metric learning that
encodes high-dimensional image features into compact binary hash codes. We use
these codes as keys in a hash table to enable real-time nearest neighbor search
and highly accurate retrieval. In this demonstration, we showcase the
efficiency of MiLaN by integrating it with EarthQube, a browser and search
engine within AgoraEO. EarthQube supports interactive visual exploration and
Query-by-Example over satellite image repositories. Demo visitors will interact
with EarthQube playing the role of different users that search images in a
large-scale remote sensing archive by their semantic content and apply other
filters.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Distance-Metric-Learning 
      
        Neural-Hashing 
      
        Scalability 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/adir2022privacy/">Privacy-preserving record linkage using local sensitive hash and private set intersection</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Privacy-preserving record linkage using local sensitive hash and private set intersection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Privacy-preserving record linkage using local sensitive hash and private set intersection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Adir Allon, Aharoni Ehud, Drucker Nir, Kushnir Eyal, Masalha Ramy, Mirkin Michael, Soceanu Omri</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>5</td>
    <td><p>The amount of data stored in data repositories increases every year. This
makes it challenging to link records between different datasets across
companies and even internally, while adhering to privacy regulations. Address
or name changes, and even different spelling used for entity data, can prevent
companies from using private deduplication or record-linking solutions such as
private set intersection (PSI). To this end, we propose a new and efficient
privacy-preserving record linkage (PPRL) protocol that combines PSI and local
sensitive hash (LSH) functions, and runs in linear time. We explain the privacy
guarantees that our protocol provides and demonstrate its practicality by
executing the protocol over two datasets with \(2^{20}\) records each, in \(11-45\)
minutes, depending on network settings.</p>
</td>
    <td>
      
        Datasets 
      
        Locality-Sensitive-Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/zhan2021learning/">Learning Discrete Representations via Constrained Clustering for Effective and Efficient Dense Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Discrete Representations via Constrained Clustering for Effective and Efficient Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Discrete Representations via Constrained Clustering for Effective and Efficient Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhan Jingtao, Mao Jiaxin, Liu Yiqun, Guo Jiafeng, Zhang Min, Ma Shaoping</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining</td>
    <td>32</td>
    <td><p>Dense Retrieval (DR) has achieved state-of-the-art first-stage ranking
effectiveness. However, the efficiency of most existing DR models is limited by
the large memory cost of storing dense vectors and the time-consuming nearest
neighbor search (NNS) in vector space. Therefore, we present RepCONC, a novel
retrieval model that learns discrete Representations via CONstrained
Clustering. RepCONC jointly trains dual-encoders and the Product Quantization
(PQ) method to learn discrete document representations and enables fast
approximate NNS with compact indexes. It models quantization as a constrained
clustering process, which requires the document embeddings to be uniformly
clustered around the quantization centroids and supports end-to-end
optimization of the quantization method and dual-encoders. We theoretically
demonstrate the importance of the uniform clustering constraint in RepCONC and
derive an efficient approximate solution for constrained clustering by reducing
it to an instance of the optimal transport problem. Besides constrained
clustering, RepCONC further adopts a vector-based inverted file system (IVF) to
support highly efficient vector search on CPUs. Extensive experiments on two
popular ad-hoc retrieval benchmarks show that RepCONC achieves better ranking
effectiveness than competitive vector quantization baselines under different
compression ratio settings. It also substantially outperforms a wide range of
existing retrieval models in terms of retrieval effectiveness, memory
efficiency, and time efficiency.</p>
</td>
    <td>
      
        Quantization 
      
        Vector-Indexing 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/yeh2022embedding/">Embedding Compression with Hashing for Efficient Representation Learning in Large-Scale Graph</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Embedding Compression with Hashing for Efficient Representation Learning in Large-Scale Graph' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Embedding Compression with Hashing for Efficient Representation Learning in Large-Scale Graph' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yeh Chin-chia Michael, Gu Mengting, Zheng Yan, Chen Huiyuan, Ebrahimi Javid, Zhuang Zhongfang, Wang Junpeng, Wang Liang, Zhang Wei</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</td>
    <td>13</td>
    <td><p>Graph neural networks (GNNs) are deep learning models designed specifically
for graph data, and they typically rely on node features as the input to the
first layer. When applying such a type of network on the graph without node
features, one can extract simple graph-based node features (e.g., number of
degrees) or learn the input node representations (i.e., embeddings) when
training the network. While the latter approach, which trains node embeddings,
more likely leads to better performance, the number of parameters associated
with the embeddings grows linearly with the number of nodes. It is therefore
impractical to train the input node embeddings together with GNNs within
graphics processing unit (GPU) memory in an end-to-end fashion when dealing
with industrial-scale graph data. Inspired by the embedding compression methods
developed for natural language processing (NLP) tasks, we develop a node
embedding compression method where each node is compactly represented with a
bit vector instead of a floating-point vector. The parameters utilized in the
compression method can be trained together with GNNs. We show that the proposed
node embedding compression method achieves superior performance compared to the
alternatives.</p>
</td>
    <td>
      
        Graph-Based-ANN 
      
        Hashing-Methods 
      
        KDD 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/xue2022cross/">Cross-Scale Context Extracted Hashing for Fine-Grained Image Binary Encoding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-Scale Context Extracted Hashing for Fine-Grained Image Binary Encoding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-Scale Context Extracted Hashing for Fine-Grained Image Binary Encoding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xue Xuetong, Shi Jiaying, He Xinxue, Xu Shenghui, Pan Zhaoming</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>Deep hashing has been widely applied to large-scale image retrieval tasks
owing to efficient computation and low storage cost by encoding
high-dimensional image data into binary codes. Since binary codes do not
contain as much information as float features, the essence of binary encoding
is preserving the main context to guarantee retrieval quality. However, the
existing hashing methods have great limitations on suppressing redundant
background information and accurately encoding from Euclidean space to Hamming
space by a simple sign function. In order to solve these problems, a
Cross-Scale Context Extracted Hashing Network (CSCE-Net) is proposed in this
paper. Firstly, we design a two-branch framework to capture fine-grained local
information while maintaining high-level global semantic information. Besides,
Attention guided Information Extraction module (AIE) is introduced between two
branches, which suppresses areas of low context information cooperated with
global sliding windows. Unlike previous methods, our CSCE-Net learns a
content-related Dynamic Sign Function (DSF) to replace the original simple sign
function. Therefore, the proposed CSCE-Net is context-sensitive and able to
perform well on accurate image binary encoding. We further demonstrate that our
CSCE-Net is superior to the existing hashing methods, which improves retrieval
performance on standard benchmarks.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Memory-Efficiency 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/xiao2022deeply/">Deeply Activated Salient Region for Instance Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deeply Activated Salient Region for Instance Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deeply Activated Salient Region for Instance Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xiao Hui-chu, Zhao Wan-lei, Lin Jie, Ngo Chong-wah</td> <!-- 🔧 You were missing this -->
    <td>ACM Transactions on Multimedia Computing, Communications, and Applications</td>
    <td>5</td>
    <td><p>The performance of instance search depends heavily on the ability to locate
and describe a wide variety of object instances in a video/image collection.
Due to the lack of proper mechanism in locating instances and deriving feature
representation, instance search is generally only effective for retrieving
instances of known object categories. In this paper, a simple but effective
instance-level feature representation is presented. Different from other
approaches, the issues in class-agnostic instance localization and distinctive
feature representation are considered. The former is achieved by detecting
salient instance regions from an image by a layer-wise back-propagation
process. The back-propagation starts from the last convolution layer of a
pre-trained CNN that is originally used for classification. The
back-propagation proceeds layer-by-layer until it reaches the input layer. This
allows the salient instance regions in the input image from both known and
unknown categories to be activated. Each activated salient region covers the
full or more usually a major range of an instance. The distinctive feature
representation is produced by average-pooling on the feature map of certain
layer with the detected instance region. Experiments show that such kind of
feature representation demonstrates considerably better performance over most
of the existing approaches. In addition, we show that the proposed feature
descriptor is also suitable for content-based image search.</p>
</td>
    <td>
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/xiao2022progressively/">Progressively Optimized Bi-Granular Document Representation for Scalable Embedding Based Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Progressively Optimized Bi-Granular Document Representation for Scalable Embedding Based Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Progressively Optimized Bi-Granular Document Representation for Scalable Embedding Based Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xiao Shitao, Liu Zheng, Han Weihao, Zhang Jianjin, Shao Yingxia, Lian Defu, Li Chaozhuo, Sun Hao, Deng Denvy, Zhang Liangjie, Zhang Qi, Xie Xing</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the ACM Web Conference 2022</td>
    <td>14</td>
    <td><p>Ad-hoc search calls for the selection of appropriate answers from a
massive-scale corpus. Nowadays, the embedding-based retrieval (EBR) becomes a
promising solution, where deep learning based document representation and ANN
search techniques are allied to handle this task. However, a major challenge is
that the ANN index can be too large to fit into memory, given the considerable
size of answer corpus. In this work, we tackle this problem with Bi-Granular
Document Representation, where the lightweight sparse embeddings are indexed
and standby in memory for coarse-grained candidate search, and the heavyweight
dense embeddings are hosted in disk for fine-grained post verification. For the
best of retrieval accuracy, a Progressive Optimization framework is designed.
The sparse embeddings are learned ahead for high-quality search of candidates.
Conditioned on the candidate distribution induced by the sparse embeddings, the
dense embeddings are continuously learned to optimize the discrimination of
ground-truth from the shortlisted candidates. Besides, two techniques: the
contrastive quantization and the locality-centric sampling are introduced for
the learning of sparse and dense embeddings, which substantially contribute to
their performances. Thanks to the above features, our method effectively
handles massive-scale EBR with strong advantages in accuracy: with up to +4.3%
recall gain on million-scale corpus, and up to +17.5% recall gain on
billion-scale corpus. Besides, Our method is applied to a major sponsored
search platform with substantial gains on revenue (+1.95%), Recall (+1.01%) and
CTR (+0.49%). Our code is available at https://github.com/microsoft/BiDR.</p>
</td>
    <td>
      
        Vector-Indexing 
      
        Tools-&-Libraries 
      
        Quantization 
      
        Scalability 
      
        Large-Scale-Search 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/wu2022learning/">Learning Deep Semantic Model for Code Search using CodeSearchNet Corpus</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Deep Semantic Model for Code Search using CodeSearchNet Corpus' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Deep Semantic Model for Code Search using CodeSearchNet Corpus' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu Chen, Yan Ming</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>6</td>
    <td><p>Semantic code search is the task of retrieving relevant code snippet given a
natural language query. Different from typical information retrieval tasks,
code search requires to bridge the semantic gap between the programming
language and natural language, for better describing intrinsic concepts and
semantics. Recently, deep neural network for code search has been a hot
research topic. Typical methods for neural code search first represent the code
snippet and query text as separate embeddings, and then use vector distance
(e.g. dot-product or cosine) to calculate the semantic similarity between them.
There exist many different ways for aggregating the variable length of code or
query tokens into a learnable embedding, including bi-encoder, cross-encoder,
and poly-encoder. The goal of the query encoder and code encoder is to produce
embeddings that are close with each other for a related pair of query and the
corresponding desired code snippet, in which the choice and design of encoder
is very significant.
  In this paper, we propose a novel deep semantic model which makes use of the
utilities of not only the multi-modal sources, but also feature extractors such
as self-attention, the aggregated vectors, combination of the intermediate
representations. We apply the proposed model to tackle the CodeSearchNet
challenge about semantic code search. We align cross-lingual embedding for
multi-modality learning with large batches and hard example mining, and combine
different learned representations for better enhancing the representation
learning. Our model is trained on CodeSearchNet corpus and evaluated on the
held-out data, the final model achieves 0.384 NDCG and won the first place in
this benchmark. Models and code are available at
https://github.com/overwindows/SemanticCodeSearch.git.</p>
</td>
    <td>
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/xiao2020deeply/">Deeply Activated Salient Region for Instance Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deeply Activated Salient Region for Instance Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deeply Activated Salient Region for Instance Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xiao Hui-chu, Zhao Wan-lei, Lin Jie, Ngo Chong-wah</td> <!-- 🔧 You were missing this -->
    <td>ACM Transactions on Multimedia Computing, Communications, and Applications</td>
    <td>5</td>
    <td><p>The performance of instance search depends heavily on the ability to locate
and describe a wide variety of object instances in a video/image collection.
Due to the lack of proper mechanism in locating instances and deriving feature
representation, instance search is generally only effective for retrieving
instances of known object categories. In this paper, a simple but effective
instance-level feature representation is presented. Different from other
approaches, the issues in class-agnostic instance localization and distinctive
feature representation are considered. The former is achieved by detecting
salient instance regions from an image by a layer-wise back-propagation
process. The back-propagation starts from the last convolution layer of a
pre-trained CNN that is originally used for classification. The
back-propagation proceeds layer-by-layer until it reaches the input layer. This
allows the salient instance regions in the input image from both known and
unknown categories to be activated. Each activated salient region covers the
full or more usually a major range of an instance. The distinctive feature
representation is produced by average-pooling on the feature map of certain
layer with the detected instance region. Experiments show that such kind of
feature representation demonstrates considerably better performance over most
of the existing approaches. In addition, we show that the proposed feature
descriptor is also suitable for content-based image search.</p>
</td>
    <td>
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/weinzaepfel2022learning/">Learning Super-Features for Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Super-Features for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Super-Features for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Weinzaepfel Philippe, Lucas Thomas, Larlus Diane, Kalantidis Yannis</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>11</td>
    <td><p>Methods that combine local and global features have recently shown excellent
performance on multiple challenging deep image retrieval benchmarks, but their
use of local features raises at least two issues. First, these local features
simply boil down to the localized map activations of a neural network, and
hence can be extremely redundant. Second, they are typically trained with a
global loss that only acts on top of an aggregation of local features; by
contrast, testing is based on local feature matching, which creates a
discrepancy between training and testing. In this paper, we propose a novel
architecture for deep image retrieval, based solely on mid-level features that
we call Super-features. These Super-features are constructed by an iterative
attention module and constitute an ordered set in which each element focuses on
a localized and discriminant image pattern. For training, they require only
image labels. A contrastive loss operates directly at the level of
Super-features and focuses on those that match across images. A second
complementary loss encourages diversity. Experiments on common landmark
retrieval benchmarks validate that Super-features substantially outperform
state-of-the-art methods when using the same number of features, and only
require a significantly smaller memory footprint to match their performance.
Code and models are available at: https://github.com/naver/FIRe.</p>
</td>
    <td>
      
        Memory-Efficiency 
      
        Distance-Metric-Learning 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/wei2022accurate/">Accurate Instance-Level CAD Model Retrieval in a Large-Scale Database</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Accurate Instance-Level CAD Model Retrieval in a Large-Scale Database' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Accurate Instance-Level CAD Model Retrieval in a Large-Scale Database' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wei Jiaxin, Hu Lan, Wang Chenyu, Kneip Laurent</td> <!-- 🔧 You were missing this -->
    <td>2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</td>
    <td>5</td>
    <td><p>We present a new solution to the fine-grained retrieval of clean CAD models
from a large-scale database in order to recover detailed object shape
geometries for RGBD scans. Unlike previous work simply indexing into a
moderately small database using an object shape descriptor and accepting the
top retrieval result, we argue that in the case of a large-scale database a
more accurate model may be found within a neighborhood of the descriptor. More
importantly, we propose that the distinctiveness deficiency of shape
descriptors at the instance level can be compensated by a geometry-based
re-ranking of its neighborhood. Our approach first leverages the discriminative
power of learned representations to distinguish between different categories of
models and then uses a novel robust point set distance metric to re-rank the
CAD neighborhood, enabling fine-grained retrieval in a large shape database.
Evaluation on a real-world dataset shows that our geometry-based re-ranking is
a conceptually simple but highly effective method that can lead to a
significant improvement in retrieval accuracy compared to the state-of-the-art.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Hybrid-ANN-Methods 
      
        Re-Ranking 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/xin2021zero/">Zero-Shot Dense Retrieval with Momentum Adversarial Domain Invariant Representations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Zero-Shot Dense Retrieval with Momentum Adversarial Domain Invariant Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Zero-Shot Dense Retrieval with Momentum Adversarial Domain Invariant Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xin Ji, Xiong Chenyan, Srinivasan Ashwin, Sharma Ankita, Jose Damien, Bennett Paul N.</td> <!-- 🔧 You were missing this -->
    <td>Findings of the Association for Computational Linguistics: ACL 2022</td>
    <td>17</td>
    <td><p>Dense retrieval (DR) methods conduct text retrieval by first encoding texts
in the embedding space and then matching them by nearest neighbor search. This
requires strong locality properties from the representation space, i.e, the
close allocations of each small group of relevant texts, which are hard to
generalize to domains without sufficient training data. In this paper, we aim
to improve the generalization ability of DR models from source training domains
with rich supervision signals to target domains without any relevant labels, in
the zero-shot setting. To achieve that, we propose Momentum adversarial Domain
Invariant Representation learning (MoDIR), which introduces a momentum method
in the DR training process to train a domain classifier distinguishing source
versus target, and then adversarially updates the DR encoder to learn domain
invariant representations. Our experiments show that MoDIR robustly outperforms
its baselines on 10+ ranking datasets from the BEIR benchmark in the zero-shot
setup, with more than 10% relative gains on datasets with enough sensitivity
for DR models’ evaluation. Source code of this paper will be released.</p>
</td>
    <td>
      
        Few-Shot-&-Zero-Shot 
      
        Text-Retrieval 
      
        Datasets 
      
        Evaluation 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/wang2022neural/">A Neural Corpus Indexer for Document Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Neural Corpus Indexer for Document Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Neural Corpus Indexer for Document Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Yujing, Hou Yingyan, Wang Haonan, Miao Ziming, Wu Shibin, Sun Hao, Chen Qi, Xia Yuqing, Chi Chengmin, Zhao Guoshuai, Liu Zheng, Xie Xing, Sun Hao Allen, Deng Weiwei, Zhang Qi, Yang Mao</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>43</td>
    <td><p>Current state-of-the-art document retrieval solutions mainly follow an
index-retrieve paradigm, where the index is hard to be directly optimized for
the final retrieval target. In this paper, we aim to show that an end-to-end
deep neural network unifying training and indexing stages can significantly
improve the recall performance of traditional methods. To this end, we propose
Neural Corpus Indexer (NCI), a sequence-to-sequence network that generates
relevant document identifiers directly for a designated query. To optimize the
recall performance of NCI, we invent a prefix-aware weight-adaptive decoder
architecture, and leverage tailored techniques including query generation,
semantic document identifiers, and consistency-based regularization. Empirical
studies demonstrated the superiority of NCI on two commonly used academic
benchmarks, achieving +21.4% and +16.8% relative enhancement for Recall@1 on
NQ320k dataset and R-Precision on TriviaQA dataset, respectively, compared to
the best baseline method.</p>
</td>
    <td>
      
        Datasets 
      
        Text-Retrieval 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/wang2022english/">English Contrastive Learning Can Learn Universal Cross-lingual Sentence Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=English Contrastive Learning Can Learn Universal Cross-lingual Sentence Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=English Contrastive Learning Can Learn Universal Cross-lingual Sentence Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Yau-shian, Wu Ashley, Neubig Graham</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</td>
    <td>9</td>
    <td><p>Universal cross-lingual sentence embeddings map semantically similar
cross-lingual sentences into a shared embedding space. Aligning cross-lingual
sentence embeddings usually requires supervised cross-lingual parallel
sentences. In this work, we propose mSimCSE, which extends SimCSE to
multilingual settings and reveal that contrastive learning on English data can
surprisingly learn high-quality universal cross-lingual sentence embeddings
without any parallel data. In unsupervised and weakly supervised settings,
mSimCSE significantly improves previous sentence embedding methods on
cross-lingual retrieval and multilingual STS tasks. The performance of
unsupervised mSimCSE is comparable to fully supervised methods in retrieving
low-resource languages and multilingual STS. The performance can be further
enhanced when cross-lingual NLI data is available. Our code is publicly
available at https://github.com/yaushian/mSimCSE.</p>
</td>
    <td>
      
        Unsupervised 
      
        Supervised 
      
        Evaluation 
      
        Self-Supervised 
      
        EMNLP 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/wang2022cross/">Cross-Lingual Cross-Modal Retrieval with Noise-Robust Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-Lingual Cross-Modal Retrieval with Noise-Robust Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-Lingual Cross-Modal Retrieval with Noise-Robust Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Yabing, Dong Jianfeng, Liang Tianxiang, Zhang Minsong, Cai Rui, Wang Xun</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 30th ACM International Conference on Multimedia</td>
    <td>18</td>
    <td><p>Despite the recent developments in the field of cross-modal retrieval, there
has been less research focusing on low-resource languages due to the lack of
manually annotated datasets. In this paper, we propose a noise-robust
cross-lingual cross-modal retrieval method for low-resource languages. To this
end, we use Machine Translation (MT) to construct pseudo-parallel sentence
pairs for low-resource languages. However, as MT is not perfect, it tends to
introduce noise during translation, rendering textual embeddings corrupted and
thereby compromising the retrieval performance. To alleviate this, we introduce
a multi-view self-distillation method to learn noise-robust target-language
representations, which employs a cross-attention module to generate soft
pseudo-targets to provide direct supervision from the similarity-based view and
feature-based view. Besides, inspired by the back-translation in unsupervised
MT, we minimize the semantic discrepancies between origin sentences and
back-translated sentences to further improve the noise robustness of the
textual encoder. Extensive experiments are conducted on three video-text and
image-text cross-modal retrieval benchmarks across different languages, and the
results demonstrate that our method significantly improves the overall
performance without using extra human-labeled data. In addition, equipped with
a pre-trained visual encoder from a recent vision-and-language pre-training
framework, i.e., CLIP, our model achieves a significant performance gain,
showing that our method is compatible with popular pre-training models. Code
and data are available at https://github.com/HuiGuanLab/nrccr.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Datasets 
      
        Unsupervised 
      
        Robustness 
      
        Evaluation 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/wang2022hybrid/">Hybrid Contrastive Quantization for Efficient Cross-View Video Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hybrid Contrastive Quantization for Efficient Cross-View Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hybrid Contrastive Quantization for Efficient Cross-View Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Jinpeng, Chen Bin, Liao Dongliang, Zeng Ziyun, Li Gongfu, Xia Shu-tao, Xu Jin</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the ACM Web Conference 2022</td>
    <td>5</td>
    <td><p>With the recent boom of video-based social platforms (e.g., YouTube and
TikTok), video retrieval using sentence queries has become an important demand
and attracts increasing research attention. Despite the decent performance,
existing text-video retrieval models in vision and language communities are
impractical for large-scale Web search because they adopt brute-force search
based on high-dimensional embeddings. To improve efficiency, Web search engines
widely apply vector compression libraries (e.g., FAISS) to post-process the
learned embeddings. Unfortunately, separate compression from feature encoding
degrades the robustness of representations and incurs performance decay. To
pursue a better balance between performance and efficiency, we propose the
first quantized representation learning method for cross-view video retrieval,
namely Hybrid Contrastive Quantization (HCQ). Specifically, HCQ learns both
coarse-grained and fine-grained quantizations with transformers, which provide
complementary understandings for texts and videos and preserve comprehensive
semantic information. By performing Asymmetric-Quantized Contrastive Learning
(AQ-CL) across views, HCQ aligns texts and videos at coarse-grained and
multiple fine-grained levels. This hybrid-grained learning strategy serves as
strong supervision on the cross-view video quantization model, where
contrastive learning at different levels can be mutually promoted. Extensive
experiments on three Web video benchmark datasets demonstrate that HCQ achieves
competitive performance with state-of-the-art non-compressed retrieval methods
while showing high efficiency in storage and computation. Code and
configurations are available at https://github.com/gimpong/WWW22-HCQ.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Datasets 
      
        Quantization 
      
        Video-Retrieval 
      
        Efficiency 
      
        Self-Supervised 
      
        Scalability 
      
        Evaluation 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/wang2022text/">Text Embeddings by Weakly-Supervised Contrastive Pre-training</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Text Embeddings by Weakly-Supervised Contrastive Pre-training' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Text Embeddings by Weakly-Supervised Contrastive Pre-training' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Liang, Yang Nan, Huang Xiaolong, Jiao Binxing, Yang Linjun, Jiang Daxin, Majumder Rangan, Wei Furu</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>74</td>
    <td><p>This paper presents E5, a family of state-of-the-art text embeddings that
transfer well to a wide range of tasks. The model is trained in a contrastive
manner with weak supervision signals from our curated large-scale text pair
dataset (called CCPairs). E5 can be readily used as a general-purpose embedding
model for any tasks requiring a single-vector representation of texts such as
retrieval, clustering, and classification, achieving strong performance in both
zero-shot and fine-tuned settings. We conduct extensive evaluations on 56
datasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the
first model that outperforms the strong BM25 baseline on the BEIR retrieval
benchmark without using any labeled data. When fine-tuned, E5 obtains the best
results on the MTEB benchmark, beating existing embedding models with 40x more
parameters.</p>
</td>
    <td>
      
        Few-Shot-&-Zero-Shot 
      
        Supervised 
      
        Datasets 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/wang2022binary/">Binary Representation via Jointly Personalized Sparse Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Binary Representation via Jointly Personalized Sparse Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Binary Representation via Jointly Personalized Sparse Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Xiaoqin, Chen Chen, Lan Rushi, Liu Licheng, Liu Zhenbing, Zhou Huiyu, Luo Xiaonan</td> <!-- 🔧 You were missing this -->
    <td>ACM Transactions on Multimedia Computing, Communications, and Applications</td>
    <td>18</td>
    <td><p>Unsupervised hashing has attracted much attention for binary representation
learning due to the requirement of economical storage and efficiency of binary
codes. It aims to encode high-dimensional features in the Hamming space with
similarity preservation between instances. However, most existing methods learn
hash functions in manifold-based approaches. Those methods capture the local
geometric structures (i.e., pairwise relationships) of data, and lack
satisfactory performance in dealing with real-world scenarios that produce
similar features (e.g. color and shape) with different semantic information. To
address this challenge, in this work, we propose an effective unsupervised
method, namely Jointly Personalized Sparse Hashing (JPSH), for binary
representation learning. To be specific, firstly, we propose a novel
personalized hashing module, i.e., Personalized Sparse Hashing (PSH). Different
personalized subspaces are constructed to reflect category-specific attributes
for different clusters, adaptively mapping instances within the same cluster to
the same Hamming space. In addition, we deploy sparse constraints for different
personalized subspaces to select important features. We also collect the
strengths of the other clusters to build the PSH module with avoiding
over-fitting. Then, to simultaneously preserve semantic and pairwise
similarities in our JPSH, we incorporate the PSH and manifold-based hash
learning into the seamless formulation. As such, JPSH not only distinguishes
the instances from different clusters, but also preserves local neighborhood
structures within the cluster. Finally, an alternating optimization algorithm
is adopted to iteratively capture analytical solutions of the JPSH model.
Extensive experiments on four benchmark datasets verify that the JPSH
outperforms several hashing algorithms on the similarity search task.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Supervised 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Unsupervised 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2022</td>
    <td>
      <a href="/publications/wang2022contrastive/">Contrastive Masked Autoencoders for Self-Supervised Video Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Contrastive Masked Autoencoders for Self-Supervised Video Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Contrastive Masked Autoencoders for Self-Supervised Video Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Yuting, Wang Jinpeng, Chen Bin, Zeng Ziyun, Xia Shutao</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>146</td>
    <td><p>Self-Supervised Video Hashing (SSVH) models learn to generate short binary
representations for videos without ground-truth supervision, facilitating
large-scale video retrieval efficiency and attracting increasing research
attention. The success of SSVH lies in the understanding of video content and
the ability to capture the semantic relation among unlabeled videos. Typically,
state-of-the-art SSVH methods consider these two points in a two-stage training
pipeline, where they firstly train an auxiliary network by instance-wise
mask-and-predict tasks and secondly train a hashing model to preserve the
pseudo-neighborhood structure transferred from the auxiliary network. This
consecutive training strategy is inflexible and also unnecessary. In this
paper, we propose a simple yet effective one-stage SSVH method called ConMH,
which incorporates video semantic information and video similarity relationship
understanding in a single stage. To capture video semantic information for
better hashing learning, we adopt an encoder-decoder structure to reconstruct
the video from its temporal-masked frames. Particularly, we find that a higher
masking ratio helps video understanding. Besides, we fully exploit the
similarity relationship between videos by maximizing agreement between two
augmented views of a video, which contributes to more discriminative and robust
hash codes. Extensive experiments on three large-scale video datasets (i.e.,
FCVID, ActivityNet and YFCC) indicate that ConMH achieves state-of-the-art
results. Code is available at https://github.com/huangmozhi9527/ConMH.</p>
</td>
    <td>
      
        Supervised 
      
        Hashing-Methods 
      
        Datasets 
      
        Video-Retrieval 
      
        Self-Supervised 
      
        Scalability 
      
        Efficiency 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/torres2021compact/">Compact and Effective Representations for Sketch-based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Compact and Effective Representations for Sketch-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Compact and Effective Representations for Sketch-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Torres Pablo, Saavedra Jose M.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>12</td>
    <td><p>Sketch-based image retrieval (SBIR) has undergone an increasing interest in
the community of computer vision bringing high impact in real applications. For
instance, SBIR brings an increased benefit to eCommerce search engines because
it allows users to formulate a query just by drawing what they need to buy.
However, current methods showing high precision in retrieval work in a high
dimensional space, which negatively affects aspects like memory consumption and
time processing. Although some authors have also proposed compact
representations, these drastically degrade the performance in a low dimension.
Therefore in this work, we present different results of evaluating methods for
producing compact embeddings in the context of sketch-based image retrieval.
Our main interest is in strategies aiming to keep the local structure of the
original space. The recent unsupervised local-topology preserving dimension
reduction method UMAP fits our requirements and shows outstanding performance,
improving even the precision achieved by SOTA methods. We evaluate six methods
in two different datasets. We use Flickr15K and eCommerce datasets; the latter
is another contribution of this work. We show that UMAP allows us to have
feature vectors of 16 bytes improving precision by more than 35%.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Datasets 
      
        CVPR 
      
        Unsupervised 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/tonellotto2021query/">Query Embedding Pruning for Dense Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Query Embedding Pruning for Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Query Embedding Pruning for Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tonellotto Nicola, Macdonald Craig</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</td>
    <td>16</td>
    <td><p>Recent advances in dense retrieval techniques have offered the promise of
being able not just to re-rank documents using contextualised language models
such as BERT, but also to use such models to identify documents from the
collection in the first place. However, when using dense retrieval approaches
that use multiple embedded representations for each query, a large number of
documents can be retrieved for each query, hindering the efficiency of the
method. Hence, this work is the first to consider efficiency improvements in
the context of a dense retrieval approach (namely ColBERT), by pruning query
term embeddings that are estimated not to be useful for retrieving relevant
documents. Our proposed query embeddings pruning reduces the cost of the dense
retrieval operation, as well as reducing the number of documents that are
retrieved and hence require to be fully scored. Experiments conducted on the
MSMARCO passage ranking corpus demonstrate that, when reducing the number of
query embeddings used from 32 to 3 based on the collection frequency of the
corresponding tokens, query embedding pruning results in no statistically
significant differences in effectiveness, while reducing the number of
documents retrieved by 70%. In terms of mean response time for the end-to-end
to end system, this results in a 2.65x speedup.</p>
</td>
    <td>
      
        Efficiency 
      
        CIKM 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/tchayekondi2020new/">A new hashing based nearest neighbors selection technique for big datasets</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A new hashing based nearest neighbors selection technique for big datasets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A new hashing based nearest neighbors selection technique for big datasets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tchaye-kondi Jude, Zhai Yanlong, Zhu Liehuang</td> <!-- 🔧 You were missing this -->
    <td>Computer Science &amp; Information Technology (CS &amp; IT)</td>
    <td>5</td>
    <td><p>KNN has the reputation to be the word simplest but efficient supervised
learning algorithm used for either classification or regression. KNN prediction
efficiency highly depends on the size of its training data but when this
training data grows KNN suffers from slowness in making decisions since it
needs to search nearest neighbors within the entire dataset at each decision
making. This paper proposes a new technique that enables the selection of
nearest neighbors directly in the neighborhood of a given observation. The
proposed approach consists of dividing the data space into subcells of a
virtual grid built on top of data space. The mapping between the data points
and subcells is performed using hashing. When it comes to select the nearest
neighbors of a given observation, we firstly identify the cell the observation
belongs by using hashing, and then we look for nearest neighbors from that
central cell and cells around it layer by layer. From our experiment
performance analysis on publicly available datasets, our algorithm outperforms
the original KNN in time efficiency with a prediction quality as good as that
of KNN it also offers competitive performance with solutions like KDtree</p>
</td>
    <td>
      
        Supervised 
      
        Hashing-Methods 
      
        Datasets 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/tan2021instance/">Instance-level Image Retrieval using Reranking Transformers</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Instance-level Image Retrieval using Reranking Transformers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Instance-level Image Retrieval using Reranking Transformers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tan Fuwen, Yuan Jiangbo, Ordonez Vicente</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>71</td>
    <td><p>Instance-level image retrieval is the task of searching in a large database
for images that match an object in a query image. To address this task, systems
usually rely on a retrieval step that uses global image descriptors, and a
subsequent step that performs domain-specific refinements or reranking by
leveraging operations such as geometric verification based on local features.
In this work, we propose Reranking Transformers (RRTs) as a general model to
incorporate both local and global features to rerank the matching images in a
supervised fashion and thus replace the relatively expensive process of
geometric verification. RRTs are lightweight and can be easily parallelized so
that reranking a set of top matching results can be performed in a single
forward-pass. We perform extensive experiments on the Revisited Oxford and
Paris datasets, and the Google Landmarks v2 dataset, showing that RRTs
outperform previous reranking approaches while using much fewer local
descriptors. Moreover, we demonstrate that, unlike existing approaches, RRTs
can be optimized jointly with the feature extractor, which can lead to feature
representations tailored to downstream tasks and further accuracy improvements.
The code and trained models are publicly available at
https://github.com/uvavision/RerankingTransformer.</p>
</td>
    <td>
      
        ICCV 
      
        Supervised 
      
        Image-Retrieval 
      
        Datasets 
      
        Re-Ranking 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/tang2021improving/">Improving Document Representations by Generating Pseudo Query Embeddings for Dense Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Improving Document Representations by Generating Pseudo Query Embeddings for Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Improving Document Representations by Generating Pseudo Query Embeddings for Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tang Hongyin, Sun Xingwu, Jin Beihong, Wang Jingang, Zhang Fuzheng, Wu Wei</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</td>
    <td>23</td>
    <td><p>Recently, the retrieval models based on dense representations have been
gradually applied in the first stage of the document retrieval tasks, showing
better performance than traditional sparse vector space models. To obtain high
efficiency, the basic structure of these models is Bi-encoder in most cases.
However, this simple structure may cause serious information loss during the
encoding of documents since the queries are agnostic. To address this problem,
we design a method to mimic the queries on each of the documents by an
iterative clustering process and represent the documents by multiple pseudo
queries (i.e., the cluster centroids). To boost the retrieval process using
approximate nearest neighbor search library, we also optimize the matching
function with a two-step score calculation procedure. Experimental results on
several popular ranking and QA datasets show that our model can achieve
state-of-the-art results.</p>
</td>
    <td>
      
        Text-Retrieval 
      
        Tools-&-Libraries 
      
        Datasets 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/sun2021real/">Real-time Human Action Recognition Using Locally Aggregated Kinematic-Guided Skeletonlet and Supervised Hashing-by-Analysis Model</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Real-time Human Action Recognition Using Locally Aggregated Kinematic-Guided Skeletonlet and Supervised Hashing-by-Analysis Model' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Real-time Human Action Recognition Using Locally Aggregated Kinematic-Guided Skeletonlet and Supervised Hashing-by-Analysis Model' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sun Bin, Wang Shaofan, Kong Dehui, Wang Lichun, Yin Baocai</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Cybernetics</td>
    <td>15</td>
    <td><p>3D action recognition is referred to as the classification of action
sequences which consist of 3D skeleton joints. While many research work are
devoted to 3D action recognition, it mainly suffers from three problems: highly
complicated articulation, a great amount of noise, and a low implementation
efficiency. To tackle all these problems, we propose a real-time 3D action
recognition framework by integrating the locally aggregated kinematic-guided
skeletonlet (LAKS) with a supervised hashing-by-analysis (SHA) model. We first
define the skeletonlet as a few combinations of joint offsets grouped in terms
of kinematic principle, and then represent an action sequence using LAKS, which
consists of a denoising phase and a locally aggregating phase. The denoising
phase detects the noisy action data and adjust it by replacing all the features
within it with the features of the corresponding previous frame, while the
locally aggregating phase sums the difference between an offset feature of the
skeletonlet and its cluster center together over all the offset features of the
sequence. Finally, the SHA model which combines sparse representation with a
hashing model, aiming at promoting the recognition accuracy while maintaining a
high efficiency. Experimental results on MSRAction3D, UTKinectAction3D and
Florence3DAction datasets demonstrate that the proposed method outperforms
state-of-the-art methods in both recognition accuracy and implementation
efficiency.</p>
</td>
    <td>
      
        Supervised 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/sumbul2020deep/">Deep Learning for Image Search and Retrieval in Large Remote Sensing Archives</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Learning for Image Search and Retrieval in Large Remote Sensing Archives' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Learning for Image Search and Retrieval in Large Remote Sensing Archives' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sumbul Gencer, Kang Jian, Demir Begüm</td> <!-- 🔧 You were missing this -->
    <td>Deep Learning for the Earth Sciences</td>
    <td>24</td>
    <td><p>This chapter presents recent advances in content based image search and
retrieval (CBIR) systems in remote sensing (RS) for fast and accurate
information discovery from massive data archives. Initially, we analyze the
limitations of the traditional CBIR systems that rely on the hand-crafted RS
image descriptors. Then, we focus our attention on the advances in RS CBIR
systems for which deep learning (DL) models are at the forefront. In
particular, we present the theoretical properties of the most recent DL based
CBIR systems for the characterization of the complex semantic content of RS
images. After discussing their strengths and limitations, we present the deep
hashing based CBIR systems that have high time-efficient search capability
within huge data archives. Finally, the most promising research directions in
RS CBIR are discussed.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/zhang2020leveraging/">Leveraging Local and Global Descriptors in Parallel to Search Correspondences for Visual Localization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Leveraging Local and Global Descriptors in Parallel to Search Correspondences for Visual Localization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Leveraging Local and Global Descriptors in Parallel to Search Correspondences for Visual Localization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Pengju, Wu Yihong, Liu Bingxi</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition</td>
    <td>13</td>
    <td><p>Visual localization to compute 6DoF camera pose from a given image has wide
applications such as in robotics, virtual reality, augmented reality, etc. Two
kinds of descriptors are important for the visual localization. One is global
descriptors that extract the whole feature from each image. The other is local
descriptors that extract the local feature from each image patch usually
enclosing a key point. More and more methods of the visual localization have
two stages: at first to perform image retrieval by global descriptors and then
from the retrieval feedback to make 2D-3D point correspondences by local
descriptors. The two stages are in serial for most of the methods. This simple
combination has not achieved superiority of fusing local and global
descriptors. The 3D points obtained from the retrieval feedback are as the
nearest neighbor candidates of the 2D image points only by global descriptors.
Each of the 2D image points is also called a query local feature when
performing the 2D-3D point correspondences. In this paper, we propose a novel
parallel search framework, which leverages advantages of both local and global
descriptors to get nearest neighbor candidates of a query local feature.
Specifically, besides using deep learning based global descriptors, we also
utilize local descriptors to construct random tree structures for obtaining
nearest neighbor candidates of the query local feature. We propose a new
probabilistic model and a new deep learning based local descriptor when
constructing the random trees. A weighted Hamming regularization term to keep
discriminativeness after binarization is given in the loss function for the
proposed local descriptor. The loss function co-trains both real and binary
descriptors of which the results are integrated into the random trees.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Tools-&-Libraries 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/staszewski2020new/">A new approach to descriptors generation for image retrieval by analyzing activations of deep neural network layers</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A new approach to descriptors generation for image retrieval by analyzing activations of deep neural network layers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A new approach to descriptors generation for image retrieval by analyzing activations of deep neural network layers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Staszewski Paweł, Jaworski Maciej, Cao Jinde, Rutkowski Leszek</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Neural Networks and Learning Systems</td>
    <td>22</td>
    <td><p>In this paper, we consider the problem of descriptors construction for the
task of content-based image retrieval using deep neural networks. The idea of
neural codes, based on fully connected layers activations, is extended by
incorporating the information contained in convolutional layers. It is known
that the total number of neurons in the convolutional part of the network is
large and the majority of them have little influence on the final
classification decision. Therefore, in the paper we propose a novel algorithm
that allows us to extract the most significant neuron activations and utilize
this information to construct effective descriptors. The descriptors consisting
of values taken from both the fully connected and convolutional layers
perfectly represent the whole image content. The images retrieved using these
descriptors match semantically very well to the query image, and also they are
similar in other secondary image characteristics, like background, textures or
color distribution. These features of the proposed descriptors are verified
experimentally based on the IMAGENET1M dataset using the VGG16 neural network.</p>
</td>
    <td>
      
        Datasets 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/stein2021self/">Self-supervised similarity search for large scientific datasets</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Self-supervised similarity search for large scientific datasets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Self-supervised similarity search for large scientific datasets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Stein George, Harrington Peter, Blaum Jacqueline, Medan Tomislav, Lukic Zarija</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>13</td>
    <td><p>We present the use of self-supervised learning to explore and exploit large
unlabeled datasets. Focusing on 42 million galaxy images from the latest data
release of the Dark Energy Spectroscopic Instrument (DESI) Legacy Imaging
Surveys, we first train a self-supervised model to distill low-dimensional
representations that are robust to symmetries, uncertainties, and noise in each
image. We then use the representations to construct and publicly release an
interactive semantic similarity search tool. We demonstrate how our tool can be
used to rapidly discover rare objects given only a single example, increase the
speed of crowd-sourcing campaigns, and construct and improve training sets for
supervised applications. While we focus on images from sky surveys, the
technique is straightforward to apply to any scientific dataset of any
dimensionality. The similarity search web app can be found at
https://github.com/georgestein/galaxy_search</p>
</td>
    <td>
      
        Similarity-Search 
      
        Self-Supervised 
      
        Datasets 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/schubert2020graph/">Graph-based non-linear least squares optimization for visual place recognition in changing environments</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Graph-based non-linear least squares optimization for visual place recognition in changing environments' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Graph-based non-linear least squares optimization for visual place recognition in changing environments' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Schubert Stefan, Neubert Peer, Protzel Peter</td> <!-- 🔧 You were missing this -->
    <td>IEEE Robotics and Automation Letters</td>
    <td>9</td>
    <td><p>Visual place recognition is an important subproblem of mobile robot
localization. Since it is a special case of image retrieval, the basic source
of information is the pairwise similarity of image descriptors. However, the
embedding of the image retrieval problem in this robotic task provides
additional structure that can be exploited, e.g. spatio-temporal consistency.
Several algorithms exist to exploit this structure, e.g., sequence processing
approaches or descriptor standardization approaches for changing environments.
In this paper, we propose a graph-based framework to systematically exploit
different types of additional structure and information. The graphical model is
used to formulate a non-linear least squares problem that can be optimized with
standard tools. Beyond sequences and standardization, we propose the usage of
intra-set similarities within the database and/or the query image set as
additional source of information. If available, our approach also allows to
seamlessly integrate additional knowledge about poses of database images. We
evaluate the system on a variety of standard place recognition datasets and
demonstrate performance improvements for a large number of different
configurations including different sources of information, different types of
constraints, and online or offline place recognition setups.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Graph-Based-ANN 
      
        Image-Retrieval 
      
        Datasets 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/schubert2021triangle/">A Triangle Inequality for Cosine Similarity</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Triangle Inequality for Cosine Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Triangle Inequality for Cosine Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Schubert Erich</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>20</td>
    <td><p>Similarity search is a fundamental problem for many data analysis techniques.
Many efficient search techniques rely on the triangle inequality of metrics,
which allows pruning parts of the search space based on transitive bounds on
distances. Recently, Cosine similarity has become a popular alternative choice
to the standard Euclidean metric, in particular in the context of textual data
and neural network embeddings. Unfortunately, Cosine similarity is not metric
and does not satisfy the standard triangle inequality. Instead, many search
techniques for Cosine rely on approximation techniques such as locality
sensitive hashing. In this paper, we derive a triangle inequality for Cosine
similarity that is suitable for efficient similarity search with many standard
search structures (such as the VP-tree, Cover-tree, and M-tree); show that this
bound is tight and discuss fast approximations for it. We hope that this spurs
new research on accelerating exact similarity search for cosine similarity, and
possible other similarity measures beyond the existing work for distance
metrics.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Distance-Metric-Learning 
      
        Similarity-Search 
      
        Tree-Based-ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/sanakoyeu2021improving/">Improving Deep Metric Learning by Divide and Conquer</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Improving Deep Metric Learning by Divide and Conquer' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Improving Deep Metric Learning by Divide and Conquer' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sanakoyeu Artsiom, Ma Pingchuan, Tschernezki Vadim, Ommer Björn</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>16</td>
    <td><p>Deep metric learning (DML) is a cornerstone of many computer vision
applications. It aims at learning a mapping from the input domain to an
embedding space, where semantically similar objects are located nearby and
dissimilar objects far from another. The target similarity on the training data
is defined by user in form of ground-truth class labels. However, while the
embedding space learns to mimic the user-provided similarity on the training
data, it should also generalize to novel categories not seen during training.
Besides user-provided groundtruth training labels, a lot of additional visual
factors (such as viewpoint changes or shape peculiarities) exist and imply
different notions of similarity between objects, affecting the generalization
on the images unseen during training. However, existing approaches usually
directly learn a single embedding space on all available training data,
struggling to encode all different types of relationships, and do not
generalize well. We propose to build a more expressive representation by
jointly splitting the embedding space and the data hierarchically into smaller
sub-parts. We successively focus on smaller subsets of the training data,
reducing its variance and learning a different embedding subspace for each data
subset. Moreover, the subspaces are learned jointly to cover not only the
intricacies, but the breadth of the data as well. Only after that, we build the
final embedding from the subspaces in the conquering stage. The proposed
algorithm acts as a transparent wrapper that can be placed around arbitrary
existing DML methods. Our approach significantly improves upon the
state-of-the-art on image retrieval, clustering, and re-identification tasks
evaluated using CUB200-2011, CARS196, Stanford Online Products, In-shop
Clothes, and PKU VehicleID datasets.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/seidenschwarz2021learning/">Learning Intra-Batch Connections for Deep Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Intra-Batch Connections for Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Intra-Batch Connections for Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Seidenschwarz Jenny, Elezi Ismail, Leal-taixé Laura</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>23</td>
    <td><p>The goal of metric learning is to learn a function that maps samples to a
lower-dimensional space where similar samples lie closer than dissimilar ones.
Particularly, deep metric learning utilizes neural networks to learn such a
mapping. Most approaches rely on losses that only take the relations between
pairs or triplets of samples into account, which either belong to the same
class or two different classes. However, these methods do not explore the
embedding space in its entirety. To this end, we propose an approach based on
message passing networks that takes all the relations in a mini-batch into
account. We refine embedding vectors by exchanging messages among all samples
in a given batch allowing the training process to be aware of its overall
structure. Since not all samples are equally important to predict a decision
boundary, we use an attention mechanism during message passing to allow samples
to weigh the importance of each neighbor accordingly. We achieve
state-of-the-art results on clustering and image retrieval on the CUB-200-2011,
Cars196, Stanford Online Products, and In-Shop Clothes datasets. To facilitate
further research, we make available the code and the models at
https://github.com/dvl-tum/intra_batch_connections.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/zhang2021improved/">Improved Deep Classwise Hashing With Centers Similarity Learning for Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Improved Deep Classwise Hashing With Centers Similarity Learning for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Improved Deep Classwise Hashing With Centers Similarity Learning for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Ming, Yan Hong</td> <!-- 🔧 You were missing this -->
    <td>2020 25th International Conference on Pattern Recognition (ICPR)</td>
    <td>8</td>
    <td><p>Deep supervised hashing for image retrieval has attracted researchers’
attention due to its high efficiency and superior retrieval performance. Most
existing deep supervised hashing works, which are based on pairwise/triplet
labels, suffer from the expensive computational cost and insufficient
utilization of the semantics information. Recently, deep classwise hashing
introduced a classwise loss supervised by class labels information
alternatively; however, we find it still has its drawback. In this paper, we
propose an improved deep classwise hashing, which enables hashing learning and
class centers learning simultaneously. Specifically, we design a two-step
strategy on center similarity learning. It interacts with the classwise loss to
attract the class center to concentrate on the intra-class samples while
pushing other class centers as far as possible. The centers similarity learning
contributes to generating more compact and discriminative hashing codes. We
conduct experiments on three benchmark datasets. It shows that the proposed
method effectively surpasses the original method and outperforms
state-of-the-art baselines under various commonly-used evaluation metrics for
image retrieval.</p>
</td>
    <td>
      
        Supervised 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/rosin2020event/">Event-Driven Query Expansion</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Event-Driven Query Expansion' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Event-Driven Query Expansion' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Rosin Guy D., Guy Ido, Radinsky Kira</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 14th ACM International Conference on Web Search and Data Mining</td>
    <td>8</td>
    <td><p>A significant number of event-related queries are issued in Web search. In
this paper, we seek to improve retrieval performance by leveraging events and
specifically target the classic task of query expansion. We propose a method to
expand an event-related query by first detecting the events related to it.
Then, we derive the candidates for expansion as terms semantically related to
both the query and the events. To identify the candidates, we utilize a novel
mechanism to simultaneously embed words and events in the same vector space. We
show that our proposed method of leveraging events improves query expansion
performance significantly compared with state-of-the-art methods on various
newswire TREC datasets.</p>
</td>
    <td>
      
        Datasets 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/riba2020learning/">Learning Graph Edit Distance by Graph Neural Networks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Graph Edit Distance by Graph Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Graph Edit Distance by Graph Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Riba Pau, Fischer Andreas, Lladós Josep, Fornés Alicia</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition</td>
    <td>26</td>
    <td><p>The emergence of geometric deep learning as a novel framework to deal with
graph-based representations has faded away traditional approaches in favor of
completely new methodologies. In this paper, we propose a new framework able to
combine the advances on deep metric learning with traditional approximations of
the graph edit distance. Hence, we propose an efficient graph distance based on
the novel field of geometric deep learning. Our method employs a message
passing neural network to capture the graph structure, and thus, leveraging
this information for its use on a distance computation. The performance of the
proposed graph distance is validated on two different scenarios. On the one
hand, in a graph retrieval of handwritten words~\ie~keyword spotting, showing
its superior performance when compared with (approximate) graph edit distance
benchmarks. On the other hand, demonstrating competitive results for graph
similarity learning when compared with the current state-of-the-art on a recent
benchmark dataset.</p>
</td>
    <td>
      
        Evaluation 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Graph-Based-ANN 
      
        Tools-&-Libraries 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/qiu2021unsupervised/">Unsupervised Hashing with Contrastive Information Bottleneck</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Hashing with Contrastive Information Bottleneck' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Hashing with Contrastive Information Bottleneck' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Qiu Zexuan, Su Qinliang, Ou Zijing, Yu Jianxing, Chen Changyou</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence</td>
    <td>51</td>
    <td><p>Many unsupervised hashing methods are implicitly established on the idea of
reconstructing the input data, which basically encourages the hashing codes to
retain as much information of original data as possible. However, this
requirement may force the models spending lots of their effort on
reconstructing the unuseful background information, while ignoring to preserve
the discriminative semantic information that is more important for the hashing
task. To tackle this problem, inspired by the recent success of contrastive
learning in learning continuous representations, we propose to adapt this
framework to learn binary hashing codes. Specifically, we first propose to
modify the objective function to meet the specific requirement of hashing and
then introduce a probabilistic binary representation layer into the model to
facilitate end-to-end training of the entire model. We further prove the strong
connection between the proposed contrastive-learning-based hashing method and
the mutual information, and show that the proposed model can be considered
under the broader framework of the information bottleneck (IB). Under this
perspective, a more general hashing model is naturally obtained. Extensive
experimental results on three benchmark image datasets demonstrate that the
proposed hashing method significantly outperforms existing baselines.</p>
</td>
    <td>
      
        Evaluation 
      
        Datasets 
      
        Unsupervised 
      
        AAAI 
      
        Hashing-Methods 
      
        Tools-&-Libraries 
      
        Neural-Hashing 
      
        IJCAI 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/zhang2025high/">High-order nonlocal Hashing for unsupervised cross-modal retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=High-order nonlocal Hashing for unsupervised cross-modal retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=High-order nonlocal Hashing for unsupervised cross-modal retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Peng-fei, Luo, Huang, Xu, Song</td> <!-- 🔧 You were missing this -->
    <td>World Wide Web</td>
    <td>62</td>
    <td><p>In light of the ability to enable efficient storage and fast query for big data, hashing techniques for cross-modal search have aroused extensive attention. Despite the great success achieved, unsupervised cross-modal hashing still suffers from lacking reliable similarity supervision and struggles with handling the heterogeneity issue between different modalities. To cope with these, in this paper, we devise a new deep hashing model, termed as High-order Nonlocal Hashing (HNH) to facilitate cross-modal retrieval with the following advantages. First, different from existing methods that mainly leverage low-level local-view similarity as the guidance for hashing learning, we propose a high-order affinity measure that considers the multi-modal neighbourhood structures from a nonlocal perspective, thereby comprehensively capturing the similarity relationships between data items. Second, a common representation is introduced to correlate different modalities. By enforcing the modal-specific descriptors and the common representation to be aligned with each other, the proposed HNH significantly bridges the modality gap and maintains the intra-consistency. Third, an effective affinity preserving objective function is delicately designed to generate high-quality binary codes. Extensive experiments evidence the superiority of the proposed HNH in unsupervised cross-modal retrieval tasks over the state-of-the-art baselines.</p>
</td>
    <td>
      
        Neural-Hashing 
      
        Multimodal-Retrieval 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/portilloquintero2021straightforward/">A Straightforward Framework For Video Retrieval Using CLIP</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Straightforward Framework For Video Retrieval Using CLIP' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Straightforward Framework For Video Retrieval Using CLIP' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Portillo-quintero Jesús Andrés, Ortiz-bayliss José Carlos, Terashima-marín Hugo</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>79</td>
    <td><p>Video Retrieval is a challenging task where a text query is matched to a
video or vice versa. Most of the existing approaches for addressing such a
problem rely on annotations made by the users. Although simple, this approach
is not always feasible in practice. In this work, we explore the application of
the language-image model, CLIP, to obtain video representations without the
need for said annotations. This model was explicitly trained to learn a common
space where images and text can be compared. Using various techniques described
in this document, we extended its application to videos, obtaining
state-of-the-art results on the MSR-VTT and MSVD benchmarks.</p>
</td>
    <td>
      
        Video-Retrieval 
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/zhang2025fast/">Fast Discrete Cross-Modal Hashing Based on Label Relaxation and Matrix Factorization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Discrete Cross-Modal Hashing Based on Label Relaxation and Matrix Factorization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Discrete Cross-Modal Hashing Based on Label Relaxation and Matrix Factorization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Donglin, Wu, Liu, Yu, Kittler</td> <!-- 🔧 You were missing this -->
    <td>2020 25th International Conference on Pattern Recognition (ICPR)</td>
    <td>9</td>
    <td><p>In recent years, cross-media retrieval has drawn considerable attention due to the exponential growth of multimedia data. Many hashing approaches have been proposed for the cross-media search task. However, there are still open problems that warrant investigation. For example, most existing supervised hashing approaches employ a binary label matrix, which achieves small margins between wrong labels (0) and true labels (1). This may affect the retrieval performance by generating many false negatives and false positives. In addition, some methods adopt a relaxation scheme to solve the binary constraints, which may cause large quantization errors. There are also some discrete hashing methods that have been presented, but most of them are time-consuming. To conquer these problems, we present a label relaxation and discrete matrix factorization method (LRMF) for cross-modal retrieval. It offers a number of innovations. First of all, the proposed approach employs a novel label relaxation scheme to control the margins adaptively, which has the benefit of reducing the quantization error. Second, by virtue of the proposed discrete matrix factorization method designed to learn the binary codes, large quantization errors caused by relaxation can be avoided. The experimental results obtained on two widely-used databases demonstrate that LRMF outperforms state-of-the-art cross-media methods.</p>
</td>
    <td>
      
        Neural-Hashing 
      
        Multimodal-Retrieval 
      
        Quantization 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/park2021unsupervised/">Unsupervised Representation Learning via Neural Activation Coding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Representation Learning via Neural Activation Coding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Representation Learning via Neural Activation Coding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Park Yookoon, Lee Sangho, Kim Gunhee, Blei David M.</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>29</td>
    <td><p>We present neural activation coding (NAC) as a novel approach for learning
deep representations from unlabeled data for downstream applications. We argue
that the deep encoder should maximize its nonlinear expressivity on the data
for downstream predictors to take full advantage of its representation power.
To this end, NAC maximizes the mutual information between activation patterns
of the encoder and the data over a noisy communication channel. We show that
learning for a noise-robust activation code increases the number of distinct
linear regions of ReLU encoders, hence the maximum nonlinear expressivity. More
interestingly, NAC learns both continuous and discrete representations of data,
which we respectively evaluate on two downstream tasks: (i) linear
classification on CIFAR-10 and ImageNet-1K and (ii) nearest neighbor retrieval
on CIFAR-10 and FLICKR-25K. Empirical results show that NAC attains better or
comparable performance on both tasks over recent baselines including SimCLR and
DistillHash. In addition, NAC pretraining provides significant benefits to the
training of deep generative models. Our code is available at
https://github.com/yookoon/nac.</p>
</td>
    <td>
      
        CVPR 
      
        Self-Supervised 
      
        Unsupervised 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/nguyen2021deep/">A Deep Local and Global Scene-Graph Matching for Image-Text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Deep Local and Global Scene-Graph Matching for Image-Text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Deep Local and Global Scene-Graph Matching for Image-Text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Nguyen Manh-duy, Nguyen Binh T., Gurrin Cathal</td> <!-- 🔧 You were missing this -->
    <td>Frontiers in Artificial Intelligence and Applications</td>
    <td>24</td>
    <td><p>Conventional approaches to image-text retrieval mainly focus on indexing
visual objects appearing in pictures but ignore the interactions between these
objects. Such objects occurrences and interactions are equivalently useful and
important in this field as they are usually mentioned in the text. Scene graph
presentation is a suitable method for the image-text matching challenge and
obtained good results due to its ability to capture the inter-relationship
information. Both images and text are represented in scene graph levels and
formulate the retrieval challenge as a scene graph matching challenge. In this
paper, we introduce the Local and Global Scene Graph Matching (LGSGM) model
that enhances the state-of-the-art method by integrating an extra graph
convolution network to capture the general information of a graph.
Specifically, for a pair of scene graphs of an image and its caption, two
separate models are used to learn the features of each graph’s nodes and edges.
Then a Siamese-structure graph convolution model is employed to embed graphs
into vector forms. We finally combine the graph-level and the vector-level to
calculate the similarity of this image-text pair. The empirical experiments
show that our enhancement with the combination of levels can improve the
performance of the baseline method by increasing the recall by more than 10% on
the Flickr30k dataset.</p>
</td>
    <td>
      
        Datasets 
      
        Text-Retrieval 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/moskvyak2020keypoint/">Keypoint-Aligned Embeddings for Image Retrieval and Re-identification</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Keypoint-Aligned Embeddings for Image Retrieval and Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Keypoint-Aligned Embeddings for Image Retrieval and Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Moskvyak Olga, Maire Frederic, Dayoub Feras, Baktashmotlagh Mahsa</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>22</td>
    <td><p>Learning embeddings that are invariant to the pose of the object is crucial
in visual image retrieval and re-identification. The existing approaches for
person, vehicle, or animal re-identification tasks suffer from high intra-class
variance due to deformable shapes and different camera viewpoints. To overcome
this limitation, we propose to align the image embedding with a predefined
order of the keypoints. The proposed keypoint aligned embeddings model
(KAE-Net) learns part-level features via multi-task learning which is guided by
keypoint locations. More specifically, KAE-Net extracts channels from a feature
map activated by a specific keypoint through learning the auxiliary task of
heatmap reconstruction for this keypoint. The KAE-Net is compact, generic and
conceptually simple. It achieves state of the art performance on the benchmark
datasets of CUB-200-2011, Cars196 and VeRi-776 for retrieval and
re-identification tasks.</p>
</td>
    <td>
      
        Datasets 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/messina2020fine/">Fine-grained Visual Textual Alignment for Cross-Modal Retrieval using Transformer Encoders</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fine-grained Visual Textual Alignment for Cross-Modal Retrieval using Transformer Encoders' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fine-grained Visual Textual Alignment for Cross-Modal Retrieval using Transformer Encoders' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Messina Nicola, Amato Giuseppe, Esuli Andrea, Falchi Fabrizio, Gennaro Claudio, Marchand-maillet Stéphane</td> <!-- 🔧 You were missing this -->
    <td>ACM Transactions on Multimedia Computing, Communications, and Applications</td>
    <td>116</td>
    <td><p>Despite the evolution of deep-learning-based visual-textual processing
systems, precise multi-modal matching remains a challenging task. In this work,
we tackle the task of cross-modal retrieval through image-sentence matching
based on word-region alignments, using supervision only at the global
image-sentence level. Specifically, we present a novel approach called
Transformer Encoder Reasoning and Alignment Network (TERAN). TERAN enforces a
fine-grained match between the underlying components of images and sentences,
i.e., image regions and words, respectively, in order to preserve the
informative richness of both modalities. TERAN obtains state-of-the-art results
on the image retrieval task on both MS-COCO and Flickr30k datasets. Moreover,
on MS-COCO, it also outperforms current approaches on the sentence retrieval
task.
  Focusing on scalable cross-modal information retrieval, TERAN is designed to
keep the visual and textual data pipelines well separated. Cross-attention
links invalidate any chance to separately extract visual and textual features
needed for the online search and the offline indexing steps in large-scale
retrieval systems. In this respect, TERAN merges the information from the two
domains only during the final alignment phase, immediately before the loss
computation. We argue that the fine-grained alignments produced by TERAN pave
the way towards the research for effective and efficient methods for
large-scale cross-modal information retrieval. We compare the effectiveness of
our approach against relevant state-of-the-art methods. On the MS-COCO 1K test
set, we obtain an improvement of 5.7% and 3.5% respectively on the image and
the sentence retrieval tasks on the Recall@1 metric. The code used for the
experiments is publicly available on GitHub at
https://github.com/mesnico/TERAN.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Datasets 
      
        Scalability 
      
        Evaluation 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/melekhov2021digging/">Digging Into Self-Supervised Learning of Feature Descriptors</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Digging Into Self-Supervised Learning of Feature Descriptors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Digging Into Self-Supervised Learning of Feature Descriptors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Melekhov Iaroslav, Laskar Zakaria, Li Xiaotian, Wang Shuzhe, Kannala Juho</td> <!-- 🔧 You were missing this -->
    <td>2021 International Conference on 3D Vision (3DV)</td>
    <td>7</td>
    <td><p>Fully-supervised CNN-based approaches for learning local image descriptors
have shown remarkable results in a wide range of geometric tasks. However, most
of them require per-pixel ground-truth keypoint correspondence data which is
difficult to acquire at scale. To address this challenge, recent weakly- and
self-supervised methods can learn feature descriptors from relative camera
poses or using only synthetic rigid transformations such as homographies. In
this work, we focus on understanding the limitations of existing
self-supervised approaches and propose a set of improvements that combined lead
to powerful feature descriptors. We show that increasing the search space from
in-pair to in-batch for hard negative mining brings consistent improvement. To
enhance the discriminativeness of feature descriptors, we propose a
coarse-to-fine method for mining local hard negatives from a wider search space
by using global visual image descriptors. We demonstrate that a combination of
synthetic homography transformation, color augmentation, and photorealistic
image stylization produces useful representations that are viewpoint and
illumination invariant. The feature descriptors learned by the proposed
approach perform competitively and surpass their fully- and weakly-supervised
counterparts on various geometric benchmarks such as image-based localization,
sparse feature matching, and image retrieval.</p>
</td>
    <td>
      
        Self-Supervised 
      
        Supervised 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/messina2020transformer/">Transformer Reasoning Network for Image-Text Matching and Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Transformer Reasoning Network for Image-Text Matching and Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Transformer Reasoning Network for Image-Text Matching and Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Messina Nicola, Falchi Fabrizio, Esuli Andrea, Amato Giuseppe</td> <!-- 🔧 You were missing this -->
    <td>2020 25th International Conference on Pattern Recognition (ICPR)</td>
    <td>56</td>
    <td><p>Image-text matching is an interesting and fascinating task in modern AI
research. Despite the evolution of deep-learning-based image and text
processing systems, multi-modal matching remains a challenging problem. In this
work, we consider the problem of accurate image-text matching for the task of
multi-modal large-scale information retrieval. State-of-the-art results in
image-text matching are achieved by inter-playing image and text features from
the two different processing pipelines, usually using mutual attention
mechanisms. However, this invalidates any chance to extract separate visual and
textual features needed for later indexing steps in large-scale retrieval
systems. In this regard, we introduce the Transformer Encoder Reasoning Network
(TERN), an architecture built upon one of the modern relationship-aware
self-attentive architectures, the Transformer Encoder (TE). This architecture
is able to separately reason on the two different modalities and to enforce a
final common abstract concept space by sharing the weights of the deeper
transformer layers. Thanks to this design, the implemented network is able to
produce compact and very rich visual and textual features available for the
successive indexing step. Experiments are conducted on the MS-COCO dataset, and
we evaluate the results using a discounted cumulative gain metric with
relevance computed exploiting caption similarities, in order to assess possibly
non-exact but relevant search results. We demonstrate that on this metric we
are able to achieve state-of-the-art results in the image retrieval task. Our
code is freely available at https://github.com/mesnico/TERN.</p>
</td>
    <td>
      
        Datasets 
      
        Scalability 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/mazumder2021few/">Few-Shot Keyword Spotting in Any Language</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Few-Shot Keyword Spotting in Any Language' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Few-Shot Keyword Spotting in Any Language' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mazumder Mark, Banbury Colby, Meyer Josh, Warden Pete, Reddi Vijay Janapa</td> <!-- 🔧 You were missing this -->
    <td>Interspeech 2021</td>
    <td>22</td>
    <td><p>We introduce a few-shot transfer learning method for keyword spotting in any
language. Leveraging open speech corpora in nine languages, we automate the
extraction of a large multilingual keyword bank and use it to train an
embedding model. With just five training examples, we fine-tune the embedding
model for keyword spotting and achieve an average F1 score of 0.75 on keyword
classification for 180 new keywords unseen by the embedding model in these nine
languages. This embedding model also generalizes to new languages. We achieve
an average F1 score of 0.65 on 5-shot models for 260 keywords sampled across 13
new languages unseen by the embedding model. We investigate streaming accuracy
for our 5-shot models in two contexts: keyword spotting and keyword search.
Across 440 keywords in 22 languages, we achieve an average streaming keyword
spotting accuracy of 87.4% with a false acceptance rate of 4.3%, and observe
promising initial results on keyword search.</p>
</td>
    <td>
      
        Few-Shot-&-Zero-Shot 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/messina2021towards/">Towards Efficient Cross-Modal Visual Textual Retrieval using Transformer-Encoder Deep Features</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Towards Efficient Cross-Modal Visual Textual Retrieval using Transformer-Encoder Deep Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Towards Efficient Cross-Modal Visual Textual Retrieval using Transformer-Encoder Deep Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Messina Nicola, Amato Giuseppe, Falchi Fabrizio, Gennaro Claudio, Marchand-maillet Stéphane</td> <!-- 🔧 You were missing this -->
    <td>2021 International Conference on Content-Based Multimedia Indexing (CBMI)</td>
    <td>7</td>
    <td><p>Cross-modal retrieval is an important functionality in modern search engines,
as it increases the user experience by allowing queries and retrieved objects
to pertain to different modalities. In this paper, we focus on the
image-sentence retrieval task, where the objective is to efficiently find
relevant images for a given sentence (image-retrieval) or the relevant
sentences for a given image (sentence-retrieval). Computer vision literature
reports the best results on the image-sentence matching task using deep neural
networks equipped with attention and self-attention mechanisms. They evaluate
the matching performance on the retrieval task by performing sequential scans
of the whole dataset. This method does not scale well with an increasing amount
of images or captions. In this work, we explore different preprocessing
techniques to produce sparsified deep multi-modal features extracting them from
state-of-the-art deep-learning architectures for image-text matching. Our main
objective is to lay down the paths for efficient indexing of complex
multi-modal descriptions. We use the recently introduced TERN architecture as
an image-sentence features extractor. It is designed for producing fixed-size
1024-d vectors describing whole images and sentences, as well as
variable-length sets of 1024-d vectors describing the various building
components of the two modalities (image regions and sentence words
respectively). All these vectors are enforced by the TERN design to lie into
the same common space. Our experiments show interesting preliminary results on
the explored methods and suggest further experimentation in this important
research direction.</p>
</td>
    <td>
      
        Datasets 
      
        Evaluation 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/malali2022learning/">Learning to embed semantic similarity for joint image-text retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning to embed semantic similarity for joint image-text retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning to embed semantic similarity for joint image-text retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Malali Noam, Keller Yosi</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>8</td>
    <td><p>We present a deep learning approach for learning the joint semantic
embeddings of images and captions in a Euclidean space, such that the semantic
similarity is approximated by the L2 distances in the embedding space. For
that, we introduce a metric learning scheme that utilizes multitask learning to
learn the embedding of identical semantic concepts using a center loss. By
introducing a differentiable quantization scheme into the end-to-end trainable
network, we derive a semantic embedding of semantically similar concepts in
Euclidean space. We also propose a novel metric learning formulation using an
adaptive margin hinge loss, that is refined during the training phase. The
proposed scheme was applied to the MS-COCO, Flicke30K and Flickr8K datasets,
and was shown to compare favorably with contemporary state-of-the-art
approaches.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Quantization 
      
        Datasets 
      
        Text-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/maji2020cbir/">CBIR using features derived by Deep Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=CBIR using features derived by Deep Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=CBIR using features derived by Deep Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Maji Subhadip, Bose Smarajit</td> <!-- 🔧 You were missing this -->
    <td>ACM/IMS Transactions on Data Science</td>
    <td>10</td>
    <td><p>In a Content Based Image Retrieval (CBIR) System, the task is to retrieve
similar images from a large database given a query image. The usual procedure
is to extract some useful features from the query image, and retrieve images
which have similar set of features. For this purpose, a suitable similarity
measure is chosen, and images with high similarity scores are retrieved.
Naturally the choice of these features play a very important role in the
success of this system, and high level features are required to reduce the
semantic gap.
  In this paper, we propose to use features derived from pre-trained network
models from a deep-learning convolution network trained for a large image
classification problem. This approach appears to produce vastly superior
results for a variety of databases, and it outperforms many contemporary CBIR
systems. We analyse the retrieval time of the method, and also propose a
pre-clustering of the database based on the above-mentioned features which
yields comparable results in a much shorter time in most of the cases.</p>
</td>
    <td>
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/maheshwari2021scene/">Scene Graph Embeddings Using Relative Similarity Supervision</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Scene Graph Embeddings Using Relative Similarity Supervision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Scene Graph Embeddings Using Relative Similarity Supervision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Maheshwari Paridhi, Chaudhry Ritwick, Vinay Vishwa</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>10</td>
    <td><p>Scene graphs are a powerful structured representation of the underlying
content of images, and embeddings derived from them have been shown to be
useful in multiple downstream tasks. In this work, we employ a graph
convolutional network to exploit structure in scene graphs and produce image
embeddings useful for semantic image retrieval. Different from
classification-centric supervision traditionally available for learning image
representations, we address the task of learning from relative similarity
labels in a ranking context. Rooted within the contrastive learning paradigm,
we propose a novel loss function that operates on pairs of similar and
dissimilar images and imposes relative ordering between them in embedding
space. We demonstrate that this Ranking loss, coupled with an intuitive triple
sampling strategy, leads to robust representations that outperform well-known
contrastive losses on the retrieval task. In addition, we provide qualitative
evidence of how retrieved results that utilize structured scene information
capture the global context of the scene, different from visual similarity
search.</p>
</td>
    <td>
      
        AAAI 
      
        Distance-Metric-Learning 
      
        Self-Supervised 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/macdonald2021approximate/">On Approximate Nearest Neighbour Selection for Multi-Stage Dense Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=On Approximate Nearest Neighbour Selection for Multi-Stage Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=On Approximate Nearest Neighbour Selection for Multi-Stage Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Macdonald Craig, Tonellotto Nicola</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</td>
    <td>14</td>
    <td><p>Dense retrieval, which describes the use of contextualised language models
such as BERT to identify documents from a collection by leveraging approximate
nearest neighbour (ANN) techniques, has been increasing in popularity. Two
families of approaches have emerged, depending on whether documents and queries
are represented by single or multiple embeddings. ColBERT, the exemplar of the
latter, uses an ANN index and approximate scores to identify a set of candidate
documents for each query embedding, which are then re-ranked using accurate
document representations. In this manner, a large number of documents can be
retrieved for each query, hindering the efficiency of the approach. In this
work, we investigate the use of ANN scores for ranking the candidate documents,
in order to decrease the number of candidate documents being fully scored.
Experiments conducted on the MSMARCO passage ranking corpus demonstrate that,
by cutting of the candidate set by using the approximate scores to only 200
documents, we can still obtain an effective ranking without statistically
significant differences in effectiveness, and resulting in a 2x speedup in
efficiency.</p>
</td>
    <td>
      
        Efficiency 
      
        Similarity-Search 
      
        Vector-Indexing 
      
        CIKM 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/yu2019unsupervised/">Unsupervised Multi-modal Hashing for Cross-modal retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Multi-modal Hashing for Cross-modal retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Multi-modal Hashing for Cross-modal retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yu Jun, Wu Xiao-jun</td> <!-- 🔧 You were missing this -->
    <td>Cognitive Computation</td>
    <td>9</td>
    <td><p>With the advantage of low storage cost and high efficiency, hashing learning
has received much attention in the domain of Big Data. In this paper, we
propose a novel unsupervised hashing learning method to cope with this open
problem to directly preserve the manifold structure by hashing. To address this
problem, both the semantic correlation in textual space and the locally
geometric structure in the visual space are explored simultaneously in our
framework. Besides, the `2;1-norm constraint is imposed on the projection
matrices to learn the discriminative hash function for each modality. Extensive
experiments are performed to evaluate the proposed method on the three publicly
available datasets and the experimental results show that our method can
achieve superior performance over the state-of-the-art methods.</p>
</td>
    <td>
      
        Supervised 
      
        Multimodal-Retrieval 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Memory-Efficiency 
      
        Unsupervised 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/lu2018domain/">Domain-Aware SE Network for Sketch-based Image Retrieval with Multiplicative Euclidean Margin Softmax</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Domain-Aware SE Network for Sketch-based Image Retrieval with Multiplicative Euclidean Margin Softmax' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Domain-Aware SE Network for Sketch-based Image Retrieval with Multiplicative Euclidean Margin Softmax' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lu Peng, Huang Gao, Lin Hangyu, Yang Wenming, Guo Guodong, Fu Yanwei</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 29th ACM International Conference on Multimedia</td>
    <td>12</td>
    <td><p>This paper proposes a novel approach for Sketch-Based Image Retrieval (SBIR),
for which the key is to bridge the gap between sketches and photos in terms of
the data representation. Inspired by channel-wise attention explored in recent
years, we present a Domain-Aware Squeeze-and-Excitation (DASE) network, which
seamlessly incorporates the prior knowledge of sample sketch or photo into SE
module and make the SE module capable of emphasizing appropriate channels
according to domain signal. Accordingly, the proposed network can switch its
mode to achieve a better domain feature with lower intra-class discrepancy.
Moreover, while previous works simply focus on minimizing intra-class distance
and maximizing inter-class distance, we introduce a loss function, named
Multiplicative Euclidean Margin Softmax (MEMS), which introduces multiplicative
Euclidean margin into feature space and ensure that the maximum intra-class
distance is smaller than the minimum inter-class distance. This facilitates
learning a highly discriminative feature space and ensures a more accurate
image retrieval result. Extensive experiments are conducted on two widely used
SBIR benchmark datasets. Our approach achieves better results on both datasets,
surpassing the state-of-the-art methods by a large margin.</p>
</td>
    <td>
      
        Datasets 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/liu2021image/">Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Zheyuan, Rodriguez-opazo Cristian, Teney Damien, Gould Stephen</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>108</td>
    <td><p>We extend the task of composed image retrieval, where an input query consists
of an image and short textual description of how to modify the image. Existing
methods have only been applied to non-complex images within narrow domains,
such as fashion products, thereby limiting the scope of study on in-depth
visual reasoning in rich image and language contexts. To address this issue, we
collect the Compose Image Retrieval on Real-life images (CIRR) dataset, which
consists of over 36,000 pairs of crowd-sourced, open-domain images with
human-generated modifying text. To extend current methods to the open-domain,
we propose CIRPLANT, a transformer based model that leverages rich pre-trained
vision-and-language (V&amp;L) knowledge for modifying visual features conditioned
on natural language. Retrieval is then done by nearest neighbor lookup on the
modified features. We demonstrate that with a relatively simple architecture,
CIRPLANT outperforms existing methods on open-domain images, while matching
state-of-the-art accuracy on the existing narrow datasets, such as fashion.
Together with the release of CIRR, we believe this work will inspire further
research on composed image retrieval.</p>
</td>
    <td>
      
        ICCV 
      
        Datasets 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/zhang2021fast/">Fast Discrete Cross-Modal Hashing Based on Label Relaxation and Matrix Factorization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Discrete Cross-Modal Hashing Based on Label Relaxation and Matrix Factorization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Discrete Cross-Modal Hashing Based on Label Relaxation and Matrix Factorization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Donglin, Wu, Liu, Yu, Kittler</td> <!-- 🔧 You were missing this -->
    <td>2020 25th International Conference on Pattern Recognition (ICPR)</td>
    <td>9</td>
    <td><p>In recent years, cross-media retrieval has drawn considerable attention due to the exponential growth of multimedia data. Many hashing approaches have been proposed for the cross-media search task. However, there are still open problems that warrant investigation. For example, most existing supervised hashing approaches employ a binary label matrix, which achieves small margins between wrong labels (0) and true labels (1). This may affect the retrieval performance by generating many false negatives and false positives. In addition, some methods adopt a relaxation scheme to solve the binary constraints, which may cause large quantization errors. There are also some discrete hashing methods that have been presented, but most of them are time-consuming. To conquer these problems, we present a label relaxation and discrete matrix factorization method (LRMF) for cross-modal retrieval. It offers a number of innovations. First of all, the proposed approach employs a novel label relaxation scheme to control the margins adaptively, which has the benefit of reducing the quantization error. Second, by virtue of the proposed discrete matrix factorization method designed to learn the binary codes, large quantization errors caused by relaxation can be avoided. The experimental results obtained on two widely-used databases demonstrate that LRMF outperforms state-of-the-art cross-media methods.</p>
</td>
    <td>
      
        Neural-Hashing 
      
        Multimodal-Retrieval 
      
        Quantization 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/lin2021large/">Large-Scale Network Embedding in Apache Spark</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Large-Scale Network Embedding in Apache Spark' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Large-Scale Network Embedding in Apache Spark' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin Wenqing</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</td>
    <td>14</td>
    <td><p>Network embedding has been widely used in social recommendation and network
analysis, such as recommendation systems and anomaly detection with graphs.
However, most of previous approaches cannot handle large graphs efficiently,
due to that (i) computation on graphs is often costly and (ii) the size of
graph or the intermediate results of vectors could be prohibitively large,
rendering it difficult to be processed on a single machine. In this paper, we
propose an efficient and effective distributed algorithm for network embedding
on large graphs using Apache Spark, which recursively partitions a graph into
several small-sized subgraphs to capture the internal and external structural
information of nodes, and then computes the network embedding for each subgraph
in parallel. Finally, by aggregating the outputs on all subgraphs, we obtain
the embeddings of nodes in a linear cost. After that, we demonstrate in various
experiments that our proposed approach is able to handle graphs with billions
of edges within a few hours and is at least 4 times faster than the
state-of-the-art approaches. Besides, it achieves up to \(4.25%\) and \(4.27%\)
improvements on link prediction and node classification tasks respectively. In
the end, we deploy the proposed algorithms in two online games of Tencent with
the applications of friend recommendation and item recommendation, which
improve the competitors by up to \(91.11%\) in running time and up to \(12.80%\)
in the corresponding evaluation metrics.</p>
</td>
    <td>
      
        KDD 
      
        Scalability 
      
        Recommender-Systems 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/zhang2025deep/">Deep Center-Based Dual-Constrained Hashing for Discriminative Face Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Center-Based Dual-Constrained Hashing for Discriminative Face Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Center-Based Dual-Constrained Hashing for Discriminative Face Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Ming, Zhe, Yan</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition</td>
    <td>21</td>
    <td><p>With the advantages of low storage cost and extremely fast retrieval speed, deep hashing methods have attracted much attention for image retrieval recently. However, large-scale face image retrieval with significant intra-class variations is still challenging. Neither existing pairwise/triplet labels-based nor softmax classification loss-based deep hashing works can generate compact and discriminative binary codes. Considering these issues, we propose a center-based framework integrating end-to-end hashing learning and class centers learning simultaneously. The framework minimizes the intra-class variance by clustering intra-class samples into a learnable class center. To strengthen inter-class separability, it additionally imposes a novel regularization term to enlarge the Hamming distance between pairwise class centers. Moreover, a simple yet effective regression matrix is introduced to encourage intra-class samples to generate the same binary codes, which further enhances the hashing codes compactness. Experiments on four large-scale datasets show the proposed method outperforms state-of-the-art baselines under various code lengths and commonly-used evaluation metrics.</p>
</td>
    <td>
      
        Scalability 
      
        Efficiency 
      
        Evaluation 
      
        Datasets 
      
        Compact-Codes 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Neural-Hashing 
      
        Memory-Efficiency 
      
        Image-Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/liang2021dynamic/">Dynamic Sampling for Deep Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Dynamic Sampling for Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Dynamic Sampling for Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liang Chang-hui, Zhao Wan-lei, Chen Run-qing</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition Letters</td>
    <td>5</td>
    <td><p>Deep metric learning maps visually similar images onto nearby locations and
visually dissimilar images apart from each other in an embedding manifold. The
learning process is mainly based on the supplied image negative and positive
training pairs. In this paper, a dynamic sampling strategy is proposed to
organize the training pairs in an easy-to-hard order to feed into the network.
It allows the network to learn general boundaries between categories from the
easy training pairs at its early stages and finalize the details of the model
mainly relying on the hard training samples in the later. Compared to the
existing training sample mining approaches, the hard samples are mined with
little harm to the learned general model. This dynamic sampling strategy is
formularized as two simple terms that are compatible with various loss
functions. Consistent performance boost is observed when it is integrated with
several popular loss functions on fashion search, fine-grained classification,
and person re-identification tasks.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/lin2021deep/">Deep Self-Adaptive Hashing for Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Self-Adaptive Hashing for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Self-Adaptive Hashing for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin Qinghong, Chen Xiaojun, Zhang Qin, Tian Shangxuan, Chen Yudong</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</td>
    <td>8</td>
    <td><p>Hashing technology has been widely used in image retrieval due to its
computational and storage efficiency. Recently, deep unsupervised hashing
methods have attracted increasing attention due to the high cost of human
annotations in the real world and the superiority of deep learning technology.
However, most deep unsupervised hashing methods usually pre-compute a
similarity matrix to model the pairwise relationship in the pre-trained feature
space. Then this similarity matrix would be used to guide hash learning, in
which most of the data pairs are treated equivalently. The above process is
confronted with the following defects: 1) The pre-computed similarity matrix is
inalterable and disconnected from the hash learning process, which cannot
explore the underlying semantic information. 2) The informative data pairs may
be buried by the large number of less-informative data pairs. To solve the
aforementioned problems, we propose a Deep Self-Adaptive Hashing (DSAH) model
to adaptively capture the semantic information with two special designs:
Adaptive Neighbor Discovery (AND) and Pairwise Information Content (PIC).
Firstly, we adopt the AND to initially construct a neighborhood-based
similarity matrix, and then refine this initial similarity matrix with a novel
update strategy to further investigate the semantic structure behind the
learned representation. Secondly, we measure the priorities of data pairs with
PIC and assign adaptive weights to them, which is relies on the assumption that
more dissimilar data pairs contain more discriminative information for hash
learning. Extensive experiments on several datasets demonstrate that the above
two technologies facilitate the deep hashing model to achieve superior
performance.</p>
</td>
    <td>
      
        Evaluation 
      
        Efficiency 
      
        Datasets 
      
        Unsupervised 
      
        CIKM 
      
        Hashing-Methods 
      
        Neural-Hashing 
      
        Image-Retrieval 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/li2025self/">Self-Supervised Video Hashing via Bidirectional Transformers</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Self-Supervised Video Hashing via Bidirectional Transformers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Self-Supervised Video Hashing via Bidirectional Transformers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Shuyan, Li, Lu, Zhou</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>38</td>
    <td><p>Most existing unsupervised video hashing methods are built on unidirectional models with less reliable training objectives, which underuse the correlations among frames and the similarity structure between videos. To enable efficient scalable video retrieval, we propose a self-supervised video Hashing method based on Bidirectional Transformers (BTH). Based on the encoder-decoder structure of transformers, we design a visual cloze task to fully exploit the bidirectional correlations between frames. To unveil the similarity structure between unlabeled video data, we further develop a similarity reconstruction task by establishing reliable and effective similarity connections in the video space. Furthermore, we develop a cluster assignment task to exploit the structural statistics of the whole dataset such that more discriminative binary codes can be learned. Extensive experiments implemented on three public benchmark datasets, FCVID, ActivityNet and YFCC, demonstrate the superiority of our proposed approach.</p>
</td>
    <td>
      
        Video-Retrieval 
      
        Datasets 
      
        CVPR 
      
        Self-Supervised 
      
        Supervised 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/liang2020dynamic/">Dynamic Sampling for Deep Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Dynamic Sampling for Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Dynamic Sampling for Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liang Chang-hui, Zhao Wan-lei, Chen Run-qing</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition Letters</td>
    <td>5</td>
    <td><p>Deep metric learning maps visually similar images onto nearby locations and
visually dissimilar images apart from each other in an embedding manifold. The
learning process is mainly based on the supplied image negative and positive
training pairs. In this paper, a dynamic sampling strategy is proposed to
organize the training pairs in an easy-to-hard order to feed into the network.
It allows the network to learn general boundaries between categories from the
easy training pairs at its early stages and finalize the details of the model
mainly relying on the hard training samples in the later. Compared to the
existing training sample mining approaches, the hard samples are mined with
little harm to the learned general model. This dynamic sampling strategy is
formularized as two simple terms that are compatible with various loss
functions. Consistent performance boost is observed when it is integrated with
several popular loss functions on fashion search, fine-grained classification,
and person re-identification tasks.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/li2021more/">More Robust Dense Retrieval with Contrastive Dual Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=More Robust Dense Retrieval with Contrastive Dual Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=More Robust Dense Retrieval with Contrastive Dual Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Yizhi, Liu Zhenghao, Xiong Chenyan, Liu Zhiyuan</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2021 ACM SIGIR International Conference on Theory of Information Retrieval</td>
    <td>23</td>
    <td><p>Dense retrieval conducts text retrieval in the embedding space and has shown
many advantages compared to sparse retrieval. Existing dense retrievers
optimize representations of queries and documents with contrastive training and
map them to the embedding space. The embedding space is optimized by aligning
the matched query-document pairs and pushing the negative documents away from
the query. However, in such training paradigm, the queries are only optimized
to align to the documents and are coarsely positioned, leading to an
anisotropic query embedding space. In this paper, we analyze the embedding
space distributions and propose an effective training paradigm, Contrastive
Dual Learning for Approximate Nearest Neighbor (DANCE) to learn fine-grained
query representations for dense retrieval. DANCE incorporates an additional
dual training object of query retrieval, inspired by the classic information
retrieval training axiom, query likelihood. With contrastive learning, the dual
training object of DANCE learns more tailored representations for queries and
documents to keep the embedding space smooth and uniform, thriving on the
ranking performance of DANCE on the MS MARCO document retrieval task. Different
from ANCE that only optimized with the document retrieval task, DANCE
concentrates the query embeddings closer to document representations while
making the document distribution more discriminative. Such concentrated query
embedding distribution assigns more uniform negative sampling probabilities to
queries and helps to sufficiently optimize query representations in the query
retrieval task. Our codes are released at https://github.com/thunlp/DANCE.</p>
</td>
    <td>
      
        Self-Supervised 
      
        SIGIR 
      
        Text-Retrieval 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/li2021self/">Self-Supervised Video Hashing via Bidirectional Transformers</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Self-Supervised Video Hashing via Bidirectional Transformers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Self-Supervised Video Hashing via Bidirectional Transformers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Shuyan, Li, Lu, Zhou</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>38</td>
    <td><p>Most existing unsupervised video hashing methods are built on unidirectional models with less reliable training objectives, which underuse the correlations among frames and the similarity structure between videos. To enable efficient scalable video retrieval, we propose a self-supervised video Hashing method based on Bidirectional Transformers (BTH). Based on the encoder-decoder structure of transformers, we design a visual cloze task to fully exploit the bidirectional correlations between frames. To unveil the similarity structure between unlabeled video data, we further develop a similarity reconstruction task by establishing reliable and effective similarity connections in the video space. Furthermore, we develop a cluster assignment task to exploit the structural statistics of the whole dataset such that more discriminative binary codes can be learned. Extensive experiments implemented on three public benchmark datasets, FCVID, ActivityNet and YFCC, demonstrate the superiority of our proposed approach.</p>
</td>
    <td>
      
        Video-Retrieval 
      
        Datasets 
      
        CVPR 
      
        Self-Supervised 
      
        Supervised 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/levi2020rethinking/">Rethinking preventing class-collapsing in metric learning with margin-based losses</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Rethinking preventing class-collapsing in metric learning with margin-based losses' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Rethinking preventing class-collapsing in metric learning with margin-based losses' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Levi Elad, Xiao Tete, Wang Xiaolong, Darrell Trevor</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>15</td>
    <td><p>Metric learning seeks perceptual embeddings where visually similar instances
are close and dissimilar instances are apart, but learned representations can
be sub-optimal when the distribution of intra-class samples is diverse and
distinct sub-clusters are present. Although theoretically with optimal
assumptions, margin-based losses such as the triplet loss and margin loss have
a diverse family of solutions. We theoretically prove and empirically show that
under reasonable noise assumptions, margin-based losses tend to project all
samples of a class with various modes onto a single point in the embedding
space, resulting in a class collapse that usually renders the space ill-sorted
for classification or retrieval. To address this problem, we propose a simple
modification to the embedding losses such that each sample selects its nearest
same-class counterpart in a batch as the positive element in the tuple. This
allows for the presence of multiple sub-clusters within each class. The
adaptation can be integrated into a wide range of metric learning losses. The
proposed sampling method demonstrates clear benefits on various fine-grained
image retrieval datasets over a variety of existing losses; qualitative
retrieval results show that samples with similar visual patterns are indeed
closer in the embedding space.</p>
</td>
    <td>
      
        ICCV 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/li2020task/">Task-adaptive Asymmetric Deep Cross-modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Task-adaptive Asymmetric Deep Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Task-adaptive Asymmetric Deep Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Fengling, Wang Tong, Zhu Lei, Zhang Zheng, Wang Xinhua</td> <!-- 🔧 You were missing this -->
    <td>Knowledge-Based Systems</td>
    <td>17</td>
    <td><p>Supervised cross-modal hashing aims to embed the semantic correlations of
heterogeneous modality data into the binary hash codes with discriminative
semantic labels. Because of its advantages on retrieval and storage efficiency,
it is widely used for solving efficient cross-modal retrieval. However,
existing researches equally handle the different tasks of cross-modal
retrieval, and simply learn the same couple of hash functions in a symmetric
way for them. Under such circumstance, the uniqueness of different cross-modal
retrieval tasks are ignored and sub-optimal performance may be brought.
Motivated by this, we present a Task-adaptive Asymmetric Deep Cross-modal
Hashing (TA-ADCMH) method in this paper. It can learn task-adaptive hash
functions for two sub-retrieval tasks via simultaneous modality representation
and asymmetric hash learning. Unlike previous cross-modal hashing approaches,
our learning framework jointly optimizes semantic preserving that transforms
deep features of multimedia data into binary hash codes, and the semantic
regression which directly regresses query modality representation to explicit
label. With our model, the binary codes can effectively preserve semantic
correlations across different modalities, meanwhile, adaptively capture the
query semantics. The superiority of TA-ADCMH is proved on two standard datasets
from many aspects.</p>
</td>
    <td>
      
        Supervised 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Datasets 
      
        Compact-Codes 
      
        Efficiency 
      
        Evaluation 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/leyvavallina2021generalized/">Generalized Contrastive Optimization of Siamese Networks for Place Recognition</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Generalized Contrastive Optimization of Siamese Networks for Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Generalized Contrastive Optimization of Siamese Networks for Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Leyva-vallina María, Strisciuglio Nicola, Petkov Nicolai</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>13</td>
    <td><p>Visual place recognition is a challenging task in computer vision and a key
component of camera-based localization and navigation systems. Recently,
Convolutional Neural Networks (CNNs) achieved high results and good
generalization capabilities. They are usually trained using pairs or triplets
of images labeled as either similar or dissimilar, in a binary fashion. In
practice, the similarity between two images is not binary, but continuous.
Furthermore, training these CNNs is computationally complex and involves costly
pair and triplet mining strategies. We propose a Generalized Contrastive loss
(GCL) function that relies on image similarity as a continuous measure, and use
it to train a siamese CNN. Furthermore, we present three techniques for
automatic annotation of image pairs with labels indicating their degree of
similarity, and deploy them to re-annotate the MSLS, TB-Places, and 7Scenes
datasets. We demonstrate that siamese CNNs trained using the GCL function and
the improved annotations consistently outperform their binary counterparts. Our
models trained on MSLS outperform the state-of-the-art methods, including
NetVLAD, NetVLAD-SARE, AP-GeM and Patch-NetVLAD, and generalize well on the
Pittsburgh30k, Tokyo 24/7, RobotCar Seasons v2 and Extended CMU Seasons
datasets. Furthermore, training a siamese network using the GCL function does
not require complex pair mining. We release the source code at
https://github.com/marialeyvallina/generalized_contrastive_loss.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/lassance2022composite/">Composite Code Sparse Autoencoders for first stage retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Composite Code Sparse Autoencoders for first stage retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Composite Code Sparse Autoencoders for first stage retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lassance Carlos, Formal Thibault, Clinchant Stephane</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>5</td>
    <td><p>We propose a Composite Code Sparse Autoencoder (CCSA) approach for
Approximate Nearest Neighbor (ANN) search of document representations based on
Siamese-BERT models. In Information Retrieval (IR), the ranking pipeline is
generally decomposed in two stages: the first stage focus on retrieving a
candidate set from the whole collection. The second stage re-ranks the
candidate set by relying on more complex models. Recently, Siamese-BERT models
have been used as first stage ranker to replace or complement the traditional
bag-of-word models. However, indexing and searching a large document collection
require efficient similarity search on dense vectors and this is why ANN
techniques come into play. Since composite codes are naturally sparse, we first
show how CCSA can learn efficient parallel inverted index thanks to an
uniformity regularizer. Second, CCSA can be used as a binary quantization
method and we propose to combine it with the recent graph based ANN techniques.
Our experiments on MSMARCO dataset reveal that CCSA outperforms IVF with
product quantization. Furthermore, CCSA binary quantization is beneficial for
the index size, and memory usage for the graph-based HNSW method, while
maintaining a good level of recall and MRR. Third, we compare with recent
supervised quantization methods for image retrieval and find that CCSA is able
to outperform them.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Vector-Indexing 
      
        Supervised 
      
        Image-Retrieval 
      
        Graph-Based-ANN 
      
        Datasets 
      
        SIGIR 
      
        Memory-Efficiency 
      
        Quantization 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/krishna2021evaluating/">Evaluating Contrastive Models for Instance-based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Evaluating Contrastive Models for Instance-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Evaluating Contrastive Models for Instance-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Krishna Tarun, Mcguinness Kevin, O'connor Noel</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2021 International Conference on Multimedia Retrieval</td>
    <td>23</td>
    <td><p>In this work, we evaluate contrastive models for the task of image retrieval.
We hypothesise that models that are learned to encode semantic similarity among
instances via discriminative learning should perform well on the task of image
retrieval, where relevancy is defined in terms of instances of the same object.
Through our extensive evaluation, we find that representations from models
trained using contrastive methods perform on-par with (and outperforms) a
pre-trained supervised baseline trained on the ImageNet labels in retrieval
tasks under various configurations. This is remarkable given that the
contrastive models require no explicit supervision. Thus, we conclude that
these models can be used to bootstrap base models to build more robust image
retrieval engines.</p>
</td>
    <td>
      
        Evaluation 
      
        Image-Retrieval 
      
        Supervised 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/kosti%C4%872021multi/">Multi-modal Retrieval of Tables and Texts Using Tri-encoder Models</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multi-modal Retrieval of Tables and Texts Using Tri-encoder Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multi-modal Retrieval of Tables and Texts Using Tri-encoder Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kostić Bogdan, Risch Julian, Möller Timo</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 3rd Workshop on Machine Reading for Question Answering</td>
    <td>12</td>
    <td><p>Open-domain extractive question answering works well on textual data by first
retrieving candidate texts and then extracting the answer from those
candidates. However, some questions cannot be answered by text alone but
require information stored in tables. In this paper, we present an approach for
retrieving both texts and tables relevant to a question by jointly encoding
texts, tables and questions into a single vector space. To this end, we create
a new multi-modal dataset based on text and table datasets from related work
and compare the retrieval performance of different encoding schemata. We find
that dense vector embeddings of transformer models outperform sparse embeddings
on four out of six evaluation datasets. Comparing different dense embedding
models, tri-encoders with one encoder for each question, text and table,
increase retrieval performance compared to bi-encoders with one encoder for the
question and one for both text and tables. We release the newly created
multi-modal dataset to the community so that it can be used for training and
evaluation.</p>
</td>
    <td>
      
        Evaluation 
      
        Graph-Based-ANN 
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/kordopatiszilos2021leveraging/">Leveraging EfficientNet and Contrastive Learning for Accurate Global-scale Location Estimation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Leveraging EfficientNet and Contrastive Learning for Accurate Global-scale Location Estimation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Leveraging EfficientNet and Contrastive Learning for Accurate Global-scale Location Estimation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kordopatis-zilos Giorgos, Galopoulos Panagiotis, Papadopoulos Symeon, Kompatsiaris Ioannis</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2021 International Conference on Multimedia Retrieval</td>
    <td>15</td>
    <td><p>In this paper, we address the problem of global-scale image geolocation,
proposing a mixed classification-retrieval scheme. Unlike other methods that
strictly tackle the problem as a classification or retrieval task, we combine
the two practices in a unified solution leveraging the advantages of each
approach with two different modules. The first leverages the EfficientNet
architecture to assign images to a specific geographic cell in a robust way.
The second introduces a new residual architecture that is trained with
contrastive learning to map input images to an embedding space that minimizes
the pairwise geodesic distance of same-location images. For the final location
estimation, the two modules are combined with a search-within-cell scheme,
where the locations of most similar images from the predicted geographic cell
are aggregated based on a spatial clustering scheme. Our approach demonstrates
very competitive performance on four public datasets, achieving new
state-of-the-art performance in fine granularity scales, i.e., 15.0% at 1km
range on Im2GPS3k.</p>
</td>
    <td>
      
        Evaluation 
      
        Multimodal-Retrieval 
      
        Self-Supervised 
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/%C3%B1anculef2020self/">Self-Supervised Bernoulli Autoencoders for Semi-Supervised Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Self-Supervised Bernoulli Autoencoders for Semi-Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Self-Supervised Bernoulli Autoencoders for Semi-Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ñanculef Ricardo, Mena Francisco, Macaluso Antonio, Lodi Stefano, Sartori Claudio</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>5</td>
    <td><p>Semantic hashing is an emerging technique for large-scale similarity search
based on representing high-dimensional data using similarity-preserving binary
codes used for efficient indexing and search. It has recently been shown that
variational autoencoders, with Bernoulli latent representations parametrized by
neural nets, can be successfully trained to learn such codes in supervised and
unsupervised scenarios, improving on more traditional methods thanks to their
ability to handle the binary constraints architecturally. However, the scenario
where labels are scarce has not been studied yet.
  This paper investigates the robustness of hashing methods based on
variational autoencoders to the lack of supervision, focusing on two
semi-supervised approaches currently in use. The first augments the variational
autoencoder’s training objective to jointly model the distribution over the
data and the class labels. The second approach exploits the annotations to
define an additional pairwise loss that enforces consistency between the
similarity in the code (Hamming) space and the similarity in the label space.
Our experiments show that both methods can significantly increase the hash
codes’ quality. The pairwise approach can exhibit an advantage when the number
of labelled points is large. However, we found that this method degrades
quickly and loses its advantage when labelled samples decrease. To circumvent
this problem, we propose a novel supervision method in which the model uses its
label distribution predictions to implement the pairwise objective. Compared to
the best baseline, this procedure yields similar performance in fully
supervised settings but improves the results significantly when labelled data
is scarce. Our code is made publicly available at
https://github.com/amacaluso/SSB-VAE.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Unsupervised 
      
        Supervised 
      
        Text-Retrieval 
      
        Hashing-Methods 
      
        Neural-Hashing 
      
        Self-Supervised 
      
        Scalability 
      
        Evaluation 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/kobayashi2020decomposing/">Decomposing Normal and Abnormal Features of Medical Images for Content-based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Decomposing Normal and Abnormal Features of Medical Images for Content-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Decomposing Normal and Abnormal Features of Medical Images for Content-based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kobayashi Kazuma, Hataya Ryuichiro, Kurose Yusuke, Harada Tatsuya, Hamamoto Ryuji</td> <!-- 🔧 You were missing this -->
    <td>Medical Image Analysis</td>
    <td>31</td>
    <td><p>Medical images can be decomposed into normal and abnormal features, which is
considered as the compositionality. Based on this idea, we propose an
encoder-decoder network to decompose a medical image into two discrete latent
codes: a normal anatomy code and an abnormal anatomy code. Using these latent
codes, we demonstrate a similarity retrieval by focusing on either normal or
abnormal features of medical images.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/kim2021multi/">Multi-level Distance Regularization for Deep Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multi-level Distance Regularization for Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multi-level Distance Regularization for Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kim Yonghyun, Park Wonpyo</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>11</td>
    <td><p>We propose a novel distance-based regularization method for deep metric
learning called Multi-level Distance Regularization (MDR). MDR explicitly
disturbs a learning procedure by regularizing pairwise distances between
embedding vectors into multiple levels that represents a degree of similarity
between a pair. In the training stage, the model is trained with both MDR and
an existing loss function of deep metric learning, simultaneously; the two
losses interfere with the objective of each other, and it makes the learning
process difficult. Moreover, MDR prevents some examples from being ignored or
overly influenced in the learning process. These allow the parameters of the
embedding network to be settle on a local optima with better generalization.
Without bells and whistles, MDR with simple Triplet loss achieves
the-state-of-the-art performance in various benchmark datasets: CUB-200-2011,
Cars-196, Stanford Online Products, and In-Shop Clothes Retrieval. We
extensively perform ablation studies on its behaviors to show the effectiveness
of MDR. By easily adopting our MDR, the previous approaches can be improved in
performance and generalization ability.</p>
</td>
    <td>
      
        AAAI 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/jin2025unsupervised/">Unsupervised Discrete Hashing with Affinity Similarity</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Discrete Hashing with Affinity Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Discrete Hashing with Affinity Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jin Sheng, Yao, Zhou, Liu, Huang, Hua</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>14</td>
    <td><p>In recent years, supervised hashing has been validated to greatly boost the performance of image retrieval. However, the label-hungry property requires massive label collection, making it intractable in practical scenarios. To liberate the model training procedure from laborious manual annotations, some unsupervised methods are proposed. However, the following two factors make unsupervised algorithms inferior to their supervised counterparts: (1) Without manually-defined labels, it is difficult to capture the semantic information across data, which is of crucial importance to guide robust binary code learning. (2) The widely adopted relaxation on binary constraints results in quantization error accumulation in the optimization procedure. To address the above-mentioned problems, in this paper, we propose a novel Unsupervised Discrete Hashing method (UDH). Specifically, to capture the semantic information, we propose a balanced graph-based semantic loss which explores the affinity priors in the original feature space. Then, we propose a novel self-supervised loss, termed orthogonal consistent loss, which can leverage semantic loss of instance and impose independence of codes. Moreover, by integrating the discrete optimization into the proposed unsupervised framework, the binary constraints are consistently preserved, alleviating the influence of quantization errors. Extensive experiments demonstrate that UDH outperforms state-of-the-art unsupervised methods for image retrieval.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Neural-Hashing 
      
        Self-Supervised 
      
        Tools-&-Libraries 
      
        Supervised 
      
        Compact-Codes 
      
        Quantization 
      
        Hashing-Methods 
      
        Evaluation 
      
        Unsupervised 
      
        Graph-Based-ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/jin2021unsupervised/">Unsupervised Discrete Hashing with Affinity Similarity</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Discrete Hashing with Affinity Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Discrete Hashing with Affinity Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jin Sheng, Yao, Zhou, Liu, Huang, Hua</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>14</td>
    <td><p>In recent years, supervised hashing has been validated to greatly boost the performance of image retrieval. However, the label-hungry property requires massive label collection, making it intractable in practical scenarios. To liberate the model training procedure from laborious manual annotations, some unsupervised methods are proposed. However, the following two factors make unsupervised algorithms inferior to their supervised counterparts: (1) Without manually-defined labels, it is difficult to capture the semantic information across data, which is of crucial importance to guide robust binary code learning. (2) The widely adopted relaxation on binary constraints results in quantization error accumulation in the optimization procedure. To address the above-mentioned problems, in this paper, we propose a novel Unsupervised Discrete Hashing method (UDH). Specifically, to capture the semantic information, we propose a balanced graph-based semantic loss which explores the affinity priors in the original feature space. Then, we propose a novel self-supervised loss, termed orthogonal consistent loss, which can leverage semantic loss of instance and impose independence of codes. Moreover, by integrating the discrete optimization into the proposed unsupervised framework, the binary constraints are consistently preserved, alleviating the influence of quantization errors. Extensive experiments demonstrate that UDH outperforms state-of-the-art unsupervised methods for image retrieval.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Neural-Hashing 
      
        Self-Supervised 
      
        Tools-&-Libraries 
      
        Supervised 
      
        Compact-Codes 
      
        Quantization 
      
        Hashing-Methods 
      
        Evaluation 
      
        Unsupervised 
      
        Graph-Based-ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/jose2020optimized/">Optimized Feature Space Learning for Generating Efficient Binary Codes for Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Optimized Feature Space Learning for Generating Efficient Binary Codes for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Optimized Feature Space Learning for Generating Efficient Binary Codes for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jose Abin, Ottlik Erik Stefan, Rohlfing Christian, Ohm Jens-rainer</td> <!-- 🔧 You were missing this -->
    <td>Signal Processing: Image Communication</td>
    <td>7</td>
    <td><p>In this paper we propose an approach for learning low dimensional optimized
feature space with minimum intra-class variance and maximum inter-class
variance. We address the problem of high-dimensionality of feature vectors
extracted from neural networks by taking care of the global statistics of
feature space. Classical approach of Linear Discriminant Analysis (LDA) is
generally used for generating an optimized low dimensional feature space for
single-labeled images. Since, image retrieval involves both multi-labeled and
single-labeled images, we utilize the equivalence between LDA and Canonical
Correlation Analysis (CCA) to generate an optimized feature space for
single-labeled images and use CCA to generate an optimized feature space for
multi-labeled images. Our approach correlates the projections of feature
vectors with label vectors in our CCA based network architecture. The neural
network minimize a loss function which maximizes the correlation coefficients.
We binarize our generated feature vectors with the popular Iterative
Quantization (ITQ) approach and also propose an ensemble network to generate
binary codes of desired bit length for image retrieval. Our measurement of mean
average precision shows competitive results on other state-of-the-art
single-labeled and multi-labeled image retrieval datasets.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Datasets 
      
        Compact-Codes 
      
        Quantization 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/jang2021ultra/">Ultra-High Dimensional Sparse Representations with Binarization for Efficient Text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Ultra-High Dimensional Sparse Representations with Binarization for Efficient Text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Ultra-High Dimensional Sparse Representations with Binarization for Efficient Text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jang Kyoung-rok, Kang Junmo, Hong Giwon, Myaeng Sung-hyon, Park Joohee, Yoon Taewon, Seo Heecheol</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</td>
    <td>9</td>
    <td><p>The semantic matching capabilities of neural information retrieval can
ameliorate synonymy and polysemy problems of symbolic approaches. However,
neural models’ dense representations are more suitable for re-ranking, due to
their inefficiency. Sparse representations, either in symbolic or latent form,
are more efficient with an inverted index. Taking the merits of the sparse and
dense representations, we propose an ultra-high dimensional (UHD)
representation scheme equipped with directly controllable sparsity. UHD’s large
capacity and minimal noise and interference among the dimensions allow for
binarized representations, which are highly efficient for storage and search.
Also proposed is a bucketing method, where the embeddings from multiple layers
of BERT are selected/merged to represent diverse linguistic aspects. We test
our models with MS MARCO and TREC CAR, showing that our models outperforms
other sparse models</p>
</td>
    <td>
      
        EMNLP 
      
        Hybrid-ANN-Methods 
      
        Re-Ranking 
      
        Text-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/jang2021self/">Self-supervised Product Quantization for Deep Unsupervised Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Self-supervised Product Quantization for Deep Unsupervised Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Self-supervised Product Quantization for Deep Unsupervised Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jang Young Kyun, Cho Nam Ik</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>52</td>
    <td><p>Supervised deep learning-based hash and vector quantization are enabling fast
and large-scale image retrieval systems. By fully exploiting label annotations,
they are achieving outstanding retrieval performances compared to the
conventional methods. However, it is painstaking to assign labels precisely for
a vast amount of training data, and also, the annotation process is
error-prone. To tackle these issues, we propose the first deep unsupervised
image retrieval method dubbed Self-supervised Product Quantization (SPQ)
network, which is label-free and trained in a self-supervised manner. We design
a Cross Quantized Contrastive learning strategy that jointly learns codewords
and deep visual descriptors by comparing individually transformed images
(views). Our method analyzes the image contents to extract descriptive
features, allowing us to understand image representations for accurate
retrieval. By conducting extensive experiments on benchmarks, we demonstrate
that the proposed method yields state-of-the-art results even without
supervised pretraining.</p>
</td>
    <td>
      
        ICCV 
      
        Unsupervised 
      
        Supervised 
      
        Image-Retrieval 
      
        Quantization 
      
        Self-Supervised 
      
        Scalability 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/yuan2021multimodal/">Multimodal Contrastive Training for Visual Representation Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multimodal Contrastive Training for Visual Representation Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multimodal Contrastive Training for Visual Representation Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yuan Xin, Lin Zhe, Kuen Jason, Zhang Jianming, Wang Yilin, Maire Michael, Kale Ajinkya, Faieta Baldo</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>125</td>
    <td><p>We develop an approach to learning visual representations that embraces
multimodal data, driven by a combination of intra- and inter-modal similarity
preservation objectives. Unlike existing visual pre-training methods, which
solve a proxy prediction task in a single domain, our method exploits intrinsic
data properties within each modality and semantic information from cross-modal
correlation simultaneously, hence improving the quality of learned visual
representations. By including multimodal training in a unified framework with
different types of contrastive losses, our method can learn more powerful and
generic visual features. We first train our model on COCO and evaluate the
learned visual representations on various downstream tasks including image
classification, object detection, and instance segmentation. For example, the
visual representations pre-trained on COCO by our method achieve
state-of-the-art top-1 validation accuracy of \(55.3%\) on ImageNet
classification, under the common transfer protocol. We also evaluate our method
on the large-scale Stock images dataset and show its effectiveness on
multi-label image tagging, and cross-modal retrieval tasks.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        CVPR 
      
        Scalability 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/yuan2022exploring/">Exploring a Fine-Grained Multiscale Method for Cross-Modal Remote Sensing Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Exploring a Fine-Grained Multiscale Method for Cross-Modal Remote Sensing Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Exploring a Fine-Grained Multiscale Method for Cross-Modal Remote Sensing Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yuan Zhiqiang, Zhang Wenkai, Fu Kun, Li Xuan, Deng Chubo, Wang Hongqi, Sun Xian</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Geoscience and Remote Sensing</td>
    <td>110</td>
    <td><p>Remote sensing (RS) cross-modal text-image retrieval has attracted extensive
attention for its advantages of flexible input and efficient query. However,
traditional methods ignore the characteristics of multi-scale and redundant
targets in RS image, leading to the degradation of retrieval accuracy. To cope
with the problem of multi-scale scarcity and target redundancy in RS multimodal
retrieval task, we come up with a novel asymmetric multimodal feature matching
network (AMFMN). Our model adapts to multi-scale feature inputs, favors
multi-source retrieval methods, and can dynamically filter redundant features.
AMFMN employs the multi-scale visual self-attention (MVSA) module to extract
the salient features of RS image and utilizes visual features to guide the text
representation. Furthermore, to alleviate the positive samples ambiguity caused
by the strong intraclass similarity in RS image, we propose a triplet loss
function with dynamic variable margin based on prior similarity of sample
pairs. Finally, unlike the traditional RS image-text dataset with coarse text
and higher intraclass similarity, we construct a fine-grained and more
challenging Remote sensing Image-Text Match dataset (RSITMD), which supports RS
image retrieval through keywords and sentence separately and jointly.
Experiments on four RS text-image datasets demonstrate that the proposed model
can achieve state-of-the-art performance in cross-modal RS text-image retrieval
task.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/hekmatfar2020embedding/">Embedding Ranking-Oriented Recommender System Graphs</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Embedding Ranking-Oriented Recommender System Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Embedding Ranking-Oriented Recommender System Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hekmatfar Taher, Haratizadeh Saman, Goliaei Sama</td> <!-- 🔧 You were missing this -->
    <td>Expert Systems with Applications</td>
    <td>8</td>
    <td><p>Graph-based recommender systems (GRSs) analyze the structural information in
the graphical representation of data to make better recommendations, especially
when the direct user-item relation data is sparse. Ranking-oriented GRSs that
form a major class of recommendation systems, mostly use the graphical
representation of preference (or rank) data for measuring node similarities,
from which they can infer a recommendation list using a neighborhood-based
mechanism. In this paper, we propose PGRec, a novel graph-based
ranking-oriented recommendation framework. PGRec models the preferences of the
users over items, by a novel graph structure called PrefGraph. This graph is
then exploited by an improved embedding approach, taking advantage of both
factorization and deep learning methods, to extract vectors representing users,
items, and preferences. The resulting embedding are then used for predicting
users’ unknown pairwise preferences from which the final recommendation lists
are inferred. We have evaluated the performance of the proposed method against
the state of the art model-based and neighborhood-based recommendation methods,
and our experiments show that PGRec outperforms the baseline algorithms up to
3.2% in terms of NDCG@10 in different MovieLens datasets.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Graph-Based-ANN 
      
        Datasets 
      
        Recommender-Systems 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/hansen2021unsupervised/">Unsupervised Multi-Index Semantic Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Multi-Index Semantic Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Multi-Index Semantic Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hansen Christian, Hansen Casper, Simonsen Jakob Grue, Alstrup Stephen, Lioma Christina</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Web Conference 2021</td>
    <td>8</td>
    <td><p>Semantic hashing represents documents as compact binary vectors (hash codes)
and allows both efficient and effective similarity search in large-scale
information retrieval. The state of the art has primarily focused on learning
hash codes that improve similarity search effectiveness, while assuming a
brute-force linear scan strategy for searching over all the hash codes, even
though much faster alternatives exist. One such alternative is multi-index
hashing, an approach that constructs a smaller candidate set to search over,
which depending on the distribution of the hash codes can lead to sub-linear
search time. In this work, we propose Multi-Index Semantic Hashing (MISH), an
unsupervised hashing model that learns hash codes that are both effective and
highly efficient by being optimized for multi-index hashing. We derive novel
training objectives, which enable to learn hash codes that reduce the candidate
sets produced by multi-index hashing, while being end-to-end trainable. In
fact, our proposed training objectives are model agnostic, i.e., not tied to
how the hash codes are generated specifically in MISH, and are straight-forward
to include in existing and future semantic hashing models. We experimentally
compare MISH to state-of-the-art semantic hashing baselines in the task of
document similarity search. We find that even though multi-index hashing also
improves the efficiency of the baselines compared to a linear scan, they are
still upwards of 33% slower than MISH, while MISH is still able to obtain
state-of-the-art effectiveness.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Vector-Indexing 
      
        Supervised 
      
        Text-Retrieval 
      
        Hashing-Methods 
      
        Neural-Hashing 
      
        Unsupervised 
      
        Scalability 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/han2021text/">Text-Based Person Search with Limited Data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Text-Based Person Search with Limited Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Text-Based Person Search with Limited Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Han Xiao, He Sen, Zhang Li, Xiang Tao</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>36</td>
    <td><p>Text-based person search (TBPS) aims at retrieving a target person from an
image gallery with a descriptive text query. Solving such a fine-grained
cross-modal retrieval task is challenging, which is further hampered by the
lack of large-scale datasets. In this paper, we present a framework with two
novel components to handle the problems brought by limited data. Firstly, to
fully utilize the existing small-scale benchmarking datasets for more
discriminative feature learning, we introduce a cross-modal momentum
contrastive learning framework to enrich the training data for a given
mini-batch. Secondly, we propose to transfer knowledge learned from existing
coarse-grained large-scale datasets containing image-text pairs from
drastically different problem domains to compensate for the lack of TBPS
training data. A transfer learning method is designed so that useful
information can be transferred despite the large domain gap. Armed with these
components, our method achieves new state of the art on the CUHK-PEDES dataset
with significant improvements over the prior art in terms of Rank-1 and mAP.
Our code is available at https://github.com/BrandonHanx/TextReID.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Datasets 
      
        Self-Supervised 
      
        Scalability 
      
        Evaluation 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/gu2021multimodal/">Multimodal Representation for Neural Code Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multimodal Representation for Neural Code Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multimodal Representation for Neural Code Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gu Jian, Chen Zimin, Monperrus Martin</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE International Conference on Software Maintenance and Evolution (ICSME)</td>
    <td>32</td>
    <td><p>Semantic code search is about finding semantically relevant code snippets for
a given natural language query. In the state-of-the-art approaches, the
semantic similarity between code and query is quantified as the distance of
their representation in the shared vector space. In this paper, to improve the
vector space, we introduce tree-serialization methods on a simplified form of
AST and build the multimodal representation for the code data. We conduct
extensive experiments using a single corpus that is large-scale and
multi-language: CodeSearchNet. Our results show that both our tree-serialized
representations and multimodal learning model improve the performance of code
search. Last, we define intuitive quantification metrics oriented to the
completeness of semantic and syntactic information of the code data, to help
understand the experimental findings.</p>
</td>
    <td>
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/gkelios2021investigating/">Investigating the Vision Transformer Model for Image Retrieval Tasks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Investigating the Vision Transformer Model for Image Retrieval Tasks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Investigating the Vision Transformer Model for Image Retrieval Tasks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gkelios Socratis, Boutalis Yiannis, Chatzichristofis Savvas A.</td> <!-- 🔧 You were missing this -->
    <td>2021 17th International Conference on Distributed Computing in Sensor Systems (DCOSS)</td>
    <td>30</td>
    <td><p>This paper introduces a plug-and-play descriptor that can be effectively
adopted for image retrieval tasks without prior initialization or preparation.
The description method utilizes the recently proposed Vision Transformer
network while it does not require any training data to adjust parameters. In
image retrieval tasks, the use of Handcrafted global and local descriptors has
been very successfully replaced, over the last years, by the Convolutional
Neural Networks (CNN)-based methods. However, the experimental evaluation
conducted in this paper on several benchmarking datasets against 36
state-of-the-art descriptors from the literature demonstrates that a neural
network that contains no convolutional layer, such as Vision Transformer, can
shape a global descriptor and achieve competitive results. As fine-tuning is
not required, the presented methodology’s low complexity encourages adoption of
the architecture as an image retrieval baseline model, replacing the
traditional and well adopted CNN-based approaches and inaugurating a new era in
image retrieval approaches.</p>
</td>
    <td>
      
        Datasets 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/ge2021structured/">Structured Multi-modal Feature Embedding and Alignment for Image-Sentence Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Structured Multi-modal Feature Embedding and Alignment for Image-Sentence Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Structured Multi-modal Feature Embedding and Alignment for Image-Sentence Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ge Xuri, Chen Fuhai, Jose Joemon M., Ji Zhilong, Wu Zhongqin, Liu Xiao</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 29th ACM International Conference on Multimedia</td>
    <td>43</td>
    <td><p>The current state-of-the-art image-sentence retrieval methods implicitly
align the visual-textual fragments, like regions in images and words in
sentences, and adopt attention modules to highlight the relevance of
cross-modal semantic correspondences. However, the retrieval performance
remains unsatisfactory due to a lack of consistent representation in both
semantics and structural spaces. In this work, we propose to address the above
issue from two aspects: (i) constructing intrinsic structure (along with
relations) among the fragments of respective modalities, e.g., “dog \(\to\) play
\(\to\) ball” in semantic structure for an image, and (ii) seeking explicit
inter-modal structural and semantic correspondence between the visual and
textual modalities. In this paper, we propose a novel Structured Multi-modal
Feature Embedding and Alignment (SMFEA) model for image-sentence retrieval. In
order to jointly and explicitly learn the visual-textual embedding and the
cross-modal alignment, SMFEA creates a novel multi-modal structured module with
a shared context-aware referral tree. In particular, the relations of the
visual and textual fragments are modeled by constructing Visual Context-aware
Structured Tree encoder (VCS-Tree) and Textual Context-aware Structured Tree
encoder (TCS-Tree) with shared labels, from which visual and textual features
can be jointly learned and optimized. We utilize the multi-modal tree structure
to explicitly align the heterogeneous image-sentence data by maximizing the
semantic and structural similarity between corresponding inter-modal tree
nodes. Extensive experiments on Microsoft COCO and Flickr30K benchmarks
demonstrate the superiority of the proposed model in comparison to the
state-of-the-art methods.</p>
</td>
    <td>
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/guo2020deep/">Deep Kernel Supervised Hashing for Node Classification in Structural Networks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Kernel Supervised Hashing for Node Classification in Structural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Kernel Supervised Hashing for Node Classification in Structural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Guo Jia-nan, Mao Xian-ling, Lin Shu-yang, Wei Wei, Huang Heyan</td> <!-- 🔧 You were missing this -->
    <td>Information Sciences</td>
    <td>5</td>
    <td><p>Node classification in structural networks has been proven to be useful in
many real world applications. With the development of network embedding, the
performance of node classification has been greatly improved. However, nearly
all the existing network embedding based methods are hard to capture the actual
category features of a node because of the linearly inseparable problem in
low-dimensional space; meanwhile they cannot incorporate simultaneously network
structure information and node label information into network embedding. To
address the above problems, in this paper, we propose a novel Deep Kernel
Supervised Hashing (DKSH) method to learn the hashing representations of nodes
for node classification. Specifically, a deep multiple kernel learning is first
proposed to map nodes into suitable Hilbert space to deal with linearly
inseparable problem. Then, instead of only considering structural similarity
between two nodes, a novel similarity matrix is designed to merge both network
structure information and node label information. Supervised by the similarity
matrix, the learned hashing representations of nodes simultaneously preserve
the two kinds of information well from the learned Hilbert space. Extensive
experiments show that the proposed method significantly outperforms the
state-of-the-art baselines over three real world benchmark datasets.</p>
</td>
    <td>
      
        Supervised 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/ganea2021incremental/">Incremental Few-Shot Instance Segmentation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Incremental Few-Shot Instance Segmentation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Incremental Few-Shot Instance Segmentation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ganea Dan Andrei, Boom Bas, Poppe Ronald</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>55</td>
    <td><p>Few-shot instance segmentation methods are promising when labeled training
data for novel classes is scarce. However, current approaches do not facilitate
flexible addition of novel classes. They also require that examples of each
class are provided at train and test time, which is memory intensive. In this
paper, we address these limitations by presenting the first incremental
approach to few-shot instance segmentation: iMTFA. We learn discriminative
embeddings for object instances that are merged into class representatives.
Storing embedding vectors rather than images effectively solves the memory
overhead problem. We match these class embeddings at the RoI-level using cosine
similarity. This allows us to add new classes without the need for further
training or access to previous training data. In a series of experiments, we
consistently outperform the current state-of-the-art. Moreover, the reduced
memory requirements allow us to evaluate, for the first time, few-shot instance
segmentation performance on all classes in COCO jointly.</p>
</td>
    <td>
      
        Few-Shot-&-Zero-Shot 
      
        CVPR 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/fu2020deep/">Deep Momentum Uncertainty Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Momentum Uncertainty Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Momentum Uncertainty Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fu Chaoyou, Wang Guoli, Wu Xiang, Zhang Qian, He Ran</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition</td>
    <td>15</td>
    <td><p>Combinatorial optimization (CO) has been a hot research topic because of its
theoretic and practical importance. As a classic CO problem, deep hashing aims
to find an optimal code for each data from finite discrete possibilities, while
the discrete nature brings a big challenge to the optimization process.
Previous methods usually mitigate this challenge by binary approximation,
substituting binary codes for real-values via activation functions or
regularizations. However, such approximation leads to uncertainty between
real-values and binary ones, degrading retrieval performance. In this paper, we
propose a novel Deep Momentum Uncertainty Hashing (DMUH). It explicitly
estimates the uncertainty during training and leverages the uncertainty
information to guide the approximation process. Specifically, we model
bit-level uncertainty via measuring the discrepancy between the output of a
hashing network and that of a momentum-updated network. The discrepancy of each
bit indicates the uncertainty of the hashing network to the approximate output
of that bit. Meanwhile, the mean discrepancy of all bits in a hashing code can
be regarded as image-level uncertainty. It embodies the uncertainty of the
hashing network to the corresponding input image. The hashing bit and image
with higher uncertainty are paid more attention during optimization. To the
best of our knowledge, this is the first work to study the uncertainty in
hashing bits. Extensive experiments are conducted on four datasets to verify
the superiority of our method, including CIFAR-10, NUS-WIDE, MS-COCO, and a
million-scale dataset Clothing1M. Our method achieves the best performance on
all of the datasets and surpasses existing state-of-the-art methods by a large
margin.</p>
</td>
    <td>
      
        Evaluation 
      
        Datasets 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Neural-Hashing 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/fernandes2020locality/">Locality Sensitive Hashing with Extended Differential Privacy</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Locality Sensitive Hashing with Extended Differential Privacy' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Locality Sensitive Hashing with Extended Differential Privacy' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fernandes Natasha, Kawamoto Yusuke, Murakami Takao</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>7</td>
    <td><p>Extended differential privacy, a generalization of standard differential
privacy (DP) using a general metric, has been widely studied to provide
rigorous privacy guarantees while keeping high utility. However, existing works
on extended DP are limited to few metrics, such as the Euclidean metric.
Consequently, they have only a small number of applications, such as
location-based services and document processing. In this paper, we propose a
couple of mechanisms providing extended DP with a different metric: angular
distance (or cosine distance). Our mechanisms are based on locality sensitive
hashing (LSH), which can be applied to the angular distance and work well for
personal data in a high-dimensional space. We theoretically analyze the privacy
properties of our mechanisms, and prove extended DP for input data by taking
into account that LSH preserves the original metric only approximately. We
apply our mechanisms to friend matching based on high-dimensional personal data
with angular distance in the local model, and evaluate our mechanisms using two
real datasets. We show that LDP requires a very large privacy budget and that
RAPPOR does not work in this application. Then we show that our mechanisms
enable friend matching with high utility and rigorous privacy guarantees based
on extended DP.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        Locality-Sensitive-Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/fang2021combating/">Combating Ambiguity for Hash-code Learning in Medical Instance Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Combating Ambiguity for Hash-code Learning in Medical Instance Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Combating Ambiguity for Hash-code Learning in Medical Instance Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fang Jiansheng, Fu Huazhu, Zeng Dan, Yan Xiao, Yan Yuguang, Liu Jiang</td> <!-- 🔧 You were missing this -->
    <td>IEEE Journal of Biomedical and Health Informatics</td>
    <td>10</td>
    <td><p>When encountering a dubious diagnostic case, medical instance retrieval can
help radiologists make evidence-based diagnoses by finding images containing
instances similar to a query case from a large image database. The similarity
between the query case and retrieved similar cases is determined by visual
features extracted from pathologically abnormal regions. However, the
manifestation of these regions often lacks specificity, i.e., different
diseases can have the same manifestation, and different manifestations may
occur at different stages of the same disease. To combat the manifestation
ambiguity in medical instance retrieval, we propose a novel deep framework
called Y-Net, encoding images into compact hash-codes generated from
convolutional features by feature aggregation. Y-Net can learn highly
discriminative convolutional features by unifying the pixel-wise segmentation
loss and classification loss. The segmentation loss allows exploring subtle
spatial differences for good spatial-discriminability while the classification
loss utilizes class-aware semantic information for good semantic-separability.
As a result, Y-Net can enhance the visual features in pathologically abnormal
regions and suppress the disturbing of the background during model training,
which could effectively embed discriminative features into the hash-codes in
the retrieval stage. Extensive experiments on two medical image datasets
demonstrate that Y-Net can alleviate the ambiguity of pathologically abnormal
regions and its retrieval performance outperforms the state-of-the-art method
by an average of 9.27% on the returned list of 10.</p>
</td>
    <td>
      
        Evaluation 
      
        Datasets 
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/fang2021deep/">Deep Triplet Hashing Network for Case-based Medical Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Triplet Hashing Network for Case-based Medical Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Triplet Hashing Network for Case-based Medical Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fang Jiansheng, Fu Huazhu, Liu Jiang</td> <!-- 🔧 You were missing this -->
    <td>Medical Image Analysis</td>
    <td>52</td>
    <td><p>Deep hashing methods have been shown to be the most efficient approximate
nearest neighbor search techniques for large-scale image retrieval. However,
existing deep hashing methods have a poor small-sample ranking performance for
case-based medical image retrieval. The top-ranked images in the returned query
results may be as a different class than the query image. This ranking problem
is caused by classification, regions of interest (ROI), and small-sample
information loss in the hashing space. To address the ranking problem, we
propose an end-to-end framework, called Attention-based Triplet Hashing (ATH)
network, to learn low-dimensional hash codes that preserve the classification,
ROI, and small-sample information. We embed a spatial-attention module into the
network structure of our ATH to focus on ROI information. The spatial-attention
module aggregates the spatial information of feature maps by utilizing
max-pooling, element-wise maximum, and element-wise mean operations jointly
along the channel axis. The triplet cross-entropy loss can help to map the
classification information of images and similarity between images into the
hash codes. Extensive experiments on two case-based medical datasets
demonstrate that our proposed ATH can further improve the retrieval performance
compared to the state-of-the-art deep hashing methods and boost the ranking
performance for small samples. Compared to the other loss methods, the triplet
cross-entropy loss can enhance the classification performance and hash
code-discriminability</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/elnouby2021training/">Training Vision Transformers for Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Training Vision Transformers for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Training Vision Transformers for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>El-nouby Alaaeldin, Neverova Natalia, Laptev Ivan, Jégou Hervé</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>120</td>
    <td><p>Transformers have shown outstanding results for natural language
understanding and, more recently, for image classification. We here extend this
work and propose a transformer-based approach for image retrieval: we adopt
vision transformers for generating image descriptors and train the resulting
model with a metric learning objective, which combines a contrastive loss with
a differential entropy regularizer. Our results show consistent and significant
improvements of transformers over convolution-based approaches. In particular,
our method outperforms the state of the art on several public benchmarks for
category-level retrieval, namely Stanford Online Product, In-Shop and CUB-200.
Furthermore, our experiments on ROxford and RParis also show that, in
comparable settings, transformers are competitive for particular object
retrieval, especially in the regime of short vector representations and
low-resolution images.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/dubey2020decade/">A Decade Survey of Content Based Image Retrieval using Deep Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Decade Survey of Content Based Image Retrieval using Deep Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Decade Survey of Content Based Image Retrieval using Deep Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dubey Shiv Ram</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Circuits and Systems for Video Technology</td>
    <td>242</td>
    <td><p>The content based image retrieval aims to find the similar images from a
large scale dataset against a query image. Generally, the similarity between
the representative features of the query image and dataset images is used to
rank the images for retrieval. In early days, various hand designed feature
descriptors have been investigated based on the visual cues such as color,
texture, shape, etc. that represent the images. However, the deep learning has
emerged as a dominating alternative of hand-designed feature engineering from a
decade. It learns the features automatically from the data. This paper presents
a comprehensive survey of deep learning based developments in the past decade
for content based image retrieval. The categorization of existing
state-of-the-art methods from different perspectives is also performed for
greater understanding of the progress. The taxonomy used in this survey covers
different supervision, different networks, different descriptor type and
different retrieval type. A performance analysis is also performed using the
state-of-the-art methods. The insights are also presented for the benefit of
the researchers to observe the progress and to make the best choices. The
survey presented in this paper will help in further research progress in image
retrieval using deep learning.</p>
</td>
    <td>
      
        Datasets 
      
        Survey-Paper 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/dong2020using/">Using Text to Teach Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Using Text to Teach Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Using Text to Teach Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dong Haoyu, Wang Ze, Qiu Qiang, Sapiro Guillermo</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>5</td>
    <td><p>Image retrieval relies heavily on the quality of the data modeling and the
distance measurement in the feature space. Building on the concept of image
manifold, we first propose to represent the feature space of images, learned
via neural networks, as a graph. Neighborhoods in the feature space are now
defined by the geodesic distance between images, represented as graph vertices
or manifold samples. When limited images are available, this manifold is
sparsely sampled, making the geodesic computation and the corresponding
retrieval harder. To address this, we augment the manifold samples with
geometrically aligned text, thereby using a plethora of sentences to teach us
about images. In addition to extensive results on standard datasets
illustrating the power of text to help in image retrieval, a new public dataset
based on CLEVR is introduced to quantify the semantic similarity between visual
data and text data. The experimental results show that the joint embedding
manifold is a robust representation, allowing it to be a better basis to
perform image retrieval given only an image and a textual instruction on the
desired modifications over the image</p>
</td>
    <td>
      
        Datasets 
      
        CVPR 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/yu2021deep/">Deep Graph-neighbor Coherence Preserving Network for Unsupervised Cross-modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Graph-neighbor Coherence Preserving Network for Unsupervised Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Graph-neighbor Coherence Preserving Network for Unsupervised Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yu Jun, Zhou, Zhan, Tao</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>136</td>
    <td><p>Unsupervised cross-modal hashing (UCMH) has become a hot topic recently. Current UCMH focuses on exploring data similarities. However, current UCMH methods calculate the similarity between two data, mainly relying on the two data’s cross-modal features. These methods suffer from inaccurate similarity problems that result in a suboptimal retrieval Hamming space, because the cross-modal features between the data are not sufficient to describe the complex data relationships, such as situations where two data have different feature representations but share the inherent concepts. In this paper, we devise a deep graph-neighbor coherence preserving network (DGCPN). Specifically, DGCPN stems from graph models and explores graph-neighbor coherence by consolidating the information between data and their neighbors. DGCPN regulates comprehensive similarity preserving losses by exploiting three types of data similarities (i.e., the graph-neighbor coherence, the coexistent similarity, and the intra- and inter-modality consistency) and designs a half-real and half-binary optimization strategy to reduce the quantization errors during hashing. Essentially, DGCPN addresses the inaccurate similarity problem by exploring and exploiting the data’s intrinsic relationships in a graph. We conduct extensive experiments on three public UCMH datasets. The experimental results demonstrate the superiority of DGCPN, e.g., by improving the mean average precision from 0.722 to 0.751 on MIRFlickr-25K using 64-bit hashing codes to retrieval texts from images. We will release the source code package and the trained model on https://github.com/Atmegal/DGCPN.</p>
</td>
    <td>
      
        Datasets 
      
        Evaluation 
      
        Quantization 
      
        AAAI 
      
        Hashing-Methods 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/dash2020open/">Open Knowledge Graphs Canonicalization using Variational Autoencoders</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Open Knowledge Graphs Canonicalization using Variational Autoencoders' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Open Knowledge Graphs Canonicalization using Variational Autoencoders' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dash Sarthak, Rossiello Gaetano, Mihindukulasooriya Nandana, Bagchi Sugato, Gliozzo Alfio</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</td>
    <td>12</td>
    <td><p>Noun phrases and Relation phrases in open knowledge graphs are not
canonicalized, leading to an explosion of redundant and ambiguous
subject-relation-object triples. Existing approaches to solve this problem take
a two-step approach. First, they generate embedding representations for both
noun and relation phrases, then a clustering algorithm is used to group them
using the embeddings as features. In this work, we propose Canonicalizing Using
Variational Autoencoders (CUVA), a joint model to learn both embeddings and
cluster assignments in an end-to-end approach, which leads to a better vector
representation for the noun and relation phrases. Our evaluation over multiple
benchmarks shows that CUVA outperforms the existing state-of-the-art
approaches. Moreover, we introduce CanonicNell, a novel dataset to evaluate
entity canonicalization systems.</p>
</td>
    <td>
      
        Datasets 
      
        EMNLP 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/coleman2021graph/">Graph Reordering for Cache-Efficient Near Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Graph Reordering for Cache-Efficient Near Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Graph Reordering for Cache-Efficient Near Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Coleman Benjamin, Segarra Santiago, Shrivastava Anshumali, Smola Alex</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>Graph search is one of the most successful algorithmic trends in near
neighbor search. Several of the most popular and empirically successful
algorithms are, at their core, a simple walk along a pruned near neighbor
graph. Such algorithms consistently perform at the top of industrial speed
benchmarks for applications such as embedding search. However, graph traversal
applications often suffer from poor memory access patterns, and near neighbor
search is no exception to this rule. Our measurements show that popular search
indices such as the hierarchical navigable small-world graph (HNSW) can have
poor cache miss performance. To address this problem, we apply graph reordering
algorithms to near neighbor graphs. Graph reordering is a memory layout
optimization that groups commonly-accessed nodes together in memory. We present
exhaustive experiments applying several reordering algorithms to a leading
graph-based near neighbor method based on the HNSW index. We find that
reordering improves the query time by up to 40%, and we demonstrate that the
time needed to reorder the graph is negligible compared to the time required to
construct the index.</p>
</td>
    <td>
      
        Graph-Based-ANN 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/chun2021probabilistic/">Probabilistic Embeddings for Cross-Modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Probabilistic Embeddings for Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Probabilistic Embeddings for Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chun Sanghyuk, Oh Seong Joon, de Rezende Rafael Sampaio, Kalantidis Yannis, Larlus Diane</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>169</td>
    <td><p>Cross-modal retrieval methods build a common representation space for samples
from multiple modalities, typically from the vision and the language domains.
For images and their captions, the multiplicity of the correspondences makes
the task particularly challenging. Given an image (respectively a caption),
there are multiple captions (respectively images) that equally make sense. In
this paper, we argue that deterministic functions are not sufficiently powerful
to capture such one-to-many correspondences. Instead, we propose to use
Probabilistic Cross-Modal Embedding (PCME), where samples from the different
modalities are represented as probabilistic distributions in the common
embedding space. Since common benchmarks such as COCO suffer from
non-exhaustive annotations for cross-modal matches, we propose to additionally
evaluate retrieval on the CUB dataset, a smaller yet clean database where all
possible image-caption pairs are annotated. We extensively ablate PCME and
demonstrate that it not only improves the retrieval performance over its
deterministic counterpart but also provides uncertainty estimates that render
the embeddings more interpretable. Code is available at
https://github.com/naver-ai/pcme</p>
</td>
    <td>
      
        Datasets 
      
        CVPR 
      
        Evaluation 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/yu2021improving/">Improving Query Representations for Dense Retrieval with Pseudo Relevance Feedback</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Improving Query Representations for Dense Retrieval with Pseudo Relevance Feedback' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Improving Query Representations for Dense Retrieval with Pseudo Relevance Feedback' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yu Hongchien, Xiong Chenyan, Callan Jamie</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</td>
    <td>44</td>
    <td><p>Dense retrieval systems conduct first-stage retrieval using embedded
representations and simple similarity metrics to match a query to documents.
Its effectiveness depends on encoded embeddings to capture the semantics of
queries and documents, a challenging task due to the shortness and ambiguity of
search queries. This paper proposes ANCE-PRF, a new query encoder that uses
pseudo relevance feedback (PRF) to improve query representations for dense
retrieval. ANCE-PRF uses a BERT encoder that consumes the query and the top
retrieved documents from a dense retrieval model, ANCE, and it learns to
produce better query embeddings directly from relevance labels. It also keeps
the document index unchanged to reduce overhead. ANCE-PRF significantly
outperforms ANCE and other recent dense retrieval systems on several datasets.
Analysis shows that the PRF encoder effectively captures the relevant and
complementary information from PRF documents, while ignoring the noise with its
learned attention mechanism.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Datasets 
      
        CIKM 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/zhan2021jointly/">Jointly Optimizing Query Encoder and Product Quantization to Improve Retrieval Performance</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Jointly Optimizing Query Encoder and Product Quantization to Improve Retrieval Performance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Jointly Optimizing Query Encoder and Product Quantization to Improve Retrieval Performance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhan Jingtao, Mao Jiaxin, Liu Yiqun, Guo Jiafeng, Zhang Min, Ma Shaoping</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</td>
    <td>58</td>
    <td><p>Recently, Information Retrieval community has witnessed fast-paced advances
in Dense Retrieval (DR), which performs first-stage retrieval with
embedding-based search. Despite the impressive ranking performance, previous
studies usually adopt brute-force search to acquire candidates, which is
prohibitive in practical Web search scenarios due to its tremendous memory
usage and time cost. To overcome these problems, vector compression methods
have been adopted in many practical embedding-based retrieval applications. One
of the most popular methods is Product Quantization (PQ). However, although
existing vector compression methods including PQ can help improve the
efficiency of DR, they incur severely decayed retrieval performance due to the
separation between encoding and compression. To tackle this problem, we present
JPQ, which stands for Joint optimization of query encoding and Product
Quantization. It trains the query encoder and PQ index jointly in an end-to-end
manner based on three optimization strategies, namely ranking-oriented loss, PQ
centroid optimization, and end-to-end negative sampling. We evaluate JPQ on two
publicly available retrieval benchmarks. Experimental results show that JPQ
significantly outperforms popular vector compression methods. Compared with
previous DR models that use brute-force search, JPQ almost matches the best
retrieval performance with 30x compression on index size. The compressed index
further brings 10x speedup on CPU and 2x speedup on GPU in query latency.</p>
</td>
    <td>
      
        Evaluation 
      
        Efficiency 
      
        Quantization 
      
        CIKM 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/chen2025long/">Long-Tail Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Long-Tail Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Long-Tail Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen Yong, Hou, Leng, Hu, Lin, Zhang</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>9</td>
    <td><p>Hashing, which represents data items as compact binary codes, has
been becoming a more and more popular technique, e.g., for large-scale image retrieval, owing to its super fast search speed as well
as its extremely economical memory consumption. However, existing hashing methods all try to learn binary codes from artificially
balanced datasets which are not commonly available in real-world
scenarios. In this paper, we propose Long-Tail Hashing Network
(LTHNet), a novel two-stage deep hashing approach that addresses
the problem of learning to hash for more realistic datasets where
the data labels roughly exhibit a long-tail distribution. Specifically,
the first stage is to learn relaxed embeddings of the given dataset
with its long-tail characteristic taken into account via an end-to-end deep neural network; the second stage is to binarize those
obtained embeddings. A critical part of LTHNet is its extended dynamic meta-embedding module which can adaptively realize visual
knowledge transfer between head and tail classes, and thus enrich
image representations for hashing. Our experiments have shown
that LTHNet achieves dramatic performance improvements over all
state-of-the-art competitors on long-tail datasets, with no or little
sacrifice on balanced datasets. Further analyses reveal that while to
our surprise directly manipulating class weights in the loss function
has little effect, the extended dynamic meta-embedding module, the
usage of cross-entropy loss instead of square loss, and the relatively
small batch-size for training all contribute to LTHNet’s success.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Datasets 
      
        Neural-Hashing 
      
        SIGIR 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/chen2021multimodal/">Multimodal Clustering Networks for Self-supervised Learning from Unlabeled Videos</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multimodal Clustering Networks for Self-supervised Learning from Unlabeled Videos' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multimodal Clustering Networks for Self-supervised Learning from Unlabeled Videos' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen Brian, Rouditchenko Andrew, Duarte Kevin, Kuehne Hilde, Thomas Samuel, Boggust Angie, Panda Rameswar, Kingsbury Brian, Feris Rogerio, Harwath David, Glass James, Picheny Michael, Chang Shih-fu</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>52</td>
    <td><p>Multimodal self-supervised learning is getting more and more attention as it
allows not only to train large networks without human supervision but also to
search and retrieve data across various modalities. In this context, this paper
proposes a self-supervised training framework that learns a common multimodal
embedding space that, in addition to sharing representations across different
modalities, enforces a grouping of semantically similar instances. To this end,
we extend the concept of instance-level contrastive learning with a multimodal
clustering step in the training pipeline to capture semantic similarities
across modalities. The resulting embedding space enables retrieval of samples
across all modalities, even from unseen datasets and different domains. To
evaluate our approach, we train our model on the HowTo100M dataset and evaluate
its zero-shot retrieval capabilities in two challenging domains, namely
text-to-video retrieval, and temporal action localization, showing
state-of-the-art results on four different datasets.</p>
</td>
    <td>
      
        ICCV 
      
        Few-Shot-&-Zero-Shot 
      
        Supervised 
      
        Tools-&-Libraries 
      
        Datasets 
      
        Video-Retrieval 
      
        Self-Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/chen2021long/">Long-Tail Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Long-Tail Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Long-Tail Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen Yong, Hou, Leng, Hu, Lin, Zhang</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>9</td>
    <td><p>Hashing, which represents data items as compact binary codes, has
been becoming a more and more popular technique, e.g., for large-scale image retrieval, owing to its super fast search speed as well
as its extremely economical memory consumption. However, existing hashing methods all try to learn binary codes from artificially
balanced datasets which are not commonly available in real-world
scenarios. In this paper, we propose Long-Tail Hashing Network
(LTHNet), a novel two-stage deep hashing approach that addresses
the problem of learning to hash for more realistic datasets where
the data labels roughly exhibit a long-tail distribution. Specifically,
the first stage is to learn relaxed embeddings of the given dataset
with its long-tail characteristic taken into account via an end-to-end deep neural network; the second stage is to binarize those
obtained embeddings. A critical part of LTHNet is its extended dynamic meta-embedding module which can adaptively realize visual
knowledge transfer between head and tail classes, and thus enrich
image representations for hashing. Our experiments have shown
that LTHNet achieves dramatic performance improvements over all
state-of-the-art competitors on long-tail datasets, with no or little
sacrifice on balanced datasets. Further analyses reveal that while to
our surprise directly manipulating class weights in the loss function
has little effect, the extended dynamic meta-embedding module, the
usage of cross-entropy loss instead of square loss, and the relatively
small batch-size for training all contribute to LTHNet’s success.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Datasets 
      
        Neural-Hashing 
      
        SIGIR 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/vanblokland2021partial/">Partial 3D Object Retrieval using Local Binary QUICCI Descriptors and Dissimilarity Tree Indexing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Partial 3D Object Retrieval using Local Binary QUICCI Descriptors and Dissimilarity Tree Indexing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Partial 3D Object Retrieval using Local Binary QUICCI Descriptors and Dissimilarity Tree Indexing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>van Blokland Bart Iver, Theoharis Theoharis</td> <!-- 🔧 You were missing this -->
    <td>Computers &amp; Graphics</td>
    <td>6</td>
    <td><p>A complete pipeline is presented for accurate and efficient partial 3D object
retrieval based on Quick Intersection Count Change Image (QUICCI) binary local
descriptors and a novel indexing tree. It is shown how a modification to the
QUICCI query descriptor makes it ideal for partial retrieval. An indexing
structure called Dissimilarity Tree is proposed which can significantly
accelerate searching the large space of local descriptors; this is applicable
to QUICCI and other binary descriptors. The index exploits the distribution of
bits within descriptors for efficient retrieval. The retrieval pipeline is
tested on the artificial part of SHREC’16 dataset with near-ideal retrieval
results.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/chen2019efficient/">Efficient Object Embedding for Spliced Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Object Embedding for Spliced Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient Object Embedding for Spliced Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen Bor-chun, Wu Zuxuan, Davis Larry S., Lim Ser-nam</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>12</td>
    <td><p>Detecting spliced images is one of the emerging challenges in computer
vision. Unlike prior methods that focus on detecting low-level artifacts
generated during the manipulation process, we use an image retrieval approach
to tackle this problem. When given a spliced query image, our goal is to
retrieve the original image from a database of authentic images. To achieve
this goal, we propose representing an image by its constituent objects based on
the intuition that the finest granularity of manipulations is oftentimes at the
object-level. We introduce a framework, object embeddings for spliced image
retrieval (OE-SIR), that utilizes modern object detectors to localize object
regions. Each region is then embedded and collectively used to represent the
image. Further, we propose a student-teacher training paradigm for learning
discriminative embeddings within object regions to avoid expensive multiple
forward passes. Detailed analysis of the efficacy of different feature
embedding models is also provided in this study. Extensive experimental results
show that the OE-SIR achieves state-of-the-art performance in spliced image
retrieval.</p>
</td>
    <td>
      
        CVPR 
      
        Evaluation 
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/yamada2021efficient/">Efficient Passage Retrieval with Hashing for Open-domain Question Answering</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Passage Retrieval with Hashing for Open-domain Question Answering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient Passage Retrieval with Hashing for Open-domain Question Answering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yamada Ikuya, Asai Akari, Hajishirzi Hannaneh</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</td>
    <td>46</td>
    <td><p>Most state-of-the-art open-domain question answering systems use a neural
retrieval model to encode passages into continuous vectors and extract them
from a knowledge source. However, such retrieval models often require large
memory to run because of the massive size of their passage index. In this
paper, we introduce Binary Passage Retriever (BPR), a memory-efficient neural
retrieval model that integrates a learning-to-hash technique into the
state-of-the-art Dense Passage Retriever (DPR) to represent the passage index
using compact binary codes rather than continuous vectors. BPR is trained with
a multi-task objective over two tasks: efficient candidate generation based on
binary codes and accurate reranking based on continuous vectors. Compared with
DPR, BPR substantially reduces the memory cost from 65GB to 2GB without a loss
of accuracy on two standard open-domain question answering benchmarks: Natural
Questions and TriviaQA. Our code and trained models are available at
https://github.com/studio-ousia/bpr.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Compact-Codes 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/zhang2021joint/">Joint Learning of Deep Retrieval Model and Product Quantization based Embedding Index</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Joint Learning of Deep Retrieval Model and Product Quantization based Embedding Index' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Joint Learning of Deep Retrieval Model and Product Quantization based Embedding Index' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Han, Shen Hongwei, Qiu Yiming, Jiang Yunjiang, Wang Songlin, Xu Sulong, Xiao Yun, Long Bo, Yang Wen-yun</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>6</td>
    <td><p>Embedding index that enables fast approximate nearest neighbor(ANN) search,
serves as an indispensable component for state-of-the-art deep retrieval
systems. Traditional approaches, often separating the two steps of embedding
learning and index building, incur additional indexing time and decayed
retrieval accuracy. In this paper, we propose a novel method called Poeem,
which stands for product quantization based embedding index jointly trained
with deep retrieval model, to unify the two separate steps within an end-to-end
training, by utilizing a few techniques including the gradient straight-through
estimator, warm start strategy, optimal space decomposition and Givens
rotation. Extensive experimental results show that the proposed method not only
improves retrieval accuracy significantly but also reduces the indexing time to
almost none. We have open sourced our approach for the sake of comparison and
reproducibility.</p>
</td>
    <td>
      
        Quantization 
      
        SIGIR 
      
        Vector-Indexing 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/bencohen2021semantic/">Semantic Diversity Learning for Zero-Shot Multi-label Classification</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semantic Diversity Learning for Zero-Shot Multi-label Classification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semantic Diversity Learning for Zero-Shot Multi-label Classification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ben-cohen Avi, Zamir Nadav, Baruch Emanuel Ben, Friedman Itamar, Zelnik-manor Lihi</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>29</td>
    <td><p>Training a neural network model for recognizing multiple labels associated
with an image, including identifying unseen labels, is challenging, especially
for images that portray numerous semantically diverse labels. As challenging as
this task is, it is an essential task to tackle since it represents many
real-world cases, such as image retrieval of natural images. We argue that
using a single embedding vector to represent an image, as commonly practiced,
is not sufficient to rank both relevant seen and unseen labels accurately. This
study introduces an end-to-end model training for multi-label zero-shot
learning that supports semantic diversity of the images and labels. We propose
to use an embedding matrix having principal embedding vectors trained using a
tailored loss function. In addition, during training, we suggest up-weighting
in the loss function image samples presenting higher semantic diversity to
encourage the diversity of the embedding matrix. Extensive experiments show
that our proposed method improves the zero-shot model’s quality in tag-based
image retrieval achieving SoTA results on several common datasets (NUS-Wide,
COCO, Open Images).</p>
</td>
    <td>
      
        ICCV 
      
        Datasets 
      
        Few-Shot-&-Zero-Shot 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/avgoustinakis2020audio/">Audio-based Near-Duplicate Video Retrieval with Audio Similarity Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Audio-based Near-Duplicate Video Retrieval with Audio Similarity Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Audio-based Near-Duplicate Video Retrieval with Audio Similarity Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Avgoustinakis Pavlos, Kordopatis-zilos Giorgos, Papadopoulos Symeon, Symeonidis Andreas L., Kompatsiaris Ioannis</td> <!-- 🔧 You were missing this -->
    <td>2020 25th International Conference on Pattern Recognition (ICPR)</td>
    <td>8</td>
    <td><p>In this work, we address the problem of audio-based near-duplicate video
retrieval. We propose the Audio Similarity Learning (AuSiL) approach that
effectively captures temporal patterns of audio similarity between video pairs.
For the robust similarity calculation between two videos, we first extract
representative audio-based video descriptors by leveraging transfer learning
based on a Convolutional Neural Network (CNN) trained on a large scale dataset
of audio events, and then we calculate the similarity matrix derived from the
pairwise similarity of these descriptors. The similarity matrix is subsequently
fed to a CNN network that captures the temporal structures existing within its
content. We train our network following a triplet generation process and
optimizing the triplet loss function. To evaluate the effectiveness of the
proposed approach, we have manually annotated two publicly available video
datasets based on the audio duplicity between their videos. The proposed
approach achieves very competitive results compared to three state-of-the-art
methods. Also, unlike the competing methods, it is very robust to the retrieval
of audio duplicates generated with speed transformations.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Video-Retrieval 
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/avgoustinakis2021audio/">Audio-based Near-Duplicate Video Retrieval with Audio Similarity Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Audio-based Near-Duplicate Video Retrieval with Audio Similarity Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Audio-based Near-Duplicate Video Retrieval with Audio Similarity Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Avgoustinakis Pavlos, Kordopatis-zilos Giorgos, Papadopoulos Symeon, Symeonidis Andreas L., Kompatsiaris Ioannis</td> <!-- 🔧 You were missing this -->
    <td>2020 25th International Conference on Pattern Recognition (ICPR)</td>
    <td>8</td>
    <td><p>In this work, we address the problem of audio-based near-duplicate video
retrieval. We propose the Audio Similarity Learning (AuSiL) approach that
effectively captures temporal patterns of audio similarity between video pairs.
For the robust similarity calculation between two videos, we first extract
representative audio-based video descriptors by leveraging transfer learning
based on a Convolutional Neural Network (CNN) trained on a large scale dataset
of audio events, and then we calculate the similarity matrix derived from the
pairwise similarity of these descriptors. The similarity matrix is subsequently
fed to a CNN network that captures the temporal structures existing within its
content. We train our network following a triplet generation process and
optimizing the triplet loss function. To evaluate the effectiveness of the
proposed approach, we have manually annotated two publicly available video
datasets based on the audio duplicity between their videos. The proposed
approach achieves very competitive results compared to three state-of-the-art
methods. Also, unlike the competing methods, it is very robust to the retrieval
of audio duplicates generated with speed transformations.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Video-Retrieval 
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/bianchi2021contrastive/">Contrastive Language-Image Pre-training for the Italian Language</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Contrastive Language-Image Pre-training for the Italian Language' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Contrastive Language-Image Pre-training for the Italian Language' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Bianchi Federico, Attanasio Giuseppe, Pisoni Raphael, Terragni Silvia, Sarti Gabriele, Lakshmi Sri</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>19</td>
    <td><p>CLIP (Contrastive Language-Image Pre-training) is a very recent multi-modal
model that jointly learns representations of images and texts. The model is
trained on a massive amount of English data and shows impressive performance on
zero-shot classification tasks. Training the same model on a different language
is not trivial, since data in other languages might be not enough and the model
needs high-quality translations of the texts to guarantee a good performance.
In this paper, we present the first CLIP model for the Italian Language
(CLIP-Italian), trained on more than 1.4 million image-text pairs. Results show
that CLIP-Italian outperforms the multilingual CLIP model on the tasks of image
retrieval and zero-shot classification.</p>
</td>
    <td>
      
        Few-Shot-&-Zero-Shot 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/zheng2021deep/">Deep Relational Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Relational Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Relational Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zheng Wenzhao, Zhang Borui, Lu Jiwen, Zhou Jie</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>36</td>
    <td><p>This paper presents a deep relational metric learning (DRML) framework for
image clustering and retrieval. Most existing deep metric learning methods
learn an embedding space with a general objective of increasing interclass
distances and decreasing intraclass distances. However, the conventional losses
of metric learning usually suppress intraclass variations which might be
helpful to identify samples of unseen classes. To address this problem, we
propose to adaptively learn an ensemble of features that characterizes an image
from different aspects to model both interclass and intraclass distributions.
We further employ a relational module to capture the correlations among each
feature in the ensemble and construct a graph to represent an image. We then
perform relational inference on the graph to integrate the ensemble and obtain
a relation-aware embedding to measure the similarities. Extensive experiments
on the widely-used CUB-200-2011, Cars196, and Stanford Online Products datasets
demonstrate that our framework improves existing deep metric learning methods
and achieves very competitive results.</p>
</td>
    <td>
      
        ICCV 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/amrouche2021hashing/">Hashing and metric learning for charged particle tracking</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing and metric learning for charged particle tracking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing and metric learning for charged particle tracking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Amrouche Sabrina, Kiehn Moritz, Golling Tobias, Salzburger Andreas</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>We propose a novel approach to charged particle tracking at high intensity
particle colliders based on Approximate Nearest Neighbors search. With hundreds
of thousands of measurements per collision to be reconstructed e.g. at the High
Luminosity Large Hadron Collider, the currently employed combinatorial track
finding approaches become inadequate. Here, we use hashing techniques to
separate measurements into buckets of 20-50 hits and increase their purity
using metric learning. Two different approaches are studied to further resolve
tracks inside buckets: Local Fisher Discriminant Analysis and Neural Networks
for triplet similarity learning. We demonstrate the proposed approach on
simulated collisions and show significant speed improvement with bucket
tracking efficiency of 96% and a fake rate of 8% on unseen particle events.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Distance-Metric-Learning 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/agrawal2020tag/">Tag Embedding Based Personalized Point Of Interest Recommendation System</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Tag Embedding Based Personalized Point Of Interest Recommendation System' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Tag Embedding Based Personalized Point Of Interest Recommendation System' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Agrawal Suraj, Roy Dwaipayan, Mitra Mandar</td> <!-- 🔧 You were missing this -->
    <td>Information Processing &amp; Management</td>
    <td>21</td>
    <td><p>Personalized Point of Interest recommendation is very helpful for satisfying
users’ needs at new places. In this article, we propose a tag embedding based
method for Personalized Recommendation of Point Of Interest. We model the
relationship between tags corresponding to Point Of Interest. The model
provides representative embedding corresponds to a tag in a way that related
tags will be closer. We model Point of Interest-based on tag embedding and also
model the users (user profile) based on the Point Of Interest rated by them.
finally, we rank the user’s candidate Point Of Interest based on cosine
similarity between user’s embedding and Point of Interest’s embedding. Further,
we find the parameters required to model user by discrete optimizing over
different measures (like ndcg@5, MRR, …). We also analyze the result while
considering the same parameters for all users and individual parameters for
each user. Along with it we also analyze the effect on the result while
changing the dataset to model the relationship between tags. Our method also
minimizes the privacy leak issue. We used TREC Contextual Suggestion 2016 Phase
2 dataset and have significant improvement over all the measures on the state
of the art method. It improves ndcg@5 by 12.8%, p@5 by 4.3%, and MRR by 7.8%,
which shows the effectiveness of the method.</p>
</td>
    <td>
      
        Datasets 
      
        Recommender-Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/bianchi2021query2prod2vec/">Query2Prod2Vec Grounded Word Embeddings for eCommerce</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Query2Prod2Vec Grounded Word Embeddings for eCommerce' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Query2Prod2Vec Grounded Word Embeddings for eCommerce' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Bianchi Federico, Tagliabue Jacopo, Yu Bingqing</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers</td>
    <td>11</td>
    <td><p>We present Query2Prod2Vec, a model that grounds lexical representations for
product search in product embeddings: in our model, meaning is a mapping
between words and a latent space of products in a digital shop. We leverage
shopping sessions to learn the underlying space and use merchandising
annotations to build lexical analogies for evaluation: our experiments show
that our model is more accurate than known techniques from the NLP and IR
literature. Finally, we stress the importance of data efficiency for product
search outside of retail giants, and highlight how Query2Prod2Vec fits with
practical constraints faced by most practitioners.</p>
</td>
    <td>
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/yan2021binary/">Binary Code based Hash Embedding for Web-scale Applications</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Binary Code based Hash Embedding for Web-scale Applications' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Binary Code based Hash Embedding for Web-scale Applications' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yan Bencheng, Wang Pengjie, Liu Jinquan, Lin Wei, Lee Kuang-chih, Xu Jian, Zheng Bo</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</td>
    <td>10</td>
    <td><p>Nowadays, deep learning models are widely adopted in web-scale applications
such as recommender systems, and online advertising. In these applications,
embedding learning of categorical features is crucial to the success of deep
learning models. In these models, a standard method is that each categorical
feature value is assigned a unique embedding vector which can be learned and
optimized. Although this method can well capture the characteristics of the
categorical features and promise good performance, it can incur a huge memory
cost to store the embedding table, especially for those web-scale applications.
Such a huge memory cost significantly holds back the effectiveness and
usability of EDRMs. In this paper, we propose a binary code based hash
embedding method which allows the size of the embedding table to be reduced in
arbitrary scale without compromising too much performance. Experimental
evaluation results show that one can still achieve 99% performance even if the
embedding table size is reduced 1000\(\times\) smaller than the original one with
our proposed method.</p>
</td>
    <td>
      
        Scalability 
      
        Evaluation 
      
        CIKM 
      
        Large-Scale-Search 
      
        Recommender-Systems 
      
        Compact-Codes 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/zhan2020weakly/">Weakly-Supervised Online Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Weakly-Supervised Online Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Weakly-Supervised Online Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhan Yu-wei, Luo Xin, Sun Yu, Wang Yongxin, Chen Zhen-duo, Xu Xin-shun</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE International Conference on Multimedia and Expo (ICME)</td>
    <td>9</td>
    <td><p>With the rapid development of social websites, recent years have witnessed an
explosive growth of social images with user-provided tags which continuously
arrive in a streaming fashion. Due to the fast query speed and low storage
cost, hashing-based methods for image search have attracted increasing
attention. However, existing hashing methods for social image retrieval are
based on batch mode which violates the nature of social images, i.e., social
images are usually generated periodically or collected in a stream fashion.
Although there exist many online image hashing methods, they either adopt
unsupervised learning which ignore the relevant tags, or are designed in the
supervised manner which needs high-quality labels. In this paper, to overcome
the above limitations, we propose a new method named Weakly-supervised Online
Hashing (WOH). In order to learn high-quality hash codes, WOH exploits the weak
supervision by considering the semantics of tags and removing the noise.
Besides, We develop a discrete online optimization algorithm for WOH, which is
efficient and scalable. Extensive experiments conducted on two real-world
datasets demonstrate the superiority of WOH compared with several
state-of-the-art hashing baselines.</p>
</td>
    <td>
      
        Supervised 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/yu2025deep/">Deep Graph-neighbor Coherence Preserving Network for Unsupervised Cross-modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Graph-neighbor Coherence Preserving Network for Unsupervised Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Graph-neighbor Coherence Preserving Network for Unsupervised Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yu Jun, Zhou, Zhan, Tao</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>136</td>
    <td><p>Unsupervised cross-modal hashing (UCMH) has become a hot topic recently. Current UCMH focuses on exploring data similarities. However, current UCMH methods calculate the similarity between two data, mainly relying on the two data’s cross-modal features. These methods suffer from inaccurate similarity problems that result in a suboptimal retrieval Hamming space, because the cross-modal features between the data are not sufficient to describe the complex data relationships, such as situations where two data have different feature representations but share the inherent concepts. In this paper, we devise a deep graph-neighbor coherence preserving network (DGCPN). Specifically, DGCPN stems from graph models and explores graph-neighbor coherence by consolidating the information between data and their neighbors. DGCPN regulates comprehensive similarity preserving losses by exploiting three types of data similarities (i.e., the graph-neighbor coherence, the coexistent similarity, and the intra- and inter-modality consistency) and designs a half-real and half-binary optimization strategy to reduce the quantization errors during hashing. Essentially, DGCPN addresses the inaccurate similarity problem by exploring and exploiting the data’s intrinsic relationships in a graph. We conduct extensive experiments on three public UCMH datasets. The experimental results demonstrate the superiority of DGCPN, e.g., by improving the mean average precision from 0.722 to 0.751 on MIRFlickr-25K using 64-bit hashing codes to retrieval texts from images. We will release the source code package and the trained model on https://github.com/Atmegal/DGCPN.</p>
</td>
    <td>
      
        Datasets 
      
        Evaluation 
      
        Quantization 
      
        AAAI 
      
        Hashing-Methods 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/yan2021hierarchical/">Hierarchical Attention Fusion for Geo-Localization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hierarchical Attention Fusion for Geo-Localization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hierarchical Attention Fusion for Geo-Localization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yan Liqi, Cui Yiming, Chen Yingjie, Liu Dongfang</td> <!-- 🔧 You were missing this -->
    <td>ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</td>
    <td>32</td>
    <td><p>Geo-localization is a critical task in computer vision. In this work, we cast
the geo-localization as a 2D image retrieval task. Current state-of-the-art
methods for 2D geo-localization are not robust to locate a scene with drastic
scale variations because they only exploit features from one semantic level for
image representations. To address this limitation, we introduce a hierarchical
attention fusion network using multi-scale features for geo-localization. We
extract the hierarchical feature maps from a convolutional neural network (CNN)
and organically fuse the extracted features for image representations. Our
training is self-supervised using adaptive weights to control the attention of
feature emphasis from each hierarchical level. Evaluation results on the image
retrieval and the large-scale geo-localization benchmarks indicate that our
method outperforms the existing state-of-the-art methods. Code is available
here: https://github.com/YanLiqi/HAF.</p>
</td>
    <td>
      
        Supervised 
      
        ICASSP 
      
        Image-Retrieval 
      
        Self-Supervised 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/wang2025prototype/">Prototype-Supervised Adversarial Network for Targeted Attack of Deep Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Prototype-Supervised Adversarial Network for Targeted Attack of Deep Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Prototype-Supervised Adversarial Network for Targeted Attack of Deep Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Xunguang, Zhang, Wu, Shen, Lu</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>43</td>
    <td><p>Due to its powerful capability of representation learning and high-efficiency computation, deep hashing has made significant progress in large-scale image retrieval. However, deep hashing networks are vulnerable to adversarial examples, which is a practical secure problem but seldom studied in hashing-based retrieval field. In this paper, we propose a novel prototype-supervised adversarial network (ProS-GAN), which formulates a flexible generative architecture for efficient and effective targeted hashing attack. To the best of our knowledge, this is the first generation-based method to attack deep hashing networks. Generally, our proposed framework consists of three parts, i.e., a PrototypeNet, a generator and a discriminator. Specifically, the designed PrototypeNet embeds the target label into the semantic representation and learns the prototype code as the category-level representative of the target label. Moreover, the semantic representation and the original image are jointly fed into the generator for flexible targeted attack. Particularly, the prototype code is adopted to supervise the generator to construct the targeted adversarial example by minimizing the Hamming distance between the hash code of the adversarial example and the prototype code. Furthermore, the generator is against the discriminator to simultaneously encourage the adversarial examples visually realistic and the semantic representation informative. Extensive experiments verify that the proposed framework can efficiently produce adversarial examples with better targeted attack performance and transferability over state-of-the-art targeted attack methods of deep hashing.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Efficiency 
      
        CVPR 
      
        Neural-Hashing 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/won2020multimodal/">Multimodal Metric Learning for Tag-based Music Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multimodal Metric Learning for Tag-based Music Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multimodal Metric Learning for Tag-based Music Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Won Minz, Oramas Sergio, Nieto Oriol, Gouyon Fabien, Serra Xavier</td> <!-- 🔧 You were missing this -->
    <td>ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</td>
    <td>28</td>
    <td><p>Tag-based music retrieval is crucial to browse large-scale music libraries
efficiently. Hence, automatic music tagging has been actively explored, mostly
as a classification task, which has an inherent limitation: a fixed vocabulary.
On the other hand, metric learning enables flexible vocabularies by using
pretrained word embeddings as side information. Also, metric learning has
already proven its suitability for cross-modal retrieval tasks in other domains
(e.g., text-to-image) by jointly learning a multimodal embedding space. In this
paper, we investigate three ideas to successfully introduce multimodal metric
learning for tag-based music retrieval: elaborate triplet sampling, acoustic
and cultural music information, and domain-specific word embeddings. Our
experimental results show that the proposed ideas enhance the retrieval system
quantitatively, and qualitatively. Furthermore, we release the MSD500, a subset
of the Million Song Dataset (MSD) containing 500 cleaned tags, 7 manually
annotated tag categories, and user taste profiles.</p>
</td>
    <td>
      
        ICASSP 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Scalability 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/wu2021hashing/">Hashing-Accelerated Graph Neural Networks for Link Prediction</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing-Accelerated Graph Neural Networks for Link Prediction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing-Accelerated Graph Neural Networks for Link Prediction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu Wei, Li Bin, Luo Chuan, Nejdl Wolfgang</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Web Conference 2021</td>
    <td>30</td>
    <td><p>Networks are ubiquitous in the real world. Link prediction, as one of the key
problems for network-structured data, aims to predict whether there exists a
link between two nodes. The traditional approaches are based on the explicit
similarity computation between the compact node representation by embedding
each node into a low-dimensional space. In order to efficiently handle the
intensive similarity computation in link prediction, the hashing technique has
been successfully used to produce the node representation in the Hamming space.
However, the hashing-based link prediction algorithms face accuracy loss from
the randomized hashing techniques or inefficiency from the learning to hash
techniques in the embedding process. Currently, the Graph Neural Network (GNN)
framework has been widely applied to the graph-related tasks in an end-to-end
manner, but it commonly requires substantial computational resources and memory
costs due to massive parameter learning, which makes the GNN-based algorithms
impractical without the help of a powerful workhorse. In this paper, we propose
a simple and effective model called #GNN, which balances the trade-off between
accuracy and efficiency. #GNN is able to efficiently acquire node
representation in the Hamming space for link prediction by exploiting the
randomized hashing technique to implement message passing and capture
high-order proximity in the GNN framework. Furthermore, we characterize the
discriminative power of #GNN in probability. The extensive experimental results
demonstrate that the proposed #GNN algorithm achieves accuracy comparable to
the learning-based algorithms and outperforms the randomized algorithm, while
running significantly faster than the learning-based algorithms. Also, the
proposed algorithm shows excellent scalability on a large-scale network with
the limited resources.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Scalability 
      
        Tools-&-Libraries 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/xiao2021neural/">Neural PathSim for Inductive Similarity Search in Heterogeneous Information Networks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Neural PathSim for Inductive Similarity Search in Heterogeneous Information Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Neural PathSim for Inductive Similarity Search in Heterogeneous Information Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xiao Wenyi, Zhao Huan, Zheng Vincent W., Song Yangqiu</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</td>
    <td>7</td>
    <td><p>PathSim is a widely used meta-path-based similarity in heterogeneous
information networks. Numerous applications rely on the computation of PathSim,
including similarity search and clustering. Computing PathSim scores on large
graphs is computationally challenging due to its high time and storage
complexity. In this paper, we propose to transform the problem of approximating
the ground truth PathSim scores into a learning problem. We design an
encoder-decoder based framework, NeuPath, where the algorithmic structure of
PathSim is considered. Specifically, the encoder module identifies Top T
optimized path instances, which can approximate the ground truth PathSim, and
maps each path instance to an embedding vector. The decoder transforms each
embedding vector into a scalar respectively, which identifies the similarity
score. We perform extensive experiments on two real-world datasets in different
domains, ACM and IMDB. Our results demonstrate that NeuPath performs better
than state-of-the-art baselines in the PathSim approximation task and
similarity search task.</p>
</td>
    <td>
      
        Similarity-Search 
      
        CIKM 
      
        Tools-&-Libraries 
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/xie2021learning/">Learning Text-Image Joint Embedding for Efficient Cross-Modal Retrieval with Deep Feature Engineering</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Text-Image Joint Embedding for Efficient Cross-Modal Retrieval with Deep Feature Engineering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Text-Image Joint Embedding for Efficient Cross-Modal Retrieval with Deep Feature Engineering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xie Zhongwei, Liu Ling, Wu Yanzhao, Zhong Luo, Li Lin</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Services Computing</td>
    <td>25</td>
    <td><p>This paper introduces a two-phase deep feature engineering framework for
efficient learning of semantics enhanced joint embedding, which clearly
separates the deep feature engineering in data preprocessing from training the
text-image joint embedding model. We use the Recipe1M dataset for the technical
description and empirical validation. In preprocessing, we perform deep feature
engineering by combining deep feature engineering with semantic context
features derived from raw text-image input data. We leverage LSTM to identify
key terms, deep NLP models from the BERT family, TextRank, or TF-IDF to produce
ranking scores for key terms before generating the vector representation for
each key term by using word2vec. We leverage wideResNet50 and word2vec to
extract and encode the image category semantics of food images to help semantic
alignment of the learned recipe and image embeddings in the joint latent space.
In joint embedding learning, we perform deep feature engineering by optimizing
the batch-hard triplet loss function with soft-margin and double negative
sampling, taking into account also the category-based alignment loss and
discriminator-based alignment loss. Extensive experiments demonstrate that our
SEJE approach with deep feature engineering significantly outperforms the
state-of-the-art approaches.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Tools-&-Libraries 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/wieczorek2021unreasonable/">On the Unreasonable Effectiveness of Centroids in Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=On the Unreasonable Effectiveness of Centroids in Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=On the Unreasonable Effectiveness of Centroids in Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wieczorek Mikolaj, Rychalska Barbara, Dabrowski Jacek</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>84</td>
    <td><p>Image retrieval task consists of finding similar images to a query image from
a set of gallery (database) images. Such systems are used in various
applications e.g. person re-identification (ReID) or visual product search.
Despite active development of retrieval models it still remains a challenging
task mainly due to large intra-class variance caused by changes in view angle,
lighting, background clutter or occlusion, while inter-class variance may be
relatively low. A large portion of current research focuses on creating more
robust features and modifying objective functions, usually based on Triplet
Loss. Some works experiment with using centroid/proxy representation of a class
to alleviate problems with computing speed and hard samples mining used with
Triplet Loss. However, these approaches are used for training alone and
discarded during the retrieval stage. In this paper we propose to use the mean
centroid representation both during training and retrieval. Such an aggregated
representation is more robust to outliers and assures more stable features. As
each class is represented by a single embedding - the class centroid - both
retrieval time and storage requirements are reduced significantly. Aggregating
multiple embeddings results in a significant reduction of the search space due
to lowering the number of candidate target vectors, which makes the method
especially suitable for production deployments. Comprehensive experiments
conducted on two ReID and Fashion Retrieval datasets demonstrate effectiveness
of our method, which outperforms the current state-of-the-art. We propose
centroid training and retrieval as a viable method for both Fashion Retrieval
and ReID applications.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/wang2024weakly/">Weakly Supervised Deep Hyperspherical Quantization for Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Weakly Supervised Deep Hyperspherical Quantization for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Weakly Supervised Deep Hyperspherical Quantization for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Jinpeng, Chen Bin, Zhang Qiang, Meng Zaiqiao, Liang Shangsong, Xia Shu-tao</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>9</td>
    <td><p>Deep quantization methods have shown high efficiency on large-scale image
retrieval. However, current models heavily rely on ground-truth information,
hindering the application of quantization in label-hungry scenarios. A more
realistic demand is to learn from inexhaustible uploaded images that are
associated with informal tags provided by amateur users. Though such sketchy
tags do not obviously reveal the labels, they actually contain useful semantic
information for supervising deep quantization. To this end, we propose
Weakly-Supervised Deep Hyperspherical Quantization (WSDHQ), which is the first
work to learn deep quantization from weakly tagged images. Specifically, 1) we
use word embeddings to represent the tags and enhance their semantic
information based on a tag correlation graph. 2) To better preserve semantic
information in quantization codes and reduce quantization error, we jointly
learn semantics-preserving embeddings and supervised quantizer on hypersphere
by employing a well-designed fusion layer and tailor-made loss functions.
Extensive experiments show that WSDHQ can achieve state-of-art performance on
weakly-supervised compact coding. Code is available at
https://github.com/gimpong/AAAI21-WSDHQ.</p>
</td>
    <td>
      
        Supervised 
      
        Image-Retrieval 
      
        Quantization 
      
        AAAI 
      
        Scalability 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/zhang2021deep/">Deep Center-Based Dual-Constrained Hashing for Discriminative Face Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Center-Based Dual-Constrained Hashing for Discriminative Face Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Center-Based Dual-Constrained Hashing for Discriminative Face Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Ming, Zhe, Yan</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition</td>
    <td>21</td>
    <td><p>With the advantages of low storage cost and extremely fast retrieval speed, deep hashing methods have attracted much attention for image retrieval recently. However, large-scale face image retrieval with significant intra-class variations is still challenging. Neither existing pairwise/triplet labels-based nor softmax classification loss-based deep hashing works can generate compact and discriminative binary codes. Considering these issues, we propose a center-based framework integrating end-to-end hashing learning and class centers learning simultaneously. The framework minimizes the intra-class variance by clustering intra-class samples into a learnable class center. To strengthen inter-class separability, it additionally imposes a novel regularization term to enlarge the Hamming distance between pairwise class centers. Moreover, a simple yet effective regression matrix is introduced to encourage intra-class samples to generate the same binary codes, which further enhances the hashing codes compactness. Experiments on four large-scale datasets show the proposed method outperforms state-of-the-art baselines under various code lengths and commonly-used evaluation metrics.</p>
</td>
    <td>
      
        Scalability 
      
        Efficiency 
      
        Evaluation 
      
        Datasets 
      
        Compact-Codes 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Neural-Hashing 
      
        Memory-Efficiency 
      
        Image-Retrieval 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/wang2021scene/">Scene Text Retrieval via Joint Text Detection and Similarity Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Scene Text Retrieval via Joint Text Detection and Similarity Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Scene Text Retrieval via Joint Text Detection and Similarity Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Hao, Bai Xiang, Yang Mingkun, Zhu Shenggao, Wang Jing, Liu Wenyu</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>41</td>
    <td><p>Scene text retrieval aims to localize and search all text instances from an
image gallery, which are the same or similar to a given query text. Such a task
is usually realized by matching a query text to the recognized words, outputted
by an end-to-end scene text spotter. In this paper, we address this problem by
directly learning a cross-modal similarity between a query text and each text
instance from natural images. Specifically, we establish an end-to-end
trainable network, jointly optimizing the procedures of scene text detection
and cross-modal similarity learning. In this way, scene text retrieval can be
simply performed by ranking the detected text instances with the learned
similarity. Experiments on three benchmark datasets demonstrate our method
consistently outperforms the state-of-the-art scene text spotting/retrieval
approaches. In particular, the proposed framework of joint detection and
similarity learning achieves significantly better performance than separated
methods. Code is available at: https://github.com/lanfeng4659/STR-TDSL.</p>
</td>
    <td>
      
        Text-Retrieval 
      
        Tools-&-Libraries 
      
        Datasets 
      
        CVPR 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/wang2021pseudo/">Pseudo-Relevance Feedback for Multiple Representation Dense Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Pseudo-Relevance Feedback for Multiple Representation Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Pseudo-Relevance Feedback for Multiple Representation Dense Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Xiao, Macdonald Craig, Tonellotto Nicola, Ounis Iadh</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2021 ACM SIGIR International Conference on Theory of Information Retrieval</td>
    <td>54</td>
    <td><p>Pseudo-relevance feedback mechanisms, from Rocchio to the relevance models,
have shown the usefulness of expanding and reweighting the users’ initial
queries using information occurring in an initial set of retrieved documents,
known as the pseudo-relevant set. Recently, dense retrieval – through the use
of neural contextual language models such as BERT for analysing the documents’
and queries’ contents and computing their relevance scores – has shown a
promising performance on several information retrieval tasks still relying on
the traditional inverted index for identifying documents relevant to a query.
Two different dense retrieval families have emerged: the use of single embedded
representations for each passage and query (e.g. using BERT’s [CLS] token), or
via multiple representations (e.g. using an embedding for each token of the
query and document). In this work, we conduct the first study into the
potential for multiple representation dense retrieval to be enhanced using
pseudo-relevance feedback. In particular, based on the pseudo-relevant set of
documents identified using a first-pass dense retrieval, we extract
representative feedback embeddings (using KMeans clustering) – while ensuring
that these embeddings discriminate among passages (based on IDF) – which are
then added to the query representation. These additional feedback embeddings
are shown to both enhance the effectiveness of a reranking as well as an
additional dense retrieval operation. Indeed, experiments on the MSMARCO
passage ranking dataset show that MAP can be improved by upto 26% on the TREC
2019 query set and 10% on the TREC 2020 query set by the application of our
proposed ColBERT-PRF method on a ColBERT dense retrieval approach.</p>
</td>
    <td>
      
        Datasets 
      
        SIGIR 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/wang2021domain/">Domain-Smoothing Network for Zero-Shot Sketch-Based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Domain-Smoothing Network for Zero-Shot Sketch-Based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Domain-Smoothing Network for Zero-Shot Sketch-Based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Zhipeng, Wang Hao, Yan Jiexi, Wu Aming, Deng Cheng</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence</td>
    <td>28</td>
    <td><p>Zero-Shot Sketch-Based Image Retrieval (ZS-SBIR) is a novel cross-modal
retrieval task, where abstract sketches are used as queries to retrieve natural
images under zero-shot scenario. Most existing methods regard ZS-SBIR as a
traditional classification problem and employ a cross-entropy or triplet-based
loss to achieve retrieval, which neglect the problems of the domain gap between
sketches and natural images and the large intra-class diversity in sketches.
Toward this end, we propose a novel Domain-Smoothing Network (DSN) for ZS-SBIR.
Specifically, a cross-modal contrastive method is proposed to learn generalized
representations to smooth the domain gap by mining relations with additional
augmented samples. Furthermore, a category-specific memory bank with sketch
features is explored to reduce intra-class diversity in the sketch domain.
Extensive experiments demonstrate that our approach notably outperforms the
state-of-the-art methods in both Sketchy and TU-Berlin datasets. Our source
code is publicly available at https://github.com/haowang1992/DSN.</p>
</td>
    <td>
      
        AAAI 
      
        Datasets 
      
        Few-Shot-&-Zero-Shot 
      
        IJCAI 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/wang2021comprehensive/">A Comprehensive Survey and Experimental Comparison of Graph-Based Approximate Nearest Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Comprehensive Survey and Experimental Comparison of Graph-Based Approximate Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Comprehensive Survey and Experimental Comparison of Graph-Based Approximate Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Mengzhao, Xu Xiaoliang, Yue Qiang, Wang Yuxiang</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the VLDB Endowment</td>
    <td>119</td>
    <td><p>Approximate nearest neighbor search (ANNS) constitutes an important operation
in a multitude of applications, including recommendation systems, information
retrieval, and pattern recognition. In the past decade, graph-based ANNS
algorithms have been the leading paradigm in this domain, with dozens of
graph-based ANNS algorithms proposed. Such algorithms aim to provide effective,
efficient solutions for retrieving the nearest neighbors for a given query.
Nevertheless, these efforts focus on developing and optimizing algorithms with
different approaches, so there is a real need for a comprehensive survey about
the approaches’ relative performance, strengths, and pitfalls. Thus here we
provide a thorough comparative analysis and experimental evaluation of 13
representative graph-based ANNS algorithms via a new taxonomy and fine-grained
pipeline. We compared each algorithm in a uniform test environment on eight
real-world datasets and 12 synthetic datasets with varying sizes and
characteristics. Our study yields novel discoveries, offerings several useful
principles to improve algorithms, thus designing an optimized method that
outperforms the state-of-the-art algorithms. This effort also helped us
pinpoint algorithms’ working portions, along with rule-of-thumb recommendations
about promising research directions and suitable algorithms for practitioners
in different fields.</p>
</td>
    <td>
      
        Graph-Based-ANN 
      
        Datasets 
      
        Recommender-Systems 
      
        Survey-Paper 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/wang2021cross/">Cross-modal Zero-shot Hashing by Label Attributes Embedding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-modal Zero-shot Hashing by Label Attributes Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-modal Zero-shot Hashing by Label Attributes Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Runmin, Yu Guoxian, Liu Lei, Cui Lizhen, Domeniconi Carlotta, Zhang Xiangliang</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>31</td>
    <td><p>Cross-modal hashing (CMH) is one of the most promising methods in cross-modal
approximate nearest neighbor search. Most CMH solutions ideally assume the
labels of training and testing set are identical. However, the assumption is
often violated, causing a zero-shot CMH problem. Recent efforts to address this
issue focus on transferring knowledge from the seen classes to the unseen ones
using label attributes. However, the attributes are isolated from the features
of multi-modal data. To reduce the information gap, we introduce an approach
called LAEH (Label Attributes Embedding for zero-shot cross-modal Hashing).
LAEH first gets the initial semantic attribute vectors of labels by word2vec
model and then uses a transformation network to transform them into a common
subspace. Next, it leverages the hash vectors and the feature similarity matrix
to guide the feature extraction network of different modalities. At the same
time, LAEH uses the attribute similarity as the supplement of label similarity
to rectify the label embedding and common subspace. Experiments show that LAEH
outperforms related representative zero-shot and cross-modal hashing methods.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Few-Shot-&-Zero-Shot 
      
        SIGIR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/wang2021prototype/">Prototype-Supervised Adversarial Network for Targeted Attack of Deep Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Prototype-Supervised Adversarial Network for Targeted Attack of Deep Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Prototype-Supervised Adversarial Network for Targeted Attack of Deep Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Xunguang, Zhang, Wu, Shen, Lu</td> <!-- 🔧 You were missing this -->
    <td>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>43</td>
    <td><p>Due to its powerful capability of representation learning and high-efficiency computation, deep hashing has made significant progress in large-scale image retrieval. However, deep hashing networks are vulnerable to adversarial examples, which is a practical secure problem but seldom studied in hashing-based retrieval field. In this paper, we propose a novel prototype-supervised adversarial network (ProS-GAN), which formulates a flexible generative architecture for efficient and effective targeted hashing attack. To the best of our knowledge, this is the first generation-based method to attack deep hashing networks. Generally, our proposed framework consists of three parts, i.e., a PrototypeNet, a generator and a discriminator. Specifically, the designed PrototypeNet embeds the target label into the semantic representation and learns the prototype code as the category-level representative of the target label. Moreover, the semantic representation and the original image are jointly fed into the generator for flexible targeted attack. Particularly, the prototype code is adopted to supervise the generator to construct the targeted adversarial example by minimizing the Hamming distance between the hash code of the adversarial example and the prototype code. Furthermore, the generator is against the discriminator to simultaneously encourage the adversarial examples visually realistic and the semantic representation informative. Extensive experiments verify that the proposed framework can efficiently produce adversarial examples with better targeted attack performance and transferability over state-of-the-art targeted attack methods of deep hashing.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Efficiency 
      
        CVPR 
      
        Neural-Hashing 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/xiong2020approximate/">Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xiong Lee, Xiong Chenyan, Li Ye, Tang Kwok-fung, Liu Jialin, Bennett Paul, Ahmed Junaid, Overwijk Arnold</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>359</td>
    <td><p>Conducting text retrieval in a dense learned representation space has many
intriguing advantages over sparse retrieval. Yet the effectiveness of dense
retrieval (DR) often requires combination with sparse retrieval. In this paper,
we identify that the main bottleneck is in the training mechanisms, where the
negative instances used in training are not representative of the irrelevant
documents in testing. This paper presents Approximate nearest neighbor Negative
Contrastive Estimation (ANCE), a training mechanism that constructs negatives
from an Approximate Nearest Neighbor (ANN) index of the corpus, which is
parallelly updated with the learning process to select more realistic negative
training instances. This fundamentally resolves the discrepancy between the
data distribution used in the training and testing of DR. In our experiments,
ANCE boosts the BERT-Siamese DR model to outperform all competitive dense and
sparse retrieval baselines. It nearly matches the accuracy of
sparse-retrieval-and-BERT-reranking using dot-product in the ANCE-learned
representation space and provides almost 100x speed-up.</p>
</td>
    <td>
      
        Self-Supervised 
      
        Text-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/zhang2021high/">High-order nonlocal Hashing for unsupervised cross-modal retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=High-order nonlocal Hashing for unsupervised cross-modal retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=High-order nonlocal Hashing for unsupervised cross-modal retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Peng-fei, Luo, Huang, Xu, Song</td> <!-- 🔧 You were missing this -->
    <td>World Wide Web</td>
    <td>62</td>
    <td><p>In light of the ability to enable efficient storage and fast query for big data, hashing techniques for cross-modal search have aroused extensive attention. Despite the great success achieved, unsupervised cross-modal hashing still suffers from lacking reliable similarity supervision and struggles with handling the heterogeneity issue between different modalities. To cope with these, in this paper, we devise a new deep hashing model, termed as High-order Nonlocal Hashing (HNH) to facilitate cross-modal retrieval with the following advantages. First, different from existing methods that mainly leverage low-level local-view similarity as the guidance for hashing learning, we propose a high-order affinity measure that considers the multi-modal neighbourhood structures from a nonlocal perspective, thereby comprehensively capturing the similarity relationships between data items. Second, a common representation is introduced to correlate different modalities. By enforcing the modal-specific descriptors and the common representation to be aligned with each other, the proposed HNH significantly bridges the modality gap and maintains the intra-consistency. Third, an effective affinity preserving objective function is delicately designed to generate high-quality binary codes. Extensive experiments evidence the superiority of the proposed HNH in unsupervised cross-modal retrieval tasks over the state-of-the-art baselines.</p>
</td>
    <td>
      
        Neural-Hashing 
      
        Multimodal-Retrieval 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/wang2020asymmetric/">Asymmetric Correlation Quantization Hashing for Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Asymmetric Correlation Quantization Hashing for Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Asymmetric Correlation Quantization Hashing for Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Lu, Yang Jie</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>21</td>
    <td><p>Due to the superiority in similarity computation and database storage for
large-scale multiple modalities data, cross-modal hashing methods have
attracted extensive attention in similarity retrieval across the heterogeneous
modalities. However, there are still some limitations to be further taken into
account: (1) most current CMH methods transform real-valued data points into
discrete compact binary codes under the binary constraints, limiting the
capability of representation for original data on account of abundant loss of
information and producing suboptimal hash codes; (2) the discrete binary
constraint learning model is hard to solve, where the retrieval performance may
greatly reduce by relaxing the binary constraints for large quantization error;
(3) handling the learning problem of CMH in a symmetric framework, leading to
difficult and complex optimization objective. To address above challenges, in
this paper, a novel Asymmetric Correlation Quantization Hashing (ACQH) method
is proposed. Specifically, ACQH learns the projection matrixs of heterogeneous
modalities data points for transforming query into a low-dimensional
real-valued vector in latent semantic space and constructs the stacked
compositional quantization embedding in a coarse-to-fine manner for indicating
database points by a series of learnt real-valued codeword in the codebook with
the help of pointwise label information regression simultaneously. Besides, the
unified hash codes across modalities can be directly obtained by the discrete
iterative optimization framework devised in the paper. Comprehensive
experiments on diverse three benchmark datasets have shown the effectiveness
and rationality of ACQH.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Datasets 
      
        Quantization 
      
        Compact-Codes 
      
        Scalability 
      
        Evaluation 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2021</td>
    <td>
      <a href="/publications/wang2020distilling/">Distilling Knowledge by Mimicking Features</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Distilling Knowledge by Mimicking Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Distilling Knowledge by Mimicking Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Guo-hua, Ge Yifan, Wu Jianxin</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>21</td>
    <td><p>Knowledge distillation (KD) is a popular method to train efficient networks
(“student”) with the help of high-capacity networks (“teacher”). Traditional
methods use the teacher’s soft logits as extra supervision to train the student
network. In this paper, we argue that it is more advantageous to make the
student mimic the teacher’s features in the penultimate layer. Not only the
student can directly learn more effective information from the teacher feature,
feature mimicking can also be applied for teachers trained without a softmax
layer. Experiments show that it can achieve higher accuracy than traditional
KD. To further facilitate feature mimicking, we decompose a feature vector into
the magnitude and the direction. We argue that the teacher should give more
freedom to the student feature’s magnitude, and let the student pay more
attention on mimicking the feature direction. To meet this requirement, we
propose a loss term based on locality-sensitive hashing (LSH). With the help of
this new loss, our method indeed mimics feature directions more accurately,
relaxes constraints on feature magnitudes, and achieves state-of-the-art
distillation accuracy. We provide theoretical analyses of how LSH facilitates
feature direction mimicking, and further extend feature mimicking to
multi-label recognition and object detection.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Locality-Sensitive-Hashing 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/lin2019hadamard/">Hadamard Matrix Guided Online Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hadamard Matrix Guided Online Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hadamard Matrix Guided Online Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin Mingbao, Ji Rongrong, Liu Hong, Sun Xiaoshuai, Chen Shen, Tian Qi</td> <!-- 🔧 You were missing this -->
    <td>International Journal of Computer Vision</td>
    <td>41</td>
    <td><p>Online image hashing has attracted increasing research attention recently,
which receives large-scale data in a streaming manner to update the hash
functions on-the-fly. Its key challenge lies in the difficulty of balancing the
learning timeliness and model accuracy. To this end, most works follow a
supervised setting, i.e., using class labels to boost the hashing performance,
which defects in two aspects: First, strong constraints, e.g., orthogonal or
similarity preserving, are used, which however are typically relaxed and lead
to large accuracy drop. Second, large amounts of training batches are required
to learn the up-to-date hash functions, which largely increase the learning
complexity. To handle the above challenges, a novel supervised online hashing
scheme termed Hadamard Matrix Guided Online Hashing (HMOH) is proposed in this
paper. Our key innovation lies in introducing Hadamard matrix, which is an
orthogonal binary matrix built via Sylvester method. In particular, to release
the need of strong constraints, we regard each column of Hadamard matrix as the
target code for each class label, which by nature satisfies several desired
properties of hashing codes. To accelerate the online training, LSH is first
adopted to align the lengths of target code and to-be-learned binary code. We
then treat the learning of hash functions as a set of binary classification
problems to fit the assigned target code. Finally, extensive experiments
demonstrate the superior accuracy and efficiency of the proposed method over
various state-of-the-art methods. Codes are available at
https://github.com/lmbxmu/mycode.</p>
</td>
    <td>
      
        Supervised 
      
        Locality-Sensitive-Hashing 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Compact-Codes 
      
        Scalability 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/lin2019fashion/">Fashion Outfit Complementary Item Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fashion Outfit Complementary Item Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fashion Outfit Complementary Item Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin Yen-liang, Tran Son, Davis Larry S.</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>81</td>
    <td><p>Complementary fashion item recommendation is critical for fashion outfit
completion. Existing methods mainly focus on outfit compatibility prediction
but not in a retrieval setting. We propose a new framework for outfit
complementary item retrieval. Specifically, a category-based subspace attention
network is presented, which is a scalable approach for learning the subspace
attentions. In addition, we introduce an outfit ranking loss that better models
the item relationships of an entire outfit. We evaluate our method on the
outfit compatibility, FITB and new retrieval tasks. Experimental results
demonstrate that our approach outperforms state-of-the-art methods in both
compatibility prediction and complementary item retrieval</p>
</td>
    <td>
      
        Recommender-Systems 
      
        CVPR 
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/lin2020fast/">Fast Class-wise Updating for Online Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Class-wise Updating for Online Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Class-wise Updating for Online Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin Mingbao, Ji Rongrong, Sun Xiaoshuai, Zhang Baochang, Huang Feiyue, Tian Yonghong, Tao Dacheng</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>22</td>
    <td><p>Online image hashing has received increasing research attention recently,
which processes large-scale data in a streaming fashion to update the hash
functions on-the-fly. To this end, most existing works exploit this problem
under a supervised setting, i.e., using class labels to boost the hashing
performance, which suffers from the defects in both adaptivity and efficiency:
First, large amounts of training batches are required to learn up-to-date hash
functions, which leads to poor online adaptivity. Second, the training is
time-consuming, which contradicts with the core need of online learning. In
this paper, a novel supervised online hashing scheme, termed Fast Class-wise
Updating for Online Hashing (FCOH), is proposed to address the above two
challenges by introducing a novel and efficient inner product operation. To
achieve fast online adaptivity, a class-wise updating method is developed to
decompose the binary code learning and alternatively renew the hash functions
in a class-wise fashion, which well addresses the burden on large amounts of
training batches. Quantitatively, such a decomposition further leads to at
least 75% storage saving. To further achieve online efficiency, we propose a
semi-relaxation optimization, which accelerates the online training by treating
different binary constraints independently. Without additional constraints and
variables, the time complexity is significantly reduced. Such a scheme is also
quantitatively shown to well preserve past information during updating hashing
functions. We have quantitatively demonstrated that the collective effort of
class-wise updating and semi-relaxation optimization provides a superior
performance comparing to various state-of-the-art methods, which is verified
through extensive experiments on three widely-used datasets.</p>
</td>
    <td>
      
        Supervised 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Compact-Codes 
      
        Scalability 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/yuan2019central/">Central Similarity Quantization for Efficient Image and Video Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Central Similarity Quantization for Efficient Image and Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Central Similarity Quantization for Efficient Image and Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yuan Li, Wang Tao, Zhang Xiaopeng, Tay Francis Eh, Jie Zequn, Liu Wei, Feng Jiashi</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>288</td>
    <td><p>Existing data-dependent hashing methods usually learn hash functions from
pairwise or triplet data relationships, which only capture the data similarity
locally, and often suffer from low learning efficiency and low collision rate.
In this work, we propose a new <em>global</em> similarity metric, termed as
<em>central similarity</em>, with which the hash codes of similar data pairs are
encouraged to approach a common center and those for dissimilar pairs to
converge to different centers, to improve hash learning efficiency and
retrieval accuracy. We principally formulate the computation of the proposed
central similarity metric by introducing a new concept, i.e., <em>hash
center</em> that refers to a set of data points scattered in the Hamming space with
a sufficient mutual distance between each other. We then provide an efficient
method to construct well separated hash centers by leveraging the Hadamard
matrix and Bernoulli distributions. Finally, we propose the Central Similarity
Quantization (CSQ) that optimizes the central similarity between data points
w.r.t.\ their hash centers instead of optimizing the local similarity. CSQ is
generic and applicable to both image and video hashing scenarios. Extensive
experiments on large-scale image and video retrieval tasks demonstrate that CSQ
can generate cohesive hash codes for similar data pairs and dispersed hash
codes for dissimilar pairs, achieving a noticeable boost in retrieval
performance, i.e. 3%-20% in mAP over the previous state-of-the-arts. The code
is at: https://github.com/yuanli2333/Hadamard-Matrix-for-hashing</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Distance-Metric-Learning 
      
        Quantization 
      
        Video-Retrieval 
      
        CVPR 
      
        Scalability 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/yu2020retrieval/">Retrieval of Family Members Using Siamese Neural Network</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Retrieval of Family Members Using Siamese Neural Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Retrieval of Family Members Using Siamese Neural Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yu Jun, Xie Guochen, Li Mengyan, Hao Xinlong</td> <!-- 🔧 You were missing this -->
    <td>2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)</td>
    <td>13</td>
    <td><p>Retrieval of family members in the wild aims at finding family members of the
given subject in the dataset, which is useful in finding the lost children and
analyzing the kinship. However, due to the diversity in age, gender, pose and
illumination of the collected data, this task is always challenging. To solve
this problem, we propose our solution with deep Siamese neural network. Our
solution can be divided into two parts: similarity computation and ranking. In
training procedure, the Siamese network firstly takes two candidate images as
input and produces two feature vectors. And then, the similarity between the
two vectors is computed with several fully connected layers. While in inference
procedure, we try another similarity computing method by dropping the followed
several fully connected layers and directly computing the cosine similarity of
the two feature vectors. After similarity computation, we use the ranking
algorithm to merge the similarity scores with the same identity and output the
ordered list according to their similarities. To gain further improvement, we
try different combinations of backbones, training methods and similarity
computing methods. Finally, we submit the best combination as our solution and
our team(ustc-nelslip) obtains favorable result in the track3 of the RFIW2020
challenge with the first runner-up, which verifies the effectiveness of our
method. Our code is available at: https://github.com/gniknoil/FG2020-kinship</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/li2020deep/">Deep Unsupervised Image Hashing by Maximizing Bit Entropy</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Unsupervised Image Hashing by Maximizing Bit Entropy' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Unsupervised Image Hashing by Maximizing Bit Entropy' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Yunqiang, van Gemert Jan</td> <!-- 🔧 You were missing this -->
    <td>2020 Data Compression Conference (DCC)</td>
    <td>8</td>
    <td><p>Unsupervised hashing is important for indexing huge image or video
collections without having expensive annotations available. Hashing aims to
learn short binary codes for compact storage and efficient semantic retrieval.
We propose an unsupervised deep hashing layer called Bi-half Net that maximizes
entropy of the binary codes. Entropy is maximal when both possible values of
the bit are uniformly (half-half) distributed. To maximize bit entropy, we do
not add a term to the loss function as this is difficult to optimize and tune.
Instead, we design a new parameter-free network layer to explicitly force
continuous image features to approximate the optimal half-half bit
distribution. This layer is shown to minimize a penalized term of the
Wasserstein distance between the learned continuous image features and the
optimal half-half bit distribution. Experimental results on the image datasets
Flickr25k, Nus-wide, Cifar-10, Mscoco, Mnist and the video datasets Ucf-101 and
Hmdb-51 show that our approach leads to compact codes and compares favorably to
the current state-of-the-art.</p>
</td>
    <td>
      
        Supervised 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/zhao2020stacked/">Stacked Convolutional Deep Encoding Network for Video-Text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Stacked Convolutional Deep Encoding Network for Video-Text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Stacked Convolutional Deep Encoding Network for Video-Text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhao Rui, Zheng Kecheng, Zha Zheng-jun</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE International Conference on Multimedia and Expo (ICME)</td>
    <td>11</td>
    <td><p>Existing dominant approaches for cross-modal video-text retrieval task are to
learn a joint embedding space to measure the cross-modal similarity. However,
these methods rarely explore long-range dependency inside video frames or
textual words leading to insufficient textual and visual details. In this
paper, we propose a stacked convolutional deep encoding network for video-text
retrieval task, which considers to simultaneously encode long-range and
short-range dependency in the videos and texts. Specifically, a multi-scale
dilated convolutional (MSDC) block within our approach is able to encode
short-range temporal cues between video frames or text words by adopting
different scales of kernel size and dilation size of convolutional layer. A
stacked structure is designed to expand the receptive fields by repeatedly
adopting the MSDC block, which further captures the long-range relations
between these cues. Moreover, to obtain more robust textual representations, we
fully utilize the powerful language model named Transformer in two stages:
pretraining phrase and fine-tuning phrase. Extensive experiments on two
different benchmark datasets (MSR-VTT, MSVD) show that our proposed method
outperforms other state-of-the-art approaches.</p>
</td>
    <td>
      
        Datasets 
      
        Text-Retrieval 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/lei2020locality/">Locality-Sensitive Hashing Scheme based on Longest Circular Co-Substring</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Locality-Sensitive Hashing Scheme based on Longest Circular Co-Substring' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Locality-Sensitive Hashing Scheme based on Longest Circular Co-Substring' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lei Yifan, Huang Qiang, Kankanhalli Mohan, Tung Anthony K. H.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data</td>
    <td>24</td>
    <td><p>Locality-Sensitive Hashing (LSH) is one of the most popular methods for
\(c\)-Approximate Nearest Neighbor Search (\(c\)-ANNS) in high-dimensional spaces.
In this paper, we propose a novel LSH scheme based on the Longest Circular
Co-Substring (LCCS) search framework (LCCS-LSH) with a theoretical guarantee.
We introduce a novel concept of LCCS and a new data structure named Circular
Shift Array (CSA) for \(k\)-LCCS search. The insight of LCCS search framework is
that close data objects will have a longer LCCS than the far-apart ones with
high probability. LCCS-LSH is <em>LSH-family-independent</em>, and it supports
\(c\)-ANNS with different kinds of distance metrics. We also introduce a
multi-probe version of LCCS-LSH and conduct extensive experiments over five
real-life datasets. The experimental results demonstrate that LCCS-LSH
outperforms state-of-the-art LSH schemes.</p>
</td>
    <td>
      
        Locality-Sensitive-Hashing 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Distance-Metric-Learning 
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/liang2020embedding/">Embedding-based Zero-shot Retrieval through Query Generation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Embedding-based Zero-shot Retrieval through Query Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Embedding-based Zero-shot Retrieval through Query Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liang Davis, Xu Peng, Shakeri Siamak, Santos Cicero Nogueira Dos, Nallapati Ramesh, Huang Zhiheng, Xiang Bing</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>23</td>
    <td><p>Passage retrieval addresses the problem of locating relevant passages,
usually from a large corpus, given a query. In practice, lexical term-matching
algorithms like BM25 are popular choices for retrieval owing to their
efficiency. However, term-based matching algorithms often miss relevant
passages that have no lexical overlap with the query and cannot be finetuned to
downstream datasets. In this work, we consider the embedding-based two-tower
architecture as our neural retrieval model. Since labeled data can be scarce
and because neural retrieval models require vast amounts of data to train, we
propose a novel method for generating synthetic training data for retrieval.
Our system produces remarkable results, significantly outperforming BM25 on 5
out of 6 datasets tested, by an average of 2.45 points for Recall@1. In some
cases, our model trained on synthetic data can even outperform the same model
trained on real data</p>
</td>
    <td>
      
        Datasets 
      
        Few-Shot-&-Zero-Shot 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/yuan2020central/">Central Similarity Hashing for Efficient Image and Video Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Central Similarity Hashing for Efficient Image and Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Central Similarity Hashing for Efficient Image and Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yuan Li, Wang, Zhang, Jie, Tay, Feng</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>288</td>
    <td><p>Existing data-dependent hashing methods usually learn
hash functions from the pairwise or triplet data relationships, which only capture the data similarity locally, and
often suffer low learning efficiency and low collision rate.
In this work, we propose a new global similarity metric,
termed as central similarity, with which the hash codes for
similar data pairs are encouraged to approach a common
center and those for dissimilar pairs to converge to different centers, to improve hash learning efficiency and retrieval accuracy. We principally formulate the computation of the proposed central similarity metric by introducing a new concept, i.e. hash center that refers to a set
of data points scattered in the Hamming space with sufficient mutual distance between each other. We then provide an efficient method to construct well separated hash
centers by leveraging the Hadamard matrix and Bernoulli
distributions. Finally, we propose the Central Similarity
Hashing (CSH) that optimizes the central similarity between data points w.r.t. their hash centers instead of optimizing the local similarity. The CSH is generic and applicable to both image and video hashing. Extensive experiments on large-scale image and video retrieval demonstrate CSH can generate cohesive hash codes for similar
data pairs and dispersed hash codes for dissimilar pairs,
and achieve noticeable boost in retrieval performance, i.e.
3%-20% in mAP over the previous state-of-the-art. The
codes are in: https://github.com/yuanli2333/
Hadamard-Matrix-for-hashing</p>
</td>
    <td>
      
        Video-Retrieval 
      
        Scalability 
      
        Efficiency 
      
        Distance-Metric-Learning 
      
        CVPR 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/laskar2019geometric/">Geometric Image Correspondence Verification by Dense Pixel Matching</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Geometric Image Correspondence Verification by Dense Pixel Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Geometric Image Correspondence Verification by Dense Pixel Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Laskar Zakaria, Melekhov Iaroslav, Tavakoli Hamed R., Ylioinas Juha, Kannala Juho</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>12</td>
    <td><p>This paper addresses the problem of determining dense pixel correspondences
between two images and its application to geometric correspondence verification
in image retrieval. The main contribution is a geometric correspondence
verification approach for re-ranking a shortlist of retrieved database images
based on their dense pair-wise matching with the query image at a pixel level.
We determine a set of cyclically consistent dense pixel matches between the
pair of images and evaluate local similarity of matched pixels using neural
network based image descriptors. Final re-ranking is based on a novel
similarity function, which fuses the local similarity metric with a global
similarity metric and a geometric consistency measure computed for the matched
pixels. For dense matching our approach utilizes a modified version of a
recently proposed dense geometric correspondence network (DGC-Net), which we
also improve by optimizing the architecture. The proposed model and similarity
metric compare favourably to the state-of-the-art image retrieval methods. In
addition, we apply our method to the problem of long-term visual localization
demonstrating promising results and generalization across datasets.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Hybrid-ANN-Methods 
      
        Re-Ranking 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/kwok2025learning/">Learning to Hash with a Dimension Analysis-based Quantizer for Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning to Hash with a Dimension Analysis-based Quantizer for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning to Hash with a Dimension Analysis-based Quantizer for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kwok Yuan</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>10</td>
    <td><p>The last few years have witnessed the rise of the big data era in which approximate nearest neighbor search is a fundamental problem in many applications, such as large-scale image retrieval. Recently, many research results have demonstrated that hashing can achieve promising performance due to its appealing storage and search efficiency. Since complex optimization problems for loss functions are difficult to solve, most hashing methods decompose the hash code learning problem into two steps: projection and quantization. In the quantization step, binary codes are widely used because ranking them by the Hamming distance is very efficient. However, the massive information loss produced by the quantization step should be reduced in applications where high search accuracy is required, such as in image retrieval. Since many two-step hashing methods produce uneven projected dimensions in the projection step, in this paper, we propose a novel dimension analysis-based quantization (DAQ) on two-step hashing methods for image retrieval. We first perform an importance analysis of the projected dimensions and select a subset of them that are more informative than others, and then we divide the selected projected dimensions into several regions with our quantizer. Every region is quantized with its corresponding codebook. Finally, the similarity between two hash codes is estimated by the Manhattan distance between their corresponding codebooks, which is also efficient. We conduct experiments on three public benchmarks containing up to one million descriptors and show that the proposed DAQ method consistently leads to significant accuracy improvements over state-of-the-art quantization methods.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Efficiency 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Quantization 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/kwok2020learning/">Learning to Hash with a Dimension Analysis-based Quantizer for Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning to Hash with a Dimension Analysis-based Quantizer for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning to Hash with a Dimension Analysis-based Quantizer for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kwok Yuan</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>10</td>
    <td><p>The last few years have witnessed the rise of the big data era in which approximate nearest neighbor search is a fundamental problem in many applications, such as large-scale image retrieval. Recently, many research results have demonstrated that hashing can achieve promising performance due to its appealing storage and search efficiency. Since complex optimization problems for loss functions are difficult to solve, most hashing methods decompose the hash code learning problem into two steps: projection and quantization. In the quantization step, binary codes are widely used because ranking them by the Hamming distance is very efficient. However, the massive information loss produced by the quantization step should be reduced in applications where high search accuracy is required, such as in image retrieval. Since many two-step hashing methods produce uneven projected dimensions in the projection step, in this paper, we propose a novel dimension analysis-based quantization (DAQ) on two-step hashing methods for image retrieval. We first perform an importance analysis of the projected dimensions and select a subset of them that are more informative than others, and then we divide the selected projected dimensions into several regions with our quantizer. Every region is quantized with its corresponding codebook. Finally, the similarity between two hash codes is estimated by the Manhattan distance between their corresponding codebooks, which is also efficient. We conduct experiments on three public benchmarks containing up to one million descriptors and show that the proposed DAQ method consistently leads to significant accuracy improvements over state-of-the-art quantization methods.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Efficiency 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Quantization 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/zhang2020model/">Model Size Reduction Using Frequency Based Double Hashing for Recommender Systems</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Model Size Reduction Using Frequency Based Double Hashing for Recommender Systems' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Model Size Reduction Using Frequency Based Double Hashing for Recommender Systems' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Caojin, Liu Yicun, Xie Yuanpu, Ktena Sofia Ira, Tejani Alykhan, Gupta Akshay, Myana Pranay Kumar, Dilipkumar Deepak, Paul Suvadip, Ihara Ikuhiro, Upadhyaya Prasang, Huszar Ferenc, Shi Wenzhe</td> <!-- 🔧 You were missing this -->
    <td>Fourteenth ACM Conference on Recommender Systems</td>
    <td>31</td>
    <td><p>Deep Neural Networks (DNNs) with sparse input features have been widely used
in recommender systems in industry. These models have large memory requirements
and need a huge amount of training data. The large model size usually entails a
cost, in the range of millions of dollars, for storage and communication with
the inference services. In this paper, we propose a hybrid hashing method to
combine frequency hashing and double hashing techniques for model size
reduction, without compromising performance. We evaluate the proposed models on
two product surfaces. In both cases, experiment results demonstrated that we
can reduce the model size by around 90 % while keeping the performance on par
with the original baselines.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Recommender-Systems 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/yuan2025central/">Central Similarity Quantization for Efficient Image and Video Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Central Similarity Quantization for Efficient Image and Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Central Similarity Quantization for Efficient Image and Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yuan Li, Wang, Zhang, Tay, Jie, Liu, Feng</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>288</td>
    <td><p>Existing data-dependent hashing methods usually learn hash functions from pairwise or triplet data relationships, which only capture the data similarity locally, and often suffer from low learning efficiency and low collision rate. In this work, we propose a new global similarity metric, termed as central similarity, with which the hash codes of similar data pairs are encouraged to approach a common center and those for dissimilar pairs to converge to different centers, to improve hash learning efficiency and retrieval accuracy. We principally formulate the computation of the proposed central similarity metric by introducing a new concept, i.e., hash center that refers to a set of data points scattered in the Hamming space with a sufficient mutual distance between each other. We then provide an efficient method to construct well separated hash centers by leveraging the Hadamard matrix and Bernoulli distributions. Finally, we propose the Central Similarity Quantization (CSQ) that optimizes the central similarity between data points w.r.t. their hash centers instead of optimizing the local similarity. CSQ is generic and applicable to both image and video hashing scenarios. Extensive experiments on large-scale image and video retrieval tasks demonstrate that CSQ can generate cohesive hash codes for similar data pairs and dispersed hash codes for dissimilar pairs, achieving a noticeable boost in retrieval performance, i.e. 3%-20% in mAP over the previous state-of-the-arts.</p>
</td>
    <td>
      
        Video-Retrieval 
      
        Scalability 
      
        Efficiency 
      
        Distance-Metric-Learning 
      
        CVPR 
      
        Hashing-Methods 
      
        Evaluation 
      
        Quantization 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/lee2019contextualized/">Contextualized Sparse Representations for Real-Time Open-Domain Question Answering</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Contextualized Sparse Representations for Real-Time Open-Domain Question Answering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Contextualized Sparse Representations for Real-Time Open-Domain Question Answering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lee Jinhyuk, Seo Minjoon, Hajishirzi Hannaneh, Kang Jaewoo</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</td>
    <td>36</td>
    <td><p>Open-domain question answering can be formulated as a phrase retrieval
problem, in which we can expect huge scalability and speed benefit but often
suffer from low accuracy due to the limitation of existing phrase
representation models. In this paper, we aim to improve the quality of each
phrase embedding by augmenting it with a contextualized sparse representation
(Sparc). Unlike previous sparse vectors that are term-frequency-based (e.g.,
tf-idf) or directly learned (only few thousand dimensions), we leverage
rectified self-attention to indirectly learn sparse vectors in n-gram
vocabulary space. By augmenting the previous phrase retrieval model (Seo et
al., 2019) with Sparc, we show 4%+ improvement in CuratedTREC and SQuAD-Open.
Our CuratedTREC score is even better than the best known retrieve &amp; read model
with at least 45x faster inference speed.</p>
</td>
    <td>
      
        Scalability 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/kang2020learning/">Learning to Embed Categorical Features without Embedding Tables for Recommendation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning to Embed Categorical Features without Embedding Tables for Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning to Embed Categorical Features without Embedding Tables for Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kang Wang-cheng, Cheng Derek Zhiyuan, Yao Tiansheng, Yi Xinyang, Chen Ting, Hong Lichan, Chi Ed H.</td> <!-- 🔧 You were missing this -->
    <td>Companion Proceedings of the Web Conference 2020</td>
    <td>32</td>
    <td><p>Embedding learning of categorical features (e.g. user/item IDs) is at the
core of various recommendation models including matrix factorization and neural
collaborative filtering. The standard approach creates an embedding table where
each row represents a dedicated embedding vector for every unique feature
value. However, this method fails to efficiently handle high-cardinality
features and unseen feature values (e.g. new video ID) that are prevalent in
real-world recommendation systems. In this paper, we propose an alternative
embedding framework Deep Hash Embedding (DHE), replacing embedding tables by a
deep embedding network to compute embeddings on the fly. DHE first encodes the
feature value to a unique identifier vector with multiple hashing functions and
transformations, and then applies a DNN to convert the identifier vector to an
embedding. The encoding module is deterministic, non-learnable, and free of
storage, while the embedding network is updated during the training time to
learn embedding generation. Empirical results show that DHE achieves comparable
AUC against the standard one-hot full embedding, with smaller model sizes. Our
work sheds light on the design of DNN-based alternative embedding schemes for
categorical features without using embedding table lookup.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Recommender-Systems 
      
        Neural-Hashing 
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/kanda2020succinct/">Succinct Trit-array Trie for Scalable Trajectory Similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Succinct Trit-array Trie for Scalable Trajectory Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Succinct Trit-array Trie for Scalable Trajectory Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kanda Shunsuke, Takeuchi Koh, Fujii Keisuke, Tabei Yasuo</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 28th International Conference on Advances in Geographic Information Systems</td>
    <td>11</td>
    <td><p>Massive datasets of spatial trajectories representing the mobility of a
diversity of moving objects are ubiquitous in research and industry. Similarity
search of a large collection of trajectories is indispensable for turning these
datasets into knowledge. Locality sensitive hashing (LSH) is a powerful
technique for fast similarity searches. Recent methods employ LSH and attempt
to realize an efficient similarity search of trajectories; however, those
methods are inefficient in terms of search time and memory when applied to
massive datasets. To address this problem, we present the trajectory-indexing
succinct trit-array trie (tSTAT), which is a scalable method leveraging LSH for
trajectory similarity searches. tSTAT quickly performs the search on a tree
data structure called trie. We also present two novel techniques that enable to
dramatically enhance the memory efficiency of tSTAT. One is a node reduction
technique that substantially omits redundant trie nodes while maintaining the
time performance. The other is a space-efficient representation that leverages
the idea behind succinct data structures (i.e., a compressed data structure
supporting fast data operations). We experimentally test tSTAT on its ability
to retrieve similar trajectories for a query from large collections of
trajectories and show that tSTAT performs superiorly in comparison to
state-of-the-art similarity search methods.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Locality-Sensitive-Hashing 
      
        Hashing-Methods 
      
        Datasets 
      
        Memory-Efficiency 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/khandelwal2020nearest/">Nearest Neighbor Machine Translation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Nearest Neighbor Machine Translation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Nearest Neighbor Machine Translation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Khandelwal Urvashi, Fan Angela, Jurafsky Dan, Zettlemoyer Luke, Lewis Mike</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>130</td>
    <td><p>We introduce \(k\)-nearest-neighbor machine translation (\(k\)NN-MT), which
predicts tokens with a nearest neighbor classifier over a large datastore of
cached examples, using representations from a neural translation model for
similarity search. This approach requires no additional training and scales to
give the decoder direct access to billions of examples at test time, resulting
in a highly expressive model that consistently improves performance across many
settings. Simply adding nearest neighbor search improves a state-of-the-art
German-English translation model by 1.5 BLEU. \(k\)NN-MT allows a single model to
be adapted to diverse domains by using a domain-specific datastore, improving
results by an average of 9.2 BLEU over zero-shot transfer, and achieving new
state-of-the-art results – without training on these domains. A massively
multilingual model can also be specialized for particular language pairs, with
improvements of 3 BLEU for translating from English into German and Chinese.
Qualitatively, \(k\)NN-MT is easily interpretable; it combines source and target
context to retrieve highly relevant examples.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Few-Shot-&-Zero-Shot 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/jin2025deep/">Deep Saliency Hashing for Fine-grained Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Saliency Hashing for Fine-grained Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Saliency Hashing for Fine-grained Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jin Sheng, Yao, Sun, Zhou, Zhang, Hua</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>59</td>
    <td><p>In recent years, hashing methods have been proved to be
effective and efficient for the large-scale Web media search.
However, the existing general hashing methods have limited discriminative power for describing fine-grained objects that share similar overall appearance but have subtle
difference. To solve this problem, we for the first time introduce the attention mechanism to the learning of fine-grained
hashing codes. Specifically, we propose a novel deep hashing model, named deep saliency hashing (DSaH), which
automatically mines salient regions and learns semanticpreserving hashing codes simultaneously. DSaH is a twostep end-to-end model consisting of an attention network
and a hashing network. Our loss function contains three
basic components, including the semantic loss, the saliency
loss, and the quantization loss. As the core of DSaH, the
saliency loss guides the attention network to mine discriminative regions from pairs of images. We conduct extensive experiments on both fine-grained and general retrieval
datasets for performance evaluation. Experimental results
on fine grained dataset, including Oxford Flowers-17, Stanford Dogs-120 and CUB Bird demonstrate that our DSaH
performs the best for fine-grained retrieval task and beats
strongest competitor (DTQ) by approximately 10% on both
Stanford Dogs-120 and CUB Bird. DSaH is also comparable to several state-of-the-art hashing methods on general
datasets, including CIFAR-10 and NUS-WIDE.</p>
</td>
    <td>
      
        Scalability 
      
        Datasets 
      
        Neural-Hashing 
      
        Hashing-Methods 
      
        Evaluation 
      
        Quantization 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/lee2020metric/">Metric Learning vs Classification for Disentangled Music Representation Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Metric Learning vs Classification for Disentangled Music Representation Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Metric Learning vs Classification for Disentangled Music Representation Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lee Jongpil, Bryan Nicholas J., Salamon Justin, Jin Zeyu, Nam Juhan</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>11</td>
    <td><p>Deep representation learning offers a powerful paradigm for mapping input
data onto an organized embedding space and is useful for many music information
retrieval tasks. Two central methods for representation learning include deep
metric learning and classification, both having the same goal of learning a
representation that can generalize well across tasks. Along with
generalization, the emerging concept of disentangled representations is also of
great interest, where multiple semantic concepts (e.g., genre, mood,
instrumentation) are learned jointly but remain separable in the learned
representation space. In this paper we present a single representation learning
framework that elucidates the relationship between metric learning,
classification, and disentanglement in a holistic manner. For this, we (1)
outline past work on the relationship between metric learning and
classification, (2) extend this relationship to multi-label data by exploring
three different learning approaches and their disentangled versions, and (3)
evaluate all models on four tasks (training time, similarity retrieval,
auto-tagging, and triplet prediction). We find that classification-based models
are generally advantageous for training time, similarity retrieval, and
auto-tagging, while deep metric learning exhibits better performance for
triplet-prediction. Finally, we show that our proposed approach yields
state-of-the-art results for music auto-tagging.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Distance-Metric-Learning 
      
        Evaluation 
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/wang2025deep/">Deep Hashing with Active Pairwise Supervision</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Hashing with Active Pairwise Supervision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Hashing with Active Pairwise Supervision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Ziwei, Zheng, Lu, Zhou</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>9</td>
    <td><p>In this paper, we propose a Deep Hashing method with Active Pairwise Supervision(DH-APS). Conventional methods with passive
pairwise supervision obtain labeled data for training and require large
amount of annotations to reach their full potential, which are not feasible in realistic retrieval tasks. On the contrary, we actively select a small
quantity of informative samples for annotation to provide effective pairwise supervision so that discriminative hash codes can be obtained with
limited annotation budget. Specifically, we generalize the structural risk
minimization principle and obtain three criteria for the pairwise supervision acquisition: uncertainty, representativeness and diversity. Accordingly, samples involved in the following training pairs should be labeled:
pairs with most uncertain similarity, pairs that minimize the discrepancy
between labeled and unlabeled data, and pairs which are most different
from the annotated data, so that the discriminality and generalization ability of the learned hash codes are significantly strengthened. Moreover,
our DH-APS can also be employed as a plug-and-play module for semisupervised hashing methods to further enhance the performance. Experiments demonstrate that the presented DH-APS achieves the accuracy
of supervised hashing methods with only 30% labeled training samples
and improves the semi-supervised binary codes by a sizable margin.</p>
</td>
    <td>
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/wang2025online/">Online Collective Matrix Factorization Hashing for Large-Scale Cross-Media Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Online Collective Matrix Factorization Hashing for Large-Scale Cross-Media Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Online Collective Matrix Factorization Hashing for Large-Scale Cross-Media Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang di, Wang, An, Gao, Tian</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>47</td>
    <td><p>Cross-modal hashing has been widely investigated recently for its efficiency in large-scale cross-media retrieval. However, most existing cross-modal hashing methods learn hash functions in a batch-based learning mode. Such mode is not suitable for large-scale data sets due to the large memory consumption and loses its efficiency when training streaming data. Online cross-modal hashing can deal with the above problems by learning hash model in an online learning process. However, existing online cross-modal hashing methods cannot update hash codes of old data by the newly learned model. In this paper, we propose Online Collective Matrix Factorization Hashing (OCMFH) based on collective matrix factorization hashing (CMFH), which can adaptively update hash codes of old data according to dynamic changes of hash model without accessing to old data. Specifically, it learns discriminative hash codes for streaming data by collective matrix factorization in an online optimization scheme. Unlike conventional CMFH which needs to load the entire data points into memory, the proposed OCMFH retrains hash functions only by newly arriving data points. Meanwhile, it generates hash codes of new data and updates hash codes of old data by the latest updated hash model. In such way, hash codes of new data and old data are well-matched. Furthermore, a zero mean strategy is developed to solve the mean-varying problem in the online hash learning process. Extensive experiments on three benchmark data sets demonstrate the effectiveness and efficiency of OCMFH on online cross-media retrieval.</p>
</td>
    <td>
      
        Scalability 
      
        Efficiency 
      
        SIGIR 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/jiang2019graph/">Graph-based Multi-view Binary Learning for Image Clustering</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Graph-based Multi-view Binary Learning for Image Clustering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Graph-based Multi-view Binary Learning for Image Clustering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jiang Guangqi, Wang Huibing, Peng Jinjia, Chen Dongyan, Fu Xianping</td> <!-- 🔧 You were missing this -->
    <td>Neurocomputing</td>
    <td>28</td>
    <td><p>Hashing techniques, also known as binary code learning, have recently gained
increasing attention in large-scale data analysis and storage. Generally, most
existing hash clustering methods are single-view ones, which lack complete
structure or complementary information from multiple views. For cluster tasks,
abundant prior researches mainly focus on learning discrete hash code while few
works take original data structure into consideration. To address these
problems, we propose a novel binary code algorithm for clustering, which adopts
graph embedding to preserve the original data structure, called (Graph-based
Multi-view Binary Learning) GMBL in this paper. GMBL mainly focuses on encoding
the information of multiple views into a compact binary code, which explores
complementary information from multiple views. In particular, in order to
maintain the graph-based structure of the original data, we adopt a Laplacian
matrix to preserve the local linear relationship of the data and map it to the
Hamming space. Considering different views have distinctive contributions to
the final clustering results, GMBL adopts a strategy of automatically assign
weights for each view to better guide the clustering. Finally, An alternating
iterative optimization method is adopted to optimize discrete binary codes
directly instead of relaxing the binary constraint in two steps. Experiments on
five public datasets demonstrate the superiority of our proposed method
compared with previous approaches in terms of clustering performance.</p>
</td>
    <td>
      
        Graph-Based-ANN 
      
        Hashing-Methods 
      
        Datasets 
      
        Compact-Codes 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/jin2019deep/">Deep Semantic Multimodal Hashing Network for Scalable Image-Text and Video-Text Retrievals</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Semantic Multimodal Hashing Network for Scalable Image-Text and Video-Text Retrievals' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Semantic Multimodal Hashing Network for Scalable Image-Text and Video-Text Retrievals' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jin Lu, Li Zechao, Tang Jinhui</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Neural Networks and Learning Systems</td>
    <td>91</td>
    <td><p>Hashing has been widely applied to multimodal retrieval on large-scale
multimedia data due to its efficiency in computation and storage. In this
article, we propose a novel deep semantic multimodal hashing network (DSMHN)
for scalable image-text and video-text retrieval. The proposed deep hashing
framework leverages 2-D convolutional neural networks (CNN) as the backbone
network to capture the spatial information for image-text retrieval, while the
3-D CNN as the backbone network to capture the spatial and temporal information
for video-text retrieval. In the DSMHN, two sets of modality-specific hash
functions are jointly learned by explicitly preserving both intermodality
similarities and intramodality semantic labels. Specifically, with the
assumption that the learned hash codes should be optimal for the classification
task, two stream networks are jointly trained to learn the hash functions by
embedding the semantic labels on the resultant hash codes. Moreover, a unified
deep multimodal hashing framework is proposed to learn compact and high-quality
hash codes by exploiting the feature representation learning, intermodality
similarity-preserving learning, semantic label-preserving learning, and hash
function learning with different types of loss functions simultaneously. The
proposed DSMHN method is a generic and scalable deep hashing framework for both
image-text and video-text retrievals, which can be flexibly integrated with
different types of loss functions. We conduct extensive experiments for both
single modal- and cross-modal-retrieval tasks on four widely used
multimodal-retrieval data sets. Experimental results on both image-text- and
video-text-retrieval tasks demonstrate that the DSMHN significantly outperforms
the state-of-the-art methods.</p>
</td>
    <td>
      
        Text-Retrieval 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Neural-Hashing 
      
        Efficiency 
      
        Scalability 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/jin2020deep/">Deep Saliency Hashing for Fine-grained Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Saliency Hashing for Fine-grained Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Saliency Hashing for Fine-grained Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jin Sheng, Yao, Sun, Zhou, Zhang, Hua</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>59</td>
    <td><p>In recent years, hashing methods have been proved to be
effective and efficient for the large-scale Web media search.
However, the existing general hashing methods have limited discriminative power for describing fine-grained objects that share similar overall appearance but have subtle
difference. To solve this problem, we for the first time introduce the attention mechanism to the learning of fine-grained
hashing codes. Specifically, we propose a novel deep hashing model, named deep saliency hashing (DSaH), which
automatically mines salient regions and learns semanticpreserving hashing codes simultaneously. DSaH is a twostep end-to-end model consisting of an attention network
and a hashing network. Our loss function contains three
basic components, including the semantic loss, the saliency
loss, and the quantization loss. As the core of DSaH, the
saliency loss guides the attention network to mine discriminative regions from pairs of images. We conduct extensive experiments on both fine-grained and general retrieval
datasets for performance evaluation. Experimental results
on fine grained dataset, including Oxford Flowers-17, Stanford Dogs-120 and CUB Bird demonstrate that our DSaH
performs the best for fine-grained retrieval task and beats
strongest competitor (DTQ) by approximately 10% on both
Stanford Dogs-120 and CUB Bird. DSaH is also comparable to several state-of-the-art hashing methods on general
datasets, including CIFAR-10 and NUS-WIDE.</p>
</td>
    <td>
      
        Scalability 
      
        Datasets 
      
        Neural-Hashing 
      
        Hashing-Methods 
      
        Evaluation 
      
        Quantization 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/jang2020generalized/">Generalized Product Quantization Network for Semi-supervised Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Generalized Product Quantization Network for Semi-supervised Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Generalized Product Quantization Network for Semi-supervised Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jang Young Kyun, Cho Nam Ik</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>41</td>
    <td><p>Image retrieval methods that employ hashing or vector quantization have
achieved great success by taking advantage of deep learning. However, these
approaches do not meet expectations unless expensive label information is
sufficient. To resolve this issue, we propose the first quantization-based
semi-supervised image retrieval scheme: Generalized Product Quantization (GPQ)
network. We design a novel metric learning strategy that preserves semantic
similarity between labeled data, and employ entropy regularization term to
fully exploit inherent potentials of unlabeled data. Our solution increases the
generalization capacity of the quantization network, which allows overcoming
previous limitations in the retrieval community. Extensive experimental results
demonstrate that GPQ yields state-of-the-art performance on large-scale real
image benchmark datasets.</p>
</td>
    <td>
      
        Supervised 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Quantization 
      
        CVPR 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/ji2018attribute/">Attribute-Guided Network for Cross-Modal Zero-Shot Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Attribute-Guided Network for Cross-Modal Zero-Shot Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Attribute-Guided Network for Cross-Modal Zero-Shot Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ji Zhong, Sun Yuxin, Yu Yunlong, Pang Yanwei, Han Jungong</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Neural Networks and Learning Systems</td>
    <td>83</td>
    <td><p>Zero-Shot Hashing aims at learning a hashing model that is trained only by
instances from seen categories but can generate well to those of unseen
categories. Typically, it is achieved by utilizing a semantic embedding space
to transfer knowledge from seen domain to unseen domain. Existing efforts
mainly focus on single-modal retrieval task, especially Image-Based Image
Retrieval (IBIR). However, as a highlighted research topic in the field of
hashing, cross-modal retrieval is more common in real world applications. To
address the Cross-Modal Zero-Shot Hashing (CMZSH) retrieval task, we propose a
novel Attribute-Guided Network (AgNet), which can perform not only IBIR, but
also Text-Based Image Retrieval (TBIR). In particular, AgNet aligns different
modal data into a semantically rich attribute space, which bridges the gap
caused by modality heterogeneity and zero-shot setting. We also design an
effective strategy that exploits the attribute to guide the generation of hash
codes for image and text within the same network. Extensive experimental
results on three benchmark datasets (AwA, SUN, and ImageNet) demonstrate the
superiority of AgNet on both cross-modal and single-modal zero-shot image
retrieval tasks.</p>
</td>
    <td>
      
        Few-Shot-&-Zero-Shot 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Evaluation 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/wang2020online/">Online Collective Matrix Factorization Hashing for Large-Scale Cross-Media Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Online Collective Matrix Factorization Hashing for Large-Scale Cross-Media Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Online Collective Matrix Factorization Hashing for Large-Scale Cross-Media Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang di, Wang, An, Gao, Tian</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>47</td>
    <td><p>Cross-modal hashing has been widely investigated recently for its efficiency in large-scale cross-media retrieval. However, most existing cross-modal hashing methods learn hash functions in a batch-based learning mode. Such mode is not suitable for large-scale data sets due to the large memory consumption and loses its efficiency when training streaming data. Online cross-modal hashing can deal with the above problems by learning hash model in an online learning process. However, existing online cross-modal hashing methods cannot update hash codes of old data by the newly learned model. In this paper, we propose Online Collective Matrix Factorization Hashing (OCMFH) based on collective matrix factorization hashing (CMFH), which can adaptively update hash codes of old data according to dynamic changes of hash model without accessing to old data. Specifically, it learns discriminative hash codes for streaming data by collective matrix factorization in an online optimization scheme. Unlike conventional CMFH which needs to load the entire data points into memory, the proposed OCMFH retrains hash functions only by newly arriving data points. Meanwhile, it generates hash codes of new data and updates hash codes of old data by the latest updated hash model. In such way, hash codes of new data and old data are well-matched. Furthermore, a zero mean strategy is developed to solve the mean-varying problem in the online hash learning process. Extensive experiments on three benchmark data sets demonstrate the effectiveness and efficiency of OCMFH on online cross-media retrieval.</p>
</td>
    <td>
      
        Scalability 
      
        Efficiency 
      
        SIGIR 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/wang2020faster/">Faster Person Re-Identification</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Faster Person Re-Identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Faster Person Re-Identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Guan'an, Gong Shaogang, Cheng Jian, Hou Zengguang</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>54</td>
    <td><p>Fast person re-identification (ReID) aims to search person images quickly and
accurately. The main idea of recent fast ReID methods is the hashing algorithm,
which learns compact binary codes and performs fast Hamming distance and
counting sort. However, a very long code is needed for high accuracy (e.g.
2048), which compromises search speed. In this work, we introduce a new
solution for fast ReID by formulating a novel Coarse-to-Fine (CtF) hashing code
search strategy, which complementarily uses short and long codes, achieving
both faster speed and better accuracy. It uses shorter codes to coarsely rank
broad matching similarities and longer codes to refine only a few top
candidates for more accurate instance ReID. Specifically, we design an
All-in-One (AiO) framework together with a Distance Threshold Optimization
(DTO) algorithm. In AiO, we simultaneously learn and enhance multiple codes of
different lengths in a single model. It learns multiple codes in a pyramid
structure, and encourage shorter codes to mimic longer codes by
self-distillation. DTO solves a complex threshold search problem by a simple
optimization process, and the balance between accuracy and speed is easily
controlled by a single parameter. It formulates the optimization target as a
\(F_{\beta}\) score that can be optimised by Gaussian cumulative distribution
functions. Experimental results on 2 datasets show that our proposed method
(CtF) is not only 8% more accurate but also 5x faster than contemporary hashing
ReID methods. Compared with non-hashing ReID methods, CtF is \(50\times\) faster
with comparable accuracy. Code is available at
https://github.com/wangguanan/light-reid.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        Compact-Codes 
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/hoang2020unsupervised/">Unsupervised Deep Cross-modality Spectral Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Deep Cross-modality Spectral Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Deep Cross-modality Spectral Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hoang Tuan, Do Thanh-toan, Nguyen Tam V., Cheung Ngai-man</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>28</td>
    <td><p>This paper presents a novel framework, namely Deep Cross-modality Spectral
Hashing (DCSH), to tackle the unsupervised learning problem of binary hash
codes for efficient cross-modal retrieval. The framework is a two-step hashing
approach which decouples the optimization into (1) binary optimization and (2)
hashing function learning. In the first step, we propose a novel spectral
embedding-based algorithm to simultaneously learn single-modality and binary
cross-modality representations. While the former is capable of well preserving
the local structure of each modality, the latter reveals the hidden patterns
from all modalities. In the second step, to learn mapping functions from
informative data inputs (images and word embeddings) to binary codes obtained
from the first step, we leverage the powerful CNN for images and propose a
CNN-based deep architecture to learn text modality. Quantitative evaluations on
three standard benchmark datasets demonstrate that the proposed DCSH method
consistently outperforms other state-of-the-art methods.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Datasets 
      
        Compact-Codes 
      
        Unsupervised 
      
        Evaluation 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/henkel2020supporting/">Supporting large-scale image recognition with out-of-domain samples</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Supporting large-scale image recognition with out-of-domain samples' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Supporting large-scale image recognition with out-of-domain samples' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Henkel Christof, Singer Philipp</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>This article presents an efficient end-to-end method to perform
instance-level recognition employed to the task of labeling and ranking
landmark images. In a first step, we embed images in a high dimensional feature
space using convolutional neural networks trained with an additive angular
margin loss and classify images using visual similarity. We then efficiently
re-rank predictions and filter noise utilizing similarity to out-of-domain
images. Using this approach we achieved the 1st place in the 2020 edition of
the Google Landmark Recognition challenge.</p>
</td>
    <td>
      
        Scalability 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/zhou2020ladder/">Ladder Loss for Coherent Visual-Semantic Embedding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Ladder Loss for Coherent Visual-Semantic Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Ladder Loss for Coherent Visual-Semantic Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhou Mo, Niu Zhenxing, Wang Le, Gao Zhanning, Zhang Qilin, Hua Gang</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>27</td>
    <td><p>For visual-semantic embedding, the existing methods normally treat the
relevance between queries and candidates in a bipolar way – relevant or
irrelevant, and all “irrelevant” candidates are uniformly pushed away from the
query by an equal margin in the embedding space, regardless of their various
proximity to the query. This practice disregards relatively discriminative
information and could lead to suboptimal ranking in the retrieval results and
poorer user experience, especially in the long-tail query scenario where a
matching candidate may not necessarily exist. In this paper, we introduce a
continuous variable to model the relevance degree between queries and multiple
candidates, and propose to learn a coherent embedding space, where candidates
with higher relevance degrees are mapped closer to the query than those with
lower relevance degrees. In particular, the new ladder loss is proposed by
extending the triplet loss inequality to a more general inequality chain, which
implements variable push-away margins according to respective relevance
degrees. In addition, a proper Coherent Score metric is proposed to better
measure the ranking results including those “irrelevant” candidates. Extensive
experiments on multiple datasets validate the efficacy of our proposed method,
which achieves significant improvement over existing state-of-the-art methods.</p>
</td>
    <td>
      
        AAAI 
      
        Distance-Metric-Learning 
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/hansen2020unsupervised/">Unsupervised Semantic Hashing with Pairwise Reconstruction</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Semantic Hashing with Pairwise Reconstruction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Semantic Hashing with Pairwise Reconstruction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hansen Casper, Hansen Christian, Simonsen Jakob Grue, Alstrup Stephen, Lioma Christina</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>22</td>
    <td><p>Semantic Hashing is a popular family of methods for efficient similarity
search in large-scale datasets. In Semantic Hashing, documents are encoded as
short binary vectors (i.e., hash codes), such that semantic similarity can be
efficiently computed using the Hamming distance. Recent state-of-the-art
approaches have utilized weak supervision to train better performing hashing
models. Inspired by this, we present Semantic Hashing with Pairwise
Reconstruction (PairRec), which is a discrete variational autoencoder based
hashing model. PairRec first encodes weakly supervised training pairs (a query
document and a semantically similar document) into two hash codes, and then
learns to reconstruct the same query document from both of these hash codes
(i.e., pairwise reconstruction). This pairwise reconstruction enables our model
to encode local neighbourhood structures within the hash code directly through
the decoder. We experimentally compare PairRec to traditional and
state-of-the-art approaches, and obtain significant performance improvements in
the task of document similarity search.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Supervised 
      
        Text-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        SIGIR 
      
        Unsupervised 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/hansen2020content/">Content-aware Neural Hashing for Cold-start Recommendation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Content-aware Neural Hashing for Cold-start Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Content-aware Neural Hashing for Cold-start Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hansen Casper, Hansen, Simonsen, Alstrup, Lioma</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>36</td>
    <td><p>Content-aware recommendation approaches are essential for providing meaningful recommendations for new (i.e., cold-start) items in a recommender system. We present a content-aware neural hashing-based collaborative filtering approach (NeuHash-CF), which generates binary hash codes for users and items, such that the highly efficient Hamming distance can be used for estimating user-item relevance. NeuHash-CF is modelled as an autoencoder architecture, consisting of two joint hashing components for generating user and item hash codes. Inspired from semantic hashing, the item hashing component generates a hash code directly from an item’s content information (i.e., it generates cold-start and seen item hash codes in the same manner). This contrasts existing state-of-the-art models, which treat the two item cases separately. The user hash codes are generated directly based on user id, through learning a user embedding matrix. We show experimentally that NeuHash-CF significantly outperforms state-of-the-art baselines by up to 12% NDCG and 13% MRR in cold-start recommendation settings, and up to 4% in both NDCG and MRR in standard settings where all items are present while training. Our approach uses 2-4x shorter hash codes, while obtaining the same or better performance compared to the state of the art, thus consequently also enabling a notable storage reduction.</p>
</td>
    <td>
      
        Recommender-Systems 
      
        Neural-Hashing 
      
        Text-Retrieval 
      
        SIGIR 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/hansen2025content/">Content-aware Neural Hashing for Cold-start Recommendation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Content-aware Neural Hashing for Cold-start Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Content-aware Neural Hashing for Cold-start Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hansen Casper, Hansen, Simonsen, Alstrup, Lioma</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>36</td>
    <td><p>Content-aware recommendation approaches are essential for providing meaningful recommendations for new (i.e., cold-start) items in a recommender system. We present a content-aware neural hashing-based collaborative filtering approach (NeuHash-CF), which generates binary hash codes for users and items, such that the highly efficient Hamming distance can be used for estimating user-item relevance. NeuHash-CF is modelled as an autoencoder architecture, consisting of two joint hashing components for generating user and item hash codes. Inspired from semantic hashing, the item hashing component generates a hash code directly from an item’s content information (i.e., it generates cold-start and seen item hash codes in the same manner). This contrasts existing state-of-the-art models, which treat the two item cases separately. The user hash codes are generated directly based on user id, through learning a user embedding matrix. We show experimentally that NeuHash-CF significantly outperforms state-of-the-art baselines by up to 12% NDCG and 13% MRR in cold-start recommendation settings, and up to 4% in both NDCG and MRR in standard settings where all items are present while training. Our approach uses 2-4x shorter hash codes, while obtaining the same or better performance compared to the state of the art, thus consequently also enabling a notable storage reduction.</p>
</td>
    <td>
      
        Recommender-Systems 
      
        Neural-Hashing 
      
        Text-Retrieval 
      
        SIGIR 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/zheng2020generative/">Generative Semantic Hashing Enhanced via Boltzmann Machines</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Generative Semantic Hashing Enhanced via Boltzmann Machines' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Generative Semantic Hashing Enhanced via Boltzmann Machines' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zheng Lin, Su Qinliang, Shen Dinghan, Chen Changyou</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</td>
    <td>10</td>
    <td><p>Generative semantic hashing is a promising technique for large-scale
information retrieval thanks to its fast retrieval speed and small memory
footprint. For the tractability of training, existing generative-hashing
methods mostly assume a factorized form for the posterior distribution,
enforcing independence among the bits of hash codes. From the perspectives of
both model representation and code space size, independence is always not the
best assumption. In this paper, to introduce correlations among the bits of
hash codes, we propose to employ the distribution of Boltzmann machine as the
variational posterior. To address the intractability issue of training, we
first develop an approximate method to reparameterize the distribution of a
Boltzmann machine by augmenting it as a hierarchical concatenation of a
Gaussian-like distribution and a Bernoulli distribution. Based on that, an
asymptotically-exact lower bound is further derived for the evidence lower
bound (ELBO). With these novel techniques, the entire model can be optimized
efficiently. Extensive experimental results demonstrate that by effectively
modeling correlations among different bits within a hash code, our model can
achieve significant performance gains.</p>
</td>
    <td>
      
        Text-Retrieval 
      
        Hashing-Methods 
      
        Scalability 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/gu2020symmetrical/">Symmetrical Synthesis for Deep Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Symmetrical Synthesis for Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Symmetrical Synthesis for Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gu Geonmo, Ko Byungsoo</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>21</td>
    <td><p>Deep metric learning aims to learn embeddings that contain semantic
similarity information among data points. To learn better embeddings, methods
to generate synthetic hard samples have been proposed. Existing methods of
synthetic hard sample generation are adopting autoencoders or generative
adversarial networks, but this leads to more hyper-parameters, harder
optimization, and slower training speed. In this paper, we address these
problems by proposing a novel method of synthetic hard sample generation called
symmetrical synthesis. Given two original feature points from the same class,
the proposed method firstly generates synthetic points with each other as an
axis of symmetry. Secondly, it performs hard negative pair mining within the
original and synthetic points to select a more informative negative pair for
computing the metric learning loss. Our proposed method is hyper-parameter free
and plug-and-play for existing metric learning losses without network
modification. We demonstrate the superiority of our proposed method over
existing methods for a variety of loss functions on clustering and image
retrieval tasks. Our implementations is publicly available.</p>
</td>
    <td>
      
        AAAI 
      
        Distance-Metric-Learning 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/gerritse2025graph/">Graph-Embedding Empowered Entity Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Graph-Embedding Empowered Entity Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Graph-Embedding Empowered Entity Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gerritse Emma J., Hasibi Faegheh, de Vries Arjen P.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>25</td>
    <td><p>In this research, we investigate methods for entity retrieval using graph embeddings. While various methods have been proposed over the years, most utilize a single graph embedding and entity linking approach. This hinders our understanding of how different graph embedding and entity linking methods impact entity retrieval. To address this gap, we investigate the effects of three different categories of graph embedding techniques and five different entity linking methods. We perform a reranking of entities using the distance between the embeddings of annotated entities and the entities we wish to rerank. We conclude that the selection of both graph embeddings and entity linkers significantly impacts the effectiveness of entity retrieval. For graph embeddings, methods that incorporate both graph structure and textual descriptions of entities are the most effective. For entity linking, both precision and recall concerning concepts are important for optimal retrieval performance. Additionally, it is essential for the graph to encompass as many entities as possible.</p>
</td>
    <td>
      
        Evaluation 
      
        Re-Ranking 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/gerritse2020graph/">Graph-Embedding Empowered Entity Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Graph-Embedding Empowered Entity Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Graph-Embedding Empowered Entity Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gerritse Emma J., Hasibi Faegheh, de Vries Arjen P.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>25</td>
    <td><p>In this research, we improve upon the current state of the art in entity
retrieval by re-ranking the result list using graph embeddings. The paper shows
that graph embeddings are useful for entity-oriented search tasks. We
demonstrate empirically that encoding information from the knowledge graph into
(graph) embeddings contributes to a higher increase in effectiveness of entity
retrieval results than using plain word embeddings. We analyze the impact of
the accuracy of the entity linker on the overall retrieval effectiveness. Our
analysis further deploys the cluster hypothesis to explain the observed
advantages of graph embeddings over the more widely used word embeddings, for
user tasks involving ranking entities.</p>
</td>
    <td>
      
        Hybrid-ANN-Methods 
      
        Re-Ranking 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/ge2020self/">Self-supervising Fine-grained Region Similarities for Large-scale Image Localization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Self-supervising Fine-grained Region Similarities for Large-scale Image Localization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Self-supervising Fine-grained Region Similarities for Large-scale Image Localization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ge Yixiao, Wang Haibo, Zhu Feng, Zhao Rui, Li Hongsheng</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>107</td>
    <td><p>The task of large-scale retrieval-based image localization is to estimate the
geographical location of a query image by recognizing its nearest reference
images from a city-scale dataset. However, the general public benchmarks only
provide noisy GPS labels associated with the training images, which act as weak
supervisions for learning image-to-image similarities. Such label noise
prevents deep neural networks from learning discriminative features for
accurate localization. To tackle this challenge, we propose to self-supervise
image-to-region similarities in order to fully explore the potential of
difficult positive images alongside their sub-regions. The estimated
image-to-region similarities can serve as extra training supervision for
improving the network in generations, which could in turn gradually refine the
fine-grained similarities to achieve optimal performance. Our proposed
self-enhanced image-to-region similarity labels effectively deal with the
training bottleneck in the state-of-the-art pipelines without any additional
parameters or manual annotations in both training and inference. Our method
outperforms state-of-the-arts on the standard localization benchmarks by
noticeable margins and shows excellent generalization capability on multiple
image retrieval datasets.</p>
</td>
    <td>
      
        Datasets 
      
        Scalability 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/weng2025online/">Online Hashing with Efficient Updating of Binary Codes</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Online Hashing with Efficient Updating of Binary Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Online Hashing with Efficient Updating of Binary Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Weng Zhenyu, Zhu</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>15</td>
    <td><p>Online hashing methods are efficient in learning the hash functions from the streaming data. However, when the hash functions change, the binary codes for the database have to be recomputed to guarantee the retrieval accuracy. Recomputing the binary codes by accumulating the whole database brings a timeliness challenge to the online retrieval process. In this paper, we propose a novel online hashing framework to update the binary codes efficiently without accumulating the whole database. In our framework, the hash functions are fixed and the projection functions are introduced to learn online from the streaming data. Therefore, inefficient updating of the binary codes by accumulating the whole database can be transformed to efficient updating of the binary codes by projecting the binary codes into another binary space. The queries and the binary code database are projected asymmetrically to further improve the retrieval accuracy. The experiments on two multi-label image databases demonstrate the effectiveness and the efficiency of our method for multi-label image retrieval.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Efficiency 
      
        Tools-&-Libraries 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/gao2020complementing/">Complementing Lexical Retrieval with Semantic Residual Embedding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Complementing Lexical Retrieval with Semantic Residual Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Complementing Lexical Retrieval with Semantic Residual Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gao Luyu, Dai Zhuyun, Chen Tongfei, Fan Zhen, van Durme Benjamin, Callan Jamie</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>61</td>
    <td><p>This paper presents CLEAR, a retrieval model that seeks to complement
classical lexical exact-match models such as BM25 with semantic matching
signals from a neural embedding matching model. CLEAR explicitly trains the
neural embedding to encode language structures and semantics that lexical
retrieval fails to capture with a novel residual-based embedding learning
method. Empirical evaluations demonstrate the advantages of CLEAR over
state-of-the-art retrieval models, and that it can substantially improve the
end-to-end accuracy and efficiency of reranking pipelines.</p>
</td>
    <td>
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/zhou2019ladder/">Ladder Loss for Coherent Visual-Semantic Embedding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Ladder Loss for Coherent Visual-Semantic Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Ladder Loss for Coherent Visual-Semantic Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhou Mo, Niu Zhenxing, Wang Le, Gao Zhanning, Zhang Qilin, Hua Gang</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>27</td>
    <td><p>For visual-semantic embedding, the existing methods normally treat the
relevance between queries and candidates in a bipolar way – relevant or
irrelevant, and all “irrelevant” candidates are uniformly pushed away from the
query by an equal margin in the embedding space, regardless of their various
proximity to the query. This practice disregards relatively discriminative
information and could lead to suboptimal ranking in the retrieval results and
poorer user experience, especially in the long-tail query scenario where a
matching candidate may not necessarily exist. In this paper, we introduce a
continuous variable to model the relevance degree between queries and multiple
candidates, and propose to learn a coherent embedding space, where candidates
with higher relevance degrees are mapped closer to the query than those with
lower relevance degrees. In particular, the new ladder loss is proposed by
extending the triplet loss inequality to a more general inequality chain, which
implements variable push-away margins according to respective relevance
degrees. In addition, a proper Coherent Score metric is proposed to better
measure the ranking results including those “irrelevant” candidates. Extensive
experiments on multiple datasets validate the efficacy of our proposed method,
which achieves significant improvement over existing state-of-the-art methods.</p>
</td>
    <td>
      
        AAAI 
      
        Distance-Metric-Learning 
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/frady2020neuromorphic/">Neuromorphic Nearest-Neighbor Search Using Intel's Pohoiki Springs</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Neuromorphic Nearest-Neighbor Search Using Intel's Pohoiki Springs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Neuromorphic Nearest-Neighbor Search Using Intel's Pohoiki Springs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Frady E. Paxon, Orchard Garrick, Florey David, Imam Nabil, Liu Ruokun, Mishra Joyesh, Tse Jonathan, Wild Andreas, Sommer Friedrich T., Davies Mike</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Neuro-inspired Computational Elements Workshop</td>
    <td>47</td>
    <td><p>Neuromorphic computing applies insights from neuroscience to uncover
innovations in computing technology. In the brain, billions of interconnected
neurons perform rapid computations at extremely low energy levels by leveraging
properties that are foreign to conventional computing systems, such as temporal
spiking codes and finely parallelized processing units integrating both memory
and computation. Here, we showcase the Pohoiki Springs neuromorphic system, a
mesh of 768 interconnected Loihi chips that collectively implement 100 million
spiking neurons in silicon. We demonstrate a scalable approximate k-nearest
neighbor (k-NN) algorithm for searching large databases that exploits
neuromorphic principles. Compared to state-of-the-art conventional CPU-based
implementations, we achieve superior latency, index build time, and energy
efficiency when evaluated on several standard datasets containing over 1
million high-dimensional patterns. Further, the system supports adding new data
points to the indexed database online in O(1) time unlike all but brute force
conventional k-NN implementations.</p>
</td>
    <td>
      
        Datasets 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/feng2020unifying/">Unifying Specialist Image Embedding into Universal Image Embedding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unifying Specialist Image Embedding into Universal Image Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unifying Specialist Image Embedding into Universal Image Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Feng Yang, Peng Futang, Zhang Xu, Zhu Wei, Zhang Shanfeng, Zhou Howard, Li Zhen, Duerig Tom, Chang Shih-fu, Luo Jiebo</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>Deep image embedding provides a way to measure the semantic similarity of two
images. It plays a central role in many applications such as image search, face
verification, and zero-shot learning. It is desirable to have a universal deep
embedding model applicable to various domains of images. However, existing
methods mainly rely on training specialist embedding models each of which is
applicable to images from a single domain. In this paper, we study an important
but unexplored task: how to train a single universal image embedding model to
match the performance of several specialists on each specialist’s domain.
Simply fusing the training data from multiple domains cannot solve this problem
because some domains become overfitted sooner when trained together using
existing methods. Therefore, we propose to distill the knowledge in multiple
specialists into a universal embedding to solve this problem. In contrast to
existing embedding distillation methods that distill the absolute distances
between images, we transform the absolute distances between images into a
probabilistic distribution and minimize the KL-divergence between the
distributions of the specialists and the universal embedding. Using several
public datasets, we validate that our proposed method accomplishes the goal of
universal image embedding.</p>
</td>
    <td>
      
        Datasets 
      
        Few-Shot-&-Zero-Shot 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/feng2020adversarial/">Adversarial Attack on Deep Product Quantization Network for Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Adversarial Attack on Deep Product Quantization Network for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Adversarial Attack on Deep Product Quantization Network for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Feng Yan, Chen Bin, Dai Tao, Xia Shutao</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>43</td>
    <td><p>Deep product quantization network (DPQN) has recently received much attention
in fast image retrieval tasks due to its efficiency of encoding
high-dimensional visual features especially when dealing with large-scale
datasets. Recent studies show that deep neural networks (DNNs) are vulnerable
to input with small and maliciously designed perturbations (a.k.a., adversarial
examples). This phenomenon raises the concern of security issues for DPQN in
the testing/deploying stage as well. However, little effort has been devoted to
investigating how adversarial examples affect DPQN. To this end, we propose
product quantization adversarial generation (PQ-AG), a simple yet effective
method to generate adversarial examples for product quantization based
retrieval systems. PQ-AG aims to generate imperceptible adversarial
perturbations for query images to form adversarial queries, whose nearest
neighbors from a targeted product quantizaiton model are not semantically
related to those from the original queries. Extensive experiments show that our
PQ-AQ successfully creates adversarial examples to mislead targeted product
quantization retrieval models. Besides, we found that our PQ-AG significantly
degrades retrieval performance in both white-box and black-box settings.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Datasets 
      
        Efficiency 
      
        AAAI 
      
        Scalability 
      
        Quantization 
      
        Evaluation 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/fang2020attention/">Attention-based Saliency Hashing for Ophthalmic Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Attention-based Saliency Hashing for Ophthalmic Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Attention-based Saliency Hashing for Ophthalmic Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fang Jiansheng, Xu Yanwu, Zhang Xiaoqing, Hu Yan, Liu Jiang</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</td>
    <td>12</td>
    <td><p>Deep hashing methods have been proved to be effective for the large-scale
medical image search assisting reference-based diagnosis for clinicians.
However, when the salient region plays a maximal discriminative role in
ophthalmic image, existing deep hashing methods do not fully exploit the
learning ability of the deep network to capture the features of salient regions
pointedly. The different grades or classes of ophthalmic images may be share
similar overall performance but have subtle differences that can be
differentiated by mining salient regions. To address this issue, we propose a
novel end-to-end network, named Attention-based Saliency Hashing (ASH), for
learning compact hash-code to represent ophthalmic images. ASH embeds a
spatial-attention module to focus more on the representation of salient regions
and highlights their essential role in differentiating ophthalmic images.
Benefiting from the spatial-attention module, the information of salient
regions can be mapped into the hash-code for similarity calculation. In the
training stage, we input the image pairs to share the weights of the network,
and a pairwise loss is designed to maximize the discriminability of the
hash-code. In the retrieval stage, ASH obtains the hash-code by inputting an
image with an end-to-end manner, then the hash-code is used to similarity
calculation to return the most similar images. Extensive experiments on two
different modalities of ophthalmic image datasets demonstrate that the proposed
ASH can further improve the retrieval performance compared to the
state-of-the-art deep hashing methods due to the huge contributions of the
spatial-attention module.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/forcen2020co/">Co-occurrence of deep convolutional features for image search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Co-occurrence of deep convolutional features for image search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Co-occurrence of deep convolutional features for image search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Forcen J. I., Pagola Miguel, Barrenechea Edurne, Bustince Humberto</td> <!-- 🔧 You were missing this -->
    <td>Image and Vision Computing</td>
    <td>19</td>
    <td><p>Image search can be tackled using deep features from pre-trained
Convolutional Neural Networks (CNN). The feature map from the last
convolutional layer of a CNN encodes descriptive information from which a
discriminative global descriptor can be obtained. We propose a new
representation of co-occurrences from deep convolutional features to extract
additional relevant information from this last convolutional layer. Combining
this co-occurrence map with the feature map, we achieve an improved image
representation. We present two different methods to get the co-occurrence
representation, the first one based on direct aggregation of activations, and
the second one, based on a trainable co-occurrence representation. The image
descriptors derived from our methodology improve the performance in very
well-known image retrieval datasets as we prove in the experiments.</p>
</td>
    <td>
      
        Datasets 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/fu2020hard/">Hard Example Generation by Texture Synthesis for Cross-domain Shape Similarity Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hard Example Generation by Texture Synthesis for Cross-domain Shape Similarity Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hard Example Generation by Texture Synthesis for Cross-domain Shape Similarity Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fu Huan, Li Shunming, Jia Rongfei, Gong Mingming, Zhao Binqiang, Tao Dacheng</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>6</td>
    <td><p>Image-based 3D shape retrieval (IBSR) aims to find the corresponding 3D shape
of a given 2D image from a large 3D shape database. The common routine is to
map 2D images and 3D shapes into an embedding space and define (or learn) a
shape similarity measure. While metric learning with some adaptation techniques
seems to be a natural solution to shape similarity learning, the performance is
often unsatisfactory for fine-grained shape retrieval. In the paper, we
identify the source of the poor performance and propose a practical solution to
this problem. We find that the shape difference between a negative pair is
entangled with the texture gap, making metric learning ineffective in pushing
away negative pairs. To tackle this issue, we develop a geometry-focused
multi-view metric learning framework empowered by texture synthesis. The
synthesis of textures for 3D shape models creates hard triplets, which suppress
the adverse effects of rich texture in 2D images, thereby push the network to
focus more on discovering geometric characteristics. Our approach shows
state-of-the-art performance on a recently released large-scale 3D-FUTURE[1]
repository, as well as three widely studied benchmarks, including Pix3D[2],
Stanford Cars[3], and Comp Cars[4]. Codes will be made publicly available at:
https://github.com/3D-FRONT-FUTURE/IBSR-texture</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Scalability 
      
        Evaluation 
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/weng2019online/">Online Hashing with Efficient Updating of Binary Codes</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Online Hashing with Efficient Updating of Binary Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Online Hashing with Efficient Updating of Binary Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Weng Zhenyu, Zhu Yuesheng</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>15</td>
    <td><p>Online hashing methods are efficient in learning the hash functions from the
streaming data. However, when the hash functions change, the binary codes for
the database have to be recomputed to guarantee the retrieval accuracy.
Recomputing the binary codes by accumulating the whole database brings a
timeliness challenge to the online retrieval process. In this paper, we propose
a novel online hashing framework to update the binary codes efficiently without
accumulating the whole database. In our framework, the hash functions are fixed
and the projection functions are introduced to learn online from the streaming
data. Therefore, inefficient updating of the binary codes by accumulating the
whole database can be transformed to efficient updating of the binary codes by
projecting the binary codes into another binary space. The queries and the
binary code database are projected asymmetrically to further improve the
retrieval accuracy. The experiments on two multi-label image databases
demonstrate the effectiveness and the efficiency of our method for multi-label
image retrieval.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Compact-Codes 
      
        AAAI 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/xuan2019improved/">Improved Embeddings with Easy Positive Triplet Mining</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Improved Embeddings with Easy Positive Triplet Mining' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Improved Embeddings with Easy Positive Triplet Mining' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xuan Hong, Stylianou Abby, Pless Robert</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>131</td>
    <td><p>Deep metric learning seeks to define an embedding where semantically similar
images are embedded to nearby locations, and semantically dissimilar images are
embedded to distant locations. Substantial work has focused on loss functions
and strategies to learn these embeddings by pushing images from the same class
as close together in the embedding space as possible. In this paper, we propose
an alternative, loosened embedding strategy that requires the embedding
function only map each training image to the most similar examples from the
same class, an approach we call “Easy Positive” mining. We provide a collection
of experiments and visualizations that highlight that this Easy Positive mining
leads to embeddings that are more flexible and generalize better to new unseen
data. This simple mining strategy yields recall performance that exceeds state
of the art approaches (including those with complicated loss functions and
ensemble methods) on image retrieval datasets including CUB, Stanford Online
Products, In-Shop Clothes and Hotels-50K.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/ertl2019probminhash/">ProbMinHash -- A Class of Locality-Sensitive Hash Algorithms for the (Probability) Jaccard Similarity</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=ProbMinHash -- A Class of Locality-Sensitive Hash Algorithms for the (Probability) Jaccard Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=ProbMinHash -- A Class of Locality-Sensitive Hash Algorithms for the (Probability) Jaccard Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ertl Otmar</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Knowledge and Data Engineering</td>
    <td>23</td>
    <td><p>The probability Jaccard similarity was recently proposed as a natural
generalization of the Jaccard similarity to measure the proximity of sets whose
elements are associated with relative frequencies or probabilities. In
combination with a hash algorithm that maps those weighted sets to compact
signatures which allow fast estimation of pairwise similarities, it constitutes
a valuable method for big data applications such as near-duplicate detection,
nearest neighbor search, or clustering. This paper introduces a class of
one-pass locality-sensitive hash algorithms that are orders of magnitude faster
than the original approach. The performance gain is achieved by calculating
signature components not independently, but collectively. Four different
algorithms are proposed based on this idea. Two of them are statistically
equivalent to the original approach and can be used as drop-in replacements.
The other two may even improve the estimation error by introducing statistical
dependence between signature components. Moreover, the presented techniques can
be specialized for the conventional Jaccard similarity, resulting in highly
efficient algorithms that outperform traditional minwise hashing and that are
able to compete with the state of the art.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/elezi2019group/">The Group Loss for Deep Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=The Group Loss for Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=The Group Loss for Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Elezi Ismail, Vascon Sebastiano, Torcinovich Alessandro, Pelillo Marcello, Leal-taixe Laura</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>48</td>
    <td><p>Deep metric learning has yielded impressive results in tasks such as
clustering and image retrieval by leveraging neural networks to obtain highly
discriminative feature embeddings, which can be used to group samples into
different classes. Much research has been devoted to the design of smart loss
functions or data mining strategies for training such networks. Most methods
consider only pairs or triplets of samples within a mini-batch to compute the
loss function, which is commonly based on the distance between embeddings. We
propose Group Loss, a loss function based on a differentiable label-propagation
method that enforces embedding similarity across all samples of a group while
promoting, at the same time, low-density regions amongst data points belonging
to different groups. Guided by the smoothness assumption that “similar objects
should belong to the same group”, the proposed loss trains the neural network
for a classification task, enforcing a consistent labelling amongst samples
within a class. We show state-of-the-art results on clustering and image
retrieval on several datasets, and show the potential of our method when
combined with other techniques such as ensembles</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/fan2020deep/">Deep Polarized Network for Supervised Learning of Accurate Binary Hashing Codes</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Polarized Network for Supervised Learning of Accurate Binary Hashing Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Polarized Network for Supervised Learning of Accurate Binary Hashing Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fan Lixin, Ng, Ju, Zhang, Chan</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</td>
    <td>81</td>
    <td><p>This paper proposes a novel deep polarized network (DPN) for learning to hash, in which each channel in the network outputs is pushed far away
from zero by employing a differentiable bit-wise hinge-like loss which is dubbed as polarization loss. Reformulated within a generic Hamming Distance Metric Learning framework [Norouzi et al.,
2012], the proposed polarization loss bypasses the requirement to prepare pairwise labels for (dis-)similar items and, yet, the proposed loss strictly bounds from above the pairwise Hamming Distance based losses. The intrinsic connection between pairwise and pointwise label information, as
disclosed in this paper, brings about the following methodological improvements: (a) we may directly employ the proposed differentiable polarization loss with no large deviations incurred from
the target Hamming distance based loss; and (b) the subtask of assigning binary codes becomes extremely simple — even random codes assigned to each class suffice to result in state-of-the-art performances, as demonstrated in CIFAR10, NUS-WIDE and ImageNet100 datasets.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Datasets 
      
        AAAI 
      
        Compact-Codes 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        IJCAI 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/dou2020learning/">Learning Global and Local Consistent Representations for Unsupervised Image Retrieval via Deep Graph Diffusion Networks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Global and Local Consistent Representations for Unsupervised Image Retrieval via Deep Graph Diffusion Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Global and Local Consistent Representations for Unsupervised Image Retrieval via Deep Graph Diffusion Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dou Zhiyong, Cui Haotian, Zhang Lin, Wang Bo</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>7</td>
    <td><p>Diffusion has shown great success in improving accuracy of unsupervised image
retrieval systems by utilizing high-order structures of image manifold.
However, existing diffusion methods suffer from three major limitations: 1)
they usually rely on local structures without considering global manifold
information; 2) they focus on improving pair-wise similarities within existing
images input output transductively while lacking flexibility to learn
representations for novel unseen instances inductively; 3) they fail to scale
to large datasets due to prohibitive memory consumption and computational
burden due to intrinsic high-order operations on the whole graph. In this
paper, to address these limitations, we propose a novel method, Graph Diffusion
Networks (GRAD-Net), that adopts graph neural networks (GNNs), a novel variant
of deep learning algorithms on irregular graphs. GRAD-Net learns semantic
representations by exploiting both local and global structures of image
manifold in an unsupervised fashion. By utilizing sparse coding techniques,
GRAD-Net not only preserves global information on the image manifold, but also
enables scalable training and efficient querying. Experiments on several large
benchmark datasets demonstrate effectiveness of our method over
state-of-the-art diffusion algorithms for unsupervised image retrieval.</p>
</td>
    <td>
      
        Unsupervised 
      
        Datasets 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/dong2025learning/">Learning Space Partitions for Nearest Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Space Partitions for Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Space Partitions for Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dong Yihe, Indyk, Razenshteyn, Wagner</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>26</td>
    <td><p>Space partitions of underlie a vast and important
class of fast nearest neighbor search (NNS) algorithms. Inspired by recent theoretical work on NNS for general metric spaces (Andoni et al. 2018b,c), we develop a new framework for building space partitions reducing the problem to balanced graph partitioning followed by supervised classification.
We instantiate this general approach with the KaHIP graph partitioner (Sanders and Schulz 2013) and neural networks, respectively, to obtain a new partitioning procedure called Neural Locality-Sensitive Hashing (Neural LSH). On several standard benchmarks for NNS (Aumuller et al. 2017), our experiments show that the partitions obtained by Neural LSH consistently outperform partitions found by quantization-based and tree-based methods as well as classic, data-oblivious LSH.</p>
</td>
    <td>
      
        Tree-Based-ANN 
      
        Locality-Sensitive-Hashing 
      
        Tools-&-Libraries 
      
        Quantization 
      
        Hashing-Methods 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/dong2019learning/">Learning Space Partitions for Nearest Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Space Partitions for Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Space Partitions for Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dong Yihe, Indyk Piotr, Razenshteyn Ilya, Wagner Tal</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>26</td>
    <td><p>Space partitions of \(\mathbb{R}^d\) underlie a vast and important class of
fast nearest neighbor search (NNS) algorithms. Inspired by recent theoretical
work on NNS for general metric spaces [Andoni, Naor, Nikolov, Razenshteyn,
Waingarten STOC 2018, FOCS 2018], we develop a new framework for building space
partitions reducing the problem to balanced graph partitioning followed by
supervised classification. We instantiate this general approach with the KaHIP
graph partitioner [Sanders, Schulz SEA 2013] and neural networks, respectively,
to obtain a new partitioning procedure called Neural Locality-Sensitive Hashing
(Neural LSH). On several standard benchmarks for NNS, our experiments show that
the partitions obtained by Neural LSH consistently outperform partitions found
by quantization-based and tree-based methods as well as classic, data-oblivious
LSH.</p>
</td>
    <td>
      
        Supervised 
      
        Tools-&-Libraries 
      
        Locality-Sensitive-Hashing 
      
        Hashing-Methods 
      
        Quantization 
      
        Tree-Based-ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/dong2020learning/">Learning Space Partitions for Nearest Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Space Partitions for Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Space Partitions for Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dong Yihe, Indyk, Razenshteyn, Wagner</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>26</td>
    <td><p>Space partitions of underlie a vast and important
class of fast nearest neighbor search (NNS) algorithms. Inspired by recent theoretical work on NNS for general metric spaces (Andoni et al. 2018b,c), we develop a new framework for building space partitions reducing the problem to balanced graph partitioning followed by supervised classification.
We instantiate this general approach with the KaHIP graph partitioner (Sanders and Schulz 2013) and neural networks, respectively, to obtain a new partitioning procedure called Neural Locality-Sensitive Hashing (Neural LSH). On several standard benchmarks for NNS (Aumuller et al. 2017), our experiments show that the partitions obtained by Neural LSH consistently outperform partitions found by quantization-based and tree-based methods as well as classic, data-oblivious LSH.</p>
</td>
    <td>
      
        Tree-Based-ANN 
      
        Locality-Sensitive-Hashing 
      
        Tools-&-Libraries 
      
        Quantization 
      
        Hashing-Methods 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/fan2025deep/">Deep Polarized Network for Supervised Learning of Accurate Binary Hashing Codes</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Polarized Network for Supervised Learning of Accurate Binary Hashing Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Polarized Network for Supervised Learning of Accurate Binary Hashing Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fan Lixin, Ng, Ju, Zhang, Chan</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</td>
    <td>81</td>
    <td><p>This paper proposes a novel deep polarized network (DPN) for learning to hash, in which each channel in the network outputs is pushed far away
from zero by employing a differentiable bit-wise hinge-like loss which is dubbed as polarization loss. Reformulated within a generic Hamming Distance Metric Learning framework [Norouzi et al.,
2012], the proposed polarization loss bypasses the requirement to prepare pairwise labels for (dis-)similar items and, yet, the proposed loss strictly bounds from above the pairwise Hamming Distance based losses. The intrinsic connection between pairwise and pointwise label information, as
disclosed in this paper, brings about the following methodological improvements: (a) we may directly employ the proposed differentiable polarization loss with no large deviations incurred from
the target Hamming distance based loss; and (b) the subtask of assigning binary codes becomes extremely simple — even random codes assigned to each class suffice to result in state-of-the-art performances, as demonstrated in CIFAR10, NUS-WIDE and ImageNet100 datasets.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Datasets 
      
        AAAI 
      
        Compact-Codes 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        IJCAI 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/dolhansky2020adversarial/">Adversarial collision attacks on image hashing functions</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Adversarial collision attacks on image hashing functions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Adversarial collision attacks on image hashing functions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dolhansky Brian, Ferrer Cristian Canton</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>9</td>
    <td><p>Hashing images with a perceptual algorithm is a common approach to solving
duplicate image detection problems. However, perceptual image hashing
algorithms are differentiable, and are thus vulnerable to gradient-based
adversarial attacks. We demonstrate that not only is it possible to modify an
image to produce an unrelated hash, but an exact image hash collision between a
source and target image can be produced via minuscule adversarial
perturbations. In a white box setting, these collisions can be replicated
across nearly every image pair and hash type (including both deep and
non-learned hashes). Furthermore, by attacking points other than the output of
a hashing function, an attacker can avoid having to know the details of a
particular algorithm, resulting in collisions that transfer across different
hash sizes or model architectures. Using these techniques, an adversary can
poison the image lookup table of a duplicate image detection service, resulting
in undefined or unwanted behavior. Finally, we offer several potential
mitigations to gradient-based image hash attacks.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Image-Retrieval 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/dai2020convolutional/">Convolutional Embedding for Edit Distance</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Convolutional Embedding for Edit Distance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Convolutional Embedding for Edit Distance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dai Xinyan, Yan Xiao, Zhou Kaiwen, Wang Yuxuan, Yang Han, Cheng James</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>7</td>
    <td><p>Edit-distance-based string similarity search has many applications such as
spell correction, data de-duplication, and sequence alignment. However,
computing edit distance is known to have high complexity, which makes string
similarity search challenging for large datasets. In this paper, we propose a
deep learning pipeline (called CNN-ED) that embeds edit distance into Euclidean
distance for fast approximate similarity search. A convolutional neural network
(CNN) is used to generate fixed-length vector embeddings for a dataset of
strings and the loss function is a combination of the triplet loss and the
approximation error. To justify our choice of using CNN instead of other
structures (e.g., RNN) as the model, theoretical analysis is conducted to show
that some basic operations in our CNN model preserve edit distance.
Experimental results show that CNN-ED outperforms data-independent CGK
embedding and RNN-based GRU embedding in terms of both accuracy and efficiency
by a large margin. We also show that string similarity search can be
significantly accelerated using CNN-based embeddings, sometimes by orders of
magnitude.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        SIGIR 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/dadaneh2020pairwise/">Pairwise Supervised Hashing with Bernoulli Variational Auto-Encoder and Self-Control Gradient Estimator</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Pairwise Supervised Hashing with Bernoulli Variational Auto-Encoder and Self-Control Gradient Estimator' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Pairwise Supervised Hashing with Bernoulli Variational Auto-Encoder and Self-Control Gradient Estimator' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dadaneh Siamak Zamani, Boluki Shahin, Yin Mingzhang, Zhou Mingyuan, Qian Xiaoning</td> <!-- 🔧 You were missing this -->
    <td>Uncertainty in Artificial Intelligence Conference (UAI) 2020</td>
    <td>11</td>
    <td><p>Semantic hashing has become a crucial component of fast similarity search in
many large-scale information retrieval systems, in particular, for text data.
Variational auto-encoders (VAEs) with binary latent variables as hashing codes
provide state-of-the-art performance in terms of precision for document
retrieval. We propose a pairwise loss function with discrete latent VAE to
reward within-class similarity and between-class dissimilarity for supervised
hashing. Instead of solving the optimization relying on existing biased
gradient estimators, an unbiased low-variance gradient estimator is adopted to
optimize the hashing function by evaluating the non-differentiable loss
function over two correlated sets of binary hashing codes to control the
variance of gradient estimates. This new semantic hashing framework achieves
superior performance compared to the state-of-the-arts, as demonstrated by our
comprehensive experiments.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Supervised 
      
        Text-Retrieval 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Neural-Hashing 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/westermann2021sentence/">Sentence Embeddings and High-speed Similarity Search for Fast Computer Assisted Annotation of Legal Documents</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Sentence Embeddings and High-speed Similarity Search for Fast Computer Assisted Annotation of Legal Documents' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Sentence Embeddings and High-speed Similarity Search for Fast Computer Assisted Annotation of Legal Documents' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Westermann Hannes, Savelka Jaromir, Walker Vern R., Ashley Kevin D., Benyekhlef Karim</td> <!-- 🔧 You were missing this -->
    <td>Frontiers in Artificial Intelligence and Applications</td>
    <td>22</td>
    <td><p>Human-performed annotation of sentences in legal documents is an important
prerequisite to many machine learning based systems supporting legal tasks.
Typically, the annotation is done sequentially, sentence by sentence, which is
often time consuming and, hence, expensive. In this paper, we introduce a
proof-of-concept system for annotating sentences “laterally.” The approach is
based on the observation that sentences that are similar in meaning often have
the same label in terms of a particular type system. We use this observation in
allowing annotators to quickly view and annotate sentences that are
semantically similar to a given sentence, across an entire corpus of documents.
Here, we present the interface of the system and empirically evaluate the
approach. The experiments show that lateral annotation has the potential to
make the annotation process quicker and more consistent.</p>
</td>
    <td>
      
        Similarity-Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/chen2025strongly/">Strongly Constrained Discrete Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Strongly Constrained Discrete Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Strongly Constrained Discrete Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen Yong, Tian, Zhang, Wang, Zhang</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>50</td>
    <td><p>Learning to hash is a fundamental technique widely used in large-scale image retrieval. Most existing methods for learning to hash address the involved discrete optimization problem by the continuous relaxation of the binary constraint, which usually leads to large quantization errors and consequently suboptimal binary codes. A few discrete hashing methods have emerged recently. However, they either completely ignore some useful constraints (specifically the balance and decorrelation of hash bits) or just turn those constraints into regularizers that would make the optimization easier but less accurate. In this paper, we propose a novel supervised hashing method named Strongly Constrained Discrete Hashing (SCDH) which overcomes such limitations. It can learn the binary codes for all examples in the training set, and meanwhile obtain a hash function for unseen samples with the above mentioned constraints preserved. Although the model of SCDH is fairly sophisticated, we are able to find closed-form solutions to all of its optimization subproblems and thus design an efficient algorithm that converges quickly. In addition, we extend SCDH to a kernelized version SCDH K . Our experiments on three large benchmark datasets have demonstrated that not only can SCDH and SCDH K achieve substantially higher MAP scores than state-of-the-art baselines, but they train much faster than those that are also supervised as well.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Datasets 
      
        Neural-Hashing 
      
        Quantization 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/cheng2020robust/">Robust Unsupervised Cross-modal Hashing for Multimedia Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Robust Unsupervised Cross-modal Hashing for Multimedia Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Robust Unsupervised Cross-modal Hashing for Multimedia Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cheng Miaomiao, Jing, Ng</td> <!-- 🔧 You were missing this -->
    <td>ACM Transactions on Information Systems</td>
    <td>48</td>
    <td><p>With the quick development of social websites, there are more opportunities to have different media types (such as text, image, video, etc.) describing the same topic from large-scale heterogeneous data sources. To efficiently identify the inter-media correlations for multimedia retrieval, unsupervised cross-modal hashing (UCMH) has gained increased interest due to the significant reduction in computation and storage. However, most UCMH methods assume that the data from different modalities are well paired. As a result, existing UCMH methods may not achieve satisfactory performance when partially paired data are given only. In this article, we propose a new-type of UCMH method called robust unsupervised cross-modal hashing (RUCMH). The major contribution lies in jointly learning modal-specific hash function, exploring the correlations among modalities with partial or even without any pairwise correspondence, and preserving the information of original features as much as possible. The learning process can be modeled via a joint minimization problem, and the corresponding optimization algorithm is presented. A series of experiments is conducted on four real-world datasets (Wiki, MIRFlickr, NUS-WIDE, and MS-COCO). The results demonstrate that RUCMH can significantly outperform the state-of-the-art unsupervised cross-modal hashing methods, especially for the partially paired case, which validates the effectiveness of RUCMH.</p>
</td>
    <td>
      
        Scalability 
      
        Datasets 
      
        Hashing-Methods 
      
        Evaluation 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/wu2018review/">A Review for Weighted MinHash Algorithms</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Review for Weighted MinHash Algorithms' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Review for Weighted MinHash Algorithms' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu Wei, Li Bin, Chen Ling, Gao Junbin, Zhang Chengqi</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Knowledge and Data Engineering</td>
    <td>31</td>
    <td><p>Data similarity (or distance) computation is a fundamental research topic
which underpins many high-level applications based on similarity measures in
machine learning and data mining. However, in large-scale real-world scenarios,
the exact similarity computation has become daunting due to “3V” nature
(volume, velocity and variety) of big data. In such cases, the hashing
techniques have been verified to efficiently conduct similarity estimation in
terms of both theory and practice. Currently, MinHash is a popular technique
for efficiently estimating the Jaccard similarity of binary sets and
furthermore, weighted MinHash is generalized to estimate the generalized
Jaccard similarity of weighted sets. This review focuses on categorizing and
discussing the existing works of weighted MinHash algorithms. In this review,
we mainly categorize the Weighted MinHash algorithms into quantization-based
approaches, “active index”-based ones and others, and show the evolution and
inherent connection of the weighted MinHash algorithms, from the integer
weighted MinHash algorithms to real-valued weighted MinHash ones (particularly
the Consistent Weighted Sampling scheme). Also, we have developed a python
toolbox for the algorithms, and released it in our github. Based on the
toolbox, we experimentally conduct a comprehensive comparative study of the
standard MinHash algorithm and the weighted MinHash ones.</p>
</td>
    <td>
      
        Locality-Sensitive-Hashing 
      
        Hashing-Methods 
      
        Quantization 
      
        Scalability 
      
        Survey-Paper 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/zhu2020dual/">Dual-level Semantic Transfer Deep Hashing for Efficient Social Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Dual-level Semantic Transfer Deep Hashing for Efficient Social Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Dual-level Semantic Transfer Deep Hashing for Efficient Social Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhu Lei, Cui Hui, Cheng Zhiyong, Li Jingjing, Zhang Zheng</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Circuits and Systems for Video Technology</td>
    <td>38</td>
    <td><p>Social network stores and disseminates a tremendous amount of user shared
images. Deep hashing is an efficient indexing technique to support large-scale
social image retrieval, due to its deep representation capability, fast
retrieval speed and low storage cost. Particularly, unsupervised deep hashing
has well scalability as it does not require any manually labelled data for
training. However, owing to the lacking of label guidance, existing methods
suffer from severe semantic shortage when optimizing a large amount of deep
neural network parameters. Differently, in this paper, we propose a Dual-level
Semantic Transfer Deep Hashing (DSTDH) method to alleviate this problem with a
unified deep hash learning framework. Our model targets at learning the
semantically enhanced deep hash codes by specially exploiting the
user-generated tags associated with the social images. Specifically, we design
a complementary dual-level semantic transfer mechanism to efficiently discover
the potential semantics of tags and seamlessly transfer them into binary hash
codes. On the one hand, instance-level semantics are directly preserved into
hash codes from the associated tags with adverse noise removing. Besides, an
image-concept hypergraph is constructed for indirectly transferring the latent
high-order semantic correlations of images and tags into hash codes. Moreover,
the hash codes are obtained simultaneously with the deep representation
learning by the discrete hash optimization strategy. Extensive experiments on
two public social image retrieval datasets validate the superior performance of
our method compared with state-of-the-art hashing methods. The source codes of
our method can be obtained at https://github.com/research2020-1/DSTDH</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Memory-Efficiency 
      
        Unsupervised 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/chen2020strongly/">Strongly Constrained Discrete Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Strongly Constrained Discrete Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Strongly Constrained Discrete Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen Yong, Tian, Zhang, Wang, Zhang</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>50</td>
    <td><p>Learning to hash is a fundamental technique widely used in large-scale image retrieval. Most existing methods for learning to hash address the involved discrete optimization problem by the continuous relaxation of the binary constraint, which usually leads to large quantization errors and consequently suboptimal binary codes. A few discrete hashing methods have emerged recently. However, they either completely ignore some useful constraints (specifically the balance and decorrelation of hash bits) or just turn those constraints into regularizers that would make the optimization easier but less accurate. In this paper, we propose a novel supervised hashing method named Strongly Constrained Discrete Hashing (SCDH) which overcomes such limitations. It can learn the binary codes for all examples in the training set, and meanwhile obtain a hash function for unseen samples with the above mentioned constraints preserved. Although the model of SCDH is fairly sophisticated, we are able to find closed-form solutions to all of its optimization subproblems and thus design an efficient algorithm that converges quickly. In addition, we extend SCDH to a kernelized version SCDH K . Our experiments on three large benchmark datasets have demonstrated that not only can SCDH and SCDH K achieve substantially higher MAP scores than state-of-the-art baselines, but they train much faster than those that are also supervised as well.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Datasets 
      
        Neural-Hashing 
      
        Quantization 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/cheng2025robust/">Robust Unsupervised Cross-modal Hashing for Multimedia Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Robust Unsupervised Cross-modal Hashing for Multimedia Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Robust Unsupervised Cross-modal Hashing for Multimedia Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cheng Miaomiao, Jing, Ng</td> <!-- 🔧 You were missing this -->
    <td>ACM Transactions on Information Systems</td>
    <td>48</td>
    <td><p>With the quick development of social websites, there are more opportunities to have different media types (such as text, image, video, etc.) describing the same topic from large-scale heterogeneous data sources. To efficiently identify the inter-media correlations for multimedia retrieval, unsupervised cross-modal hashing (UCMH) has gained increased interest due to the significant reduction in computation and storage. However, most UCMH methods assume that the data from different modalities are well paired. As a result, existing UCMH methods may not achieve satisfactory performance when partially paired data are given only. In this article, we propose a new-type of UCMH method called robust unsupervised cross-modal hashing (RUCMH). The major contribution lies in jointly learning modal-specific hash function, exploring the correlations among modalities with partial or even without any pairwise correspondence, and preserving the information of original features as much as possible. The learning process can be modeled via a joint minimization problem, and the corresponding optimization algorithm is presented. A series of experiments is conducted on four real-world datasets (Wiki, MIRFlickr, NUS-WIDE, and MS-COCO). The results demonstrate that RUCMH can significantly outperform the state-of-the-art unsupervised cross-modal hashing methods, especially for the partially paired case, which validates the effectiveness of RUCMH.</p>
</td>
    <td>
      
        Scalability 
      
        Datasets 
      
        Hashing-Methods 
      
        Evaluation 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/zhang2020collaborative/">Collaborative Generative Hashing for Marketing and Fast Cold-start Recommendation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Collaborative Generative Hashing for Marketing and Fast Cold-start Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Collaborative Generative Hashing for Marketing and Fast Cold-start Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Yan, Tsang Ivor W., Duan Lixin</td> <!-- 🔧 You were missing this -->
    <td>IEEE Intelligent Systems</td>
    <td>6</td>
    <td><p>Cold-start has being a critical issue in recommender systems with the
explosion of data in e-commerce. Most existing studies proposed to alleviate
the cold-start problem are also known as hybrid recommender systems that learn
representations of users and items by combining user-item interactive and
user/item content information. However, previous hybrid methods regularly
suffered poor efficiency bottlenecking in online recommendations with
large-scale items, because they were designed to project users and items into
continuous latent space where the online recommendation is expensive. To this
end, we propose a collaborative generated hashing (CGH) framework to improve
the efficiency by denoting users and items as binary codes, then fast hashing
search techniques can be used to speed up the online recommendation. In
addition, the proposed CGH can generate potential users or items for marketing
application where the generative network is designed with the principle of
Minimum Description Length (MDL), which is used to learn compact and
informative binary codes. Extensive experiments on two public datasets show the
advantages for recommendations in various settings over competing baselines and
analyze its feasibility in marketing application.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Datasets 
      
        Recommender-Systems 
      
        Compact-Codes 
      
        Scalability 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/weyand2020google/">Google Landmarks Dataset v2 -- A Large-Scale Benchmark for Instance-Level Recognition and Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Google Landmarks Dataset v2 -- A Large-Scale Benchmark for Instance-Level Recognition and Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Google Landmarks Dataset v2 -- A Large-Scale Benchmark for Instance-Level Recognition and Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Weyand Tobias, Araujo Andre, Cao Bingyi, Sim Jack</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>235</td>
    <td><p>While image retrieval and instance recognition techniques are progressing
rapidly, there is a need for challenging datasets to accurately measure their
performance – while posing novel challenges that are relevant for practical
applications. We introduce the Google Landmarks Dataset v2 (GLDv2), a new
benchmark for large-scale, fine-grained instance recognition and image
retrieval in the domain of human-made and natural landmarks. GLDv2 is the
largest such dataset to date by a large margin, including over 5M images and
200k distinct instance labels. Its test set consists of 118k images with ground
truth annotations for both the retrieval and recognition tasks. The ground
truth construction involved over 800 hours of human annotator work. Our new
dataset has several challenging properties inspired by real world applications
that previous datasets did not consider: An extremely long-tailed class
distribution, a large fraction of out-of-domain test photos and large
intra-class variability. The dataset is sourced from Wikimedia Commons, the
world’s largest crowdsourced collection of landmark photos. We provide baseline
results for both recognition and retrieval tasks based on state-of-the-art
methods as well as competitive results from a public challenge. We further
demonstrate the suitability of the dataset for transfer learning by showing
that image embeddings trained on it achieve competitive retrieval performance
on independent datasets. The dataset images, ground-truth and metric scoring
code are available at https://github.com/cvdfoundation/google-landmark.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Datasets 
      
        CVPR 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/chen2020fine/">Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen Shizhe, Zhao Yida, Jin Qin, Wu Qi</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>272</td>
    <td><p>Cross-modal retrieval between videos and texts has attracted growing
attentions due to the rapid emergence of videos on the web. The current
dominant approach for this problem is to learn a joint embedding space to
measure cross-modal similarities. However, simple joint embeddings are
insufficient to represent complicated visual and textual details, such as
scenes, objects, actions and their compositions. To improve fine-grained
video-text retrieval, we propose a Hierarchical Graph Reasoning (HGR) model,
which decomposes video-text matching into global-to-local levels. To be
specific, the model disentangles texts into hierarchical semantic graph
including three levels of events, actions, entities and relationships across
levels. Attention-based graph reasoning is utilized to generate hierarchical
textual embeddings, which can guide the learning of diverse and hierarchical
video representations. The HGR model aggregates matchings from different
video-text levels to capture both global and local details. Experimental
results on three video-text datasets demonstrate the advantages of our model.
Such hierarchical decomposition also enables better generalization across
datasets and improves the ability to distinguish fine-grained semantic
differences.</p>
</td>
    <td>
      
        Datasets 
      
        CVPR 
      
        Text-Retrieval 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/charikar2020kernel/">Kernel Density Estimation through Density Constrained Near Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Kernel Density Estimation through Density Constrained Near Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Kernel Density Estimation through Density Constrained Near Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Charikar Moses, Kapralov Michael, Nouri Navid, Siminelakis Paris</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS)</td>
    <td>8</td>
    <td><p>In this paper we revisit the kernel density estimation problem: given a
kernel \(K(x, y)\) and a dataset of \(n\) points in high dimensional Euclidean
space, prepare a data structure that can quickly output, given a query \(q\), a
\((1+\epsilon)\)-approximation to \(\mu:=\frac1{|P|}\sum_{p\in P} K(p, q)\). First,
we give a single data structure based on classical near neighbor search
techniques that improves upon or essentially matches the query time and space
complexity for all radial kernels considered in the literature so far. We then
show how to improve both the query complexity and runtime by using recent
advances in data-dependent near neighbor search.
  We achieve our results by giving a new implementation of the natural
importance sampling scheme. Unlike previous approaches, our algorithm first
samples the dataset uniformly (considering a geometric sequence of sampling
rates), and then uses existing approximate near neighbor search techniques on
the resulting smaller dataset to retrieve the sampled points that lie at an
appropriate distance from the query. We show that the resulting sampled dataset
has strong geometric structure, making approximate near neighbor search return
the required samples much more efficiently than for worst case datasets of the
same size. As an example application, we show that this approach yields a data
structure that achieves query time \(\mu^{-(1+o(1))/4}\) and space complexity
\(\mu^{-(1+o(1))}\) for the Gaussian kernel. Our data dependent approach achieves
query time \(\mu^{-0.173-o(1)}\) and space \(\mu^{-(1+o(1))}\) for the Gaussian
kernel. The data dependent analysis relies on new techniques for tracking the
geometric structure of the input datasets in a recursive hashing process that
we hope will be of interest in other applications in near neighbor search.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/chaudhuri2021crossatnet/">CrossATNet - A Novel Cross-Attention Based Framework for Sketch-Based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=CrossATNet - A Novel Cross-Attention Based Framework for Sketch-Based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=CrossATNet - A Novel Cross-Attention Based Framework for Sketch-Based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chaudhuri Ushasi, Banerjee Biplab, Bhattacharya Avik, Datcu Mihai</td> <!-- 🔧 You were missing this -->
    <td>Image and Vision Computing</td>
    <td>28</td>
    <td><p>We propose a novel framework for cross-modal zero-shot learning (ZSL) in the
context of sketch-based image retrieval (SBIR). Conventionally, the SBIR schema
mainly considers simultaneous mappings among the two image views and the
semantic side information. Therefore, it is desirable to consider fine-grained
classes mainly in the sketch domain using highly discriminative and
semantically rich feature space. However, the existing deep generative
modeling-based SBIR approaches majorly focus on bridging the gaps between the
seen and unseen classes by generating pseudo-unseen-class samples. Besides,
violating the ZSL protocol by not utilizing any unseen-class information during
training, such techniques do not pay explicit attention to modeling the
discriminative nature of the shared space. Also, we note that learning a
unified feature space for both the multi-view visual data is a tedious task
considering the significant domain difference between sketches and color
images. In this respect, as a remedy, we introduce a novel framework for
zero-shot SBIR. While we define a cross-modal triplet loss to ensure the
discriminative nature of the shared space, an innovative cross-modal attention
learning strategy is also proposed to guide feature extraction from the image
domain exploiting information from the respective sketch counterpart. In order
to preserve the semantic consistency of the shared space, we consider a graph
CNN-based module that propagates the semantic class topology to the shared
space. To ensure an improved response time during inference, we further explore
the possibility of representing the shared space in terms of hash codes.
Experimental results obtained on the benchmark TU-Berlin and the Sketchy
datasets confirm the superiority of CrossATNet in yielding state-of-the-art
results.</p>
</td>
    <td>
      
        Few-Shot-&-Zero-Shot 
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/wu2019scalable/">Scalable Zero-shot Entity Linking with Dense Entity Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Scalable Zero-shot Entity Linking with Dense Entity Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Scalable Zero-shot Entity Linking with Dense Entity Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu Ledell, Petroni Fabio, Josifoski Martin, Riedel Sebastian, Zettlemoyer Luke</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</td>
    <td>290</td>
    <td><p>This paper introduces a conceptually simple, scalable, and highly effective
BERT-based entity linking model, along with an extensive evaluation of its
accuracy-speed trade-off. We present a two-stage zero-shot linking algorithm,
where each entity is defined only by a short textual description. The first
stage does retrieval in a dense space defined by a bi-encoder that
independently embeds the mention context and the entity descriptions. Each
candidate is then re-ranked with a cross-encoder, that concatenates the mention
and entity text. Experiments demonstrate that this approach is state of the art
on recent zero-shot benchmarks (6 point absolute gains) and also on more
established non-zero-shot evaluations (e.g. TACKBP-2010), despite its relative
simplicity (e.g. no explicit entity embeddings or manually engineered mention
tables). We also show that bi-encoder linking is very fast with nearest
neighbour search (e.g. linking with 5.9 million candidates in 2 milliseconds),
and that much of the accuracy gain from the more expensive cross-encoder can be
transferred to the bi-encoder via knowledge distillation. Our code and models
are available at https://github.com/facebookresearch/BLINK.</p>
</td>
    <td>
      
        Few-Shot-&-Zero-Shot 
      
        EMNLP 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/zhang2020learning/">Learning to Represent Image and Text with Denotation Graph</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning to Represent Image and Text with Denotation Graph' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning to Represent Image and Text with Denotation Graph' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Bowen, Hu Hexiang, Jain Vihan, Ie Eugene, Sha Fei</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</td>
    <td>23</td>
    <td><p>Learning to fuse vision and language information and representing them is an
important research problem with many applications. Recent progresses have
leveraged the ideas of pre-training (from language modeling) and attention
layers in Transformers to learn representation from datasets containing images
aligned with linguistic expressions that describe the images. In this paper, we
propose learning representations from a set of implied, visually grounded
expressions between image and text, automatically mined from those datasets. In
particular, we use denotation graphs to represent how specific concepts (such
as sentences describing images) can be linked to abstract and generic concepts
(such as short phrases) that are also visually grounded. This type of
generic-to-specific relations can be discovered using linguistic analysis
tools. We propose methods to incorporate such relations into learning
representation. We show that state-of-the-art multimodal learning models can be
further improved by leveraging automatically harvested structural relations.
The representations lead to stronger empirical results on downstream tasks of
cross-modal image retrieval, referring expression, and compositional
attribute-object recognition. Both our codes and the extracted denotation
graphs on the Flickr30K and the COCO datasets are publically available on
https://sha-lab.github.io/DG.</p>
</td>
    <td>
      
        Datasets 
      
        EMNLP 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/cao2020unifying/">Unifying Deep Local and Global Features for Image Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unifying Deep Local and Global Features for Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unifying Deep Local and Global Features for Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cao Bingyi, Araujo Andre, Sim Jack</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>293</td>
    <td><p>Image retrieval is the problem of searching an image database for items that
are similar to a query image. To address this task, two main types of image
representations have been studied: global and local image features. In this
work, our key contribution is to unify global and local features into a single
deep model, enabling accurate retrieval with efficient feature extraction. We
refer to the new model as DELG, standing for DEep Local and Global features. We
leverage lessons from recent feature learning work and propose a model that
combines generalized mean pooling for global features and attentive selection
for local features. The entire network can be learned end-to-end by carefully
balancing the gradient flow between two heads – requiring only image-level
labels. We also introduce an autoencoder-based dimensionality reduction
technique for local features, which is integrated into the model, improving
training efficiency and matching performance. Comprehensive experiments show
that our model achieves state-of-the-art image retrieval on the Revisited
Oxford and Paris datasets, and state-of-the-art single-model instance-level
recognition on the Google Landmarks dataset v2. Code and models are available
at https://github.com/tensorflow/models/tree/master/research/delf .</p>
</td>
    <td>
      
        Datasets 
      
        Image-Retrieval 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/bai2025targeted/">Targeted Attack for Deep Hashing based Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Targeted Attack for Deep Hashing based Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Targeted Attack for Deep Hashing based Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Bai Jiawang, Chen, Li, Wu, Guo, Xia, Yang</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>68</td>
    <td><p>The deep hashing based retrieval method is widely adopted in large-scale image and video retrieval. However, there is little investigation on its security. In this paper, we propose a novel method, dubbed deep hashing targeted attack (DHTA), to study the targeted attack on such retrieval. Specifically, we first formulate the targeted attack as a point-to-set optimization, which minimizes the average distance between the hash code of an adversarial example and those of a set of objects with the target label. Then we design a novel component-voting scheme to obtain an anchor code as the representative of the set of hash codes of objects with the target label, whose optimality guarantee is also theoretically derived. To balance the performance and perceptibility, we propose to minimize the Hamming distance between the hash code of the adversarial example and the anchor code under the ℓ∞ restriction on the perturbation. Extensive experiments verify that DHTA is effective in attacking both deep hashing based image retrieval and video retrieval.</p>
</td>
    <td>
      
        Video-Retrieval 
      
        Image-Retrieval 
      
        Scalability 
      
        Neural-Hashing 
      
        Hashing-Methods 
      
        Evaluation 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/bai2018learning/">Learning-based Efficient Graph Similarity Computation via Multi-Scale Convolutional Set Matching</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning-based Efficient Graph Similarity Computation via Multi-Scale Convolutional Set Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning-based Efficient Graph Similarity Computation via Multi-Scale Convolutional Set Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Bai Yunsheng, Ding Hao, Sun Yizhou, Wang Wei</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>64</td>
    <td><p>Graph similarity computation is one of the core operations in many
graph-based applications, such as graph similarity search, graph database
analysis, graph clustering, etc. Since computing the exact distance/similarity
between two graphs is typically NP-hard, a series of approximate methods have
been proposed with a trade-off between accuracy and speed. Recently, several
data-driven approaches based on neural networks have been proposed, most of
which model the graph-graph similarity as the inner product of their
graph-level representations, with different techniques proposed for generating
one embedding per graph. However, using one fixed-dimensional embedding per
graph may fail to fully capture graphs in varying sizes and link structures, a
limitation that is especially problematic for the task of graph similarity
computation, where the goal is to find the fine-grained difference between two
graphs. In this paper, we address the problem of graph similarity computation
from another perspective, by directly matching two sets of node embeddings
without the need to use fixed-dimensional vectors to represent whole graphs for
their similarity computation. The model, GraphSim, achieves the
state-of-the-art performance on four real-world graph datasets under six out of
eight settings (here we count a specific dataset and metric combination as one
setting), compared to existing popular methods for approximate Graph Edit
Distance (GED) and Maximum Common Subgraph (MCS) computation.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Graph-Based-ANN 
      
        Datasets 
      
        AAAI 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/bai2020targeted/">Targeted Attack for Deep Hashing based Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Targeted Attack for Deep Hashing based Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Targeted Attack for Deep Hashing based Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Bai Jiawang, Chen, Li, Wu, Guo, Xia, Yang</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>68</td>
    <td><p>The deep hashing based retrieval method is widely adopted in large-scale image and video retrieval. However, there is little investigation on its security. In this paper, we propose a novel method, dubbed deep hashing targeted attack (DHTA), to study the targeted attack on such retrieval. Specifically, we first formulate the targeted attack as a point-to-set optimization, which minimizes the average distance between the hash code of an adversarial example and those of a set of objects with the target label. Then we design a novel component-voting scheme to obtain an anchor code as the representative of the set of hash codes of objects with the target label, whose optimality guarantee is also theoretically derived. To balance the performance and perceptibility, we propose to minimize the Hamming distance between the hash code of the adversarial example and the anchor code under the ℓ∞ restriction on the perturbation. Extensive experiments verify that DHTA is effective in attacking both deep hashing based image retrieval and video retrieval.</p>
</td>
    <td>
      
        Video-Retrieval 
      
        Image-Retrieval 
      
        Scalability 
      
        Neural-Hashing 
      
        Hashing-Methods 
      
        Evaluation 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/aum%C3%BCller2020differentially/">Differentially Private Sketches for Jaccard Similarity Estimation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Differentially Private Sketches for Jaccard Similarity Estimation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Differentially Private Sketches for Jaccard Similarity Estimation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Aumüller Martin, Bourgeat Anders, Schmurr Jana</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>5</td>
    <td><p>This paper describes two locally-differential private algorithms for
releasing user vectors such that the Jaccard similarity between these vectors
can be efficiently estimated. The basic building block is the well known
MinHash method. To achieve a privacy-utility trade-off, MinHash is extended in
two ways using variants of Generalized Randomized Response and the Laplace
Mechanism. A theoretical analysis provides bounds on the absolute error and
experiments show the utility-privacy trade-off on synthetic and real-world
data. The paper ends with a critical discussion of related work.</p>
</td>
    <td>
      
        Locality-Sensitive-Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/zhang2020deep/">Deep Pairwise Hashing for Cold-start Recommendation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Pairwise Hashing for Cold-start Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Pairwise Hashing for Cold-start Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Yan, Tsang Ivor W., Yin Hongzhi, Yang Guowu, Lian Defu, Li Jingjing</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Knowledge and Data Engineering</td>
    <td>9</td>
    <td><p>Recommendation efficiency and data sparsity problems have been regarded as
two challenges of improving performance for online recommendation. Most of the
previous related work focus on improving recommendation accuracy instead of
efficiency. In this paper, we propose a Deep Pairwise Hashing (DPH) to map
users and items to binary vectors in Hamming space, where a user’s preference
for an item can be efficiently calculated by Hamming distance, which
significantly improves the efficiency of online recommendation. To alleviate
data sparsity and cold-start problems, the user-item interactive information
and item content information are unified to learn effective representations of
items and users. Specifically, we first pre-train robust item representation
from item content data by a Denoising Auto-encoder instead of other
deterministic deep learning frameworks; then we finetune the entire framework
by adding a pairwise loss objective with discrete constraints; moreover, DPH
aims to minimize a pairwise ranking loss that is consistent with the ultimate
goal of recommendation. Finally, we adopt the alternating optimization method
to optimize the proposed model with discrete constraints. Extensive experiments
on three different datasets show that DPH can significantly advance the
state-of-the-art frameworks regarding data sparsity and item cold-start
recommendation.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Datasets 
      
        Recommender-Systems 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/zhang2019part/">Part-Guided Attention Learning for Vehicle Instance Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Part-Guided Attention Learning for Vehicle Instance Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Part-Guided Attention Learning for Vehicle Instance Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Xinyu, Zhang Rufeng, Cao Jiewei, Gong Dong, You Mingyu, Shen Chunhua</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Intelligent Transportation Systems</td>
    <td>56</td>
    <td><p>Vehicle instance retrieval often requires one to recognize the fine-grained
visual differences between vehicles. Besides the holistic appearance of
vehicles which is easily affected by the viewpoint variation and distortion,
vehicle parts also provide crucial cues to differentiate near-identical
vehicles. Motivated by these observations, we introduce a Part-Guided Attention
Network (PGAN) to pinpoint the prominent part regions and effectively combine
the global and part information for discriminative feature learning. PGAN first
detects the locations of different part components and salient regions
regardless of the vehicle identity, which serve as the bottom-up attention to
narrow down the possible searching regions. To estimate the importance of
detected parts, we propose a Part Attention Module (PAM) to adaptively locate
the most discriminative regions with high-attention weights and suppress the
distraction of irrelevant parts with relatively low weights. The PAM is guided
by the instance retrieval loss and therefore provides top-down attention that
enables attention to be calculated at the level of car parts and other salient
regions. Finally, we aggregate the global appearance and part features to
improve the feature performance further. The PGAN combines part-guided
bottom-up and top-down attention, global and part visual features in an
end-to-end framework. Extensive experiments demonstrate that the proposed
method achieves new state-of-the-art vehicle instance retrieval performance on
four large-scale benchmark datasets.</p>
</td>
    <td>
      
        Datasets 
      
        Scalability 
      
        Evaluation 
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/atighehchi2019cryptanalysis/">A Cryptanalysis of Two Cancelable Biometric Schemes based on Index-of-Max Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Cryptanalysis of Two Cancelable Biometric Schemes based on Index-of-Max Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Cryptanalysis of Two Cancelable Biometric Schemes based on Index-of-Max Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Atighehchi Kevin, Ghammam Loubna, Karabina Koray, Lacharme Patrick</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Information Forensics and Security</td>
    <td>56</td>
    <td><p>Cancelable biometric schemes generate secure biometric templates by combining
user specific tokens and biometric data. The main objective is to create
irreversible, unlinkable, and revocable templates, with high accuracy in
matching. In this paper, we cryptanalyze two recent cancelable biometric
schemes based on a particular locality sensitive hashing function, index-of-max
(IoM): Gaussian Random Projection-IoM (GRP-IoM) and Uniformly Random
Permutation-IoM (URP-IoM). As originally proposed, these schemes were claimed
to be resistant against reversibility, authentication, and linkability attacks
under the stolen token scenario. We propose several attacks against GRP-IoM and
URP-IoM, and argue that both schemes are severely vulnerable against
authentication and linkability attacks. We also propose better, but not yet
practical, reversibility attacks against GRP-IoM. The correctness and practical
impact of our attacks are verified over the same dataset provided by the
authors of these two schemes.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        Locality-Sensitive-Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/zou2019transductive/">Transductive Zero-Shot Hashing for Multilabel Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Transductive Zero-Shot Hashing for Multilabel Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Transductive Zero-Shot Hashing for Multilabel Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zou Qin, Zhang Zheng, Cao Ling, Chen Long, Wang Song</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Neural Networks and Learning Systems</td>
    <td>14</td>
    <td><p>Hash coding has been widely used in approximate nearest neighbor search for
large-scale image retrieval. Given semantic annotations such as class labels
and pairwise similarities of the training data, hashing methods can learn and
generate effective and compact binary codes. While some newly introduced images
may contain undefined semantic labels, which we call unseen images, zeor-shot
hashing techniques have been studied. However, existing zeor-shot hashing
methods focus on the retrieval of single-label images, and cannot handle
multi-label images. In this paper, for the first time, a novel transductive
zero-shot hashing method is proposed for multi-label unseen image retrieval. In
order to predict the labels of the unseen/target data, a visual-semantic bridge
is built via instance-concept coherence ranking on the seen/source data. Then,
pairwise similarity loss and focal quantization loss are constructed for
training a hashing model using both the seen/source and unseen/target data.
Extensive evaluations on three popular multi-label datasets demonstrate that,
the proposed hashing method achieves significantly better results than the
competing methods.</p>
</td>
    <td>
      
        Few-Shot-&-Zero-Shot 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Quantization 
      
        Compact-Codes 
      
        Scalability 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/yang2019learning/">Learning Shared Semantic Space with Correlation Alignment for Cross-modal Event Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Shared Semantic Space with Correlation Alignment for Cross-modal Event Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Shared Semantic Space with Correlation Alignment for Cross-modal Event Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yang Zhenguo, Lin Zehang, Kang Peipei, Lv Jianming, Li Qing, Liu Wenyin</td> <!-- 🔧 You were missing this -->
    <td>ACM Transactions on Multimedia Computing, Communications, and Applications</td>
    <td>28</td>
    <td><p>In this paper, we propose to learn shared semantic space with correlation
alignment (\({S}^{3}CA\)) for multimodal data representations, which aligns
nonlinear correlations of multimodal data distributions in deep neural networks
designed for heterogeneous data. In the context of cross-modal (event)
retrieval, we design a neural network with convolutional layers and
fully-connected layers to extract features for images, including images on
Flickr-like social media. Simultaneously, we exploit a fully-connected neural
network to extract semantic features for texts, including news articles from
news media. In particular, nonlinear correlations of layer activations in the
two neural networks are aligned with correlation alignment during the joint
training of the networks. Furthermore, we project the multimodal data into a
shared semantic space for cross-modal (event) retrieval, where the distances
between heterogeneous data samples can be measured directly. In addition, we
contribute a Wiki-Flickr Event dataset, where the multimodal data samples are
not describing each other in pairs like the existing paired datasets, but all
of them are describing semantic events. Extensive experiments conducted on both
paired and unpaired datasets manifest the effectiveness of \({S}^{3}CA\),
outperforming the state-of-the-art methods.</p>
</td>
    <td>
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/wu2024interpretable/">Interpretable Embedding for Ad-hoc Video Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Embedding for Ad-hoc Video Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Interpretable Embedding for Ad-hoc Video Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu Jiaxin, Ngo Chong-wah</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 28th ACM International Conference on Multimedia</td>
    <td>30</td>
    <td><p>Answering query with semantic concepts has long been the mainstream approach
for video search. Until recently, its performance is surpassed by concept-free
approach, which embeds queries in a joint space as videos. Nevertheless, the
embedded features as well as search results are not interpretable, hindering
subsequent steps in video browsing and query reformulation. This paper
integrates feature embedding and concept interpretation into a neural network
for unified dual-task learning. In this way, an embedding is associated with a
list of semantic concepts as an interpretation of video content. This paper
empirically demonstrates that, by using either the embedding features or
concepts, considerable search improvement is attainable on TRECVid benchmarked
datasets. Concepts are not only effective in pruning false positive videos, but
also highly complementary to concept-free search, leading to large margin of
improvement compared to state-of-the-art approaches.</p>
</td>
    <td>
      
        Datasets 
      
        Video-Retrieval 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/weng2020online/">Online Hashing with Efficient Updating of Binary Codes</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Online Hashing with Efficient Updating of Binary Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Online Hashing with Efficient Updating of Binary Codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Weng Zhenyu, Zhu</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>15</td>
    <td><p>Online hashing methods are efficient in learning the hash functions from the streaming data. However, when the hash functions change, the binary codes for the database have to be recomputed to guarantee the retrieval accuracy. Recomputing the binary codes by accumulating the whole database brings a timeliness challenge to the online retrieval process. In this paper, we propose a novel online hashing framework to update the binary codes efficiently without accumulating the whole database. In our framework, the hash functions are fixed and the projection functions are introduced to learn online from the streaming data. Therefore, inefficient updating of the binary codes by accumulating the whole database can be transformed to efficient updating of the binary codes by projecting the binary codes into another binary space. The queries and the binary code database are projected asymmetrically to further improve the retrieval accuracy. The experiments on two multi-label image databases demonstrate the effectiveness and the efficiency of our method for multi-label image retrieval.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Efficiency 
      
        Tools-&-Libraries 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        AAAI 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/yan2020deep/">Deep Multi-View Enhancement Hashing for Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Multi-View Enhancement Hashing for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Multi-View Enhancement Hashing for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yan Chenggang, Gong Biao, Wei Yuxuan, Gao Yue</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>413</td>
    <td><p>Hashing is an efficient method for nearest neighbor search in large-scale
data space by embedding high-dimensional feature descriptors into a similarity
preserving Hamming space with a low dimension. However, large-scale high-speed
retrieval through binary code has a certain degree of reduction in retrieval
accuracy compared to traditional retrieval methods. We have noticed that
multi-view methods can well preserve the diverse characteristics of data.
Therefore, we try to introduce the multi-view deep neural network into the hash
learning field, and design an efficient and innovative retrieval model, which
has achieved a significant improvement in retrieval performance. In this paper,
we propose a supervised multi-view hash model which can enhance the multi-view
information through neural networks. This is a completely new hash learning
method that combines multi-view and deep learning methods. The proposed method
utilizes an effective view stability evaluation method to actively explore the
relationship among views, which will affect the optimization direction of the
entire network. We have also designed a variety of multi-data fusion methods in
the Hamming space to preserve the advantages of both convolution and
multi-view. In order to avoid excessive computing resources on the enhancement
procedure during retrieval, we set up a separate structure called memory
network which participates in training together. The proposed method is
systematically evaluated on the CIFAR-10, NUS-WIDE and MS-COCO datasets, and
the results show that our method significantly outperforms the state-of-the-art
single-view and multi-view hashing methods.</p>
</td>
    <td>
      
        Supervised 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Compact-Codes 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/sain2020cross/">Cross-Modal Hierarchical Modelling for Fine-Grained Sketch Based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-Modal Hierarchical Modelling for Fine-Grained Sketch Based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-Modal Hierarchical Modelling for Fine-Grained Sketch Based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sain Aneeshan, Bhunia Ayan Kumar, Yang Yongxin, Xiang Tao, Song Yi-zhe</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>21</td>
    <td><p>Sketch as an image search query is an ideal alternative to text in capturing
the fine-grained visual details. Prior successes on fine-grained sketch-based
image retrieval (FG-SBIR) have demonstrated the importance of tackling the
unique traits of sketches as opposed to photos, e.g., temporal vs. static,
strokes vs. pixels, and abstract vs. pixel-perfect. In this paper, we study a
further trait of sketches that has been overlooked to date, that is, they are
hierarchical in terms of the levels of detail – a person typically sketches up
to various extents of detail to depict an object. This hierarchical structure
is often visually distinct. In this paper, we design a novel network that is
capable of cultivating sketch-specific hierarchies and exploiting them to match
sketch with photo at corresponding hierarchical levels. In particular, features
from a sketch and a photo are enriched using cross-modal co-attention, coupled
with hierarchical node fusion at every level to form a better embedding space
to conduct retrieval. Experiments on common benchmarks show our method to
outperform state-of-the-arts by a significant margin.</p>
</td>
    <td>
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/roy2019metric/">Metric-Learning based Deep Hashing Network for Content Based Retrieval of Remote Sensing Images</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Metric-Learning based Deep Hashing Network for Content Based Retrieval of Remote Sensing Images' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Metric-Learning based Deep Hashing Network for Content Based Retrieval of Remote Sensing Images' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Roy Subhankar, Sangineto Enver, Demir Begüm, Sebe Nicu</td> <!-- 🔧 You were missing this -->
    <td>IEEE Geoscience and Remote Sensing Letters</td>
    <td>75</td>
    <td><p>Hashing methods have been recently found very effective in retrieval of
remote sensing (RS) images due to their computational efficiency and fast
search speed. The traditional hashing methods in RS usually exploit
hand-crafted features to learn hash functions to obtain binary codes, which can
be insufficient to optimally represent the information content of RS images. To
overcome this problem, in this paper we introduce a metric-learning based
hashing network, which learns: 1) a semantic-based metric space for effective
feature representation; and 2) compact binary hash codes for fast archive
search. Our network considers an interplay of multiple loss functions that
allows to jointly learn a metric based semantic space facilitating similar
images to be clustered together in that target space and at the same time
producing compact final activations that lose negligible information when
binarized. Experiments carried out on two benchmark RS archives point out that
the proposed network significantly improves the retrieval performance under the
same retrieval time when compared to the state-of-the-art hashing methods in
RS.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/paria2020minimizing/">Minimizing FLOPs to Learn Efficient Sparse Representations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Minimizing FLOPs to Learn Efficient Sparse Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Minimizing FLOPs to Learn Efficient Sparse Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Paria Biswajit, Yeh Chih-kuan, Yen Ian E. H., Xu Ning, Ravikumar Pradeep, Póczos Barnabás</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>14</td>
    <td><p>Deep representation learning has become one of the most widely adopted
approaches for visual search, recommendation, and identification. Retrieval of
such representations from a large database is however computationally
challenging. Approximate methods based on learning compact representations,
have been widely explored for this problem, such as locality sensitive hashing,
product quantization, and PCA. In this work, in contrast to learning compact
representations, we propose to learn high dimensional and sparse
representations that have similar representational capacity as dense embeddings
while being more efficient due to sparse matrix multiplication operations which
can be much faster than dense multiplication. Following the key insight that
the number of operations decreases quadratically with the sparsity of
embeddings provided the non-zero entries are distributed uniformly across
dimensions, we propose a novel approach to learn such distributed sparse
embeddings via the use of a carefully constructed regularization function that
directly minimizes a continuous relaxation of the number of floating-point
operations (FLOPs) incurred during retrieval. Our experiments show that our
approach is competitive to the other baselines and yields a similar or better
speed-vs-accuracy tradeoff on practical datasets.</p>
</td>
    <td>
      
        Locality-Sensitive-Hashing 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Recommender-Systems 
      
        Quantization 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/vaccaro2020image/">Image Retrieval using Multi-scale CNN Features Pooling</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Image Retrieval using Multi-scale CNN Features Pooling' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Image Retrieval using Multi-scale CNN Features Pooling' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Vaccaro Federico, Bertini Marco, Uricchio Tiberio, del Bimbo Alberto</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2020 International Conference on Multimedia Retrieval</td>
    <td>16</td>
    <td><p>In this paper, we address the problem of image retrieval by learning images
representation based on the activations of a Convolutional Neural Network. We
present an end-to-end trainable network architecture that exploits a novel
multi-scale local pooling based on NetVLAD and a triplet mining procedure based
on samples difficulty to obtain an effective image representation. Extensive
experiments show that our approach is able to reach state-of-the-art results on
three standard datasets.</p>
</td>
    <td>
      
        Datasets 
      
        Image-Retrieval 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/morgado2025deep/">Deep Hashing with Hash-Consistent Large Margin Proxy Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Hashing with Hash-Consistent Large Margin Proxy Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Hashing with Hash-Consistent Large Margin Proxy Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Morgado Pedro, Li, Pereira, Saberian, Vasconcelos</td> <!-- 🔧 You were missing this -->
    <td>International Journal of Computer Vision</td>
    <td>6</td>
    <td><p>Image hash codes are produced by binarizing
the embeddings of convolutional neural networks (CNN)
trained for either classification or retrieval. While proxy
embeddings achieve good performance on both tasks,
they are non-trivial to binarize, due to a rotational ambiguity that encourages non-binary embeddings. The use
of a fixed set of proxies (weights of the CNN classification layer) is proposed to eliminate this ambiguity, and
a procedure to design proxy sets that are nearly optimal
for both classification and hashing is introduced. The
resulting hash-consistent large margin (HCLM) proxies
are shown to encourage saturation of hashing units, thus
guaranteeing a small binarization error, while producing
highly discriminative hash-codes. A semantic extension
(sHCLM), aimed to improve hashing performance in
a transfer scenario, is also proposed. Extensive experiments show that sHCLM embeddings achieve significant
improvements over state-of-the-art hashing procedures
on several small and large datasets, both within and
beyond the set of training classes.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Evaluation 
      
        Datasets 
      
        Neural-Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/morgado2020deep/">Deep Hashing with Hash-Consistent Large Margin Proxy Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Hashing with Hash-Consistent Large Margin Proxy Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Hashing with Hash-Consistent Large Margin Proxy Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Morgado Pedro, Li, Pereira, Saberian, Vasconcelos</td> <!-- 🔧 You were missing this -->
    <td>International Journal of Computer Vision</td>
    <td>6</td>
    <td><p>Image hash codes are produced by binarizing
the embeddings of convolutional neural networks (CNN)
trained for either classification or retrieval. While proxy
embeddings achieve good performance on both tasks,
they are non-trivial to binarize, due to a rotational ambiguity that encourages non-binary embeddings. The use
of a fixed set of proxies (weights of the CNN classification layer) is proposed to eliminate this ambiguity, and
a procedure to design proxy sets that are nearly optimal
for both classification and hashing is introduced. The
resulting hash-consistent large margin (HCLM) proxies
are shown to encourage saturation of hashing units, thus
guaranteeing a small binarization error, while producing
highly discriminative hash-codes. A semantic extension
(sHCLM), aimed to improve hashing performance in
a transfer scenario, is also proposed. Extensive experiments show that sHCLM embeddings achieve significant
improvements over state-of-the-art hashing procedures
on several small and large datasets, both within and
beyond the set of training classes.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Evaluation 
      
        Datasets 
      
        Neural-Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/misraa2020multi/">Multi-Modal Retrieval using Graph Neural Networks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multi-Modal Retrieval using Graph Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multi-Modal Retrieval using Graph Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Misraa Aashish Kumar, Kale Ajinkya, Aggarwal Pranav, Aminian Ali</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>Most real world applications of image retrieval such as Adobe Stock, which is
a marketplace for stock photography and illustrations, need a way for users to
find images which are both visually (i.e. aesthetically) and conceptually (i.e.
containing the same salient objects) as a query image. Learning visual-semantic
representations from images is a well studied problem for image retrieval.
Filtering based on image concepts or attributes is traditionally achieved with
index-based filtering (e.g. on textual tags) or by re-ranking after an initial
visual embedding based retrieval. In this paper, we learn a joint vision and
concept embedding in the same high-dimensional space. This joint model gives
the user fine-grained control over the semantics of the result set, allowing
them to explore the catalog of images more rapidly. We model the visual and
concept relationships as a graph structure, which captures the rich information
through node neighborhood. This graph structure helps us learn multi-modal node
embeddings using Graph Neural Networks. We also introduce a novel inference
time control, based on selective neighborhood connectivity allowing the user
control over the retrieval algorithm. We evaluate these multi-modal embeddings
quantitatively on the downstream relevance task of image retrieval on MS-COCO
dataset and qualitatively on MS-COCO and an Adobe Stock dataset.</p>
</td>
    <td>
      
        Datasets 
      
        Hybrid-ANN-Methods 
      
        Re-Ranking 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/yang2025nonlinear/">Nonlinear Robust Discrete Hashing for Cross-Modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Nonlinear Robust Discrete Hashing for Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Nonlinear Robust Discrete Hashing for Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yang Zhan, Long, Zhu, Huang</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>22</td>
    <td><p>Hashing techniques have recently been successfully applied to solve similarity search problems in the information retrieval field because of their significantly reduced storage and high-speed search capabilities. However, the hash codes learned from most recent cross-modal hashing methods lack the ability to comprehensively preserve adequate information, resulting in a less than desirable performance. To solve this limitation, we propose a novel method termed Nonlinear Robust Discrete Hashing (NRDH), for cross-modal retrieval. The main idea behind NRDH is motivated by the success of neural networks, i.e., nonlinear descriptors, in the field of representation learning, and the use of nonlinear descriptors instead of simple linear transformations is more in line with the complex relationships that exist between common latent representation and heterogeneous multimedia data in the real world. In NRDH, we first learn a common latent representation through nonlinear descriptors to encode complementary and consistent information from the features of the heterogeneous multimedia data. Moreover, an asymmetric learning scheme is proposed to correlate the learned hash codes with the common latent representation. Empirically, we demonstrate that NRDH is able to successfully generate a comprehensive common latent representation that significantly improves the quality of the learned hash codes. Then, NRDH adopts a linear learning strategy to fast learn the hash function with the learned hash codes. Extensive experiments performed on two benchmark datasets highlight the superiority of NRDH over several state-of-the-art methods.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Multimodal-Retrieval 
      
        SIGIR 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/yang2020nonlinear/">Nonlinear Robust Discrete Hashing for Cross-Modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Nonlinear Robust Discrete Hashing for Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Nonlinear Robust Discrete Hashing for Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yang Zhan, Long, Zhu, Huang</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>22</td>
    <td><p>Hashing techniques have recently been successfully applied to solve similarity search problems in the information retrieval field because of their significantly reduced storage and high-speed search capabilities. However, the hash codes learned from most recent cross-modal hashing methods lack the ability to comprehensively preserve adequate information, resulting in a less than desirable performance. To solve this limitation, we propose a novel method termed Nonlinear Robust Discrete Hashing (NRDH), for cross-modal retrieval. The main idea behind NRDH is motivated by the success of neural networks, i.e., nonlinear descriptors, in the field of representation learning, and the use of nonlinear descriptors instead of simple linear transformations is more in line with the complex relationships that exist between common latent representation and heterogeneous multimedia data in the real world. In NRDH, we first learn a common latent representation through nonlinear descriptors to encode complementary and consistent information from the features of the heterogeneous multimedia data. Moreover, an asymmetric learning scheme is proposed to correlate the learned hash codes with the common latent representation. Empirically, we demonstrate that NRDH is able to successfully generate a comprehensive common latent representation that significantly improves the quality of the learned hash codes. Then, NRDH adopts a linear learning strategy to fast learn the hash function with the learned hash codes. Extensive experiments performed on two benchmark datasets highlight the superiority of NRDH over several state-of-the-art methods.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Multimodal-Retrieval 
      
        SIGIR 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/matsubara2019target/">Target-Oriented Deformation of Visual-Semantic Embedding Space</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Target-Oriented Deformation of Visual-Semantic Embedding Space' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Target-Oriented Deformation of Visual-Semantic Embedding Space' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Matsubara Takashi</td> <!-- 🔧 You were missing this -->
    <td>IEICE Transactions on Information and Systems</td>
    <td>8</td>
    <td><p>Multimodal embedding is a crucial research topic for cross-modal
understanding, data mining, and translation. Many studies have attempted to
extract representations from given entities and align them in a shared
embedding space. However, because entities in different modalities exhibit
different abstraction levels and modality-specific information, it is
insufficient to embed related entities close to each other. In this study, we
propose the Target-Oriented Deformation Network (TOD-Net), a novel module that
continuously deforms the embedding space into a new space under a given
condition, thereby adjusting similarities between entities. Unlike methods
based on cross-modal attention, TOD-Net is a post-process applied to the
embedding space learned by existing embedding systems and improves their
performances of retrieval. In particular, when combined with cutting-edge
models, TOD-Net gains the state-of-the-art cross-modal retrieval model
associated with the MSCOCO dataset. Qualitative analysis reveals that TOD-Net
successfully emphasizes entity-specific concepts and retrieves diverse targets
via handling higher levels of diversity than existing models.</p>
</td>
    <td>
      
        Datasets 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/wang2019cluster/">Cluster-wise Unsupervised Hashing for Cross-Modal Similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cluster-wise Unsupervised Hashing for Cross-Modal Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cluster-wise Unsupervised Hashing for Cross-Modal Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Lu, Yang Jie</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition</td>
    <td>25</td>
    <td><p>Large-scale cross-modal hashing similarity retrieval has attracted more and
more attention in modern search applications such as search engines and
autopilot, showing great superiority in computation and storage. However,
current unsupervised cross-modal hashing methods still have some limitations:
(1)many methods relax the discrete constraints to solve the optimization
objective which may significantly degrade the retrieval performance;(2)most
existing hashing model project heterogenous data into a common latent space,
which may always lose sight of diversity in heterogenous data;(3)transforming
real-valued data point to binary codes always results in abundant loss of
information, producing the suboptimal continuous latent space. To overcome
above problems, in this paper, a novel Cluster-wise Unsupervised Hashing (CUH)
method is proposed. Specifically, CUH jointly performs the multi-view
clustering that projects the original data points from different modalities
into its own low-dimensional latent semantic space and finds the cluster
centroid points and the common clustering indicators in its own low-dimensional
space, and learns the compact hash codes and the corresponding linear hash
functions. An discrete optimization framework is developed to learn the unified
binary codes across modalities under the guidance cluster-wise code-prototypes.
The reasonableness and effectiveness of CUH is well demonstrated by
comprehensive experiments on diverse benchmark datasets.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Scalability 
      
        Evaluation 
      
        Datasets 
      
        Unsupervised 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Tools-&-Libraries 
      
        Neural-Hashing 
      
        Supervised 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/wang2020deep/">Deep Collaborative Discrete Hashing with Semantic-Invariant Structure</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Collaborative Discrete Hashing with Semantic-Invariant Structure' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Collaborative Discrete Hashing with Semantic-Invariant Structure' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Zijian, Zhang, Huang</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>9</td>
    <td><p>Existing deep hashing approaches fail to fully explore semantic correlations and neglect the effect of linguistic context on visual attention learning, leading to inferior performance. This paper proposes a dual-stream learning framework, dubbed Deep Collaborative Discrete Hashing (DCDH), which constructs a discriminative common discrete space by collaboratively incorporating the shared and individual semantics deduced from visual features and semantic labels. Specifically, the context-aware representations are generated by employing the outer product of visual embeddings and semantic encodings. Moreover, we reconstruct the labels and introduce the focal loss to take advantage of frequent and rare concepts. The common binary code space is built on the joint learning of the visual representations attended by language, the semantic-invariant structure construction and the label distribution correction. Extensive experiments demonstrate the superiority of our method.</p>
</td>
    <td>
      
        Neural-Hashing 
      
        Tools-&-Libraries 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/maheshwari2020learning/">Learning Colour Representations of Search Queries</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Colour Representations of Search Queries' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Colour Representations of Search Queries' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Maheshwari Paridhi, Ghuhan Manoj, Vinay Vishwa</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>5</td>
    <td><p>Image search engines rely on appropriately designed ranking features that
capture various aspects of the content semantics as well as the historic
popularity. In this work, we consider the role of colour in this relevance
matching process. Our work is motivated by the observation that a significant
fraction of user queries have an inherent colour associated with them. While
some queries contain explicit colour mentions (such as ‘black car’ and ‘yellow
daisies’), other queries have implicit notions of colour (such as ‘sky’ and
‘grass’). Furthermore, grounding queries in colour is not a mapping to a single
colour, but a distribution in colour space. For instance, a search for ‘trees’
tends to have a bimodal distribution around the colours green and brown. We
leverage historical clickthrough data to produce a colour representation for
search queries and propose a recurrent neural network architecture to encode
unseen queries into colour space. We also show how this embedding can be learnt
alongside a cross-modal relevance ranker from impression logs where a subset of
the result images were clicked. We demonstrate that the use of a query-image
colour distance feature leads to an improvement in the ranker performance as
measured by users’ preferences of clicked versus skipped images.</p>
</td>
    <td>
      
        SIGIR 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/manandhar2019semantic/">Semantic Granularity Metric Learning for Visual Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semantic Granularity Metric Learning for Visual Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semantic Granularity Metric Learning for Visual Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Manandhar Dipu, Bastan Muhammet, Yap Kim-hui</td> <!-- 🔧 You were missing this -->
    <td>Journal of Visual Communication and Image Representation</td>
    <td>10</td>
    <td><p>Deep metric learning applied to various applications has shown promising
results in identification, retrieval and recognition. Existing methods often do
not consider different granularity in visual similarity. However, in many
domain applications, images exhibit similarity at multiple granularities with
visual semantic concepts, e.g. fashion demonstrates similarity ranging from
clothing of the exact same instance to similar looks/design or a common
category. Therefore, training image triplets/pairs used for metric learning
inherently possess different degree of information. However, the existing
methods often treats them with equal importance during training. This hinders
capturing the underlying granularities in feature similarity required for
effective visual search.
  In view of this, we propose a new deep semantic granularity metric learning
(SGML) that develops a novel idea of leveraging attribute semantic space to
capture different granularity of similarity, and then integrate this
information into deep metric learning. The proposed method simultaneously
learns image attributes and embeddings using multitask CNNs. The two tasks are
not only jointly optimized but are further linked by the semantic granularity
similarity mappings to leverage the correlations between the tasks. To this
end, we propose a new soft-binomial deviance loss that effectively integrates
the degree of information in training samples, which helps to capture visual
similarity at multiple granularities. Compared to recent ensemble-based
methods, our framework is conceptually elegant, computationally simple and
provides better performance. We perform extensive experiments on benchmark
metric learning datasets and demonstrate that our method outperforms recent
state-of-the-art methods, e.g., 1-4.5% improvement in Recall@1 over the
previous state-of-the-arts [1],[2] on DeepFashion In-Shop dataset.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/mafla2020fine/">Fine-grained Image Classification and Retrieval by Combining Visual and Locally Pooled Textual Features</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fine-grained Image Classification and Retrieval by Combining Visual and Locally Pooled Textual Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fine-grained Image Classification and Retrieval by Combining Visual and Locally Pooled Textual Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mafla Andres, Dey Sounak, Biten Ali Furkan, Gomez Lluis, Karatzas Dimosthenis</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>30</td>
    <td><p>Text contained in an image carries high-level semantics that can be exploited
to achieve richer image understanding. In particular, the mere presence of text
provides strong guiding content that should be employed to tackle a diversity
of computer vision tasks such as image retrieval, fine-grained classification,
and visual question answering. In this paper, we address the problem of
fine-grained classification and image retrieval by leveraging textual
information along with visual cues to comprehend the existing intrinsic
relation between the two modalities. The novelty of the proposed model consists
of the usage of a PHOC descriptor to construct a bag of textual words along
with a Fisher Vector Encoding that captures the morphology of text. This
approach provides a stronger multimodal representation for this task and as our
experiments demonstrate, it achieves state-of-the-art results on two different
tasks, fine-grained classification and image retrieval.</p>
</td>
    <td>
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/yokoo2020two/">Two-stage Discriminative Re-ranking for Large-scale Landmark Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Two-stage Discriminative Re-ranking for Large-scale Landmark Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Two-stage Discriminative Re-ranking for Large-scale Landmark Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yokoo Shuhei, Ozaki Kohei, Simo-serra Edgar, Iizuka Satoshi</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>20</td>
    <td><p>We propose an efficient pipeline for large-scale landmark image retrieval
that addresses the diversity of the dataset through two-stage discriminative
re-ranking. Our approach is based on embedding the images in a feature-space
using a convolutional neural network trained with a cosine softmax loss. Due to
the variance of the images, which include extreme viewpoint changes such as
having to retrieve images of the exterior of a landmark from images of the
interior, this is very challenging for approaches based exclusively on visual
similarity. Our proposed re-ranking approach improves the results in two steps:
in the sort-step, \(k\)-nearest neighbor search with soft-voting to sort the
retrieved results based on their label similarity to the query images, and in
the insert-step, we add additional samples from the dataset that were not
retrieved by image-similarity. This approach allows overcoming the low visual
diversity in retrieved images. In-depth experimental results show that the
proposed approach significantly outperforms existing approaches on the
challenging Google Landmarks Datasets. Using our methods, we achieved 1st place
in the Google Landmark Retrieval 2019 challenge and 3rd place in the Google
Landmark Recognition 2019 challenge on Kaggle. Our code is publicly available
here: https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Datasets 
      
        CVPR 
      
        Hybrid-ANN-Methods 
      
        Re-Ranking 
      
        Scalability 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/yang2020tree/">Tree-Augmented Cross-Modal Encoding for Complex-Query Video Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Tree-Augmented Cross-Modal Encoding for Complex-Query Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Tree-Augmented Cross-Modal Encoding for Complex-Query Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yang Xun, Dong Jianfeng, Cao Yixin, Wang Xun, Wang Meng, Chua Tat-seng</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>123</td>
    <td><p>The rapid growth of user-generated videos on the Internet has intensified the
need for text-based video retrieval systems. Traditional methods mainly favor
the concept-based paradigm on retrieval with simple queries, which are usually
ineffective for complex queries that carry far more complex semantics.
Recently, embedding-based paradigm has emerged as a popular approach. It aims
to map the queries and videos into a shared embedding space where
semantically-similar texts and videos are much closer to each other. Despite
its simplicity, it forgoes the exploitation of the syntactic structure of text
queries, making it suboptimal to model the complex queries.
  To facilitate video retrieval with complex queries, we propose a
Tree-augmented Cross-modal Encoding method by jointly learning the linguistic
structure of queries and the temporal representation of videos. Specifically,
given a complex user query, we first recursively compose a latent semantic tree
to structurally describe the text query. We then design a tree-augmented query
encoder to derive structure-aware query representation and a temporal attentive
video encoder to model the temporal characteristics of videos. Finally, both
the query and videos are mapped into a joint embedding space for matching and
ranking. In this approach, we have a better understanding and modeling of the
complex queries, thereby achieving a better video retrieval performance.
Extensive experiments on large scale video retrieval benchmark datasets
demonstrate the effectiveness of our approach.</p>
</td>
    <td>
      
        Datasets 
      
        Video-Retrieval 
      
        SIGIR 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/luo2018collaborative/">Collaborative Learning for Extremely Low Bit Asymmetric Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Collaborative Learning for Extremely Low Bit Asymmetric Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Collaborative Learning for Extremely Low Bit Asymmetric Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Luo Yadan, Huang Zi, Li Yang, Shen Fumin, Yang Yang, Cui Peng</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Knowledge and Data Engineering</td>
    <td>14</td>
    <td><p>Hashing techniques are in great demand for a wide range of real-world
applications such as image retrieval and network compression. Nevertheless,
existing approaches could hardly guarantee a satisfactory performance with the
extremely low-bit (e.g., 4-bit) hash codes due to the severe information loss
and the shrink of the discrete solution space. In this paper, we propose a
novel \textit{Collaborative Learning} strategy that is tailored for generating
high-quality low-bit hash codes. The core idea is to jointly distill
bit-specific and informative representations for a group of pre-defined code
lengths. The learning of short hash codes among the group can benefit from the
manifold shared with other long codes, where multiple views from different hash
codes provide the supplementary guidance and regularization, making the
convergence faster and more stable. To achieve that, an asymmetric hashing
framework with two variants of multi-head embedding structures is derived,
termed as Multi-head Asymmetric Hashing (MAH), leading to great efficiency of
training and querying. Extensive experiments on three benchmark datasets have
been conducted to verify the superiority of the proposed MAH, and have shown
that the 8-bit hash codes generated by MAH achieve \(94.3%\) of the MAP (Mean
Average Precision (MAP)) score on the CIFAR-10 dataset, which significantly
surpasses the performance of the 48-bit codes by the state-of-the-arts in image
retrieval tasks.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/xu2020multi/">Multi-Feature Discrete Collaborative Filtering for Fast Cold-start Recommendation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multi-Feature Discrete Collaborative Filtering for Fast Cold-start Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multi-Feature Discrete Collaborative Filtering for Fast Cold-start Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xu Yang, Zhu Lei, Cheng Zhiyong, Li Jingjing, Sun Jiande</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>21</td>
    <td><p>Hashing is an effective technique to address the large-scale recommendation
problem, due to its high computation and storage efficiency on calculating the
user preferences on items. However, existing hashing-based recommendation
methods still suffer from two important problems: 1) Their recommendation
process mainly relies on the user-item interactions and single specific content
feature. When the interaction history or the content feature is unavailable
(the cold-start problem), their performance will be seriously deteriorated. 2)
Existing methods learn the hash codes with relaxed optimization or adopt
discrete coordinate descent to directly solve binary hash codes, which results
in significant quantization loss or consumes considerable computation time. In
this paper, we propose a fast cold-start recommendation method, called
Multi-Feature Discrete Collaborative Filtering (MFDCF), to solve these
problems. Specifically, a low-rank self-weighted multi-feature fusion module is
designed to adaptively project the multiple content features into binary yet
informative hash codes by fully exploiting their complementarity. Additionally,
we develop a fast discrete optimization algorithm to directly compute the
binary hash codes with simple operations. Experiments on two public
recommendation datasets demonstrate that MFDCF outperforms the
state-of-the-arts on various aspects.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        Recommender-Systems 
      
        Quantization 
      
        AAAI 
      
        Scalability 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/wang2020consensus/">Consensus-Aware Visual-Semantic Embedding for Image-Text Matching</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Consensus-Aware Visual-Semantic Embedding for Image-Text Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Consensus-Aware Visual-Semantic Embedding for Image-Text Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Haoran, Zhang Ying, Ji Zhong, Pang Yanwei, Ma Lin</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>149</td>
    <td><p>Image-text matching plays a central role in bridging vision and language.
Most existing approaches only rely on the image-text instance pair to learn
their representations, thereby exploiting their matching relationships and
making the corresponding alignments. Such approaches only exploit the
superficial associations contained in the instance pairwise data, with no
consideration of any external commonsense knowledge, which may hinder their
capabilities to reason the higher-level relationships between image and text.
In this paper, we propose a Consensus-aware Visual-Semantic Embedding (CVSE)
model to incorporate the consensus information, namely the commonsense
knowledge shared between both modalities, into image-text matching.
Specifically, the consensus information is exploited by computing the
statistical co-occurrence correlations between the semantic concepts from the
image captioning corpus and deploying the constructed concept correlation graph
to yield the consensus-aware concept (CAC) representations. Afterwards, CVSE
learns the associations and alignments between image and text based on the
exploited consensus as well as the instance-level representations for both
modalities. Extensive experiments conducted on two public datasets verify that
the exploited consensus makes significant contributions to constructing more
meaningful visual-semantic embeddings, with the superior performances over the
state-of-the-art approaches on the bidirectional image and text retrieval task.
Our code of this paper is available at: https://github.com/BruceW91/CVSE.</p>
</td>
    <td>
      
        Datasets 
      
        Text-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/liu2025joint/">Joint-modal Distribution-based Similarity Hashing for Large-scale Unsupervised Deep Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Joint-modal Distribution-based Similarity Hashing for Large-scale Unsupervised Deep Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Joint-modal Distribution-based Similarity Hashing for Large-scale Unsupervised Deep Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Song, Qian, Guan, Zhan, Ying</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>148</td>
    <td><p>Hashing-based cross-modal search which aims to map multiple modality features into binary codes has attracted increasingly attention due to its storage and search efficiency especially in large-scale database retrieval. Recent unsupervised deep cross-modal hashing methods have shown promising results. However, existing approaches typically suffer from two limitations: (1) They usually learn cross-modal similarity information separately or in a redundant fusion manner, which may fail to capture semantic correlations among instances from different modalities sufficiently and effectively. (2) They seldom consider the sampling and weighting schemes for unsupervised cross-modal hashing, resulting in the lack of satisfactory discriminative ability in hash codes. To overcome these limitations, we propose a novel unsupervised deep cross-modal hashing method called Joint-modal Distribution-based Similarity Hashing (JDSH) for large-scale cross-modal retrieval. Firstly, we propose a novel cross-modal joint-training method by constructing a joint-modal similarity matrix to fully preserve the cross-modal semantic correlations among instances. Secondly, we propose a sampling and weighting scheme termed the Distribution-based Similarity Decision and Weighting (DSDW) method for unsupervised cross-modal hashing, which is able to generate more discriminative hash codes by pushing semantic similar instance pairs closer and pulling semantic dissimilar instance pairs apart. The experimental results demonstrate the superiority of JDSH compared with several unsupervised cross-modal hashing methods on two public datasets NUS-WIDE and MIRFlickr.</p>
</td>
    <td>
      
        Scalability 
      
        Efficiency 
      
        Datasets 
      
        Multimodal-Retrieval 
      
        SIGIR 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/liu2025model/">Model Optimization Boosting Framework for Linear Model Hash Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Model Optimization Boosting Framework for Linear Model Hash Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Model Optimization Boosting Framework for Linear Model Hash Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Xingbo, Nie, Zhou, Nie, Yin</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>23</td>
    <td><p>Efficient hashing techniques have attracted extensive research interests in both storage and retrieval of high dimensional data, such as images and videos. In existing hashing methods, a linear model is commonly utilized owing to its efficiency. To obtain better accuracy, linear-based hashing methods focus on designing a generalized linear objective function with different constraints or penalty terms that consider the inherent characteristics and neighborhood information of samples. Differing from existing hashing methods, in this study, we propose a self-improvement framework called Model Boost (MoBoost) to improve model parameter optimization for linear-based hashing methods without adding new constraints or penalty terms. In the proposed MoBoost, for a linear-based hashing method, we first repeatedly execute the hashing method to obtain several hash codes to training samples. Then, utilizing two novel fusion strategies, these codes are fused into a single set. We also propose two new criteria to evaluate the goodness of hash bits during the fusion process. Based on the fused set of hash codes, we learn new parameters for the linear hash function that can significantly improve the accuracy. In general, the proposed MoBoost can be adopted by existing linear-based hashing methods, achieving more precise and stable performance compared to the original methods, and adopting the proposed MoBoost will incur negligible time and space costs. To evaluate the proposed MoBoost, we performed extensive experiments on four benchmark datasets, and the results demonstrate superior performance.</p>
</td>
    <td>
      
        Efficiency 
      
        Datasets 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/liu2020joint/">Joint-modal Distribution-based Similarity Hashing for Large-scale Unsupervised Deep Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Joint-modal Distribution-based Similarity Hashing for Large-scale Unsupervised Deep Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Joint-modal Distribution-based Similarity Hashing for Large-scale Unsupervised Deep Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Song, Qian, Guan, Zhan, Ying</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>148</td>
    <td><p>Hashing-based cross-modal search which aims to map multiple modality features into binary codes has attracted increasingly attention due to its storage and search efficiency especially in large-scale database retrieval. Recent unsupervised deep cross-modal hashing methods have shown promising results. However, existing approaches typically suffer from two limitations: (1) They usually learn cross-modal similarity information separately or in a redundant fusion manner, which may fail to capture semantic correlations among instances from different modalities sufficiently and effectively. (2) They seldom consider the sampling and weighting schemes for unsupervised cross-modal hashing, resulting in the lack of satisfactory discriminative ability in hash codes. To overcome these limitations, we propose a novel unsupervised deep cross-modal hashing method called Joint-modal Distribution-based Similarity Hashing (JDSH) for large-scale cross-modal retrieval. Firstly, we propose a novel cross-modal joint-training method by constructing a joint-modal similarity matrix to fully preserve the cross-modal semantic correlations among instances. Secondly, we propose a sampling and weighting scheme termed the Distribution-based Similarity Decision and Weighting (DSDW) method for unsupervised cross-modal hashing, which is able to generate more discriminative hash codes by pushing semantic similar instance pairs closer and pulling semantic dissimilar instance pairs apart. The experimental results demonstrate the superiority of JDSH compared with several unsupervised cross-modal hashing methods on two public datasets NUS-WIDE and MIRFlickr.</p>
</td>
    <td>
      
        Scalability 
      
        Efficiency 
      
        Datasets 
      
        Multimodal-Retrieval 
      
        SIGIR 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/liu2020model/">Model Optimization Boosting Framework for Linear Model Hash Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Model Optimization Boosting Framework for Linear Model Hash Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Model Optimization Boosting Framework for Linear Model Hash Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Xingbo, Nie, Zhou, Nie, Yin</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>23</td>
    <td><p>Efficient hashing techniques have attracted extensive research interests in both storage and retrieval of high dimensional data, such as images and videos. In existing hashing methods, a linear model is commonly utilized owing to its efficiency. To obtain better accuracy, linear-based hashing methods focus on designing a generalized linear objective function with different constraints or penalty terms that consider the inherent characteristics and neighborhood information of samples. Differing from existing hashing methods, in this study, we propose a self-improvement framework called Model Boost (MoBoost) to improve model parameter optimization for linear-based hashing methods without adding new constraints or penalty terms. In the proposed MoBoost, for a linear-based hashing method, we first repeatedly execute the hashing method to obtain several hash codes to training samples. Then, utilizing two novel fusion strategies, these codes are fused into a single set. We also propose two new criteria to evaluate the goodness of hash bits during the fusion process. Based on the fused set of hash codes, we learn new parameters for the linear hash function that can significantly improve the accuracy. In general, the proposed MoBoost can be adopted by existing linear-based hashing methods, achieving more precise and stable performance compared to the original methods, and adopting the proposed MoBoost will incur negligible time and space costs. To evaluate the proposed MoBoost, we performed extensive experiments on four benchmark datasets, and the results demonstrate superior performance.</p>
</td>
    <td>
      
        Efficiency 
      
        Datasets 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/liu2020reinforcing/">Reinforcing Short-Length Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Reinforcing Short-Length Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Reinforcing Short-Length Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Xingbo, Nie Xiushan, Dai Qi, Huang Yupan, Yin Yilong</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Circuits and Systems for Video Technology</td>
    <td>28</td>
    <td><p>Due to the compelling efficiency in retrieval and storage,
similarity-preserving hashing has been widely applied to approximate nearest
neighbor search in large-scale image retrieval. However, existing methods have
poor performance in retrieval using an extremely short-length hash code due to
weak ability of classification and poor distribution of hash bit. To address
this issue, in this study, we propose a novel reinforcing short-length hashing
(RSLH). In this proposed RSLH, mutual reconstruction between the hash
representation and semantic labels is performed to preserve the semantic
information. Furthermore, to enhance the accuracy of hash representation, a
pairwise similarity matrix is designed to make a balance between accuracy and
training expenditure on memory. In addition, a parameter boosting strategy is
integrated to reinforce the precision with hash bits fusion. Extensive
experiments on three large-scale image benchmarks demonstrate the superior
performance of RSLH under various short-length hashing scenarios.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Scalability 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/vanblokland2020indexing/">An Indexing Scheme and Descriptor for 3D Object Retrieval Based on Local Shape Querying</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=An Indexing Scheme and Descriptor for 3D Object Retrieval Based on Local Shape Querying' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=An Indexing Scheme and Descriptor for 3D Object Retrieval Based on Local Shape Querying' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>van Blokland Bart Iver, Theoharis Theoharis</td> <!-- 🔧 You were missing this -->
    <td>Computers &amp; Graphics</td>
    <td>14</td>
    <td><p>A binary descriptor indexing scheme based on Hamming distance called the
Hamming tree for local shape queries is presented. A new binary clutter
resistant descriptor named Quick Intersection Count Change Image (QUICCI) is
also introduced. This local shape descriptor is extremely small and fast to
compare. Additionally, a novel distance function called Weighted Hamming
applicable to QUICCI images is proposed for retrieval applications. The
effectiveness of the indexing scheme and QUICCI is demonstrated on 828 million
QUICCI images derived from the SHREC2017 dataset, while the clutter resistance
of QUICCI is shown using the clutterbox experiment.</p>
</td>
    <td>
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/tu2019deep/">Deep Cross-Modal Hashing with Hashing Functions and Unified Hash Codes Jointly Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Cross-Modal Hashing with Hashing Functions and Unified Hash Codes Jointly Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Cross-Modal Hashing with Hashing Functions and Unified Hash Codes Jointly Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tu Rong-cheng, Mao Xian-ling, Ma Bing, Hu Yong, Yan Tan, Wei Wei, Huang Heyan</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Knowledge and Data Engineering</td>
    <td>62</td>
    <td><p>Due to their high retrieval efficiency and low storage cost, cross-modal
hashing methods have attracted considerable attention. Generally, compared with
shallow cross-modal hashing methods, deep cross-modal hashing methods can
achieve a more satisfactory performance by integrating feature learning and
hash codes optimizing into a same framework. However, most existing deep
cross-modal hashing methods either cannot learn a unified hash code for the two
correlated data-points of different modalities in a database instance or cannot
guide the learning of unified hash codes by the feedback of hashing function
learning procedure, to enhance the retrieval accuracy. To address the issues
above, in this paper, we propose a novel end-to-end Deep Cross-Modal Hashing
with Hashing Functions and Unified Hash Codes Jointly Learning (DCHUC).
Specifically, by an iterative optimization algorithm, DCHUC jointly learns
unified hash codes for image-text pairs in a database and a pair of hash
functions for unseen query image-text pairs. With the iterative optimization
algorithm, the learned unified hash codes can be used to guide the hashing
function learning procedure; Meanwhile, the learned hashing functions can
feedback to guide the unified hash codes optimizing procedure. Extensive
experiments on three public datasets demonstrate that the proposed method
outperforms the state-of-the-art cross-modal hashing methods.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Datasets 
      
        Memory-Efficiency 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/shi2019compositional/">Compositional Embeddings Using Complementary Partitions for Memory-Efficient Recommendation Systems</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Compositional Embeddings Using Complementary Partitions for Memory-Efficient Recommendation Systems' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Compositional Embeddings Using Complementary Partitions for Memory-Efficient Recommendation Systems' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shi Hao-jun Michael, Mudigere Dheevatsa, Naumov Maxim, Yang Jiyan</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</td>
    <td>39</td>
    <td><p>Modern deep learning-based recommendation systems exploit hundreds to
thousands of different categorical features, each with millions of different
categories ranging from clicks to posts. To respect the natural diversity
within the categorical data, embeddings map each category to a unique dense
representation within an embedded space. Since each categorical feature could
take on as many as tens of millions of different possible categories, the
embedding tables form the primary memory bottleneck during both training and
inference. We propose a novel approach for reducing the embedding size in an
end-to-end fashion by exploiting complementary partitions of the category set
to produce a unique embedding vector for each category without explicit
definition. By storing multiple smaller embedding tables based on each
complementary partition and combining embeddings from each table, we define a
unique embedding for each category at smaller memory cost. This approach may be
interpreted as using a specific fixed codebook to ensure uniqueness of each
category’s representation. Our experimental results demonstrate the
effectiveness of our approach over the hashing trick for reducing the size of
the embedding tables in terms of model loss and accuracy, while retaining a
similar reduction in the number of parameters.</p>
</td>
    <td>
      
        KDD 
      
        Hashing-Methods 
      
        Recommender-Systems 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/shi2018scalable/">A Scalable Optimization Mechanism for Pairwise based Discrete Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Scalable Optimization Mechanism for Pairwise based Discrete Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Scalable Optimization Mechanism for Pairwise based Discrete Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shi Xiaoshuang, Xing Fuyong, Zhang Zizhao, Sapkota Manish, Guo Zhenhua, Yang Lin</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>11</td>
    <td><p>Maintaining the pair similarity relationship among originally
high-dimensional data into a low-dimensional binary space is a popular strategy
to learn binary codes. One simiple and intutive method is to utilize two
identical code matrices produced by hash functions to approximate a pairwise
real label matrix. However, the resulting quartic problem is difficult to
directly solve due to the non-convex and non-smooth nature of the objective. In
this paper, unlike previous optimization methods using various relaxation
strategies, we aim to directly solve the original quartic problem using a novel
alternative optimization mechanism to linearize the quartic problem by
introducing a linear regression model. Additionally, we find that gradually
learning each batch of binary codes in a sequential mode, i.e. batch by batch,
is greatly beneficial to the convergence of binary code learning. Based on this
significant discovery and the proposed strategy, we introduce a scalable
symmetric discrete hashing algorithm that gradually and smoothly updates each
batch of binary codes. To further improve the smoothness, we also propose a
greedy symmetric discrete hashing algorithm to update each bit of batch binary
codes. Moreover, we extend the proposed optimization mechanism to solve the
non-convex optimization problems for binary code learning in many other
pairwise based hashing algorithms. Extensive experiments on benchmark
single-label and multi-label databases demonstrate the superior performance of
the proposed mechanism over recent state-of-the-art methods.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Compact-Codes 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/shen2020auto/">Auto-Encoding Twin-Bottleneck Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Auto-Encoding Twin-Bottleneck Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Auto-Encoding Twin-Bottleneck Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shen Yuming, Qin, Chen, Yu, Liu, Zhu, Shen, Shao</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>113</td>
    <td><p>Conventional unsupervised hashing methods usually take advantage of similarity graphs, which are either pre-computed in the high-dimensional space or obtained from random anchor points. On the one hand, existing methods uncouple the procedures of hash function learning and graph construction. On the other hand, graphs empirically built upon original data could introduce biased prior knowledge of data relevance, leading to sub-optimal retrieval performance. In this paper, we tackle the above problems by proposing an efficient and adaptive code-driven graph, which is updated by decoding in the context of an auto-encoder. Specifically, we introduce into our framework twin bottlenecks (i.e., latent variables) that exchange crucial information collaboratively. One bottleneck (i.e., binary codes) conveys the high-level intrinsic data structure captured by the code-driven graph to the other (i.e., continuous variables for low-level detail information), which in turn propagates the updated network feedback for the encoder to learn more discriminative binary codes. The auto-encoding learning objective literally rewards the code-driven graph to learn an optimal encoder. Moreover, the proposed model can be simply optimized by gradient descent without violating the binary constraints. Experiments on benchmarked datasets clearly show the superiority of our framework over the state-of-the-art hashing methods.</p>
</td>
    <td>
      
        Datasets 
      
        CVPR 
      
        Neural-Hashing 
      
        Tools-&-Libraries 
      
        Supervised 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Unsupervised 
      
        Graph-Based-ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/shen2025auto/">Auto-Encoding Twin-Bottleneck Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Auto-Encoding Twin-Bottleneck Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Auto-Encoding Twin-Bottleneck Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shen Yuming, Qin, Chen, Yu, Liu, Zhu, Shen, Shao</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>113</td>
    <td><p>Conventional unsupervised hashing methods usually take advantage of similarity graphs, which are either pre-computed in the high-dimensional space or obtained from random anchor points. On the one hand, existing methods uncouple the procedures of hash function learning and graph construction. On the other hand, graphs empirically built upon original data could introduce biased prior knowledge of data relevance, leading to sub-optimal retrieval performance. In this paper, we tackle the above problems by proposing an efficient and adaptive code-driven graph, which is updated by decoding in the context of an auto-encoder. Specifically, we introduce into our framework twin bottlenecks (i.e., latent variables) that exchange crucial information collaboratively. One bottleneck (i.e., binary codes) conveys the high-level intrinsic data structure captured by the code-driven graph to the other (i.e., continuous variables for low-level detail information), which in turn propagates the updated network feedback for the encoder to learn more discriminative binary codes. The auto-encoding learning objective literally rewards the code-driven graph to learn an optimal encoder. Moreover, the proposed model can be simply optimized by gradient descent without violating the binary constraints. Experiments on benchmarked datasets clearly show the superiority of our framework over the state-of-the-art hashing methods.</p>
</td>
    <td>
      
        Datasets 
      
        CVPR 
      
        Neural-Hashing 
      
        Tools-&-Libraries 
      
        Supervised 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Unsupervised 
      
        Graph-Based-ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/schroeder2020structured/">Structured Query-Based Image Retrieval Using Scene Graphs</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Structured Query-Based Image Retrieval Using Scene Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Structured Query-Based Image Retrieval Using Scene Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Schroeder Brigit, Tripathi Subarna</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>49</td>
    <td><p>A structured query can capture the complexity of object interactions (e.g.
‘woman rides motorcycle’) unlike single objects (e.g. ‘woman’ or ‘motorcycle’).
Retrieval using structured queries therefore is much more useful than single
object retrieval, but a much more challenging problem. In this paper we present
a method which uses scene graph embeddings as the basis for an approach to
image retrieval. We examine how visual relationships, derived from scene
graphs, can be used as structured queries. The visual relationships are
directed subgraphs of the scene graph with a subject and object as nodes
connected by a predicate relationship. Notably, we are able to achieve high
recall even on low to medium frequency objects found in the long-tailed
COCO-Stuff dataset, and find that adding a visual relationship-inspired loss
boosts our recall by 10% in the best case.</p>
</td>
    <td>
      
        Datasets 
      
        CVPR 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/talreja2020deep/">Deep Hashing for Secure Multimodal Biometrics</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Hashing for Secure Multimodal Biometrics' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Hashing for Secure Multimodal Biometrics' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Talreja Veeru, Valenti Matthew, Nasrabadi Nasser</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Information Forensics and Security</td>
    <td>24</td>
    <td><p>When compared to unimodal systems, multimodal biometric systems have several
advantages, including lower error rate, higher accuracy, and larger population
coverage. However, multimodal systems have an increased demand for integrity
and privacy because they must store multiple biometric traits associated with
each user. In this paper, we present a deep learning framework for
feature-level fusion that generates a secure multimodal template from each
user’s face and iris biometrics. We integrate a deep hashing (binarization)
technique into the fusion architecture to generate a robust binary multimodal
shared latent representation. Further, we employ a hybrid secure architecture
by combining cancelable biometrics with secure sketch techniques and integrate
it with a deep hashing framework, which makes it computationally prohibitive to
forge a combination of multiple biometrics that pass the authentication. The
efficacy of the proposed approach is shown using a multimodal database of face
and iris and it is observed that the matching performance is improved due to
the fusion of multiple biometrics. Furthermore, the proposed approach also
provides cancelability and unlinkability of the templates along with improved
privacy of the biometric data. Additionally, we also test the proposed hashing
function for an image retrieval application using a benchmark dataset. The main
goal of this paper is to develop a method for integrating multimodal fusion,
deep hashing, and biometric security, with an emphasis on structural data from
modalities like face and iris. The proposed approach is in no way a general
biometric security framework that can be applied to all biometric modalities,
as further research is needed to extend the proposed framework to other
unconstrained biometric modalities.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/tan2020learning/">Learning to Hash with Graph Neural Networks for Recommender Systems</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning to Hash with Graph Neural Networks for Recommender Systems' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning to Hash with Graph Neural Networks for Recommender Systems' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tan Qiaoyu, Liu Ninghao, Zhao Xing, Yang Hongxia, Zhou Jingren, Hu Xia</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of The Web Conference 2020</td>
    <td>77</td>
    <td><p>Graph representation learning has attracted much attention in supporting high
quality candidate search at scale. Despite its effectiveness in learning
embedding vectors for objects in the user-item interaction network, the
computational costs to infer users’ preferences in continuous embedding space
are tremendous. In this work, we investigate the problem of hashing with graph
neural networks (GNNs) for high quality retrieval, and propose a simple yet
effective discrete representation learning framework to jointly learn
continuous and discrete codes. Specifically, a deep hashing with GNNs (HashGNN)
is presented, which consists of two components, a GNN encoder for learning node
representations, and a hash layer for encoding representations to hash codes.
The whole architecture is trained end-to-end by jointly optimizing two losses,
i.e., reconstruction loss from reconstructing observed links, and ranking loss
from preserving the relative ordering of hash codes. A novel discrete
optimization strategy based on straight through estimator (STE) with guidance
is proposed. The principal idea is to avoid gradient magnification in
back-propagation of STE with continuous embedding guidance, in which we begin
from learning an easier network that mimic the continuous embedding and let it
evolve during the training until it finally goes back to STE. Comprehensive
experiments over three publicly available and one real-world Alibaba company
datasets demonstrate that our model not only can achieve comparable performance
compared with its continuous counterpart but also runs multiple times faster
during inference.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Datasets 
      
        Recommender-Systems 
      
        Neural-Hashing 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/taherkhani2020error/">Error-Corrected Margin-Based Deep Cross-Modal Hashing for Facial Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Error-Corrected Margin-Based Deep Cross-Modal Hashing for Facial Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Error-Corrected Margin-Based Deep Cross-Modal Hashing for Facial Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Taherkhani Fariborz, Talreja Veeru, Valenti Matthew C., Nasrabadi Nasser M.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Biometrics, Behavior, and Identity Science</td>
    <td>14</td>
    <td><p>Cross-modal hashing facilitates mapping of heterogeneous multimedia data into
a common Hamming space, which can beutilized for fast and flexible retrieval
across different modalities. In this paper, we propose a novel cross-modal
hashingarchitecture-deep neural decoder cross-modal hashing (DNDCMH), which
uses a binary vector specifying the presence of certainfacial attributes as an
input query to retrieve relevant face images from a database. The DNDCMH
network consists of two separatecomponents: an attribute-based deep cross-modal
hashing (ADCMH) module, which uses a margin (m)-based loss function
toefficiently learn compact binary codes to preserve similarity between
modalities in the Hamming space, and a neural error correctingdecoder (NECD),
which is an error correcting decoder implemented with a neural network. The
goal of NECD network in DNDCMH isto error correct the hash codes generated by
ADCMH to improve the retrieval efficiency. The NECD network is trained such
that it hasan error correcting capability greater than or equal to the margin
(m) of the margin-based loss function. This results in NECD cancorrect the
corrupted hash codes generated by ADCMH up to the Hamming distance of m. We
have evaluated and comparedDNDCMH with state-of-the-art cross-modal hashing
methods on standard datasets to demonstrate the superiority of our method.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        Compact-Codes 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/thomas2020preserving/">Preserving Semantic Neighborhoods for Robust Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Preserving Semantic Neighborhoods for Robust Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Preserving Semantic Neighborhoods for Robust Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Thomas Christopher, Kovashka Adriana</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>27</td>
    <td><p>The abundance of multimodal data (e.g. social media posts) has inspired
interest in cross-modal retrieval methods. Popular approaches rely on a variety
of metric learning losses, which prescribe what the proximity of image and text
should be, in the learned space. However, most prior methods have focused on
the case where image and text convey redundant information; in contrast,
real-world image-text pairs convey complementary information with little
overlap. Further, images in news articles and media portray topics in a
visually diverse fashion; thus, we need to take special care to ensure a
meaningful image representation. We propose novel within-modality losses which
encourage semantic coherency in both the text and image subspaces, which does
not necessarily align with visual coherency. Our method ensures that not only
are paired images and texts close, but the expected image-image and text-text
relationships are also observed. Our approach improves the results of
cross-modal retrieval on four datasets compared to five baselines.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/sun2020benchmarking/">A Benchmarking Study of Embedding-based Entity Alignment for Knowledge Graphs</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Benchmarking Study of Embedding-based Entity Alignment for Knowledge Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Benchmarking Study of Embedding-based Entity Alignment for Knowledge Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sun Zequn, Zhang Qingheng, Hu Wei, Wang Chengming, Chen Muhao, Akrami Farahnaz, Li Chengkai</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the VLDB Endowment</td>
    <td>79</td>
    <td><p>Entity alignment seeks to find entities in different knowledge graphs (KGs)
that refer to the same real-world object. Recent advancement in KG embedding
impels the advent of embedding-based entity alignment, which encodes entities
in a continuous embedding space and measures entity similarities based on the
learned embeddings. In this paper, we conduct a comprehensive experimental
study of this emerging field. We survey 23 recent embedding-based entity
alignment approaches and categorize them based on their techniques and
characteristics. We also propose a new KG sampling algorithm, with which we
generate a set of dedicated benchmark datasets with various heterogeneity and
distributions for a realistic evaluation. We develop an open-source library
including 12 representative embedding-based entity alignment approaches, and
extensively evaluate these approaches, to understand their strengths and
limitations. Additionally, for several directions that have not been explored
in current approaches, we perform exploratory experiments and report our
preliminary findings for future studies. The benchmark datasets, open-source
library and experimental results are all accessible online and will be duly
maintained.</p>
</td>
    <td>
      
        Datasets 
      
        Survey-Paper 
      
        Evaluation 
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/sbai2018unsupervised/">Unsupervised Image Decomposition in Vector Layers</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Image Decomposition in Vector Layers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Image Decomposition in Vector Layers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sbai Othman, Couprie Camille, Aubry Mathieu</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE International Conference on Image Processing (ICIP)</td>
    <td>8</td>
    <td><p>Deep image generation is becoming a tool to enhance artists and designers
creativity potential. In this paper, we aim at making the generation process
more structured and easier to interact with. Inspired by vector graphics
systems, we propose a new deep image reconstruction paradigm where the outputs
are composed from simple layers, defined by their color and a vector
transparency mask. This presents a number of advantages compared to the
commonly used convolutional network architectures. In particular, our layered
decomposition allows simple user interaction, for example to update a given
mask, or change the color of a selected layer. From a compact code, our
architecture also generates vector images with a virtually infinite resolution,
the color at each point in an image being a parametric function of its
coordinates. We validate the efficiency of our approach by comparing
reconstructions with state-of-the-art baselines given similar memory resources
on CelebA and ImageNet datasets. Most importantly, we demonstrate several
applications of our new image representation obtained in an unsupervised
manner, including editing, vectorization and image search.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Compact-Codes 
      
        Unsupervised 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/xu2019hashing/">Hashing based Answer Selection</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing based Answer Selection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing based Answer Selection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xu Dong, Li Wu-jun</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>8</td>
    <td><p>Answer selection is an important subtask of question answering (QA), where
deep models usually achieve better performance. Most deep models adopt
question-answer interaction mechanisms, such as attention, to get vector
representations for answers. When these interaction based deep models are
deployed for online prediction, the representations of all answers need to be
recalculated for each question. This procedure is time-consuming for deep
models with complex encoders like BERT which usually have better accuracy than
simple encoders. One possible solution is to store the matrix representation
(encoder output) of each answer in memory to avoid recalculation. But this will
bring large memory cost. In this paper, we propose a novel method, called
hashing based answer selection (HAS), to tackle this problem. HAS adopts a
hashing strategy to learn a binary matrix representation for each answer, which
can dramatically reduce the memory cost for storing the matrix representations
of answers. Hence, HAS can adopt complex encoders like BERT in the model, but
the online prediction of HAS is still fast with a low memory cost. Experimental
results on three popular answer selection datasets show that HAS can outperform
existing models to achieve state-of-the-art performance.</p>
</td>
    <td>
      
        AAAI 
      
        Datasets 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2020</td>
    <td>
      <a href="/publications/wang2019cross/">Cross-modal Scene Graph Matching for Relationship-aware Image-Text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-modal Scene Graph Matching for Relationship-aware Image-Text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-modal Scene Graph Matching for Relationship-aware Image-Text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Sijin, Wang Ruiping, Yao Ziwei, Shan Shiguang, Chen Xilin</td> <!-- 🔧 You were missing this -->
    <td>2020 IEEE Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>219</td>
    <td><p>Image-text retrieval of natural scenes has been a popular research topic.
Since image and text are heterogeneous cross-modal data, one of the key
challenges is how to learn comprehensive yet unified representations to express
the multi-modal data. A natural scene image mainly involves two kinds of visual
concepts, objects and their relationships, which are equally essential to
image-text retrieval. Therefore, a good representation should account for both
of them. In the light of recent success of scene graph in many CV and NLP tasks
for describing complex natural scenes, we propose to represent image and text
with two kinds of scene graphs: visual scene graph (VSG) and textual scene
graph (TSG), each of which is exploited to jointly characterize objects and
relationships in the corresponding modality. The image-text retrieval task is
then naturally formulated as cross-modal scene graph matching. Specifically, we
design two particular scene graph encoders in our model for VSG and TSG, which
can refine the representation of each node on the graph by aggregating
neighborhood information. As a result, both object-level and relationship-level
cross-modal features can be obtained, which favorably enables us to evaluate
the similarity of image and text in the two levels in a more plausible way. We
achieve state-of-the-art results on Flickr30k and MSCOCO, which verifies the
advantages of our graph matching based approach for image-text retrieval.</p>
</td>
    <td>
      
        Text-Retrieval 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/yang2025adaptive/">Adaptive Labeling for Deep Learning to Hash</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Adaptive Labeling for Deep Learning to Hash' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Adaptive Labeling for Deep Learning to Hash' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yang Huei-fang, Tu, Chen</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>8</td>
    <td><p>Hash function learning has been widely used for largescale image retrieval because of the efficiency of computation and storage. We introduce AdaLabelHash, a binary
hash function learning approach via deep neural networks
in this paper. In AdaLabelHash, class label representations are variables that are adapted during the backward
network training procedure. We express the labels as hypercube vertices in a K-dimensional space, and the class
label representations together with the network weights are
updated in the learning process. As the label representations (or referred to as codewords in this work), are learned
from data, semantically similar classes will be assigned
with the codewords that are close to each other in terms
of Hamming distance in the label space. The codewords
then serve as the desired output of the hash function learning, and yield compact and discriminating binary hash representations. AdaLabelHash is easy to implement, which
can jointly learn label representations and infer compact
binary codes from data. It is applicable to both supervised
and semi-supervised hash. Experimental results on standard benchmarks demonstrate the satisfactory performance
of AdaLabelHash.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Efficiency 
      
        CVPR 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/talreja2019learning/">Learning to Authenticate with Deep Multibiometric Hashing and Neural Network Decoding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning to Authenticate with Deep Multibiometric Hashing and Neural Network Decoding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning to Authenticate with Deep Multibiometric Hashing and Neural Network Decoding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Talreja Veeru, Soleymani Sobhan, Valenti Matthew C., Nasrabadi Nasser M.</td> <!-- 🔧 You were missing this -->
    <td>ICC 2019 - 2019 IEEE International Conference on Communications (ICC)</td>
    <td>19</td>
    <td><p>In this paper, we propose a novel multimodal deep hashing neural decoder
(MDHND) architecture, which integrates a deep hashing framework with a neural
network decoder (NND) to create an effective multibiometric authentication
system. The MDHND consists of two separate modules: a multimodal deep hashing
(MDH) module, which is used for feature-level fusion and binarization of
multiple biometrics, and a neural network decoder (NND) module, which is used
to refine the intermediate binary codes generated by the MDH and compensate for
the difference between enrollment and probe biometrics (variations in pose,
illumination, etc.). Use of NND helps to improve the performance of the overall
multimodal authentication system. The MDHND framework is trained in 3 steps
using joint optimization of the two modules. In Step 1, the MDH parameters are
trained and learned to generate a shared multimodal latent code; in Step 2, the
latent codes from Step 1 are passed through a conventional error-correcting
code (ECC) decoder to generate the ground truth to train a neural network
decoder (NND); in Step 3, the NND decoder is trained using the ground truth
from Step 2 and the MDH and NND are jointly optimized. Experimental results on
a standard multimodal dataset demonstrate the superiority of our method
relative to other current multimodal authentication systems</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/sun2020multi/">Multi-Graph Convolution Collaborative Filtering</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multi-Graph Convolution Collaborative Filtering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multi-Graph Convolution Collaborative Filtering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sun Jianing, Zhang Yingxue, Ma Chen, Coates Mark, Guo Huifeng, Tang Ruiming, He Xiuqiang</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE International Conference on Data Mining (ICDM)</td>
    <td>127</td>
    <td><p>Personalized recommendation is ubiquitous, playing an important role in many
online services. Substantial research has been dedicated to learning vector
representations of users and items with the goal of predicting a user’s
preference for an item based on the similarity of the representations.
Techniques range from classic matrix factorization to more recent deep learning
based methods. However, we argue that existing methods do not make full use of
the information that is available from user-item interaction data and the
similarities between user pairs and item pairs. In this work, we develop a
graph convolution-based recommendation framework, named Multi-Graph Convolution
Collaborative Filtering (Multi-GCCF), which explicitly incorporates multiple
graphs in the embedding learning process. Multi-GCCF not only expressively
models the high-order information via a partite user-item interaction graph,
but also integrates the proximal information by building and processing
user-user and item-item graphs. Furthermore, we consider the intrinsic
difference between user nodes and item nodes when performing graph convolution
on the bipartite graph. We conduct extensive experiments on four publicly
accessible benchmarks, showing significant improvements relative to several
state-of-the-art collaborative filtering and graph neural network-based
recommendation models. Further experiments quantitatively verify the
effectiveness of each component of our proposed model and demonstrate that the
learned embeddings capture the important relationship structure.</p>
</td>
    <td>
      
        Recommender-Systems 
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/li2025neighborhood/">Neighborhood Preserving Hashing for Scalable Video Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Neighborhood Preserving Hashing for Scalable Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Neighborhood Preserving Hashing for Scalable Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Shuyan, Chen, Lu, Li, Zhou</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>40</td>
    <td><p>In this paper, we propose a Neighborhood Preserving
Hashing (NPH) method for scalable video retrieval in an
unsupervised manner. Unlike most existing deep video
hashing methods which indiscriminately compress an entire video into a binary code, we embed the spatial-temporal
neighborhood information into the encoding network such
that the neighborhood-relevant visual content of a video can
be preferentially encoded into a binary code under the guidance of the neighborhood information. Specifically, we propose a neighborhood attention mechanism which focuses
on partial useful content of each input frame conditioned
on the neighborhood information. We then integrate the
neighborhood attention mechanism into an RNN-based reconstruction scheme to encourage the binary codes to capture the spatial-temporal structure in a video which is consistent with that in the neighborhood. As a consequence, the
learned hashing functions can map similar videos to similar
binary codes. Extensive experiments on three widely-used
benchmark datasets validate the effectiveness of our proposed approach.</p>
</td>
    <td>
      
        Video-Retrieval 
      
        ICCV 
      
        Datasets 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/yan2019deep/">Deep Hashing by Discriminating Hard Examples</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Hashing by Discriminating Hard Examples' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Hashing by Discriminating Hard Examples' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yan Cheng, Pang, Bai, Shen, Zhou, Hancock</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 27th ACM International Conference on Multimedia</td>
    <td>27</td>
    <td><p>This paper tackles a rarely explored but critical problem within learning to hash, i.e., to learn hash codes that effectively discriminate hard similar and dissimilar examples, to empower large-scale image retrieval. Hard similar examples refer to image pairs from the same semantic class that demonstrate some shared appearance but have different fine-grained appearance. Hard dissimilar examples are image pairs that come from different semantic classes but exhibit similar appearance. These hard examples generally have a small distance due to the shared appearance. Therefore, effective encoding of the hard examples can well discriminate the relevant images within a small Hamming distance, enabling more accurate retrieval in the top-ranked returned images. However, most existing hashing methods cannot capture this key information as their optimization is dominated byeasy examples, i.e., distant similar/dissimilar pairs that share no or limited appearance. To address this problem, we introduce a novel Gamma distribution-enabled and symmetric Kullback-Leibler divergence-based loss, which is dubbed dual hinge loss because it works similarly as imposing two smoothed hinge losses on the respective similar and dissimilar pairs. Specifically, the loss enforces exponentially variant penalization on the hard similar (dissimilar) examples to emphasize and learn their fine-grained difference. It meanwhile imposes a bounding penalization on easy similar (dissimilar) examples to prevent the dominance of the easy examples in the optimization while preserving the high-level similarity (dissimilarity). This enables our model to well encode the key information carried by both easy and hard examples. Extensive empirical results on three widely-used image retrieval datasets show that (i) our method consistently and substantially outperforms state-of-the-art competing methods using hash codes of the same length and (ii) our method can use significantly (e.g., 50%-75%) shorter hash codes to perform substantially better than, or comparably well to, the competing methods.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Datasets 
      
        Neural-Hashing 
      
        Hashing-Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/zhang2018semantic/">Semantic Cluster Unary Loss for Efficient Deep Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semantic Cluster Unary Loss for Efficient Deep Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semantic Cluster Unary Loss for Efficient Deep Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Shifeng, Li Jianmin, Zhang Bo</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>13</td>
    <td><p>Hashing method maps similar data to binary hashcodes with smaller hamming
distance, which has received a broad attention due to its low storage cost and
fast retrieval speed. With the rapid development of deep learning, deep hashing
methods have achieved promising results in efficient information retrieval.
Most of the existing deep hashing methods adopt pairwise or triplet losses to
deal with similarities underlying the data, but the training is difficult and
less efficient because \(O(n^2)\) data pairs and \(O(n^3)\) triplets are involved.
To address these issues, we propose a novel deep hashing algorithm with unary
loss which can be trained very efficiently. We first of all introduce a Unary
Upper Bound of the traditional triplet loss, thus reducing the complexity to
\(O(n)\) and bridging the classification-based unary loss and the triplet loss.
Second, we propose a novel Semantic Cluster Deep Hashing (SCDH) algorithm by
introducing a modified Unary Upper Bound loss, named Semantic Cluster Unary
Loss (SCUL). The resultant hashcodes form several compact clusters, which means
hashcodes in the same cluster have similar semantic information. We also
demonstrate that the proposed SCDH is easy to be extended to semi-supervised
settings by incorporating the state-of-the-art semi-supervised learning
algorithms. Experiments on large-scale datasets show that the proposed method
is superior to state-of-the-art hashing algorithms.</p>
</td>
    <td>
      
        Supervised 
      
        Hashing-Methods 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Neural-Hashing 
      
        Memory-Efficiency 
      
        Scalability 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/li2019memory/">Memory-Based Neighbourhood Embedding for Visual Recognition</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Memory-Based Neighbourhood Embedding for Visual Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Memory-Based Neighbourhood Embedding for Visual Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Suichan, Chen Dapeng, Liu Bin, Yu Nenghai, Zhao Rui</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>44</td>
    <td><p>Learning discriminative image feature embeddings is of great importance to
visual recognition. To achieve better feature embeddings, most current methods
focus on designing different network structures or loss functions, and the
estimated feature embeddings are usually only related to the input images. In
this paper, we propose Memory-based Neighbourhood Embedding (MNE) to enhance a
general CNN feature by considering its neighbourhood. The method aims to solve
two critical problems, i.e., how to acquire more relevant neighbours in the
network training and how to aggregate the neighbourhood information for a more
discriminative embedding. We first augment an episodic memory module into the
network, which can provide more relevant neighbours for both training and
testing. Then the neighbours are organized in a tree graph with the target
instance as the root node. The neighbourhood information is gradually
aggregated to the root node in a bottom-up manner, and aggregation weights are
supervised by the class relationships between the nodes. We apply MNE on image
search and few shot learning tasks. Extensive ablation studies demonstrate the
effectiveness of each component, and our method significantly outperforms the
state-of-the-art approaches.</p>
</td>
    <td>
      
        ICCV 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/li2019neighborhood/">Neighborhood Preserving Hashing for Scalable Video Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Neighborhood Preserving Hashing for Scalable Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Neighborhood Preserving Hashing for Scalable Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Shuyan, Chen, Lu, Li, Zhou</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>40</td>
    <td><p>In this paper, we propose a Neighborhood Preserving
Hashing (NPH) method for scalable video retrieval in an
unsupervised manner. Unlike most existing deep video
hashing methods which indiscriminately compress an entire video into a binary code, we embed the spatial-temporal
neighborhood information into the encoding network such
that the neighborhood-relevant visual content of a video can
be preferentially encoded into a binary code under the guidance of the neighborhood information. Specifically, we propose a neighborhood attention mechanism which focuses
on partial useful content of each input frame conditioned
on the neighborhood information. We then integrate the
neighborhood attention mechanism into an RNN-based reconstruction scheme to encourage the binary codes to capture the spatial-temporal structure in a video which is consistent with that in the neighborhood. As a consequence, the
learned hashing functions can map similar videos to similar
binary codes. Extensive experiments on three widely-used
benchmark datasets validate the effectiveness of our proposed approach.</p>
</td>
    <td>
      
        Video-Retrieval 
      
        ICCV 
      
        Datasets 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/li2018dual/">Dual Asymmetric Deep Hashing Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Dual Asymmetric Deep Hashing Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Dual Asymmetric Deep Hashing Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Jinxing, Zhang Bob, Lu Guangming, Zhang David</td> <!-- 🔧 You were missing this -->
    <td>IEEE Access</td>
    <td>16</td>
    <td><p>Due to the impressive learning power, deep learning has achieved a remarkable
performance in supervised hash function learning. In this paper, we propose a
novel asymmetric supervised deep hashing method to preserve the semantic
structure among different categories and generate the binary codes
simultaneously. Specifically, two asymmetric deep networks are constructed to
reveal the similarity between each pair of images according to their semantic
labels. The deep hash functions are then learned through two networks by
minimizing the gap between the learned features and discrete codes.
Furthermore, since the binary codes in the Hamming space also should keep the
semantic affinity existing in the original space, another asymmetric pairwise
loss is introduced to capture the similarity between the binary codes and
real-value features. This asymmetric loss not only improves the retrieval
performance, but also contributes to a quick convergence at the training phase.
By taking advantage of the two-stream deep structures and two types of
asymmetric pairwise functions, an alternating algorithm is designed to optimize
the deep features and high-quality binary codes efficiently. Experimental
results on three real-world datasets substantiate the effectiveness and
superiority of our approach as compared with state-of-the-art.</p>
</td>
    <td>
      
        Supervised 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/li2018sign/">Sign-Full Random Projections</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Sign-Full Random Projections' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Sign-Full Random Projections' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Ping</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>8</td>
    <td><p>The method of 1-bit (“sign-sign”) random projections has been a popular tool
for efficient search and machine learning on large datasets. Given two \(D\)-dim
data vectors \(u\), \(v\in\mathbb{R}^D\), one can generate \(x = \sum_{i=1}^D u_i
r_i\), and \(y = \sum_{i=1}^D v_i r_i\), where \(r_i\sim N(0,1)\) iid. The
“collision probability” is \({Pr}\left(sgn(x)=sgn(y)\right) =
1-\frac{\cos^{-1}\rho}{\pi}\), where \(\rho = \rho(u,v)\) is the cosine
similarity.
  We develop “sign-full” random projections by estimating \(\rho\) from (e.g.,)
the expectation \(E(sgn(x)y)=\sqrt{\frac{2}{\pi}} \rho\), which can be further
substantially improved by normalizing \(y\). For nonnegative data, we recommend
an interesting estimator based on \(E\left(y_- 1<em>{x\geq 0} + y</em>+ 1_{x&lt;0}\right)\)
and its normalized version. The recommended estimator almost matches the
accuracy of the (computationally expensive) maximum likelihood estimator. At
high similarity (\(\rho\rightarrow1\)), the asymptotic variance of recommended
estimator is only \(\frac{4}{3\pi} \approx 0.4\) of the estimator for sign-sign
projections. At small \(k\) and high similarity, the improvement would be even
much more substantial.</p>
</td>
    <td>
      
        AAAI 
      
        Datasets 
      
        Locality-Sensitive-Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/lejeune2019adaptive/">Adaptive Estimation for Approximate k-Nearest-Neighbor Computations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Adaptive Estimation for Approximate k-Nearest-Neighbor Computations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Adaptive Estimation for Approximate k-Nearest-Neighbor Computations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lejeune Daniel, Baraniuk Richard G., Heckel Reinhard</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of Machine Learning Research 89 (2019)3099-3107</td>
    <td>8</td>
    <td><p>Algorithms often carry out equally many computations for “easy” and “hard”
problem instances. In particular, algorithms for finding nearest neighbors
typically have the same running time regardless of the particular problem
instance. In this paper, we consider the approximate k-nearest-neighbor
problem, which is the problem of finding a subset of O(k) points in a given set
of points that contains the set of k nearest neighbors of a given query point.
We propose an algorithm based on adaptively estimating the distances, and show
that it is essentially optimal out of algorithms that are only allowed to
adaptively estimate distances. We then demonstrate both theoretically and
experimentally that the algorithm can achieve significant speedups relative to
the naive method.</p>
</td>
    <td>
      
        Uncategorized 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/lin2019towards/">Towards Optimal Discrete Online Hashing with Balanced Similarity</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Towards Optimal Discrete Online Hashing with Balanced Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Towards Optimal Discrete Online Hashing with Balanced Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin Mingbao, Ji Rongrong, Liu Hong, Sun Xiaoshuai, Wu Yongjian, Wu Yunsheng</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>50</td>
    <td><p>When facing large-scale image datasets, online hashing serves as a promising
solution for online retrieval and prediction tasks. It encodes the online
streaming data into compact binary codes, and simultaneously updates the hash
functions to renew codes of the existing dataset. To this end, the existing
methods update hash functions solely based on the new data batch, without
investigating the correlation between such new data and the existing dataset.
In addition, existing works update the hash functions using a relaxation
process in its corresponding approximated continuous space. And it remains as
an open problem to directly apply discrete optimizations in online hashing. In
this paper, we propose a novel supervised online hashing method, termed
Balanced Similarity for Online Discrete Hashing (BSODH), to solve the above
problems in a unified framework. BSODH employs a well-designed hashing
algorithm to preserve the similarity between the streaming data and the
existing dataset via an asymmetric graph regularization. We further identify
the “data-imbalance” problem brought by the constructed asymmetric graph, which
restricts the application of discrete optimization in our problem. Therefore, a
novel balanced similarity is further proposed, which uses two equilibrium
factors to balance the similar and dissimilar weights and eventually enables
the usage of discrete optimizations. Extensive experiments conducted on three
widely-used benchmarks demonstrate the advantages of the proposed method over
the state-of-the-art methods.</p>
</td>
    <td>
      
        Supervised 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Datasets 
      
        Compact-Codes 
      
        AAAI 
      
        Scalability 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/lei2019semi/">Semi-Heterogeneous Three-Way Joint Embedding Network for Sketch-Based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semi-Heterogeneous Three-Way Joint Embedding Network for Sketch-Based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semi-Heterogeneous Three-Way Joint Embedding Network for Sketch-Based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lei Jianjun, Song Yuxin, Peng Bo, Ma Zhanyu, Shao Ling, Song Yi-zhe</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Circuits and Systems for Video Technology</td>
    <td>49</td>
    <td><p>Sketch-based image retrieval (SBIR) is a challenging task due to the large
cross-domain gap between sketches and natural images. How to align abstract
sketches and natural images into a common high-level semantic space remains a
key problem in SBIR. In this paper, we propose a novel semi-heterogeneous
three-way joint embedding network (Semi3-Net), which integrates three branches
(a sketch branch, a natural image branch, and an edgemap branch) to learn more
discriminative cross-domain feature representations for the SBIR task. The key
insight lies with how we cultivate the mutual and subtle relationships amongst
the sketches, natural images, and edgemaps. A semi-heterogeneous feature
mapping is designed to extract bottom features from each domain, where the
sketch and edgemap branches are shared while the natural image branch is
heterogeneous to the other branches. In addition, a joint semantic embedding is
introduced to embed the features from different domains into a common
high-level semantic space, where all of the three branches are shared. To
further capture informative features common to both natural images and the
corresponding edgemaps, a co-attention model is introduced to conduct common
channel-wise feature recalibration between different domains. A hybrid-loss
mechanism is designed to align the three branches, where an alignment loss and
a sketch-edgemap contrastive loss are presented to encourage the network to
learn invariant cross-domain representations. Experimental results on two
widely used category-level datasets (Sketchy and TU-Berlin Extension)
demonstrate that the proposed method outperforms state-of-the-art methods.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/tu2018object/">Object Detection based Deep Unsupervised Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Object Detection based Deep Unsupervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Object Detection based Deep Unsupervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tu Rong-cheng, Mao Xian-ling, Feng Bo-si, Bian Bing-bing, Ying Yu-shu</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</td>
    <td>10</td>
    <td><p>Recently, similarity-preserving hashing methods have been extensively studied
for large-scale image retrieval. Compared with unsupervised hashing, supervised
hashing methods for labeled data have usually better performance by utilizing
semantic label information. Intuitively, for unlabeled data, it will improve
the performance of unsupervised hashing methods if we can first mine some
supervised semantic ‘label information’ from unlabeled data and then
incorporate the ‘label information’ into the training process. Thus, in this
paper, we propose a novel Object Detection based Deep Unsupervised Hashing
method (ODDUH). Specifically, a pre-trained object detection model is utilized
to mining supervised ‘label information’, which is used to guide the learning
process to generate high-quality hash codes.Extensive experiments on two public
datasets demonstrate that the proposed method outperforms the state-of-the-art
unsupervised hashing methods in the image retrieval task.</p>
</td>
    <td>
      
        Scalability 
      
        Evaluation 
      
        Datasets 
      
        Unsupervised 
      
        AAAI 
      
        Hashing-Methods 
      
        Neural-Hashing 
      
        IJCAI 
      
        Image-Retrieval 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/kuang2019fashion/">Fashion Retrieval via Graph Reasoning Networks on a Similarity Pyramid</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fashion Retrieval via Graph Reasoning Networks on a Similarity Pyramid' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fashion Retrieval via Graph Reasoning Networks on a Similarity Pyramid' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kuang Zhanghui, Gao Yiming, Li Guanbin, Luo Ping, Chen Yimin, Lin Liang, Zhang Wayne</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>87</td>
    <td><p>Matching clothing images from customers and online shopping stores has rich
applications in E-commerce. Existing algorithms encoded an image as a global
feature vector and performed retrieval with the global representation. However,
discriminative local information on clothes are submerged in this global
representation, resulting in sub-optimal performance. To address this issue, we
propose a novel Graph Reasoning Network (GRNet) on a Similarity Pyramid, which
learns similarities between a query and a gallery cloth by using both global
and local representations in multiple scales. The similarity pyramid is
represented by a Graph of similarity, where nodes represent similarities
between clothing components at different scales, and the final matching score
is obtained by message passing along edges. In GRNet, graph reasoning is solved
by training a graph convolutional network, enabling to align salient clothing
components to improve clothing retrieval. To facilitate future researches, we
introduce a new benchmark FindFashion, containing rich annotations of bounding
boxes, views, occlusions, and cropping. Extensive experiments show that GRNet
obtains new state-of-the-art results on two challenging benchmarks, e.g.,
pushing the top-1, top-20, and top-50 accuracies on DeepFashion to 26%, 64%,
and 75% (i.e., 4%, 10%, and 10% absolute improvements), outperforming
competitors with large margins. On FindFashion, GRNet achieves considerable
improvements on all empirical settings.</p>
</td>
    <td>
      
        ICCV 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/komorowski2017random/">Random Binary Trees for Approximate Nearest Neighbour Search in Binary Space</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Random Binary Trees for Approximate Nearest Neighbour Search in Binary Space' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Random Binary Trees for Approximate Nearest Neighbour Search in Binary Space' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Komorowski Michal, Trzcinski Tomasz</td> <!-- 🔧 You were missing this -->
    <td>Applied Soft Computing</td>
    <td>9</td>
    <td><p>Approximate nearest neighbour (ANN) search is one of the most important
problems in computer science fields such as data mining or computer vision. In
this paper, we focus on ANN for high-dimensional binary vectors and we propose
a simple yet powerful search method that uses Random Binary Search Trees
(RBST). We apply our method to a dataset of 1.25M binary local feature
descriptors obtained from a real-life image-based localisation system provided
by Google as a part of Project Tango. An extensive evaluation of our method
against the state-of-the-art variations of Locality Sensitive Hashing (LSH),
namely Uniform LSH and Multi-probe LSH, shows the superiority of our method in
terms of retrieval precision with performance boost of over 20%</p>
</td>
    <td>
      
        Similarity-Search 
      
        Locality-Sensitive-Hashing 
      
        Hashing-Methods 
      
        Datasets 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/klein2017end/">End-to-End Supervised Product Quantization for Image Search and Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=End-to-End Supervised Product Quantization for Image Search and Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=End-to-End Supervised Product Quantization for Image Search and Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Klein Benjamin, Wolf Lior</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>59</td>
    <td><p>Product Quantization, a dictionary based hashing method, is one of the
leading unsupervised hashing techniques. While it ignores the labels, it
harnesses the features to construct look up tables that can approximate the
feature space. In recent years, several works have achieved state of the art
results on hashing benchmarks by learning binary representations in a
supervised manner. This work presents Deep Product Quantization (DPQ), a
technique that leads to more accurate retrieval and classification than the
latest state of the art methods, while having similar computational complexity
and memory footprint as the Product Quantization method. To our knowledge, this
is the first work to introduce a dictionary-based representation that is
inspired by Product Quantization and which is learned end-to-end, and thus
benefits from the supervised signal. DPQ explicitly learns soft and hard
representations to enable an efficient and accurate asymmetric search, by using
a straight-through estimator. Our method obtains state of the art results on an
extensive array of retrieval and classification experiments.</p>
</td>
    <td>
      
        Supervised 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Neural-Hashing 
      
        Memory-Efficiency 
      
        CVPR 
      
        Unsupervised 
      
        Quantization 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/khurshid2020cross/">Cross-View Image Retrieval -- Ground to Aerial Image Retrieval through Deep Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-View Image Retrieval -- Ground to Aerial Image Retrieval through Deep Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-View Image Retrieval -- Ground to Aerial Image Retrieval through Deep Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Khurshid Numan, Hanif Talha, Tharani Mohbat, Taj Murtaza</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>5</td>
    <td><p>Cross-modal retrieval aims to measure the content similarity between
different types of data. The idea has been previously applied to visual, text,
and speech data. In this paper, we present a novel cross-modal retrieval method
specifically for multi-view images, called Cross-view Image Retrieval CVIR. Our
approach aims to find a feature space as well as an embedding space in which
samples from street-view images are compared directly to satellite-view images
(and vice-versa). For this comparison, a novel deep metric learning based
solution “DeepCVIR” has been proposed. Previous cross-view image datasets are
deficient in that they (1) lack class information; (2) were originally
collected for cross-view image geolocalization task with coupled images; (3) do
not include any images from off-street locations. To train, compare, and
evaluate the performance of cross-view image retrieval, we present a new 6
class cross-view image dataset termed as CrossViewRet which comprises of images
including freeway, mountain, palace, river, ship, and stadium with 700
high-resolution dual-view images for each class. Results show that the proposed
DeepCVIR outperforms conventional matching approaches on the CVIR task for the
given dataset and would also serve as the baseline for future research.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Evaluation 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/kim2019deep/">Deep Metric Learning Beyond Binary Supervision</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Metric Learning Beyond Binary Supervision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Metric Learning Beyond Binary Supervision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kim Sungyeon, Seo Minkyo, Laptev Ivan, Cho Minsu, Kwak Suha</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>88</td>
    <td><p>Metric Learning for visual similarity has mostly adopted binary supervision
indicating whether a pair of images are of the same class or not. Such a binary
indicator covers only a limited subset of image relations, and is not
sufficient to represent semantic similarity between images described by
continuous and/or structured labels such as object poses, image captions, and
scene graphs. Motivated by this, we present a novel method for deep metric
learning using continuous labels. First, we propose a new triplet loss that
allows distance ratios in the label space to be preserved in the learned metric
space. The proposed loss thus enables our model to learn the degree of
similarity rather than just the order. Furthermore, we design a triplet mining
strategy adapted to metric learning with continuous labels. We address three
different image retrieval tasks with continuous labels in terms of human poses,
room layouts and image captions, and demonstrate the superior performance of
our approach compared to previous methods.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        CVPR 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/zhao2021large/">Large-Scale Visual Search with Binary Distributed Graph at Alibaba</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Large-Scale Visual Search with Binary Distributed Graph at Alibaba' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Large-Scale Visual Search with Binary Distributed Graph at Alibaba' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhao Kang, Pan Pan, Zheng Yun, Zhang Yanhao, Wang Changxu, Zhang Yingya, Xu Yinghui, Jin Rong</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 28th ACM International Conference on Information and Knowledge Management</td>
    <td>10</td>
    <td><p>Graph-based approximate nearest neighbor search has attracted more and more
attentions due to its online search advantages. Numbers of methods studying the
enhancement of speed and recall have been put forward. However, few of them
focus on the efficiency and scale of offline graph-construction. For a deployed
visual search system with several billions of online images in total, building
a billion-scale offline graph in hours is essential, which is almost
unachievable by most existing methods. In this paper, we propose a novel
algorithm called Binary Distributed Graph to solve this problem. Specifically,
we combine binary codes with graph structure to speedup online and offline
procedures, and achieve comparable performance with the ones in real-value
based scenarios by recalling more binary candidates. Furthermore, the
graph-construction is optimized to completely distributed implementation, which
significantly accelerates the offline process and gets rid of the limitation of
memory and disk within a single machine. Experimental comparisons on Alibaba
Commodity Data Set (more than three billion images) show that the proposed
method outperforms the state-of-the-art with respect to the online/offline
trade-off.</p>
</td>
    <td>
      
        CIKM 
      
        Graph-Based-ANN 
      
        Image-Retrieval 
      
        Compact-Codes 
      
        Scalability 
      
        Large-Scale-Search 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/karaman2019unsupervised/">Unsupervised Rank-Preserving Hashing for Large-Scale Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Rank-Preserving Hashing for Large-Scale Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Rank-Preserving Hashing for Large-Scale Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Karaman Svebor, Lin Xudong, Hu Xuefeng, Chang Shih-fu</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2019 on International Conference on Multimedia Retrieval</td>
    <td>14</td>
    <td><p>We propose an unsupervised hashing method which aims to produce binary codes
that preserve the ranking induced by a real-valued representation. Such compact
hash codes enable the complete elimination of real-valued feature storage and
allow for significant reduction of the computation complexity and storage cost
of large-scale image retrieval applications. Specifically, we learn a neural
network-based model, which transforms the input representation into a binary
representation. We formalize the training objective of the network in an
intuitive and effective way, considering each training sample as a query and
aiming to obtain the same retrieval results using the produced hash codes as
those obtained with the original features. This training formulation directly
optimizes the hashing model for the target usage of the hash codes it produces.
We further explore the addition of a decoder trained to obtain an approximated
reconstruction of the original features. At test time, we retrieved the most
promising database samples with an efficient graph-based search procedure using
only our hash codes and perform re-ranking using the reconstructed features,
thus without needing to access the original features at all. Experiments
conducted on multiple publicly available large-scale datasets show that our
method consistently outperforms all compared state-of-the-art unsupervised
hashing methods and that the reconstruction procedure can effectively boost the
search accuracy with a minimal constant additional cost.</p>
</td>
    <td>
      
        Scalability 
      
        Hybrid-ANN-Methods 
      
        Datasets 
      
        Unsupervised 
      
        Graph-Based-ANN 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Multimodal-Retrieval 
      
        Neural-Hashing 
      
        Memory-Efficiency 
      
        Image-Retrieval 
      
        Supervised 
      
        Re-Ranking 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/keisler2020visual/">Visual search over billions of aerial and satellite images</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Visual search over billions of aerial and satellite images' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Visual search over billions of aerial and satellite images' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Keisler Ryan, Skillman Samuel W., Gonnabathula Sunny, Poehnelt Justin, Rudelis Xander, Warren Michael S.</td> <!-- 🔧 You were missing this -->
    <td>Computer Vision and Image Understanding</td>
    <td>21</td>
    <td><p>We present a system for performing visual search over billions of aerial and
satellite images. The purpose of visual search is to find images that are
visually similar to a query image. We define visual similarity using 512
abstract visual features generated by a convolutional neural network that has
been trained on aerial and satellite imagery. The features are converted to
binary values to reduce data and compute requirements. We employ a hash-based
search using Bigtable, a scalable database service from Google Cloud. Searching
the continental United States at 1-meter pixel resolution, corresponding to
approximately 2 billion images, takes approximately 0.1 seconds. This system
enables real-time visual search over the surface of the earth, and an
interactive demo is available at https://search.descarteslabs.com.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/kang2019maximum/">Maximum-Margin Hamming Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Maximum-Margin Hamming Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Maximum-Margin Hamming Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kang Rong, Cao, (b), Wang, Yu</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>37</td>
    <td><p>Deep hashing enables computation and memory efficient
image search through end-to-end learning of feature representations and binary codes. While linear scan over binary
hash codes is more efficient than over the high-dimensional
representations, its linear-time complexity is still unacceptable for very large databases. Hamming space retrieval enables constant-time search through hash lookups, where for
each query, there is a Hamming ball centered at the query
and the data points within the ball are returned as relevant.
Since inside the Hamming ball implies retrievable while
outside irretrievable, it is crucial to explicitly characterize
the Hamming ball. The main idea of this work is to directly
embody the Hamming radius into the loss functions, leading
to Maximum-Margin Hamming Hashing (MMHH), a new
model specifically optimized for Hamming space retrieval.
We introduce a max-margin t-distribution loss, where the
t-distribution concentrates more similar data points to be
within the Hamming ball, and the margin characterizes the
Hamming radius such that less penalization is applied to
similar data points within the Hamming ball. The loss function also introduces robustness to data noise, where the similarity supervision may be inaccurate in practical problems.
The model is trained end-to-end using a new semi-batch optimization algorithm tailored to extremely imbalanced data.
Our method yields state-of-the-art results on four datasets
and shows superior performance on noisy data.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        ICCV 
      
        Datasets 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/kang2019candidate/">Candidate Generation with Binary Codes for Large-Scale Top-N Recommendation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Candidate Generation with Binary Codes for Large-Scale Top-N Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Candidate Generation with Binary Codes for Large-Scale Top-N Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kang Wang-cheng, Mcauley Julian</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 28th ACM International Conference on Information and Knowledge Management</td>
    <td>53</td>
    <td><p>Generating the Top-N recommendations from a large corpus is computationally
expensive to perform at scale. Candidate generation and re-ranking based
approaches are often adopted in industrial settings to alleviate efficiency
problems. However it remains to be fully studied how well such schemes
approximate complete rankings (or how many candidates are required to achieve a
good approximation), or to develop systematic approaches to generate
high-quality candidates efficiently. In this paper, we seek to investigate
these questions via proposing a candidate generation and re-ranking based
framework (CIGAR), which first learns a preference-preserving binary embedding
for building a hash table to retrieve candidates, and then learns to re-rank
the candidates using real-valued ranking models with a candidate-oriented
objective. We perform a comprehensive study on several large-scale real-world
datasets consisting of millions of users/items and hundreds of millions of
interactions. Our results show that CIGAR significantly boosts the Top-N
accuracy against state-of-the-art recommendation models, while reducing the
query time by orders of magnitude. We hope that this work could draw more
attention to the candidate generation problem in recommender systems.</p>
</td>
    <td>
      
        CIKM 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Datasets 
      
        Recommender-Systems 
      
        Compact-Codes 
      
        Hybrid-ANN-Methods 
      
        Re-Ranking 
      
        Scalability 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/kang2025maximum/">Maximum-Margin Hamming Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Maximum-Margin Hamming Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Maximum-Margin Hamming Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kang Rong, Cao, (b), Wang, Yu</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>37</td>
    <td><p>Deep hashing enables computation and memory efficient
image search through end-to-end learning of feature representations and binary codes. While linear scan over binary
hash codes is more efficient than over the high-dimensional
representations, its linear-time complexity is still unacceptable for very large databases. Hamming space retrieval enables constant-time search through hash lookups, where for
each query, there is a Hamming ball centered at the query
and the data points within the ball are returned as relevant.
Since inside the Hamming ball implies retrievable while
outside irretrievable, it is crucial to explicitly characterize
the Hamming ball. The main idea of this work is to directly
embody the Hamming radius into the loss functions, leading
to Maximum-Margin Hamming Hashing (MMHH), a new
model specifically optimized for Hamming space retrieval.
We introduce a max-margin t-distribution loss, where the
t-distribution concentrates more similar data points to be
within the Hamming ball, and the margin characterizes the
Hamming radius such that less penalization is applied to
similar data points within the Hamming ball. The loss function also introduces robustness to data noise, where the similarity supervision may be inaccurate in practical problems.
The model is trained end-to-end using a new semi-batch optimization algorithm tailored to extremely imbalanced data.
Our method yields state-of-the-art results on four datasets
and shows superior performance on noisy data.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        ICCV 
      
        Datasets 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/j%C3%A4%C3%A4saari2018efficient/">Efficient Autotuning of Hyperparameters in Approximate Nearest Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Autotuning of Hyperparameters in Approximate Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient Autotuning of Hyperparameters in Approximate Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jääsaari Elias, Hyvönen Ville, Roos Teemu</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>12</td>
    <td><p>Approximate nearest neighbor algorithms are used to speed up nearest neighbor
search in a wide array of applications. However, current indexing methods
feature several hyperparameters that need to be tuned to reach an acceptable
accuracy–speed trade-off. A grid search in the parameter space is often
impractically slow due to a time-consuming index-building procedure. Therefore,
we propose an algorithm for automatically tuning the hyperparameters of
indexing methods based on randomized space-partitioning trees. In particular,
we present results using randomized k-d trees, random projection trees and
randomized PCA trees. The tuning algorithm adds minimal overhead to the
index-building process but is able to find the optimal hyperparameters
accurately. We demonstrate that the algorithm is significantly faster than
existing approaches, and that the indexing methods used are competitive with
the state-of-the-art methods in query time while being faster to build.</p>
</td>
    <td>
      
        Tree-Based-ANN 
      
        Locality-Sensitive-Hashing 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/jun2019combination/">Combination of Multiple Global Descriptors for Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Combination of Multiple Global Descriptors for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Combination of Multiple Global Descriptors for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jun Heejae, Ko Byungsoo, Kim Youngjoon, Kim Insik, Kim Jongtack</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>41</td>
    <td><p>Recent studies in image retrieval task have shown that ensembling different
models and combining multiple global descriptors lead to performance
improvement. However, training different models for the ensemble is not only
difficult but also inefficient with respect to time and memory. In this paper,
we propose a novel framework that exploits multiple global descriptors to get
an ensemble effect while it can be trained in an end-to-end manner. The
proposed framework is flexible and expandable by the global descriptor, CNN
backbone, loss, and dataset. Moreover, we investigate the effectiveness of
combining multiple global descriptors with quantitative and qualitative
analysis. Our extensive experiments show that the combined descriptor
outperforms a single global descriptor, as it can utilize different types of
feature properties. In the benchmark evaluation, the proposed framework
achieves the state-of-the-art performance on the CARS196, CUB200-2011, In-shop
Clothes, and Stanford Online Products on image retrieval tasks. Our model
implementations and pretrained models are publicly available.</p>
</td>
    <td>
      
        Datasets 
      
        Evaluation 
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/johnson2017billion/">Billion-scale similarity search with GPUs</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Billion-scale similarity search with GPUs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Billion-scale similarity search with GPUs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Johnson Jeff, Douze Matthijs, Jégou Hervé</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Big Data</td>
    <td>2024</td>
    <td><p>Similarity search finds application in specialized database systems handling
complex data such as images or videos, which are typically represented by
high-dimensional features and require specific indexing structures. This paper
tackles the problem of better utilizing GPUs for this task. While GPUs excel at
data-parallel tasks, prior approaches are bottlenecked by algorithms that
expose less parallelism, such as k-min selection, or make poor use of the
memory hierarchy.
  We propose a design for k-selection that operates at up to 55% of theoretical
peak performance, enabling a nearest neighbor implementation that is 8.5x
faster than prior GPU state of the art. We apply it in different similarity
search scenarios, by proposing optimized design for brute-force, approximate
and compressed-domain search based on product quantization. In all these
setups, we outperform the state of the art by large margins. Our implementation
enables the construction of a high accuracy k-NN graph on 95 million images
from the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion
vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced
our approach for the sake of comparison and reproducibility.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Datasets 
      
        Scalability 
      
        Quantization 
      
        Large-Scale-Search 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/sarafijanovicdjukic2020fast/">Fast Distance-based Anomaly Detection in Images Using an Inception-like Autoencoder</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Distance-based Anomaly Detection in Images Using an Inception-like Autoencoder' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Distance-based Anomaly Detection in Images Using an Inception-like Autoencoder' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sarafijanovic-djukic Natasa, Davis Jesse</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>32</td>
    <td><p>The goal of anomaly detection is to identify examples that deviate from
normal or expected behavior. We tackle this problem for images. We consider a
two-phase approach. First, using normal examples, a convolutional autoencoder
(CAE) is trained to extract a low-dimensional representation of the images.
Here, we propose a novel architectural choice when designing the CAE, an
Inception-like CAE. It combines convolutional filters of different kernel sizes
and it uses a Global Average Pooling (GAP) operation to extract the
representations from the CAE’s bottleneck layer. Second, we employ a
distanced-based anomaly detector in the low-dimensional space of the learned
representation for the images. However, instead of computing the exact
distance, we compute an approximate distance using product quantization. This
alleviates the high memory and prediction time costs of distance-based anomaly
detectors. We compare our proposed approach to a number of baselines and
state-of-the-art methods on four image datasets, and we find that our approach
resulted in improved predictive performance.</p>
</td>
    <td>
      
        Datasets 
      
        Quantization 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/jin2018unsupervised/">Unsupervised Semantic Deep Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Semantic Deep Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Semantic Deep Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jin Sheng</td> <!-- 🔧 You were missing this -->
    <td>Neurocomputing</td>
    <td>25</td>
    <td><p>In recent years, deep hashing methods have been proved to be efficient since
it employs convolutional neural network to learn features and hashing codes
simultaneously. However, these methods are mostly supervised. In real-world
application, it is a time-consuming and overloaded task for annotating a large
number of images. In this paper, we propose a novel unsupervised deep hashing
method for large-scale image retrieval. Our method, namely unsupervised
semantic deep hashing (\textbf{USDH}), uses semantic information preserved in
the CNN feature layer to guide the training of network. We enforce four
criteria on hashing codes learning based on VGG-19 model: 1) preserving
relevant information of feature space in hashing space; 2) minimizing
quantization loss between binary-like codes and hashing codes; 3) improving the
usage of each bit in hashing codes by using maximum information entropy, and 4)
invariant to image rotation. Extensive experiments on CIFAR-10, NUSWIDE have
demonstrated that \textbf{USDH} outperforms several state-of-the-art
unsupervised hashing methods for image retrieval. We also conduct experiments
on Oxford 17 datasets for fine-grained classification to verify its efficiency
for other computer vision tasks.</p>
</td>
    <td>
      
        Supervised 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Quantization 
      
        Neural-Hashing 
      
        Unsupervised 
      
        Scalability 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/jiang2017discrete/">Discrete Latent Factor Model for Cross-Modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Discrete Latent Factor Model for Cross-Modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Discrete Latent Factor Model for Cross-Modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jiang Qing-yuan, Li Wu-jun</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>143</td>
    <td><p>Due to its storage and retrieval efficiency, cross-modal hashing~(CMH) has
been widely used for cross-modal similarity search in multimedia applications.
According to the training strategy, existing CMH methods can be mainly divided
into two categories: relaxation-based continuous methods and discrete methods.
In general, the training of relaxation-based continuous methods is faster than
discrete methods, but the accuracy of relaxation-based continuous methods is
not satisfactory. On the contrary, the accuracy of discrete methods is
typically better than relaxation-based continuous methods, but the training of
discrete methods is time-consuming. In this paper, we propose a novel CMH
method, called discrete latent factor model based cross-modal hashing~(DLFH),
for cross modal similarity search. DLFH is a discrete method which can directly
learn the binary hash codes for CMH. At the same time, the training of DLFH is
efficient. Experiments on real datasets show that DLFH can achieve
significantly better accuracy than existing methods, and the training time of
DLFH is comparable to that of relaxation-based continuous methods which are
much faster than existing discrete methods.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        Similarity-Search 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/schall2019deep/">Deep Metric Learning using Similarities from Nonlinear Rank Approximations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Metric Learning using Similarities from Nonlinear Rank Approximations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Metric Learning using Similarities from Nonlinear Rank Approximations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Schall Konstantin, Barthel Kai Uwe, Hezel Nico, Jung Klaus</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE 21st International Workshop on Multimedia Signal Processing (MMSP)</td>
    <td>7</td>
    <td><p>In recent years, deep metric learning has achieved promising results in
learning high dimensional semantic feature embeddings where the spatial
relationships of the feature vectors match the visual similarities of the
images. Similarity search for images is performed by determining the vectors
with the smallest distances to a query vector. However, high retrieval quality
does not depend on the actual distances of the feature vectors, but rather on
the ranking order of the feature vectors from similar images. In this paper, we
introduce a metric learning algorithm that focuses on identifying and modifying
those feature vectors that most strongly affect the retrieval quality. We
compute normalized approximated ranks and convert them to similarities by
applying a nonlinear transfer function. These similarities are used in a newly
proposed loss function that better contracts similar and disperses dissimilar
samples. Experiments demonstrate significant improvement over existing deep
feature embedding methods on the CUB-200-2011, Cars196, and Stanford Online
Products data sets for all embedding sizes.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Distance-Metric-Learning 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/jia2019efficient/">Efficient Task-Specific Data Valuation for Nearest Neighbor Algorithms</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Task-Specific Data Valuation for Nearest Neighbor Algorithms' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient Task-Specific Data Valuation for Nearest Neighbor Algorithms' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jia Ruoxi, Dao David, Wang Boxin, Hubis Frances Ann, Gurel Nezihe Merve, Li Bo, Zhang Ce, Spanos Costas J., Song Dawn</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the VLDB Endowment</td>
    <td>129</td>
    <td><p>Given a data set \(\mathcal{D}\) containing millions of data points and a data
consumer who is willing to pay for $\(X\) to train a machine learning (ML) model
over \(\mathcal{D}\), how should we distribute this $\(X\) to each data point to
reflect its “value”? In this paper, we define the “relative value of data” via
the Shapley value, as it uniquely possesses properties with appealing
real-world interpretations, such as fairness, rationality and
decentralizability. For general, bounded utility functions, the Shapley value
is known to be challenging to compute: to get Shapley values for all \(N\) data
points, it requires \(O(2^N)\) model evaluations for exact computation and
\(O(Nlog N)\) for \((\epsilon, \delta)\)-approximation. In this paper, we focus on
one popular family of ML models relying on \(K\)-nearest neighbors (\(K\)NN). The
most surprising result is that for unweighted \(K\)NN classifiers and regressors,
the Shapley value of all \(N\) data points can be computed, exactly, in \(O(Nlog
N)\) time – an exponential improvement on computational complexity! Moreover,
for \((\epsilon, \delta)\)-approximation, we are able to develop an algorithm
based on Locality Sensitive Hashing (LSH) with only sublinear complexity
\(O(N^{h(\epsilon,K)}log N)\) when \(\epsilon\) is not too small and \(K\) is not
too large. We empirically evaluate our algorithms on up to \(10\) million data
points and even our exact algorithm is up to three orders of magnitude faster
than the baseline approximation algorithm. The LSH-based approximation
algorithm can accelerate the value calculation process even further. We then
extend our algorithms to other scenarios such as (1) weighed \(K\)NN classifiers,
(2) different data points are clustered by different data curators, and (3)
there are data analysts providing computation who also requires proper
valuation.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Locality-Sensitive-Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/schlegel2018adding/">Adding Cues to Binary Feature Descriptors for Visual Place Recognition</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Adding Cues to Binary Feature Descriptors for Visual Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Adding Cues to Binary Feature Descriptors for Visual Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Schlegel Dominik, Grisetti Giorgio</td> <!-- 🔧 You were missing this -->
    <td>2019 International Conference on Robotics and Automation (ICRA)</td>
    <td>6</td>
    <td><p>In this paper we propose an approach to embed continuous and selector cues in
binary feature descriptors used for visual place recognition. The embedding is
achieved by extending each feature descriptor with a binary string that encodes
a cue and supports the Hamming distance metric. Augmenting the descriptors in
such a way has the advantage of being transparent to the procedure used to
compare them. We present two concrete applications of our methodology,
demonstrating the two considered types of cues. In addition to that, we
conducted on these applications a broad quantitative and comparative evaluation
covering five benchmark datasets and several state-of-the-art image retrieval
approaches in combination with various binary descriptor types.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Datasets 
      
        Distance-Metric-Learning 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/yang2019asymmetric/">Asymmetric Deep Semantic Quantization for Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Asymmetric Deep Semantic Quantization for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Asymmetric Deep Semantic Quantization for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yang Zhan, Raymond Osolo Ian, Sun Wuqing, Long Jun</td> <!-- 🔧 You were missing this -->
    <td>IEEE Access</td>
    <td>5</td>
    <td><p>Due to its fast retrieval and storage efficiency capabilities, hashing has
been widely used in nearest neighbor retrieval tasks. By using deep learning
based techniques, hashing can outperform non-learning based hashing technique
in many applications. However, we argue that the current deep learning based
hashing methods ignore some critical problems (e.g., the learned hash codes are
not discriminative due to the hashing methods being unable to discover rich
semantic information and the training strategy having difficulty optimizing the
discrete binary codes). In this paper, we propose a novel image hashing method,
termed as \textbf{\underline{A}}symmetric \textbf{\underline{D}}eep
\textbf{\underline{S}}emantic \textbf{\underline{Q}}uantization
(\textbf{ADSQ}). \textbf{ADSQ} is implemented using three stream frameworks,
which consist of one <em>LabelNet</em> and two <em>ImgNets</em>. The
<em>LabelNet</em> leverages the power of three fully-connected layers, which are
used to capture rich semantic information between image pairs. For the two
<em>ImgNets</em>, they each adopt the same convolutional neural network
structure, but with different weights (i.e., asymmetric convolutional neural
networks). The two <em>ImgNets</em> are used to generate discriminative compact
hash codes. Specifically, the function of the <em>LabelNet</em> is to capture
rich semantic information that is used to guide the two <em>ImgNets</em> in
minimizing the gap between the real-continuous features and the discrete binary
codes. Furthermore, \textbf{ADSQ} can utilize the most critical semantic
information to guide the feature learning process and consider the consistency
of the common semantic space and Hamming space. Experimental results on three
benchmarks (i.e., CIFAR-10, NUS-WIDE, and ImageNet) demonstrate that the
proposed \textbf{ADSQ} can outperforms current state-of-the-art methods.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Compact-Codes 
      
        Quantization 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/ishaq2019clustered/">Clustered Hierarchical Entropy-Scaling Search of Astronomical and Biological Data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Clustered Hierarchical Entropy-Scaling Search of Astronomical and Biological Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Clustered Hierarchical Entropy-Scaling Search of Astronomical and Biological Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ishaq Najib, Student George, Daniels Noah M.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE International Conference on Big Data (Big Data)</td>
    <td>5</td>
    <td><p>Both astronomy and biology are experiencing explosive growth of data,
resulting in a “big data” problem that stands in the way of a “big data”
opportunity for discovery. One common question asked of such data is that of
approximate search (\(\rho-\)nearest neighbors search). We present a hierarchical
search algorithm for such data sets that takes advantage of particular
geometric properties apparent in both astronomical and biological data sets,
namely the metric entropy and fractal dimensionality of the data. We present
CHESS (Clustered Hierarchical Entropy-Scaling Search), a search tool with
virtually no loss in specificity or sensitivity, demonstrating a \(13.6\times\)
speedup over linear search on the Sloan Digital Sky Survey’s APOGEE data set
and a \(68\times\) speedup on the GreenGenes 16S metagenomic data set, as well as
asymptotically fewer distance comparisons on APOGEE when compared to the
FALCONN locality-sensitive hashing library. CHESS demonstrates an asymptotic
complexity not directly dependent on data set size, and is in practice at least
an order of magnitude faster than linear search by performing fewer distance
comparisons. Unlike locality-sensitive hashing approaches, CHESS can work with
any user-defined distance function. CHESS also allows for implicit data
compression, which we demonstrate on the APOGEE data set. We also discuss an
extension allowing for efficient k-nearest neighbors search.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Survey-Paper 
      
        Tools-&-Libraries 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/imbriaco2019aggregated/">Aggregated Deep Local Features for Remote Sensing Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Aggregated Deep Local Features for Remote Sensing Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Aggregated Deep Local Features for Remote Sensing Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Imbriaco Raffaele, Sebastian Clint, Bondarev Egor, de With Peter H. N.</td> <!-- 🔧 You were missing this -->
    <td>Remote Sensing</td>
    <td>73</td>
    <td><p>Remote Sensing Image Retrieval remains a challenging topic due to the special
nature of Remote Sensing Imagery. Such images contain various different
semantic objects, which clearly complicates the retrieval task. In this paper,
we present an image retrieval pipeline that uses attentive, local convolutional
features and aggregates them using the Vector of Locally Aggregated Descriptors
(VLAD) to produce a global descriptor. We study various system parameters such
as the multiplicative and additive attention mechanisms and descriptor
dimensionality. We propose a query expansion method that requires no external
inputs. Experiments demonstrate that even without training, the local
convolutional features and global representation outperform other systems.
After system tuning, we can achieve state-of-the-art or competitive results.
Furthermore, we observe that our query expansion method increases overall
system performance by about 3%, using only the top-three retrieved images.
Finally, we show how dimensionality reduction produces compact descriptors with
increased retrieval performance and fast retrieval computation times, e.g. 50%
faster than the current systems.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/rossetto2019query/">Query by Semantic Sketch</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Query by Semantic Sketch' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Query by Semantic Sketch' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Rossetto Luca, Gasser Ralph, Schuldt Heiko</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>Sketch-based query formulation is very common in image and video retrieval as
these techniques often complement textual retrieval methods that are based on
either manual or machine generated annotations. In this paper, we present a
retrieval approach that allows to query visual media collections by sketching
concept maps, thereby merging sketch-based retrieval with the search for
semantic labels. Users can draw a spatial distribution of different concept
labels, such as “sky”, “sea” or “person” and then use these sketches to find
images or video scenes that exhibit a similar distribution of these concepts.
Hence, this approach does not only take the semantic concepts themselves into
account, but also their semantic relations as well as their spatial context.
The efficient vector representation enables efficient retrieval even in large
multimedia collections. We have integrated the semantic sketch query mode into
our retrieval engine vitrivr and demonstrated its effectiveness.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Video-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/hu2025separated/">Separated Variational Hashing Networks for Cross-Modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Separated Variational Hashing Networks for Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Separated Variational Hashing Networks for Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hu Peng, Wang, Zhen, Peng</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 27th ACM International Conference on Multimedia</td>
    <td>30</td>
    <td><p>Cross-modal hashing, due to its low storage cost and high query speed, has been successfully used for similarity search in multimedia retrieval applications. It projects high-dimensional data into a shared isomorphic Hamming space with similar binary codes for semantically-similar data. In some applications, all modalities may not be obtained or trained simultaneously for some reasons, such as privacy, secret, storage limitation, and computational resource limitation. However, most existing cross-modal hashing methods need all modalities to jointly learn the common Hamming space, thus hindering them from handling these problems. In this paper, we propose a novel approach called Separated Variational Hashing Networks (SVHNs) to overcome the above challenge. Firstly, it adopts a label network (LabNet) to exploit available and nonspecific label annotations to learn a latent common Hamming space by projecting each semantic label into a common binary representation. Then, each modality-specific network can separately map the samples of the corresponding modality into their binary semantic codes learned by LabNet. We achieve it by conducting variational inference to match the aggregated posterior of the hashing code of LabNet with an arbitrary prior distribution. The effectiveness and efficiency of our SVHNs are verified by extensive experiments carried out on four widely-used multimedia databases, in comparison with 11 state-of-the-art approaches.</p>
</td>
    <td>
      
        Efficiency 
      
        Multimodal-Retrieval 
      
        Memory-Efficiency 
      
        Compact-Codes 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/huang2019accelerate/">Accelerate Learning of Deep Hashing With Gradient Attention</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Accelerate Learning of Deep Hashing With Gradient Attention' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Accelerate Learning of Deep Hashing With Gradient Attention' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Huang Long-kai, Chen, Pan</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>23</td>
    <td><p>Recent years have witnessed the success of learning to hash in fast large-scale image retrieval. As deep learning has shown its superior performance on many computer vision applications, recent designs of learning-based hashing models have been moving from shallow ones to deep architectures. However, based on our analysis, we find that gradient descent based algorithms used in deep hashing models would potentially cause hash codes of a pair of training instances to be updated towards the directions of each other simultaneously during optimization. In the worst case, the paired hash codes switch their directions after update, and consequently, their corresponding distance in the Hamming space remain unchanged. This makes the overall learning process highly inefficient. To address this issue, we propose a new deep hashing model integrated with a novel gradient attention mechanism. Extensive experimental results on three benchmark datasets show that our proposed algorithm is able to accelerate the learning process and obtain competitive retrieval performance compared with state-of-the-art deep hashing models.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        ICCV 
      
        Datasets 
      
        Neural-Hashing 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/hu2019separated/">Separated Variational Hashing Networks for Cross-Modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Separated Variational Hashing Networks for Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Separated Variational Hashing Networks for Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hu Peng, Wang, Zhen, Peng</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 27th ACM International Conference on Multimedia</td>
    <td>30</td>
    <td><p>Cross-modal hashing, due to its low storage cost and high query speed, has been successfully used for similarity search in multimedia retrieval applications. It projects high-dimensional data into a shared isomorphic Hamming space with similar binary codes for semantically-similar data. In some applications, all modalities may not be obtained or trained simultaneously for some reasons, such as privacy, secret, storage limitation, and computational resource limitation. However, most existing cross-modal hashing methods need all modalities to jointly learn the common Hamming space, thus hindering them from handling these problems. In this paper, we propose a novel approach called Separated Variational Hashing Networks (SVHNs) to overcome the above challenge. Firstly, it adopts a label network (LabNet) to exploit available and nonspecific label annotations to learn a latent common Hamming space by projecting each semantic label into a common binary representation. Then, each modality-specific network can separately map the samples of the corresponding modality into their binary semantic codes learned by LabNet. We achieve it by conducting variational inference to match the aggregated posterior of the hashing code of LabNet with an arbitrary prior distribution. The effectiveness and efficiency of our SVHNs are verified by extensive experiments carried out on four widely-used multimedia databases, in comparison with 11 state-of-the-art approaches.</p>
</td>
    <td>
      
        Efficiency 
      
        Multimodal-Retrieval 
      
        Memory-Efficiency 
      
        Compact-Codes 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/zhe2018directional/">Directional Statistics-based Deep Metric Learning for Image Classification and Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Directional Statistics-based Deep Metric Learning for Image Classification and Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Directional Statistics-based Deep Metric Learning for Image Classification and Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhe Xuefei, Chen Shifeng, Yan Hong</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition</td>
    <td>72</td>
    <td><p>Deep distance metric learning (DDML), which is proposed to learn image
similarity metrics in an end-to-end manner based on the convolution neural
network, has achieved encouraging results in many computer vision
tasks.\(L2\)-normalization in the embedding space has been used to improve the
performance of several DDML methods. However, the commonly used Euclidean
distance is no longer an accurate metric for \(L2\)-normalized embedding space,
i.e., a hyper-sphere. Another challenge of current DDML methods is that their
loss functions are usually based on rigid data formats, such as the triplet
tuple. Thus, an extra process is needed to prepare data in specific formats. In
addition, their losses are obtained from a limited number of samples, which
leads to a lack of the global view of the embedding space. In this paper, we
replace the Euclidean distance with the cosine similarity to better utilize the
\(L2\)-normalization, which is able to attenuate the curse of dimensionality.
More specifically, a novel loss function based on the von Mises-Fisher
distribution is proposed to learn a compact hyper-spherical embedding space.
Moreover, a new efficient learning algorithm is developed to better capture the
global structure of the embedding space. Experiments for both classification
and retrieval tasks on several standard datasets show that our method achieves
state-of-the-art performance with a simpler training procedure. Furthermore, we
demonstrate that, even with a small number of convolutional layers, our model
can still obtain significantly better classification performance than the
widely used softmax loss.</p>
</td>
    <td>
      
        Evaluation 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/he2025k/">K-Nearest Neighbors Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=K-Nearest Neighbors Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=K-Nearest Neighbors Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>He Xiangyu, Wang, Cheng</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>31</td>
    <td><p>Hashing based approximate nearest neighbor search embeds high dimensional data to compact binary codes, which
enables efficient similarity search and storage. However,
the non-isometry sign(·) function makes it hard to project
the nearest neighbors in continuous data space into the
closest codewords in discrete Hamming space. In this work,
we revisit the sign(·) function from the perspective of space partitioning.
In specific, we bridge the gap between
k-nearest neighbors and binary hashing codes with Shannon entropy. We further propose a novel K-Nearest Neighbors Hashing (KNNH) method to learn binary representations from KNN within the subspaces generated by sign(·).
Theoretical and experimental results show that the KNN relation is of central importance to neighbor preserving embeddings, and the proposed method outperforms the state-of-the-arts on benchmark datasets.</p>
</td>
    <td>
      
        Datasets 
      
        CVPR 
      
        Compact-Codes 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/he2019view/">View N-gram Network for 3D Object Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=View N-gram Network for 3D Object Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=View N-gram Network for 3D Object Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>He Xinwei, Huang Tengteng, Bai Song, Bai Xiang</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>57</td>
    <td><p>How to aggregate multi-view representations of a 3D object into an
informative and discriminative one remains a key challenge for multi-view 3D
object retrieval. Existing methods either use view-wise pooling strategies
which neglect the spatial information across different views or employ
recurrent neural networks which may face the efficiency problem. To address
these issues, we propose an effective and efficient framework called View
N-gram Network (VNN). Inspired by n-gram models in natural language processing,
VNN divides the view sequence into a set of visual n-grams, which involve
overlapping consecutive view sub-sequences. By doing so, spatial information
across multiple views is captured, which helps to learn a discriminative global
embedding for each 3D object. Experiments on 3D shape retrieval benchmarks,
including ModelNet10, ModelNet40 and ShapeNetCore55 datasets, demonstrate the
superiority of our proposed method.</p>
</td>
    <td>
      
        ICCV 
      
        Datasets 
      
        Tools-&-Libraries 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/he2019k/">K-Nearest Neighbors Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=K-Nearest Neighbors Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=K-Nearest Neighbors Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>He Xiangyu, Wang, Cheng</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>31</td>
    <td><p>Hashing based approximate nearest neighbor search embeds high dimensional data to compact binary codes, which
enables efficient similarity search and storage. However,
the non-isometry sign(·) function makes it hard to project
the nearest neighbors in continuous data space into the
closest codewords in discrete Hamming space. In this work,
we revisit the sign(·) function from the perspective of space partitioning.
In specific, we bridge the gap between
k-nearest neighbors and binary hashing codes with Shannon entropy. We further propose a novel K-Nearest Neighbors Hashing (KNNH) method to learn binary representations from KNN within the subspaces generated by sign(·).
Theoretical and experimental results show that the KNN relation is of central importance to neighbor preserving embeddings, and the proposed method outperforms the state-of-the-arts on benchmark datasets.</p>
</td>
    <td>
      
        Datasets 
      
        CVPR 
      
        Compact-Codes 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/hansen2025unsupervised/">Unsupervised Neural Generative Semantic Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Neural Generative Semantic Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Neural Generative Semantic Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hansen Casper, Hansen, Simonsen, Alstrup, Lioma</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>25</td>
    <td><p>Fast similarity search is a key component in large-scale information retrieval, where semantic hashing has become a popular strategy for representing documents as binary hash codes. Recent advances in this area have been obtained through neural network based models: generative models trained by learning to reconstruct the original documents. We present a novel unsupervised generative semantic hashing approach, \textit{Ranking based Semantic Hashing} (RBSH) that consists of both a variational and a ranking based component. Similarly to variational autoencoders, the variational component is trained to reconstruct the original document conditioned on its generated hash code, and as in prior work, it only considers documents individually. The ranking component solves this limitation by incorporating inter-document similarity into the hash code generation, modelling document ranking through a hinge loss. To circumvent the need for labelled data to compute the hinge loss, we use a weak labeller and thus keep the approach fully unsupervised.
Extensive experimental evaluation on four publicly available datasets against traditional baselines and recent state-of-the-art methods for semantic hashing shows that RBSH significantly outperforms all other methods across all evaluated hash code lengths. In fact, RBSH hash codes are able to perform similarly to state-of-the-art hash codes while using 2-4x fewer bits.</p>
</td>
    <td>
      
        Scalability 
      
        Datasets 
      
        Text-Retrieval 
      
        SIGIR 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Evaluation 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/zeng2019simultaneous/">Simultaneous Region Localization and Hash Coding for Fine-grained Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Simultaneous Region Localization and Hash Coding for Fine-grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Simultaneous Region Localization and Hash Coding for Fine-grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zeng Haien, Lai Hanjiang, Yin Jian</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>Fine-grained image hashing is a challenging problem due to the difficulties
of discriminative region localization and hash code generation. Most existing
deep hashing approaches solve the two tasks independently. While these two
tasks are correlated and can reinforce each other. In this paper, we propose a
deep fine-grained hashing to simultaneously localize the discriminative regions
and generate the efficient binary codes. The proposed approach consists of a
region localization module and a hash coding module. The region localization
module aims to provide informative regions to the hash coding module. The hash
coding module aims to generate effective binary codes and give feedback for
learning better localizer. Moreover, to better capture subtle differences,
multi-scale regions at different layers are learned without the need of
bounding-box/part annotations. Extensive experiments are conducted on two
public benchmark fine-grained datasets. The results demonstrate significant
improvements in the performance of our method relative to other fine-grained
hashing algorithms.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/tuinhof2018image/">Image Based Fashion Product Recommendation with Deep Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Image Based Fashion Product Recommendation with Deep Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Image Based Fashion Product Recommendation with Deep Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tuinhof Hessel, Pirker Clemens, Haltmeier Markus</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>65</td>
    <td><p>We develop a two-stage deep learning framework that recommends fashion images
based on other input images of similar style. For that purpose, a neural
network classifier is used as a data-driven, visually-aware feature extractor.
The latter then serves as input for similarity-based recommendations using a
ranking algorithm. Our approach is tested on the publicly available Fashion
dataset. Initialization strategies using transfer learning from larger product
databases are presented. Combined with more traditional content-based
recommendation systems, our framework can help to increase robustness and
performance, for example, by better matching a particular customer style.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Datasets 
      
        Recommender-Systems 
      
        Evaluation 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/hansen2019unsupervised/">Unsupervised Semantic Hashing with Pairwise Reconstruction</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Semantic Hashing with Pairwise Reconstruction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Semantic Hashing with Pairwise Reconstruction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hansen Casper, Hansen, Simonsen, Alstrup, Lioma</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>25</td>
    <td><p>Semantic Hashing is a popular family of methods for efficient similarity search in large-scale datasets. In Semantic Hashing, documents are encoded as short binary vectors (i.e., hash codes), such that semantic similarity can be efficiently computed using the Hamming distance. Recent state-of-the-art approaches have utilized weak supervision to train better performing hashing models. Inspired by this, we present Semantic Hashing with Pairwise Reconstruction (PairRec), which is a discrete variational autoencoder based hashing model. PairRec first encodes weakly supervised training pairs (a query document and a semantically similar document) into two hash codes, and then learns to reconstruct the same query document from both of these hash codes (i.e., pairwise reconstruction). This pairwise reconstruction enables our model to encode local neighbourhood structures within the hash code directly through the decoder. We experimentally compare PairRec to traditional and state-of-the-art approaches, and obtain significant performance improvements in the task of document similarity search.</p>
</td>
    <td>
      
        Scalability 
      
        Datasets 
      
        Text-Retrieval 
      
        SIGIR 
      
        Supervised 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Evaluation 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/sharma2019retrieving/">Retrieving Similar E-Commerce Images Using Deep Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Retrieving Similar E-Commerce Images Using Deep Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Retrieving Similar E-Commerce Images Using Deep Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sharma Rishab, Vishvakarma Anirudha</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>20</td>
    <td><p>In this paper, we propose a deep convolutional neural network for learning
the embeddings of images in order to capture the notion of visual similarity.
We present a deep siamese architecture that when trained on positive and
negative pairs of images learn an embedding that accurately approximates the
ranking of images in order of visual similarity notion. We also implement a
novel loss calculation method using an angular loss metrics based on the
problems requirement. The final embedding of the image is combined
representation of the lower and top-level embeddings. We used fractional
distance matrix to calculate the distance between the learned embeddings in
n-dimensional space. In the end, we compare our architecture with other
existing deep architecture and go on to demonstrate the superiority of our
solution in terms of image retrieval by testing the architecture on four
datasets. We also show how our suggested network is better than the other
traditional deep CNNs used for capturing fine-grained image similarities by
learning an optimum embedding.</p>
</td>
    <td>
      
        Datasets 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/guan2019post/">Post-Training 4-bit Quantization on Embedding Tables</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Post-Training 4-bit Quantization on Embedding Tables' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Post-Training 4-bit Quantization on Embedding Tables' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Guan Hui, Malevich Andrey, Yang Jiyan, Park Jongsoo, Yuen Hector</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>16</td>
    <td><p>Continuous representations have been widely adopted in recommender systems
where a large number of entities are represented using embedding vectors. As
the cardinality of the entities increases, the embedding components can easily
contain millions of parameters and become the bottleneck in both storage and
inference due to large memory consumption. This work focuses on post-training
4-bit quantization on the continuous embeddings. We propose row-wise uniform
quantization with greedy search and codebook-based quantization that
consistently outperforms state-of-the-art quantization approaches on reducing
accuracy degradation. We deploy our uniform quantization technique on a
production model in Facebook and demonstrate that it can reduce the model size
to only 13.89% of the single-precision version while the model quality stays
neutral.</p>
</td>
    <td>
      
        Recommender-Systems 
      
        Quantization 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/guo2019accelerating/">Accelerating Large-Scale Inference with Anisotropic Vector Quantization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Accelerating Large-Scale Inference with Anisotropic Vector Quantization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Accelerating Large-Scale Inference with Anisotropic Vector Quantization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Guo Ruiqi, Sun Philip, Lindgren Erik, Geng Quan, Simcha David, Chern Felix, Kumar Sanjiv</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>91</td>
    <td><p>Quantization based techniques are the current state-of-the-art for scaling
maximum inner product search to massive databases. Traditional approaches to
quantization aim to minimize the reconstruction error of the database points.
Based on the observation that for a given query, the database points that have
the largest inner products are more relevant, we develop a family of
anisotropic quantization loss functions. Under natural statistical assumptions,
we show that quantization with these loss functions leads to a new variant of
vector quantization that more greatly penalizes the parallel component of a
datapoint’s residual relative to its orthogonal component. The proposed
approach achieves state-of-the-art results on the public benchmarks available
at \url{ann-benchmarks.com}.</p>
</td>
    <td>
      
        Scalability 
      
        Quantization 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/sanakoyeu2019divide/">Divide and Conquer the Embedding Space for Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Divide and Conquer the Embedding Space for Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Divide and Conquer the Embedding Space for Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sanakoyeu Artsiom, Tschernezki Vadim, Büchler Uta, Ommer Björn</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>122</td>
    <td><p>Learning the embedding space, where semantically similar objects are located
close together and dissimilar objects far apart, is a cornerstone of many
computer vision applications. Existing approaches usually learn a single metric
in the embedding space for all available data points, which may have a very
complex non-uniform distribution with different notions of similarity between
objects, e.g. appearance, shape, color or semantic meaning. Approaches for
learning a single distance metric often struggle to encode all different types
of relationships and do not generalize well. In this work, we propose a novel
easy-to-implement divide and conquer approach for deep metric learning, which
significantly improves the state-of-the-art performance of metric learning. Our
approach utilizes the embedding space more efficiently by jointly splitting the
embedding space and data into \(K\) smaller sub-problems. It divides both, the
data and the embedding space into \(K\) subsets and learns \(K\) separate distance
metrics in the non-overlapping subspaces of the embedding space, defined by
groups of neurons in the embedding layer of the neural network. The proposed
approach increases the convergence speed and improves generalization since the
complexity of each sub-problem is reduced compared to the original one. We show
that our approach outperforms the state-of-the-art by a large margin in
retrieval, clustering and re-identification tasks on CUB200-2011, CARS196,
Stanford Online Products, In-shop Clothes and PKU VehicleID datasets.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Datasets 
      
        CVPR 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/gomez2019self/">Self-Supervised Learning from Web Data for Multimodal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Self-Supervised Learning from Web Data for Multimodal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Self-Supervised Learning from Web Data for Multimodal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gomez Raul, Gomez Lluis, Gibert Jaume, Karatzas Dimosthenis</td> <!-- 🔧 You were missing this -->
    <td>Multimodal Scene Understanding</td>
    <td>13</td>
    <td><p>Self-Supervised learning from multimodal image and text data allows deep
neural networks to learn powerful features with no need of human annotated
data. Web and Social Media platforms provide a virtually unlimited amount of
this multimodal data. In this work we propose to exploit this free available
data to learn a multimodal image and text embedding, aiming to leverage the
semantic knowledge learnt in the text domain and transfer it to a visual model
for semantic image retrieval. We demonstrate that the proposed pipeline can
learn from images with associated textwithout supervision and analyze the
semantic structure of the learnt joint image and text embedding space. We
perform a thorough analysis and performance comparison of five different state
of the art text embeddings in three different benchmarks. We show that the
embeddings learnt with Web and Social Media data have competitive performances
over supervised methods in the text based image retrieval task, and we clearly
outperform state of the art in the MIRFlickr dataset when training in the
target data. Further, we demonstrate how semantic multimodal image retrieval
can be performed using the learnt embeddings, going beyond classical
instance-level retrieval problems. Finally, we present a new dataset,
InstaCities1M, composed by Instagram images and their associated texts that can
be used for fair comparison of image-text embeddings.</p>
</td>
    <td>
      
        Supervised 
      
        Image-Retrieval 
      
        Datasets 
      
        Self-Supervised 
      
        Evaluation 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/gomez2018learning/">Learning to Learn from Web Data through Deep Semantic Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning to Learn from Web Data through Deep Semantic Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning to Learn from Web Data through Deep Semantic Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gomez Raul, Gomez Lluis, Gibert Jaume, Karatzas Dimosthenis</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>23</td>
    <td><p>In this paper we propose to learn a multimodal image and text embedding from
Web and Social Media data, aiming to leverage the semantic knowledge learnt in
the text domain and transfer it to a visual model for semantic image retrieval.
We demonstrate that the pipeline can learn from images with associated text
without supervision and perform a thourough analysis of five different text
embeddings in three different benchmarks. We show that the embeddings learnt
with Web and Social Media data have competitive performances over supervised
methods in the text based image retrieval task, and we clearly outperform state
of the art in the MIRFlickr dataset when training in the target data. Further
we demonstrate how semantic multimodal image retrieval can be performed using
the learnt embeddings, going beyond classical instance-level retrieval
problems. Finally, we present a new dataset, InstaCities1M, composed by
Instagram images and their associated texts that can be used for fair
comparison of image-text embeddings.</p>
</td>
    <td>
      
        Datasets 
      
        Supervised 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/gominski2019challenging/">Challenging deep image descriptors for retrieval in heterogeneous iconographic collections</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Challenging deep image descriptors for retrieval in heterogeneous iconographic collections' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Challenging deep image descriptors for retrieval in heterogeneous iconographic collections' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gominski Dimitri Lastig, Poreba Martyna Lastig, Gouet-brunet Valérie Lastig, Chen Liming Lastig</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 1st Workshop on Structuring and Understanding of Multimedia heritAge Contents</td>
    <td>7</td>
    <td><p>This article proposes to study the behavior of recent and efficient
state-of-the-art deep-learning based image descriptors for content-based image
retrieval, facing a panel of complex variations appearing in heterogeneous
image datasets, in particular in cultural collections that may involve
multi-source, multi-date and multi-view Permission to make digital</p>
</td>
    <td>
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/guo2019hierarchical/">Hierarchical Document Encoder for Parallel Corpus Mining</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hierarchical Document Encoder for Parallel Corpus Mining' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hierarchical Document Encoder for Parallel Corpus Mining' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Guo Mandy, Yang Yinfei, Stevens Keith, Cer Daniel, Ge Heming, Sung Yun-hsuan, Strope Brian, Kurzweil Ray</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)</td>
    <td>22</td>
    <td><p>We explore using multilingual document embeddings for nearest neighbor mining
of parallel data. Three document-level representations are investigated: (i)
document embeddings generated by simply averaging multilingual sentence
embeddings; (ii) a neural bag-of-words (BoW) document encoding model; (iii) a
hierarchical multilingual document encoder (HiDE) that builds on our
sentence-level model. The results show document embeddings derived from
sentence-level averaging are surprisingly effective for clean datasets, but
suggest models trained hierarchically at the document-level are more effective
on noisy data. Analysis experiments demonstrate our hierarchical models are
very robust to variations in the underlying sentence embedding quality. Using
document embeddings trained with HiDE achieves state-of-the-art performance on
United Nations (UN) parallel document mining, 94.9% P@1 for en-fr and 97.3% P@1
for en-es.</p>
</td>
    <td>
      
        Datasets 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/shen2018matchable/">Matchable Image Retrieval by Learning from Surface Reconstruction</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Matchable Image Retrieval by Learning from Surface Reconstruction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Matchable Image Retrieval by Learning from Surface Reconstruction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shen Tianwei, Luo Zixin, Zhou Lei, Zhang Runze, Zhu Siyu, Fang Tian, Quan Long</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>49</td>
    <td><p>Convolutional Neural Networks (CNNs) have achieved superior performance on
object image retrieval, while Bag-of-Words (BoW) models with handcrafted local
features still dominate the retrieval of overlapping images in 3D
reconstruction. In this paper, we narrow down this gap by presenting an
efficient CNN-based method to retrieve images with overlaps, which we refer to
as the matchable image retrieval problem. Different from previous methods that
generates training data based on sparse reconstruction, we create a large-scale
image database with rich 3D geometrics and exploit information from surface
reconstruction to obtain fine-grained training data. We propose a batched
triplet-based loss function combined with mesh re-projection to effectively
learn the CNN representation. The proposed method significantly accelerates the
image retrieval process in 3D reconstruction and outperforms the
state-of-the-art CNN-based and BoW methods for matchable image retrieval. The
code and data are available at https://github.com/hlzz/mirror.</p>
</td>
    <td>
      
        Scalability 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/gillick2019learning/">Learning Dense Representations for Entity Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Dense Representations for Entity Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Dense Representations for Entity Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gillick Daniel, Kulkarni Sayali, Lansing Larry, Presta Alessandro, Baldridge Jason, Ie Eugene, Garcia-olano Diego</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</td>
    <td>189</td>
    <td><p>We show that it is feasible to perform entity linking by training a dual
encoder (two-tower) model that encodes mentions and entities in the same dense
vector space, where candidate entities are retrieved by approximate nearest
neighbor search. Unlike prior work, this setup does not rely on an alias table
followed by a re-ranker, and is thus the first fully learned entity retrieval
model. We show that our dual encoder, trained using only anchor-text links in
Wikipedia, outperforms discrete alias table and BM25 baselines, and is
competitive with the best comparable results on the standard TACKBP-2010
dataset. In addition, it can retrieve candidates extremely fast, and
generalizes well to a new dataset derived from Wikinews. On the modeling side,
we demonstrate the dramatic value of an unsupervised negative mining algorithm
for this task.</p>
</td>
    <td>
      
        Unsupervised 
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/gildenblat2019self/">Self-Supervised Similarity Learning for Digital Pathology</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Self-Supervised Similarity Learning for Digital Pathology' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Self-Supervised Similarity Learning for Digital Pathology' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gildenblat Jacob, Klaiman Eldad</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>36</td>
    <td><p>Using features extracted from networks pretrained on ImageNet is a common
practice in applications of deep learning for digital pathology. However it
presents the downside of missing domain specific image information. In digital
pathology, supervised training data is expensive and difficult to collect. We
propose a self-supervised method for feature extraction by similarity learning
on whole slide images (WSI) that is simple to implement and allows creation of
robust and compact image descriptors. We train a siamese network, exploiting
image spatial continuity and assuming spatially adjacent tiles in the image are
more similar to each other than distant tiles. Our network outputs feature
vectors of length 128, which allows dramatically lower memory storage and
faster processing than networks pretrained on ImageNet. We apply the method on
digital pathology WSIs from the Camelyon16 train set and assess and compare our
method by measuring image retrieval of tumor tiles and descriptor pair distance
ratio for distant/near tiles in the Camelyon16 test set. We show that our
method yields better retrieval task results than existing ImageNet based and
generic self-supervised feature extraction methods. To the best of our
knowledge, this is also the first published method for self-supervised learning
tailored for digital pathology.</p>
</td>
    <td>
      
        Self-Supervised 
      
        Supervised 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/gattupalli2025weakly/">Weakly Supervised Deep Image Hashing through Tag Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Weakly Supervised Deep Image Hashing through Tag Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Weakly Supervised Deep Image Hashing through Tag Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gattupalli Vijetha, Zhuo, Li</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>42</td>
    <td><p>Many approaches to semantic image hashing have been formulated as supervised learning problems that utilize images and label information to learn the binary hash codes. However, large-scale labeled image data is expensive to obtain, thus imposing a restriction on the usage of such algorithms. On the other hand, unlabelled image data is abundant due to the existence of many Web image repositories. Such Web images may often come with images tags that contain useful information, although raw tags, in general, do not readily lead to semantic labels.
Motivated by this scenario, we formulate the problem of semantic image hashing as a weakly-supervised learning problem. We utilize the information contained in the user-generated tags associated with the images to learn the hash codes. More specifically, we extract the word2vec semantic embeddings of the tags and use the information contained in them for constraining the learning.
Accordingly, we name our model Weakly Supervised Deep Hashing using Tag Embeddings (WDHT). WDHT is tested for the task of semantic image retrieval and is compared against several state-of-art models. Results show that our approach sets a new state-of-art in the area of weekly supervised image hashing.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        CVPR 
      
        Neural-Hashing 
      
        Hashing-Methods 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/gattupalli2019weakly/">Weakly Supervised Deep Image Hashing through Tag Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Weakly Supervised Deep Image Hashing through Tag Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Weakly Supervised Deep Image Hashing through Tag Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gattupalli Vijetha, Zhuo, Li</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>42</td>
    <td><p>Many approaches to semantic image hashing have been formulated as supervised learning problems that utilize images and label information to learn the binary hash codes. However, large-scale labeled image data is expensive to obtain, thus imposing a restriction on the usage of such algorithms. On the other hand, unlabelled image data is abundant due to the existence of many Web image repositories. Such Web images may often come with images tags that contain useful information, although raw tags, in general, do not readily lead to semantic labels.
Motivated by this scenario, we formulate the problem of semantic image hashing as a weakly-supervised learning problem. We utilize the information contained in the user-generated tags associated with the images to learn the hash codes. More specifically, we extract the word2vec semantic embeddings of the tags and use the information contained in them for constraining the learning.
Accordingly, we name our model Weakly Supervised Deep Hashing using Tag Embeddings (WDHT). WDHT is tested for the task of semantic image retrieval and is compared against several state-of-art models. Results show that our approach sets a new state-of-art in the area of weekly supervised image hashing.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        CVPR 
      
        Neural-Hashing 
      
        Hashing-Methods 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/garcia2017learning/">Learning Non-Metric Visual Similarity for Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Non-Metric Visual Similarity for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Non-Metric Visual Similarity for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Garcia Noa, Vogiatzis George</td> <!-- 🔧 You were missing this -->
    <td>Image and Vision Computing</td>
    <td>40</td>
    <td><p>Measuring visual similarity between two or more instances within a data
distribution is a fundamental task in image retrieval. Theoretically,
non-metric distances are able to generate a more complex and accurate
similarity model than metric distances, provided that the non-linear data
distribution is precisely captured by the system. In this work, we explore
neural networks models for learning a non-metric similarity function for
instance search. We argue that non-metric similarity functions based on neural
networks can build a better model of human visual perception than standard
metric distances. As our proposed similarity function is differentiable, we
explore a real end-to-end trainable approach for image retrieval, i.e. we learn
the weights from the input image pixels to the final similarity score.
Experimental evaluation shows that non-metric similarity networks are able to
learn visual similarities between images and improve performance on top of
state-of-the-art image representations, boosting results in standard image
retrieval datasets with respect standard metric distances.</p>
</td>
    <td>
      
        Datasets 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/shin2019semi/">Semi-supervised Feature-Level Attribute Manipulation for Fashion Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semi-supervised Feature-Level Attribute Manipulation for Fashion Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semi-supervised Feature-Level Attribute Manipulation for Fashion Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shin Minchul, Park Sanghyuk, Kim Taeksoo</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>9</td>
    <td><p>With a growing demand for the search by image, many works have studied the
task of fashion instance-level image retrieval (FIR). Furthermore, the recent
works introduce a concept of fashion attribute manipulation (FAM) which
manipulates a specific attribute (e.g color) of a fashion item while
maintaining the rest of the attributes (e.g shape, and pattern). In this way,
users can search not only “the same” items but also “similar” items with the
desired attributes. FAM is a challenging task in that the attributes are hard
to define, and the unique characteristics of a query are hard to be preserved.
Although both FIR and FAM are important in real-life applications, most of the
previous studies have focused on only one of these problem. In this study, we
aim to achieve competitive performance on both FIR and FAM. To do so, we
propose a novel method that converts a query into a representation with the
desired attributes. We introduce a new idea of attribute manipulation at the
feature level, by matching the distribution of manipulated features with real
features. In this fashion, the attribute manipulation can be done independently
from learning a representation from the image. By introducing the feature-level
attribute manipulation, the previous methods for FIR can perform attribute
manipulation without sacrificing their retrieval performance.</p>
</td>
    <td>
      
        Supervised 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/garcia2019context/">Context-Aware Embeddings for Automatic Art Analysis</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Context-Aware Embeddings for Automatic Art Analysis' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Context-Aware Embeddings for Automatic Art Analysis' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Garcia Noa, Renoust Benjamin, Nakashima Yuta</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2019 on International Conference on Multimedia Retrieval</td>
    <td>26</td>
    <td><p>Automatic art analysis aims to classify and retrieve artistic representations
from a collection of images by using computer vision and machine learning
techniques. In this work, we propose to enhance visual representations from
neural networks with contextual artistic information. Whereas visual
representations are able to capture information about the content and the style
of an artwork, our proposed context-aware embeddings additionally encode
relationships between different artistic attributes, such as author, school, or
historical period. We design two different approaches for using context in
automatic art analysis. In the first one, contextual data is obtained through a
multi-task learning model, in which several attributes are trained together to
find visual relationships between elements. In the second approach, context is
obtained through an art-specific knowledge graph, which encodes relationships
between artistic attributes. An exhaustive evaluation of both of our models in
several art analysis problems, such as author identification, type
classification, or cross-modal retrieval, show that performance is improved by
up to 7.3% in art classification and 37.24% in retrieval when context-aware
embeddings are used.</p>
</td>
    <td>
      
        Evaluation 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/gattupalli2018weakly/">Weakly Supervised Deep Image Hashing through Tag Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Weakly Supervised Deep Image Hashing through Tag Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Weakly Supervised Deep Image Hashing through Tag Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gattupalli Vijetha, Zhuo Yaoxin, Li Baoxin</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>42</td>
    <td><p>Many approaches to semantic image hashing have been formulated as supervised
learning problems that utilize images and label information to learn the binary
hash codes. However, large-scale labeled image data is expensive to obtain,
thus imposing a restriction on the usage of such algorithms. On the other hand,
unlabelled image data is abundant due to the existence of many Web image
repositories. Such Web images may often come with images tags that contain
useful information, although raw tags, in general, do not readily lead to
semantic labels. Motivated by this scenario, we formulate the problem of
semantic image hashing as a weakly-supervised learning problem. We utilize the
information contained in the user-generated tags associated with the images to
learn the hash codes. More specifically, we extract the word2vec semantic
embeddings of the tags and use the information contained in them for
constraining the learning. Accordingly, we name our model Weakly Supervised
Deep Hashing using Tag Embeddings (WDHT). WDHT is tested for the task of
semantic image retrieval and is compared against several state-of-art models.
Results show that our approach sets a new state-of-art in the area of weekly
supervised image hashing.</p>
</td>
    <td>
      
        Supervised 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Neural-Hashing 
      
        CVPR 
      
        Scalability 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/wen2019adversarial/">Adversarial Cross-Modal Retrieval via Learning and Transferring Single-Modal Similarities</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Adversarial Cross-Modal Retrieval via Learning and Transferring Single-Modal Similarities' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Adversarial Cross-Modal Retrieval via Learning and Transferring Single-Modal Similarities' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wen Xin, Han Zhizhong, Yin Xinyu, Liu Yu-shen</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE International Conference on Multimedia and Expo (ICME)</td>
    <td>18</td>
    <td><p>Cross-modal retrieval aims to retrieve relevant data across different
modalities (e.g., texts vs. images). The common strategy is to apply
element-wise constraints between manually labeled pair-wise items to guide the
generators to learn the semantic relationships between the modalities, so that
the similar items can be projected close to each other in the common
representation subspace. However, such constraints often fail to preserve the
semantic structure between unpaired but semantically similar items (e.g. the
unpaired items with the same class label are more similar than items with
different labels). To address the above problem, we propose a novel cross-modal
similarity transferring (CMST) method to learn and preserve the semantic
relationships between unpaired items in an unsupervised way. The key idea is to
learn the quantitative similarities in single-modal representation subspace,
and then transfer them to the common representation subspace to establish the
semantic relationships between unpaired items across modalities. Experiments
show that our method outperforms the state-of-the-art approaches both in the
class-based and pair-based retrieval tasks.</p>
</td>
    <td>
      
        Unsupervised 
      
        Multimodal-Retrieval 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/fu2017fast/">Fast Approximate Nearest Neighbor Search With The Navigating Spreading-out Graph</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Approximate Nearest Neighbor Search With The Navigating Spreading-out Graph' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Approximate Nearest Neighbor Search With The Navigating Spreading-out Graph' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fu Cong, Xiang Chao, Wang Changxu, Cai Deng</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the VLDB Endowment</td>
    <td>219</td>
    <td><p>Approximate nearest neighbor search (ANNS) is a fundamental problem in databases and data mining. A scalable ANNS algorithm should be both memory-efficient and fast. Some early graph-based approaches have shown attractive theoretical guarantees on search time complexity, but they all suffer from the problem of high indexing time complexity. Recently, some graph-based methods have been proposed to reduce indexing complexity by approximating the traditional graphs; these methods have achieved revolutionary performance on million-scale datasets. Yet, they still can not scale to billion-node databases. In this paper, to further improve the search-efficiency and scalability of graph-based methods, we start by introducing four aspects: (1) ensuring the connectivity of the graph; (2) lowering the average out-degree of the graph for fast traversal; (3) shortening the search path; and (4) reducing the index size. Then, we propose a novel graph structure called Monotonic Relative Neighborhood Graph (MRNG) which guarantees very low search complexity (close to logarithmic time). To further lower the indexing complexity and make it practical for billion-node ANNS problems, we propose a novel graph structure named Navigating Spreading-out Graph (NSG) by approximating the MRNG. The NSG takes the four aspects into account simultaneously. Extensive experiments show that NSG outperforms all the existing algorithms significantly. In addition, NSG shows superior performance in the E-commercial search scenario of Taobao (Alibaba Group) and has been integrated into their search engine at billion-node scale.</p>
</td>
    <td>
      
        Graph-Based-ANN 
      
        Datasets 
      
        Scalability 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/zhai2019learning/">Learning a Unified Embedding for Visual Search at Pinterest</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning a Unified Embedding for Visual Search at Pinterest' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning a Unified Embedding for Visual Search at Pinterest' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhai Andrew, Wu Hao-yu, Tzeng Eric, Park Dong Huk, Rosenberg Charles</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</td>
    <td>33</td>
    <td><p>At Pinterest, we utilize image embeddings throughout our search and
recommendation systems to help our users navigate through visual content by
powering experiences like browsing of related content and searching for exact
products for shopping. In this work we describe a multi-task deep metric
learning system to learn a single unified image embedding which can be used to
power our multiple visual search products. The solution we present not only
allows us to train for multiple application objectives in a single deep neural
network architecture, but takes advantage of correlated information in the
combination of all training data from each application to generate a unified
embedding that outperforms all specialized embeddings previously deployed for
each product. We discuss the challenges of handling images from different
domains such as camera photos, high quality web images, and clean product
catalog images. We also detail how to jointly train for multiple product
objectives and how to leverage both engagement data and human labeled data. In
addition, our trained embeddings can also be binarized for efficient storage
and retrieval without compromising precision and recall. Through comprehensive
evaluations on offline metrics, user studies, and online A/B experiments, we
demonstrate that our proposed unified embedding improves both relevance and
engagement of our visual search products for both browsing and searching
purposes when compared to existing specialized embeddings. Finally, the
deployment of the unified embedding at Pinterest has drastically reduced the
operation and engineering cost of maintaining multiple embeddings while
improving quality.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Recommender-Systems 
      
        Compact-Codes 
      
        KDD 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/zhong2020compact/">Compact Deep Aggregation for Set Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Compact Deep Aggregation for Set Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Compact Deep Aggregation for Set Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhong Yujie, Arandjelović Relja, Zisserman Andrew</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>8</td>
    <td><p>The objective of this work is to learn a compact embedding of a set of
descriptors that is suitable for efficient retrieval and ranking, whilst
maintaining discriminability of the individual descriptors. We focus on a
specific example of this general problem – that of retrieving images
containing multiple faces from a large scale dataset of images. Here the set
consists of the face descriptors in each image, and given a query for multiple
identities, the goal is then to retrieve, in order, images which contain all
the identities, all but one, \etc
  To this end, we make the following contributions: first, we propose a CNN
architecture – {\em SetNet} – to achieve the objective: it learns face
descriptors and their aggregation over a set to produce a compact fixed length
descriptor designed for set retrieval, and the score of an image is a count of
the number of identities that match the query; second, we show that this
compact descriptor has minimal loss of discriminability up to two faces per
image, and degrades slowly after that – far exceeding a number of baselines;
third, we explore the speed vs.\ retrieval quality trade-off for set retrieval
using this compact descriptor; and, finally, we collect and annotate a large
dataset of images containing various number of celebrities, which we use for
evaluation and is publicly released.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Datasets 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/eghbali2019deep/">Deep Spherical Quantization for Image Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Spherical Quantization for Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Spherical Quantization for Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Eghbali Sepehr, Tahvildari Ladan</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>27</td>
    <td><p>Hashing methods, which encode high-dimensional images with compact discrete
codes, have been widely applied to enhance large-scale image retrieval. In this
paper, we put forward Deep Spherical Quantization (DSQ), a novel method to make
deep convolutional neural networks generate supervised and compact binary codes
for efficient image search. Our approach simultaneously learns a mapping that
transforms the input images into a low-dimensional discriminative space, and
quantizes the transformed data points using multi-codebook quantization. To
eliminate the negative effect of norm variance on codebook learning, we force
the network to L_2 normalize the extracted features and then quantize the
resulting vectors using a new supervised quantization technique specifically
designed for points lying on a unit hypersphere. Furthermore, we introduce an
easy-to-implement extension of our quantization technique that enforces
sparsity on the codebooks. Extensive experiments demonstrate that DSQ and its
sparse variant can generate semantically separable compact binary codes
outperforming many state-of-the-art image retrieval methods on three
benchmarks.</p>
</td>
    <td>
      
        Supervised 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Quantization 
      
        Compact-Codes 
      
        CVPR 
      
        Scalability 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/sankar2019transferable/">Transferable Neural Projection Representations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Transferable Neural Projection Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Transferable Neural Projection Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sankar Chinnadhurai, Ravi Sujith, Kozareva Zornitsa</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2019 Conference of the North</td>
    <td>7</td>
    <td><p>Neural word representations are at the core of many state-of-the-art natural
language processing models. A widely used approach is to pre-train, store and
look up word or character embedding matrices. While useful, such
representations occupy huge memory making it hard to deploy on-device and often
do not generalize to unknown words due to vocabulary pruning.
  In this paper, we propose a skip-gram based architecture coupled with
Locality-Sensitive Hashing (LSH) projections to learn efficient dynamically
computable representations. Our model does not need to store lookup tables as
representations are computed on-the-fly and require low memory footprint. The
representations can be trained in an unsupervised fashion and can be easily
transferred to other NLP tasks. For qualitative evaluation, we analyze the
nearest neighbors of the word representations and discover semantically similar
words even with misspellings. For quantitative evaluation, we plug our
transferable projections into a simple LSTM and run it on multiple NLP tasks
and show how our transferable projections achieve better performance compared
to prior work.</p>
</td>
    <td>
      
        Locality-Sensitive-Hashing 
      
        Hashing-Methods 
      
        Memory-Efficiency 
      
        Unsupervised 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/ducau2019automatic/">Automatic Malware Description via Attribute Tagging and Similarity Embedding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Automatic Malware Description via Attribute Tagging and Similarity Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Automatic Malware Description via Attribute Tagging and Similarity Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ducau Felipe N., Rudd Ethan M., Heppner Tad M., Long Alex, Berlin Konstantin</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>11</td>
    <td><p>With the rapid proliferation and increased sophistication of malicious
software (malware), detection methods no longer rely only on manually generated
signatures but have also incorporated more general approaches like machine
learning detection. Although powerful for conviction of malicious artifacts,
these methods do not produce any further information about the type of threat
that has been detected neither allows for identifying relationships between
malware samples. In this work, we address the information gap between machine
learning and signature-based detection methods by learning a representation
space for malware samples in which files with similar malicious behaviors
appear close to each other. We do so by introducing a deep learning based
tagging model trained to generate human-interpretable semantic descriptions of
malicious software, which, at the same time provides potentially more useful
and flexible information than malware family names.
  We show that the malware descriptions generated with the proposed approach
correctly identify more than 95% of eleven possible tag descriptions for a
given sample, at a deployable false positive rate of 1% per tag. Furthermore,
we use the learned representation space to introduce a similarity index between
malware files, and empirically demonstrate using dynamic traces from files’
execution, that is not only more effective at identifying samples from the same
families, but also 32 times smaller than those based on raw feature vectors.</p>
</td>
    <td>
      
        Uncategorized 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/doras2019cover/">Cover Detection using Dominant Melody Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cover Detection using Dominant Melody Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cover Detection using Dominant Melody Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Doras Guillaume, Peeters Geoffroy</td> <!-- 🔧 You were missing this -->
    <td>20th International Society for Music Information Retrieval Conference Delft The Netherlands 2019</td>
    <td>22</td>
    <td><p>Automatic cover detection – the task of finding in an audio database all the
covers of one or several query tracks – has long been seen as a challenging
theoretical problem in the MIR community and as an acute practical problem for
authors and composers societies. Original algorithms proposed for this task
have proven their accuracy on small datasets, but are unable to scale up to
modern real-life audio corpora. On the other hand, faster approaches designed
to process thousands of pairwise comparisons resulted in lower accuracy, making
them unsuitable for practical use.
  In this work, we propose a neural network architecture that is trained to
represent each track as a single embedding vector. The computation burden is
therefore left to the embedding extraction – that can be conducted offline and
stored, while the pairwise comparison task reduces to a simple Euclidean
distance computation. We further propose to extract each track’s embedding out
of its dominant melody representation, obtained by another neural network
trained for this task. We then show that this architecture improves
state-of-the-art accuracy both on small and large datasets, and is able to
scale to query databases of thousands of tracks in a few seconds.</p>
</td>
    <td>
      
        Datasets 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/zhang2019generic/">Generic Intent Representation in Web Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Generic Intent Representation in Web Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Generic Intent Representation in Web Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Hongfei, Song Xia, Xiong Chenyan, Rosset Corby, Bennett Paul N., Craswell Nick, Tiwary Saurabh</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>33</td>
    <td><p>This paper presents GEneric iNtent Encoder (GEN Encoder) which learns a
distributed representation space for user intent in search. Leveraging large
scale user clicks from Bing search logs as weak supervision of user intent, GEN
Encoder learns to map queries with shared clicks into similar embeddings
end-to-end and then finetunes on multiple paraphrase tasks. Experimental
results on an intrinsic evaluation task - query intent similarity modeling -
demonstrate GEN Encoder’s robust and significant advantages over previous
representation methods. Ablation studies reveal the crucial role of learning
from implicit user feedback in representing user intent and the contributions
of multi-task learning in representation generality. We also demonstrate that
GEN Encoder alleviates the sparsity of tail search traffic and cuts down half
of the unseen queries by using an efficient approximate nearest neighbor search
to effectively identify previous queries with the same search intent. Finally,
we demonstrate distances between GEN encodings reflect certain information
seeking behaviors in search sessions.</p>
</td>
    <td>
      
        SIGIR 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/dong2019document/">Document Hashing with Mixture-Prior Generative Models</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Document Hashing with Mixture-Prior Generative Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Document Hashing with Mixture-Prior Generative Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dong Wei, Su Qinliang, Shen Dinghan, Chen Changyou</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</td>
    <td>19</td>
    <td><p>Hashing is promising for large-scale information retrieval tasks thanks to
the efficiency of distance evaluation between binary codes. Generative hashing
is often used to generate hashing codes in an unsupervised way. However,
existing generative hashing methods only considered the use of simple priors,
like Gaussian and Bernoulli priors, which limits these methods to further
improve their performance. In this paper, two mixture-prior generative models
are proposed, under the objective to produce high-quality hashing codes for
documents. Specifically, a Gaussian mixture prior is first imposed onto the
variational auto-encoder (VAE), followed by a separate step to cast the
continuous latent representation of VAE into binary code. To avoid the
performance loss caused by the separate casting, a model using a Bernoulli
mixture prior is further developed, in which an end-to-end training is admitted
by resorting to the straight-through (ST) discrete gradient estimator.
Experimental results on several benchmark datasets demonstrate that the
proposed methods, especially the one using Bernoulli mixture priors,
consistently outperform existing ones by a substantial margin.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        Compact-Codes 
      
        EMNLP 
      
        Unsupervised 
      
        Scalability 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/sim%C3%A9oni2019local/">Local Features and Visual Words Emerge in Activations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Local Features and Visual Words Emerge in Activations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Local Features and Visual Words Emerge in Activations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Siméoni Oriane, Avrithis Yannis, Chum Ondrej</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>70</td>
    <td><p>We propose a novel method of deep spatial matching (DSM) for image retrieval.
Initial ranking is based on image descriptors extracted from convolutional
neural network activations by global pooling, as in recent state-of-the-art
work. However, the same sparse 3D activation tensor is also approximated by a
collection of local features. These local features are then robustly matched to
approximate the optimal alignment of the tensors. This happens without any
network modification, additional layers or training. No local feature detection
happens on the original image. No local feature descriptors and no visual
vocabulary are needed throughout the whole process.
  We experimentally show that the proposed method achieves the state-of-the-art
performance on standard benchmarks across different network architectures and
different global pooling methods. The highest gain in performance is achieved
when diffusion on the nearest-neighbor graph of global descriptors is initiated
from spatially verified images.</p>
</td>
    <td>
      
        CVPR 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/shen2019embarrassingly/">Embarrassingly Simple Binary Representation Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Embarrassingly Simple Binary Representation Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Embarrassingly Simple Binary Representation Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shen Yuming, Qin, Chen Jiaxin, Liu, Zhu</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</td>
    <td>20</td>
    <td><p>Recent binary representation learning models usually require sophisticated binary optimization, similarity measure or even generative models as auxiliaries. However, one may wonder whether these non-trivial components are needed to formulate practical and effective hashing models. In this paper, we answer the above question by proposing an embarrassingly simple approach to binary representation learning. With a simple classification objective, our model only incorporates two additional fully-connected layers onto the top of an arbitrary backbone network, whilst complying with the binary constraints during training. The proposed model lower-bounds the Information Bottleneck (IB) between data samples and their semantics, and can be related to many recent `learning to hash’ paradigms. We show that, when properly designed, even such a simple network can generate effective binary codes, by fully exploring data semantics without any held-out alternating updating steps or auxiliary models. Experiments are conducted on conventional large-scale benchmarks, i.e., CIFAR-10, NUS-WIDE, and ImageNet, where the proposed simple model outperforms the state-of-the-art methods.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Scalability 
      
        ICCV 
      
        Compact-Codes 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/do2018selective/">From Selective Deep Convolutional Features to Compact Binary Representations for Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=From Selective Deep Convolutional Features to Compact Binary Representations for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=From Selective Deep Convolutional Features to Compact Binary Representations for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Do Thanh-toan, Hoang Tuan, Tan Dang-khoa Le, Le Huu, Nguyen Tam V., Cheung Ngai-man</td> <!-- 🔧 You were missing this -->
    <td>ACM Transactions on Multimedia Computing, Communications, and Applications</td>
    <td>30</td>
    <td><p>In the large-scale image retrieval task, the two most important requirements
are the discriminability of image representations and the efficiency in
computation and storage of representations. Regarding the former requirement,
Convolutional Neural Network (CNN) is proven to be a very powerful tool to
extract highly discriminative local descriptors for effective image search.
Additionally, in order to further improve the discriminative power of the
descriptors, recent works adopt fine-tuned strategies. In this paper, taking a
different approach, we propose a novel, computationally efficient, and
competitive framework. Specifically, we firstly propose various strategies to
compute masks, namely SIFT-mask, SUM-mask, and MAX-mask, to select a
representative subset of local convolutional features and eliminate redundant
features. Our in-depth analyses demonstrate that proposed masking schemes are
effective to address the burstiness drawback and improve retrieval accuracy.
Secondly, we propose to employ recent embedding and aggregating methods which
can significantly boost the feature discriminability. Regarding the computation
and storage efficiency, we include a hashing module to produce very compact
binary image representations. Extensive experiments on six image retrieval
benchmarks demonstrate that our proposed framework achieves the
state-of-the-art retrieval performances.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Scalability 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/do2017compact/">Compact Hash Code Learning with Binary Deep Neural Network</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Compact Hash Code Learning with Binary Deep Neural Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Compact Hash Code Learning with Binary Deep Neural Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Do Thanh-toan, Hoang Tuan, Tan Dang-khoa Le, Doan Anh-dzung, Cheung Ngai-man</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>23</td>
    <td><p>Learning compact binary codes for image retrieval problem using deep neural
networks has recently attracted increasing attention. However, training deep
hashing networks is challenging due to the binary constraints on the hash
codes. In this paper, we propose deep network models and learning algorithms
for learning binary hash codes given image representations under both
unsupervised and supervised manners. The novelty of our network design is that
we constrain one hidden layer to directly output the binary codes. This design
has overcome a challenging problem in some previous works: optimizing
non-smooth objective functions because of binarization. In addition, we propose
to incorporate independence and balance properties in the direct and strict
forms into the learning schemes. We also include a similarity preserving
property in our objective functions. The resulting optimizations involving
these binary, independence, and balance constraints are difficult to solve. To
tackle this difficulty, we propose to learn the networks with alternating
optimization and careful relaxation. Furthermore, by leveraging the powerful
capacity of convolutional neural networks, we propose an end-to-end
architecture that jointly learns to extract visual features and produce binary
hash codes. Experimental results for the benchmark datasets show that the
proposed methods compare favorably or outperform the state of the art.</p>
</td>
    <td>
      
        Supervised 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Compact-Codes 
      
        Unsupervised 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/do2018binary/">Binary Constrained Deep Hashing Network for Image Retrieval without Manual Annotation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Binary Constrained Deep Hashing Network for Image Retrieval without Manual Annotation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Binary Constrained Deep Hashing Network for Image Retrieval without Manual Annotation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Do Thanh-toan, Hoang Tuan, Tan Dang-khoa Le, Pham Trung, Le Huu, Cheung Ngai-man, Reid Ian</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>9</td>
    <td><p>Learning compact binary codes for image retrieval task using deep neural
networks has attracted increasing attention recently. However, training deep
hashing networks for the task is challenging due to the binary constraints on
the hash codes, the similarity preserving property, and the requirement for a
vast amount of labelled images. To the best of our knowledge, none of the
existing methods has tackled all of these challenges completely in a unified
framework. In this work, we propose a novel end-to-end deep learning approach
for the task, in which the network is trained to produce binary codes directly
from image pixels without the need of manual annotation. In particular, to deal
with the non-smoothness of binary constraints, we propose a novel pairwise
constrained loss function, which simultaneously encodes the distances between
pairs of hash codes, and the binary quantization error. In order to train the
network with the proposed loss function, we propose an efficient parameter
learning algorithm. In addition, to provide similar / dissimilar training
images to train the network, we exploit 3D models reconstructed from unlabelled
images for automatic generation of enormous training image pairs. The extensive
experiments on image retrieval benchmark datasets demonstrate the improvements
of the proposed method over the state-of-the-art compact representation methods
on the image retrieval problem.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Quantization 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/do2019simultaneous/">Simultaneous Feature Aggregating and Hashing for Compact Binary Code Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Simultaneous Feature Aggregating and Hashing for Compact Binary Code Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Simultaneous Feature Aggregating and Hashing for Compact Binary Code Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Do Thanh-toan, Le Khoa, Hoang Tuan, Le Huu, Nguyen Tam V., Cheung Ngai-man</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>14</td>
    <td><p>Representing images by compact hash codes is an attractive approach for
large-scale content-based image retrieval. In most state-of-the-art
hashing-based image retrieval systems, for each image, local descriptors are
first aggregated as a global representation vector. This global vector is then
subjected to a hashing function to generate a binary hash code. In previous
works, the aggregating and the hashing processes are designed independently.
Hence these frameworks may generate suboptimal hash codes. In this paper, we
first propose a novel unsupervised hashing framework in which feature
aggregating and hashing are designed simultaneously and optimized jointly.
Specifically, our joint optimization generates aggregated representations that
can be better reconstructed by some binary codes. This leads to more
discriminative binary hash codes and improved retrieval accuracy. In addition,
the proposed method is flexible. It can be extended for supervised hashing.
When the data label is available, the framework can be adapted to learn binary
codes which minimize the reconstruction loss w.r.t. label vectors. Furthermore,
we also propose a fast version of the state-of-the-art hashing method Binary
Autoencoder to be used in our proposed frameworks. Extensive experiments on
benchmark datasets under various settings show that the proposed methods
outperform state-of-the-art unsupervised and supervised hashing methods.</p>
</td>
    <td>
      
        Supervised 
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Unsupervised 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/ding2019bilinear/">Bilinear Supervised Hashing Based on 2D Image Features</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Bilinear Supervised Hashing Based on 2D Image Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Bilinear Supervised Hashing Based on 2D Image Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ding Yujuan, Wong Wai Kueng, Lai Zhihui, Zhang Zheng</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Circuits and Systems for Video Technology</td>
    <td>14</td>
    <td><p>Hashing has been recognized as an efficient representation learning method to
effectively handle big data due to its low computational complexity and memory
cost. Most of the existing hashing methods focus on learning the
low-dimensional vectorized binary features based on the high-dimensional raw
vectorized features. However, studies on how to obtain preferable binary codes
from the original 2D image features for retrieval is very limited. This paper
proposes a bilinear supervised discrete hashing (BSDH) method based on 2D image
features which utilizes bilinear projections to binarize the image matrix
features such that the intrinsic characteristics in the 2D image space are
preserved in the learned binary codes. Meanwhile, the bilinear projection
approximation and vectorization binary codes regression are seamlessly
integrated together to formulate the final robust learning framework.
Furthermore, a discrete optimization strategy is developed to alternatively
update each variable for obtaining the high-quality binary codes. In addition,
two 2D image features, traditional SURF-based FVLAD feature and CNN-based
AlexConv5 feature are designed for further improving the performance of the
proposed BSDH method. Results of extensive experiments conducted on four
benchmark datasets show that the proposed BSDH method almost outperforms all
competing hashing methods with different input features by different evaluation
protocols.</p>
</td>
    <td>
      
        Supervised 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/singh2019one/">One Embedding To Do Them All</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=One Embedding To Do Them All' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=One Embedding To Do Them All' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Singh Loveperteek, Singh Shreya, Arora Sagar, Borar Sumit</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>6</td>
    <td><p>Online shopping caters to the needs of millions of users daily. Search,
recommendations, personalization have become essential building blocks for
serving customer needs. Efficacy of such systems is dependent on a thorough
understanding of products and their representation. Multiple information
sources and data types provide a complete picture of the product on the
platform. While each of these tasks shares some common characteristics,
typically product embeddings are trained and used in isolation.
  In this paper, we propose a framework to combine multiple data sources and
learn unified embeddings for products on our e-commerce platform. Our product
embeddings are built from three types of data sources - catalog text data, a
user’s clickstream session data and product images. We use various techniques
like denoising auto-encoders for text, Bayesian personalized ranking (BPR) for
clickstream data, Siamese neural network architecture for image data and
combined ensemble over the above methods for unified embeddings. Further, we
compare and analyze the performance of these embeddings across three unrelated
real-world e-commerce tasks specifically checking product attribute coverage,
finding similar products and predicting returns. We show that unified product
embeddings perform uniformly well across all these tasks.</p>
</td>
    <td>
      
        Evaluation 
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/dalins2019pdq/">PDQ & TMK + PDQF -- A Test Drive of Facebook's Perceptual Hashing Algorithms</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=PDQ & TMK + PDQF -- A Test Drive of Facebook's Perceptual Hashing Algorithms' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=PDQ & TMK + PDQF -- A Test Drive of Facebook's Perceptual Hashing Algorithms' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dalins Janis, Wilson Campbell, Boudry Douglas</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>Efficient and reliable automated detection of modified image and multimedia
files has long been a challenge for law enforcement, compounded by the harm
caused by repeated exposure to psychologically harmful materials. In August
2019 Facebook open-sourced their PDQ and TMK + PDQF algorithms for image and
video similarity measurement, respectively. In this report, we review the
algorithms’ performance on detecting commonly encountered transformations on
real-world case data, sourced from contemporary investigations. We also provide
a reference implementation to demonstrate the potential application and
integration of such algorithms within existing law enforcement systems.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Survey-Paper 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/deng2019two/">Two-Stream Deep Hashing With Class-Specific Centers for Supervised Image Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Two-Stream Deep Hashing With Class-Specific Centers for Supervised Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Two-Stream Deep Hashing With Class-Specific Centers for Supervised Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Deng Cheng, Yang, Liu, Tao</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Neural Networks and Learning Systems</td>
    <td>75</td>
    <td><p>Hashing has been widely used for large-scale approximate nearest neighbor search due to its storage and search efficiency. Recent supervised hashing research has shown that deep learning-based methods can significantly outperform nondeep methods. Most existing supervised deep hashing methods exploit supervisory signals to generate similar and dissimilar image pairs for training. However, natural images can have large intraclass and small interclass variations, which may degrade the accuracy of hash codes. To address this problem, we propose a novel two-stream ConvNet architecture, which learns hash codes with class-specific representation centers. Our basic idea is that if we can learn a unified binary representation for each class as a center and encourage hash codes of images to be close to the corresponding centers, the intraclass variation will be greatly reduced. Accordingly, we design a neural network that leverages label information and outputs a unified binary representation for each class. Moreover, we also design an image network to learn hash codes from images and force these hash codes to be close to the corresponding class-specific centers. These two neural networks are then seamlessly incorporated to create a unified, end-to-end trainable framework. Extensive experiments on three popular benchmarks corroborate that our proposed method outperforms current state-of-the-art methods.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Efficiency 
      
        Neural-Hashing 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/wiggers2019image/">Image Retrieval and Pattern Spotting using Siamese Neural Network</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Image Retrieval and Pattern Spotting using Siamese Neural Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Image Retrieval and Pattern Spotting using Siamese Neural Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wiggers Kelly L., Britto Alceu S. Jr., Heutte Laurent, Koerich Alessandro L., Oliveira Luiz S.</td> <!-- 🔧 You were missing this -->
    <td>2019 International Joint Conference on Neural Networks (IJCNN)</td>
    <td>37</td>
    <td><p>This paper presents a novel approach for image retrieval and pattern spotting
in document image collections. The manual feature engineering is avoided by
learning a similarity-based representation using a Siamese Neural Network
trained on a previously prepared subset of image pairs from the ImageNet
dataset. The learned representation is used to provide the similarity-based
feature maps used to find relevant image candidates in the data collection
given an image query. A robust experimental protocol based on the public
Tobacco800 document image collection shows that the proposed method compares
favorably against state-of-the-art document image retrieval methods, reaching
0.94 and 0.83 of mean average precision (mAP) for retrieval and pattern
spotting (IoU=0.7), respectively. Besides, we have evaluated the proposed
method considering feature maps of different sizes, showing the impact of
reducing the number of features in the retrieval performance and
time-consuming.</p>
</td>
    <td>
      
        Datasets 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/deng2025two/">Two-Stream Deep Hashing With Class-Specific Centers for Supervised Image Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Two-Stream Deep Hashing With Class-Specific Centers for Supervised Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Two-Stream Deep Hashing With Class-Specific Centers for Supervised Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Deng Cheng, Yang, Liu, Tao</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Neural Networks and Learning Systems</td>
    <td>75</td>
    <td><p>Hashing has been widely used for large-scale approximate nearest neighbor search due to its storage and search efficiency. Recent supervised hashing research has shown that deep learning-based methods can significantly outperform nondeep methods. Most existing supervised deep hashing methods exploit supervisory signals to generate similar and dissimilar image pairs for training. However, natural images can have large intraclass and small interclass variations, which may degrade the accuracy of hash codes. To address this problem, we propose a novel two-stream ConvNet architecture, which learns hash codes with class-specific representation centers. Our basic idea is that if we can learn a unified binary representation for each class as a center and encourage hash codes of images to be close to the corresponding centers, the intraclass variation will be greatly reduced. Accordingly, we design a neural network that leverages label information and outputs a unified binary representation for each class. Moreover, we also design an image network to learn hash codes from images and force these hash codes to be close to the corresponding class-specific centers. These two neural networks are then seamlessly incorporated to create a unified, end-to-end trainable framework. Extensive experiments on three popular benchmarks corroborate that our proposed method outperforms current state-of-the-art methods.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Efficiency 
      
        Neural-Hashing 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/wray2019fine/">Fine-Grained Action Retrieval Through Multiple Parts-of-Speech Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fine-Grained Action Retrieval Through Multiple Parts-of-Speech Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fine-Grained Action Retrieval Through Multiple Parts-of-Speech Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wray Michael, Larlus Diane, Csurka Gabriela, Damen Dima</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>130</td>
    <td><p>We address the problem of cross-modal fine-grained action retrieval between
text and video. Cross-modal retrieval is commonly achieved through learning a
shared embedding space, that can indifferently embed modalities. In this paper,
we propose to enrich the embedding by disentangling parts-of-speech (PoS) in
the accompanying captions. We build a separate multi-modal embedding space for
each PoS tag. The outputs of multiple PoS embeddings are then used as input to
an integrated multi-modal space, where we perform action retrieval. All
embeddings are trained jointly through a combination of PoS-aware and
PoS-agnostic losses. Our proposal enables learning specialised embedding spaces
that offer multiple views of the same embedded entities.
  We report the first retrieval results on fine-grained actions for the
large-scale EPIC dataset, in a generalised zero-shot setting. Results show the
advantage of our approach for both video-to-text and text-to-video action
retrieval. We also demonstrate the benefit of disentangling the PoS for the
generic task of cross-modal video retrieval on the MSR-VTT dataset.</p>
</td>
    <td>
      
        ICCV 
      
        Few-Shot-&-Zero-Shot 
      
        Datasets 
      
        Video-Retrieval 
      
        Scalability 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/christiani2017fast/">Fast Locality-Sensitive Hashing Frameworks for Approximate Near Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Locality-Sensitive Hashing Frameworks for Approximate Near Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Locality-Sensitive Hashing Frameworks for Approximate Near Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Christiani Tobias</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>14</td>
    <td><p>The Indyk-Motwani Locality-Sensitive Hashing (LSH) framework (STOC 1998) is a
general technique for constructing a data structure to answer approximate near
neighbor queries by using a distribution \(\mathcal{H}\) over locality-sensitive
hash functions that partition space. For a collection of \(n\) points, after
preprocessing, the query time is dominated by \(O(n^{\rho} log n)\) evaluations
of hash functions from \(\mathcal{H}\) and \(O(n^{\rho})\) hash table lookups and
distance computations where \(\rho \in (0,1)\) is determined by the
locality-sensitivity properties of \(\mathcal{H}\). It follows from a recent
result by Dahlgaard et al. (FOCS 2017) that the number of locality-sensitive
hash functions can be reduced to \(O(log^2 n)\), leaving the query time to be
dominated by \(O(n^{\rho})\) distance computations and \(O(n^{\rho} log n)\)
additional word-RAM operations. We state this result as a general framework and
provide a simpler analysis showing that the number of lookups and distance
computations closely match the Indyk-Motwani framework, making it a viable
replacement in practice. Using ideas from another locality-sensitive hashing
framework by Andoni and Indyk (SODA 2006) we are able to reduce the number of
additional word-RAM operations to \(O(n^\rho)\).</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Locality-Sensitive-Hashing 
      
        Tools-&-Libraries 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/huang2025accelerate/">Accelerate Learning of Deep Hashing With Gradient Attention</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Accelerate Learning of Deep Hashing With Gradient Attention' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Accelerate Learning of Deep Hashing With Gradient Attention' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Huang Long-kai, Chen, Pan</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>23</td>
    <td><p>Recent years have witnessed the success of learning to hash in fast large-scale image retrieval. As deep learning has shown its superior performance on many computer vision applications, recent designs of learning-based hashing models have been moving from shallow ones to deep architectures. However, based on our analysis, we find that gradient descent based algorithms used in deep hashing models would potentially cause hash codes of a pair of training instances to be updated towards the directions of each other simultaneously during optimization. In the worst case, the paired hash codes switch their directions after update, and consequently, their corresponding distance in the Hamming space remain unchanged. This makes the overall learning process highly inefficient. To address this issue, we propose a new deep hashing model integrated with a novel gradient attention mechanism. Extensive experimental results on three benchmark datasets show that our proposed algorithm is able to accelerate the learning process and obtain competitive retrieval performance compared with state-of-the-art deep hashing models.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        ICCV 
      
        Datasets 
      
        Neural-Hashing 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/shen2025embarrassingly/">Embarrassingly Simple Binary Representation Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Embarrassingly Simple Binary Representation Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Embarrassingly Simple Binary Representation Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shen Yuming, Qin, Chen Jiaxin, Liu, Zhu</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</td>
    <td>20</td>
    <td><p>Recent binary representation learning models usually require sophisticated binary optimization, similarity measure or even generative models as auxiliaries. However, one may wonder whether these non-trivial components are needed to formulate practical and effective hashing models. In this paper, we answer the above question by proposing an embarrassingly simple approach to binary representation learning. With a simple classification objective, our model only incorporates two additional fully-connected layers onto the top of an arbitrary backbone network, whilst complying with the binary constraints during training. The proposed model lower-bounds the Information Bottleneck (IB) between data samples and their semantics, and can be related to many recent `learning to hash’ paradigms. We show that, when properly designed, even such a simple network can generate effective binary codes, by fully exploring data semantics without any held-out alternating updating steps or auxiliary models. Experiments are conducted on conventional large-scale benchmarks, i.e., CIFAR-10, NUS-WIDE, and ImageNet, where the proposed simple model outperforms the state-of-the-art methods.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Scalability 
      
        ICCV 
      
        Compact-Codes 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/chen2025two/">A Two-step Cross-modal Hashing by Exploiting Label Correlations and Preserving Similarity in Both Steps</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Two-step Cross-modal Hashing by Exploiting Label Correlations and Preserving Similarity in Both Steps' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Two-step Cross-modal Hashing by Exploiting Label Correlations and Preserving Similarity in Both Steps' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen Zhen-duo, Wang, Li, Luo, Nie, Xin-shun</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 27th ACM International Conference on Multimedia</td>
    <td>45</td>
    <td><p>In this paper, we present a novel Two-stEp Cross-modal Hashing method, TECH for short, for cross-modal retrieval tasks. As a two-step method, it first learns hash codes based on semantic labels, while preserving the similarity in the original space and exploiting the label correlations in the label space. In the light of this, it is able to make better use of label information and generate better binary codes. In addition, different from other two-step methods that mainly focus on the hash codes learning, TECH adopts a new hash function learning strategy in the second step, which also preserves the similarity in the original space. Moreover, with the help of well designed objective function and optimization scheme, it is able to generate hash codes discretely and scalable for large scale data. To the best of our knowledge, it is the first cross-modal hashing method exploiting label correlations, and also the first two-step hashing model preserving the similarity while leaning hash function. Extensive experiments demonstrate that the proposed approach outperforms some state-of-the-art cross-modal hashing methods.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Multimodal-Retrieval 
      
        Compact-Codes 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/stylianou2019visualizing/">Visualizing Deep Similarity Networks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Visualizing Deep Similarity Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Visualizing Deep Similarity Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Stylianou Abby, Souvenir Richard, Pless Robert</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>46</td>
    <td><p>For convolutional neural network models that optimize an image embedding, we
propose a method to highlight the regions of images that contribute most to
pairwise similarity. This work is a corollary to the visualization tools
developed for classification networks, but applicable to the problem domains
better suited to similarity learning. The visualization shows how similarity
networks that are fine-tuned learn to focus on different features. We also
generalize our approach to embedding networks that use different pooling
strategies and provide a simple mechanism to support image similarity searches
on objects or sub-regions in the query image.</p>
</td>
    <td>
      
        Similarity-Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/yang2019feature/">Feature Pyramid Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Feature Pyramid Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Feature Pyramid Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yang Yifan, Geng Libing, Lai Hanjiang, Pan Yan, Yin Jian</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2019 on International Conference on Multimedia Retrieval</td>
    <td>22</td>
    <td><p>In recent years, deep-networks-based hashing has become a leading approach
for large-scale image retrieval. Most deep hashing approaches use the high
layer to extract the powerful semantic representations. However, these methods
have limited ability for fine-grained image retrieval because the semantic
features extracted from the high layer are difficult in capturing the subtle
differences. To this end, we propose a novel two-pyramid hashing architecture
to learn both the semantic information and the subtle appearance details for
fine-grained image search. Inspired by the feature pyramids of convolutional
neural network, a vertical pyramid is proposed to capture the high-layer
features and a horizontal pyramid combines multiple low-layer features with
structural information to capture the subtle differences. To fuse the low-level
features, a novel combination strategy, called consensus fusion, is proposed to
capture all subtle information from several low-layers for finer retrieval.
Extensive evaluation on two fine-grained datasets CUB-200-2011 and Stanford
Dogs demonstrate that the proposed method achieves significant performance
compared with the state-of-art baselines.</p>
</td>
    <td>
      
        Scalability 
      
        Evaluation 
      
        Datasets 
      
        Hashing-Methods 
      
        Multimodal-Retrieval 
      
        Neural-Hashing 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/chiu2018learning/">Learning to Index for Nearest Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning to Index for Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning to Index for Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chiu Chih-yi, Prayoonwong Amorntip, Liao Yin-chih</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>24</td>
    <td><p>In this study, we present a novel ranking model based on learning
neighborhood relationships embedded in the index space. Given a query point,
conventional approximate nearest neighbor search calculates the distances to
the cluster centroids, before ranking the clusters from near to far based on
the distances. The data indexed in the top-ranked clusters are retrieved and
treated as the nearest neighbor candidates for the query. However, the loss of
quantization between the data and cluster centroids will inevitably harm the
search accuracy. To address this problem, the proposed model ranks clusters
based on their nearest neighbor probabilities rather than the query-centroid
distances. The nearest neighbor probabilities are estimated by employing neural
networks to characterize the neighborhood relationships, i.e., the density
function of nearest neighbors with respect to the query. The proposed
probability-based ranking can replace the conventional distance-based ranking
for finding candidate clusters, and the predicted probability can be used to
determine the data quantity to be retrieved from the candidate cluster. Our
experimental results demonstrated that the proposed ranking model could boost
the search performance effectively in billion-scale datasets.</p>
</td>
    <td>
      
        Datasets 
      
        Scalability 
      
        Quantization 
      
        Large-Scale-Search 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/chen2020making/">Making Online Sketching Hashing Even Faster</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Making Online Sketching Hashing Even Faster' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Making Online Sketching Hashing Even Faster' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen Xixian, Yang Haiqin, Zhao Shenglin, Lyu Michael R., King Irwin</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Knowledge and Data Engineering</td>
    <td>21</td>
    <td><p>Data-dependent hashing methods have demonstrated good performance in various
machine learning applications to learn a low-dimensional representation from
the original data. However, they still suffer from several obstacles: First,
most of existing hashing methods are trained in a batch mode, yielding
inefficiency for training streaming data. Second, the computational cost and
the memory consumption increase extraordinarily in the big data setting, which
perplexes the training procedure. Third, the lack of labeled data hinders the
improvement of the model performance. To address these difficulties, we utilize
online sketching hashing (OSH) and present a FasteR Online Sketching Hashing
(FROSH) algorithm to sketch the data in a more compact form via an independent
transformation. We provide theoretical justification to guarantee that our
proposed FROSH consumes less time and achieves a comparable sketching precision
under the same memory cost of OSH. We also extend FROSH to its distributed
implementation, namely DFROSH, to further reduce the training time cost of
FROSH while deriving the theoretical bound of the sketching precision. Finally,
we conduct extensive experiments on both synthetic and real datasets to
demonstrate the attractive merits of FROSH and DFROSH.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/song2019deep/">Deep Hashing Learning for Visual and Semantic Retrieval of Remote Sensing Images</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Hashing Learning for Visual and Semantic Retrieval of Remote Sensing Images' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Hashing Learning for Visual and Semantic Retrieval of Remote Sensing Images' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Song Weiwei, Li Shutao, Benediktsson Jon Atli</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</td>
    <td>17</td>
    <td><p>Driven by the urgent demand for managing remote sensing big data, large-scale
remote sensing image retrieval (RSIR) attracts increasing attention in the
remote sensing field. In general, existing retrieval methods can be regarded as
visual-based retrieval approaches which search and return a set of similar
images from a database to a given query image. Although retrieval methods have
achieved great success, there is still a question that needs to be responded
to: Can we obtain the accurate semantic labels of the returned similar images
to further help analyzing and processing imagery? Inspired by the above
question, in this paper, we redefine the image retrieval problem as visual and
semantic retrieval of images. Specifically, we propose a novel deep hashing
convolutional neural network (DHCNN) to simultaneously retrieve the similar
images and classify their semantic labels in a unified framework. In more
detail, a convolutional neural network (CNN) is used to extract
high-dimensional deep features. Then, a hash layer is perfectly inserted into
the network to transfer the deep features into compact hash codes. In addition,
a fully connected layer with a softmax function is performed on hash layer to
generate class distribution. Finally, a loss function is elaborately designed
to simultaneously consider the label loss of each image and similarity loss of
pairs of images. Experimental results on two remote sensing datasets
demonstrate that the proposed method achieves the state-of-art retrieval and
classification performance.</p>
</td>
    <td>
      
        Scalability 
      
        Evaluation 
      
        Datasets 
      
        AAAI 
      
        Hashing-Methods 
      
        Tools-&-Libraries 
      
        Neural-Hashing 
      
        IJCAI 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/chen2019hybrid/">Hybrid-Attention based Decoupled Metric Learning for Zero-Shot Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hybrid-Attention based Decoupled Metric Learning for Zero-Shot Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hybrid-Attention based Decoupled Metric Learning for Zero-Shot Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen Binghui, Deng Weihong</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>66</td>
    <td><p>In zero-shot image retrieval (ZSIR) task, embedding learning becomes more
attractive, however, many methods follow the traditional metric learning idea
and omit the problems behind zero-shot settings. In this paper, we first
emphasize the importance of learning visual discriminative metric and
preventing the partial/selective learning behavior of learner in ZSIR, and then
propose the Decoupled Metric Learning (DeML) framework to achieve these
individually. Instead of coarsely optimizing an unified metric, we decouple it
into multiple attention-specific parts so as to recurrently induce the
discrimination and explicitly enhance the generalization. And they are mainly
achieved by our object-attention module based on random walk graph propagation
and the channel-attention module based on the adversary constraint,
respectively. We demonstrate the necessity of addressing the vital problems in
ZSIR on the popular benchmarks, outperforming the state-of-theart methods by a
significant margin. Code is available at http://www.bhchen.cn</p>
</td>
    <td>
      
        Few-Shot-&-Zero-Shot 
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
        Distance-Metric-Learning 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/chen2019two/">A Two-step Cross-modal Hashing by Exploiting Label Correlations and Preserving Similarity in Both Steps</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Two-step Cross-modal Hashing by Exploiting Label Correlations and Preserving Similarity in Both Steps' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Two-step Cross-modal Hashing by Exploiting Label Correlations and Preserving Similarity in Both Steps' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen Zhen-duo, Wang, Li, Luo, Nie, Xin-shun</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 27th ACM International Conference on Multimedia</td>
    <td>45</td>
    <td><p>In this paper, we present a novel Two-stEp Cross-modal Hashing method, TECH for short, for cross-modal retrieval tasks. As a two-step method, it first learns hash codes based on semantic labels, while preserving the similarity in the original space and exploiting the label correlations in the label space. In the light of this, it is able to make better use of label information and generate better binary codes. In addition, different from other two-step methods that mainly focus on the hash codes learning, TECH adopts a new hash function learning strategy in the second step, which also preserves the similarity in the original space. Moreover, with the help of well designed objective function and optimization scheme, it is able to generate hash codes discretely and scalable for large scale data. To the best of our knowledge, it is the first cross-modal hashing method exploiting label correlations, and also the first two-step hashing model preserving the similarity while leaning hash function. Extensive experiments demonstrate that the proposed approach outperforms some state-of-the-art cross-modal hashing methods.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Multimodal-Retrieval 
      
        Compact-Codes 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/chen2019vector/">Vector and Line Quantization for Billion-scale Similarity Search on GPUs</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Vector and Line Quantization for Billion-scale Similarity Search on GPUs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Vector and Line Quantization for Billion-scale Similarity Search on GPUs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen Wei, Chen Jincai, Zou Fuhao, Li Yuan-fang, Lu Ping, Wang Qiang, Zhao Wei</td> <!-- 🔧 You were missing this -->
    <td>Future Generation Computer Systems</td>
    <td>7</td>
    <td><p>Billion-scale high-dimensional approximate nearest neighbour (ANN) search has
become an important problem for searching similar objects among the vast amount
of images and videos available online. The existing ANN methods are usually
characterized by their specific indexing structures, including the inverted
index and the inverted multi-index structure. The inverted index structure is
amenable to GPU-based implementations, and the state-of-the-art systems such as
Faiss are able to exploit the massive parallelism offered by GPUs. However, the
inverted index requires high memory overhead to index the dataset effectively.
The inverted multi-index structure is difficult to implement for GPUs, and also
ineffective in dealing with database with different data distributions. In this
paper we propose a novel hierarchical inverted index structure generated by
vector and line quantization methods. Our quantization method improves both
search efficiency and accuracy, while maintaining comparable memory
consumption. This is achieved by reducing search space and increasing the
number of indexed regions. We introduce a new ANN search system, VLQ-ADC, that
is based on the proposed inverted index, and perform extensive evaluation on
two public billion-scale benchmark datasets SIFT1B and DEEP1B. Our evaluation
shows that VLQ-ADC significantly outperforms the state-of-the-art GPU- and
CPU-based systems in terms of both accuracy and search speed. The source code
of VLQ-ADC is available at
https://github.com/zjuchenwei/vector-line-quantization.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Vector-Indexing 
      
        Tools-&-Libraries 
      
        Datasets 
      
        Quantization 
      
        Scalability 
      
        Large-Scale-Search 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/chen2018improving/">Improving Deep Binary Embedding Networks by Order-aware Reweighting of Triplets</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Improving Deep Binary Embedding Networks by Order-aware Reweighting of Triplets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Improving Deep Binary Embedding Networks by Order-aware Reweighting of Triplets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen Jikai, Lai Hanjiang, Geng Libing, Pan Yan</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Circuits and Systems for Video Technology</td>
    <td>12</td>
    <td><p>In this paper, we focus on triplet-based deep binary embedding networks for
image retrieval task. The triplet loss has been shown to be most effective for
the ranking problem. However, most of the previous works treat the triplets
equally or select the hard triplets based on the loss. Such strategies do not
consider the order relations, which is important for retrieval task. To this
end, we propose an order-aware reweighting method to effectively train the
triplet-based deep networks, which up-weights the important triplets and
down-weights the uninformative triplets. First, we present the order-aware
weighting factors to indicate the importance of the triplets, which depend on
the rank order of binary codes. Then, we reshape the triplet loss to the
squared triplet loss such that the loss function will put more weights on the
important triplets. Extensive evaluations on four benchmark datasets show that
the proposed method achieves significant performance compared with the
state-of-the-art baselines.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Compact-Codes 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/chen2019differentiable/">Differentiable Product Quantization for End-to-End Embedding Compression</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Differentiable Product Quantization for End-to-End Embedding Compression' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Differentiable Product Quantization for End-to-End Embedding Compression' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen Ting, Li Lala, Sun Yizhou</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>13</td>
    <td><p>Embedding layers are commonly used to map discrete symbols into continuous
embedding vectors that reflect their semantic meanings. Despite their
effectiveness, the number of parameters in an embedding layer increases
linearly with the number of symbols and poses a critical challenge on memory
and storage constraints. In this work, we propose a generic and end-to-end
learnable compression framework termed differentiable product quantization
(DPQ). We present two instantiations of DPQ that leverage different
approximation techniques to enable differentiability in end-to-end learning.
Our method can readily serve as a drop-in alternative for any existing
embedding layer. Empirically, DPQ offers significant compression ratios
(14-238\(\times\)) at negligible or no performance cost on 10 datasets across
three different language tasks.</p>
</td>
    <td>
      
        Datasets 
      
        Quantization 
      
        Evaluation 
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/studer2019comprehensive/">A Comprehensive Study of ImageNet Pre-Training for Historical Document Image Analysis</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Comprehensive Study of ImageNet Pre-Training for Historical Document Image Analysis' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Comprehensive Study of ImageNet Pre-Training for Historical Document Image Analysis' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Studer Linda, Alberti Michele, Pondenkandath Vinaychandran, Goktepe Pinar, Kolonko Thomas, Fischer Andreas, Liwicki Marcus, Ingold Rolf</td> <!-- 🔧 You were missing this -->
    <td>2019 International Conference on Document Analysis and Recognition (ICDAR)</td>
    <td>66</td>
    <td><p>Automatic analysis of scanned historical documents comprises a wide range of
image analysis tasks, which are often challenging for machine learning due to a
lack of human-annotated learning samples. With the advent of deep neural
networks, a promising way to cope with the lack of training data is to
pre-train models on images from a different domain and then fine-tune them on
historical documents. In the current research, a typical example of such
cross-domain transfer learning is the use of neural networks that have been
pre-trained on the ImageNet database for object recognition. It remains a
mostly open question whether or not this pre-training helps to analyse
historical documents, which have fundamentally different image properties when
compared with ImageNet. In this paper, we present a comprehensive empirical
survey on the effect of ImageNet pre-training for diverse historical document
analysis tasks, including character recognition, style classification,
manuscript dating, semantic segmentation, and content-based retrieval. While we
obtain mixed results for semantic segmentation at pixel-level, we observe a
clear trend across different network architectures that ImageNet pre-training
has a positive effect on classification as well as content-based retrieval.</p>
</td>
    <td>
      
        Survey-Paper 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/yan2025deep/">Deep Hashing by Discriminating Hard Examples</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Hashing by Discriminating Hard Examples' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Hashing by Discriminating Hard Examples' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yan Cheng, Pang, Bai, Shen, Zhou, Hancock</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 27th ACM International Conference on Multimedia</td>
    <td>27</td>
    <td><p>This paper tackles a rarely explored but critical problem within learning to hash, i.e., to learn hash codes that effectively discriminate hard similar and dissimilar examples, to empower large-scale image retrieval. Hard similar examples refer to image pairs from the same semantic class that demonstrate some shared appearance but have different fine-grained appearance. Hard dissimilar examples are image pairs that come from different semantic classes but exhibit similar appearance. These hard examples generally have a small distance due to the shared appearance. Therefore, effective encoding of the hard examples can well discriminate the relevant images within a small Hamming distance, enabling more accurate retrieval in the top-ranked returned images. However, most existing hashing methods cannot capture this key information as their optimization is dominated byeasy examples, i.e., distant similar/dissimilar pairs that share no or limited appearance. To address this problem, we introduce a novel Gamma distribution-enabled and symmetric Kullback-Leibler divergence-based loss, which is dubbed dual hinge loss because it works similarly as imposing two smoothed hinge losses on the respective similar and dissimilar pairs. Specifically, the loss enforces exponentially variant penalization on the hard similar (dissimilar) examples to emphasize and learn their fine-grained difference. It meanwhile imposes a bounding penalization on easy similar (dissimilar) examples to prevent the dominance of the easy examples in the optimization while preserving the high-level similarity (dissimilarity). This enables our model to well encode the key information carried by both easy and hard examples. Extensive empirical results on three widely-used image retrieval datasets show that (i) our method consistently and substantially outperforms state-of-the-art competing methods using hash codes of the same length and (ii) our method can use significantly (e.g., 50%-75%) shorter hash codes to perform substantially better than, or comparably well to, the competing methods.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Datasets 
      
        Neural-Hashing 
      
        Hashing-Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/song2019polysemous/">Polysemous Visual-Semantic Embedding for Cross-Modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Polysemous Visual-Semantic Embedding for Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Polysemous Visual-Semantic Embedding for Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Song Yale, Soleymani Mohammad</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>223</td>
    <td><p>Visual-semantic embedding aims to find a shared latent space where related
visual and textual instances are close to each other. Most current methods
learn injective embedding functions that map an instance to a single point in
the shared space. Unfortunately, injective embedding cannot effectively handle
polysemous instances with multiple possible meanings; at best, it would find an
average representation of different meanings. This hinders its use in
real-world scenarios where individual instances and their cross-modal
associations are often ambiguous. In this work, we introduce Polysemous
Instance Embedding Networks (PIE-Nets) that compute multiple and diverse
representations of an instance by combining global context with locally-guided
features via multi-head self-attention and residual learning. To learn
visual-semantic embedding, we tie-up two PIE-Nets and optimize them jointly in
the multiple instance learning framework. Most existing work on cross-modal
retrieval focuses on image-text data. Here, we also tackle a more challenging
case of video-text retrieval. To facilitate further research in video-text
retrieval, we release a new dataset of 50K video-sentence pairs collected from
social media, dubbed MRW (my reaction when). We demonstrate our approach on
both image-text and video-text retrieval scenarios using MS-COCO, TGIF, and our
new MRW dataset.</p>
</td>
    <td>
      
        Text-Retrieval 
      
        Tools-&-Libraries 
      
        Datasets 
      
        CVPR 
      
        Evaluation 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/cao2018end/">End-to-End Latent Fingerprint Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=End-to-End Latent Fingerprint Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=End-to-End Latent Fingerprint Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cao Kai, Nguyen Dinh-luan, Tymoszek Cori, Jain A. K.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Information Forensics and Security</td>
    <td>65</td>
    <td><p>Latent fingerprints are one of the most important and widely used sources of
evidence in law enforcement and forensic agencies. Yet the performance of the
state-of-the-art latent recognition systems is far from satisfactory, and they
often require manual markups to boost the latent search performance. Further,
the COTS systems are proprietary and do not output the true comparison scores
between a latent and reference prints to conduct quantitative evidential
analysis. We present an end-to-end latent fingerprint search system, including
automated region of interest (ROI) cropping, latent image preprocessing,
feature extraction, feature comparison , and outputs a candidate list. Two
separate minutiae extraction models provide complementary minutiae templates.
To compensate for the small number of minutiae in small area and poor quality
latents, a virtual minutiae set is generated to construct a texture template. A
96-dimensional descriptor is extracted for each minutia from its neighborhood.
For computational efficiency, the descriptor length for virtual minutiae is
further reduced to 16 using product quantization. Our end-to-end system is
evaluated on three latent databases: NIST SD27 (258 latents); MSP (1,200
latents), WVU (449 latents) and N2N (10,000 latents) against a background set
of 100K rolled prints, which includes the true rolled mates of the latents with
rank-1 retrieval rates of 65.7%, 69.4%, 65.5%, and 7.6% respectively. A
multi-core solution implemented on 24 cores obtains 1ms per latent to rolled
comparison.</p>
</td>
    <td>
      
        Quantization 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/camara2019spatio/">Spatio-Semantic ConvNet-Based Visual Place Recognition</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Spatio-Semantic ConvNet-Based Visual Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Spatio-Semantic ConvNet-Based Visual Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Camara Luis G., Přeučil Libor</td> <!-- 🔧 You were missing this -->
    <td>2019 European Conference on Mobile Robots (ECMR)</td>
    <td>25</td>
    <td><p>We present a Visual Place Recognition system that follows the two-stage
format common to image retrieval pipelines. The system encodes images of places
by employing the activations of different layers of a pre-trained,
off-the-shelf, VGG16 Convolutional Neural Network (CNN) architecture. In the
first stage of our method and given a query image of a place, a number of top
candidate images is retrieved from a previously stored database of places. In
the second stage, we propose an exhaustive comparison of the query image
against these candidates by encoding semantic and spatial information in the
form of CNN features. Results from our approach outperform by a large margin
state-of-the-art visual place recognition methods on five of the most commonly
used benchmark datasets. The performance gain is especially remarkable on the
most challenging datasets, with more than a twofold recognition improvement
with respect to the latest published work.</p>
</td>
    <td>
      
        Datasets 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/cao2019enhancing/">Enhancing Remote Sensing Image Retrieval with Triplet Deep Metric Learning Network</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Enhancing Remote Sensing Image Retrieval with Triplet Deep Metric Learning Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Enhancing Remote Sensing Image Retrieval with Triplet Deep Metric Learning Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cao Rui, Zhang Qian, Zhu Jiasong, Li Qing, Li Qingquan, Liu Bozhi, Qiu Guoping</td> <!-- 🔧 You were missing this -->
    <td>International Journal of Remote Sensing</td>
    <td>94</td>
    <td><p>With the rapid growing of remotely sensed imagery data, there is a high
demand for effective and efficient image retrieval tools to manage and exploit
such data. In this letter, we present a novel content-based remote sensing
image retrieval method based on Triplet deep metric learning convolutional
neural network (CNN). By constructing a Triplet network with metric learning
objective function, we extract the representative features of the images in a
semantic space in which images from the same class are close to each other
while those from different classes are far apart. In such a semantic space,
simple metric measures such as Euclidean distance can be used directly to
compare the similarity of images and effectively retrieve images of the same
class. We also investigate a supervised and an unsupervised learning methods
for reducing the dimensionality of the learned semantic features. We present
comprehensive experimental results on two publicly available remote sensing
image retrieval datasets and show that our method significantly outperforms
state-of-the-art.</p>
</td>
    <td>
      
        Supervised 
      
        Image-Retrieval 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/cakir2018hashing/">Hashing with Mutual Information</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing with Mutual Information' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing with Mutual Information' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cakir Fatih, He Kun, Bargal Sarah Adel, Sclaroff Stan</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>91</td>
    <td><p>Binary vector embeddings enable fast nearest neighbor retrieval in large
databases of high-dimensional objects, and play an important role in many
practical applications, such as image and video retrieval. We study the problem
of learning binary vector embeddings under a supervised setting, also known as
hashing. We propose a novel supervised hashing method based on optimizing an
information-theoretic quantity: mutual information. We show that optimizing
mutual information can reduce ambiguity in the induced neighborhood structure
in the learned Hamming space, which is essential in obtaining high retrieval
performance. To this end, we optimize mutual information in deep neural
networks with minibatch stochastic gradient descent, with a formulation that
maximally and efficiently utilizes available supervision. Experiments on four
image retrieval benchmarks, including ImageNet, confirm the effectiveness of
our method in learning high-quality binary embeddings for nearest neighbor
retrieval.</p>
</td>
    <td>
      
        Supervised 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Neural-Hashing 
      
        Video-Retrieval 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/cakir2019hashing/">Hashing with Binary Matrix Pursuit</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing with Binary Matrix Pursuit' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing with Binary Matrix Pursuit' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cakir F., He, Sclaroff</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>91</td>
    <td><p>We propose theoretical and empirical improvements for two-stage hashing methods. We first provide a theoretical analysis on the quality of the binary codes and show that, under mild assumptions, a residual learning scheme can construct binary codes that fit any neighborhood structure with arbitrary accuracy. Secondly, we show that with high-capacity hash functions such as CNNs, binary code inference can be greatly simplified for many standard neighborhood definitions, yielding smaller optimization problems and more robust codes. Incorporating our findings, we propose a novel two-stage hashing method that significantly outperforms previous hashing studies on widely used image retrieval benchmarks.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Image-Retrieval 
      
        Compact-Codes 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/cakir2025hashing/">Hashing with Mutual Information</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing with Mutual Information' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing with Mutual Information' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cakir F., He, Bargal, Sclaroff</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>91</td>
    <td><p>Binary vector embeddings enable fast nearest neighbor retrieval in large databases of high-dimensional objects, and play an important role in many practical applications, such as image and video retrieval. We study the problem of learning binary vector embeddings under a supervised setting, also known as hashing. We propose a novel supervised hashing method based on optimizing an information-theoretic quantity: mutual information. We show that optimizing mutual information can reduce ambiguity in the induced neighborhood structure in the learned Hamming space, which is essential in obtaining high retrieval performance. To this end, we optimize mutual information in deep neural networks with minibatch stochastic gradient descent, with a formulation that maximally and efficiently utilizes available supervision. Experiments on four image retrieval benchmarks, including ImageNet, confirm the effectiveness of our method in learning high-quality binary embeddings for nearest neighbor retrieval.</p>
</td>
    <td>
      
        Video-Retrieval 
      
        Image-Retrieval 
      
        Neural-Hashing 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/cai2016revisit/">A Revisit of Hashing Algorithms for Approximate Nearest Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Revisit of Hashing Algorithms for Approximate Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Revisit of Hashing Algorithms for Approximate Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cai Deng</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Knowledge and Data Engineering</td>
    <td>30</td>
    <td><p>Approximate Nearest Neighbor Search (ANNS) is a fundamental problem in many
areas of machine learning and data mining. During the past decade, numerous
hashing algorithms are proposed to solve this problem. Every proposed algorithm
claims outperform other state-of-the-art hashing methods. However, the
evaluation of these hashing papers was not thorough enough, and those claims
should be re-examined. The ultimate goal of an ANNS method is returning the
most accurate answers (nearest neighbors) in the shortest time. If implemented
correctly, almost all the hashing methods will have their performance improved
as the code length increases. However, many existing hashing papers only report
the performance with the code length shorter than 128. In this paper, we
carefully revisit the problem of search with a hash index, and analyze the pros
and cons of two popular hash index search procedures. Then we proposed a very
simple but effective two level index structures and make a thorough comparison
of eleven popular hashing algorithms. Surprisingly, the random-projection-based
Locality Sensitive Hashing (LSH) is the best performed algorithm, which is in
contradiction to the claims in all the other ten hashing papers. Despite the
extreme simplicity of random-projection-based LSH, our results show that the
capability of this algorithm has been far underestimated. For the sake of
reproducibility, all the codes used in the paper are released on GitHub, which
can be used as a testing platform for a fair comparison between various hashing
algorithms.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Vector-Indexing 
      
        Evaluation 
      
        Locality-Sensitive-Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/cao2019unsupervised/">Unsupervised Deep Metric Learning via Auxiliary Rotation Loss</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Deep Metric Learning via Auxiliary Rotation Loss' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Deep Metric Learning via Auxiliary Rotation Loss' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cao Xuefei, Chen Bor-chun, Lim Ser-nam</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>12</td>
    <td><p>Deep metric learning is an important area due to its applicability to many
domains such as image retrieval and person re-identification. The main drawback
of such models is the necessity for labeled data. In this work, we propose to
generate pseudo-labels for deep metric learning directly from clustering
assignment and we introduce unsupervised deep metric learning (UDML)
regularized by a self-supervision (SS) task. In particular, we propose to
regularize the training process by predicting image rotations. Our method
(UDML-SS) jointly learns discriminative embeddings, unsupervised clustering
assignments of the embeddings, as well as a self-supervised pretext task.
UDML-SS iteratively cluster embeddings using traditional clustering algorithm
(e.g., k-means), and sampling training pairs based on the cluster assignment
for metric learning, while optimizing self-supervised pretext task in a
multi-task fashion. The role of self-supervision is to stabilize the training
process and encourages the model to learn meaningful feature representations
that are not distorted due to unreliable clustering assignments. The proposed
method performs well on standard benchmarks for metric learning, where it
outperforms current state-of-the-art approaches by a large margin and it also
shows competitive performance with various metric learning loss functions.</p>
</td>
    <td>
      
        Unsupervised 
      
        Supervised 
      
        Image-Retrieval 
      
        Distance-Metric-Learning 
      
        Self-Supervised 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/zhao2019weakly/">A weakly supervised adaptive triplet loss for deep metric learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A weakly supervised adaptive triplet loss for deep metric learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A weakly supervised adaptive triplet loss for deep metric learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhao Xiaonan, Qi Huan, Luo Rui, Davis Larry</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</td>
    <td>22</td>
    <td><p>We address the problem of distance metric learning in visual similarity
search, defined as learning an image embedding model which projects images into
Euclidean space where semantically and visually similar images are closer and
dissimilar images are further from one another. We present a weakly supervised
adaptive triplet loss (ATL) capable of capturing fine-grained semantic
similarity that encourages the learned image embedding models to generalize
well on cross-domain data. The method uses weakly labeled product description
data to implicitly determine fine grained semantic classes, avoiding the need
to annotate large amounts of training data. We evaluate on the Amazon fashion
retrieval benchmark and DeepFashion in-shop retrieval data. The method boosts
the performance of triplet loss baseline by 10.6% on cross-domain data and
out-performs the state-of-art model on all evaluation metrics.</p>
</td>
    <td>
      
        ICCV 
      
        Distance-Metric-Learning 
      
        Supervised 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/tissier2018near/">Near-lossless Binarization of Word Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Near-lossless Binarization of Word Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Near-lossless Binarization of Word Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tissier Julien, Gravier Christophe, Habrard Amaury</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>43</td>
    <td><p>Word embeddings are commonly used as a starting point in many NLP models to
achieve state-of-the-art performances. However, with a large vocabulary and
many dimensions, these floating-point representations are expensive both in
terms of memory and calculations which makes them unsuitable for use on
low-resource devices. The method proposed in this paper transforms real-valued
embeddings into binary embeddings while preserving semantic information,
requiring only 128 or 256 bits for each vector. This leads to a small memory
footprint and fast vector operations. The model is based on an autoencoder
architecture, which also allows to reconstruct original vectors from the binary
ones. Experimental results on semantic similarity, text classification and
sentiment analysis tasks show that the binarization of word embeddings only
leads to a loss of ~2% in accuracy while vector size is reduced by 97%.
Furthermore, a top-k benchmark demonstrates that using these binary vectors is
30 times faster than using real-valued vectors.</p>
</td>
    <td>
      
        AAAI 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/bhunia2018texture/">Texture Synthesis Guided Deep Hashing for Texture Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Texture Synthesis Guided Deep Hashing for Texture Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Texture Synthesis Guided Deep Hashing for Texture Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Bhunia Ayan Kumar, Kishore Perla Sai Raj, Mukherjee Pranay, Das Abhirup, Roy Partha Pratim</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>17</td>
    <td><p>With the large-scale explosion of images and videos over the internet,
efficient hashing methods have been developed to facilitate memory and time
efficient retrieval of similar images. However, none of the existing works uses
hashing to address texture image retrieval mostly because of the lack of
sufficiently large texture image databases. Our work addresses this problem by
developing a novel deep learning architecture that generates binary hash codes
for input texture images. For this, we first pre-train a Texture Synthesis
Network (TSN) which takes a texture patch as input and outputs an enlarged view
of the texture by injecting newer texture content. Thus it signifies that the
TSN encodes the learnt texture specific information in its intermediate layers.
In the next stage, a second network gathers the multi-scale feature
representations from the TSN’s intermediate layers using channel-wise
attention, combines them in a progressive manner to a dense continuous
representation which is finally converted into a binary hash code with the help
of individual and pairwise label information. The new enlarged texture patches
also help in data augmentation to alleviate the problem of insufficient texture
data and are used to train the second stage of the network. Experiments on
three public texture image retrieval datasets indicate the superiority of our
texture synthesis guided hashing approach over current state-of-the-art
methods.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Scalability 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/beck2019distributed/">A Distributed and Approximated Nearest Neighbors Algorithm for an Efficient Large Scale Mean Shift Clustering</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Distributed and Approximated Nearest Neighbors Algorithm for an Efficient Large Scale Mean Shift Clustering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Distributed and Approximated Nearest Neighbors Algorithm for an Efficient Large Scale Mean Shift Clustering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Beck Gaël, Duong Tarn, Lebbah Mustapha, Azzag Hanane, Cérin Christophe</td> <!-- 🔧 You were missing this -->
    <td>Journal of Parallel and Distributed Computing</td>
    <td>29</td>
    <td><p>In this paper we target the class of modal clustering methods where clusters
are defined in terms of the local modes of the probability density function
which generates the data. The most well-known modal clustering method is the
k-means clustering. Mean Shift clustering is a generalization of the k-means
clustering which computes arbitrarily shaped clusters as defined as the basins
of attraction to the local modes created by the density gradient ascent paths.
Despite its potential, the Mean Shift approach is a computationally expensive
method for unsupervised learning. Thus, we introduce two contributions aiming
to provide clustering algorithms with a linear time complexity, as opposed to
the quadratic time complexity for the exact Mean Shift clustering. Firstly we
propose a scalable procedure to approximate the density gradient ascent.
Second, our proposed scalable cluster labeling technique is presented. Both
propositions are based on Locality Sensitive Hashing (LSH) to approximate
nearest neighbors. These two techniques may be used for moderate sized
datasets. Furthermore, we show that using our proposed approximations of the
density gradient ascent as a pre-processing step in other clustering methods
can also improve dedicated classification metrics. For the latter, a
distributed implementation, written for the Spark/Scala ecosystem is proposed.
For all these considered clustering methods, we present experimental results
illustrating their labeling accuracy and their potential to solve concrete
problems.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Unsupervised 
      
        Datasets 
      
        Locality-Sensitive-Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/barz2018hierarchy/">Hierarchy-based Image Embeddings for Semantic Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hierarchy-based Image Embeddings for Semantic Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hierarchy-based Image Embeddings for Semantic Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Barz Björn, Denzler Joachim</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>52</td>
    <td><p>Deep neural networks trained for classification have been found to learn
powerful image representations, which are also often used for other tasks such
as comparing images w.r.t. their visual similarity. However, visual similarity
does not imply semantic similarity. In order to learn semantically
discriminative features, we propose to map images onto class embeddings whose
pair-wise dot products correspond to a measure of semantic similarity between
classes. Such an embedding does not only improve image retrieval results, but
could also facilitate integrating semantics for other tasks, e.g., novelty
detection or few-shot learning. We introduce a deterministic algorithm for
computing the class centroids directly based on prior world-knowledge encoded
in a hierarchy of classes such as WordNet. Experiments on CIFAR-100, NABirds,
and ImageNet show that our learned semantic image embeddings improve the
semantic consistency of image retrieval results by a large margin.</p>
</td>
    <td>
      
        Few-Shot-&-Zero-Shot 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/chowdhury2018instance/">Instance-based Inductive Deep Transfer Learning by Cross-Dataset Querying with Locality Sensitive Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Instance-based Inductive Deep Transfer Learning by Cross-Dataset Querying with Locality Sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Instance-based Inductive Deep Transfer Learning by Cross-Dataset Querying with Locality Sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chowdhury Somnath Basu Roy, Annervaz K M, Dukkipati Ambedkar</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)</td>
    <td>5</td>
    <td><p>Supervised learning models are typically trained on a single dataset and the
performance of these models rely heavily on the size of the dataset, i.e.,
amount of data available with the ground truth. Learning algorithms try to
generalize solely based on the data that is presented with during the training.
In this work, we propose an inductive transfer learning method that can augment
learning models by infusing similar instances from different learning tasks in
the Natural Language Processing (NLP) domain. We propose to use instance
representations from a source dataset, \textit{without inheriting anything}
from the source learning model. Representations of the instances of
\textit{source} \&amp; \textit{target} datasets are learned, retrieval of relevant
source instances is performed using soft-attention mechanism and
\textit{locality sensitive hashing}, and then, augmented into the model during
training on the target dataset. Our approach simultaneously exploits the local
\textit{instance level information} as well as the macro statistical viewpoint
of the dataset. Using this approach we have shown significant improvements for
three major news classification datasets over the baseline. Experimental
evaluations also show that the proposed approach reduces dependency on labeled
data by a significant margin for comparable performance. With our proposed
cross dataset learning procedure we show that one can achieve
competitive/better performance than learning from a single dataset.</p>
</td>
    <td>
      
        Supervised 
      
        Locality-Sensitive-Hashing 
      
        Hashing-Methods 
      
        Datasets 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/baranchuk2019learning/">Learning to Route in Similarity Graphs</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning to Route in Similarity Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning to Route in Similarity Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Baranchuk Dmitry, Persiyanov Dmitry, Sinitsin Anton, Babenko Artem</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>8</td>
    <td><p>Recently similarity graphs became the leading paradigm for efficient nearest
neighbor search, outperforming traditional tree-based and LSH-based methods.
Similarity graphs perform the search via greedy routing: a query traverses the
graph and in each vertex moves to the adjacent vertex that is the closest to
this query. In practice, similarity graphs are often susceptible to local
minima, when queries do not reach its nearest neighbors, getting stuck in
suboptimal vertices. In this paper we propose to learn the routing function
that overcomes local minima via incorporating information about the graph
global structure. In particular, we augment the vertices of a given graph with
additional representations that are learned to provide the optimal routing from
the start vertex to the query nearest neighbor. By thorough experiments, we
demonstrate that the proposed learnable routing successfully diminishes the
local minima problem and significantly improves the overall search performance.</p>
</td>
    <td>
      
        Tree-Based-ANN 
      
        Evaluation 
      
        Locality-Sensitive-Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/sun2019supervised/">Supervised Hierarchical Cross-Modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Hierarchical Cross-Modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Supervised Hierarchical Cross-Modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sun Changchang, Song, Feng, Zhao, Nie</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>48</td>
    <td><p>Recently, due to the unprecedented growth of multimedia data,
cross-modal hashing has gained increasing attention for the
efficient cross-media retrieval. Typically, existing methods on crossmodal hashing treat labels of one instance independently but
overlook the correlations among labels. Indeed, in many real-world
scenarios, like the online fashion domain, instances (items) are
labeled with a set of categories correlated by certain hierarchy. In
this paper, we propose a new end-to-end solution for supervised
cross-modal hashing, named HiCHNet, which explicitly exploits the
hierarchical labels of instances. In particular, by the pre-established
label hierarchy, we comprehensively characterize each modality
of the instance with a set of layer-wise hash representations. In
essence, hash codes are encouraged to not only preserve the layerwise semantic similarities encoded by the label hierarchy, but also
retain the hierarchical discriminative capabilities. Due to the lack
of benchmark datasets, apart from adapting the existing dataset
FashionVC from fashion domain, we create a dataset from the
online fashion platform Ssense consisting of 15, 696 image-text
pairs labeled by 32 hierarchical categories. Extensive experiments
on two real-world datasets demonstrate the superiority of our model
over the state-of-the-art methods.</p>
</td>
    <td>
      
        Datasets 
      
        SIGIR 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/yang2019adaptive/">Adaptive Labeling for Deep Learning to Hash</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Adaptive Labeling for Deep Learning to Hash' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Adaptive Labeling for Deep Learning to Hash' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yang Huei-fang, Tu, Chen</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>8</td>
    <td><p>Hash function learning has been widely used for largescale image retrieval because of the efficiency of computation and storage. We introduce AdaLabelHash, a binary
hash function learning approach via deep neural networks
in this paper. In AdaLabelHash, class label representations are variables that are adapted during the backward
network training procedure. We express the labels as hypercube vertices in a K-dimensional space, and the class
label representations together with the network weights are
updated in the learning process. As the label representations (or referred to as codewords in this work), are learned
from data, semantically similar classes will be assigned
with the codewords that are close to each other in terms
of Hamming distance in the label space. The codewords
then serve as the desired output of the hash function learning, and yield compact and discriminating binary hash representations. AdaLabelHash is easy to implement, which
can jointly learn label representations and infer compact
binary codes from data. It is applicable to both supervised
and semi-supervised hash. Experimental results on standard benchmarks demonstrate the satisfactory performance
of AdaLabelHash.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Efficiency 
      
        CVPR 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/zhang2019pairwise/">Pairwise Teacher-Student Network for Semi-Supervised Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Pairwise Teacher-Student Network for Semi-Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Pairwise Teacher-Student Network for Semi-Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Shifeng, Li Jianmin, Zhang Bo</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>7</td>
    <td><p>Hashing method maps similar high-dimensional data to binary hashcodes with
smaller hamming distance, and it has received broad attention due to its low
storage cost and fast retrieval speed. Pairwise similarity is easily obtained
and widely used for retrieval, and most supervised hashing algorithms are
carefully designed for the pairwise supervisions. As labeling all data pairs is
difficult, semi-supervised hashing is proposed which aims at learning efficient
codes with limited labeled pairs and abundant unlabeled ones. Existing methods
build graphs to capture the structure of dataset, but they are not working well
for complex data as the graph is built based on the data representations and
determining the representations of complex data is difficult. In this paper, we
propose a novel teacher-student semi-supervised hashing framework in which the
student is trained with the pairwise information produced by the teacher
network. The network follows the smoothness assumption, which achieves
consistent distances for similar data pairs so that the retrieval results are
similar for neighborhood queries. Experiments on large-scale datasets show that
the proposed method reaches impressive gain over the supervised baselines and
is superior to state-of-the-art semi-supervised hashing methods.</p>
</td>
    <td>
      
        Supervised 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Memory-Efficiency 
      
        CVPR 
      
        Scalability 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/su2025deep/">Deep Joint-Semantics Reconstructing Hashing for Large-Scale Unsupervised Cross-Modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Joint-Semantics Reconstructing Hashing for Large-Scale Unsupervised Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Joint-Semantics Reconstructing Hashing for Large-Scale Unsupervised Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Su Shupeng, Zhong, Zhang</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>255</td>
    <td><p><img src="https://github.com/zzs1994/DJSRH/blob/master/page_image/DJRSH.png?raw=true" alt="Deep Joint-Semantics Reconstructing Hashing for Large-Scale Unsupervised Cross-Modal Retrieval" title="Deep Joint-Semantics Reconstructing Hashing for Large-Scale Unsupervised Cross-Modal Retrieval" /></p>

<p>Cross-modal hashing encodes the multimedia data into a common binary hash space in which the correlations among the samples from different modalities can be effectively measured. Deep cross-modal hashing further improves the retrieval performance as the deep neural networks can generate more semantic relevant features and hash codes. In this paper, we study the unsupervised deep cross-modal hash coding and propose Deep Joint Semantics Reconstructing Hashing (DJSRH), which has the following two main advantages. First, to learn binary codes that preserve the neighborhood structure of the original data, DJSRH constructs a novel joint-semantics affinity matrix which elaborately integrates the original neighborhood information from different modalities and accordingly is capable to capture the latent intrinsic semantic affinity for the input multi-modal instances. Second, DJSRH later trains the networks to generate binary codes that maximally reconstruct above joint-semantics relations via the proposed reconstructing framework, which is more competent for the batch-wise training as it reconstructs the specific similarity value unlike the common Laplacian constraint merely preserving the similarity order. Extensive experiments demonstrate the significant improvement by DJSRH in various cross-modal retrieval tasks.</p>
</td>
    <td>
      
        Scalability 
      
        ICCV 
      
        Multimodal-Retrieval 
      
        Tools-&-Libraries 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/artetxe2019bilingual/">Bilingual Lexicon Induction through Unsupervised Machine Translation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Bilingual Lexicon Induction through Unsupervised Machine Translation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Bilingual Lexicon Induction through Unsupervised Machine Translation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Artetxe Mikel, Labaka Gorka, Agirre Eneko</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</td>
    <td>53</td>
    <td><p>A recent research line has obtained strong results on bilingual lexicon
induction by aligning independently trained word embeddings in two languages
and using the resulting cross-lingual embeddings to induce word translation
pairs through nearest neighbor or related retrieval methods. In this paper, we
propose an alternative approach to this problem that builds on the recent work
on unsupervised machine translation. This way, instead of directly inducing a
bilingual lexicon from cross-lingual embeddings, we use them to build a
phrase-table, combine it with a language model, and use the resulting machine
translation system to generate a synthetic parallel corpus, from which we
extract the bilingual lexicon using statistical word alignment techniques. As
such, our method can work with any word embedding and cross-lingual mapping
technique, and it does not require any additional resource besides the
monolingual corpus used to train the embeddings. When evaluated on the exact
same cross-lingual embeddings, our proposed method obtains an average
improvement of 6 accuracy points over nearest neighbor and 4 points over CSLS
retrieval, establishing a new state-of-the-art in the standard MUSE dataset.</p>
</td>
    <td>
      
        Unsupervised 
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/artetxe2018massively/">Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Artetxe Mikel, Schwenk Holger</td> <!-- 🔧 You were missing this -->
    <td>Transactions of the Association for Computational Linguistics</td>
    <td>747</td>
    <td><p>We introduce an architecture to learn joint multilingual sentence
representations for 93 languages, belonging to more than 30 different families
and written in 28 different scripts. Our system uses a single BiLSTM encoder
with a shared BPE vocabulary for all languages, which is coupled with an
auxiliary decoder and trained on publicly available parallel corpora. This
enables us to learn a classifier on top of the resulting embeddings using
English annotated data only, and transfer it to any of the 93 languages without
any modification. Our experiments in cross-lingual natural language inference
(XNLI dataset), cross-lingual document classification (MLDoc dataset) and
parallel corpus mining (BUCC dataset) show the effectiveness of our approach.
We also introduce a new test set of aligned sentences in 112 languages, and
show that our sentence embeddings obtain strong results in multilingual
similarity search even for low-resource languages. Our implementation, the
pre-trained encoder and the multilingual test set are available at
https://github.com/facebookresearch/LASER</p>
</td>
    <td>
      
        Similarity-Search 
      
        Few-Shot-&-Zero-Shot 
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/artetxe2019margin/">Margin-based Parallel Corpus Mining with Multilingual Sentence Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Margin-based Parallel Corpus Mining with Multilingual Sentence Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Margin-based Parallel Corpus Mining with Multilingual Sentence Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Artetxe Mikel, Schwenk Holger</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</td>
    <td>163</td>
    <td><p>Machine translation is highly sensitive to the size and quality of the
training data, which has led to an increasing interest in collecting and
filtering large parallel corpora. In this paper, we propose a new method for
this task based on multilingual sentence embeddings. In contrast to previous
approaches, which rely on nearest neighbor retrieval with a hard threshold over
cosine similarity, our proposed method accounts for the scale inconsistencies
of this measure, considering the margin between a given sentence pair and its
closest candidates instead. Our experiments show large improvements over
existing methods. We outperform the best published results on the BUCC mining
task and the UN reconstruction task by more than 10 F1 and 30 precision points,
respectively. Filtering the English-German ParaCrawl corpus with our approach,
we obtain 31.2 BLEU points on newstest2014, an improvement of more than one
point over the best official filtered version.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/artetxe2018margin/">Margin-based Parallel Corpus Mining with Multilingual Sentence Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Margin-based Parallel Corpus Mining with Multilingual Sentence Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Margin-based Parallel Corpus Mining with Multilingual Sentence Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Artetxe Mikel, Schwenk Holger</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</td>
    <td>163</td>
    <td><p>Machine translation is highly sensitive to the size and quality of the
training data, which has led to an increasing interest in collecting and
filtering large parallel corpora. In this paper, we propose a new method for
this task based on multilingual sentence embeddings. In contrast to previous
approaches, which rely on nearest neighbor retrieval with a hard threshold over
cosine similarity, our proposed method accounts for the scale inconsistencies
of this measure, considering the margin between a given sentence pair and its
closest candidates instead. Our experiments show large improvements over
existing methods. We outperform the best published results on the BUCC mining
task and the UN reconstruction task by more than 10 F1 and 30 precision points,
respectively. Filtering the English-German ParaCrawl corpus with our approach,
we obtain 31.2 BLEU points on newstest2014, an improvement of more than one
point over the best official filtered version.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/zhang2017effective/">Effective Image Retrieval via Multilinear Multi-index Fusion</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Effective Image Retrieval via Multilinear Multi-index Fusion' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Effective Image Retrieval via Multilinear Multi-index Fusion' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Zhizhong, Xie Yuan, Zhang Wensheng, Tian Qi</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>24</td>
    <td><p>Multi-index fusion has demonstrated impressive performances in retrieval task
by integrating different visual representations in a unified framework.
However, previous works mainly consider propagating similarities via neighbor
structure, ignoring the high order information among different visual
representations. In this paper, we propose a new multi-index fusion scheme for
image retrieval. By formulating this procedure as a multilinear based
optimization problem, the complementary information hidden in different indexes
can be explored more thoroughly. Specially, we first build our multiple indexes
from various visual representations. Then a so-called index-specific functional
matrix, which aims to propagate similarities, is introduced for updating the
original index. The functional matrices are then optimized in a unified tensor
space to achieve a refinement, such that the relevant images can be pushed more
closer. The optimization problem can be efficiently solved by the augmented
Lagrangian method with theoretical convergence guarantee. Unlike the
traditional multi-index fusion scheme, our approach embeds the multi-index
subspace structure into the new indexes with sparse constraint, thus it has
little additional memory consumption in online query stage. Experimental
evaluation on three benchmark datasets reveals that the proposed approach
achieves the state-of-the-art performance, i.e., N-score 3.94 on UKBench, mAP
94.1% on Holiday and 62.39% on Market-1501.</p>
</td>
    <td>
      
        Vector-Indexing 
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
        Datasets 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/alemu2018multi/">Multi-feature Fusion for Image Retrieval Using Constrained Dominant Sets</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multi-feature Fusion for Image Retrieval Using Constrained Dominant Sets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multi-feature Fusion for Image Retrieval Using Constrained Dominant Sets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Alemu Leulseged Tesfaye, Pelillo Marcello</td> <!-- 🔧 You were missing this -->
    <td>Image and Vision Computing</td>
    <td>23</td>
    <td><p>Aggregating different image features for image retrieval has recently shown
its effectiveness. While highly effective, though, the question of how to
uplift the impact of the best features for a specific query image persists as
an open computer vision problem. In this paper, we propose a computationally
efficient approach to fuse several hand-crafted and deep features, based on the
probabilistic distribution of a given membership score of a constrained cluster
in an unsupervised manner. First, we introduce an incremental nearest neighbor
(NN) selection method, whereby we dynamically select k-NN to the query. We then
build several graphs from the obtained NN sets and employ constrained dominant
sets (CDS) on each graph G to assign edge weights which consider the intrinsic
manifold structure of the graph, and detect false matches to the query.
Finally, we elaborate the computation of feature positive-impact weight (PIW)
based on the dispersive degree of the characteristics vector. To this end, we
exploit the entropy of a cluster membership-score distribution. In addition,
the final NN set bypasses a heuristic voting scheme. Experiments on several
retrieval benchmark datasets show that our method can improve the
state-of-the-art result.</p>
</td>
    <td>
      
        Unsupervised 
      
        Datasets 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/aggarwal2018learning/">Learning Style Compatibility for Furniture</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Style Compatibility for Furniture' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Style Compatibility for Furniture' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Aggarwal Divyansh, Valiyev Elchin, Sener Fadime, Yao Angela</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>12</td>
    <td><p>When judging style, a key question that often arises is whether or not a pair
of objects are compatible with each other. In this paper we investigate how
Siamese networks can be used efficiently for assessing the style compatibility
between images of furniture items. We show that the middle layers of pretrained
CNNs can capture essential information about furniture style, which allows for
efficient applications of such networks for this task. We also use a joint
image-text embedding method that allows for the querying of stylistically
compatible furniture items, along with additional attribute constraints based
on text. To evaluate our methods, we collect and present a large scale dataset
of images of furniture of different style categories accompanied by text
attributes.</p>
</td>
    <td>
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/andoni2018approximate/">Approximate Nearest Neighbor Search in High Dimensions</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Approximate Nearest Neighbor Search in High Dimensions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Approximate Nearest Neighbor Search in High Dimensions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Andoni Alexandr, Indyk Piotr, Razenshteyn Ilya</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the International Congress of Mathematicians (ICM 2018)</td>
    <td>49</td>
    <td><p>The nearest neighbor problem is defined as follows: Given a set \(P\) of \(n\)
points in some metric space \((X,D)\), build a data structure that, given any
point \(q\), returns a point in \(P\) that is closest to \(q\) (its “nearest
neighbor” in \(P\)). The data structure stores additional information about the
set \(P\), which is then used to find the nearest neighbor without computing all
distances between \(q\) and \(P\). The problem has a wide range of applications in
machine learning, computer vision, databases and other fields.
  To reduce the time needed to find nearest neighbors and the amount of memory
used by the data structure, one can formulate the {\em approximate} nearest
neighbor problem, where the the goal is to return any point \(p’ \in P\) such
that the distance from \(q\) to \(p’\) is at most \(c \cdot \min_{p \in P} D(q,p)\),
for some \(c \geq 1\). Over the last two decades, many efficient solutions to
this problem were developed. In this article we survey these developments, as
well as their connections to questions in geometric functional analysis and
combinatorial geometry.</p>
</td>
    <td>
      
        Survey-Paper 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/vemulapalli2019compact/">A Compact Embedding for Facial Expression Similarity</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Compact Embedding for Facial Expression Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Compact Embedding for Facial Expression Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Vemulapalli Raviteja, Agarwala Aseem</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>102</td>
    <td><p>Most of the existing work on automatic facial expression analysis focuses on
discrete emotion recognition, or facial action unit detection. However, facial
expressions do not always fall neatly into pre-defined semantic categories.
Also, the similarity between expressions measured in the action unit space need
not correspond to how humans perceive expression similarity. Different from
previous work, our goal is to describe facial expressions in a continuous
fashion using a compact embedding space that mimics human visual preferences.
To achieve this goal, we collect a large-scale faces-in-the-wild dataset with
human annotations in the form: Expressions A and B are visually more similar
when compared to expression C, and use this dataset to train a neural network
that produces a compact (16-dimensional) expression embedding. We
experimentally demonstrate that the learned embedding can be successfully used
for various applications such as expression retrieval, photo album
summarization, and emotion recognition. We also show that the embedding learned
using the proposed dataset performs better than several other embeddings
learned using existing emotion or action unit datasets.</p>
</td>
    <td>
      
        Datasets 
      
        Scalability 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/vo2018composing/">Composing Text and Image for Image Retrieval - An Empirical Odyssey</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Composing Text and Image for Image Retrieval - An Empirical Odyssey' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Composing Text and Image for Image Retrieval - An Empirical Odyssey' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Vo Nam, Jiang Lu, Sun Chen, Murphy Kevin, Li Li-jia, Fei-fei Li, Hays James</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>317</td>
    <td><p>In this paper, we study the task of image retrieval, where the input query is
specified in the form of an image plus some text that describes desired
modifications to the input image. For example, we may present an image of the
Eiffel tower, and ask the system to find images which are visually similar but
are modified in small ways, such as being taken at nighttime instead of during
the day. To tackle this task, we learn a similarity metric between a target
image and a source image plus source text, an embedding and composing function
such that target image feature is close to the source image plus text
composition feature. We propose a new way to combine image and text using such
function that is designed for the retrieval task. We show this outperforms
existing approaches on 3 different datasets, namely Fashion-200k, MIT-States
and a new synthetic dataset we create based on CLEVR. We also show that our
approach can be used to classify input queries, in addition to image retrieval.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Datasets 
      
        CVPR 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/su2019deep/">Deep Joint-Semantics Reconstructing Hashing for Large-Scale Unsupervised Cross-Modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Joint-Semantics Reconstructing Hashing for Large-Scale Unsupervised Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Joint-Semantics Reconstructing Hashing for Large-Scale Unsupervised Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Su Shupeng, Zhong, Zhang</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>255</td>
    <td><p><img src="https://github.com/zzs1994/DJSRH/blob/master/page_image/DJRSH.png?raw=true" alt="Deep Joint-Semantics Reconstructing Hashing for Large-Scale Unsupervised Cross-Modal Retrieval" title="Deep Joint-Semantics Reconstructing Hashing for Large-Scale Unsupervised Cross-Modal Retrieval" /></p>

<p>Cross-modal hashing encodes the multimedia data into a common binary hash space in which the correlations among the samples from different modalities can be effectively measured. Deep cross-modal hashing further improves the retrieval performance as the deep neural networks can generate more semantic relevant features and hash codes. In this paper, we study the unsupervised deep cross-modal hash coding and propose Deep Joint Semantics Reconstructing Hashing (DJSRH), which has the following two main advantages. First, to learn binary codes that preserve the neighborhood structure of the original data, DJSRH constructs a novel joint-semantics affinity matrix which elaborately integrates the original neighborhood information from different modalities and accordingly is capable to capture the latent intrinsic semantic affinity for the input multi-modal instances. Second, DJSRH later trains the networks to generate binary codes that maximally reconstruct above joint-semantics relations via the proposed reconstructing framework, which is more competent for the batch-wise training as it reconstructs the specific similarity value unlike the common Laplacian constraint merely preserving the similarity order. Extensive experiments demonstrate the significant improvement by DJSRH in various cross-modal retrieval tasks.</p>
</td>
    <td>
      
        Scalability 
      
        ICCV 
      
        Multimodal-Retrieval 
      
        Tools-&-Libraries 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/nawaz2019do/">Do Cross Modal Systems Leverage Semantic Relationships?</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Do Cross Modal Systems Leverage Semantic Relationships?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Do Cross Modal Systems Leverage Semantic Relationships?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Nawaz Shah, Janjua Muhammad Kamran, Gallo Ignazio, Mahmood Arif, Calefati Alessandro, Shafait Faisal</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</td>
    <td>9</td>
    <td><p>Current cross-modal retrieval systems are evaluated using R@K measure which
does not leverage semantic relationships rather strictly follows the manually
marked image text query pairs. Therefore, current systems do not generalize
well for the unseen data in the wild. To handle this, we propose a new measure,
SemanticMap, to evaluate the performance of cross-modal systems. Our proposed
measure evaluates the semantic similarity between the image and text
representations in the latent embedding space. We also propose a novel
cross-modal retrieval system using a single stream network for bidirectional
retrieval. The proposed system is based on a deep neural network trained using
extended center loss, minimizing the distance of image and text descriptions in
the latent space from the class centers. In our system, the text descriptions
are also encoded as images which enabled us to use a single stream network for
both text and images. To the best of our knowledge, our work is the first of
its kind in terms of employing a single stream network for cross-modal
retrieval systems. The proposed system is evaluated on two publicly available
datasets including MSCOCO and Flickr30K and has shown comparable results to the
current state-of-the-art methods.</p>
</td>
    <td>
      
        ICCV 
      
        Datasets 
      
        Evaluation 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/morozov2019unsupervised/">Unsupervised Neural Quantization for Compressed-Domain Similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Neural Quantization for Compressed-Domain Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Neural Quantization for Compressed-Domain Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Morozov Stanislav, Babenko Artem</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>21</td>
    <td><p>We tackle the problem of unsupervised visual descriptors compression, which
is a key ingredient of large-scale image retrieval systems. While the deep
learning machinery has benefited literally all computer vision pipelines, the
existing state-of-the-art compression methods employ shallow architectures, and
we aim to close this gap by our paper. In more detail, we introduce a DNN
architecture for the unsupervised compressed-domain retrieval, based on
multi-codebook quantization. The proposed architecture is designed to
incorporate both fast data encoding and efficient distances computation via
lookup tables. We demonstrate the exceptional advantage of our scheme over
existing quantization approaches on several datasets of visual descriptors via
outperforming the previous state-of-the-art by a large margin.</p>
</td>
    <td>
      
        ICCV 
      
        Similarity-Search 
      
        Image-Retrieval 
      
        Datasets 
      
        Quantization 
      
        Unsupervised 
      
        Scalability 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/wang2019learning/">Learning Cross-Modal Embeddings with Adversarial Networks for Cooking Recipes and Food Images</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Cross-Modal Embeddings with Adversarial Networks for Cooking Recipes and Food Images' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Cross-Modal Embeddings with Adversarial Networks for Cooking Recipes and Food Images' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Hao, Sahoo Doyen, Liu Chenghao, Lim Ee-peng, Hoi Steven C. H.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>127</td>
    <td><p>Food computing is playing an increasingly important role in human daily life,
and has found tremendous applications in guiding human behavior towards smart
food consumption and healthy lifestyle. An important task under the
food-computing umbrella is retrieval, which is particularly helpful for health
related applications, where we are interested in retrieving important
information about food (e.g., ingredients, nutrition, etc.). In this paper, we
investigate an open research task of cross-modal retrieval between cooking
recipes and food images, and propose a novel framework Adversarial Cross-Modal
Embedding (ACME) to resolve the cross-modal retrieval task in food domains.
Specifically, the goal is to learn a common embedding feature space between the
two modalities, in which our approach consists of several novel ideas: (i)
learning by using a new triplet loss scheme together with an effective sampling
strategy, (ii) imposing modality alignment using an adversarial learning
strategy, and (iii) imposing cross-modal translation consistency such that the
embedding of one modality is able to recover some important information of
corresponding instances in the other modality. ACME achieves the
state-of-the-art performance on the benchmark Recipe1M dataset, validating the
efficacy of the proposed technique.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        CVPR 
      
        Robustness 
      
        Evaluation 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/vemulapalli2018compact/">A Compact Embedding for Facial Expression Similarity</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Compact Embedding for Facial Expression Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Compact Embedding for Facial Expression Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Vemulapalli Raviteja, Agarwala Aseem</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>102</td>
    <td><p>Most of the existing work on automatic facial expression analysis focuses on
discrete emotion recognition, or facial action unit detection. However, facial
expressions do not always fall neatly into pre-defined semantic categories.
Also, the similarity between expressions measured in the action unit space need
not correspond to how humans perceive expression similarity. Different from
previous work, our goal is to describe facial expressions in a continuous
fashion using a compact embedding space that mimics human visual preferences.
To achieve this goal, we collect a large-scale faces-in-the-wild dataset with
human annotations in the form: Expressions A and B are visually more similar
when compared to expression C, and use this dataset to train a neural network
that produces a compact (16-dimensional) expression embedding. We
experimentally demonstrate that the learned embedding can be successfully used
for various applications such as expression retrieval, photo album
summarization, and emotion recognition. We also show that the embedding learned
using the proposed dataset performs better than several other embeddings
learned using existing emotion or action unit datasets.</p>
</td>
    <td>
      
        Datasets 
      
        Scalability 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/sun2025supervised/">Supervised Hierarchical Cross-Modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Hierarchical Cross-Modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Supervised Hierarchical Cross-Modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sun Changchang, Song, Feng, Zhao, Nie</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>48</td>
    <td><p>Recently, due to the unprecedented growth of multimedia data,
cross-modal hashing has gained increasing attention for the
efficient cross-media retrieval. Typically, existing methods on crossmodal hashing treat labels of one instance independently but
overlook the correlations among labels. Indeed, in many real-world
scenarios, like the online fashion domain, instances (items) are
labeled with a set of categories correlated by certain hierarchy. In
this paper, we propose a new end-to-end solution for supervised
cross-modal hashing, named HiCHNet, which explicitly exploits the
hierarchical labels of instances. In particular, by the pre-established
label hierarchy, we comprehensively characterize each modality
of the instance with a set of layer-wise hash representations. In
essence, hash codes are encouraged to not only preserve the layerwise semantic similarities encoded by the label hierarchy, but also
retain the hierarchical discriminative capabilities. Due to the lack
of benchmark datasets, apart from adapting the existing dataset
FashionVC from fashion domain, we create a dataset from the
online fashion platform Ssense consisting of 15, 696 image-text
pairs labeled by 32 hierarchical categories. Extensive experiments
on two real-world datasets demonstrate the superiority of our model
over the state-of-the-art methods.</p>
</td>
    <td>
      
        Datasets 
      
        SIGIR 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/mohammadshahi2019aligning/">Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mohammadshahi Alireza, Lebret Remi, Aberer Karl</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER)</td>
    <td>8</td>
    <td><p>In this paper, we propose a new approach to learn multimodal multilingual
embeddings for matching images and their relevant captions in two languages. We
combine two existing objective functions to make images and captions close in a
joint embedding space while adapting the alignment of word embeddings between
existing languages in our model. We show that our approach enables better
generalization, achieving state-of-the-art performance in text-to-image and
image-to-text retrieval task, and caption-caption similarity task. Two
multimodal multilingual datasets are used for evaluation: Multi30k with German
and English captions and Microsoft-COCO with English and Japanese captions.</p>
</td>
    <td>
      
        Datasets 
      
        Text-Retrieval 
      
        Evaluation 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/wang2019memory/">A Memory-Efficient Sketch Method for Estimating High Similarities in Streaming Sets</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Memory-Efficient Sketch Method for Estimating High Similarities in Streaming Sets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Memory-Efficient Sketch Method for Estimating High Similarities in Streaming Sets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Pinghui, Qi Yiyan, Zhang Yuanming, Zhai Qiaozhu, Wang Chenxu, Lui John C. S., Guan Xiaohong</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</td>
    <td>33</td>
    <td><p>Estimating set similarity and detecting highly similar sets are fundamental
problems in areas such as databases, machine learning, and information
retrieval. MinHash is a well-known technique for approximating Jaccard
similarity of sets and has been successfully used for many applications such as
similarity search and large scale learning. Its two compressed versions, b-bit
MinHash and Odd Sketch, can significantly reduce the memory usage of the
original MinHash method, especially for estimating high similarities (i.e.,
similarities around 1). Although MinHash can be applied to static sets as well
as streaming sets, of which elements are given in a streaming fashion and
cardinality is unknown or even infinite, unfortunately, b-bit MinHash and Odd
Sketch fail to deal with streaming data. To solve this problem, we design a
memory efficient sketch method, MaxLogHash, to accurately estimate Jaccard
similarities in streaming sets. Compared to MinHash, our method uses smaller
sized registers (each register consists of less than 7 bits) to build a compact
sketch for each set. We also provide a simple yet accurate estimator for
inferring Jaccard similarity from MaxLogHash sketches. In addition, we derive
formulas for bounding the estimation error and determine the smallest necessary
memory usage (i.e., the number of registers used for a MaxLogHash sketch) for
the desired accuracy. We conduct experiments on a variety of datasets, and
experimental results show that our method MaxLogHash is about 5 times more
memory efficient than MinHash with the same accuracy and computational cost for
estimating high similarities.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Locality-Sensitive-Hashing 
      
        Datasets 
      
        KDD 
      
        Memory-Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/wang2019fusion/">Fusion-supervised Deep Cross-modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fusion-supervised Deep Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fusion-supervised Deep Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Li, Zhu Lei, Yu En, Sun Jiande, Zhang Huaxiang</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE International Conference on Multimedia and Expo (ICME)</td>
    <td>15</td>
    <td><p>Deep hashing has recently received attention in cross-modal retrieval for its
impressive advantages. However, existing hashing methods for cross-modal
retrieval cannot fully capture the heterogeneous multi-modal correlation and
exploit the semantic information. In this paper, we propose a novel
<em>Fusion-supervised Deep Cross-modal Hashing</em> (FDCH) approach. Firstly,
FDCH learns unified binary codes through a fusion hash network with paired
samples as input, which effectively enhances the modeling of the correlation of
heterogeneous multi-modal data. Then, these high-quality unified hash codes
further supervise the training of the modality-specific hash networks for
encoding out-of-sample queries. Meanwhile, both pair-wise similarity
information and classification information are embedded in the hash networks
under one stream framework, which simultaneously preserves cross-modal
similarity and keeps semantic consistency. Experimental results on two
benchmark datasets demonstrate the state-of-the-art performance of FDCH.</p>
</td>
    <td>
      
        Supervised 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Evaluation 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/zhang2018improved/">Improved Deep Hashing with Soft Pairwise Similarity for Multi-label Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Improved Deep Hashing with Soft Pairwise Similarity for Multi-label Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Improved Deep Hashing with Soft Pairwise Similarity for Multi-label Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Zheng, Zou Qin, Lin Yuewei, Chen Long, Wang Song</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>144</td>
    <td><p>Hash coding has been widely used in the approximate nearest neighbor search
for large-scale image retrieval. Recently, many deep hashing methods have been
proposed and shown largely improved performance over traditional
feature-learning-based methods. Most of these methods examine the pairwise
similarity on the semantic-level labels, where the pairwise similarity is
generally defined in a hard-assignment way. That is, the pairwise similarity is
‘1’ if they share no less than one class label and ‘0’ if they do not share
any. However, such similarity definition cannot reflect the similarity ranking
for pairwise images that hold multiple labels. In this paper, a new deep
hashing method is proposed for multi-label image retrieval by re-defining the
pairwise similarity into an instance similarity, where the instance similarity
is quantified into a percentage based on the normalized semantic labels. Based
on the instance similarity, a weighted cross-entropy loss and a minimum mean
square error loss are tailored for loss-function construction, and are
efficiently used for simultaneous feature learning and hash coding. Experiments
on three popular datasets demonstrate that, the proposed method outperforms the
competing methods and achieves the state-of-the-art performance in multi-label
image retrieval.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/wang2018deep/">Deep Metric Learning by Online Soft Mining and Class-Aware Attention</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Metric Learning by Online Soft Mining and Class-Aware Attention' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Metric Learning by Online Soft Mining and Class-Aware Attention' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Xinshao, Hua Yang, Kodirov Elyor, Hu Guosheng, Robertson Neil M.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>45</td>
    <td><p>Deep metric learning aims to learn a deep embedding that can capture the
semantic similarity of data points. Given the availability of massive training
samples, deep metric learning is known to suffer from slow convergence due to a
large fraction of trivial samples. Therefore, most existing methods generally
resort to sample mining strategies for selecting nontrivial samples to
accelerate convergence and improve performance. In this work, we identify two
critical limitations of the sample mining methods, and provide solutions for
both of them. First, previous mining methods assign one binary score to each
sample, i.e., dropping or keeping it, so they only selects a subset of relevant
samples in a mini-batch. Therefore, we propose a novel sample mining method,
called Online Soft Mining (OSM), which assigns one continuous score to each
sample to make use of all samples in the mini-batch. OSM learns extended
manifolds that preserve useful intraclass variances by focusing on more similar
positives. Second, the existing methods are easily influenced by outliers as
they are generally included in the mined subset. To address this, we introduce
Class-Aware Attention (CAA) that assigns little attention to abnormal data
samples. Furthermore, by combining OSM and CAA, we propose a novel weighted
contrastive loss to learn discriminative embeddings. Extensive experiments on
two fine-grained visual categorisation datasets and two video-based person
re-identification benchmarks show that our method significantly outperforms the
state-of-the-art.</p>
</td>
    <td>
      
        AAAI 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/talreja2019zero/">Zero-Shot Deep Hashing and Neural Network Based Error Correction for Face Template Protection</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Zero-Shot Deep Hashing and Neural Network Based Error Correction for Face Template Protection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Zero-Shot Deep Hashing and Neural Network Based Error Correction for Face Template Protection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Talreja Veeru, Valenti Matthew C., Nasrabadi Nasser M.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE 10th International Conference on Biometrics Theory, Applications and Systems (BTAS)</td>
    <td>20</td>
    <td><p>In this paper, we present a novel architecture that integrates a deep hashing
framework with a neural network decoder (NND) for application to face template
protection. It improves upon existing face template protection techniques to
provide better matching performance with one-shot and multi-shot enrollment. A
key novelty of our proposed architecture is that the framework can also be used
with zero-shot enrollment. This implies that our architecture does not need to
be re-trained even if a new subject is to be enrolled into the system. The
proposed architecture consists of two major components: a deep hashing (DH)
component, which is used for robust mapping of face images to their
corresponding intermediate binary codes, and a NND component, which corrects
errors in the intermediate binary codes that are caused by differences in the
enrollment and probe biometrics due to factors such as variation in pose,
illumination, and other factors. The final binary code generated by the NND is
then cryptographically hashed and stored as a secure face template in the
database. The efficacy of our approach with zero-shot, one-shot, and multi-shot
enrollments is shown for CMU-PIE, Extended Yale B, WVU multimodal and Multi-PIE
face databases. With zero-shot enrollment, the system achieves approximately
85% genuine accept rates (GAR) at 0.01% false accept rate (FAR), and with
one-shot and multi-shot enrollments, it achieves approximately 99.95% GAR at
0.01% FAR, while providing a high level of template security.</p>
</td>
    <td>
      
        Few-Shot-&-Zero-Shot 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/tanioka2019fast/">A Fast Content-Based Image Retrieval Method Using Deep Visual Features</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Fast Content-Based Image Retrieval Method Using Deep Visual Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Fast Content-Based Image Retrieval Method Using Deep Visual Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tanioka Hiroki</td> <!-- 🔧 You were missing this -->
    <td>2019 International Conference on Document Analysis and Recognition Workshops (ICDARW)</td>
    <td>10</td>
    <td><p>Fast and scalable Content-Based Image Retrieval using visual features is
required for document analysis, Medical image analysis, etc. in the present
age. Convolutional Neural Network (CNN) activations as features achieved their
outstanding performance in this area. Deep Convolutional representations using
the softmax function in the output layer are also ones among visual features.
However, almost all the image retrieval systems hold their index of visual
features on main memory in order to high responsiveness, limiting their
applicability for big data applications. In this paper, we propose a fast
calculation method of cosine similarity with L2 norm indexed in advance on
Elasticsearch. We evaluate our approach with ImageNet Dataset and VGG-16
pre-trained model. The evaluation results show the effectiveness and efficiency
of our proposed method.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/wang2019ranked/">Ranked List Loss for Deep Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Ranked List Loss for Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Ranked List Loss for Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Xinshao, Hua Yang, Kodirov Elyor, Robertson Neil M.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>233</td>
    <td><p>The objective of deep metric learning (DML) is to learn embeddings that can
capture semantic similarity and dissimilarity information among data points.
Existing pairwise or tripletwise loss functions used in DML are known to suffer
from slow convergence due to a large proportion of trivial pairs or triplets as
the model improves. To improve this, ranking-motivated structured losses are
proposed recently to incorporate multiple examples and exploit the structured
information among them. They converge faster and achieve state-of-the-art
performance. In this work, we unveil two limitations of existing
ranking-motivated structured losses and propose a novel ranked list loss to
solve both of them. First, given a query, only a fraction of data points is
incorporated to build the similarity structure. Consequently, some useful
examples are ignored and the structure is less informative. To address this, we
propose to build a set-based similarity structure by exploiting all instances
in the gallery. The learning setting can be interpreted as few-shot retrieval:
given a mini-batch, every example is iteratively used as a query, and the rest
ones compose the gallery to search, i.e., the support set in few-shot setting.
The rest examples are split into a positive set and a negative set. For every
mini-batch, the learning objective of ranked list loss is to make the query
closer to the positive set than to the negative set by a margin. Second,
previous methods aim to pull positive pairs as close as possible in the
embedding space. As a result, the intraclass data distribution tends to be
extremely compressed. In contrast, we propose to learn a hypersphere for each
class in order to preserve useful similarity structure inside it, which
functions as regularisation. Extensive experiments demonstrate the superiority
of our proposal by comparing with the state-of-the-art methods.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Few-Shot-&-Zero-Shot 
      
        CVPR 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/magliani2019efficient/">An Efficient Approximate kNN Graph Method for Diffusion on Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=An Efficient Approximate kNN Graph Method for Diffusion on Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=An Efficient Approximate kNN Graph Method for Diffusion on Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Magliani Federico, Mcguinness Kevin, Mohedano Eva, Prati Andrea</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>8</td>
    <td><p>The application of the diffusion in many computer vision and artificial
intelligence projects has been shown to give excellent improvements in
performance. One of the main bottlenecks of this technique is the quadratic
growth of the kNN graph size due to the high-quantity of new connections
between nodes in the graph, resulting in long computation times. Several
strategies have been proposed to address this, but none are effective and
efficient. Our novel technique, based on LSH projections, obtains the same
performance as the exact kNN graph after diffusion, but in less time
(approximately 18 times faster on a dataset of a hundred thousand images). The
proposed method was validated and compared with other state-of-the-art on
several public image datasets, including Oxford5k, Paris6k, and Oxford105k.</p>
</td>
    <td>
      
        Datasets 
      
        Evaluation 
      
        Locality-Sensitive-Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/mahajan2019joint/">Joint Wasserstein Autoencoders for Aligning Multimodal Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Joint Wasserstein Autoencoders for Aligning Multimodal Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Joint Wasserstein Autoencoders for Aligning Multimodal Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mahajan Shweta, Botschen Teresa, Gurevych Iryna, Roth Stefan</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</td>
    <td>8</td>
    <td><p>One of the key challenges in learning joint embeddings of multiple
modalities, e.g. of images and text, is to ensure coherent cross-modal
semantics that generalize across datasets. We propose to address this through
joint Gaussian regularization of the latent representations. Building on
Wasserstein autoencoders (WAEs) to encode the input in each domain, we enforce
the latent embeddings to be similar to a Gaussian prior that is shared across
the two domains, ensuring compatible continuity of the encoded semantic
representations of images and texts. Semantic alignment is achieved through
supervision from matching image-text pairs. To show the benefits of our
semi-supervised representation, we apply it to cross-modal retrieval and phrase
localization. We not only achieve state-of-the-art accuracy, but significantly
better generalization across datasets, owing to the semantic continuity of the
latent space.</p>
</td>
    <td>
      
        ICCV 
      
        Datasets 
      
        Supervised 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/yang2018deep/">Deep Attention-guided Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Attention-guided Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Attention-guided Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yang Zhan, Raymond Osolo Ian, Sun Wuqing, Long Jun</td> <!-- 🔧 You were missing this -->
    <td>IEEE Access</td>
    <td>17</td>
    <td><p>With the rapid growth of multimedia data (e.g., image, audio and video etc.)
on the web, learning-based hashing techniques such as Deep Supervised Hashing
(DSH) have proven to be very efficient for large-scale multimedia search. The
recent successes seen in Learning-based hashing methods are largely due to the
success of deep learning-based hashing methods. However, there are some
limitations to previous learning-based hashing methods (e.g., the learned hash
codes containing repetitive and highly correlated information). In this paper,
we propose a novel learning-based hashing method, named Deep Attention-guided
Hashing (DAgH). DAgH is implemented using two stream frameworks. The core idea
is to use guided hash codes which are generated by the hashing network of the
first stream framework (called first hashing network) to guide the training of
the hashing network of the second stream framework (called second hashing
network). Specifically, in the first network, it leverages an attention network
and hashing network to generate the attention-guided hash codes from the
original images. The loss function we propose contains two components: the
semantic loss and the attention loss. The attention loss is used to punish the
attention network to obtain the salient region from pairs of images; in the
second network, these attention-guided hash codes are used to guide the
training of the second hashing network (i.e., these codes are treated as
supervised labels to train the second network). By doing this, DAgH can make
full use of the most critical information contained in images to guide the
second hashing network in order to learn efficient hash codes in a true
end-to-end fashion. Results from our experiments demonstrate that DAgH can
generate high quality hash codes and it outperforms current state-of-the-art
methods on three benchmark datasets, CIFAR-10, NUS-WIDE, and ImageNet.</p>
</td>
    <td>
      
        Supervised 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/yu2018discriminative/">Discriminative Supervised Hashing for Cross-Modal similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Discriminative Supervised Hashing for Cross-Modal similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Discriminative Supervised Hashing for Cross-Modal similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yu Jun, Wu Xiao-jun, Kittler Josef</td> <!-- 🔧 You were missing this -->
    <td>Image and Vision Computing</td>
    <td>15</td>
    <td><p>With the advantage of low storage cost and high retrieval efficiency, hashing
techniques have recently been an emerging topic in cross-modal similarity
search. As multiple modal data reflect similar semantic content, many
researches aim at learning unified binary codes. However, discriminative
hashing features learned by these methods are not adequate. This results in
lower accuracy and robustness. We propose a novel hashing learning framework
which jointly performs classifier learning, subspace learning and matrix
factorization to preserve class-specific semantic content, termed
Discriminative Supervised Hashing (DSH), to learn the discrimative unified
binary codes for multi-modal data. Besides, reducing the loss of information
and preserving the non-linear structure of data, DSH non-linearly projects
different modalities into the common space in which the similarity among
heterogeneous data points can be measured. Extensive experiments conducted on
the three publicly available datasets demonstrate that the framework proposed
in this paper outperforms several state-of -the-art methods.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Supervised 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Memory-Efficiency 
      
        Efficiency 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/lu2025online/">Online Multi-modal Hashing with Dynamic Query-adaption</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Online Multi-modal Hashing with Dynamic Query-adaption' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Online Multi-modal Hashing with Dynamic Query-adaption' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lu Xu, Zhu, Cheng, Zhang</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>128</td>
    <td><p>Multi-modal hashing is an effective technique to support large-scale multimedia retrieval, due to its capability of encoding heterogeneous multi-modal features into compact and similarity-preserving binary codes. Although great progress has been achieved so far, existing methods still suffer from several problems, including: 1) All existing methods simply adopt fixed modality combination weights in online hashing process to generate the query hash codes. This strategy cannot adaptively capture the variations of different queries. 2) They either suffer from insufficient semantics (for unsupervised methods) or require high computation and storage cost (for the supervised methods, which rely on pair-wise semantic matrix). 3) They solve the hash codes with relaxed optimization strategy or bit-by-bit discrete optimization, which results in significant quantization loss or consumes considerable computation time. To address the above limitations, in this paper, we propose an Online Multi-modal Hashing with Dynamic Query-adaption (OMH-DQ) method in a novel fashion. Specifically, a self-weighted fusion strategy is designed to adaptively preserve the multi-modal feature information into hash codes by exploiting their complementarity. The hash codes are learned with the supervision of pair-wise semantic labels to enhance their discriminative capability, while avoiding the challenging symmetric similarity matrix factorization. Under such learning framework, the binary hash codes can be directly obtained with efficient operations and without quantization errors. Accordingly, our method can benefit from the semantic labels, and simultaneously, avoid the high computation complexity. Moreover, to accurately capture the query variations, at the online retrieval stage, we design a parameter-free online hashing module which can adaptively learn the query hash codes according to the dynamic query contents. Extensive experiments demonstrate the state-of-the-art performance of the proposed approach from various aspects.</p>
</td>
    <td>
      
        Scalability 
      
        Memory-Efficiency 
      
        Tools-&-Libraries 
      
        SIGIR 
      
        Supervised 
      
        Compact-Codes 
      
        Quantization 
      
        Hashing-Methods 
      
        Evaluation 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/tuinhof2019image/">Image Based Fashion Product Recommendation with Deep Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Image Based Fashion Product Recommendation with Deep Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Image Based Fashion Product Recommendation with Deep Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tuinhof Hessel, Pirker Clemens, Haltmeier Markus</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>65</td>
    <td><p>We develop a two-stage deep learning framework that recommends fashion images
based on other input images of similar style. For that purpose, a neural
network classifier is used as a data-driven, visually-aware feature extractor.
The latter then serves as input for similarity-based recommendations using a
ranking algorithm. Our approach is tested on the publicly available Fashion
dataset. Initialization strategies using transfer learning from larger product
databases are presented. Combined with more traditional content-based
recommendation systems, our framework can help to increase robustness and
performance, for example, by better matching a particular customer style.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Datasets 
      
        Recommender-Systems 
      
        Evaluation 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/yang2018efficient/">Efficient Image Retrieval via Decoupling Diffusion into Online and Offline Processing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Image Retrieval via Decoupling Diffusion into Online and Offline Processing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient Image Retrieval via Decoupling Diffusion into Online and Offline Processing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yang Fan, Hinami Ryota, Matsui Yusuke, Ly Steven, Satoh Shin'ichi</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>49</td>
    <td><p>Diffusion is commonly used as a ranking or re-ranking method in retrieval
tasks to achieve higher retrieval performance, and has attracted lots of
attention in recent years. A downside to diffusion is that it performs slowly
in comparison to the naive k-NN search, which causes a non-trivial online
computational cost on large datasets. To overcome this weakness, we propose a
novel diffusion technique in this paper. In our work, instead of applying
diffusion to the query, we pre-compute the diffusion results of each element in
the database, making the online search a simple linear combination on top of
the k-NN search process. Our proposed method becomes 10~ times faster in terms
of online search speed. Moreover, we propose to use late truncation instead of
early truncation in previous works to achieve better retrieval performance.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Datasets 
      
        Hybrid-ANN-Methods 
      
        Re-Ranking 
      
        AAAI 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/lu2019online/">Online Multi-modal Hashing with Dynamic Query-adaption</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Online Multi-modal Hashing with Dynamic Query-adaption' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Online Multi-modal Hashing with Dynamic Query-adaption' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lu Xu, Zhu, Cheng, Zhang</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>128</td>
    <td><p>Multi-modal hashing is an effective technique to support large-scale multimedia retrieval, due to its capability of encoding heterogeneous multi-modal features into compact and similarity-preserving binary codes. Although great progress has been achieved so far, existing methods still suffer from several problems, including: 1) All existing methods simply adopt fixed modality combination weights in online hashing process to generate the query hash codes. This strategy cannot adaptively capture the variations of different queries. 2) They either suffer from insufficient semantics (for unsupervised methods) or require high computation and storage cost (for the supervised methods, which rely on pair-wise semantic matrix). 3) They solve the hash codes with relaxed optimization strategy or bit-by-bit discrete optimization, which results in significant quantization loss or consumes considerable computation time. To address the above limitations, in this paper, we propose an Online Multi-modal Hashing with Dynamic Query-adaption (OMH-DQ) method in a novel fashion. Specifically, a self-weighted fusion strategy is designed to adaptively preserve the multi-modal feature information into hash codes by exploiting their complementarity. The hash codes are learned with the supervision of pair-wise semantic labels to enhance their discriminative capability, while avoiding the challenging symmetric similarity matrix factorization. Under such learning framework, the binary hash codes can be directly obtained with efficient operations and without quantization errors. Accordingly, our method can benefit from the semantic labels, and simultaneously, avoid the high computation complexity. Moreover, to accurately capture the query variations, at the online retrieval stage, we design a parameter-free online hashing module which can adaptively learn the query hash codes according to the dynamic query contents. Extensive experiments demonstrate the state-of-the-art performance of the proposed approach from various aspects.</p>
</td>
    <td>
      
        Scalability 
      
        Memory-Efficiency 
      
        Tools-&-Libraries 
      
        SIGIR 
      
        Supervised 
      
        Compact-Codes 
      
        Quantization 
      
        Hashing-Methods 
      
        Evaluation 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/markchit2019effective/">Effective and Efficient Indexing in Cross-Modal Hashing-Based Datasets</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Effective and Efficient Indexing in Cross-Modal Hashing-Based Datasets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Effective and Efficient Indexing in Cross-Modal Hashing-Based Datasets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Markchit Sarawut, Chiu Chih-yi</td> <!-- 🔧 You were missing this -->
    <td>Signal Processing: Image Communication</td>
    <td>6</td>
    <td><p>To overcome the barrier of storage and computation, the hashing technique has
been widely used for nearest neighbor search in multimedia retrieval
applications recently. Particularly, cross-modal retrieval that searches across
different modalities becomes an active but challenging problem. Although dozens
of cross-modal hashing algorithms are proposed to yield compact binary codes,
the exhaustive search is impractical for the real-time purpose, and Hamming
distance computation suffers inaccurate results. In this paper, we propose a
novel search method that utilizes a probability-based index scheme over binary
hash codes in cross-modal retrieval. The proposed hash code indexing scheme
exploits a few binary bits of the hash code as the index code. We construct an
inverted index table based on index codes and train a neural network to improve
the indexing accuracy and efficiency. Experiments are performed on two
benchmark datasets for retrieval across image and text modalities, where hash
codes are generated by three cross-modal hashing methods. Results show the
proposed method effectively boost the performance on these hash methods.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        Compact-Codes 
      
        Efficiency 
      
        Evaluation 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/liu2019strong/">A Strong and Robust Baseline for Text-Image Matching</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Strong and Robust Baseline for Text-Image Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Strong and Robust Baseline for Text-Image Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Fangyu, Ye Rongtian</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</td>
    <td>13</td>
    <td><p>We review the current schemes of text-image matching models and propose
improvements for both training and inference. First, we empirically show
limitations of two popular loss (sum and max-margin loss) widely used in
training text-image embeddings and propose a trade-off: a kNN-margin loss which
1) utilizes information from hard negatives and 2) is robust to noise as all
\(K\)-most hardest samples are taken into account, tolerating <em>pseudo</em>
negatives and outliers. Second, we advocate the use of Inverted Softmax
(\textsc{Is}) and Cross-modal Local Scaling (\textsc{Csls}) during inference to
mitigate the so-called hubness problem in high-dimensional embedding space,
enhancing scores of all metrics by a large margin.</p>
</td>
    <td>
      
        Survey-Paper 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/liu2019cross/">Cross-modal Zero-shot Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-modal Zero-shot Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-modal Zero-shot Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Xuanwu, Li Zhao, Wang Jun, Yu Guoxian, Domeniconi Carlotta, Zhang Xiangliang</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE International Conference on Data Mining (ICDM)</td>
    <td>16</td>
    <td><p>Hashing has been widely studied for big data retrieval due to its low storage
cost and fast query speed. Zero-shot hashing (ZSH) aims to learn a hashing
model that is trained using only samples from seen categories, but can
generalize well to samples of unseen categories. ZSH generally uses category
attributes to seek a semantic embedding space to transfer knowledge from seen
categories to unseen ones. As a result, it may perform poorly when labeled data
are insufficient. ZSH methods are mainly designed for single-modality data,
which prevents their application to the widely spread multi-modal data. On the
other hand, existing cross-modal hashing solutions assume that all the
modalities share the same category labels, while in practice the labels of
different data modalities may be different. To address these issues, we propose
a general Cross-modal Zero-shot Hashing (CZHash) solution to effectively
leverage unlabeled and labeled multi-modality data with different label spaces.
CZHash first quantifies the composite similarity between instances using label
and feature information. It then defines an objective function to achieve deep
feature learning compatible with the composite similarity preserving, category
attribute space learning, and hashing coding function learning. CZHash further
introduces an alternative optimization procedure to jointly optimize these
learning objectives. Experiments on benchmark multi-modal datasets show that
CZHash significantly outperforms related representative hashing approaches both
on effectiveness and adaptability.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        Few-Shot-&-Zero-Shot 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/liu2019optimal/">Optimal Projection Guided Transfer Hashing for Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Optimal Projection Guided Transfer Hashing for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Optimal Projection Guided Transfer Hashing for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Ji, Zhang Lei</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Circuits and Systems for Video Technology</td>
    <td>31</td>
    <td><p>Recently, learning to hash has been widely studied for image retrieval thanks
to the computation and storage efficiency of binary codes. For most existing
learning to hash methods, sufficient training images are required and used to
learn precise hashing codes. However, in some real-world applications, there
are not always sufficient training images in the domain of interest. In
addition, some existing supervised approaches need a amount of labeled data,
which is an expensive process in term of time, label and human expertise. To
handle such problems, inspired by transfer learning, we propose a simple yet
effective unsupervised hashing method named Optimal Projection Guided Transfer
Hashing (GTH) where we borrow the images of other different but related domain
i.e., source domain to help learn precise hashing codes for the domain of
interest i.e., target domain. Besides, we propose to seek for the maximum
likelihood estimation (MLE) solution of the hashing functions of target and
source domains due to the domain gap. Furthermore,an alternating optimization
method is adopted to obtain the two projections of target and source domains
such that the domain hashing disparity is reduced gradually. Extensive
experiments on various benchmark databases verify that our method outperforms
many state-of-the-art learning to hash methods. The implementation details are
available at https://github.com/liuji93/GTH.</p>
</td>
    <td>
      
        Supervised 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Unsupervised 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/liu2019ranking/">Ranking-based Deep Cross-modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Ranking-based Deep Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Ranking-based Deep Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Xuanwu, Yu Guoxian, Domeniconi Carlotta, Wang Jun, Ren Yazhou, Guo Maozu</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>59</td>
    <td><p>Cross-modal hashing has been receiving increasing interests for its low
storage cost and fast query speed in multi-modal data retrievals. However, most
existing hashing methods are based on hand-crafted or raw level features of
objects, which may not be optimally compatible with the coding process.
Besides, these hashing methods are mainly designed to handle simple pairwise
similarity. The complex multilevel ranking semantic structure of instances
associated with multiple labels has not been well explored yet. In this paper,
we propose a ranking-based deep cross-modal hashing approach (RDCMH). RDCMH
firstly uses the feature and label information of data to derive a
semi-supervised semantic ranking list. Next, to expand the semantic
representation power of hand-crafted features, RDCMH integrates the semantic
ranking information into deep cross-modal hashing and jointly optimizes the
compatible parameters of deep feature representations and of hashing functions.
Experiments on real multi-modal datasets show that RDCMH outperforms other
competitive baselines and achieves the state-of-the-art performance in
cross-modal retrieval applications.</p>
</td>
    <td>
      
        Supervised 
      
        Hashing-Methods 
      
        Datasets 
      
        Memory-Efficiency 
      
        AAAI 
      
        Evaluation 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/wang2019multi/">Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Xun, Han Xintong, Huang Weilin, Dong Dengke, Scott Matthew R.</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>736</td>
    <td><p>A family of loss functions built on pair-based computation have been proposed
in the literature which provide a myriad of solutions for deep metric learning.
In this paper, we provide a general weighting framework for understanding
recent pair-based loss functions. Our contributions are three-fold: (1) we
establish a General Pair Weighting (GPW) framework, which casts the sampling
problem of deep metric learning into a unified view of pair weighting through
gradient analysis, providing a powerful tool for understanding recent
pair-based loss functions; (2) we show that with GPW, various existing
pair-based methods can be compared and discussed comprehensively, with clear
differences and key limitations identified; (3) we propose a new loss called
multi-similarity loss (MS loss) under the GPW, which is implemented in two
iterative steps (i.e., mining and weighting). This allows it to fully consider
three similarities for pair weighting, providing a more principled approach for
collecting and weighting informative pairs. Finally, the proposed MS loss
obtains new state-of-the-art performance on four image retrieval benchmarks,
where it outperforms the most recent approaches, such as
ABE\cite{Kim_2018_ECCV} and HTL by a large margin: 60.6% to 65.7% on CUB200,
and 80.9% to 88.0% on In-Shop Clothes Retrieval dataset at Recall@1. Code is
available at https://github.com/MalongTech/research-ms-loss.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        CVPR 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/liu2018adversarial/">Adversarial Binary Coding for Efficient Person Re-identification</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Adversarial Binary Coding for Efficient Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Adversarial Binary Coding for Efficient Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Zheng, Qin Jie, Li Annan, Wang Yunhong, van Gool Luc</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE International Conference on Multimedia and Expo (ICME)</td>
    <td>36</td>
    <td><p>Person re-identification (ReID) aims at matching persons across different
views/scenes. In addition to accuracy, the matching efficiency has received
more and more attention because of demanding applications using large-scale
data. Several binary coding based methods have been proposed for efficient
ReID, which either learn projections to map high-dimensional features to
compact binary codes, or directly adopt deep neural networks by simply
inserting an additional fully-connected layer with tanh-like activations.
However, the former approach requires time-consuming hand-crafted feature
extraction and complicated (discrete) optimizations; the latter lacks the
necessary discriminative information greatly due to the straightforward
activation functions. In this paper, we propose a simple yet effective
framework for efficient ReID inspired by the recent advances in adversarial
learning. Specifically, instead of learning explicit projections or adding
fully-connected mapping layers, the proposed Adversarial Binary Coding (ABC)
framework guides the extraction of binary codes implicitly and effectively. The
discriminability of the extracted codes is further enhanced by equipping the
ABC with a deep triplet network for the ReID task. More importantly, the ABC
and triplet network are simultaneously optimized in an end-to-end manner.
Extensive experiments on three large-scale ReID benchmarks demonstrate the
superiority of our approach over the state-of-the-art methods.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Compact-Codes 
      
        Robustness 
      
        Scalability 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/liu2018stochastic/">Stochastic Attraction-Repulsion Embedding for Large Scale Image Localization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Stochastic Attraction-Repulsion Embedding for Large Scale Image Localization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Stochastic Attraction-Repulsion Embedding for Large Scale Image Localization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Liu, Li Hongdong, Dai Yuchao</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</td>
    <td>96</td>
    <td><p>This paper tackles the problem of large-scale image-based localization (IBL)
where the spatial location of a query image is determined by finding out the
most similar reference images in a large database. For solving this problem, a
critical task is to learn discriminative image representation that captures
informative information relevant for localization. We propose a novel
representation learning method having higher location-discriminating power. It
provides the following contributions: 1) we represent a place (location) as a
set of exemplar images depicting the same landmarks and aim to maximize
similarities among intra-place images while minimizing similarities among
inter-place images; 2) we model a similarity measure as a probability
distribution on L_2-metric distances between intra-place and inter-place image
representations; 3) we propose a new Stochastic Attraction and Repulsion
Embedding (SARE) loss function minimizing the KL divergence between the learned
and the actual probability distributions; 4) we give theoretical comparisons
between SARE, triplet ranking and contrastive losses. It provides insights into
why SARE is better by analyzing gradients. Our SARE loss is easy to implement
and pluggable to any CNN. Experiments show that our proposed method improves
the localization performance on standard benchmarks by a large margin.
Demonstrating the broad applicability of our method, we obtained the third
place out of 209 teams in the 2018 Google Landmark Retrieval Challenge. Our
code and model are available at https://github.com/Liumouliu/deepIBL.</p>
</td>
    <td>
      
        ICCV 
      
        Distance-Metric-Learning 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/liu2019compositional/">Compositional Coding for Collaborative Filtering</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Compositional Coding for Collaborative Filtering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Compositional Coding for Collaborative Filtering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Chenghao, Lu Tao, Wang Xin, Cheng Zhiyong, Sun Jianling, Hoi Steven C. H.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>15</td>
    <td><p>Efficiency is crucial to the online recommender systems. Representing users
and items as binary vectors for Collaborative Filtering (CF) can achieve fast
user-item affinity computation in the Hamming space, in recent years, we have
witnessed an emerging research effort in exploiting binary hashing techniques
for CF methods. However, CF with binary codes naturally suffers from low
accuracy due to limited representation capability in each bit, which impedes it
from modeling complex structure of the data.
  In this work, we attempt to improve the efficiency without hurting the model
performance by utilizing both the accuracy of real-valued vectors and the
efficiency of binary codes to represent users/items. In particular, we propose
the Compositional Coding for Collaborative Filtering (CCCF) framework, which
not only gains better recommendation efficiency than the state-of-the-art
binarized CF approaches but also achieves even higher accuracy than the
real-valued CF method. Specifically, CCCF innovatively represents each
user/item with a set of binary vectors, which are associated with a sparse
real-value weight vector. Each value of the weight vector encodes the
importance of the corresponding binary vector to the user/item. The continuous
weight vectors greatly enhances the representation capability of binary codes,
and its sparsity guarantees the processing speed. Furthermore, an integer
weight approximation scheme is proposed to further accelerate the speed. Based
on the CCCF framework, we design an efficient discrete optimization algorithm
to learn its parameters. Extensive experiments on three real-world datasets
show that our method outperforms the state-of-the-art binarized CF methods
(even achieves better performance than the real-valued CF method) by a large
margin in terms of both recommendation accuracy and efficiency.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Datasets 
      
        Recommender-Systems 
      
        SIGIR 
      
        Compact-Codes 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/pratap2019efficient/">Efficient Sketching Algorithm for Sparse Binary Data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Sketching Algorithm for Sparse Binary Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient Sketching Algorithm for Sparse Binary Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Pratap Rameshwar, Bera Debajyoti, Revanuru Karthik</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE International Conference on Data Mining (ICDM)</td>
    <td>11</td>
    <td><p>Recent advancement of the WWW, IOT, social network, e-commerce, etc. have
generated a large volume of data. These datasets are mostly represented by high
dimensional and sparse datasets. Many fundamental subroutines of common data
analytic tasks such as clustering, classification, ranking, nearest neighbour
search, etc. scale poorly with the dimension of the dataset. In this work, we
address this problem and propose a sketching (alternatively, dimensionality
reduction) algorithm – \(\binsketch\) (Binary Data Sketch) – for sparse binary
datasets. \(\binsketch\) preserves the binary version of the dataset after
sketching and maintains estimates for multiple similarity measures such as
Jaccard, Cosine, Inner-Product similarities, and Hamming distance, on the same
sketch. We present a theoretical analysis of our algorithm and complement it
with extensive experimentation on several real-world datasets. We compare the
performance of our algorithm with the state-of-the-art algorithms on the task
of mean-square-error and ranking. Our proposed algorithm offers a comparable
accuracy while suggesting a significant speedup in the dimensionality reduction
time, with respect to the other candidate algorithms. Our proposal is simple,
easy to implement, and therefore can be adopted in practice.</p>
</td>
    <td>
      
        Datasets 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/plummer2018give/">Give me a hint! Navigating Image Databases using Human-in-the-loop Feedback</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Give me a hint! Navigating Image Databases using Human-in-the-loop Feedback' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Give me a hint! Navigating Image Databases using Human-in-the-loop Feedback' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Plummer Bryan A., Kiapour M. Hadi, Zheng Shuai, Piramuthu Robinson</td> <!-- 🔧 You were missing this -->
    <td>2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>15</td>
    <td><p>In this paper, we introduce an attribute-based interactive image search which
can leverage human-in-the-loop feedback to iteratively refine image search
results. We study active image search where human feedback is solicited
exclusively in visual form, without using relative attribute annotations used
by prior work which are not typically found in many datasets. In order to
optimize the image selection strategy, a deep reinforcement model is trained to
learn what images are informative rather than rely on hand-crafted measures
typically leveraged in prior work. Additionally, we extend the recently
introduced Conditional Similarity Network to incorporate global similarity in
training visual embeddings, which results in more natural transitions as the
user explores the learned similarity embeddings. Our experiments demonstrate
the effectiveness of our approach, producing compelling results on both active
image search and image attribute representation tasks.</p>
</td>
    <td>
      
        Datasets 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/peng2018deep/">Deep Reinforcement Learning for Image Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Reinforcement Learning for Image Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Reinforcement Learning for Image Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Peng Yuxin, Zhang Jian, Ye Zhaoda</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>44</td>
    <td><p>Deep hashing methods have received much attention recently, which achieve
promising results by taking advantage of the strong representation power of
deep networks. However, most existing deep hashing methods learn a whole set of
hashing functions independently, while ignore the correlations between
different hashing functions that can promote the retrieval accuracy greatly.
Inspired by the sequential decision ability of deep reinforcement learning, we
propose a new Deep Reinforcement Learning approach for Image Hashing (DRLIH).
Our proposed DRLIH approach models the hashing learning problem as a sequential
decision process, which learns each hashing function by correcting the errors
imposed by previous ones and promotes retrieval accuracy. To the best of our
knowledge, this is the first work to address hashing problem from deep
reinforcement learning perspective. The main contributions of our proposed
DRLIH approach can be summarized as follows: (1) We propose a deep
reinforcement learning hashing network. In the proposed network, we utilize
recurrent neural network (RNN) as agents to model the hashing functions, which
take actions of projecting images into binary codes sequentially, so that the
current hashing function learning can take previous hashing functions’ error
into account. (2) We propose a sequential learning strategy based on proposed
DRLIH. We define the state as a tuple of internal features of RNN’s hidden
layers and image features, which can reflect history decisions made by the
agents. We also propose an action group method to enhance the correlation of
hash functions in the same group. Experiments on three widely-used datasets
demonstrate the effectiveness of our proposed DRLIH approach.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Compact-Codes 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/qiao2019deep/">Deep Heterogeneous Hashing for Face Video Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Heterogeneous Hashing for Face Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Heterogeneous Hashing for Face Video Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Qiao Shishi, Wang Ruiping, Shan Shiguang, Chen Xilin</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>21</td>
    <td><p>Retrieving videos of a particular person with face image as a query via
hashing technique has many important applications. While face images are
typically represented as vectors in Euclidean space, characterizing face videos
with some robust set modeling techniques (e.g. covariance matrices as exploited
in this study, which reside on Riemannian manifold), has recently shown
appealing advantages. This hence results in a thorny heterogeneous spaces
matching problem. Moreover, hashing with handcrafted features as done in many
existing works is clearly inadequate to achieve desirable performance for this
task. To address such problems, we present an end-to-end Deep Heterogeneous
Hashing (DHH) method that integrates three stages including image feature
learning, video modeling, and heterogeneous hashing in a single framework, to
learn unified binary codes for both face images and videos. To tackle the key
challenge of hashing on the manifold, a well-studied Riemannian kernel mapping
is employed to project data (i.e. covariance matrices) into Euclidean space and
thus enables to embed the two heterogeneous representations into a common
Hamming space, where both intra-space discriminability and inter-space
compatibility are considered. To perform network optimization, the gradient of
the kernel mapping is innovatively derived via structured matrix
backpropagation in a theoretically principled way. Experiments on three
challenging datasets show that our method achieves quite competitive
performance compared with existing hashing methods.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Datasets 
      
        Compact-Codes 
      
        Video-Retrieval 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/patel2019self/">Self-Supervised Visual Representations for Cross-Modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Self-Supervised Visual Representations for Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Self-Supervised Visual Representations for Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Patel Yash, Gomez Lluis, Rusiñol Marçal, Karatzas Dimosthenis, Jawahar C. V.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2019 on International Conference on Multimedia Retrieval</td>
    <td>7</td>
    <td><p>Cross-modal retrieval methods have been significantly improved in last years
with the use of deep neural networks and large-scale annotated datasets such as
ImageNet and Places. However, collecting and annotating such datasets requires
a tremendous amount of human effort and, besides, their annotations are usually
limited to discrete sets of popular visual classes that may not be
representative of the richer semantics found on large-scale cross-modal
retrieval datasets. In this paper, we present a self-supervised cross-modal
retrieval framework that leverages as training data the correlations between
images and text on the entire set of Wikipedia articles. Our method consists in
training a CNN to predict: (1) the semantic context of the article in which an
image is more probable to appear as an illustration (global context), and (2)
the semantic context of its caption (local context). Our experiments
demonstrate that the proposed method is not only capable of learning
discriminative visual representations for solving vision tasks like image
classification and object detection, but that the learned representations are
better for cross-modal retrieval when compared to supervised pre-training of
the network on the ImageNet dataset.</p>
</td>
    <td>
      
        Supervised 
      
        Tools-&-Libraries 
      
        Datasets 
      
        Self-Supervised 
      
        Scalability 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/patel2019tinysearch/">TinySearch -- Semantics based Search Engine using Bert Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=TinySearch -- Semantics based Search Engine using Bert Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=TinySearch -- Semantics based Search Engine using Bert Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Patel Manish</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>7</td>
    <td><p>Existing search engines use keyword matching or tf-idf based matching to map
the query to the web-documents and rank them. They also consider other factors
such as page rank, hubs-and-authority scores, knowledge graphs to make the
results more meaningful. However, the existing search engines fail to capture
the meaning of query when it becomes large and complex. BERT, introduced by
Google in 2018, provides embeddings for words as well as sentences. In this
paper, I have developed a semantics-oriented search engine using neural
networks and BERT embeddings that can search for query and rank the documents
in the order of the most meaningful to least meaningful. The results shows
improvement over one existing search engine for complex queries for given set
of documents.</p>
</td>
    <td>
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2019</td>
    <td>
      <a href="/publications/yao2019efficient/">Efficient Discrete Supervised Hashing for Large-scale Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Discrete Supervised Hashing for Large-scale Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient Discrete Supervised Hashing for Large-scale Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yao Tao, Kong Xiangwei, Yan Lianshan, Tang Wenjing, Tian Qi</td> <!-- 🔧 You were missing this -->
    <td>Neurocomputing</td>
    <td>29</td>
    <td><p>Supervised cross-modal hashing has gained increasing research interest on
large-scale retrieval task owning to its satisfactory performance and
efficiency. However, it still has some challenging issues to be further
studied: 1) most of them fail to well preserve the semantic correlations in
hash codes because of the large heterogenous gap; 2) most of them relax the
discrete constraint on hash codes, leading to large quantization error and
consequent low performance; 3) most of them suffer from relatively high memory
cost and computational complexity during training procedure, which makes them
unscalable. In this paper, to address above issues, we propose a supervised
cross-modal hashing method based on matrix factorization dubbed Efficient
Discrete Supervised Hashing (EDSH). Specifically, collective matrix
factorization on heterogenous features and semantic embedding with class labels
are seamlessly integrated to learn hash codes. Therefore, the feature based
similarities and semantic correlations can be both preserved in hash codes,
which makes the learned hash codes more discriminative. Then an efficient
discrete optimal algorithm is proposed to handle the scalable issue. Instead of
learning hash codes bit-by-bit, hash codes matrix can be obtained directly
which is more efficient. Extensive experimental results on three public
real-world datasets demonstrate that EDSH produces a superior performance in
both accuracy and scalability over some existing cross-modal hashing methods.</p>
</td>
    <td>
      
        Supervised 
      
        Hashing-Methods 
      
        Datasets 
      
        Quantization 
      
        Neural-Hashing 
      
        Scalability 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/hsu2018unsupervised/">Unsupervised Multimodal Representation Learning across Medical Images and Reports</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Multimodal Representation Learning across Medical Images and Reports' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Multimodal Representation Learning across Medical Images and Reports' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hsu Tzu-ming Harry, Weng Wei-hung, Boag Willie, Mcdermott Matthew, Szolovits Peter</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>26</td>
    <td><p>Joint embeddings between medical imaging modalities and associated radiology
reports have the potential to offer significant benefits to the clinical
community, ranging from cross-domain retrieval to conditional generation of
reports to the broader goals of multimodal representation learning. In this
work, we establish baseline joint embedding results measured via both local and
global retrieval methods on the soon to be released MIMIC-CXR dataset
consisting of both chest X-ray images and the associated radiology reports. We
examine both supervised and unsupervised methods on this task and show that for
document retrieval tasks with the learned representations, only a limited
amount of supervision is needed to yield results comparable to those of
fully-supervised methods.</p>
</td>
    <td>
      
        Unsupervised 
      
        Datasets 
      
        Supervised 
      
        Text-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/hu2018web/">Web-Scale Responsive Visual Search at Bing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Web-Scale Responsive Visual Search at Bing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Web-Scale Responsive Visual Search at Bing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hu Houdong, Wang Yan, Yang Linjun, Komlev Pavel, Huang Li, Chen Xi, Huang Jiapei, Wu Ye, Merchant Meenaz, Sacheti Arun</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</td>
    <td>49</td>
    <td><p>In this paper, we introduce a web-scale general visual search system deployed
in Microsoft Bing. The system accommodates tens of billions of images in the
index, with thousands of features for each image, and can respond in less than
200 ms. In order to overcome the challenges in relevance, latency, and
scalability in such large scale of data, we employ a cascaded learning-to-rank
framework based on various latest deep learning visual features, and deploy in
a distributed heterogeneous computing platform. Quantitative and qualitative
experiments show that our system is able to support various applications on
Bing website and apps.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
        KDD 
      
        Scalability 
      
        Large-Scale-Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/sharma2018improving/">Improving Similarity Search with High-dimensional Locality-sensitive Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Improving Similarity Search with High-dimensional Locality-sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Improving Similarity Search with High-dimensional Locality-sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sharma Jaiyam, Navlakha Saket</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>6</td>
    <td><p>We propose a new class of data-independent locality-sensitive hashing (LSH)
algorithms based on the fruit fly olfactory circuit. The fundamental difference
of this approach is that, instead of assigning hashes as dense points in a low
dimensional space, hashes are assigned in a high dimensional space, which
enhances their separability. We show theoretically and empirically that this
new family of hash functions is locality-sensitive and preserves rank
similarity for inputs in any `p space. We then analyze different variations on
this strategy and show empirically that they outperform existing LSH methods
for nearest-neighbors search on six benchmark datasets. Finally, we propose a
multi-probe version of our algorithm that achieves higher performance for the
same query time, or conversely, that maintains performance of prior approaches
while taking significantly less indexing time and memory. Overall, our approach
leverages the advantages of separability provided by high-dimensional spaces,
while still remaining computationally efficient</p>
</td>
    <td>
      
        Similarity-Search 
      
        Locality-Sensitive-Hashing 
      
        Hashing-Methods 
      
        Datasets 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/long2025deep/">Deep Domain Adaptation Hashing with Adversarial Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Domain Adaptation Hashing with Adversarial Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Domain Adaptation Hashing with Adversarial Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Long Fuchen, Yao, Dai, Tian, Luo, Mei</td> <!-- 🔧 You were missing this -->
    <td>The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</td>
    <td>22</td>
    <td><p>The recent advances in deep neural networks have demonstrated high capability in a wide variety of scenarios. Nevertheless, fine-tuning deep models in a new domain still requires a significant amount of labeled data despite expensive labeling efforts. A valid question is how to leverage the source knowledge plus unlabeled or only sparsely labeled target data for learning a new model in target domain. The core problem is to bring the source and target distributions closer in the feature space. In the paper, we facilitate this issue in an adversarial learning framework, in which a domain discriminator is devised to handle domain shift. Particularly, we explore the learning in the context of hashing problem, which has been studied extensively due to its great efficiency in gigantic data. Specifically, a novel Deep Domain Adaptation Hashing with Adversarial learning (DeDAHA) architecture is presented, which mainly consists of three components: a deep convolutional neural networks (CNN) for learning basic image/frame representation followed by an adversary stream on one hand to optimize the domain discriminator, and on the other, to interact with each domain-specific hashing stream for encoding image representation to hash codes. The whole architecture is trained end-to-end by jointly optimizing two types of losses, i.e., triplet ranking loss to preserve the relative similarity ordering in the input triplets and adversarial loss to maximally fool the domain discriminator with the learnt source and target feature distributions. Extensive experiments are conducted on three domain transfer tasks, including cross-domain digits retrieval, image to image and image to video transfers, on several benchmarks. Our DeDAHA framework achieves superior results when compared to the state-of-the-art techniques.</p>
</td>
    <td>
      
        Efficiency 
      
        Tools-&-Libraries 
      
        SIGIR 
      
        Hashing-Methods 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/he2025hashing/">Hashing as Tie-Aware Learning to Rank</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing as Tie-Aware Learning to Rank' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing as Tie-Aware Learning to Rank' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>He K., Cakir, Bargal, Sclaroff</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</td>
    <td>89</td>
    <td><p>Hashing, or learning binary embeddings of data, is frequently used in nearest neighbor retrieval. In this paper, we develop learning to rank formulations for hashing, aimed at directly optimizing ranking-based evaluation metrics such as Average Precision (AP) and Normalized Discounted Cumulative Gain (NDCG). We first observe that the integer-valued Hamming distance often leads to tied rankings, and propose to use tie-aware versions of AP and NDCG to evaluate hashing for retrieval. Then, to optimize tie-aware ranking metrics, we derive their continuous relaxations, and perform gradient-based optimization with deep neural networks. Our results establish the new state-of-the-art for image retrieval by Hamming ranking in common benchmarks.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Image-Retrieval 
      
        Evaluation 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/shen2018zero/">Zero-Shot Sketch-Image Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Zero-Shot Sketch-Image Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Zero-Shot Sketch-Image Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shen Yuming, Liu Li, Shen Fumin, Shao Ling</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</td>
    <td>155</td>
    <td><p>Recent studies show that large-scale sketch-based image retrieval (SBIR) can
be efficiently tackled by cross-modal binary representation learning methods,
where Hamming distance matching significantly speeds up the process of
similarity search. Providing training and test data subjected to a fixed set of
pre-defined categories, the cutting-edge SBIR and cross-modal hashing works
obtain acceptable retrieval performance. However, most of the existing methods
fail when the categories of query sketches have never been seen during
training. In this paper, the above problem is briefed as a novel but realistic
zero-shot SBIR hashing task. We elaborate the challenges of this special task
and accordingly propose a zero-shot sketch-image hashing (ZSIH) model. An
end-to-end three-network architecture is built, two of which are treated as the
binary encoders. The third network mitigates the sketch-image heterogeneity and
enhances the semantic relations among data by utilizing the Kronecker fusion
layer and graph convolution, respectively. As an important part of ZSIH, we
formulate a generative hashing scheme in reconstructing semantic knowledge
representations for zero-shot retrieval. To the best of our knowledge, ZSIH is
the first zero-shot hashing work suitable for SBIR and cross-modal search.
Comprehensive experiments are conducted on two extended datasets, i.e., Sketchy
and TU-Berlin with a novel zero-shot train-test split. The proposed model
remarkably outperforms related works.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Few-Shot-&-Zero-Shot 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        CVPR 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/he2017hashing/">Hashing as Tie-Aware Learning to Rank</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing as Tie-Aware Learning to Rank' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing as Tie-Aware Learning to Rank' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>He Kun, Cakir Fatih, Bargal Sarah Adel, Sclaroff Stan</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</td>
    <td>89</td>
    <td><p>Hashing, or learning binary embeddings of data, is frequently used in nearest
neighbor retrieval. In this paper, we develop learning to rank formulations for
hashing, aimed at directly optimizing ranking-based evaluation metrics such as
Average Precision (AP) and Normalized Discounted Cumulative Gain (NDCG). We
first observe that the integer-valued Hamming distance often leads to tied
rankings, and propose to use tie-aware versions of AP and NDCG to evaluate
hashing for retrieval. Then, to optimize tie-aware ranking metrics, we derive
their continuous relaxations, and perform gradient-based optimization with deep
neural networks. Our results establish the new state-of-the-art for image
retrieval by Hamming ranking in common benchmarks.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        CVPR 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/he2018hashing/">Hashing as Tie-Aware Learning to Rank</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing as Tie-Aware Learning to Rank' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing as Tie-Aware Learning to Rank' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>He K., Cakir, Bargal, Sclaroff</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</td>
    <td>89</td>
    <td><p>Hashing, or learning binary embeddings of data, is frequently used in nearest neighbor retrieval. In this paper, we develop learning to rank formulations for hashing, aimed at directly optimizing ranking-based evaluation metrics such as Average Precision (AP) and Normalized Discounted Cumulative Gain (NDCG). We first observe that the integer-valued Hamming distance often leads to tied rankings, and propose to use tie-aware versions of AP and NDCG to evaluate hashing for retrieval. Then, to optimize tie-aware ranking metrics, we derive their continuous relaxations, and perform gradient-based optimization with deep neural networks. Our results establish the new state-of-the-art for image retrieval by Hamming ranking in common benchmarks.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Image-Retrieval 
      
        Evaluation 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/ying2018graph/">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Graph Convolutional Neural Networks for Web-Scale Recommender Systems' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Graph Convolutional Neural Networks for Web-Scale Recommender Systems' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ying Rex, He Ruining, Chen Kaifeng, Eksombatchai Pong, Hamilton William L., Leskovec Jure</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</td>
    <td>3167</td>
    <td><p>Recent advancements in deep neural networks for graph-structured data have
led to state-of-the-art performance on recommender system benchmarks. However,
making these methods practical and scalable to web-scale recommendation tasks
with billions of items and hundreds of millions of users remains a challenge.
Here we describe a large-scale deep recommendation engine that we developed and
deployed at Pinterest. We develop a data-efficient Graph Convolutional Network
(GCN) algorithm PinSage, which combines efficient random walks and graph
convolutions to generate embeddings of nodes (i.e., items) that incorporate
both graph structure as well as node feature information. Compared to prior GCN
approaches, we develop a novel method based on highly efficient random walks to
structure the convolutions and design a novel training strategy that relies on
harder-and-harder training examples to improve robustness and convergence of
the model. We also develop an efficient MapReduce model inference algorithm to
generate embeddings using a trained model. We deploy PinSage at Pinterest and
train it on 7.5 billion examples on a graph with 3 billion nodes representing
pins and boards, and 18 billion edges. According to offline metrics, user
studies and A/B tests, PinSage generates higher-quality recommendations than
comparable deep learning and graph-based alternatives. To our knowledge, this
is the largest application of deep graph embeddings to date and paves the way
for a new generation of web-scale recommender systems based on graph
convolutional architectures.</p>
</td>
    <td>
      
        Graph-Based-ANN 
      
        Recommender-Systems 
      
        KDD 
      
        Scalability 
      
        Large-Scale-Search 
      
        Evaluation 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/shan2018recurrent/">Recurrent Binary Embedding for GPU-Enabled Exhaustive Retrieval from Billion-Scale Semantic Vectors</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Recurrent Binary Embedding for GPU-Enabled Exhaustive Retrieval from Billion-Scale Semantic Vectors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Recurrent Binary Embedding for GPU-Enabled Exhaustive Retrieval from Billion-Scale Semantic Vectors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shan Ying, Jiao Jian, Zhu Jie, Mao Jc</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</td>
    <td>10</td>
    <td><p>Rapid advances in GPU hardware and multiple areas of Deep Learning open up a
new opportunity for billion-scale information retrieval with exhaustive search.
Building on top of the powerful concept of semantic learning, this paper
proposes a Recurrent Binary Embedding (RBE) model that learns compact
representations for real-time retrieval. The model has the unique ability to
refine a base binary vector by progressively adding binary residual vectors to
meet the desired accuracy. The refined vector enables efficient implementation
of exhaustive similarity computation with bit-wise operations, followed by a
near- lossless k-NN selection algorithm, also proposed in this paper. The
proposed algorithms are integrated into an end-to-end multi-GPU system that
retrieves thousands of top items from over a billion candidates in real-time.
The RBE model and the retrieval system were evaluated with data from a major
paid search engine. When measured against the state-of-the-art model for binary
representation and the full precision model for semantic embedding, RBE
significantly outperformed the former, and filled in over 80% of the AUC gap
in-between. Experiments comparing with our production retrieval system also
demonstrated superior performance. While the primary focus of this paper is to
build RBE based on a particular class of semantic models, generalizing to other
types is straightforward, as exemplified by two different models at the end of
the paper.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        KDD 
      
        Scalability 
      
        Large-Scale-Search 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/he2018local/">Local Descriptors Optimized for Average Precision</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Local Descriptors Optimized for Average Precision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Local Descriptors Optimized for Average Precision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>He Kun, Lu Yan, Sclaroff Stan</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</td>
    <td>209</td>
    <td><p>Extraction of local feature descriptors is a vital stage in the solution
pipelines for numerous computer vision tasks. Learning-based approaches improve
performance in certain tasks, but still cannot replace handcrafted features in
general. In this paper, we improve the learning of local feature descriptors by
optimizing the performance of descriptor matching, which is a common stage that
follows descriptor extraction in local feature based pipelines, and can be
formulated as nearest neighbor retrieval. Specifically, we directly optimize a
ranking-based retrieval performance metric, Average Precision, using deep
neural networks. This general-purpose solution can also be viewed as a listwise
learning to rank approach, which is advantageous compared to recent local
ranking approaches. On standard benchmarks, descriptors learned with our
formulation achieve state-of-the-art results in patch verification, patch
retrieval, and image matching.</p>
</td>
    <td>
      
        CVPR 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/g%C3%B3mez2018single/">Single Shot Scene Text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Single Shot Scene Text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Single Shot Scene Text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gómez Lluís, Mafla Andrés, Rusiñol Marçal, Karatzas Dimosthenis</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>54</td>
    <td><p>Textual information found in scene images provides high level semantic
information about the image and its context and it can be leveraged for better
scene understanding. In this paper we address the problem of scene text
retrieval: given a text query, the system must return all images containing the
queried text. The novelty of the proposed model consists in the usage of a
single shot CNN architecture that predicts at the same time bounding boxes and
a compact text representation of the words in them. In this way, the text based
image retrieval task can be casted as a simple nearest neighbor search of the
query text representation over the outputs of the CNN over the entire image
database. Our experiments demonstrate that the proposed architecture
outperforms previous state-of-the-art while it offers a significant increase in
processing speed.</p>
</td>
    <td>
      
        Text-Retrieval 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/wang2017learning/">Learning Two-Branch Neural Networks for Image-Text Matching Tasks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Two-Branch Neural Networks for Image-Text Matching Tasks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Two-Branch Neural Networks for Image-Text Matching Tasks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Liwei, Li Yin, Huang Jing, Lazebnik Svetlana</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>529</td>
    <td><p>Image-language matching tasks have recently attracted a lot of attention in
the computer vision field. These tasks include image-sentence matching, i.e.,
given an image query, retrieving relevant sentences and vice versa, and
region-phrase matching or visual grounding, i.e., matching a phrase to relevant
regions. This paper investigates two-branch neural networks for learning the
similarity between these two data modalities. We propose two network structures
that produce different output representations. The first one, referred to as an
embedding network, learns an explicit shared latent embedding space with a
maximum-margin ranking loss and novel neighborhood constraints. Compared to
standard triplet sampling, we perform improved neighborhood sampling that takes
neighborhood information into consideration while constructing mini-batches.
The second network structure, referred to as a similarity network, fuses the
two branches via element-wise product and is trained with regression loss to
directly predict a similarity score. Extensive experiments show that our
networks achieve high accuracies for phrase localization on the Flickr30K
Entities dataset and for bi-directional image-sentence retrieval on Flickr30K
and MSCOCO datasets.</p>
</td>
    <td>
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/long2018deep/">Deep Domain Adaptation Hashing with Adversarial Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Domain Adaptation Hashing with Adversarial Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Domain Adaptation Hashing with Adversarial Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Long Fuchen, Yao, Dai, Tian, Luo, Mei</td> <!-- 🔧 You were missing this -->
    <td>The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</td>
    <td>22</td>
    <td><p>The recent advances in deep neural networks have demonstrated high capability in a wide variety of scenarios. Nevertheless, fine-tuning deep models in a new domain still requires a significant amount of labeled data despite expensive labeling efforts. A valid question is how to leverage the source knowledge plus unlabeled or only sparsely labeled target data for learning a new model in target domain. The core problem is to bring the source and target distributions closer in the feature space. In the paper, we facilitate this issue in an adversarial learning framework, in which a domain discriminator is devised to handle domain shift. Particularly, we explore the learning in the context of hashing problem, which has been studied extensively due to its great efficiency in gigantic data. Specifically, a novel Deep Domain Adaptation Hashing with Adversarial learning (DeDAHA) architecture is presented, which mainly consists of three components: a deep convolutional neural networks (CNN) for learning basic image/frame representation followed by an adversary stream on one hand to optimize the domain discriminator, and on the other, to interact with each domain-specific hashing stream for encoding image representation to hash codes. The whole architecture is trained end-to-end by jointly optimizing two types of losses, i.e., triplet ranking loss to preserve the relative similarity ordering in the input triplets and adversarial loss to maximally fool the domain discriminator with the learnt source and target feature distributions. Extensive experiments are conducted on three domain transfer tasks, including cross-domain digits retrieval, image to image and image to video transfers, on several benchmarks. Our DeDAHA framework achieves superior results when compared to the state-of-the-art techniques.</p>
</td>
    <td>
      
        Efficiency 
      
        Tools-&-Libraries 
      
        SIGIR 
      
        Hashing-Methods 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/yu2018semi/">Semi-supervised Hashing for Semi-Paired Cross-View Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semi-supervised Hashing for Semi-Paired Cross-View Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semi-supervised Hashing for Semi-Paired Cross-View Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yu Jun, Wu Xiao-jun, Kittler Josef</td> <!-- 🔧 You were missing this -->
    <td>2018 24th International Conference on Pattern Recognition (ICPR)</td>
    <td>10</td>
    <td><p>Recently, hashing techniques have gained importance in large-scale retrieval
tasks because of their retrieval speed. Most of the existing cross-view
frameworks assume that data are well paired. However, the fully-paired
multiview situation is not universal in real applications. The aim of the
method proposed in this paper is to learn the hashing function for semi-paired
cross-view retrieval tasks. To utilize the label information of partial data,
we propose a semi-supervised hashing learning framework which jointly performs
feature extraction and classifier learning. The experimental results on two
datasets show that our method outperforms several state-of-the-art methods in
terms of retrieval accuracy.</p>
</td>
    <td>
      
        Supervised 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Scalability 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/gripon2016associative/">Associative Memories to Accelerate Approximate Nearest Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Associative Memories to Accelerate Approximate Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Associative Memories to Accelerate Approximate Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gripon Vincent, Löwe Matthias, Vermet Franck</td> <!-- 🔧 You were missing this -->
    <td>Applied Sciences</td>
    <td>9</td>
    <td><p>Nearest neighbor search is a very active field in machine learning for it
appears in many application cases, including classification and object
retrieval. In its canonical version, the complexity of the search is linear
with both the dimension and the cardinal of the collection of vectors the
search is performed in. Recently many works have focused on reducing the
dimension of vectors using quantization techniques or hashing, while providing
an approximate result. In this paper we focus instead on tackling the cardinal
of the collection of vectors. Namely, we introduce a technique that partitions
the collection of vectors and stores each part in its own associative memory.
When a query vector is given to the system, associative memories are polled to
identify which one contain the closest match. Then an exhaustive search is
conducted only on the part of vectors stored in the selected associative
memory. We study the effectiveness of the system when messages to store are
generated from i.i.d. uniform \(\pm\)1 random variables or 0-1 sparse i.i.d.
random variables. We also conduct experiment on both synthetic data and real
data and show it is possible to achieve interesting trade-offs between
complexity and accuracy.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Quantization 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/gu2018attention/">Attention-Aware Generalized Mean Pooling for Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Attention-Aware Generalized Mean Pooling for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Attention-Aware Generalized Mean Pooling for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gu Yinzheng, Li Chuanpeng, Xie Jinbin</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>28</td>
    <td><p>It has been shown that image descriptors extracted by convolutional neural
networks (CNNs) achieve remarkable results for retrieval problems. In this
paper, we apply attention mechanism to CNN, which aims at enhancing more
relevant features that correspond to important keypoints in the input image.
The generated attention-aware features are then aggregated by the previous
state-of-the-art generalized mean (GeM) pooling followed by normalization to
produce a compact global descriptor, which can be efficiently compared to other
image descriptors by the dot product. An extensive comparison of our proposed
approach with state-of-the-art methods is performed on the new challenging
ROxford5k and RParis6k retrieval benchmarks. Results indicate significant
improvement over previous work. In particular, our attention-aware GeM (AGeM)
descriptor outperforms state-of-the-art method on ROxford5k under the `Hard’
evaluation protocal.</p>
</td>
    <td>
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/luo2025fast/">Fast Scalable Supervised Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Scalable Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Scalable Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Luo Xin, Nie, He, Wu, Chen, Xu</td> <!-- 🔧 You were missing this -->
    <td>The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</td>
    <td>89</td>
    <td><p>Despite significant progress in supervised hashing, there are three
common limitations of existing methods. First, most pioneer methods discretely learn hash codes bit by bit, making the learning
procedure rather time-consuming. Second, to reduce the large complexity of the n by n pairwise similarity matrix, most methods apply
sampling strategies during training, which inevitably results in information loss and suboptimal performance; some recent methods
try to replace the large matrix with a smaller one, but the size is
still large. Third, among the methods that leverage the pairwise
similarity matrix, most of them only encode the semantic label
information in learning the hash codes, failing to fully capture
the characteristics of data. In this paper, we present a novel supervised hashing method, called Fast Scalable Supervised Hashing
(FSSH), which circumvents the use of the large similarity matrix by
introducing a pre-computed intermediate term whose size is independent with the size of training data. Moreover, FSSH can learn
the hash codes with not only the semantic information but also
the features of data. Extensive experiments on three widely used
datasets demonstrate its superiority over several state-of-the-art
methods in both accuracy and scalability. Our experiment codes
are available at: https://lcbwlx.wixsite.com/fssh.</p>
</td>
    <td>
      
        Scalability 
      
        Datasets 
      
        Neural-Hashing 
      
        SIGIR 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/liu2018discriminative/">Discriminative Cross-View Binary Representation Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Discriminative Cross-View Binary Representation Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Discriminative Cross-View Binary Representation Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Liu, Qi Hairong</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>5</td>
    <td><p>Learning compact representation is vital and challenging for large scale
multimedia data. Cross-view/cross-modal hashing for effective binary
representation learning has received significant attention with exponentially
growing availability of multimedia content. Most existing cross-view hashing
algorithms emphasize the similarities in individual views, which are then
connected via cross-view similarities. In this work, we focus on the
exploitation of the discriminative information from different views, and
propose an end-to-end method to learn semantic-preserving and discriminative
binary representation, dubbed Discriminative Cross-View Hashing (DCVH), in
light of learning multitasking binary representation for various tasks
including cross-view retrieval, image-to-image retrieval, and image
annotation/tagging. The proposed DCVH has the following key components. First,
it uses convolutional neural network (CNN) based nonlinear hashing functions
and multilabel classification for both images and texts simultaneously. Such
hashing functions achieve effective continuous relaxation during training
without explicit quantization loss by using Direct Binary Embedding (DBE)
layers. Second, we propose an effective view alignment via Hamming distance
minimization, which is efficiently accomplished by bit-wise XOR operation.
Extensive experiments on two image-text benchmark datasets demonstrate that
DCVH outperforms state-of-the-art cross-view hashing algorithms as well as
single-view image hashing algorithms. In addition, DCVH can provide competitive
performance for image annotation/tagging.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Quantization 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/ma2025progressive/">Progressive Generative Hashing for Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Progressive Generative Hashing for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Progressive Generative Hashing for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ma Yuqing, He, Ding, Hu, Li, Liu</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence</td>
    <td>18</td>
    <td><p>Recent years have witnessed the success of the emerging hashing techniques in large-scale image
retrieval. Owing to the great learning capacity,
deep hashing has become one of the most promising solutions, and achieved attractive performance
in practice. However, without semantic label information, the unsupervised deep hashing still remains
an open question. In this paper, we propose a novel
progressive generative hashing (PGH) framework
to help learn a discriminative hashing network in an
unsupervised way. Different from existing studies,
it first treats the hash codes as a kind of semantic
condition for the similar image generation, and simultaneously feeds the original image and its codes
into the generative adversarial networks (GANs).
The real images together with the synthetic ones
can further help train a discriminative hashing network based on a triplet loss. By iteratively inputting
the learnt codes into the hash conditioned GANs, we can progressively enable the hashing network
to discover the semantic relations. Extensive experiments on the widely-used image datasets demonstrate that PGH can significantly outperform stateof-the-art unsupervised hashing methods.</p>
</td>
    <td>
      
        Scalability 
      
        Evaluation 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Unsupervised 
      
        AAAI 
      
        Robustness 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Neural-Hashing 
      
        IJCAI 
      
        Image-Retrieval 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/gillick2018end/">End-to-End Retrieval in Continuous Space</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=End-to-End Retrieval in Continuous Space' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=End-to-End Retrieval in Continuous Space' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gillick Daniel, Presta Alessandro, Tomar Gaurav Singh</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>81</td>
    <td><p>Most text-based information retrieval (IR) systems index objects by words or
phrases. These discrete systems have been augmented by models that use
embeddings to measure similarity in continuous space. But continuous-space
models are typically used just to re-rank the top candidates. We consider the
problem of end-to-end continuous retrieval, where standard approximate nearest
neighbor (ANN) search replaces the usual discrete inverted index, and rely
entirely on distances between learned embeddings. By training simple models
specifically for retrieval, with an appropriate model architecture, we improve
on a discrete baseline by 8% and 26% (MAP) on two similar-question retrieval
tasks. We also discuss the problem of evaluation for retrieval systems, and
show how to modify existing pairwise similarity datasets for this purpose.</p>
</td>
    <td>
      
        Datasets 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/wu2018cycle/">Cycle-Consistent Deep Generative Hashing for Cross-Modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cycle-Consistent Deep Generative Hashing for Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cycle-Consistent Deep Generative Hashing for Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu Lin, Wang Yang, Shao Ling</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>189</td>
    <td><p>In this paper, we propose a novel deep generative approach to cross-modal
retrieval to learn hash functions in the absence of paired training samples
through the cycle consistency loss. Our proposed approach employs adversarial
training scheme to lean a couple of hash functions enabling translation between
modalities while assuming the underlying semantic relationship. To induce the
hash codes with semantics to the input-output pair, cycle consistency loss is
further proposed upon the adversarial training to strengthen the correlations
between inputs and corresponding outputs. Our approach is generative to learn
hash functions such that the learned hash codes can maximally correlate each
input-output correspondence, meanwhile can also regenerate the inputs so as to
minimize the information loss. The learning to hash embedding is thus performed
to jointly optimize the parameters of the hash functions across modalities as
well as the associated generative models. Extensive experiments on a variety of
large-scale cross-modal data sets demonstrate that our proposed method achieves
better retrieval results than the state-of-the-arts.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Scalability 
      
        Multimodal-Retrieval 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/tonioni2018deep/">A deep learning pipeline for product recognition on store shelves</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A deep learning pipeline for product recognition on store shelves' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A deep learning pipeline for product recognition on store shelves' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tonioni Alessio, Serra Eugenio, di Stefano Luigi</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE International Conference on Image Processing, Applications and Systems (IPAS)</td>
    <td>43</td>
    <td><p>Recognition of grocery products in store shelves poses peculiar challenges.
Firstly, the task mandates the recognition of an extremely high number of
different items, in the order of several thousands for medium-small shops, with
many of them featuring small inter and intra class variability. Then, available
product databases usually include just one or a few studio-quality images per
product (referred to herein as reference images), whilst at test time
recognition is performed on pictures displaying a portion of a shelf containing
several products and taken in the store by cheap cameras (referred to as query
images). Moreover, as the items on sale in a store as well as their appearance
change frequently over time, a practical recognition system should handle
seamlessly new products/packages. Inspired by recent advances in object
detection and image retrieval, we propose to leverage on state of the art
object detectors based on deep learning to obtain an initial productagnostic
item detection. Then, we pursue product recognition through a similarity search
between global descriptors computed on reference and cropped query images. To
maximize performance, we learn an ad-hoc global descriptor by a CNN trained on
reference images based on an image embedding loss. Our system is
computationally expensive at training time but can perform recognition rapidly
and accurately at test time.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/shen2025unsupervised/">Unsupervised Deep Hashing with Similarity-Adaptive and Discrete Optimization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Deep Hashing with Similarity-Adaptive and Discrete Optimization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Deep Hashing with Similarity-Adaptive and Discrete Optimization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shen Fumin, Xu, Liu, Yang, Huang, Shen</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>377</td>
    <td><p>Recent vision and learning studies show that learning compact hash codes can facilitate massive data processing
with significantly reduced storage and computation. Particularly, learning deep hash functions has greatly improved the retrieval
performance, typically under the semantic supervision. In contrast, current unsupervised deep hashing algorithms can hardly achieve
satisfactory performance due to either the relaxed optimization or absence of similarity-sensitive objective. In this work, we propose a
simple yet effective unsupervised hashing framework, named Similarity-Adaptive Deep Hashing (SADH), which alternatingly proceeds
over three training modules: deep hash model training, similarity graph updating and binary code optimization. The key difference from
the widely-used two-step hashing method is that the output representations of the learned deep model help update the similarity graph
matrix, which is then used to improve the subsequent code optimization. In addition, for producing high-quality binary codes, we devise
an effective discrete optimization algorithm which can directly handle the binary constraints with a general hashing loss. Extensive
experiments validate the efficacy of SADH, which consistently outperforms the state-of-the-arts by large gaps.</p>
</td>
    <td>
      
        Neural-Hashing 
      
        Tools-&-Libraries 
      
        Supervised 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/luo2018fast/">Fast Scalable Supervised Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Scalable Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Scalable Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Luo Xin, Nie, He, Wu, Chen, Xu</td> <!-- 🔧 You were missing this -->
    <td>The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</td>
    <td>89</td>
    <td><p>Despite significant progress in supervised hashing, there are three
common limitations of existing methods. First, most pioneer methods discretely learn hash codes bit by bit, making the learning
procedure rather time-consuming. Second, to reduce the large complexity of the n by n pairwise similarity matrix, most methods apply
sampling strategies during training, which inevitably results in information loss and suboptimal performance; some recent methods
try to replace the large matrix with a smaller one, but the size is
still large. Third, among the methods that leverage the pairwise
similarity matrix, most of them only encode the semantic label
information in learning the hash codes, failing to fully capture
the characteristics of data. In this paper, we present a novel supervised hashing method, called Fast Scalable Supervised Hashing
(FSSH), which circumvents the use of the large similarity matrix by
introducing a pre-computed intermediate term whose size is independent with the size of training data. Moreover, FSSH can learn
the hash codes with not only the semantic information but also
the features of data. Extensive experiments on three widely used
datasets demonstrate its superiority over several state-of-the-art
methods in both accuracy and scalability. Our experiment codes
are available at: https://lcbwlx.wixsite.com/fssh.</p>
</td>
    <td>
      
        Scalability 
      
        Datasets 
      
        Neural-Hashing 
      
        SIGIR 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/zhai2018classification/">Classification is a Strong Baseline for Deep Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Classification is a Strong Baseline for Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Classification is a Strong Baseline for Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhai Andrew, Wu Hao-yu</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>132</td>
    <td><p>Deep metric learning aims to learn a function mapping image pixels to
embedding feature vectors that model the similarity between images. Two major
applications of metric learning are content-based image retrieval and face
verification. For the retrieval tasks, the majority of current state-of-the-art
(SOTA) approaches are triplet-based non-parametric training. For the face
verification tasks, however, recent SOTA approaches have adopted
classification-based parametric training. In this paper, we look into the
effectiveness of classification based approaches on image retrieval datasets.
We evaluate on several standard retrieval datasets such as CAR-196,
CUB-200-2011, Stanford Online Product, and In-Shop datasets for image retrieval
and clustering, and establish that our classification-based approach is
competitive across different feature dimensions and base feature networks. We
further provide insights into the performance effects of subsampling classes
for scalable classification-based training, and the effects of binarization,
enabling efficient storage and computation for practical applications.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Compact-Codes 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/shen2018unsupervised/">Unsupervised Deep Hashing with Similarity-Adaptive and Discrete Optimization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Deep Hashing with Similarity-Adaptive and Discrete Optimization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Deep Hashing with Similarity-Adaptive and Discrete Optimization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shen Fumin, Xu, Liu, Yang, Huang, Shen</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>377</td>
    <td><p>Recent vision and learning studies show that learning compact hash codes can facilitate massive data processing
with significantly reduced storage and computation. Particularly, learning deep hash functions has greatly improved the retrieval
performance, typically under the semantic supervision. In contrast, current unsupervised deep hashing algorithms can hardly achieve
satisfactory performance due to either the relaxed optimization or absence of similarity-sensitive objective. In this work, we propose a
simple yet effective unsupervised hashing framework, named Similarity-Adaptive Deep Hashing (SADH), which alternatingly proceeds
over three training modules: deep hash model training, similarity graph updating and binary code optimization. The key difference from
the widely-used two-step hashing method is that the output representations of the learned deep model help update the similarity graph
matrix, which is then used to improve the subsequent code optimization. In addition, for producing high-quality binary codes, we devise
an effective discrete optimization algorithm which can directly handle the binary constraints with a general hashing loss. Extensive
experiments validate the efficacy of SADH, which consistently outperforms the state-of-the-arts by large gaps.</p>
</td>
    <td>
      
        Neural-Hashing 
      
        Tools-&-Libraries 
      
        Supervised 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/ma2018progressive/">Progressive Generative Hashing for Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Progressive Generative Hashing for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Progressive Generative Hashing for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ma Yuqing, He, Ding, Hu, Li, Liu</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence</td>
    <td>18</td>
    <td><p>Recent years have witnessed the success of the emerging hashing techniques in large-scale image
retrieval. Owing to the great learning capacity,
deep hashing has become one of the most promising solutions, and achieved attractive performance
in practice. However, without semantic label information, the unsupervised deep hashing still remains
an open question. In this paper, we propose a novel
progressive generative hashing (PGH) framework
to help learn a discriminative hashing network in an
unsupervised way. Different from existing studies,
it first treats the hash codes as a kind of semantic
condition for the similar image generation, and simultaneously feeds the original image and its codes
into the generative adversarial networks (GANs).
The real images together with the synthetic ones
can further help train a discriminative hashing network based on a triplet loss. By iteratively inputting
the learnt codes into the hash conditioned GANs, we can progressively enable the hashing network
to discover the semantic relations. Extensive experiments on the widely-used image datasets demonstrate that PGH can significantly outperform stateof-the-art unsupervised hashing methods.</p>
</td>
    <td>
      
        Scalability 
      
        Evaluation 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Unsupervised 
      
        AAAI 
      
        Robustness 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Neural-Hashing 
      
        IJCAI 
      
        Image-Retrieval 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/pisov2018brain/">Brain Tumor Image Retrieval via Multitask Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Brain Tumor Image Retrieval via Multitask Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Brain Tumor Image Retrieval via Multitask Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Pisov Maxim, Makarchuk Gleb, Kostjuchenko Valery, Dalechina Alexandra, Golanov Andrey, Belyaev Mikhail</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>Classification-based image retrieval systems are built by training
convolutional neural networks (CNNs) on a relevant classification problem and
using the distance in the resulting feature space as a similarity metric.
However, in practical applications, it is often desirable to have
representations which take into account several aspects of the data (e.g.,
brain tumor type and its localization). In our work, we extend the
classification-based approach with multitask learning: we train a CNN on brain
MRI scans with heterogeneous labels and implement a corresponding tumor image
retrieval system. We validate our approach on brain tumor data which contains
information about tumor types, shapes and localization. We show that our method
allows us to build representations that contain more relevant information about
tumors than single-task classification-based approaches.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/ertl2018bagminhash/">BagMinHash - Minwise Hashing Algorithm for Weighted Sets</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=BagMinHash - Minwise Hashing Algorithm for Weighted Sets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=BagMinHash - Minwise Hashing Algorithm for Weighted Sets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ertl Otmar</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</td>
    <td>15</td>
    <td><p>Minwise hashing has become a standard tool to calculate signatures which
allow direct estimation of Jaccard similarities. While very efficient
algorithms already exist for the unweighted case, the calculation of signatures
for weighted sets is still a time consuming task. BagMinHash is a new algorithm
that can be orders of magnitude faster than current state of the art without
any particular restrictions or assumptions on weights or data dimensionality.
Applied to the special case of unweighted sets, it represents the first
efficient algorithm producing independent signature components. A series of
tests finally verifies the new algorithm and also reveals limitations of other
approaches published in the recent past.</p>
</td>
    <td>
      
        KDD 
      
        Hashing-Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/eghbali2016fast/">Fast Cosine Similarity Search in Binary Space with Angular Multi-index Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Cosine Similarity Search in Binary Space with Angular Multi-index Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Cosine Similarity Search in Binary Space with Angular Multi-index Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Eghbali Sepehr, Tahvildari Ladan</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Knowledge and Data Engineering</td>
    <td>15</td>
    <td><p>Given a large dataset of binary codes and a binary query point, we address
how to efficiently find \(K\) codes in the dataset that yield the largest cosine
similarities to the query. The straightforward answer to this problem is to
compare the query with all items in the dataset, but this is practical only for
small datasets. One potential solution to enhance the search time and achieve
sublinear cost is to use a hash table populated with binary codes of the
dataset and then look up the nearby buckets to the query to retrieve the
nearest neighbors. However, if codes are compared in terms of cosine similarity
rather than the Hamming distance, then the main issue is that the order of
buckets to probe is not evident. To examine this issue, we first elaborate on
the connection between the Hamming distance and the cosine similarity. Doing
this allows us to systematically find the probing sequence in the hash table.
However, solving the nearest neighbor search with a single table is only
practical for short binary codes. To address this issue, we propose the angular
multi-index hashing search algorithm which relies on building multiple hash
tables on binary code substrings. The proposed search algorithm solves the
exact angular \(K\) nearest neighbor problem in a time that is often orders of
magnitude faster than the linear scan baseline and even approximation methods.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Vector-Indexing 
      
        Hashing-Methods 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Compact-Codes 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/malkov2016efficient/">Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Malkov Yu. A., Yashunin D. A.</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>1063</td>
    <td><p>We present a new approach for the approximate K-nearest neighbor search based
on navigable small world graphs with controllable hierarchy (Hierarchical NSW,
HNSW). The proposed solution is fully graph-based, without any need for
additional search structures, which are typically used at the coarse search
stage of the most proximity graph techniques. Hierarchical NSW incrementally
builds a multi-layer structure consisting from hierarchical set of proximity
graphs (layers) for nested subsets of the stored elements. The maximum layer in
which an element is present is selected randomly with an exponentially decaying
probability distribution. This allows producing graphs similar to the
previously studied Navigable Small World (NSW) structures while additionally
having the links separated by their characteristic distance scales. Starting
search from the upper layer together with utilizing the scale separation boosts
the performance compared to NSW and allows a logarithmic complexity scaling.
Additional employment of a heuristic for selecting proximity graph neighbors
significantly increases performance at high recall and in case of highly
clustered data. Performance evaluation has demonstrated that the proposed
general metric space search index is able to strongly outperform previous
opensource state-of-the-art vector-only approaches. Similarity of the algorithm
to the skip list structure allows straightforward balanced distributed
implementation.</p>
</td>
    <td>
      
        Evaluation 
      
        Graph-Based-ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/dutta2017stochastic/">Stochastic Graphlet Embedding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Stochastic Graphlet Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Stochastic Graphlet Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dutta Anjan, Sahbi Hichem</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Neural Networks and Learning Systems</td>
    <td>19</td>
    <td><p>Graph-based methods are known to be successful in many machine learning and
pattern classification tasks. These methods consider semi-structured data as
graphs where nodes correspond to primitives (parts, interest points, segments,
etc.) and edges characterize the relationships between these primitives.
However, these non-vectorial graph data cannot be straightforwardly plugged
into off-the-shelf machine learning algorithms without a preliminary step of –
explicit/implicit – graph vectorization and embedding. This embedding process
should be resilient to intra-class graph variations while being highly
discriminant. In this paper, we propose a novel high-order stochastic graphlet
embedding (SGE) that maps graphs into vector spaces. Our main contribution
includes a new stochastic search procedure that efficiently parses a given
graph and extracts/samples unlimitedly high-order graphlets. We consider these
graphlets, with increasing orders, to model local primitives as well as their
increasingly complex interactions. In order to build our graph representation,
we measure the distribution of these graphlets into a given graph, using
particular hash functions that efficiently assign sampled graphlets into
isomorphic sets with a very low probability of collision. When combined with
maximum margin classifiers, these graphlet-based representations have positive
impact on the performance of pattern comparison and recognition as corroborated
through extensive experiments using standard benchmark databases.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Evaluation 
      
        Graph-Based-ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/manocha2017content/">Content-based Representations of audio using Siamese neural networks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Content-based Representations of audio using Siamese neural networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Content-based Representations of audio using Siamese neural networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Manocha Pranay, Badlani Rohan, Kumar Anurag, Shah Ankit, Elizalde Benjamin, Raj Bhiksha</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</td>
    <td>38</td>
    <td><p>In this paper, we focus on the problem of content-based retrieval for audio,
which aims to retrieve all semantically similar audio recordings for a given
audio clip query. This problem is similar to the problem of query by example of
audio, which aims to retrieve media samples from a database, which are similar
to the user-provided example. We propose a novel approach which encodes the
audio into a vector representation using Siamese Neural Networks. The goal is
to obtain an encoding similar for files belonging to the same audio class, thus
allowing retrieval of semantically similar audio. Using simple similarity
measures such as those based on simple euclidean distance and cosine similarity
we show that these representations can be very effectively used for retrieving
recordings similar in audio content.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        ICASSP 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/dorfer2017end/">End-to-End Cross-Modality Retrieval with CCA Projections and Pairwise Ranking Loss</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=End-to-End Cross-Modality Retrieval with CCA Projections and Pairwise Ranking Loss' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=End-to-End Cross-Modality Retrieval with CCA Projections and Pairwise Ranking Loss' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dorfer Matthias, Schlüter Jan, Vall Andreu, Korzeniowski Filip, Widmer Gerhard</td> <!-- 🔧 You were missing this -->
    <td>International Journal of Multimedia Information Retrieval</td>
    <td>32</td>
    <td><p>Cross-modality retrieval encompasses retrieval tasks where the fetched items
are of a different type than the search query, e.g., retrieving pictures
relevant to a given text query. The state-of-the-art approach to cross-modality
retrieval relies on learning a joint embedding space of the two modalities,
where items from either modality are retrieved using nearest-neighbor search.
In this work, we introduce a neural network layer based on Canonical
Correlation Analysis (CCA) that learns better embedding spaces by analytically
computing projections that maximize correlation. In contrast to previous
approaches, the CCA Layer (CCAL) allows us to combine existing objectives for
embedding space learning, such as pairwise ranking losses, with the optimal
projections of CCA. We show the effectiveness of our approach for
cross-modality retrieval on three different scenarios (text-to-image,
audio-sheet-music and zero-shot retrieval), surpassing both Deep CCA and a
multi-view network using freely learned projections optimized by a pairwise
ranking loss, especially when little training data is available (the code for
all three methods is released at: https://github.com/CPJKU/cca_layer).</p>
</td>
    <td>
      
        Text-Retrieval 
      
        Few-Shot-&-Zero-Shot 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/wang2017supervised/">Supervised Deep Hashing for Hierarchical Labeled Data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Deep Hashing for Hierarchical Labeled Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Supervised Deep Hashing for Hierarchical Labeled Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Dan, Huang Heyan, Lu Chi, Feng Bo-si, Nie Liqiang, Wen Guihua, Mao Xian-ling</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>26</td>
    <td><p>Recently, hashing methods have been widely used in large-scale image
retrieval. However, most existing hashing methods did not consider the
hierarchical relation of labels, which means that they ignored the rich
information stored in the hierarchy. Moreover, most of previous works treat
each bit in a hash code equally, which does not meet the scenario of
hierarchical labeled data. In this paper, we propose a novel deep hashing
method, called supervised hierarchical deep hashing (SHDH), to perform hash
code learning for hierarchical labeled data. Specifically, we define a novel
similarity formula for hierarchical labeled data by weighting each layer, and
design a deep convolutional neural network to obtain a hash code for each data
point. Extensive experiments on several real-world public datasets show that
the proposed method outperforms the state-of-the-art baselines in the image
retrieval task.</p>
</td>
    <td>
      
        Supervised 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        AAAI 
      
        Scalability 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/woodbridge2018detecting/">Detecting Homoglyph Attacks with a Siamese Neural Network</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Detecting Homoglyph Attacks with a Siamese Neural Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Detecting Homoglyph Attacks with a Siamese Neural Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Woodbridge Jonathan, Anderson Hyrum S., Ahuja Anjum, Grant Daniel</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE Security and Privacy Workshops (SPW)</td>
    <td>34</td>
    <td><p>A homoglyph (name spoofing) attack is a common technique used by adversaries
to obfuscate file and domain names. This technique creates process or domain
names that are visually similar to legitimate and recognized names. For
instance, an attacker may create malware with the name svch0st.exe so that in a
visual inspection of running processes or a directory listing, the process or
file name might be mistaken as the Windows system process svchost.exe. There
has been limited published research on detecting homoglyph attacks. Current
approaches rely on string comparison algorithms (such as Levenshtein distance)
that result in computationally heavy solutions with a high number of false
positives. In addition, there is a deficiency in the number of publicly
available datasets for reproducible research, with most datasets focused on
phishing attacks, in which homoglyphs are not always used. This paper presents
a fundamentally different solution to this problem using a Siamese
convolutional neural network (CNN). Rather than leveraging similarity based on
character swaps and deletions, this technique uses a learned metric on strings
rendered as images: a CNN learns features that are optimized to detect visual
similarity of the rendered strings. The trained model is used to convert
thousands of potentially targeted process or domain names to feature vectors.
These feature vectors are indexed using randomized KD-Trees to make similarity
searches extremely fast with minimal computational processing. This technique
shows a considerable 13% to 45% improvement over baseline techniques in terms
of area under the receiver operating characteristic curve (ROC AUC). In
addition, we provide both code and data to further future research.</p>
</td>
    <td>
      
        Datasets 
      
        Tree-Based-ANN 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/yu2018modeling/">Modeling Text with Graph Convolutional Network for Cross-Modal Information Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Modeling Text with Graph Convolutional Network for Cross-Modal Information Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Modeling Text with Graph Convolutional Network for Cross-Modal Information Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yu Jing, Lu Yuhang, Qin Zengchang, Liu Yanbing, Tan Jianlong, Guo Li, Zhang Weifeng</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>44</td>
    <td><p>Cross-modal information retrieval aims to find heterogeneous data of various
modalities from a given query of one modality. The main challenge is to map
different modalities into a common semantic space, in which distance between
concepts in different modalities can be well modeled. For cross-modal
information retrieval between images and texts, existing work mostly uses
off-the-shelf Convolutional Neural Network (CNN) for image feature extraction.
For texts, word-level features such as bag-of-words or word2vec are employed to
build deep learning models to represent texts. Besides word-level semantics,
the semantic relations between words are also informative but less explored. In
this paper, we model texts by graphs using similarity measure based on
word2vec. A dual-path neural network model is proposed for couple feature
learning in cross-modal information retrieval. One path utilizes Graph
Convolutional Network (GCN) for text modeling based on graph representations.
The other path uses a neural network with layers of nonlinearities for image
modeling based on off-the-shelf features. The model is trained by a pairwise
similarity loss function to maximize the similarity of relevant text-image
pairs and minimize the similarity of irrelevant pairs. Experimental results
show that the proposed model outperforms the state-of-the-art methods
significantly, with 17% improvement on accuracy for the best case.</p>
</td>
    <td>
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/magliani2018efficient/">Efficient Nearest Neighbors Search for Large-Scale Landmark Recognition</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Nearest Neighbors Search for Large-Scale Landmark Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient Nearest Neighbors Search for Large-Scale Landmark Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Magliani Federico, Fontanini Tomaso, Prati Andrea</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>8</td>
    <td><p>The problem of landmark recognition has achieved excellent results in
small-scale datasets. When dealing with large-scale retrieval, issues that were
irrelevant with small amount of data, quickly become fundamental for an
efficient retrieval phase. In particular, computational time needs to be kept
as low as possible, whilst the retrieval accuracy has to be preserved as much
as possible. In this paper we propose a novel multi-index hashing method called
Bag of Indexes (BoI) for Approximate Nearest Neighbors (ANN) search. It allows
to drastically reduce the query time and outperforms the accuracy results
compared to the state-of-the-art methods for large-scale landmark recognition.
It has been demonstrated that this family of algorithms can be applied on
different embedding techniques like VLAD and R-MAC obtaining excellent results
in very short times on different public datasets: Holidays+Flickr1M, Oxford105k
and Paris106k.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Vector-Indexing 
      
        Hashing-Methods 
      
        Datasets 
      
        Scalability 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/talreja2019using/">Using Deep Cross Modal Hashing and Error Correcting Codes for Improving the Efficiency of Attribute Guided Facial Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Using Deep Cross Modal Hashing and Error Correcting Codes for Improving the Efficiency of Attribute Guided Facial Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Using Deep Cross Modal Hashing and Error Correcting Codes for Improving the Efficiency of Attribute Guided Facial Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Talreja Veeru, Taherkhani Fariborz, Valenti Matthew C., Nasrabadi Nasser M.</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP)</td>
    <td>25</td>
    <td><p>With benefits of fast query speed and low storage cost, hashing-based image
retrieval approaches have garnered considerable attention from the research
community. In this paper, we propose a novel Error-Corrected Deep Cross Modal
Hashing (CMH-ECC) method which uses a bitmap specifying the presence of certain
facial attributes as an input query to retrieve relevant face images from the
database. In this architecture, we generate compact hash codes using an
end-to-end deep learning module, which effectively captures the inherent
relationships between the face and attribute modality. We also integrate our
deep learning module with forward error correction codes to further reduce the
distance between different modalities of the same subject. Specifically, the
properties of deep hashing and forward error correction codes are exploited to
design a cross modal hashing framework with high retrieval performance.
Experimental results using two standard datasets with facial attributes-image
modalities indicate that our CMH-ECC face image retrieval model outperforms
most of the current attribute-based face image retrieval approaches.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Memory-Efficiency 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/dey2018learning/">Learning Cross-Modal Deep Embeddings for Multi-Object Image Retrieval using Text and Sketch</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Cross-Modal Deep Embeddings for Multi-Object Image Retrieval using Text and Sketch' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Cross-Modal Deep Embeddings for Multi-Object Image Retrieval using Text and Sketch' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dey Sounak, Dutta Anjan, Ghosh Suman K., Valveny Ernest, Lladós Josep, Pal Umapada</td> <!-- 🔧 You were missing this -->
    <td>2018 24th International Conference on Pattern Recognition (ICPR)</td>
    <td>24</td>
    <td><p>In this work we introduce a cross modal image retrieval system that allows
both text and sketch as input modalities for the query. A cross-modal deep
network architecture is formulated to jointly model the sketch and text input
modalities as well as the the image output modality, learning a common
embedding between text and images and between sketches and images. In addition,
an attention model is used to selectively focus the attention on the different
objects of the image, allowing for retrieval with multiple objects in the
query. Experiments show that the proposed method performs the best in both
single and multiple object image retrieval in standard datasets.</p>
</td>
    <td>
      
        Datasets 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/deng2019triplet/">Triplet-Based Deep Hashing Network for Cross-Modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Triplet-Based Deep Hashing Network for Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Triplet-Based Deep Hashing Network for Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Deng Cheng, Chen Zhaojia, Liu Xianglong, Gao Xinbo, Tao Dacheng</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>378</td>
    <td><p>Given the benefits of its low storage requirements and high retrieval
efficiency, hashing has recently received increasing attention. In
particular,cross-modal hashing has been widely and successfully used in
multimedia similarity search applications. However, almost all existing methods
employing cross-modal hashing cannot obtain powerful hash codes due to their
ignoring the relative similarity between heterogeneous data that contains
richer semantic information, leading to unsatisfactory retrieval performance.
In this paper, we propose a triplet-based deep hashing (TDH) network for
cross-modal retrieval. First, we utilize the triplet labels, which describes
the relative relationships among three instances as supervision in order to
capture more general semantic correlations between cross-modal instances. We
then establish a loss function from the inter-modal view and the intra-modal
view to boost the discriminative abilities of the hash codes. Finally, graph
regularization is introduced into our proposed TDH method to preserve the
original semantic similarity between hash codes in Hamming space. Experimental
results show that our proposed method outperforms several state-of-the-art
approaches on two popular cross-modal datasets.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Efficiency 
      
        Evaluation 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/meyer2017deep/">Deep Metric Learning and Image Classification with Nearest Neighbour Gaussian Kernels</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Metric Learning and Image Classification with Nearest Neighbour Gaussian Kernels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Metric Learning and Image Classification with Nearest Neighbour Gaussian Kernels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Meyer Benjamin J., Harwood Ben, Drummond Tom</td> <!-- 🔧 You were missing this -->
    <td>2018 25th IEEE International Conference on Image Processing (ICIP)</td>
    <td>29</td>
    <td><p>We present a Gaussian kernel loss function and training algorithm for
convolutional neural networks that can be directly applied to both distance
metric learning and image classification problems. Our method treats all
training features from a deep neural network as Gaussian kernel centres and
computes loss by summing the influence of a feature’s nearby centres in the
feature embedding space. Our approach is made scalable by treating it as an
approximate nearest neighbour search problem. We show how to make end-to-end
learning feasible, resulting in a well formed embedding space, in which
semantically related instances are likely to be located near one another,
regardless of whether or not the network was trained on those classes. Our
approach outperforms state-of-the-art deep metric learning approaches on
embedding learning challenges, as well as conventional softmax classification
on several datasets.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Distance-Metric-Learning 
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/misra2018bernoulli/">Bernoulli Embeddings for Graphs</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Bernoulli Embeddings for Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Bernoulli Embeddings for Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Misra Vinith, Bhatia Sumit</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>9</td>
    <td><p>Just as semantic hashing can accelerate information retrieval, binary valued
embeddings can significantly reduce latency in the retrieval of graphical data.
We introduce a simple but effective model for learning such binary vectors for
nodes in a graph. By imagining the embeddings as independent coin flips of
varying bias, continuous optimization techniques can be applied to the
approximate expected loss. Embeddings optimized in this fashion consistently
outperform the quantization of both spectral graph embeddings and various
learned real-valued embeddings, on both ranking and pre-ranking tasks for a
variety of datasets.</p>
</td>
    <td>
      
        Text-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Hybrid-ANN-Methods 
      
        Re-Ranking 
      
        AAAI 
      
        Quantization 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/song2017binary/">Binary Generative Adversarial Networks for Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Binary Generative Adversarial Networks for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Binary Generative Adversarial Networks for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Song Jingkuan</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>164</td>
    <td><p>The most striking successes in image retrieval using deep hashing have mostly
involved discriminative models, which require labels. In this paper, we use
binary generative adversarial networks (BGAN) to embed images to binary codes
in an unsupervised way. By restricting the input noise variable of generative
adversarial networks (GAN) to be binary and conditioned on the features of each
input image, BGAN can simultaneously learn a binary representation per image,
and generate an image plausibly similar to the original one. In the proposed
framework, we address two main problems: 1) how to directly generate binary
codes without relaxation? 2) how to equip the binary representation with the
ability of accurate image retrieval? We resolve these problems by proposing new
sign-activation strategy and a loss function steering the learning process,
which consists of new models for adversarial loss, a content loss, and a
neighborhood structure loss. Experimental results on standard datasets
(CIFAR-10, NUSWIDE, and Flickr) demonstrate that our BGAN significantly
outperforms existing hashing methods by up to 107% in terms of~mAP (See Table
tab.res.map.comp) Our anonymous code is available at:
https://github.com/htconquer/BGAN.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        AAAI 
      
        Unsupervised 
      
        Evaluation 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/mccauley2018adaptive/">Adaptive MapReduce Similarity Joins</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Adaptive MapReduce Similarity Joins' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Adaptive MapReduce Similarity Joins' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mccauley Samuel, Silvestri Francesco</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 5th ACM SIGMOD Workshop on Algorithms and Systems for MapReduce and Beyond</td>
    <td>6</td>
    <td><p>Similarity joins are a fundamental database operation. Given data sets S and
R, the goal of a similarity join is to find all points x in S and y in R with
distance at most r. Recent research has investigated how locality-sensitive
hashing (LSH) can be used for similarity join, and in particular two recent
lines of work have made exciting progress on LSH-based join performance. Hu,
Tao, and Yi (PODS 17) investigated joins in a massively parallel setting,
showing strong results that adapt to the size of the output. Meanwhile, Ahle,
Aum"uller, and Pagh (SODA 17) showed a sequential algorithm that adapts to the
structure of the data, matching classic bounds in the worst case but improving
them significantly on more structured data. We show that this adaptive strategy
can be adapted to the parallel setting, combining the advantages of these
approaches. In particular, we show that a simple modification to Hu et al.’s
algorithm achieves bounds that depend on the density of points in the dataset
as well as the total outsize of the output. Our algorithm uses no extra
parameters over other LSH approaches (in particular, its execution does not
depend on the structure of the dataset), and is likely to be efficient in
practice.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        Evaluation 
      
        Locality-Sensitive-Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/dong2017video/">Video retrieval based on deep convolutional neural network</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Video retrieval based on deep convolutional neural network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Video retrieval based on deep convolutional neural network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dong Yj, Li Jg</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 3rd International Conference on Multimedia Systems and Signal Processing</td>
    <td>17</td>
    <td><p>Recently, with the enormous growth of online videos, fast video retrieval
research has received increasing attention. As an extension of image hashing
techniques, traditional video hashing methods mainly depend on hand-crafted
features and transform the real-valued features into binary hash codes. As
videos provide far more diverse and complex visual information than images,
extracting features from videos is much more challenging than that from images.
Therefore, high-level semantic features to represent videos are needed rather
than low-level hand-crafted methods. In this paper, a deep convolutional neural
network is proposed to extract high-level semantic features and a binary hash
function is then integrated into this framework to achieve an end-to-end
optimization. Particularly, our approach also combines triplet loss function
which preserves the relative similarity and difference of videos and
classification loss function as the optimization objective. Experiments have
been performed on two public datasets and the results demonstrate the
superiority of our proposed method compared with other state-of-the-art video
retrieval methods.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Video-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/christiani2018confirmation/">Confirmation Sampling for Exact Nearest Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Confirmation Sampling for Exact Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Confirmation Sampling for Exact Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Christiani Tobias, Pagh Rasmus, Thorup Mikkel</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>6</td>
    <td><p>Locality-sensitive hashing (LSH), introduced by Indyk and Motwani in STOC
‘98, has been an extremely influential framework for nearest neighbor search in
high-dimensional data sets. While theoretical work has focused on the
approximate nearest neighbor problems, in practice LSH data structures with
suitably chosen parameters are used to solve the exact nearest neighbor problem
(with some error probability). Sublinear query time is often possible in
practice even for exact nearest neighbor search, intuitively because the
nearest neighbor tends to be significantly closer than other data points.
However, theory offers little advice on how to choose LSH parameters outside of
pre-specified worst-case settings.
  We introduce the technique of confirmation sampling for solving the exact
nearest neighbor problem using LSH. First, we give a general reduction that
transforms a sequence of data structures that each find the nearest neighbor
with a small, unknown probability, into a data structure that returns the
nearest neighbor with probability \(1-\delta\), using as few queries as possible.
Second, we present a new query algorithm for the LSH Forest data structure with
\(L\) trees that is able to return the exact nearest neighbor of a query point
within the same time bound as an LSH Forest of \(Ω(L)\) trees with internal
parameters specifically tuned to the query and data.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Locality-Sensitive-Hashing 
      
        Tools-&-Libraries 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/song2018self/">Self-Supervised Video Hashing with Hierarchical Binary Auto-encoder</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Self-Supervised Video Hashing with Hierarchical Binary Auto-encoder' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Self-Supervised Video Hashing with Hierarchical Binary Auto-encoder' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Song Jingkuan, Zhang, Li, Gao, Wang, Hong</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>165</td>
    <td><p>Existing video hash functions are built on three isolated stages: frame pooling, relaxed learning, and binarization, which have not adequately explored the temporal order of video frames in a joint binary optimization model, resulting in severe information loss. In this paper, we propose a novel unsupervised video hashing framework dubbed Self-Supervised Video Hashing (SSVH), that is able to capture the temporal nature of videos in an end-to-end learning-to-hash fashion. We specifically address two central problems: 1) how to design an encoder-decoder architecture to generate binary codes for videos; and 2) how to equip the binary codes with the ability of accurate video retrieval. We design a hierarchical binary autoencoder to model the temporal dependencies in videos with multiple granularities, and embed the videos into binary codes with less computations than the stacked architecture. Then, we encourage the binary codes to simultaneously reconstruct the visual content and neighborhood structure of the videos. Experiments on two real-world datasets (FCVID and YFCC) show that our SSVH method can significantly outperform the state-of-the-art methods and achieve the currently best performance on the task of unsupervised video retrieval.</p>
</td>
    <td>
      
        Video-Retrieval 
      
        Unsupervised 
      
        Datasets 
      
        Self-Supervised 
      
        Tools-&-Libraries 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/verma2019diversity/">Diversity in Fashion Recommendation using Semantic Parsing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Diversity in Fashion Recommendation using Semantic Parsing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Diversity in Fashion Recommendation using Semantic Parsing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Verma Sagar, Anand Sukhad, Arora Chetan, Rai Atul</td> <!-- 🔧 You were missing this -->
    <td>2018 25th IEEE International Conference on Image Processing (ICIP)</td>
    <td>25</td>
    <td><p>Developing recommendation system for fashion images is challenging due to the
inherent ambiguity associated with what criterion a user is looking at.
Suggesting multiple images where each output image is similar to the query
image on the basis of a different feature or part is one way to mitigate the
problem. Existing works for fashion recommendation have used Siamese or Triplet
network to learn features between a similar pair and a similar-dissimilar
triplet respectively. However, these methods do not provide basic information
such as, how two clothing images are similar, or which parts present in the two
images make them similar. In this paper, we propose to recommend images by
explicitly learning and exploiting part based similarity. We propose a novel
approach of learning discriminative features from weakly-supervised data by
using visual attention over the parts and a texture encoding network. We show
that the learned features surpass the state-of-the-art in retrieval task on
DeepFashion dataset. We then use the proposed model to recommend fashion images
having an explicit variation with respect to similarity of any of the parts.</p>
</td>
    <td>
      
        Datasets 
      
        Recommender-Systems 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/chen2025deep/">Deep Hashing via Discrepancy Minimization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Hashing via Discrepancy Minimization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Hashing via Discrepancy Minimization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen Zhixiang, Yuan, Lu*, Tian, Zhou</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</td>
    <td>60</td>
    <td><p>This paper presents a discrepancy minimizing model to
address the discrete optimization problem in hashing learning. The discrete optimization introduced by binary constraint is an NP-hard mixed integer programming problem.
It is usually addressed by relaxing the binary variables into
continuous variables to adapt to the gradient based learning of hashing functions, especially the training of deep
neural networks. To deal with the objective discrepancy
caused by relaxation, we transform the original binary optimization into differentiable optimization problem over hash
functions through series expansion. This transformation decouples the binary constraint and the similarity preserving
hashing function optimization. The transformed objective
is optimized in a tractable alternating optimization framework with gradual discrepancy minimization. Extensive experimental results on three benchmark datasets validate the
efficacy of the proposed discrepancy minimizing hashing.</p>
</td>
    <td>
      
        Datasets 
      
        CVPR 
      
        Neural-Hashing 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/srinivas2018merging/">Merging datasets through deep learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Merging datasets through deep learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Merging datasets through deep learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Srinivas Kavitha, Gale Abraham, Dolby Julian</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>7</td>
    <td><p>Merging datasets is a key operation for data analytics. A frequent
requirement for merging is joining across columns that have different surface
forms for the same entity (e.g., the name of a person might be represented as
“Douglas Adams” or “Adams, Douglas”). Similarly, ontology alignment can require
recognizing distinct surface forms of the same entity, especially when
ontologies are independently developed. However, data management systems are
currently limited to performing merges based on string equality, or at best
using string similarity. We propose an approach to performing merges based on
deep learning models. Our approach depends on (a) creating a deep learning
model that maps surface forms of an entity into a set of vectors such that
alternate forms for the same entity are closest in vector space, (b) indexing
these vectors using a nearest neighbors algorithm to find the forms that can be
potentially joined together. To build these models, we had to adapt techniques
from metric learning due to the characteristics of the data; specifically we
describe novel sample selection techniques and loss functions that work for
this problem. To evaluate our approach, we used Wikidata as ground truth and
built models from datasets with approximately 1.1M people’s names (200K
identities) and 130K company names (70K identities). We developed models that
allow for joins with precision@1 of .75-.81 and recall of .74-.81. We make the
models available for aligning people or companies across multiple datasets.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/zhang2016query/">Query-adaptive Image Retrieval by Deep Weighted Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Query-adaptive Image Retrieval by Deep Weighted Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Query-adaptive Image Retrieval by Deep Weighted Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Jian, Peng Yuxin</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>55</td>
    <td><p>Hashing methods have attracted much attention for large scale image
retrieval. Some deep hashing methods have achieved promising results by taking
advantage of the strong representation power of deep networks recently.
However, existing deep hashing methods treat all hash bits equally. On one
hand, a large number of images share the same distance to a query image due to
the discrete Hamming distance, which raises a critical issue of image retrieval
where fine-grained rankings are very important. On the other hand, different
hash bits actually contribute to the image retrieval differently, and treating
them equally greatly affects the retrieval accuracy of image. To address the
above two problems, we propose the query-adaptive deep weighted hashing (QaDWH)
approach, which can perform fine-grained ranking for different queries by
weighted Hamming distance. First, a novel deep hashing network is proposed to
learn the hash codes and corresponding class-wise weights jointly, so that the
learned weights can reflect the importance of different hash bits for different
image classes. Second, a query-adaptive image retrieval method is proposed,
which rapidly generates hash bit weights for different query images by fusing
its semantic probability and the learned class-wise weights. Fine-grained image
retrieval is then performed by the weighted Hamming distance, which can provide
more accurate ranking than the traditional Hamming distance. Experiments on
four widely used datasets show that the proposed approach outperforms eight
state-of-the-art hashing methods.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/wu2018local/">Local Density Estimation in High Dimensions</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Local Density Estimation in High Dimensions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Local Density Estimation in High Dimensions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu Xian, Charikar Moses, Natchu Vishnu</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>7</td>
    <td><p>An important question that arises in the study of high dimensional vector
representations learned from data is: given a set \(\mathcal{D}\) of vectors and
a query \(q\), estimate the number of points within a specified distance
threshold of \(q\). We develop two estimators, LSH Count and Multi-Probe Count
that use locality sensitive hashing to preprocess the data to accurately and
efficiently estimate the answers to such questions via importance sampling. A
key innovation is the ability to maintain a small number of hash tables via
preprocessing data structures and algorithms that sample from multiple buckets
in each hash table. We give bounds on the space requirements and sample
complexity of our schemes, and demonstrate their effectiveness in experiments
on a standard word embedding dataset.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        Locality-Sensitive-Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/tharani2018unsupervised/">Unsupervised Deep Features for Remote Sensing Image Matching via Discriminator Network</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Deep Features for Remote Sensing Image Matching via Discriminator Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Deep Features for Remote Sensing Image Matching via Discriminator Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tharani Mohbat, Khurshid Numan, Taj Murtaza</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>The advent of deep perceptual networks brought about a paradigm shift in
machine vision and image perception. Image apprehension lately carried out by
hand-crafted features in the latent space have been replaced by deep features
acquired from supervised networks for improved understanding. However, such
deep networks require strict supervision with a substantial amount of the
labeled data for authentic training process. These methods perform poorly in
domains lacking labeled data especially in case of remote sensing image
retrieval. Resolving this, we propose an unsupervised encoder-decoder feature
for remote sensing image matching (RSIM). Moreover, we replace the conventional
distance metrics with a deep discriminator network to identify the similarity
of the image pairs. To the best of our knowledge, discriminator network has
never been used before for solving RSIM problem. Results have been validated
with two publicly available benchmark remote sensing image datasets. The
technique has also been investigated for content-based remote sensing image
retrieval (CBRSIR); one of the widely used applications of RSIM. Results
demonstrate that our technique supersedes the state-of-the-art methods used for
unsupervised image matching with mean average precision (mAP) of 81%, and image
retrieval with an overall improvement in mAP score of about 12%.</p>
</td>
    <td>
      
        Supervised 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Unsupervised 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/chen2018deep/">Deep Supervised Hashing With Anchor Graph</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Supervised Hashing With Anchor Graph' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Supervised Hashing With Anchor Graph' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen Yudong, Lai, Ding, Lin, Wong</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</td>
    <td>60</td>
    <td><p>Recently, a series of deep supervised hashing methods were proposed for binary code learning. However, due to the high computation cost and the limited hardware’s memory, these methods will first select a subset from the training set, and then form a mini-batch data to update the network in each iteration. Therefore, the remaining labeled data cannot be fully utilized and the model cannot directly obtain the binary codes of the entire training set for retrieval. To address these problems, this paper proposes an interesting regularized deep model to seamlessly integrate the advantages of deep hashing and efficient binary code learning by using the anchor graph. As such, the deep features and label matrix can be jointly used to optimize the binary codes, and the network can obtain more discriminative feedback from the linear combinations of the learned bits. Moreover, we also reveal the algorithm mechanism and its computation essence. Experiments on three large-scale datasets indicate that the proposed method achieves better retrieval performance with less training time compared to previous deep hashing methods.</p>
</td>
    <td>
      
        Scalability 
      
        Datasets 
      
        CVPR 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/chen2018blazingly/">Blazingly Fast Video Object Segmentation with Pixel-Wise Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Blazingly Fast Video Object Segmentation with Pixel-Wise Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Blazingly Fast Video Object Segmentation with Pixel-Wise Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen Yuhua, Pont-tuset Jordi, Montes Alberto, van Gool Luc</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</td>
    <td>283</td>
    <td><p>This paper tackles the problem of video object segmentation, given some user
annotation which indicates the object of interest. The problem is formulated as
pixel-wise retrieval in a learned embedding space: we embed pixels of the same
object instance into the vicinity of each other, using a fully convolutional
network trained by a modified triplet loss as the embedding model. Then the
annotated pixels are set as reference and the rest of the pixels are classified
using a nearest-neighbor approach. The proposed method supports different kinds
of user input such as segmentation mask in the first frame (semi-supervised
scenario), or a sparse set of clicked points (interactive scenario). In the
semi-supervised scenario, we achieve results competitive with the state of the
art but at a fraction of computation cost (275 milliseconds per frame). In the
interactive scenario where the user is able to refine their input iteratively,
the proposed method provides instant response to each input, and reaches
comparable quality to competing methods with much less interaction.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        CVPR 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/chen2018distributed/">Distributed Collaborative Hashing and Its Applications in Ant Financial</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Distributed Collaborative Hashing and Its Applications in Ant Financial' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Distributed Collaborative Hashing and Its Applications in Ant Financial' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chen Chaochao, Liu Ziqi, Zhao Peilin, Li Longfei, Zhou Jun, Li Xiaolong</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</td>
    <td>7</td>
    <td><p>Collaborative filtering, especially latent factor model, has been popularly
used in personalized recommendation. Latent factor model aims to learn user and
item latent factors from user-item historic behaviors. To apply it into real
big data scenarios, efficiency becomes the first concern, including offline
model training efficiency and online recommendation efficiency. In this paper,
we propose a Distributed Collaborative Hashing (DCH) model which can
significantly improve both efficiencies. Specifically, we first propose a
distributed learning framework, following the state-of-the-art parameter server
paradigm, to learn the offline collaborative model. Our model can be learnt
efficiently by distributedly computing subgradients in minibatches on workers
and updating model parameters on servers asynchronously. We then adopt hashing
technique to speedup the online recommendation procedure. Recommendation can be
quickly made through exploiting lookup hash tables. We conduct thorough
experiments on two real large-scale datasets. The experimental results
demonstrate that, comparing with the classic and state-of-the-art (distributed)
latent factor models, DCH has comparable performance in terms of recommendation
accuracy but has both fast convergence speed in offline model training
procedure and realtime efficiency in online recommendation procedure.
Furthermore, the encouraging performance of DCH is also shown for several
real-world applications in Ant Financial.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Datasets 
      
        Recommender-Systems 
      
        KDD 
      
        Scalability 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/mithun2018webly/">Webly Supervised Joint Embedding for Cross-Modal Image-Text Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Webly Supervised Joint Embedding for Cross-Modal Image-Text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Webly Supervised Joint Embedding for Cross-Modal Image-Text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mithun Niluthpol Chowdhury, Panda Rameswar, Papalexakis Evangelos E., Roy-chowdhury Amit K.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 26th ACM international conference on Multimedia</td>
    <td>73</td>
    <td><p>Cross-modal retrieval between visual data and natural language description
remains a long-standing challenge in multimedia. While recent image-text
retrieval methods offer great promise by learning deep representations aligned
across modalities, most of these methods are plagued by the issue of training
with small-scale datasets covering a limited number of images with ground-truth
sentences. Moreover, it is extremely expensive to create a larger dataset by
annotating millions of images with sentences and may lead to a biased model.
Inspired by the recent success of webly supervised learning in deep neural
networks, we capitalize on readily-available web images with noisy annotations
to learn robust image-text joint representation. Specifically, our main idea is
to leverage web images and corresponding tags, along with fully annotated
datasets, in training for learning the visual-semantic joint embedding. We
propose a two-stage approach for the task that can augment a typical supervised
pair-wise ranking loss based formulation with weakly-annotated web images to
learn a more robust visual-semantic embedding. Experiments on two standard
benchmark datasets demonstrate that our method achieves a significant
performance gain in image-text retrieval compared to state-of-the-art
approaches.</p>
</td>
    <td>
      
        Supervised 
      
        Text-Retrieval 
      
        Datasets 
      
        Evaluation 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/chaidaroon2025deep/">Deep Semantic Text Hashing with Weak Supervision</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Semantic Text Hashing with Weak Supervision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Semantic Text Hashing with Weak Supervision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chaidaroon Suthee, Ebesu, Fang</td> <!-- 🔧 You were missing this -->
    <td>The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</td>
    <td>29</td>
    <td><p>With an ever increasing amount of data available on the web, fast similarity search has become the critical component for large-scale information retrieval systems. One solution is semantic hashing which designs binary codes to accelerate similarity search. Recently, deep learning has been successfully applied to the semantic hashing problem and produces high-quality compact binary codes compared to traditional methods. However, most state-of-the-art semantic hashing approaches require large amounts of hand-labeled training data which are often expensive and time consuming to collect. The cost of getting labeled data is the key bottleneck in deploying these hashing methods. Motivated by the recent success in machine learning that makes use of weak supervision, we employ unsupervised ranking methods such as BM25 to extract weak signals from training data. We further introduce two deep generative semantic hashing models to leverage weak signals for text hashing. The experimental results on four public datasets show that our models can generate high-quality binary codes without using hand-labeled training data and significantly outperform the competitive unsupervised semantic hashing baselines.</p>
</td>
    <td>
      
        Scalability 
      
        Datasets 
      
        Text-Retrieval 
      
        SIGIR 
      
        Compact-Codes 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/song2025self/">Self-Supervised Video Hashing with Hierarchical Binary Auto-encoder</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Self-Supervised Video Hashing with Hierarchical Binary Auto-encoder' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Self-Supervised Video Hashing with Hierarchical Binary Auto-encoder' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Song Jingkuan, Zhang, Li, Gao, Wang, Hong</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>165</td>
    <td><p>Existing video hash functions are built on three isolated stages: frame pooling, relaxed learning, and binarization, which have not adequately explored the temporal order of video frames in a joint binary optimization model, resulting in severe information loss. In this paper, we propose a novel unsupervised video hashing framework dubbed Self-Supervised Video Hashing (SSVH), that is able to capture the temporal nature of videos in an end-to-end learning-to-hash fashion. We specifically address two central problems: 1) how to design an encoder-decoder architecture to generate binary codes for videos; and 2) how to equip the binary codes with the ability of accurate video retrieval. We design a hierarchical binary autoencoder to model the temporal dependencies in videos with multiple granularities, and embed the videos into binary codes with less computations than the stacked architecture. Then, we encourage the binary codes to simultaneously reconstruct the visual content and neighborhood structure of the videos. Experiments on two real-world datasets (FCVID and YFCC) show that our SSVH method can significantly outperform the state-of-the-art methods and achieve the currently best performance on the task of unsupervised video retrieval.</p>
</td>
    <td>
      
        Video-Retrieval 
      
        Unsupervised 
      
        Datasets 
      
        Self-Supervised 
      
        Tools-&-Libraries 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/chaidaroon2018deep/">Deep Semantic Text Hashing with Weak Supervision</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Semantic Text Hashing with Weak Supervision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Semantic Text Hashing with Weak Supervision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chaidaroon Suthee, Ebesu, Fang</td> <!-- 🔧 You were missing this -->
    <td>The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</td>
    <td>29</td>
    <td><p>With an ever increasing amount of data available on the web, fast similarity search has become the critical component for large-scale information retrieval systems. One solution is semantic hashing which designs binary codes to accelerate similarity search. Recently, deep learning has been successfully applied to the semantic hashing problem and produces high-quality compact binary codes compared to traditional methods. However, most state-of-the-art semantic hashing approaches require large amounts of hand-labeled training data which are often expensive and time consuming to collect. The cost of getting labeled data is the key bottleneck in deploying these hashing methods. Motivated by the recent success in machine learning that makes use of weak supervision, we employ unsupervised ranking methods such as BM25 to extract weak signals from training data. We further introduce two deep generative semantic hashing models to leverage weak signals for text hashing. The experimental results on four public datasets show that our models can generate high-quality binary codes without using hand-labeled training data and significantly outperform the competitive unsupervised semantic hashing baselines.</p>
</td>
    <td>
      
        Scalability 
      
        Datasets 
      
        Text-Retrieval 
      
        SIGIR 
      
        Compact-Codes 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/mohedano2017saliency/">Saliency Weighted Convolutional Features for Instance Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Saliency Weighted Convolutional Features for Instance Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Saliency Weighted Convolutional Features for Instance Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mohedano Eva, Mcguinness Kevin, Giro-i-nieto Xavier, O'connor Noel E.</td> <!-- 🔧 You were missing this -->
    <td>2018 International Conference on Content-Based Multimedia Indexing (CBMI)</td>
    <td>37</td>
    <td><p>This work explores attention models to weight the contribution of local
convolutional representations for the instance search task. We present a
retrieval framework based on bags of local convolutional features (BLCF) that
benefits from saliency weighting to build an efficient image representation.
The use of human visual attention models (saliency) allows significant
improvements in retrieval performance without the need to conduct region
analysis or spatial verification, and without requiring any feature fine
tuning. We investigate the impact of different saliency models, finding that
higher performance on saliency benchmarks does not necessarily equate to
improved performance when used in instance search tasks. The proposed approach
outperforms the state-of-the-art on the challenging INSTRE benchmark by a large
margin, and provides similar performance on the Oxford and Paris benchmarks
compared to more complex methods that use off-the-shelf representations. The
source code used in this project is available at
https://imatge-upc.github.io/salbow/</p>
</td>
    <td>
      
        Evaluation 
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/cao2018deep/">Deep Priority Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Priority Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Priority Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cao Zhangjie, Sun Ziping, Long Mingsheng, Wang Jianmin, Yu Philip S.</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 26th ACM international conference on Multimedia</td>
    <td>33</td>
    <td><p>Deep hashing enables image retrieval by end-to-end learning of deep
representations and hash codes from training data with pairwise similarity
information. Subject to the distribution skewness underlying the similarity
information, most existing deep hashing methods may underperform for imbalanced
data due to misspecified loss functions. This paper presents Deep Priority
Hashing (DPH), an end-to-end architecture that generates compact and balanced
hash codes in a Bayesian learning framework. The main idea is to reshape the
standard cross-entropy loss for similarity-preserving learning such that it
down-weighs the loss associated to highly-confident pairs. This idea leads to a
novel priority cross-entropy loss, which prioritizes the training on uncertain
pairs over confident pairs. Also, we propose another priority quantization
loss, which prioritizes hard-to-quantize examples for generation of nearly
lossless hash codes. Extensive experiments demonstrate that DPH can generate
high-quality hash codes and yield state-of-the-art image retrieval results on
three datasets, ImageNet, NUS-WIDE, and MS-COCO.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Quantization 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/cao2017transfer/">Transfer Adversarial Hashing for Hamming Space Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Transfer Adversarial Hashing for Hamming Space Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Transfer Adversarial Hashing for Hamming Space Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cao Zhangjie, Long Mingsheng, Huang Chao, Wang Jianmin</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>15</td>
    <td><p>Hashing is widely applied to large-scale image retrieval due to the storage
and retrieval efficiency. Existing work on deep hashing assumes that the
database in the target domain is identically distributed with the training set
in the source domain. This paper relaxes this assumption to a transfer
retrieval setting, which allows the database and the training set to come from
different but relevant domains. However, the transfer retrieval setting will
introduce two technical difficulties: first, the hash model trained on the
source domain cannot work well on the target domain due to the large
distribution gap; second, the domain gap makes it difficult to concentrate the
database points to be within a small Hamming ball. As a consequence, transfer
retrieval performance within Hamming Radius 2 degrades significantly in
existing hashing methods. This paper presents Transfer Adversarial Hashing
(TAH), a new hybrid deep architecture that incorporates a pairwise
\(t\)-distribution cross-entropy loss to learn concentrated hash codes and an
adversarial network to align the data distributions between the source and
target domains. TAH can generate compact transfer hash codes for efficient
image retrieval on both source and target domains. Comprehensive experiments
validate that TAH yields state of the art Hamming space retrieval performance
on standard datasets.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Efficiency 
      
        AAAI 
      
        Scalability 
      
        Evaluation 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/nawaz2018revisiting/">Revisiting Cross Modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Revisiting Cross Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Revisiting Cross Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Nawaz Shah, Janjua Muhammad Kamran, Calefati Alessandro, Gallo Ignazio</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>This paper proposes a cross-modal retrieval system that leverages on image
and text encoding. Most multimodal architectures employ separate networks for
each modality to capture the semantic relationship between them. However, in
our work image-text encoding can achieve comparable results in terms of
cross-modal retrieval without having to use a separate network for each
modality. We show that text encodings can capture semantic relationships
between multiple modalities. In our knowledge, this work is the first of its
kind in terms of employing a single network and fused image-text embedding for
cross-modal retrieval. We evaluate our approach on two famous multimodal
datasets: MS-COCO and Flickr30K.</p>
</td>
    <td>
      
        Datasets 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/mu2018towards/">Towards Practical Visual Search Engine within Elasticsearch</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Towards Practical Visual Search Engine within Elasticsearch' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Towards Practical Visual Search Engine within Elasticsearch' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mu Cun, Zhao Jun, Yang Guang, Zhang Jing, Yan Zheng</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>11</td>
    <td><p>In this paper, we describe our end-to-end content-based image retrieval
system built upon Elasticsearch, a well-known and popular textual search
engine. As far as we know, this is the first time such a system has been
implemented in eCommerce, and our efforts have turned out to be highly
worthwhile. We end up with a novel and exciting visual search solution that is
extremely easy to be deployed, distributed, scaled and monitored in a
cost-friendly manner. Moreover, our platform is intrinsically flexible in
supporting multimodal searches, where visual and textual information can be
jointly leveraged in retrieval.
  The core idea is to encode image feature vectors into a collection of string
tokens in a way such that closer vectors will share more string tokens in
common. By doing that, we can utilize Elasticsearch to efficiently retrieve
similar images based on similarities within encoded sting tokens. As part of
the development, we propose a novel vector to string encoding method, which is
shown to substantially outperform the previous ones in terms of both precision
and latency.
  First-hand experiences in implementing this Elasticsearch-based platform are
extensively addressed, which should be valuable to practitioners also
interested in building visual search engine on top of Elasticsearch.</p>
</td>
    <td>
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/cakaloglu2018text/">Text Embeddings for Retrieval From a Large Knowledge Base</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Text Embeddings for Retrieval From a Large Knowledge Base' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Text Embeddings for Retrieval From a Large Knowledge Base' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cakaloglu Tolgahan, Szegedy Christian, Xu Xiaowei</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>6</td>
    <td><p>Text embedding representing natural language documents in a semantic vector
space can be used for document retrieval using nearest neighbor lookup. In
order to study the feasibility of neural models specialized for retrieval in a
semantically meaningful way, we suggest the use of the Stanford Question
Answering Dataset (SQuAD) in an open-domain question answering context, where
the first task is to find paragraphs useful for answering a given question.
First, we compare the quality of various text-embedding methods on the
performance of retrieval and give an extensive empirical comparison on the
performance of various non-augmented base embedding with, and without IDF
weighting. Our main results are that by training deep residual neural models,
specifically for retrieval purposes, can yield significant gains when it is
used to augment existing embeddings. We also establish that deeper models are
superior to this task. The best base baseline embeddings augmented by our
learned neural approach improves the top-1 paragraph recall of the system by
14%.</p>
</td>
    <td>
      
        Datasets 
      
        Text-Retrieval 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/tran2018device/">On-device Scalable Image-based Localization via Prioritized Cascade Search and Fast One-Many RANSAC</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=On-device Scalable Image-based Localization via Prioritized Cascade Search and Fast One-Many RANSAC' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=On-device Scalable Image-based Localization via Prioritized Cascade Search and Fast One-Many RANSAC' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tran Ngoc-trung, Tan Dang-khoa Le, Doan Anh-dzung, Do Thanh-toan, Bui Tuan-anh, Tan Mengxuan, Cheung Ngai-man</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>38</td>
    <td><p>We present the design of an entire on-device system for large-scale urban
localization using images. The proposed design integrates compact image
retrieval and 2D-3D correspondence search to estimate the location in extensive
city regions. Our design is GPS agnostic and does not require network
connection. In order to overcome the resource constraints of mobile devices, we
propose a system design that leverages the scalability advantage of image
retrieval and accuracy of 3D model-based localization. Furthermore, we propose
a new hashing-based cascade search for fast computation of 2D-3D
correspondences. In addition, we propose a new one-many RANSAC for accurate
pose estimation. The new one-many RANSAC addresses the challenge of repetitive
building structures (e.g. windows, balconies) in urban localization. Extensive
experiments demonstrate that our 2D-3D correspondence search achieves
state-of-the-art localization accuracy on multiple benchmark datasets.
Furthermore, our experiments on a large Google Street View (GSV) image dataset
show the potential of large-scale localization entirely on a typical mobile
device.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/bouma2019individual/">Individual common dolphin identification via metric embedding learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Individual common dolphin identification via metric embedding learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Individual common dolphin identification via metric embedding learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Bouma Soren, Pawley Matthew D. M., Hupman Krista, Gilman Andrew</td> <!-- 🔧 You were missing this -->
    <td>2018 International Conference on Image and Vision Computing New Zealand (IVCNZ)</td>
    <td>36</td>
    <td><p>Photo-identification (photo-id) of dolphin individuals is a commonly used
technique in ecological sciences to monitor state and health of individuals, as
well as to study the social structure and distribution of a population.
Traditional photo-id involves a laborious manual process of matching each
dolphin fin photograph captured in the field to a catalogue of known
individuals.
  We examine this problem in the context of open-set recognition and utilise a
triplet loss function to learn a compact representation of fin images in a
Euclidean embedding, where the Euclidean distance metric represents fin
similarity. We show that this compact representation can be successfully learnt
from a fairly small (in deep learning context) training set and still
generalise well to out-of-sample identities (completely new dolphin
individuals), with top-1 and top-5 test set (37 individuals) accuracy of
\(90.5\pm2\) and \(93.6\pm1\) percent. In the presence of 1200 distractors, top-1
accuracy dropped by \(12%\); however, top-5 accuracy saw only a \(2.8%\) drop</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/moulton2018maximally/">Maximally Consistent Sampling and the Jaccard Index of Probability Distributions</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Maximally Consistent Sampling and the Jaccard Index of Probability Distributions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Maximally Consistent Sampling and the Jaccard Index of Probability Distributions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Moulton Ryan, Jiang Yunjiang</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE International Conference on Data Mining (ICDM)</td>
    <td>15</td>
    <td><p>We introduce simple, efficient algorithms for computing a MinHash of a
probability distribution, suitable for both sparse and dense data, with
equivalent running times to the state of the art for both cases. The collision
probability of these algorithms is a new measure of the similarity of positive
vectors which we investigate in detail. We describe the sense in which this
collision probability is optimal for any Locality Sensitive Hash based on
sampling. We argue that this similarity measure is more useful for probability
distributions than the similarity pursued by other algorithms for weighted
MinHash, and is the natural generalization of the Jaccard index.</p>
</td>
    <td>
      
        Locality-Sensitive-Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/xie2019unsupervised/">Unsupervised User Identity Linkage via Factoid Embedding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised User Identity Linkage via Factoid Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised User Identity Linkage via Factoid Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xie Wei, Mu Xin, Lee Roy Ka-wei, Zhu Feida, Lim Ee-peng</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE International Conference on Data Mining (ICDM)</td>
    <td>27</td>
    <td><p>User identity linkage (UIL), the problem of matching user account across
multiple online social networks (OSNs), is widely studied and important to many
real-world applications. Most existing UIL solutions adopt a supervised or
semi-supervised approach which generally suffer from scarcity of labeled data.
In this paper, we propose Factoid Embedding, a novel framework that adopts an
unsupervised approach. It is designed to cope with different profile
attributes, content types and network links of different OSNs. The key idea is
that each piece of information about a user identity describes the real
identity owner, and thus distinguishes the owner from other users. We represent
such a piece of information by a factoid and model it as a triplet consisting
of user identity, predicate, and an object or another user identity. By
embedding these factoids, we learn the user identity latent representations and
link two user identities from different OSNs if they are close to each other in
the user embedding space. Our Factoid Embedding algorithm is designed such that
as we learn the embedding space, each embedded factoid is “translated” into a
motion in the user embedding space to bring similar user identities closer, and
different user identities further apart. Extensive experiments are conducted to
evaluate Factoid Embedding on two real-world OSNs data sets. The experiment
results show that Factoid Embedding outperforms the state-of-the-art methods
even without training data.</p>
</td>
    <td>
      
        Unsupervised 
      
        Supervised 
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/baranchuk2018revisiting/">Revisiting the Inverted Indices for Billion-Scale Approximate Nearest Neighbors</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Revisiting the Inverted Indices for Billion-Scale Approximate Nearest Neighbors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Revisiting the Inverted Indices for Billion-Scale Approximate Nearest Neighbors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Baranchuk Dmitry, Babenko Artem, Malkov Yury</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>67</td>
    <td><p>This work addresses the problem of billion-scale nearest neighbor search. The
state-of-the-art retrieval systems for billion-scale databases are currently
based on the inverted multi-index, the recently proposed generalization of the
inverted index structure. The multi-index provides a very fine-grained
partition of the feature space that allows extracting concise and accurate
short-lists of candidates for the search queries. In this paper, we argue that
the potential of the simple inverted index was not fully exploited in previous
works and advocate its usage both for the highly-entangled deep descriptors and
relatively disentangled SIFT descriptors. We introduce a new retrieval system
that is based on the inverted index and outperforms the multi-index by a large
margin for the same memory consumption and construction complexity. For
example, our system achieves the state-of-the-art recall rates several times
faster on the dataset of one billion deep descriptors compared to the efficient
implementation of the inverted multi-index from the FAISS library.</p>
</td>
    <td>
      
        Vector-Indexing 
      
        Tools-&-Libraries 
      
        Datasets 
      
        Scalability 
      
        Large-Scale-Search 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/sun2019part/">Part-based Multi-stream Model for Vehicle Searching</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Part-based Multi-stream Model for Vehicle Searching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Part-based Multi-stream Model for Vehicle Searching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sun Ya, Li Minxian, Lu Jianfeng</td> <!-- 🔧 You were missing this -->
    <td>2018 24th International Conference on Pattern Recognition (ICPR)</td>
    <td>10</td>
    <td><p>Due to the enormous requirement in public security and intelligent
transportation system, searching an identical vehicle has become more and more
important. Current studies usually treat vehicle as an integral object and then
train a distance metric to measure the similarity among vehicles. However,
these raw images may be exactly similar to ones with different identification
and include some pixels in background that may disturb the distance metric
learning. In this paper, we propose a novel and useful method to segment an
original vehicle image into several discriminative foreground parts, and these
parts consist of some fine grained regions that are named discriminative
patches. After that, these parts combined with the raw image are fed into the
proposed deep learning network. We can easily measure the similarity of two
vehicle images by computing the Euclidean distance of the features from FC
layer. Two main contributions of this paper are as follows. Firstly, a method
is proposed to estimate if a patch in a raw vehicle image is discriminative or
not. Secondly, a new Part-based Multi-Stream Model (PMSM) is designed and
optimized for vehicle retrieval and re-identification tasks. We evaluate the
proposed method on the VehicleID dataset, and the experimental results show
that our method can outperform the baseline.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/xie2018unsupervised/">Unsupervised User Identity Linkage via Factoid Embedding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised User Identity Linkage via Factoid Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised User Identity Linkage via Factoid Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xie Wei, Mu Xin, Lee Roy Ka-wei, Zhu Feida, Lim Ee-peng</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE International Conference on Data Mining (ICDM)</td>
    <td>27</td>
    <td><p>User identity linkage (UIL), the problem of matching user account across
multiple online social networks (OSNs), is widely studied and important to many
real-world applications. Most existing UIL solutions adopt a supervised or
semi-supervised approach which generally suffer from scarcity of labeled data.
In this paper, we propose Factoid Embedding, a novel framework that adopts an
unsupervised approach. It is designed to cope with different profile
attributes, content types and network links of different OSNs. The key idea is
that each piece of information about a user identity describes the real
identity owner, and thus distinguishes the owner from other users. We represent
such a piece of information by a factoid and model it as a triplet consisting
of user identity, predicate, and an object or another user identity. By
embedding these factoids, we learn the user identity latent representations and
link two user identities from different OSNs if they are close to each other in
the user embedding space. Our Factoid Embedding algorithm is designed such that
as we learn the embedding space, each embedded factoid is “translated” into a
motion in the user embedding space to bring similar user identities closer, and
different user identities further apart. Extensive experiments are conducted to
evaluate Factoid Embedding on two real-world OSNs data sets. The experiment
results show that Factoid Embedding outperforms the state-of-the-art methods
even without training data.</p>
</td>
    <td>
      
        Unsupervised 
      
        Supervised 
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/novotn%C3%BD2018implementation/">Implementation Notes for the Soft Cosine Measure</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Implementation Notes for the Soft Cosine Measure' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Implementation Notes for the Soft Cosine Measure' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Novotný Vít</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 27th ACM International Conference on Information and Knowledge Management</td>
    <td>27</td>
    <td><p>The standard bag-of-words vector space model (VSM) is efficient, and
ubiquitous in information retrieval, but it underestimates the similarity of
documents with the same meaning, but different terminology. To overcome this
limitation, Sidorov et al. proposed the Soft Cosine Measure (SCM) that
incorporates term similarity relations. Charlet and Damnati showed that the SCM
is highly effective in question answering (QA) systems. However, the
orthonormalization algorithm proposed by Sidorov et al. has an impractical time
complexity of \(\mathcal O(n^4)\), where n is the size of the vocabulary.
  In this paper, we prove a tighter lower worst-case time complexity bound of
\(\mathcal O(n^3)\). We also present an algorithm for computing the similarity
between documents and we show that its worst-case time complexity is \(\mathcal
O(1)\) given realistic conditions. Lastly, we describe implementation in
general-purpose vector databases such as Annoy, and Faiss and in the inverted
indices of text search engines such as Apache Lucene, and ElasticSearch. Our
results enable the deployment of the SCM in real-world information retrieval
systems.</p>
</td>
    <td>
      
        CIKM 
      
        Text-Retrieval 
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/aum%C3%BCller2017distance/">Distance-Sensitive hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Distance-Sensitive hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Distance-Sensitive hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Aumüller Martin, Christiani Tobias, Pagh Rasmus, Silvestri Francesco</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 37th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems</td>
    <td>18</td>
    <td><p>Locality-sensitive hashing (LSH) is an important tool for managing
high-dimensional noisy or uncertain data, for example in connection with data
cleaning (similarity join) and noise-robust search (similarity search).
However, for a number of problems the LSH framework is not known to yield good
solutions, and instead ad hoc solutions have been designed for particular
similarity and distance measures. For example, this is true for
output-sensitive similarity search/join, and for indexes supporting annulus
queries that aim to report a point close to a certain given distance from the
query point.
  In this paper we initiate the study of distance-sensitive hashing (DSH), a
generalization of LSH that seeks a family of hash functions such that the
probability of two points having the same hash value is a given function of the
distance between them. More precisely, given a distance space \((X,
\text{dist})\) and a “collision probability function” (CPF) \(f\colon
\mathbb{R}\rightarrow [0,1]\) we seek a distribution over pairs of functions
\((h,g)\) such that for every pair of points \(x, y \in X\) the collision
probability is \(\Pr[h(x)=g(y)] = f(\text{dist}(x,y))\). Locality-sensitive
hashing is the study of how fast a CPF can decrease as the distance grows. For
many spaces, \(f\) can be made exponentially decreasing even if we restrict
attention to the symmetric case where \(g=h\). We show that the asymmetry
achieved by having a pair of functions makes it possible to achieve CPFs that
are, for example, increasing or unimodal, and show how this leads to principled
solutions to problems not addressed by the LSH framework. This includes a novel
application to privacy-preserving distance estimation. We believe that the DSH
framework will find further applications in high-dimensional data management.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Similarity-Search 
      
        Locality-Sensitive-Hashing 
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/song2018cross/">Cross-Modal Retrieval with Implicit Concept Association</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-Modal Retrieval with Implicit Concept Association' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-Modal Retrieval with Implicit Concept Association' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Song Yale, Soleymani Mohammad</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>7</td>
    <td><p>Traditional cross-modal retrieval assumes explicit association of concepts
across modalities, where there is no ambiguity in how the concepts are linked
to each other, e.g., when we do the image search with a query “dogs”, we expect
to see dog images. In this paper, we consider a different setting for
cross-modal retrieval where data from different modalities are implicitly
linked via concepts that must be inferred by high-level reasoning; we call this
setting implicit concept association. To foster future research in this
setting, we present a new dataset containing 47K pairs of animated GIFs and
sentences crawled from the web, in which the GIFs depict physical or emotional
reactions to the scenarios described in the text (called “reaction GIFs”). We
report on a user study showing that, despite the presence of implicit concept
association, humans are able to identify video-sentence pairs with matching
concepts, suggesting the feasibility of our task. Furthermore, we propose a
novel visual-semantic embedding network based on multiple instance learning.
Unlike traditional approaches, we compute multiple embeddings from each
modality, each representing different concepts, and measure their similarity by
considering all possible combinations of visual-semantic embeddings in the
framework of multiple instance learning. We evaluate our approach on two
video-sentence datasets with explicit and implicit concept association and
report competitive results compared to existing approaches on cross-modal
retrieval.</p>
</td>
    <td>
      
        Datasets 
      
        Image-Retrieval 
      
        Tools-&-Libraries 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/pang2018deep/">Deep Feature Aggregation and Image Re-ranking with Heat Diffusion for Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Feature Aggregation and Image Re-ranking with Heat Diffusion for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Feature Aggregation and Image Re-ranking with Heat Diffusion for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Pang Shanmin, Ma Jin, Xue Jianru, Zhu Jihua, Ordonez Vicente</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>40</td>
    <td><p>Image retrieval based on deep convolutional features has demonstrated
state-of-the-art performance in popular benchmarks. In this paper, we present a
unified solution to address deep convolutional feature aggregation and image
re-ranking by simulating the dynamics of heat diffusion. A distinctive problem
in image retrieval is that repetitive or <em>bursty</em> features tend to
dominate final image representations, resulting in representations less
distinguishable. We show that by considering each deep feature as a heat
source, our unsupervised aggregation method is able to avoid
over-representation of <em>bursty</em> features. We additionally provide a
practical solution for the proposed aggregation method and further show the
efficiency of our method in experimental evaluation. Inspired by the
aforementioned deep feature aggregation method, we also propose a method to
re-rank a number of top ranked images for a given query image by considering
the query as the heat source. Finally, we extensively evaluate the proposed
approach with pre-trained and fine-tuned deep networks on common public
benchmarks and show superior performance compared to previous work.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Hybrid-ANN-Methods 
      
        Re-Ranking 
      
        Unsupervised 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/lee2018stacked/">Stacked Cross Attention for Image-Text Matching</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Stacked Cross Attention for Image-Text Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Stacked Cross Attention for Image-Text Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lee Kuang-huei, Chen Xi, Hua Gang, Hu Houdong, He Xiaodong</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>1151</td>
    <td><p>In this paper, we study the problem of image-text matching. Inferring the
latent semantic alignment between objects or other salient stuff (e.g. snow,
sky, lawn) and the corresponding words in sentences allows to capture
fine-grained interplay between vision and language, and makes image-text
matching more interpretable. Prior work either simply aggregates the similarity
of all possible pairs of regions and words without attending differentially to
more and less important words or regions, or uses a multi-step attentional
process to capture limited number of semantic alignments which is less
interpretable. In this paper, we present Stacked Cross Attention to discover
the full latent alignments using both image regions and words in a sentence as
context and infer image-text similarity. Our approach achieves the
state-of-the-art results on the MS-COCO and Flickr30K datasets. On Flickr30K,
our approach outperforms the current best methods by 22.1% relatively in text
retrieval from image query, and 18.2% relatively in image retrieval with text
query (based on Recall@1). On MS-COCO, our approach improves sentence retrieval
by 17.8% relatively and image retrieval by 16.6% relatively (based on Recall@1
using the 5K test set). Code has been made available at:
https://github.com/kuanghuei/SCAN.</p>
</td>
    <td>
      
        Datasets 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/lam2018word2bits/">Word2Bits - Quantized Word Vectors</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Word2Bits - Quantized Word Vectors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Word2Bits - Quantized Word Vectors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lam Maximilian</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>27</td>
    <td><p>Word vectors require significant amounts of memory and storage, posing issues
to resource limited devices like mobile phones and GPUs. We show that high
quality quantized word vectors using 1-2 bits per parameter can be learned by
introducing a quantization function into Word2Vec. We furthermore show that
training with the quantization function acts as a regularizer. We train word
vectors on English Wikipedia (2017) and evaluate them on standard word
similarity and analogy tasks and on question answering (SQuAD). Our quantized
word vectors not only take 8-16x less space than full precision (32 bit) word
vectors but also outperform them on word similarity tasks and question
answering.</p>
</td>
    <td>
      
        Quantization 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/lai2017improved/">Improved Search in Hamming Space using Deep Multi-Index Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Improved Search in Hamming Space using Deep Multi-Index Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Improved Search in Hamming Space using Deep Multi-Index Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lai Hanjiang, Pan Yan</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Circuits and Systems for Video Technology</td>
    <td>12</td>
    <td><p>Similarity-preserving hashing is a widely-used method for nearest neighbour
search in large-scale image retrieval tasks. There has been considerable
research on generating efficient image representation via the
deep-network-based hashing methods. However, the issue of efficient searching
in the deep representation space remains largely unsolved. To this end, we
propose a simple yet efficient deep-network-based multi-index hashing method
for simultaneously learning the powerful image representation and the efficient
searching. To achieve these two goals, we introduce the multi-index hashing
(MIH) mechanism into the proposed deep architecture, which divides the binary
codes into multiple substrings. Due to the non-uniformly distributed codes will
result in inefficiency searching, we add the two balanced constraints at
feature-level and instance-level, respectively. Extensive evaluations on
several benchmark image retrieval datasets show that the learned balanced
binary codes bring dramatic speedups and achieve comparable performance over
the existing baselines.</p>
</td>
    <td>
      
        Vector-Indexing 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Compact-Codes 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/lai2017transductive/">Transductive Zero-Shot Hashing via Coarse-to-Fine Similarity Mining</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Transductive Zero-Shot Hashing via Coarse-to-Fine Similarity Mining' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Transductive Zero-Shot Hashing via Coarse-to-Fine Similarity Mining' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lai Hanjiang, Pan Yan</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval</td>
    <td>9</td>
    <td><p>Zero-shot Hashing (ZSH) is to learn hashing models for novel/target classes
without training data, which is an important and challenging problem. Most
existing ZSH approaches exploit transfer learning via an intermediate shared
semantic representations between the seen/source classes and novel/target
classes. However, due to having disjoint, the hash functions learned from the
source dataset are biased when applied directly to the target classes. In this
paper, we study the transductive ZSH, i.e., we have unlabeled data for novel
classes. We put forward a simple yet efficient joint learning approach via
coarse-to-fine similarity mining which transfers knowledges from source data to
target data. It mainly consists of two building blocks in the proposed deep
architecture: 1) a shared two-streams network, which the first stream operates
on the source data and the second stream operates on the unlabeled data, to
learn the effective common image representations, and 2) a coarse-to-fine
module, which begins with finding the most representative images from target
classes and then further detect similarities among these images, to transfer
the similarities of the source data to the target data in a greedy fashion.
Extensive evaluation results on several benchmark datasets demonstrate that the
proposed hashing method achieves significant improvement over the
state-of-the-art methods.</p>
</td>
    <td>
      
        Evaluation 
      
        Datasets 
      
        Few-Shot-&-Zero-Shot 
      
        Hashing-Methods 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/radenovi%C4%872017fine/">Fine-tuning CNN Image Retrieval with No Human Annotation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fine-tuning CNN Image Retrieval with No Human Annotation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fine-tuning CNN Image Retrieval with No Human Annotation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Radenović Filip, Tolias Giorgos, Chum Ondřej</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>1178</td>
    <td><p>Image descriptors based on activations of Convolutional Neural Networks
(CNNs) have become dominant in image retrieval due to their discriminative
power, compactness of representation, and search efficiency. Training of CNNs,
either from scratch or fine-tuning, requires a large amount of annotated data,
where a high quality of annotation is often crucial. In this work, we propose
to fine-tune CNNs for image retrieval on a large collection of unordered images
in a fully automated manner. Reconstructed 3D models obtained by the
state-of-the-art retrieval and structure-from-motion methods guide the
selection of the training data. We show that both hard-positive and
hard-negative examples, selected by exploiting the geometry and the camera
positions available from the 3D models, enhance the performance of
particular-object retrieval. CNN descriptor whitening discriminatively learned
from the same training data outperforms commonly used PCA whitening. We propose
a novel trainable Generalized-Mean (GeM) pooling layer that generalizes max and
average pooling and show that it boosts retrieval performance. Applying the
proposed method to the VGG network achieves state-of-the-art performance on the
standard benchmarks: Oxford Buildings, Paris, and Holidays datasets.</p>
</td>
    <td>
      
        Datasets 
      
        Image-Retrieval 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/kim2018attention/">Attention-based Ensemble for Deep Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Attention-based Ensemble for Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Attention-based Ensemble for Deep Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kim Wonsik, Goyal Bhavya, Chawla Kunal, Lee Jungmin, Kwon Keunjoo</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>239</td>
    <td><p>Deep metric learning aims to learn an embedding function, modeled as deep
neural network. This embedding function usually puts semantically similar
images close while dissimilar images far from each other in the learned
embedding space. Recently, ensemble has been applied to deep metric learning to
yield state-of-the-art results. As one important aspect of ensemble, the
learners should be diverse in their feature embeddings. To this end, we propose
an attention-based ensemble, which uses multiple attention masks, so that each
learner can attend to different parts of the object. We also propose a
divergence loss, which encourages diversity among the learners. The proposed
method is applied to the standard benchmarks of deep metric learning and
experimental results show that it outperforms the state-of-the-art methods by a
significant margin on image retrieval tasks.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/sarfraz2017pose/">A Pose-Sensitive Embedding for Person Re-Identification with Expanded Cross Neighborhood Re-Ranking</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Pose-Sensitive Embedding for Person Re-Identification with Expanded Cross Neighborhood Re-Ranking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Pose-Sensitive Embedding for Person Re-Identification with Expanded Cross Neighborhood Re-Ranking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sarfraz M. Saquib, Schumann Arne, Eberle Andreas, Stiefelhagen Rainer</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</td>
    <td>521</td>
    <td><p>Person re identification is a challenging retrieval task that requires
matching a person’s acquired image across non overlapping camera views. In this
paper we propose an effective approach that incorporates both the fine and
coarse pose information of the person to learn a discriminative embedding. In
contrast to the recent direction of explicitly modeling body parts or
correcting for misalignment based on these, we show that a rather
straightforward inclusion of acquired camera view and/or the detected joint
locations into a convolutional neural network helps to learn a very effective
representation. To increase retrieval performance, re-ranking techniques based
on computed distances have recently gained much attention. We propose a new
unsupervised and automatic re-ranking framework that achieves state-of-the-art
re-ranking performance. We show that in contrast to the current
state-of-the-art re-ranking methods our approach does not require to compute
new rank lists for each image pair (e.g., based on reciprocal neighbors) and
performs well by using simple direct rank list based comparison or even by just
using the already computed euclidean distances between the images. We show that
both our learned representation and our re-ranking method achieve
state-of-the-art performance on a number of challenging surveillance image and
video datasets.
  The code is available online at:
https://github.com/pse-ecn/pose-sensitive-embedding</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        CVPR 
      
        Hybrid-ANN-Methods 
      
        Re-Ranking 
      
        Unsupervised 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/jin2018deep/">Deep Saliency Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Saliency Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Saliency Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jin Sheng, Yao Hongxun, Sun Xiaoshuai, Zhou Shangchen, Zhang Lei, Hua Xiansheng</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>38</td>
    <td><p>In recent years, hashing methods have been proved to be effective and
efficient for the large-scale Web media search. However, the existing general
hashing methods have limited discriminative power for describing fine-grained
objects that share similar overall appearance but have subtle difference. To
solve this problem, we for the first time introduce the attention mechanism to
the learning of fine-grained hashing codes. Specifically, we propose a novel
deep hashing model, named deep saliency hashing (DSaH), which automatically
mines salient regions and learns semantic-preserving hashing codes
simultaneously. DSaH is a two-step end-to-end model consisting of an attention
network and a hashing network. Our loss function contains three basic
components, including the semantic loss, the saliency loss, and the
quantization loss. As the core of DSaH, the saliency loss guides the attention
network to mine discriminative regions from pairs of images. We conduct
extensive experiments on both fine-grained and general retrieval datasets for
performance evaluation. Experimental results on fine-grained datasets,
including Oxford Flowers-17, Stanford Dogs-120, and CUB Bird demonstrate that
our DSaH performs the best for fine-grained retrieval task and beats the
strongest competitor (DTQ) by approximately 10% on both Stanford Dogs-120 and
CUB Bird. DSaH is also comparable to several state-of-the-art hashing methods
on general datasets, including CIFAR-10 and NUS-WIDE.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Scalability 
      
        Quantization 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/peng2017modality/">Modality-specific Cross-modal Similarity Measurement with Recurrent Attention Network</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Modality-specific Cross-modal Similarity Measurement with Recurrent Attention Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Modality-specific Cross-modal Similarity Measurement with Recurrent Attention Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Peng Yuxin, Qi Jinwei, Yuan Yuxin</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>141</td>
    <td><p>Nowadays, cross-modal retrieval plays an indispensable role to flexibly find
information across different modalities of data. Effectively measuring the
similarity between different modalities of data is the key of cross-modal
retrieval. Different modalities such as image and text have imbalanced and
complementary relationships, which contain unequal amount of information when
describing the same semantics. For example, images often contain more details
that cannot be demonstrated by textual descriptions and vice versa. Existing
works based on Deep Neural Network (DNN) mostly construct one common space for
different modalities to find the latent alignments between them, which lose
their exclusive modality-specific characteristics. Different from the existing
works, we propose modality-specific cross-modal similarity measurement (MCSM)
approach by constructing independent semantic space for each modality, which
adopts end-to-end framework to directly generate modality-specific cross-modal
similarity without explicit common representation. For each semantic space,
modality-specific characteristics within one modality are fully exploited by
recurrent attention network, while the data of another modality is projected
into this space with attention based joint embedding to utilize the learned
attention weights for guiding the fine-grained cross-modal correlation
learning, which can capture the imbalanced and complementary relationships
between different modalities. Finally, the complementarity between the semantic
spaces for different modalities is explored by adaptive fusion of the
modality-specific cross-modal similarities to perform cross-modal retrieval.
Experiments on the widely-used Wikipedia and Pascal Sentence datasets as well
as our constructed large-scale XMediaNet dataset verify the effectiveness of
our proposed approach, outperforming 9 state-of-the-art methods.</p>
</td>
    <td>
      
        Datasets 
      
        Scalability 
      
        Tools-&-Libraries 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/liu2019deep/">Deep Triplet Quantization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Triplet Quantization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Triplet Quantization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Bin, Cao Yue, Long Mingsheng, Wang Jianmin, Wang Jingdong</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 26th ACM international conference on Multimedia</td>
    <td>106</td>
    <td><p>Deep hashing establishes efficient and effective image retrieval by
end-to-end learning of deep representations and hash codes from similarity
data. We present a compact coding solution, focusing on deep learning to
quantization approach that has shown superior performance over hashing
solutions for similarity retrieval. We propose Deep Triplet Quantization (DTQ),
a novel approach to learning deep quantization models from the similarity
triplets. To enable more effective triplet training, we design a new triplet
selection approach, Group Hard, that randomly selects hard triplets in each
image group. To generate compact binary codes, we further apply a triplet
quantization with weak orthogonality during triplet training. The quantization
loss reduces the codebook redundancy and enhances the quantizability of deep
representations through back-propagation. Extensive experiments demonstrate
that DTQ can generate high-quality and compact binary codes, which yields
state-of-the-art image retrieval performance on three benchmark datasets,
NUS-WIDE, CIFAR-10, and MS-COCO.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Quantization 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/jiang2017asymmetric/">Asymmetric Deep Supervised Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Asymmetric Deep Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Asymmetric Deep Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jiang Qing-yuan, Li Wu-jun</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>250</td>
    <td><p>Hashing has been widely used for large-scale approximate nearest neighbor
search because of its storage and search efficiency. Recent work has found that
deep supervised hashing can significantly outperform non-deep supervised
hashing in many applications. However, most existing deep supervised hashing
methods adopt a symmetric strategy to learn one deep hash function for both
query points and database (retrieval) points. The training of these symmetric
deep supervised hashing methods is typically time-consuming, which makes them
hard to effectively utilize the supervised information for cases with
large-scale database. In this paper, we propose a novel deep supervised hashing
method, called asymmetric deep supervised hashing (ADSH), for large-scale
nearest neighbor search. ADSH treats the query points and database points in an
asymmetric way. More specifically, ADSH learns a deep hash function only for
query points, while the hash codes for database points are directly learned.
The training of ADSH is much more efficient than that of traditional symmetric
deep supervised hashing methods. Experiments show that ADSH can achieve
state-of-the-art performance in real applications.</p>
</td>
    <td>
      
        Supervised 
      
        Hashing-Methods 
      
        Neural-Hashing 
      
        AAAI 
      
        Scalability 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/zhu2018attention/">Attention-based Pyramid Aggregation Network for Visual Place Recognition</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Attention-based Pyramid Aggregation Network for Visual Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Attention-based Pyramid Aggregation Network for Visual Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhu Yingying, Wang Jiong, Xie Lingxi, Zheng Liang</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 26th ACM international conference on Multimedia</td>
    <td>89</td>
    <td><p>Visual place recognition is challenging in the urban environment and is
usually viewed as a large scale image retrieval task. The intrinsic challenges
in place recognition exist that the confusing objects such as cars and trees
frequently occur in the complex urban scene, and buildings with repetitive
structures may cause over-counting and the burstiness problem degrading the
image representations. To address these problems, we present an Attention-based
Pyramid Aggregation Network (APANet), which is trained in an end-to-end manner
for place recognition. One main component of APANet, the spatial pyramid
pooling, can effectively encode the multi-size buildings containing
geo-information. The other one, the attention block, is adopted as a region
evaluator for suppressing the confusing regional features while highlighting
the discriminative ones. When testing, we further propose a simple yet
effective PCA power whitening strategy, which significantly improves the widely
used PCA whitening by reasonably limiting the impact of over-counting.
Experimental evaluations demonstrate that the proposed APANet outperforms the
state-of-the-art methods on two place recognition benchmarks, and generalizes
well on standard image retrieval datasets.</p>
</td>
    <td>
      
        Datasets 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/jeong2018efficient/">Efficient end-to-end learning for quantizable representations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient end-to-end learning for quantizable representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient end-to-end learning for quantizable representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jeong Yeonwoo, Song Hyun Oh</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>9</td>
    <td><p>Embedding representation learning via neural networks is at the core
foundation of modern similarity based search. While much effort has been put in
developing algorithms for learning binary hamming code representations for
search efficiency, this still requires a linear scan of the entire dataset per
each query and trades off the search accuracy through binarization. To this
end, we consider the problem of directly learning a quantizable embedding
representation and the sparse binary hash code end-to-end which can be used to
construct an efficient hash table not only providing significant search
reduction in the number of data but also achieving the state of the art search
accuracy outperforming previous state of the art deep metric learning methods.
We also show that finding the optimal sparse binary hash code in a mini-batch
can be computed exactly in polynomial time by solving a minimum cost flow
problem. Our results on Cifar-100 and on ImageNet datasets show the state of
the art search accuracy in precision@k and NMI metrics while providing up to
98X and 478X search speedup respectively over exhaustive linear search. The
source code is available at
https://github.com/maestrojeong/Deep-Hash-Table-ICML18</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/jain2017learning/">Learning a Complete Image Indexing Pipeline</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning a Complete Image Indexing Pipeline' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning a Complete Image Indexing Pipeline' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jain Himalaya, Zepeda Joaquin, Pérez Patrick, Gribonval Rémi</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</td>
    <td>14</td>
    <td><p>To work at scale, a complete image indexing system comprises two components:
An inverted file index to restrict the actual search to only a subset that
should contain most of the items relevant to the query; An approximate distance
computation mechanism to rapidly scan these lists. While supervised deep
learning has recently enabled improvements to the latter, the former continues
to be based on unsupervised clustering in the literature. In this work, we
propose a first system that learns both components within a unifying neural
framework of structured binary encoding.</p>
</td>
    <td>
      
        Vector-Indexing 
      
        Supervised 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        CVPR 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/iscen2017fast/">Fast Spectral Ranking for Similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Spectral Ranking for Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Spectral Ranking for Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Iscen Ahmet, Avrithis Yannis, Tolias Giorgos, Furon Teddy, Chum Ondrej</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</td>
    <td>37</td>
    <td><p>Despite the success of deep learning on representing images for particular
object retrieval, recent studies show that the learned representations still
lie on manifolds in a high dimensional space. This makes the Euclidean nearest
neighbor search biased for this task. Exploring the manifolds online remains
expensive even if a nearest neighbor graph has been computed offline. This work
introduces an explicit embedding reducing manifold search to Euclidean search
followed by dot product similarity search. This is equivalent to linear graph
filtering of a sparse signal in the frequency domain. To speed up online
search, we compute an approximate Fourier basis of the graph offline. We
improve the state of art on particular object retrieval datasets including the
challenging Instre dataset containing small objects. At a scale of 10^5 images,
the offline cost is only a few hours, while query time is comparable to
standard similarity search.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Datasets 
      
        CVPR 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/jain2018learning/">Learning a Complete Image Indexing Pipeline</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning a Complete Image Indexing Pipeline' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning a Complete Image Indexing Pipeline' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jain Himalaya, Zepeda Joaquin, Pérez Patrick, Gribonval Rémi</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</td>
    <td>14</td>
    <td><p>To work at scale, a complete image indexing system comprises two components:
An inverted file index to restrict the actual search to only a subset that
should contain most of the items relevant to the query; An approximate distance
computation mechanism to rapidly scan these lists. While supervised deep
learning has recently enabled improvements to the latter, the former continues
to be based on unsupervised clustering in the literature. In this work, we
propose a first system that learns both components within a unifying neural
framework of structured binary encoding.</p>
</td>
    <td>
      
        Vector-Indexing 
      
        Supervised 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        CVPR 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/iwasaki2018optimization/">Optimization of Indexing Based on k-Nearest Neighbor Graph for Proximity Search in High-dimensional Data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Optimization of Indexing Based on k-Nearest Neighbor Graph for Proximity Search in High-dimensional Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Optimization of Indexing Based on k-Nearest Neighbor Graph for Proximity Search in High-dimensional Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Iwasaki Masajiro, Miyazaki Daisuke</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>39</td>
    <td><p>Searching for high-dimensional vector data with high accuracy is an
inevitable search technology for various types of data. Graph-based indexes are
known to reduce the query time for high-dimensional data. To further improve
the query time by using graphs, we focused on the indegrees and outdegrees of
graphs. While a sufficient number of incoming edges (indegrees) are
indispensable for increasing search accuracy, an excessive number of outgoing
edges (outdegrees) should be suppressed so as to not increase the query time.
Therefore, we propose three degree-adjustment methods: static degree adjustment
of not only outdegrees but also indegrees, dynamic degree adjustment with which
outdegrees are determined by the search accuracy users require, and path
adjustment to remove edges that have alternative search paths to reduce
outdegrees. We also show how to obtain optimal degree-adjustment parameters and
that our methods outperformed previous methods for image and textual data.</p>
</td>
    <td>
      
        Graph-Based-ANN 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/zhang2017unsupervised/">Unsupervised Generative Adversarial Cross-modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Generative Adversarial Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Generative Adversarial Cross-modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Jian, Peng Yuxin, Yuan Mingkuan</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>203</td>
    <td><p>Cross-modal hashing aims to map heterogeneous multimedia data into a common
Hamming space, which can realize fast and flexible retrieval across different
modalities. Unsupervised cross-modal hashing is more flexible and applicable
than supervised methods, since no intensive labeling work is involved. However,
existing unsupervised methods learn hashing functions by preserving inter and
intra correlations, while ignoring the underlying manifold structure across
different modalities, which is extremely helpful to capture meaningful nearest
neighbors of different modalities for cross-modal retrieval. To address the
above problem, in this paper we propose an Unsupervised Generative Adversarial
Cross-modal Hashing approach (UGACH), which makes full use of GAN’s ability for
unsupervised representation learning to exploit the underlying manifold
structure of cross-modal data. The main contributions can be summarized as
follows: (1) We propose a generative adversarial network to model cross-modal
hashing in an unsupervised fashion. In the proposed UGACH, given a data of one
modality, the generative model tries to fit the distribution over the manifold
structure, and select informative data of another modality to challenge the
discriminative model. The discriminative model learns to distinguish the
generated data and the true positive data sampled from correlation graph to
achieve better retrieval accuracy. These two models are trained in an
adversarial way to improve each other and promote hashing function learning.
(2) We propose a correlation graph based approach to capture the underlying
manifold structure across different modalities, so that data of different
modalities but within the same manifold can have smaller Hamming distance and
promote retrieval accuracy. Extensive experiments compared with 6
state-of-the-art methods verify the effectiveness of our proposed approach.</p>
</td>
    <td>
      
        Supervised 
      
        Multimodal-Retrieval 
      
        Hashing-Methods 
      
        AAAI 
      
        Unsupervised 
      
        Evaluation 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/yang2019shared/">Shared Predictive Cross-Modal Deep Quantization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Shared Predictive Cross-Modal Deep Quantization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Shared Predictive Cross-Modal Deep Quantization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yang Erkun, Deng Cheng, Li Chao, Liu Wei, Li Jie, Tao Dacheng</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Neural Networks and Learning Systems</td>
    <td>151</td>
    <td><p>With explosive growth of data volume and ever-increasing diversity of data
modalities, cross-modal similarity search, which conducts nearest neighbor
search across different modalities, has been attracting increasing interest.
This paper presents a deep compact code learning solution for efficient
cross-modal similarity search. Many recent studies have proven that
quantization-based approaches perform generally better than hashing-based
approaches on single-modal similarity search. In this paper, we propose a deep
quantization approach, which is among the early attempts of leveraging deep
neural networks into quantization-based cross-modal similarity search. Our
approach, dubbed shared predictive deep quantization (SPDQ), explicitly
formulates a shared subspace across different modalities and two private
subspaces for individual modalities, and representations in the shared subspace
and the private subspaces are learned simultaneously by embedding them to a
reproducing kernel Hilbert space, where the mean embedding of different
modality distributions can be explicitly compared. In addition, in the shared
subspace, a quantizer is learned to produce the semantics preserving compact
codes with the help of label alignment. Thanks to this novel network
architecture in cooperation with supervised quantization training, SPDQ can
preserve intramodal and intermodal similarities as much as possible and greatly
reduce quantization error. Experiments on two popular benchmarks corroborate
that our approach outperforms state-of-the-art methods.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Supervised 
      
        Hashing-Methods 
      
        Compact-Codes 
      
        Quantization 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/xu2017unsupervised/">Unsupervised Part-based Weighting Aggregation of Deep Convolutional Features for Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Part-based Weighting Aggregation of Deep Convolutional Features for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Part-based Weighting Aggregation of Deep Convolutional Features for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xu Jian, Shi Cunzhao, Qi Chengzuo, Wang Chunheng, Xiao Baihua</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>48</td>
    <td><p>In this paper, we propose a simple but effective semantic part-based
weighting aggregation (PWA) for image retrieval. The proposed PWA utilizes the
discriminative filters of deep convolutional layers as part detectors.
Moreover, we propose the effective unsupervised strategy to select some part
detectors to generate the “probabilistic proposals”, which highlight certain
discriminative parts of objects and suppress the noise of background. The final
global PWA representation could then be acquired by aggregating the regional
representations weighted by the selected “probabilistic proposals”
corresponding to various semantic content. We conduct comprehensive experiments
on four standard datasets and show that our unsupervised PWA outperforms the
state-of-the-art unsupervised and supervised aggregation methods. Code is
available at https://github.com/XJhaoren/PWA.</p>
</td>
    <td>
      
        Supervised 
      
        Image-Retrieval 
      
        Datasets 
      
        AAAI 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/quinn2017semantic/">Semantic Image Retrieval via Active Grounding of Visual Situations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semantic Image Retrieval via Active Grounding of Visual Situations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semantic Image Retrieval via Active Grounding of Visual Situations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Quinn Max H., Conser Erik, Witte Jordan M., Mitchell Melanie</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE 12th International Conference on Semantic Computing (ICSC)</td>
    <td>12</td>
    <td><p>We describe a novel architecture for semantic image retrieval—in
particular, retrieval of instances of visual situations. Visual situations are
concepts such as “a boxing match,” “walking the dog,” “a crowd waiting for a
bus,” or “a game of ping-pong,” whose instantiations in images are linked more
by their common spatial and semantic structure than by low-level visual
similarity. Given a query situation description, our architecture—called
Situate—learns models capturing the visual features of expected objects as
well the expected spatial configuration of relationships among objects. Given a
new image, Situate uses these models in an attempt to ground (i.e., to create a
bounding box locating) each expected component of the situation in the image
via an active search procedure. Situate uses the resulting grounding to compute
a score indicating the degree to which the new image is judged to contain an
instance of the situation. Such scores can be used to rank images in a
collection as part of a retrieval system. In the preliminary study described
here, we demonstrate the promise of this system by comparing Situate’s
performance with that of two baseline methods, as well as with a related
semantic image-retrieval system based on “scene graphs.”</p>
</td>
    <td>
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/li2019design/">The Design and Implementation of a Real Time Visual Search System on JD E-commerce Platform</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=The Design and Implementation of a Real Time Visual Search System on JD E-commerce Platform' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=The Design and Implementation of a Real Time Visual Search System on JD E-commerce Platform' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Jie, Liu Haifeng, Gui Chuanghua, Chen Jianyu, Ni Zhenyun, Wang Ning</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 19th International Middleware Conference Industry</td>
    <td>36</td>
    <td><p>We present the design and implementation of a visual search system for real
time image retrieval on JD.com, the world’s third largest and China’s largest
e-commerce site. We demonstrate that our system can support real time visual
search with hundreds of billions of product images at sub-second timescales and
handle frequent image updates through distributed hierarchical architecture and
efficient indexing methods. We hope that sharing our practice with our real
production system will inspire the middleware community’s interest and
appreciation for building practical large scale systems for emerging
applications, such as ecommerce visual search.</p>
</td>
    <td>
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/xuan2018deep/">Deep Randomized Ensembles for Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Randomized Ensembles for Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Randomized Ensembles for Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xuan Hong, Souvenir Richard, Pless Robert</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>124</td>
    <td><p>Learning embedding functions, which map semantically related inputs to nearby
locations in a feature space supports a variety of classification and
information retrieval tasks. In this work, we propose a novel, generalizable
and fast method to define a family of embedding functions that can be used as
an ensemble to give improved results. Each embedding function is learned by
randomly bagging the training labels into small subsets. We show experimentally
that these embedding ensembles create effective embedding functions. The
ensemble output defines a metric space that improves state of the art
performance for image retrieval on CUB-200-2011, Cars-196, In-Shop Clothes
Retrieval and VehicleID.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/li2018self/">Self-Supervised Adversarial Hashing Networks for Cross-Modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Self-Supervised Adversarial Hashing Networks for Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Self-Supervised Adversarial Hashing Networks for Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Chao, Deng Cheng, Li Ning, Liu Wei, Gao Xinbo, Tao Dacheng</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</td>
    <td>426</td>
    <td><p>Thanks to the success of deep learning, cross-modal retrieval has made
significant progress recently. However, there still remains a crucial
bottleneck: how to bridge the modality gap to further enhance the retrieval
accuracy. In this paper, we propose a self-supervised adversarial hashing
(\textbf{SSAH}) approach, which lies among the early attempts to incorporate
adversarial learning into cross-modal hashing in a self-supervised fashion. The
primary contribution of this work is that two adversarial networks are
leveraged to maximize the semantic correlation and consistency of the
representations between different modalities. In addition, we harness a
self-supervised semantic network to discover high-level semantic information in
the form of multi-label annotations. Such information guides the feature
learning process and preserves the modality relationships in both the common
semantic space and the Hamming space. Extensive experiments carried out on
three benchmark datasets validate that the proposed SSAH surpasses the
state-of-the-art methods.</p>
</td>
    <td>
      
        Supervised 
      
        Multimodal-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        CVPR 
      
        Self-Supervised 
      
        Evaluation 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/li2018discriminative/">Discriminative multi-view Privileged Information learning for image re-ranking</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Discriminative multi-view Privileged Information learning for image re-ranking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Discriminative multi-view Privileged Information learning for image re-ranking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Jun, Xu Chang, Yang Wankou, Sun Changyin, Tao Dacheng, Zhang Hong</td> <!-- 🔧 You were missing this -->
    <td>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</td>
    <td>162</td>
    <td><p>Conventional multi-view re-ranking methods usually perform asymmetrical
matching between the region of interest (ROI) in the query image and the whole
target image for similarity computation. Due to the inconsistency in the visual
appearance, this practice tends to degrade the retrieval accuracy particularly
when the image ROI, which is usually interpreted as the image objectness,
accounts for a smaller region in the image. Since Privileged Information (PI),
which can be viewed as the image prior, enables well characterizing the image
objectness, we are aiming at leveraging PI for further improving the
performance of the multi-view re-ranking accuracy in this paper. Towards this
end, we propose a discriminative multi-view re-ranking approach in which both
the original global image visual contents and the local auxiliary PI features
are simultaneously integrated into a unified training framework for generating
the latent subspaces with sufficient discriminating power. For the on-the-fly
re-ranking, since the multi-view PI features are unavailable, we only project
the original multi-view image representations onto the latent subspace, and
thus the re-ranking can be achieved by computing and sorting the distances from
the multi-view embeddings to the separating hyperplane. Extensive experimental
evaluations on the two public benchmarks Oxford5k and Paris6k reveal our
approach provides further performance boost for accurate image re-ranking,
whilst the comparative study demonstrates the advantage of our method against
other multi-view re-ranking methods.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        CVPR 
      
        Hybrid-ANN-Methods 
      
        Re-Ranking 
      
        Survey-Paper 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/li2017image/">Image Super-resolution via Feature-augmented Random Forest</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Image Super-resolution via Feature-augmented Random Forest' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Image Super-resolution via Feature-augmented Random Forest' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Hailiang, Lam Kin-man, Wang Miaohui</td> <!-- 🔧 You were missing this -->
    <td>Signal Processing: Image Communication</td>
    <td>16</td>
    <td><p>Recent random-forest (RF)-based image super-resolution approaches inherit
some properties from dictionary-learning-based algorithms, but the
effectiveness of the properties in RF is overlooked in the literature. In this
paper, we present a novel feature-augmented random forest (FARF) for image
super-resolution, where the conventional gradient-based features are augmented
with gradient magnitudes and different feature recipes are formulated on
different stages in an RF. The advantages of our method are that, firstly, the
dictionary-learning-based features are enhanced by adding gradient magnitudes,
based on the observation that the non-linear gradient magnitude are with highly
discriminative property. Secondly, generalized locality-sensitive hashing (LSH)
is used to replace principal component analysis (PCA) for feature
dimensionality reduction and original high-dimensional features are employed,
instead of the compressed ones, for the leaf-nodes’ regressors, since
regressors can benefit from higher dimensional features. This
original-compressed coupled feature sets scheme unifies the unsupervised LSH
evaluation on both image super-resolution and content-based image retrieval
(CBIR). Finally, we present a generalized weighted ridge regression (GWRR)
model for the leaf-nodes’ regressors. Experiment results on several public
benchmark datasets show that our FARF method can achieve an average gain of
about 0.3 dB, compared to traditional RF-based methods. Furthermore, a
fine-tuned FARF model can compare to or (in many cases) outperform some recent
stateof-the-art deep-learning-based algorithms.</p>
</td>
    <td>
      
        Locality-Sensitive-Hashing 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Unsupervised 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/wang2017composite/">Composite Quantization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Composite Quantization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Composite Quantization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Jingdong, Zhang Ting</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>27</td>
    <td><p>This paper studies the compact coding approach to approximate nearest
neighbor search. We introduce a composite quantization framework. It uses the
composition of several (\(M\)) elements, each of which is selected from a
different dictionary, to accurately approximate a \(D\)-dimensional vector, thus
yielding accurate search, and represents the data vector by a short code
composed of the indices of the selected elements in the corresponding
dictionaries. Our key contribution lies in introducing a near-orthogonality
constraint, which makes the search efficiency is guaranteed as the cost of the
distance computation is reduced to \(O(M)\) from \(O(D)\) through a distance table
lookup scheme. The resulting approach is called near-orthogonal composite
quantization. We theoretically justify the equivalence between near-orthogonal
composite quantization and minimizing an upper bound of a function formed by
jointly considering the quantization error and the search cost according to a
generalized triangle inequality. We empirically show the efficacy of the
proposed approach over several benchmark datasets. In addition, we demonstrate
the superior performances in other three applications: combination with
inverted multi-index, quantizing the query for mobile search, and inner-product
similarity search.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Vector-Indexing 
      
        Tools-&-Libraries 
      
        Datasets 
      
        Quantization 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/xu2017iterative/">Iterative Manifold Embedding Layer Learned by Incomplete Data for Large-scale Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Iterative Manifold Embedding Layer Learned by Incomplete Data for Large-scale Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Iterative Manifold Embedding Layer Learned by Incomplete Data for Large-scale Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xu Jian, Wang Chunheng, Qi Chengzuo, Shi Cunzhao, Xiao Baihua</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>21</td>
    <td><p>Existing manifold learning methods are not appropriate for image retrieval
task, because most of them are unable to process query image and they have much
additional computational cost especially for large scale database. Therefore,
we propose the iterative manifold embedding (IME) layer, of which the weights
are learned off-line by unsupervised strategy, to explore the intrinsic
manifolds by incomplete data. On the large scale database that contains 27000
images, IME layer is more than 120 times faster than other manifold learning
methods to embed the original representations at query time. We embed the
original descriptors of database images which lie on manifold in a high
dimensional space into manifold-based representations iteratively to generate
the IME representations in off-line learning stage. According to the original
descriptors and the IME representations of database images, we estimate the
weights of IME layer by ridge regression. In on-line retrieval stage, we employ
the IME layer to map the original representation of query image with ignorable
time cost (2 milliseconds). We experiment on five public standard datasets for
image retrieval. The proposed IME layer significantly outperforms related
dimension reduction methods and manifold learning methods. Without
post-processing, Our IME layer achieves a boost in performance of
state-of-the-art image retrieval methods with post-processing on most datasets,
and needs less computational cost.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Datasets 
      
        Re-Ranking 
      
        Unsupervised 
      
        Scalability 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/zemene2017large/">Large-scale Image Geo-Localization Using Dominant Sets</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Large-scale Image Geo-Localization Using Dominant Sets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Large-scale Image Geo-Localization Using Dominant Sets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zemene Eyasu, Tariku Yonatan, Idrees Haroon, Prati Andrea, Pelillo Marcello, Shah Mubarak</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>38</td>
    <td><p>This paper presents a new approach for the challenging problem of
geo-locating an image using image matching in a structured database of
city-wide reference images with known GPS coordinates. We cast the
geo-localization as a clustering problem on local image features. Akin to
existing approaches on the problem, our framework builds on low-level features
which allow partial matching between images. For each local feature in the
query image, we find its approximate nearest neighbors in the reference set.
Next, we cluster the features from reference images using Dominant Set
clustering, which affords several advantages over existing approaches. First,
it permits variable number of nodes in the cluster which we use to dynamically
select the number of nearest neighbors (typically coming from multiple
reference images) for each query feature based on its discrimination value.
Second, as we also quantify in our experiments, this approach is several orders
of magnitude faster than existing approaches. Thus, we obtain multiple clusters
(different local maximizers) and obtain a robust final solution to the problem
using multiple weak solutions through constrained Dominant Set clustering on
global image features, where we enforce the constraint that the query image
must be included in the cluster. This second level of clustering also bypasses
heuristic approaches to voting and selecting the reference image that matches
to the query. We evaluated the proposed framework on an existing dataset of
102k street view images as well as a new dataset of 300k images, and show that
it outperforms the state-of-the-art by 20% and 7%, respectively, on the two
datasets.</p>
</td>
    <td>
      
        Datasets 
      
        Scalability 
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/zhang2021visual/">Visual Search at Alibaba</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Visual Search at Alibaba' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Visual Search at Alibaba' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Yanhao, Pan Pan, Zheng Yun, Zhao Kang, Zhang Yingya, Ren Xiaofeng, Jin Rong</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</td>
    <td>58</td>
    <td><p>This paper introduces the large scale visual search algorithm and system
infrastructure at Alibaba. The following challenges are discussed under the
E-commercial circumstance at Alibaba (a) how to handle heterogeneous image data
and bridge the gap between real-shot images from user query and the online
images. (b) how to deal with large scale indexing for massive updating data.
(c) how to train deep models for effective feature representation without huge
human annotations. (d) how to improve the user engagement by considering the
quality of the content. We take advantage of large image collection of Alibaba
and state-of-the-art deep learning techniques to perform visual search at
scale. We present solutions and implementation details to overcome those
problems and also share our learnings from building such a large scale
commercial visual search engine. Specifically, model and search-based fusion
approach is introduced to effectively predict categories. Also, we propose a
deep CNN model for joint detection and feature learning by mining user click
behavior. The binary index engine is designed to scale up indexing without
compromising recall and precision. Finally, we apply all the stages into an
end-to-end system architecture, which can simultaneously achieve highly
efficient and scalable performance adapting to real-shot images. Extensive
experiments demonstrate the advancement of each module in our system. We hope
visual search at Alibaba becomes more widely incorporated into today’s
commercial applications.</p>
</td>
    <td>
      
        KDD 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/royoletelier2018disambiguating/">Disambiguating Music Artists at Scale with Audio Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Disambiguating Music Artists at Scale with Audio Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Disambiguating Music Artists at Scale with Audio Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Royo-letelier Jimena, Hennequin Romain, Tran Viet-anh, Moussallam Manuel</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>5</td>
    <td><p>We address the problem of disambiguating large scale catalogs through the
definition of an unknown artist clustering task. We explore the use of metric
learning techniques to learn artist embeddings directly from audio, and using a
dedicated homonym artists dataset, we compare our method with a recent approach
that learn similar embeddings using artist classifiers. While both systems have
the ability to disambiguate unknown artists relying exclusively on audio, we
show that our system is more suitable in the case when enough audio data is
available for each artist in the train dataset. We also propose a new negative
sampling method for metric learning that takes advantage of side information
such as music genre during the learning phase and shows promising results for
the artist clustering task.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/lin2018supervised/">Supervised Online Hashing via Similarity Distribution Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Online Hashing via Similarity Distribution Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Supervised Online Hashing via Similarity Distribution Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin Mingbao, Ji Rongrong, Chen Shen, Zheng Feng, Sun Xiaoshuai, Zhang Baochang, Cao Liujuan, Guo Guodong, Huang Feiyue</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 26th ACM international conference on Multimedia</td>
    <td>64</td>
    <td><p>Online hashing has attracted extensive research attention when facing
streaming data. Most online hashing methods, learning binary codes based on
pairwise similarities of training instances, fail to capture the semantic
relationship, and suffer from a poor generalization in large-scale applications
due to large variations. In this paper, we propose to model the similarity
distributions between the input data and the hashing codes, upon which a novel
supervised online hashing method, dubbed as Similarity Distribution based
Online Hashing (SDOH), is proposed, to keep the intrinsic semantic relationship
in the produced Hamming space. Specifically, we first transform the discrete
similarity matrix into a probability matrix via a Gaussian-based normalization
to address the extremely imbalanced distribution issue. And then, we introduce
a scaling Student t-distribution to solve the challenging initialization
problem, and efficiently bridge the gap between the known and unknown
distributions. Lastly, we align the two distributions via minimizing the
Kullback-Leibler divergence (KL-diverence) with stochastic gradient descent
(SGD), by which an intuitive similarity constraint is imposed to update hashing
model on the new streaming data with a powerful generalizing ability to the
past data. Extensive experiments on three widely-used benchmarks validate the
superiority of the proposed SDOH over the state-of-the-art methods in the
online retrieval task.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Scalability 
      
        Supervised 
      
        Compact-Codes 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/lin2018learning/">Learning a Disentangled Embedding for Monocular 3D Shape Retrieval and Pose Estimation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning a Disentangled Embedding for Monocular 3D Shape Retrieval and Pose Estimation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning a Disentangled Embedding for Monocular 3D Shape Retrieval and Pose Estimation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin Kyaw Zaw, Xu Weipeng, Sun Qianru, Theobalt Christian, Chua Tat-seng</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>7</td>
    <td><p>We propose a novel approach to jointly perform 3D shape retrieval and pose
estimation from monocular images.In order to make the method robust to
real-world image variations, e.g. complex textures and backgrounds, we learn an
embedding space from 3D data that only includes the relevant information,
namely the shape and pose. Our approach explicitly disentangles a shape vector
and a pose vector, which alleviates both pose bias for 3D shape retrieval and
categorical bias for pose estimation. We then train a CNN to map the images to
this embedding space, and then retrieve the closest 3D shape from the database
and estimate the 6D pose of the object. Our method achieves 10.3 median error
for pose estimation and 0.592 top-1-accuracy for category agnostic 3D object
retrieval on the Pascal3D+ dataset, outperforming the previous state-of-the-art
methods on both tasks.</p>
</td>
    <td>
      
        Datasets 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2018</td>
    <td>
      <a href="/publications/lin2019supervised/">Supervised Online Hashing via Hadamard Codebook Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Online Hashing via Hadamard Codebook Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Supervised Online Hashing via Hadamard Codebook Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin Mingbao, Ji Rongrong, Liu Hong, Liu Yongjian</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 26th ACM international conference on Multimedia</td>
    <td>64</td>
    <td><p>In recent years, binary code learning, a.k.a hashing, has received extensive
attention in large-scale multimedia retrieval. It aims to encode
high-dimensional data points to binary codes, hence the original
high-dimensional metric space can be efficiently approximated via Hamming
space. However, most existing hashing methods adopted offline batch learning,
which is not suitable to handle incremental datasets with streaming data or new
instances. In contrast, the robustness of the existing online hashing remains
as an open problem, while the embedding of supervised/semantic information
hardly boosts the performance of the online hashing, mainly due to the defect
of unknown category numbers in supervised learning. In this paper, we proposed
an online hashing scheme, termed Hadamard Codebook based Online Hashing (HCOH),
which aims to solve the above problems towards robust and supervised online
hashing. In particular, we first assign an appropriate high-dimensional binary
codes to each class label, which is generated randomly by Hadamard codes to
each class label, which is generated randomly by Hadamard codes. Subsequently,
LSH is adopted to reduce the length of such Hadamard codes in accordance with
the hash bits, which can adapt the predefined binary codes online, and
theoretically guarantee the semantic similarity. Finally, we consider the
setting of stochastic data acquisition, which facilitates our method to
efficiently learn the corresponding hashing functions via stochastic gradient
descend (SGD) online. Notably, the proposed HCOH can be embedded with
supervised labels and it not limited to a predefined category number. Extensive
experiments on three widely-used benchmarks demonstrate the merits of the
proposed scheme over the state-of-the-art methods. The code is available at
https://github.com/lmbxmu/mycode/tree/master/2018ACMMM_HCOH.</p>
</td>
    <td>
      
        Supervised 
      
        Locality-Sensitive-Hashing 
      
        Hashing-Methods 
      
        Datasets 
      
        Compact-Codes 
      
        Scalability 
      
        Evaluation 
      
        Robustness 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/song2017deep/">Deep Discrete Hashing with Self-supervised Pairwise Labels</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Discrete Hashing with Self-supervised Pairwise Labels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Discrete Hashing with Self-supervised Pairwise Labels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Song Jingkuan, He Tao, Fan Hangbo, Gao Lianli</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>308</td>
    <td><p>Hashing methods have been widely used for applications of large-scale image
retrieval and classification. Non-deep hashing methods using handcrafted
features have been significantly outperformed by deep hashing methods due to
their better feature representation and end-to-end learning framework. However,
the most striking successes in deep hashing have mostly involved discriminative
models, which require labels. In this paper, we propose a novel unsupervised
deep hashing method, named Deep Discrete Hashing (DDH), for large-scale image
retrieval and classification. In the proposed framework, we address two main
problems: 1) how to directly learn discrete binary codes? 2) how to equip the
binary representation with the ability of accurate image retrieval and
classification in an unsupervised way? We resolve these problems by introducing
an intermediate variable and a loss function steering the learning process,
which is based on the neighborhood structure in the original space.
Experimental results on standard datasets (CIFAR-10, NUS-WIDE, and Oxford-17)
demonstrate that our DDH significantly outperforms existing hashing methods by
large margin in terms of~mAP for image retrieval and object recognition. Code
is available at https://github.com/htconquer/ddh.</p>
</td>
    <td>
      
        Unsupervised 
      
        Supervised 
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        CVPR 
      
        Self-Supervised 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/ertl2017superminhash/">SuperMinHash - A New Minwise Hashing Algorithm for Jaccard Similarity Estimation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=SuperMinHash - A New Minwise Hashing Algorithm for Jaccard Similarity Estimation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=SuperMinHash - A New Minwise Hashing Algorithm for Jaccard Similarity Estimation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ertl Otmar</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>13</td>
    <td><p>This paper presents a new algorithm for calculating hash signatures of sets
which can be directly used for Jaccard similarity estimation. The new approach
is an improvement over the MinHash algorithm, because it has a better runtime
behavior and the resulting signatures allow a more precise estimation of the
Jaccard index.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Locality-Sensitive-Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/ercoli2016compact/">Compact Hash Codes for Efficient Visual Descriptors Retrieval in Large Scale Databases</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Compact Hash Codes for Efficient Visual Descriptors Retrieval in Large Scale Databases' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Compact Hash Codes for Efficient Visual Descriptors Retrieval in Large Scale Databases' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ercoli Simone, Bertini Marco, del Bimbo Alberto</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>42</td>
    <td><p>In this paper we present an efficient method for visual descriptors retrieval
based on compact hash codes computed using a multiple k-means assignment. The
method has been applied to the problem of approximate nearest neighbor (ANN)
search of local and global visual content descriptors, and it has been tested
on different datasets: three large scale public datasets of up to one billion
descriptors (BIGANN) and, supported by recent progress in convolutional neural
networks (CNNs), also on the CIFAR-10 and MNIST datasets. Experimental results
show that, despite its simplicity, the proposed method obtains a very high
performance that makes it superior to more complex state-of-the-art methods.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/vu2017search/">Search Personalization with Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Search Personalization with Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Search Personalization with Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Vu Thanh, Nguyen Dat Quoc, Johnson Mark, Song Dawei, Willis Alistair</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>48</td>
    <td><p>Recent research has shown that the performance of search personalization
depends on the richness of user profiles which normally represent the user’s
topical interests. In this paper, we propose a new embedding approach to
learning user profiles, where users are embedded on a topical interest space.
We then directly utilize the user profiles for search personalization.
Experiments on query logs from a major commercial web search engine demonstrate
that our embedding approach improves the performance of the search engine and
also achieves better search performance than other strong baselines.</p>
</td>
    <td>
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/wehrmann2017order/">Order embeddings and character-level convolutions for multimodal alignment</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Order embeddings and character-level convolutions for multimodal alignment' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Order embeddings and character-level convolutions for multimodal alignment' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wehrmann Jônatas, Mattjie Anderson, Barros Rodrigo C.</td> <!-- 🔧 You were missing this -->
    <td>Pattern Recognition Letters</td>
    <td>21</td>
    <td><p>With the novel and fast advances in the area of deep neural networks, several
challenging image-based tasks have been recently approached by researchers in
pattern recognition and computer vision. In this paper, we address one of these
tasks, which is to match image content with natural language descriptions,
sometimes referred as multimodal content retrieval. Such a task is particularly
challenging considering that we must find a semantic correspondence between
captions and the respective image, a challenge for both computer vision and
natural language processing areas. For such, we propose a novel multimodal
approach based solely on convolutional neural networks for aligning images with
their captions by directly convolving raw characters. Our proposed
character-based textual embeddings allow the replacement of both
word-embeddings and recurrent neural networks for text understanding, saving
processing time and requiring fewer learnable parameters. Our method is based
on the idea of projecting both visual and textual information into a common
embedding space. For training such embeddings we optimize a contrastive loss
function that is computed to minimize order-violations between images and their
respective descriptions. We achieve state-of-the-art performance in the largest
and most well-known image-text alignment dataset, namely Microsoft COCO, with a
method that is conceptually much simpler and that possesses considerably fewer
parameters than current approaches.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/driemel2017locality/">Locality-sensitive hashing of curves</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Locality-sensitive hashing of curves' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Locality-sensitive hashing of curves' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Driemel Anne, Silvestri Francesco</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>31</td>
    <td><p>We study data structures for storing a set of polygonal curves in \({\rm R}^d\)
such that, given a query curve, we can efficiently retrieve similar curves from
the set, where similarity is measured using the discrete Fr'echet distance or
the dynamic time warping distance. To this end we devise the first
locality-sensitive hashing schemes for these distance measures. A major
challenge is posed by the fact that these distance measures internally optimize
the alignment between the curves. We give solutions for different types of
alignments including constrained and unconstrained versions. For unconstrained
alignments, we improve over a result by Indyk from 2002 for short curves. Let
\(n\) be the number of input curves and let \(m\) be the maximum complexity of a
curve in the input. In the particular case where \(m \leq \frac{\alpha}{4d} log
n\), for some fixed \(\alpha&gt;0\), our solutions imply an approximate near-neighbor
data structure for the discrete Fr'echet distance that uses space in
\(O(n^{1+\alpha}log n)\) and achieves query time in \(O(n^{\alpha}log^2 n)\) and
constant approximation factor. Furthermore, our solutions provide a trade-off
between approximation quality and computational performance: for any parameter
\(k \in [m]\), we can give a data structure that uses space in \(O(2^{2k}m^{k-1} n
log n + nm)\), answers queries in \(O( 2^{2k} m^{k}log n)\) time and achieves
approximation factor in \(O(m/k)\).</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/wu2017structured/">Structured Deep Hashing with Convolutional Neural Networks for Fast Person Re-identification</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Structured Deep Hashing with Convolutional Neural Networks for Fast Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Structured Deep Hashing with Convolutional Neural Networks for Fast Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu Lin, Wang Yang</td> <!-- 🔧 You were missing this -->
    <td>Computer Vision and Image Understanding</td>
    <td>71</td>
    <td><p>Given a pedestrian image as a query, the purpose of person re-identification
is to identify the correct match from a large collection of gallery images
depicting the same person captured by disjoint camera views. The critical
challenge is how to construct a robust yet discriminative feature
representation to capture the compounded variations in pedestrian appearance.
To this end, deep learning methods have been proposed to extract hierarchical
features against extreme variability of appearance. However, existing methods
in this category generally neglect the efficiency in the matching stage whereas
the searching speed of a re-identification system is crucial in real-world
applications. In this paper, we present a novel deep hashing framework with
Convolutional Neural Networks (CNNs) for fast person re-identification.
Technically, we simultaneously learn both CNN features and hash functions/codes
to get robust yet discriminative features and similarity-preserving hash codes.
Thereby, person re-identification can be resolved by efficiently computing and
ranking the Hamming distances between images. A structured loss function
defined over positive pairs and hard negatives is proposed to formulate a novel
optimization problem so that fast convergence and more stable optimized
solution can be obtained. Extensive experiments on two benchmarks CUHK03
\cite{FPNN} and Market-1501 \cite{Market1501} show that the proposed deep
architecture is efficacy over state-of-the-arts.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Neural-Hashing 
      
        Tools-&-Libraries 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/song2016deep/">Deep Metric Learning via Facility Location</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Metric Learning via Facility Location' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Metric Learning via Facility Location' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Song Hyun Oh, Jegelka Stefanie, Rathod Vivek, Murphy Kevin</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>308</td>
    <td><p>Learning the representation and the similarity metric in an end-to-end
fashion with deep networks have demonstrated outstanding results for clustering
and retrieval. However, these recent approaches still suffer from the
performance degradation stemming from the local metric training procedure which
is unaware of the global structure of the embedding space.
  We propose a global metric learning scheme for optimizing the deep metric
embedding with the learnable clustering function and the clustering metric
(NMI) in a novel structured prediction framework.
  Our experiments on CUB200-2011, Cars196, and Stanford online products
datasets show state of the art performance both on the clustering and retrieval
tasks measured in the NMI and Recall@K evaluation metrics.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        CVPR 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/do2017simultaneous/">Simultaneous Feature Aggregating and Hashing for Large-scale Image Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Simultaneous Feature Aggregating and Hashing for Large-scale Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Simultaneous Feature Aggregating and Hashing for Large-scale Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Do Thanh-toan, Tan Dang-khoa Le, Pham Trung T., Cheung Ngai-man</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>38</td>
    <td><p>In most state-of-the-art hashing-based visual search systems, local image
descriptors of an image are first aggregated as a single feature vector. This
feature vector is then subjected to a hashing function that produces a binary
hash code. In previous work, the aggregating and the hashing processes are
designed independently. In this paper, we propose a novel framework where
feature aggregating and hashing are designed simultaneously and optimized
jointly. Specifically, our joint optimization produces aggregated
representations that can be better reconstructed by some binary codes. This
leads to more discriminative binary hash codes and improved retrieval accuracy.
In addition, we also propose a fast version of the recently-proposed Binary
Autoencoder to be used in our proposed framework. We perform extensive
retrieval experiments on several benchmark datasets with both SIFT and
convolutional features. Our results suggest that the proposed framework
achieves significant improvements over the state of the art.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Compact-Codes 
      
        CVPR 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/qiu2017deep/">Deep Semantic Hashing with Generative Adversarial Networks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Semantic Hashing with Generative Adversarial Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Semantic Hashing with Generative Adversarial Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Qiu Zhaofan, Pan, Yao, Mei</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>97</td>
    <td><p>Hashing has been a widely-adopted technique for nearest
neighbor search in large-scale image retrieval tasks. Recent research has shown that leveraging supervised information can
lead to high quality hashing. However, the cost of annotating
data is often an obstacle when applying supervised hashing
to a new domain. Moreover, the results can suffer from the
robustness problem as the data at training and test stage
may come from different distributions. This paper studies
the exploration of generating synthetic data through semisupervised generative adversarial networks (GANs), which
leverages largely unlabeled and limited labeled training data
to produce highly compelling data with intrinsic invariance
and global coherence, for better understanding statistical
structures of natural data. We demonstrate that the above
two limitations can be well mitigated by applying the synthetic data for hashing. Specifically, a novel deep semantic
hashing with GANs (DSH-GANs) is presented, which mainly
consists of four components: a deep convolution neural networks (CNN) for learning image representations, an adversary
stream to distinguish synthetic images from real ones, a hash
stream for encoding image representations to hash codes and
a classification stream. The whole architecture is trained endto-end by jointly optimizing three losses, i.e., adversarial loss
to correct label of synthetic or real for each sample, triplet
ranking loss to preserve the relative similarity ordering in the
input real-synthetic triplets and classification loss to classify
each sample accurately. Extensive experiments conducted on
both CIFAR-10 and NUS-WIDE image benchmarks validate the capability of exploiting synthetic images for hashing. Our
framework also achieves superior results when compared to
state-of-the-art deep hash models.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Neural-Hashing 
      
        Text-Retrieval 
      
        Tools-&-Libraries 
      
        SIGIR 
      
        Hashing-Methods 
      
        Supervised 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/do2016embedding/">Embedding based on function approximation for large scale image search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Embedding based on function approximation for large scale image search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Embedding based on function approximation for large scale image search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Do Thanh-toan, Cheung Ngai-man</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>37</td>
    <td><p>The objective of this paper is to design an embedding method that maps local
features describing an image (e.g. SIFT) to a higher dimensional representation
useful for the image retrieval problem. First, motivated by the relationship
between the linear approximation of a nonlinear function in high dimensional
space and the stateof-the-art feature representation used in image retrieval,
i.e., VLAD, we propose a new approach for the approximation. The embedded
vectors resulted by the function approximation process are then aggregated to
form a single representation for image retrieval. Second, in order to make the
proposed embedding method applicable to large scale problem, we further derive
its fast version in which the embedded vectors can be efficiently computed,
i.e., in the closed-form. We compare the proposed embedding methods with the
state of the art in the context of image search under various settings: when
the images are represented by medium length vectors, short vectors, or binary
vectors. The experimental results show that the proposed embedding methods
outperform existing the state of the art on the standard public image retrieval
benchmarks.</p>
</td>
    <td>
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/pachori2017hashing/">Hashing in the Zero Shot Framework with Domain Adaptation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing in the Zero Shot Framework with Domain Adaptation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing in the Zero Shot Framework with Domain Adaptation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Pachori Shubham, Deshpande Ameya, Raman Shanmuganathan</td> <!-- 🔧 You were missing this -->
    <td>Neurocomputing</td>
    <td>24</td>
    <td><p>Techniques to learn hash codes which can store and retrieve large dimensional
multimedia data efficiently have attracted broad research interests in the
recent years. With rapid explosion of newly emerged concepts and online data,
existing supervised hashing algorithms suffer from the problem of scarcity of
ground truth annotations due to the high cost of obtaining manual annotations.
Therefore, we propose an algorithm to learn a hash function from training
images belonging to <code class="language-plaintext highlighter-rouge">seen' classes which can efficiently encode images of
</code>unseen’ classes to binary codes. Specifically, we project the image features
from visual space and semantic features from semantic space into a common
Hamming subspace. Earlier works to generate hash codes have tried to relax the
discrete constraints on hash codes and solve the continuous optimization
problem. However, it often leads to quantization errors. In this work, we use
the max-margin classifier to learn an efficient hash function. To address the
concern of domain-shift which may arise due to the introduction of new classes,
we also introduce an unsupervised domain adaptation model in the proposed
hashing framework. Results on the three datasets show the advantage of using
domain adaptation in learning a high-quality hash function and superiority of
our method for the task of image retrieval performance as compared to several
state-of-the-art hashing methods.</p>
</td>
    <td>
      
        Supervised 
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Unsupervised 
      
        Quantization 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/deng2017learning/">Learning Deep Similarity Models with Focus Ranking for Fabric Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Deep Similarity Models with Focus Ranking for Fabric Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Deep Similarity Models with Focus Ranking for Fabric Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Deng Daiguo, Wang Ruomei, Wu Hefeng, He Huayong, Li Qi, Luo Xiaonan</td> <!-- 🔧 You were missing this -->
    <td>Image and Vision Computing</td>
    <td>37</td>
    <td><p>Fabric image retrieval is beneficial to many applications including clothing
searching, online shopping and cloth modeling. Learning pairwise image
similarity is of great importance to an image retrieval task. With the
resurgence of Convolutional Neural Networks (CNNs), recent works have achieved
significant progresses via deep representation learning with metric embedding,
which drives similar examples close to each other in a feature space, and
dissimilar ones apart from each other. In this paper, we propose a novel
embedding method termed focus ranking that can be easily unified into a CNN for
jointly learning image representations and metrics in the context of
fine-grained fabric image retrieval. Focus ranking aims to rank similar
examples higher than all dissimilar ones by penalizing ranking disorders via
the minimization of the overall cost attributed to similar samples being ranked
below dissimilar ones. At the training stage, training samples are organized
into focus ranking units for efficient optimization. We build a large-scale
fabric image retrieval dataset (FIRD) with about 25,000 images of 4,300
fabrics, and test the proposed model on the FIRD dataset. Experimental results
show the superiority of the proposed model over existing metric embedding
models.</p>
</td>
    <td>
      
        Datasets 
      
        Scalability 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/wu2017sampling/">Sampling Matters in Deep Embedding Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Sampling Matters in Deep Embedding Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Sampling Matters in Deep Embedding Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu Chao-yuan, Manmatha R., Smola Alexander J., Krähenbühl Philipp</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Conference on Computer Vision (ICCV)</td>
    <td>863</td>
    <td><p>Deep embeddings answer one simple question: How similar are two images?
Learning these embeddings is the bedrock of verification, zero-shot learning,
and visual search. The most prominent approaches optimize a deep convolutional
network with a suitable loss function, such as contrastive loss or triplet
loss. While a rich line of work focuses solely on the loss functions, we show
in this paper that selecting training examples plays an equally important role.
We propose distance weighted sampling, which selects more informative and
stable examples than traditional approaches. In addition, we show that a simple
margin based loss is sufficient to outperform all other loss functions. We
evaluate our approach on the Stanford Online Products, CAR196, and the
CUB200-2011 datasets for image retrieval and clustering, and on the LFW dataset
for face verification. Our method achieves state-of-the-art performance on all
of them.</p>
</td>
    <td>
      
        ICCV 
      
        Few-Shot-&-Zero-Shot 
      
        Image-Retrieval 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/dai2017stochastic/">Stochastic Generative Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Stochastic Generative Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Stochastic Generative Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dai Bo, Guo Ruiqi, Kumar Sanjiv, He Niao, Song Le</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>73</td>
    <td><p>Learning-based binary hashing has become a powerful paradigm for fast search
and retrieval in massive databases. However, due to the requirement of discrete
outputs for the hash functions, learning such functions is known to be very
challenging. In addition, the objective functions adopted by existing hashing
techniques are mostly chosen heuristically. In this paper, we propose a novel
generative approach to learn hash functions through Minimum Description Length
principle such that the learned hash codes maximally compress the dataset and
can also be used to regenerate the inputs. We also develop an efficient
learning algorithm based on the stochastic distributional gradient, which
avoids the notorious difficulty caused by binary output constraints, to jointly
optimize the parameters of the hash function and the associated generative
model. Extensive experiments on a variety of large-scale datasets show that the
proposed method achieves better retrieval results than the existing
state-of-the-art methods.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        Scalability 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/laskar2017context/">Context Aware Query Image Representation for Particular Object Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Context Aware Query Image Representation for Particular Object Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Context Aware Query Image Representation for Particular Object Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Laskar Zakaria, Kannala Juho</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>42</td>
    <td><p>The current models of image representation based on Convolutional Neural
Networks (CNN) have shown tremendous performance in image retrieval. Such
models are inspired by the information flow along the visual pathway in the
human visual cortex. We propose that in the field of particular object
retrieval, the process of extracting CNN representations from query images with
a given region of interest (ROI) can also be modelled by taking inspiration
from human vision. Particularly, we show that by making the CNN pay attention
on the ROI while extracting query image representation leads to significant
improvement over the baseline methods on challenging Oxford5k and Paris6k
datasets. Furthermore, we propose an extension to a recently introduced
encoding method for CNN representations, regional maximum activations of
convolutions (R-MAC). The proposed extension weights the regional
representations using a novel saliency measure prior to aggregation. This leads
to further improvement in retrieval accuracy.</p>
</td>
    <td>
      
        Datasets 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/corbi%C3%A8re2017leveraging/">Leveraging Weakly Annotated Data for Fashion Image Retrieval and Label Prediction</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Leveraging Weakly Annotated Data for Fashion Image Retrieval and Label Prediction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Leveraging Weakly Annotated Data for Fashion Image Retrieval and Label Prediction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Corbière Charles, Ben-younes Hedi, Ramé Alexandre, Ollion Charles</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Conference on Computer Vision Workshops (ICCVW)</td>
    <td>59</td>
    <td><p>In this paper, we present a method to learn a visual representation adapted
for e-commerce products. Based on weakly supervised learning, our model learns
from noisy datasets crawled on e-commerce website catalogs and does not require
any manual labeling. We show that our representation can be used for downward
classification tasks over clothing categories with different levels of
granularity. We also demonstrate that the learnt representation is suitable for
image retrieval. We achieve nearly state-of-art results on the DeepFashion
In-Shop Clothes Retrieval and Categories Attributes Prediction tasks, without
using the provided training set.</p>
</td>
    <td>
      
        ICCV 
      
        Datasets 
      
        Supervised 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/dahlgaard2017fast/">Fast Similarity Sketching</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Similarity Sketching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Similarity Sketching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dahlgaard Søren, Langhede Mathias Bæk Tejs, Houen Jakob Bæk Tejs, Thorup Mikkel</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS)</td>
    <td>30</td>
    <td><p>We consider the \(\textit{Similarity Sketching}\) problem: Given a universe
\([u] = \{0,\ldots, u-1\}\) we want a random function \(S\) mapping subsets
\(A\subseteq [u]\) into vectors \(S(A)\) of size \(t\), such that the Jaccard
similarity \(J(A,B) = |A\cap B|/|A\cup B|\) between sets \(A\) and \(B\) is
preserved. More precisely, define \(X_i = [S(A)[i] =
  S(B)[i]]\) and \(X = \sum_{i\in [t]} X_i\). We want \(E[X_i]=J(A,B)\), and we want
\(X\) to be strongly concentrated around \(E[X] = t \cdot J(A,B)\) (i.e.
Chernoff-style bounds). This is a fundamental problem which has found numerous
applications in data mining, large-scale classification, computer vision,
similarity search, etc. via the classic MinHash algorithm. The vectors \(S(A)\)
are also called \(\textit{sketches}\). Strong concentration is critical, for
often we want to sketch many sets \(B_1,\ldots,B_n\) so that we later, for a
query set \(A\), can find (one of) the most similar \(B_i\). It is then critical
that no \(B_i\) looks much more similar to \(A\) due to errors in the sketch.
  The seminal \(t\times\textit{MinHash}\) algorithm uses \(t\) random hash
functions \(h_1,\ldots, h_t\), and stores \(\left ( \min_{a\in A} h_1(A),\ldots,
\min_{a\in A} h_t(A) \right )\) as the sketch of \(A\). The main drawback of
MinHash is, however, its \(O(t\cdot |A|)\) running time, and finding a sketch
with similar properties and faster running time has been the subject of several
papers. (continued…)</p>
</td>
    <td>
      
        Similarity-Search 
      
        Scalability 
      
        Locality-Sensitive-Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/connor2017high/">High-Dimensional Simplexes for Supermetric Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=High-Dimensional Simplexes for Supermetric Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=High-Dimensional Simplexes for Supermetric Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Connor Richard, Vadicamo Lucia, Rabitti Fausto</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>19</td>
    <td><p>In 1953, Blumenthal showed that every semi-metric space that is isometrically
embeddable in a Hilbert space has the n-point property; we have previously
called such spaces supermetric spaces. Although this is a strictly stronger
property than triangle inequality, it is nonetheless closely related and many
useful metric spaces possess it. These include Euclidean, Cosine and
Jensen-Shannon spaces of any dimension. A simple corollary of the n-point
property is that, for any (n+1) objects sampled from the space, there exists an
n-dimensional simplex in Euclidean space whose edge lengths correspond to the
distances among the objects. We show how the construction of such simplexes in
higher dimensions can be used to give arbitrarily tight lower and upper bounds
on distances within the original space. This allows the construction of an
n-dimensional Euclidean space, from which lower and upper bounds of the
original space can be calculated, and which is itself an indexable space with
the n-point property. For similarity search, the engineering tradeoffs are
good: we show significant reductions in data size and metric cost with little
loss of accuracy, leading to a significant overall improvement in search
performance.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/chung2017learning/">Learning Deep Representations of Medical Images using Siamese CNNs with Application to Content-Based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Deep Representations of Medical Images using Siamese CNNs with Application to Content-Based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Deep Representations of Medical Images using Siamese CNNs with Application to Content-Based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chung Yu-an, Weng Wei-hung</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>63</td>
    <td><p>Deep neural networks have been investigated in learning latent
representations of medical images, yet most of the studies limit their approach
in a single supervised convolutional neural network (CNN), which usually rely
heavily on a large scale annotated dataset for training. To learn image
representations with less supervision involved, we propose a deep Siamese CNN
(SCNN) architecture that can be trained with only binary image pair
information. We evaluated the learned image representations on a task of
content-based medical image retrieval using a publicly available multiclass
diabetic retinopathy fundus image dataset. The experimental results show that
our proposed deep SCNN is comparable to the state-of-the-art single supervised
CNN, and requires much less supervision for training.</p>
</td>
    <td>
      
        Datasets 
      
        Supervised 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/wu2017deep/">Deep Incremental Hashing Network for Efficient Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Incremental Hashing Network for Efficient Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Incremental Hashing Network for Efficient Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu Dayan, Dai, Liu, Li, Wang</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval</td>
    <td>67</td>
    <td><p>Hashing has shown great potential in large-scale image retrieval due to its storage and computation efficiency, especially the recent deep supervised hashing methods. To achieve promising performance, deep supervised hashing methods require a large amount of training data from different classes. However, when images of new categories emerge, existing deep hashing methods have to retrain the CNN model and generate hash codes for all the database images again, which is impractical for large-scale retrieval system.
In this paper, we propose a novel deep hashing framework, called Deep Incremental Hashing Network (DIHN), for learning hash codes in an incremental manner. DIHN learns the hash codes for the new coming images directly, while keeping the old ones unchanged. Simultaneously, a deep hash function for query set is learned by preserving the similarities between training points. Extensive experiments on two widely used image retrieval benchmarks demonstrate that the proposed DIHN framework can significantly decrease the training time while keeping the state-of-the-art retrieval accuracy.</p>
</td>
    <td>
      
        Scalability 
      
        Efficiency 
      
        Evaluation 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Neural-Hashing 
      
        Multimodal-Retrieval 
      
        Image-Retrieval 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/christiani2016set/">Set Similarity Search Beyond MinHash</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Set Similarity Search Beyond MinHash' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Set Similarity Search Beyond MinHash' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Christiani Tobias, Pagh Rasmus</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing</td>
    <td>45</td>
    <td><p>We consider the problem of approximate set similarity search under
Braun-Blanquet similarity \(B(\mathbf{x}, \mathbf{y}) = |\mathbf{x} \cap
\mathbf{y}| / \max(|\mathbf{x}|, |\mathbf{y}|)\). The \((b_2, b_2)\)-approximate
Braun-Blanquet similarity search problem is to preprocess a collection of sets
\(P\) such that, given a query set \(\mathbf{q}\), if there exists \(\mathbf{x} \in
P\) with \(B(\mathbf{q}, \mathbf{x}) \geq b_1\), then we can efficiently return
\(\mathbf{x}’ \in P\) with \(B(\mathbf{q}, \mathbf{x}’) &gt; b_2\).
  We present a simple data structure that solves this problem with space usage
\(O(n^{1+\rho}log n + \sum_{\mathbf{x} \in P}|\mathbf{x}|)\) and query time
\(O(|\mathbf{q}|n^{\rho} log n)\) where \(n = |P|\) and \(\rho =
log(1/b_1)/log(1/b_2)\). Making use of existing lower bounds for
locality-sensitive hashing by O’Donnell et al. (TOCT 2014) we show that this
value of \(\rho\) is tight across the parameter space, i.e., for every choice of
constants \(0 &lt; b_2 &lt; b_1 &lt; 1\).
  In the case where all sets have the same size our solution strictly improves
upon the value of \(\rho\) that can be obtained through the use of
state-of-the-art data-independent techniques in the Indyk-Motwani
locality-sensitive hashing framework (STOC 1998) such as Broder’s MinHash (CCS
1997) for Jaccard similarity and Andoni et al.’s cross-polytope LSH (NIPS 2015)
for cosine similarity. Surprisingly, even though our solution is
data-independent, for a large part of the parameter space we outperform the
currently best data-dependent method by Andoni and Razenshteyn (STOC 2015).</p>
</td>
    <td>
      
        Similarity-Search 
      
        Tools-&-Libraries 
      
        Locality-Sensitive-Hashing 
      
        Hashing-Methods 
      
        Distance-Metric-Learning 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/christiani2016framework/">A Framework for Similarity Search with Space-Time Tradeoffs using Locality-Sensitive Filtering</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Framework for Similarity Search with Space-Time Tradeoffs using Locality-Sensitive Filtering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Framework for Similarity Search with Space-Time Tradeoffs using Locality-Sensitive Filtering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Christiani Tobias</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>21</td>
    <td><p>We present a framework for similarity search based on Locality-Sensitive
Filtering (LSF), generalizing the Indyk-Motwani (STOC 1998) Locality-Sensitive
Hashing (LSH) framework to support space-time tradeoffs. Given a family of
filters, defined as a distribution over pairs of subsets of space with certain
locality-sensitivity properties, we can solve the approximate near neighbor
problem in \(d\)-dimensional space for an \(n\)-point data set with query time
\(dn^{\rho_q+o(1)}\), update time \(dn^{\rho_u+o(1)}\), and space usage \(dn + n^{1</p>
<ul>
  <li>\rho_u + o(1)}\). The space-time tradeoff is tied to the tradeoff between
query time and update time, controlled by the exponents \(\rho_q, \rho_u\) that
are determined by the filter family. Locality-sensitive filtering was
introduced by Becker et al. (SODA 2016) together with a framework yielding a
single, balanced, tradeoff between query time and space, further relying on the
assumption of an efficient oracle for the filter evaluation algorithm. We
extend the LSF framework to support space-time tradeoffs and through a
combination of existing techniques we remove the oracle assumption.
Building on a filter family for the unit sphere by Laarhoven (arXiv 2015) we
use a kernel embedding technique by Rahimi &amp; Recht (NIPS 2007) to show a
solution to the \((r,cr)\)-near neighbor problem in \(\ell_s^d\)-space for \(0 &lt; s
\leq 2\) with query and update exponents
\(\rho_q=\frac{c^s(1+\lambda)^2}{(c^s+\lambda)^2}\) and
\(\rho_u=\frac{c^s(1-\lambda)^2}{(c^s+\lambda)^2}\) where \(\lambda\in[-1,1]\) is a
tradeoff parameter. This result improves upon the space-time tradeoff of
Kapralov (PODS 2015) and is shown to be optimal in the case of a balanced
tradeoff. Finally, we show a lower bound for the space-time tradeoff on the
unit sphere that matches Laarhoven’s and our own upper bound in the case of
random data.</li>
</ul>
</td>
    <td>
      
        Similarity-Search 
      
        Tools-&-Libraries 
      
        Locality-Sensitive-Hashing 
      
        Hashing-Methods 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/dahlgaard2017practical/">Practical Hash Functions for Similarity Estimation and Dimensionality Reduction</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Practical Hash Functions for Similarity Estimation and Dimensionality Reduction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Practical Hash Functions for Similarity Estimation and Dimensionality Reduction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Dahlgaard Søren, Knudsen Mathias Bæk Tejs, Thorup Mikkel</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>16</td>
    <td><p>Hashing is a basic tool for dimensionality reduction employed in several
aspects of machine learning. However, the perfomance analysis is often carried
out under the abstract assumption that a truly random unit cost hash function
is used, without concern for which concrete hash function is employed. The
concrete hash function may work fine on sufficiently random input. The question
is if it can be trusted in the real world when faced with more structured
input.
  In this paper we focus on two prominent applications of hashing, namely
similarity estimation with the one permutation hashing (OPH) scheme of Li et
al. [NIPS’12] and feature hashing (FH) of Weinberger et al. [ICML’09], both of
which have found numerous applications, i.e. in approximate near-neighbour
search with LSH and large-scale classification with SVM.
  We consider mixed tabulation hashing of Dahlgaard et al.[FOCS’15] which was
proved to perform like a truly random hash function in many applications,
including OPH. Here we first show improved concentration bounds for FH with
truly random hashing and then argue that mixed tabulation performs similar for
sparse input. Our main contribution, however, is an experimental comparison of
different hashing schemes when used inside FH, OPH, and LSH.
  We find that mixed tabulation hashing is almost as fast as the
multiply-mod-prime scheme ax+b mod p. Mutiply-mod-prime is guaranteed to work
well on sufficiently random data, but we demonstrate that in the above
applications, it can lead to bias and poor concentration on both real-world and
synthetic data. We also compare with the popular MurmurHash3, which has no
proven guarantees. Mixed tabulation and MurmurHash3 both perform similar to
truly random hashing in our experiments. However, mixed tabulation is 40%
faster than MurmurHash3, and it has the proven guarantee of good performance on
all possible input.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Scalability 
      
        Evaluation 
      
        Locality-Sensitive-Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/gui2019fast/">Fast Supervised Discrete Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Supervised Discrete Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Supervised Discrete Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gui Jie, Liu Tongliang, Sun Zhenan, Tao Dacheng, Tan Tieniu</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>265</td>
    <td><p>Learning-based hashing algorithms are <code class="language-plaintext highlighter-rouge">hot topics" because they can greatly
increase the scale at which existing methods operate. In this paper, we propose
a new learning-based hashing method called</code>fast supervised discrete hashing”
(FSDH) based on ``supervised discrete hashing” (SDH). Regressing the training
examples (or hash code) to the corresponding class labels is widely used in
ordinary least squares regression. Rather than adopting this method, FSDH uses
a very simple yet effective regression of the class labels of training examples
to the corresponding hash code to accelerate the algorithm. To the best of our
knowledge, this strategy has not previously been used for hashing. Traditional
SDH decomposes the optimization into three sub-problems, with the most critical
sub-problem - discrete optimization for binary hash codes - solved using
iterative discrete cyclic coordinate descent (DCC), which is time-consuming.
However, FSDH has a closed-form solution and only requires a single rather than
iterative hash code-solving step, which is highly efficient. Furthermore, FSDH
is usually faster than SDH for solving the projection matrix for least squares
regression, making FSDH generally faster than SDH. For example, our results
show that FSDH is about 12-times faster than SDH when the number of hashing
bits is 128 on the CIFAR-10 data base, and FSDH is about 151-times faster than
FastHash when the number of hashing bits is 64 on the MNIST data-base. Our
experimental results show that FSDH is not only fast, but also outperforms
other comparative methods.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/zhou2016learning/">Learning Low Dimensional Convolutional Neural Networks for High-Resolution Remote Sensing Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Low Dimensional Convolutional Neural Networks for High-Resolution Remote Sensing Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Low Dimensional Convolutional Neural Networks for High-Resolution Remote Sensing Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhou Weixun, Newsam Shawn, Li Congmin, Shao Zhenfeng</td> <!-- 🔧 You were missing this -->
    <td>Remote Sensing</td>
    <td>139</td>
    <td><p>Learning powerful feature representations for image retrieval has always been
a challenging task in the field of remote sensing. Traditional methods focus on
extracting low-level hand-crafted features which are not only time-consuming
but also tend to achieve unsatisfactory performance due to the content
complexity of remote sensing images. In this paper, we investigate how to
extract deep feature representations based on convolutional neural networks
(CNN) for high-resolution remote sensing image retrieval (HRRSIR). To this end,
two effective schemes are proposed to generate powerful feature representations
for HRRSIR. In the first scheme, the deep features are extracted from the
fully-connected and convolutional layers of the pre-trained CNN models,
respectively; in the second scheme, we propose a novel CNN architecture based
on conventional convolution layers and a three-layer perceptron. The novel CNN
model is then trained on a large remote sensing dataset to learn low
dimensional features. The two schemes are evaluated on several public and
challenging datasets, and the results indicate that the proposed schemes and in
particular the novel CNN are able to achieve state-of-the-art performance.</p>
</td>
    <td>
      
        Datasets 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/nedelec2017specializing/">Specializing Joint Representations for the task of Product Recommendation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Specializing Joint Representations for the task of Product Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Specializing Joint Representations for the task of Product Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Nedelec Thomas, Smirnova Elena, Vasile Flavian</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2nd Workshop on Deep Learning for Recommender Systems</td>
    <td>17</td>
    <td><p>We propose a unified product embedded representation that is optimized for
the task of retrieval-based product recommendation. To this end, we introduce a
new way to fuse modality-specific product embeddings into a joint product
embedding, in order to leverage both product content information, such as
textual descriptions and images, and product collaborative filtering signal. By
introducing the fusion step at the very end of our architecture, we are able to
train each modality separately, allowing us to keep a modular architecture that
is preferable in real-world recommendation deployments. We analyze our
performance on normal and hard recommendation setups such as cold-start and
cross-category recommendations and achieve good performance on a large product
shopping dataset.</p>
</td>
    <td>
      
        Datasets 
      
        Recommender-Systems 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/grzegorczyk2016binary/">Binary Paragraph Vectors</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Binary Paragraph Vectors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Binary Paragraph Vectors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Grzegorczyk Karol, Kurdziel Marcin</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2nd Workshop on Representation Learning for NLP</td>
    <td>5</td>
    <td><p>Recently Le &amp; Mikolov described two log-linear models, called Paragraph
Vector, that can be used to learn state-of-the-art distributed representations
of documents. Inspired by this work, we present Binary Paragraph Vector models:
simple neural networks that learn short binary codes for fast information
retrieval. We show that binary paragraph vectors outperform autoencoder-based
binary codes, despite using fewer bits. We also evaluate their precision in
transfer learning settings, where binary codes are inferred for documents
unrelated to the training corpus. Results from these experiments indicate that
binary paragraph vectors can capture semantics relevant for various
domain-specific documents. Finally, we present a model that simultaneously
learns short binary codes and longer, real-valued representations. This model
can be used to rapidly retrieve a short list of highly relevant documents from
a large document collection.</p>
</td>
    <td>
      
        Compact-Codes 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/gordo2016end/">End-to-end Learning of Deep Visual Representations for Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=End-to-end Learning of Deep Visual Representations for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=End-to-end Learning of Deep Visual Representations for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gordo Albert, Almazan Jon, Revaud Jerome, Larlus Diane</td> <!-- 🔧 You were missing this -->
    <td>International Journal of Computer Vision</td>
    <td>532</td>
    <td><p>While deep learning has become a key ingredient in the top performing methods
for many computer vision tasks, it has failed so far to bring similar
improvements to instance-level image retrieval. In this article, we argue that
reasons for the underwhelming results of deep methods on image retrieval are
threefold: i) noisy training data, ii) inappropriate deep architecture, and
iii) suboptimal training procedure. We address all three issues.
  First, we leverage a large-scale but noisy landmark dataset and develop an
automatic cleaning method that produces a suitable training set for deep
retrieval. Second, we build on the recent R-MAC descriptor, show that it can be
interpreted as a deep and differentiable architecture, and present improvements
to enhance it. Last, we train this network with a siamese architecture that
combines three streams with a triplet loss. At the end of the training process,
the proposed architecture produces a global image representation in a single
forward pass that is well suited for image retrieval. Extensive experiments
show that our approach significantly outperforms previous retrieval approaches,
including state-of-the-art methods based on costly local descriptor indexing
and spatial verification. On Oxford 5k, Paris 6k and Holidays, we respectively
report 94.7, 96.6, and 94.8 mean average precision. Our representations can
also be heavily compressed using product quantization with little loss in
accuracy. For additional material, please see
www.xrce.xerox.com/Deep-Image-Retrieval.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Scalability 
      
        Quantization 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/veit2016conditional/">Conditional Similarity Networks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Conditional Similarity Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Conditional Similarity Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Veit Andreas, Belongie Serge, Karaletsos Theofanis</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>168</td>
    <td><p>What makes images similar? To measure the similarity between images, they are
typically embedded in a feature-vector space, in which their distance preserve
the relative dissimilarity. However, when learning such similarity embeddings
the simplifying assumption is commonly made that images are only compared to
one unique measure of similarity. A main reason for this is that contradicting
notions of similarities cannot be captured in a single space. To address this
shortcoming, we propose Conditional Similarity Networks (CSNs) that learn
embeddings differentiated into semantically distinct subspaces that capture the
different notions of similarities. CSNs jointly learn a disentangled embedding
where features for different similarities are encoded in separate dimensions as
well as masks that select and reweight relevant dimensions to induce a subspace
that encodes a specific similarity notion. We show that our approach learns
interpretable image representations with visually relevant semantic subspaces.
Further, when evaluating on triplet questions from multiple similarity notions
our model even outperforms the accuracy obtained by training individual
specialized networks for each notion separately.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/qiu2018deep/">Deep Semantic Hashing with Generative Adversarial Networks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Semantic Hashing with Generative Adversarial Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Semantic Hashing with Generative Adversarial Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Qiu Zhaofan, Pan Yingwei, Yao Ting, Mei Tao</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>97</td>
    <td><p>Hashing has been a widely-adopted technique for nearest neighbor search in
large-scale image retrieval tasks. Recent research has shown that leveraging
supervised information can lead to high quality hashing. However, the cost of
annotating data is often an obstacle when applying supervised hashing to a new
domain. Moreover, the results can suffer from the robustness problem as the
data at training and test stage could come from similar but different
distributions. This paper studies the exploration of generating synthetic data
through semi-supervised generative adversarial networks (GANs), which leverages
largely unlabeled and limited labeled training data to produce highly
compelling data with intrinsic invariance and global coherence, for better
understanding statistical structures of natural data. We demonstrate that the
above two limitations can be well mitigated by applying the synthetic data for
hashing. Specifically, a novel deep semantic hashing with GANs (DSH-GANs) is
presented, which mainly consists of four components: a deep convolution neural
networks (CNN) for learning image representations, an adversary stream to
distinguish synthetic images from real ones, a hash stream for encoding image
representations to hash codes and a classification stream. The whole
architecture is trained end-to-end by jointly optimizing three losses, i.e.,
adversarial loss to correct label of synthetic or real for each sample, triplet
ranking loss to preserve the relative similarity ordering in the input
real-synthetic triplets and classification loss to classify each sample
accurately. Extensive experiments conducted on both CIFAR-10 and NUS-WIDE image
benchmarks validate the capability of exploiting synthetic images for hashing.
Our framework also achieves superior results when compared to state-of-the-art
deep hash models.</p>
</td>
    <td>
      
        Supervised 
      
        Text-Retrieval 
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        SIGIR 
      
        Neural-Hashing 
      
        Scalability 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/veit2017conditional/">Conditional Similarity Networks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Conditional Similarity Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Conditional Similarity Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Veit Andreas, Belongie Serge, Karaletsos Theofanis</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>168</td>
    <td><p>What makes images similar? To measure the similarity between images, they are
typically embedded in a feature-vector space, in which their distance preserve
the relative dissimilarity. However, when learning such similarity embeddings
the simplifying assumption is commonly made that images are only compared to
one unique measure of similarity. A main reason for this is that contradicting
notions of similarities cannot be captured in a single space. To address this
shortcoming, we propose Conditional Similarity Networks (CSNs) that learn
embeddings differentiated into semantically distinct subspaces that capture the
different notions of similarities. CSNs jointly learn a disentangled embedding
where features for different similarities are encoded in separate dimensions as
well as masks that select and reweight relevant dimensions to induce a subspace
that encodes a specific similarity notion. We show that our approach learns
interpretable image representations with visually relevant semantic subspaces.
Further, when evaluating on triplet questions from multiple similarity notions
our model even outperforms the accuracy obtained by training individual
specialized networks for each notion separately.</p>
</td>
    <td>
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/gella2017image/">Image Pivoting for Learning Multilingual Multimodal Representations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Image Pivoting for Learning Multilingual Multimodal Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Image Pivoting for Learning Multilingual Multimodal Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gella Spandana, Sennrich Rico, Keller Frank, Lapata Mirella</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</td>
    <td>72</td>
    <td><p>In this paper we propose a model to learn multimodal multilingual
representations for matching images and sentences in different languages, with
the aim of advancing multilingual versions of image search and image
understanding. Our model learns a common representation for images and their
descriptions in two different languages (which need not be parallel) by
considering the image as a pivot between two languages. We introduce a new
pairwise ranking loss function which can handle both symmetric and asymmetric
similarity between the two modalities. We evaluate our models on
image-description ranking for German and English, and on semantic textual
similarity of image descriptions in English. In both cases we achieve
state-of-the-art performance.</p>
</td>
    <td>
      
        Evaluation 
      
        Image-Retrieval 
      
        EMNLP 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/podlesnaya2016deep/">Deep Learning Based Semantic Video Indexing and Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Learning Based Semantic Video Indexing and Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Learning Based Semantic Video Indexing and Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Podlesnaya Anna, Podlesnyy Sergey</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Networks and Systems</td>
    <td>20</td>
    <td><p>We share the implementation details and testing results for video retrieval
system based exclusively on features extracted by convolutional neural
networks. We show that deep learned features might serve as universal signature
for semantic content of video useful in many search and retrieval tasks. We
further show that graph-based storage structure for video index allows to
efficiently retrieving the content with complicated spatial and temporal search
queries.</p>
</td>
    <td>
      
        Video-Retrieval 
      
        Graph-Based-ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/tolias2017asymmetric/">Asymmetric Feature Maps with Application to Sketch Based Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Asymmetric Feature Maps with Application to Sketch Based Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Asymmetric Feature Maps with Application to Sketch Based Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tolias Giorgos, Chum Ondřej</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>39</td>
    <td><p>We propose a novel concept of asymmetric feature maps (AFM), which allows to
evaluate multiple kernels between a query and database entries without
increasing the memory requirements. To demonstrate the advantages of the AFM
method, we derive a short vector image representation that, due to asymmetric
feature maps, supports efficient scale and translation invariant sketch-based
image retrieval. Unlike most of the short-code based retrieval systems, the
proposed method provides the query localization in the retrieved image. The
efficiency of the search is boosted by approximating a 2D translation search
via trigonometric polynomial of scores by 1D projections. The projections are a
special case of AFM. An order of magnitude speed-up is achieved compared to
traditional trigonometric polynomials. The results are boosted by an
image-based average query expansion, exceeding significantly the state of the
art on standard benchmarks.</p>
</td>
    <td>
      
        CVPR 
      
        Image-Retrieval 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/giraud2019superpixel/">Superpixel-based Color Transfer</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Superpixel-based Color Transfer' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Superpixel-based Color Transfer' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Giraud Rémi, Ta Vinh-thong, Papadakis Nicolas</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Conference on Image Processing (ICIP)</td>
    <td>6</td>
    <td><p>In this work, we propose a fast superpixel-based color transfer method (SCT)
between two images. Superpixels enable to decrease the image dimension and to
extract a reduced set of color candidates. We propose to use a fast approximate
nearest neighbor matching algorithm in which we enforce the match diversity by
limiting the selection of the same superpixels. A fusion framework is designed
to transfer the matched colors, and we demonstrate the improvement obtained
over exact matching results. Finally, we show that SCT is visually competitive
compared to state-of-the-art methods.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/shankar2017deep/">Deep Learning based Large Scale Visual Recommendation and Search for E-Commerce</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Learning based Large Scale Visual Recommendation and Search for E-Commerce' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Learning based Large Scale Visual Recommendation and Search for E-Commerce' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shankar Devashish, Narumanchi Sujay, Ananya H A, Kompalli Pramod, Chaudhury Krishnendu</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>59</td>
    <td><p>In this paper, we present a unified end-to-end approach to build a large
scale Visual Search and Recommendation system for e-commerce. Previous works
have targeted these problems in isolation. We believe a more effective and
elegant solution could be obtained by tackling them together. We propose a
unified Deep Convolutional Neural Network architecture, called VisNet, to learn
embeddings to capture the notion of visual similarity, across several semantic
granularities. We demonstrate the superiority of our approach for the task of
image retrieval, by comparing against the state-of-the-art on the Exact
Street2Shop dataset. We then share the design decisions and trade-offs made
while deploying the model to power Visual Recommendations across a catalog of
50M products, supporting 2K queries a second at Flipkart, India’s largest
e-commerce company. The deployment of our solution has yielded a significant
business impact, as measured by the conversion-rate.</p>
</td>
    <td>
      
        Datasets 
      
        Recommender-Systems 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/shu2017compressing/">Compressing Word Embeddings via Deep Compositional Code Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Compressing Word Embeddings via Deep Compositional Code Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Compressing Word Embeddings via Deep Compositional Code Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shu Raphael, Nakayama Hideki</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>95</td>
    <td><p>Natural language processing (NLP) models often require a massive number of
parameters for word embeddings, resulting in a large storage or memory
footprint. Deploying neural NLP models to mobile devices requires compressing
the word embeddings without any significant sacrifices in performance. For this
purpose, we propose to construct the embeddings with few basis vectors. For
each word, the composition of basis vectors is determined by a hash code. To
maximize the compression rate, we adopt the multi-codebook quantization
approach instead of binary coding scheme. Each code is composed of multiple
discrete numbers, such as (3, 2, 1, 8), where the value of each component is
limited to a fixed range. We propose to directly learn the discrete codes in an
end-to-end neural network by applying the Gumbel-softmax trick. Experiments
show the compression rate achieves 98% in a sentiment analysis task and 94% ~
99% in machine translation tasks without performance loss. In both tasks, the
proposed method can improve the model performance by slightly lowering the
compression rate. Compared to other approaches such as character-level
segmentation, the proposed method is language-independent and does not require
modifications to the network architecture.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Quantization 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/ferdowsi2017sparse/">Sparse Ternary Codes for similarity search have higher coding gain than dense binary codes</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Sparse Ternary Codes for similarity search have higher coding gain than dense binary codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Sparse Ternary Codes for similarity search have higher coding gain than dense binary codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ferdowsi Sohrab, Voloshynovskiy Slava, Kostadinov Dimche, Holotyak Taras</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Symposium on Information Theory (ISIT)</td>
    <td>12</td>
    <td><p>This paper addresses the problem of Approximate Nearest Neighbor (ANN) search
in pattern recognition where feature vectors in a database are encoded as
compact codes in order to speed-up the similarity search in large-scale
databases. Considering the ANN problem from an information-theoretic
perspective, we interpret it as an encoding, which maps the original feature
vectors to a less entropic sparse representation while requiring them to be as
informative as possible. We then define the coding gain for ANN search using
information-theoretic measures. We next show that the classical approach to
this problem, which consists of binarization of the projected vectors is
sub-optimal. Instead, a properly designed ternary encoding achieves higher
coding gains and lower complexity.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Scalability 
      
        Compact-Codes 
      
        Similarity-Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/feng2016deep/">Deep Image Set Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Image Set Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Image Set Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Feng Jie, Karaman Svebor, Jhuo I-hong, Chang Shih-fu</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE Winter Conference on Applications of Computer Vision (WACV)</td>
    <td>12</td>
    <td><p>In applications involving matching of image sets, the information from
multiple images must be effectively exploited to represent each set.
State-of-the-art methods use probabilistic distribution or subspace to model a
set and use specific distance measure to compare two sets. These methods are
slow to compute and not compact to use in a large scale scenario.
Learning-based hashing is often used in large scale image retrieval as they
provide a compact representation of each sample and the Hamming distance can be
used to efficiently compare two samples. However, most hashing methods encode
each image separately and discard knowledge that multiple images in the same
set represent the same object or person. We investigate the set hashing problem
by combining both set representation and hashing in a single deep neural
network. An image set is first passed to a CNN module to extract image
features, then these features are aggregated using two types of set feature to
capture both set specific and database-wide distribution information. The
computed set feature is then fed into a multilayer perceptron to learn a
compact binary embedding. Triplet loss is used to train the network by forming
set similarity relations using class labels. We extensively evaluate our
approach on datasets used for image matching and show highly competitive
performance compared to state-of-the-art methods.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/wei2016selective/">Selective Convolutional Descriptor Aggregation for Fine-Grained Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Selective Convolutional Descriptor Aggregation for Fine-Grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Selective Convolutional Descriptor Aggregation for Fine-Grained Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wei Xiu-shen, Luo Jian-hao, Wu Jianxin, Zhou Zhi-hua</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>434</td>
    <td><p>Deep convolutional neural network models pre-trained for the ImageNet
classification task have been successfully adopted to tasks in other domains,
such as texture description and object proposal generation, but these tasks
require annotations for images in the new domain. In this paper, we focus on a
novel and challenging task in the pure unsupervised setting: fine-grained image
retrieval. Even with image labels, fine-grained images are difficult to
classify, let alone the unsupervised retrieval task. We propose the Selective
Convolutional Descriptor Aggregation (SCDA) method. SCDA firstly localizes the
main object in fine-grained images, a step that discards the noisy background
and keeps useful deep descriptors. The selected descriptors are then aggregated
and dimensionality reduced into a short feature vector using the best practices
we found. SCDA is unsupervised, using no image label or bounding box
annotation. Experiments on six fine-grained datasets confirm the effectiveness
of SCDA for fine-grained image retrieval. Besides, visualization of the SCDA
features shows that they correspond to visual attributes (even subtle ones),
which might explain SCDA’s high mean average precision in fine-grained
retrieval. Moreover, on general image retrieval datasets, SCDA achieves
comparable retrieval results with state-of-the-art general image retrieval
approaches.</p>
</td>
    <td>
      
        Supervised 
      
        Image-Retrieval 
      
        Datasets 
      
        Unsupervised 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/svenstrup2017hash/">Hash Embeddings for Efficient Word Representations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hash Embeddings for Efficient Word Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hash Embeddings for Efficient Word Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Svenstrup Dan, Hansen Jonas Meinertz, Winther Ole</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>29</td>
    <td><p>We present hash embeddings, an efficient method for representing words in a
continuous vector form. A hash embedding may be seen as an interpolation
between a standard word embedding and a word embedding created using a random
hash function (the hashing trick). In hash embeddings each token is represented
by \(k\) \(d\)-dimensional embeddings vectors and one \(k\) dimensional weight
vector. The final \(d\) dimensional representation of the token is the product of
the two. Rather than fitting the embedding vectors for each token these are
selected by the hashing trick from a shared pool of \(B\) embedding vectors. Our
experiments show that hash embeddings can easily deal with huge vocabularies
consisting of millions of tokens. When using a hash embedding there is no need
to create a dictionary before training nor to perform any kind of vocabulary
pruning after training. We show that models trained using hash embeddings
exhibit at least the same level of performance as models trained using regular
embeddings across a wide range of tasks. Furthermore, the number of parameters
needed by such an embedding is only a fraction of what is required by a regular
embedding. Since standard embeddings and embeddings constructed using the
hashing trick are actually just special cases of a hash embedding, hash
embeddings can be considered an extension and improvement over the existing
regular embedding types.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/zhu2017part/">Part-based Deep Hashing for Large-scale Person Re-identification</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Part-based Deep Hashing for Large-scale Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Part-based Deep Hashing for Large-scale Person Re-identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhu Fuqing, Kong Xiangwei, Zheng Liang, Fu Haiyan, Tian Qi</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>70</td>
    <td><p>Large-scale is a trend in person re-identification (re-id). It is important
that real-time search be performed in a large gallery. While previous methods
mostly focus on discriminative learning, this paper makes the attempt in
integrating deep learning and hashing into one framework to evaluate the
efficiency and accuracy for large-scale person re-id. We integrate spatial
information for discriminative visual representation by partitioning the
pedestrian image into horizontal parts. Specifically, Part-based Deep Hashing
(PDH) is proposed, in which batches of triplet samples are employed as the
input of the deep hashing architecture. Each triplet sample contains two
pedestrian images (or parts) with the same identity and one pedestrian image
(or part) of the different identity. A triplet loss function is employed with a
constraint that the Hamming distance of pedestrian images (or parts) with the
same identity is smaller than ones with the different identity. In the
experiment, we show that the proposed Part-based Deep Hashing method yields
very competitive re-id accuracy on the large-scale Market-1501 and
Market-1501+500K datasets.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Neural-Hashing 
      
        Scalability 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/tsai2017learning/">Learning Robust Visual-Semantic Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Robust Visual-Semantic Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Robust Visual-Semantic Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tsai Yao-hung Hubert, Huang Liang-kang, Salakhutdinov Ruslan</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Conference on Computer Vision (ICCV)</td>
    <td>148</td>
    <td><p>Many of the existing methods for learning joint embedding of images and text
use only supervised information from paired images and its textual attributes.
Taking advantage of the recent success of unsupervised learning in deep neural
networks, we propose an end-to-end learning framework that is able to extract
more robust multi-modal representations across domains. The proposed method
combines representation learning models (i.e., auto-encoders) together with
cross-domain learning criteria (i.e., Maximum Mean Discrepancy loss) to learn
joint embeddings for semantic and visual features. A novel technique of
unsupervised-data adaptation inference is introduced to construct more
comprehensive embeddings for both labeled and unlabeled data. We evaluate our
method on Animals with Attributes and Caltech-UCSD Birds 200-2011 dataset with
a wide range of applications, including zero and few-shot image recognition and
retrieval, from inductive to transductive settings. Empirically, we show that
our framework improves over the current state of the art on many of the
considered tasks.</p>
</td>
    <td>
      
        ICCV 
      
        Few-Shot-&-Zero-Shot 
      
        Supervised 
      
        Tools-&-Libraries 
      
        Datasets 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/xu2017neural/">Neural Network-based Graph Embedding for Cross-Platform Binary Code Similarity Detection</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Neural Network-based Graph Embedding for Cross-Platform Binary Code Similarity Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Neural Network-based Graph Embedding for Cross-Platform Binary Code Similarity Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xu Xiaojun, Liu Chang, Feng Qian, Yin Heng, Song Le, Song Dawn</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security</td>
    <td>545</td>
    <td><p>The problem of cross-platform binary code similarity detection aims at
detecting whether two binary functions coming from different platforms are
similar or not. It has many security applications, including plagiarism
detection, malware detection, vulnerability search, etc. Existing approaches
rely on approximate graph matching algorithms, which are inevitably slow and
sometimes inaccurate, and hard to adapt to a new task. To address these issues,
in this work, we propose a novel neural network-based approach to compute the
embedding, i.e., a numeric vector, based on the control flow graph of each
binary function, then the similarity detection can be done efficiently by
measuring the distance between the embeddings for two functions. We implement a
prototype called Gemini. Our extensive evaluation shows that Gemini outperforms
the state-of-the-art approaches by large margins with respect to similarity
detection accuracy. Further, Gemini can speed up prior art’s embedding
generation time by 3 to 4 orders of magnitude and reduce the required training
time from more than 1 week down to 30 minutes to 10 hours. Our real world case
studies demonstrate that Gemini can identify significantly more vulnerable
firmware images than the state-of-the-art, i.e., Genius. Our research showcases
a successful application of deep learning on computer security problems.</p>
</td>
    <td>
      
        Compact-Codes 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/settle2017query/">Query-by-Example Search with Discriminative Neural Acoustic Word Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Query-by-Example Search with Discriminative Neural Acoustic Word Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Query-by-Example Search with Discriminative Neural Acoustic Word Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Settle Shane, Levin Keith, Kamper Herman, Livescu Karen</td> <!-- 🔧 You were missing this -->
    <td>Interspeech 2017</td>
    <td>56</td>
    <td><p>Query-by-example search often uses dynamic time warping (DTW) for comparing
queries and proposed matching segments. Recent work has shown that comparing
speech segments by representing them as fixed-dimensional vectors — acoustic
word embeddings — and measuring their vector distance (e.g., cosine distance)
can discriminate between words more accurately than DTW-based approaches. We
consider an approach to query-by-example search that embeds both the query and
database segments according to a neural model, followed by nearest-neighbor
search to find the matching segments. Earlier work on embedding-based
query-by-example, using template-based acoustic word embeddings, achieved
competitive performance. We find that our embeddings, based on recurrent neural
networks trained to optimize word discrimination, achieve substantial
improvements in performance and run-time efficiency over the previous
approaches.</p>
</td>
    <td>
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/hussein2017unified/">Unified Embedding and Metric Learning for Zero-Exemplar Event Detection</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unified Embedding and Metric Learning for Zero-Exemplar Event Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unified Embedding and Metric Learning for Zero-Exemplar Event Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hussein Noureldien, Gavves Efstratios, Smeulders Arnold W. M.</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>10</td>
    <td><p>Event detection in unconstrained videos is conceived as a content-based video
retrieval with two modalities: textual and visual. Given a text describing a
novel event, the goal is to rank related videos accordingly. This task is
zero-exemplar, no video examples are given to the novel event.
  Related works train a bank of concept detectors on external data sources.
These detectors predict confidence scores for test videos, which are ranked and
retrieved accordingly. In contrast, we learn a joint space in which the visual
and textual representations are embedded. The space casts a novel event as a
probability of pre-defined events. Also, it learns to measure the distance
between an event and its related videos.
  Our model is trained end-to-end on publicly available EventNet. When applied
to TRECVID Multimedia Event Detection dataset, it outperforms the
state-of-the-art by a considerable margin.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Datasets 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/han2017beyond/">Beyond SIFT using Binary features for Loop Closure Detection</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Beyond SIFT using Binary features for Loop Closure Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Beyond SIFT using Binary features for Loop Closure Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Han Lei, Zhou Guyue, Xu Lan, Fang Lu</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</td>
    <td>10</td>
    <td><p>In this paper a binary feature based Loop Closure Detection (LCD) method is
proposed, which for the first time achieves higher precision-recall (PR)
performance compared with state-of-the-art SIFT feature based approaches. The
proposed system originates from our previous work Multi-Index hashing for Loop
closure Detection (MILD), which employs Multi-Index Hashing
(MIH)~\cite{greene1994multi} for Approximate Nearest Neighbor (ANN) search of
binary features. As the accuracy of MILD is limited by repeating textures and
inaccurate image similarity measurement, burstiness handling is introduced to
solve this problem and achieves considerable accuracy improvement.
Additionally, a comprehensive theoretical analysis on MIH used in MILD is
conducted to further explore the potentials of hashing methods for ANN search
of binary features from probabilistic perspective. This analysis provides more
freedom on best parameter choosing in MIH for different application scenarios.
Experiments on popular public datasets show that the proposed approach achieved
the highest accuracy compared with state-of-the-art while running at 30Hz for
databases containing thousands of images.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Vector-Indexing 
      
        Hashing-Methods 
      
        Datasets 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/li2025deep/">Deep Supervised Discrete Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Supervised Discrete Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Supervised Discrete Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Qi, Sun, He, Tan</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>171</td>
    <td><p>With the rapid growth of image and video data on the web, hashing has been
extensively studied for image or video search in recent years. Benefiting from
recent advances in deep learning, deep hashing methods have achieved promising
results for image retrieval. However, there are some limitations of previous deep
hashing methods (e.g., the semantic information is not fully exploited). In this
paper, we develop a deep supervised discrete hashing algorithm based on the
assumption that the learned binary codes should be ideal for classification. Both the
pairwise label information and the classification information are used to learn the
hash codes within one stream framework. We constrain the outputs of the last layer
to be binary codes directly, which is rarely investigated in deep hashing algorithm.
Because of the discrete nature of hash codes, an alternating minimization method
is used to optimize the objective function. Experimental results have shown that
our method outperforms current state-of-the-art methods on benchmark datasets.</p>
</td>
    <td>
      
        Video-Retrieval 
      
        Image-Retrieval 
      
        Datasets 
      
        Neural-Hashing 
      
        Tools-&-Libraries 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/hu2017learning/">Learning Discrete Representations via Information Maximizing Self-Augmented Training</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Discrete Representations via Information Maximizing Self-Augmented Training' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Discrete Representations via Information Maximizing Self-Augmented Training' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hu Weihua, Miyato Takeru, Tokui Seiya, Matsumoto Eiichi, Sugiyama Masashi</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>206</td>
    <td><p>Learning discrete representations of data is a central machine learning task
because of the compactness of the representations and ease of interpretation.
The task includes clustering and hash learning as special cases. Deep neural
networks are promising to be used because they can model the non-linearity of
data and scale to large datasets. However, their model complexity is huge, and
therefore, we need to carefully regularize the networks in order to learn
useful representations that exhibit intended invariance for applications of
interest. To this end, we propose a method called Information Maximizing
Self-Augmented Training (IMSAT). In IMSAT, we use data augmentation to impose
the invariance on discrete representations. More specifically, we encourage the
predicted representations of augmented data points to be close to those of the
original data points in an end-to-end fashion. At the same time, we maximize
the information-theoretic dependency between data and their predicted discrete
representations. Extensive experiments on benchmark datasets show that IMSAT
produces state-of-the-art results for both clustering and unsupervised hash
learning.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Unsupervised 
      
        Datasets 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/hong2017content/">Content-Based Video-Music Retrieval Using Soft Intra-Modal Structure Constraint</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Content-Based Video-Music Retrieval Using Soft Intra-Modal Structure Constraint' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Content-Based Video-Music Retrieval Using Soft Intra-Modal Structure Constraint' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hong Sungeun, Im Woobin, Yang Hyun S.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>10</td>
    <td><p>Up to now, only limited research has been conducted on cross-modal retrieval
of suitable music for a specified video or vice versa. Moreover, much of the
existing research relies on metadata such as keywords, tags, or associated
description that must be individually produced and attached posterior. This
paper introduces a new content-based, cross-modal retrieval method for video
and music that is implemented through deep neural networks. We train the
network via inter-modal ranking loss such that videos and music with similar
semantics end up close together in the embedding space. However, if only the
inter-modal ranking constraint is used for embedding, modality-specific
characteristics can be lost. To address this problem, we propose a novel soft
intra-modal structure loss that leverages the relative distance relationship
between intra-modal samples before embedding. We also introduce reasonable
quantitative and qualitative experimental protocols to solve the lack of
standard protocols for less-mature video-music related tasks. Finally, we
construct a large-scale 200K video-music pair benchmark. All the datasets and
source code can be found in our online repository
(https://github.com/csehong/VM-NET).</p>
</td>
    <td>
      
        Datasets 
      
        Scalability 
      
        Evaluation 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/hoang2017selective/">Selective Deep Convolutional Features for Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Selective Deep Convolutional Features for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Selective Deep Convolutional Features for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hoang Tuan, Do Thanh-toan, Tan Dang-khoa Le, Cheung Ngai-man</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 25th ACM international conference on Multimedia</td>
    <td>58</td>
    <td><p>Convolutional Neural Network (CNN) is a very powerful approach to extract
discriminative local descriptors for effective image search. Recent work adopts
fine-tuned strategies to further improve the discriminative power of the
descriptors. Taking a different approach, in this paper, we propose a novel
framework to achieve competitive retrieval performance. Firstly, we propose
various masking schemes, namely SIFT-mask, SUM-mask, and MAX-mask, to select a
representative subset of local convolutional features and remove a large number
of redundant features. We demonstrate that this can effectively address the
burstiness issue and improve retrieval accuracy. Secondly, we propose to employ
recent embedding and aggregating methods to further enhance feature
discriminability. Extensive experiments demonstrate that our proposed framework
achieves state-of-the-art retrieval accuracy.</p>
</td>
    <td>
      
        Evaluation 
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/huang2017cross/">Cross-modal Deep Metric Learning with Multi-task Regularization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-modal Deep Metric Learning with Multi-task Regularization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-modal Deep Metric Learning with Multi-task Regularization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Huang Xin, Peng Yuxin</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Conference on Multimedia and Expo (ICME)</td>
    <td>15</td>
    <td><p>DNN-based cross-modal retrieval has become a research hotspot, by which users
can search results across various modalities like image and text. However,
existing methods mainly focus on the pairwise correlation and reconstruction
error of labeled data. They ignore the semantically similar and dissimilar
constraints between different modalities, and cannot take advantage of
unlabeled data. This paper proposes Cross-modal Deep Metric Learning with
Multi-task Regularization (CDMLMR), which integrates quadruplet ranking loss
and semi-supervised contrastive loss for modeling cross-modal semantic
similarity in a unified multi-task learning architecture. The quadruplet
ranking loss can model the semantically similar and dissimilar constraints to
preserve cross-modal relative similarity ranking information. The
semi-supervised contrastive loss is able to maximize the semantic similarity on
both labeled and unlabeled data. Compared to the existing methods, CDMLMR
exploits not only the similarity ranking information but also unlabeled
cross-modal data, and thus boosts cross-modal retrieval accuracy.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Supervised 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/liu2025discretely/">Discretely Coding Semantic Rank Orders for Supervised Image Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Discretely Coding Semantic Rank Orders for Supervised Image Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Discretely Coding Semantic Rank Orders for Supervised Image Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Li, Shao, Shen, Yu</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>49</td>
    <td><p>Learning to hash has been recognized to accomplish highly efficient storage and retrieval for large-scale visual data. Particularly, ranking-based hashing techniques have recently attracted broad research attention because ranking accuracy among the retrieved data is well explored and their objective is more applicable to realistic search tasks. However, directly optimizing discrete hash codes without continuous-relaxations on a nonlinear ranking objective is infeasible by either traditional optimization methods or even recent discrete hashing algorithms. To address this challenging issue, in this paper, we introduce a novel supervised hashing method, dubbed Discrete Semantic Ranking Hashing (DSeRH), which aims to directly embed semantic rank orders into binary codes. In DSeRH, a generalized Adaptive Discrete Minimization (ADM) approach is proposed to discretely optimize binary codes with the quadratic nonlinear ranking objective in an iterative manner and is guaranteed to converge quickly. Additionally, instead of using 0/1 independent labels to form rank orders as in previous works, we generate the listwise rank orders from the high-level semantic word embeddings which can quantitatively capture the intrinsic correlation between different categories. We evaluate our DSeRH, coupled with both linear and deep convolutional neural network (CNN) hash functions, on three image datasets, i.e., CIFAR-10, SUN397 and ImageNet100, and the results manifest that DSeRH can outperform the state-of-the-art ranking-based hashing methods.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Datasets 
      
        CVPR 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/huang2017unsupervised/">Unsupervised Triplet Hashing for Fast Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Triplet Hashing for Fast Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Triplet Hashing for Fast Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Huang Shanshan, Xiong Yichao, Zhang Ya, Wang Jia</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the on Thematic Workshops of ACM Multimedia 2017</td>
    <td>54</td>
    <td><p>Hashing has played a pivotal role in large-scale image retrieval. With the
development of Convolutional Neural Network (CNN), hashing learning has shown
great promise. But existing methods are mostly tuned for classification, which
are not optimized for retrieval tasks, especially for instance-level retrieval.
In this study, we propose a novel hashing method for large-scale image
retrieval. Considering the difficulty in obtaining labeled datasets for image
retrieval task in large scale, we propose a novel CNN-based unsupervised
hashing method, namely Unsupervised Triplet Hashing (UTH). The unsupervised
hashing network is designed under the following three principles: 1) more
discriminative representations for image retrieval; 2) minimum quantization
loss between the original real-valued feature descriptors and the learned hash
codes; 3) maximum information entropy for the learned hash codes. Extensive
experiments on CIFAR-10, MNIST and In-shop datasets have shown that UTH
outperforms several state-of-the-art unsupervised hashing methods in terms of
retrieval accuracy.</p>
</td>
    <td>
      
        Supervised 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Quantization 
      
        Neural-Hashing 
      
        Unsupervised 
      
        Scalability 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/liu2017discretely/">Discretely Coding Semantic Rank Orders for Supervised Image Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Discretely Coding Semantic Rank Orders for Supervised Image Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Discretely Coding Semantic Rank Orders for Supervised Image Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Li, Shao, Shen, Yu</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>49</td>
    <td><p>Learning to hash has been recognized to accomplish highly efficient storage and retrieval for large-scale visual data. Particularly, ranking-based hashing techniques have recently attracted broad research attention because ranking accuracy among the retrieved data is well explored and their objective is more applicable to realistic search tasks. However, directly optimizing discrete hash codes without continuous-relaxations on a nonlinear ranking objective is infeasible by either traditional optimization methods or even recent discrete hashing algorithms. To address this challenging issue, in this paper, we introduce a novel supervised hashing method, dubbed Discrete Semantic Ranking Hashing (DSeRH), which aims to directly embed semantic rank orders into binary codes. In DSeRH, a generalized Adaptive Discrete Minimization (ADM) approach is proposed to discretely optimize binary codes with the quadratic nonlinear ranking objective in an iterative manner and is guaranteed to converge quickly. Additionally, instead of using 0/1 independent labels to form rank orders as in previous works, we generate the listwise rank orders from the high-level semantic word embeddings which can quantitatively capture the intrinsic correlation between different categories. We evaluate our DSeRH, coupled with both linear and deep convolutional neural network (CNN) hash functions, on three image datasets, i.e., CIFAR-10, SUN397 and ImageNet100, and the results manifest that DSeRH can outperform the state-of-the-art ranking-based hashing methods.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Datasets 
      
        CVPR 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/wang2017survey/">A Survey on Learning to Hash</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Survey on Learning to Hash' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Survey on Learning to Hash' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Jingdong, Zhang, Song, Sebe, Shen</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>929</td>
    <td><p>Nearest neighbor search is a problem of finding the data points from the database such that the distances from them to the
query point are the smallest. Learning to hash is one of the major solutions to this problem and has been widely studied recently. In this
paper, we present a comprehensive survey of the learning to hash algorithms, categorize them according to the manners of preserving
the similarities into: pairwise similarity preserving, multiwise similarity preserving, implicit similarity preserving, as well as quantization,
and discuss their relations. We separate quantization from pairwise similarity preserving as the objective function is very different
though quantization, as we show, can be derived from preserving the pairwise similarities. In addition, we present the evaluation
protocols, and the general performance analysis, and point out that the quantization algori</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Evaluation 
      
        Quantization 
      
        Survey-Paper 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/qi2017efficient/">An efficient deep learning hashing neural network for mobile visual search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=An efficient deep learning hashing neural network for mobile visual search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=An efficient deep learning hashing neural network for mobile visual search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Qi Heng, Liu Wu, Liu Liang</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE Global Conference on Signal and Information Processing (GlobalSIP)</td>
    <td>8</td>
    <td><p>Mobile visual search applications are emerging that enable users to sense
their surroundings with smart phones. However, because of the particular
challenges of mobile visual search, achieving a high recognition bitrate has
becomes a consistent target of previous related works. In this paper, we
propose a few-parameter, low-latency, and high-accuracy deep hashing approach
for constructing binary hash codes for mobile visual search. First, we exploit
the architecture of the MobileNet model, which significantly decreases the
latency of deep feature extraction by reducing the number of model parameters
while maintaining accuracy. Second, we add a hash-like layer into MobileNet to
train the model on labeled mobile visual data. Evaluations show that the
proposed system can exceed state-of-the-art accuracy performance in terms of
the MAP. More importantly, the memory consumption is much less than that of
other deep learning models. The proposed method requires only \(13\) MB of memory
for the neural network and achieves a MAP of \(97.80%\) on the mobile location
recognition dataset used for testing.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/zoran2017learning/">Learning Deep Nearest Neighbor Representations Using Differentiable Boundary Trees</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Deep Nearest Neighbor Representations Using Differentiable Boundary Trees' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Deep Nearest Neighbor Representations Using Differentiable Boundary Trees' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zoran Daniel, Lakshminarayanan Balaji, Blundell Charles</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>9</td>
    <td><p>Nearest neighbor (kNN) methods have been gaining popularity in recent years
in light of advances in hardware and efficiency of algorithms. There is a
plethora of methods to choose from today, each with their own advantages and
disadvantages. One requirement shared between all kNN based methods is the need
for a good representation and distance measure between samples.
  We introduce a new method called differentiable boundary tree which allows
for learning deep kNN representations. We build on the recently proposed
boundary tree algorithm which allows for efficient nearest neighbor
classification, regression and retrieval. By modelling traversals in the tree
as stochastic events, we are able to form a differentiable cost function which
is associated with the tree’s predictions. Using a deep neural network to
transform the data and back-propagating through the tree allows us to learn
good representations for kNN methods.
  We demonstrate that our method is able to learn suitable representations
allowing for very efficient trees with a clearly interpretable structure.</p>
</td>
    <td>
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/wang2016survey/">A Survey on Learning to Hash</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Survey on Learning to Hash' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Survey on Learning to Hash' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Jingdong, Zhang Ting, Song Jingkuan, Sebe Nicu, Shen Heng Tao</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>929</td>
    <td><p>Nearest neighbor search is a problem of finding the data points from the
database such that the distances from them to the query point are the smallest.
Learning to hash is one of the major solutions to this problem and has been
widely studied recently. In this paper, we present a comprehensive survey of
the learning to hash algorithms, categorize them according to the manners of
preserving the similarities into: pairwise similarity preserving, multiwise
similarity preserving, implicit similarity preserving, as well as quantization,
and discuss their relations. We separate quantization from pairwise similarity
preserving as the objective function is very different though quantization, as
we show, can be derived from preserving the pairwise similarities. In addition,
we present the evaluation protocols, and the general performance analysis, and
point out that the quantization algorithms perform superiorly in terms of
search accuracy, search time cost, and space cost. Finally, we introduce a few
emerging topics.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Quantization 
      
        Survey-Paper 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/sablayrolles2016how/">How should we evaluate supervised hashing?</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=How should we evaluate supervised hashing?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=How should we evaluate supervised hashing?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sablayrolles Alexandre, Douze Matthijs, Jégou Hervé, Usunier Nicolas</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</td>
    <td>97</td>
    <td><p>Hashing produces compact representations for documents, to perform tasks like
classification or retrieval based on these short codes. When hashing is
supervised, the codes are trained using labels on the training data. This paper
first shows that the evaluation protocols used in the literature for supervised
hashing are not satisfactory: we show that a trivial solution that encodes the
output of a classifier significantly outperforms existing supervised or
semi-supervised methods, while using much shorter codes. We then propose two
alternative protocols for supervised hashing: one based on retrieval on a
disjoint set of classes, and another based on transfer learning to new classes.
We provide two baseline methods for image-related tasks to assess the
performance of (semi-)supervised hashing: without coding and with unsupervised
codes. These baselines give a lower- and upper-bound on the performance of a
supervised hashing scheme.</p>
</td>
    <td>
      
        Supervised 
      
        ICASSP 
      
        Hashing-Methods 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Unsupervised 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/wang2016deep/">Deep Supervised Hashing with Triplet Labels</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Supervised Hashing with Triplet Labels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Supervised Hashing with Triplet Labels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Xiaofang, Shi Yi, Kitani Kris M.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>192</td>
    <td><p>Hashing is one of the most popular and powerful approximate nearest neighbor
search techniques for large-scale image retrieval. Most traditional hashing
methods first represent images as off-the-shelf visual features and then
produce hashing codes in a separate stage. However, off-the-shelf visual
features may not be optimally compatible with the hash code learning procedure,
which may result in sub-optimal hash codes. Recently, deep hashing methods have
been proposed to simultaneously learn image features and hash codes using deep
neural networks and have shown superior performance over traditional hashing
methods. Most deep hashing methods are given supervised information in the form
of pairwise labels or triplet labels. The current state-of-the-art deep hashing
method DPSH~\cite{li2015feature}, which is based on pairwise labels, performs
image feature learning and hash code learning simultaneously by maximizing the
likelihood of pairwise similarities. Inspired by DPSH~\cite{li2015feature}, we
propose a triplet label based deep hashing method which aims to maximize the
likelihood of the given triplet labels. Experimental results show that our
method outperforms all the baselines on CIFAR-10 and NUS-WIDE datasets,
including the state-of-the-art method DPSH~\cite{li2015feature} and all the
previous triplet label based deep hashing methods.</p>
</td>
    <td>
      
        Supervised 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/liao2018triplet/">Triplet-based Deep Similarity Learning for Person Re-Identification</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Triplet-based Deep Similarity Learning for Person Re-Identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Triplet-based Deep Similarity Learning for Person Re-Identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liao Wentong, Yang Michael Ying, Zhan Ni, Rosenhahn Bodo</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Conference on Computer Vision Workshops (ICCVW)</td>
    <td>32</td>
    <td><p>In recent years, person re-identification (re-id) catches great attention in
both computer vision community and industry. In this paper, we propose a new
framework for person re-identification with a triplet-based deep similarity
learning using convolutional neural networks (CNNs). The network is trained
with triplet input: two of them have the same class labels and the other one is
different. It aims to learn the deep feature representation, with which the
distance within the same class is decreased, while the distance between the
different classes is increased as much as possible. Moreover, we trained the
model jointly on six different datasets, which differs from common practice -
one model is just trained on one dataset and tested also on the same one.
However, the enormous number of possible triplet data among the large number of
training samples makes the training impossible. To address this challenge, a
double-sampling scheme is proposed to generate triplets of images as effective
as possible. The proposed framework is evaluated on several benchmark datasets.
The experimental results show that, our method is effective for the task of
person re-identification and it is comparable or even outperforms the
state-of-the-art methods.</p>
</td>
    <td>
      
        ICCV 
      
        Datasets 
      
        Evaluation 
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/charikar2018hashing/">Hashing-Based-Estimators for Kernel Density in High Dimensions</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing-Based-Estimators for Kernel Density in High Dimensions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing-Based-Estimators for Kernel Density in High Dimensions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Charikar Moses, Siminelakis Paris</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS)</td>
    <td>52</td>
    <td><p>Given a set of points \(P\subset \mathbb{R}^{d}\) and a kernel \(k\), the Kernel
Density Estimate at a point \(x\in\mathbb{R}^{d}\) is defined as
\(\mathrm{KDE}<em>{P}(x)=\frac{1}{|P|}\sum</em>{y\in P} k(x,y)\). We study the problem
of designing a data structure that given a data set \(P\) and a kernel function,
returns <em>approximations to the kernel density</em> of a query point in <em>sublinear
time</em>. We introduce a class of unbiased estimators for kernel density
implemented through locality-sensitive hashing, and give general theorems
bounding the variance of such estimators. These estimators give rise to
efficient data structures for estimating the kernel density in high dimensions
for a variety of commonly used kernels. Our work is the first to provide
data-structures with theoretical guarantees that improve upon simple random
sampling in high dimensions.</p>
</td>
    <td>
      
        Hashing-Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/chaidaroon2025variational/">Variational Deep Semantic Hashing for Text Documents</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Variational Deep Semantic Hashing for Text Documents' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Variational Deep Semantic Hashing for Text Documents' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chaidaroon Suthee, Fang</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>71</td>
    <td><p>As the amount of textual data has been rapidly increasing over
the past decade, efficient similarity search methods have become
a crucial component of large-scale information retrieval systems.
A popular strategy is to represent original data samples by compact binary codes through hashing. A spectrum of machine learning methods have been utilized, but they often lack expressiveness
and flexibility in modeling to learn effective representations. The
recent advances of deep learning in a wide range of applications
has demonstrated its capability to learn robust and powerful feature representations for complex data. Especially, deep generative
models naturally combine the expressiveness of probabilistic generative models with the high capacity of deep neural networks,
which is very suitable for text modeling. However, little work has
leveraged the recent progress in deep learning for text hashing. In this paper, we propose a series of novel deep document generative models for text hashing. The first proposed model is unsupervised while the second one is supervised by utilizing document labels/tags for hashing. The third model further considers document-specific factors that affect the generation of words. The probabilistic generative formulation of the proposed models provides a principled framework for model extension, uncertainty estimation, simulation, and interpretability. Based on variational inference and reparameterization, the proposed models can be interpreted as encoder-decoder deep neural networks and thus they are capable of learning complex nonlinear distributed representations of the original documents. We conduct a comprehensive set of experiments on four public testbeds. The experimental results have demonstrated the effectiveness of the proposed supervised learning models for text hashing.</p>
</td>
    <td>
      
        Scalability 
      
        Text-Retrieval 
      
        Tools-&-Libraries 
      
        SIGIR 
      
        Supervised 
      
        Compact-Codes 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/chandrasekhar2017compression/">Compression of Deep Neural Networks for Image Instance Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Compression of Deep Neural Networks for Image Instance Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Compression of Deep Neural Networks for Image Instance Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chandrasekhar Vijay, Lin Jie, Liao Qianli, Morère Olivier, Veillard Antoine, Duan Lingyu, Poggio Tomaso</td> <!-- 🔧 You were missing this -->
    <td>2017 Data Compression Conference (DCC)</td>
    <td>21</td>
    <td><p>Image instance retrieval is the problem of retrieving images from a database
which contain the same object. Convolutional Neural Network (CNN) based
descriptors are becoming the dominant approach for generating {\it global image
descriptors} for the instance retrieval problem. One major drawback of
CNN-based {\it global descriptors} is that uncompressed deep neural network
models require hundreds of megabytes of storage making them inconvenient to
deploy in mobile applications or in custom hardware. In this work, we study the
problem of neural network model compression focusing on the image instance
retrieval task. We study quantization, coding, pruning and weight sharing
techniques for reducing model size for the instance retrieval problem. We
provide extensive experimental results on the trade-off between retrieval
performance and model size for different types of networks on several data sets
providing the most comprehensive study on this topic. We compress models to the
order of a few MBs: two orders of magnitude smaller than the uncompressed
models while achieving negligible loss in retrieval performance.</p>
</td>
    <td>
      
        Quantization 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/chaidaroon2017variational/">Variational Deep Semantic Hashing for Text Documents</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Variational Deep Semantic Hashing for Text Documents' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Variational Deep Semantic Hashing for Text Documents' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Chaidaroon Suthee, Fang</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>71</td>
    <td><p>As the amount of textual data has been rapidly increasing over
the past decade, efficient similarity search methods have become
a crucial component of large-scale information retrieval systems.
A popular strategy is to represent original data samples by compact binary codes through hashing. A spectrum of machine learning methods have been utilized, but they often lack expressiveness
and flexibility in modeling to learn effective representations. The
recent advances of deep learning in a wide range of applications
has demonstrated its capability to learn robust and powerful feature representations for complex data. Especially, deep generative
models naturally combine the expressiveness of probabilistic generative models with the high capacity of deep neural networks,
which is very suitable for text modeling. However, little work has
leveraged the recent progress in deep learning for text hashing. In this paper, we propose a series of novel deep document generative models for text hashing. The first proposed model is unsupervised while the second one is supervised by utilizing document labels/tags for hashing. The third model further considers document-specific factors that affect the generation of words. The probabilistic generative formulation of the proposed models provides a principled framework for model extension, uncertainty estimation, simulation, and interpretability. Based on variational inference and reparameterization, the proposed models can be interpreted as encoder-decoder deep neural networks and thus they are capable of learning complex nonlinear distributed representations of the original documents. We conduct a comprehensive set of experiments on four public testbeds. The experimental results have demonstrated the effectiveness of the proposed supervised learning models for text hashing.</p>
</td>
    <td>
      
        Scalability 
      
        Text-Retrieval 
      
        Tools-&-Libraries 
      
        SIGIR 
      
        Supervised 
      
        Compact-Codes 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/cao2025collective/">Collective Deep Quantization for Efficient Cross-Modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Collective Deep Quantization for Efficient Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Collective Deep Quantization for Efficient Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cao Yue, Long, Wang, Liu</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>97</td>
    <td><p>Cross-modal similarity retrieval is a problem about designing a retrieval system that supports querying across
content modalities, e.g., using an image to retrieve for
texts. This paper presents a compact coding solution for
efficient cross-modal retrieval, with a focus on the quantization approach which has already shown the superior
performance over the hashing solutions in single-modal
similarity retrieval. We propose a collective deep quantization (CDQ) approach, which is the first attempt to
introduce quantization in end-to-end deep architecture
for cross-modal retrieval. The major contribution lies in
jointly learning deep representations and the quantizers
for both modalities using carefully-crafted hybrid networks and well-specified loss functions. In addition, our
approach simultaneously learns the common quantizer
codebook for both modalities through which the crossmodal correlation can be substantially enhanced. CDQ
enables efficient and effective cross-modal retrieval using inner product distance computed based on the common codebook with fast distance table lookup. Extensive experiments show that CDQ yields state of the art
cross-modal retrieval results on standard benchmarks.</p>
</td>
    <td>
      
        Multimodal-Retrieval 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        AAAI 
      
        Evaluation 
      
        Quantization 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/laarhoven2017graph/">Graph-based time-space trade-offs for approximate near neighbors</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Graph-based time-space trade-offs for approximate near neighbors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Graph-based time-space trade-offs for approximate near neighbors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Laarhoven Thijs</td> <!-- 🔧 You were missing this -->
    <td>34th International Symposium on Computational Geometry (SoCG) pp. 571-5714 2018</td>
    <td>5</td>
    <td><p>We take a first step towards a rigorous asymptotic analysis of graph-based
approaches for finding (approximate) nearest neighbors in high-dimensional
spaces, by analyzing the complexity of (randomized) greedy walks on the
approximate near neighbor graph. For random data sets of size \(n = 2^{o(d)}\) on
the \(d\)-dimensional Euclidean unit sphere, using near neighbor graphs we can
provably solve the approximate nearest neighbor problem with approximation
factor \(c &gt; 1\) in query time \(n^{\rho_q + o(1)}\) and space \(n^{1 + \rho_s +
o(1)}\), for arbitrary \(\rho_q, \rho_s \geq 0\) satisfying \begin{align} (2c^2 -
1) \rho_q + 2 c^2 (c^2 - 1) \sqrt{\rho_s (1 - \rho_s)} \geq c^4. \end{align}
Graph-based near neighbor searching is especially competitive with hash-based
methods for small \(c\) and near-linear memory, and in this regime the asymptotic
scaling of a greedy graph-based search matches the recent optimal hash-based
trade-offs of Andoni-Laarhoven-Razenshteyn-Waingarten [SODA’17]. We further
study how the trade-offs scale when the data set is of size \(n =
2^{\Theta(d)}\), and analyze asymptotic complexities when applying these results
to lattice sieving.</p>
</td>
    <td>
      
        Graph-Based-ANN 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/li2017deep/">Deep Unsupervised Image Hashing by Maximizing Bit Entropy</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Unsupervised Image Hashing by Maximizing Bit Entropy' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Unsupervised Image Hashing by Maximizing Bit Entropy' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Yunqiang, Gemert</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>171</td>
    <td><p>Unsupervised hashing is important for indexing huge image or video collections without having expensive annotations available. Hashing aims to learn short binary codes for compact storage and efficient semantic retrieval. We propose an unsupervised deep hashing layer called Bi-half Net that maximizes entropy of the binary codes. Entropy is maximal when both possible values of the bit are uniformly (half-half) distributed. To maximize bit entropy, we do not add a term to the loss function as this is difficult to optimize and tune. Instead, we design a new parameter-free network layer to explicitly force continuous image features to approximate the optimal half-half bit distribution. This layer is shown to minimize a penalized term of the Wasserstein distance between the learned continuous image features and the optimal half-half bit distribution. Experimental results on the image datasets Flickr25k, Nus-wide, Cifar-10, Mscoco, Mnist and the video datasets Ucf-101 and Hmdb-51 show that our approach leads to compact codes and compares favorably to the current state-of-the-art.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Datasets 
      
        Neural-Hashing 
      
        Supervised 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/cao2016transitive/">Transitive Hashing Network for Heterogeneous Multimedia Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Transitive Hashing Network for Heterogeneous Multimedia Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Transitive Hashing Network for Heterogeneous Multimedia Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cao Zhangjie, Long Mingsheng, Yang Qiang</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>25</td>
    <td><p>Hashing has been widely applied to large-scale multimedia retrieval due to
the storage and retrieval efficiency. Cross-modal hashing enables efficient
retrieval from database of one modality in response to a query of another
modality. Existing work on cross-modal hashing assumes heterogeneous
relationship across modalities for hash function learning. In this paper, we
relax the strong assumption by only requiring such heterogeneous relationship
in an auxiliary dataset different from the query/database domain. We craft a
hybrid deep architecture to simultaneously learn the cross-modal correlation
from the auxiliary dataset, and align the dataset distributions between the
auxiliary dataset and the query/database domain, which generates transitive
hash codes for heterogeneous multimedia retrieval. Extensive experiments
exhibit that the proposed approach yields state of the art multimedia retrieval
performance on public datasets, i.e. NUS-WIDE, ImageNet-YahooQA.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        AAAI 
      
        Scalability 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/cao2017collective/">Collective Deep Quantization for Efficient Cross-Modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Collective Deep Quantization for Efficient Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Collective Deep Quantization for Efficient Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cao Yue, Long, Wang, Liu</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>97</td>
    <td><p>Cross-modal similarity retrieval is a problem about designing a retrieval system that supports querying across
content modalities, e.g., using an image to retrieve for
texts. This paper presents a compact coding solution for
efficient cross-modal retrieval, with a focus on the quantization approach which has already shown the superior
performance over the hashing solutions in single-modal
similarity retrieval. We propose a collective deep quantization (CDQ) approach, which is the first attempt to
introduce quantization in end-to-end deep architecture
for cross-modal retrieval. The major contribution lies in
jointly learning deep representations and the quantizers
for both modalities using carefully-crafted hybrid networks and well-specified loss functions. In addition, our
approach simultaneously learns the common quantizer
codebook for both modalities through which the crossmodal correlation can be substantially enhanced. CDQ
enables efficient and effective cross-modal retrieval using inner product distance computed based on the common codebook with fast distance table lookup. Extensive experiments show that CDQ yields state of the art
cross-modal retrieval results on standard benchmarks.</p>
</td>
    <td>
      
        Multimodal-Retrieval 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        AAAI 
      
        Evaluation 
      
        Quantization 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/mor%C3%A8re2016nested/">Nested Invariance Pooling and RBM Hashing for Image Instance Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Nested Invariance Pooling and RBM Hashing for Image Instance Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Nested Invariance Pooling and RBM Hashing for Image Instance Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Morère Olivier, Lin Jie, Veillard Antoine, Chandrasekhar Vijay, Poggio Tomaso</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval</td>
    <td>18</td>
    <td><p>The goal of this work is the computation of very compact binary hashes for
image instance retrieval. Our approach has two novel contributions. The first
one is Nested Invariance Pooling (NIP), a method inspired from i-theory, a
mathematical theory for computing group invariant transformations with
feed-forward neural networks. NIP is able to produce compact and
well-performing descriptors with visual representations extracted from
convolutional neural networks. We specifically incorporate scale, translation
and rotation invariances but the scheme can be extended to any arbitrary sets
of transformations. We also show that using moments of increasing order
throughout nesting is important. The NIP descriptors are then hashed to the
target code size (32-256 bits) with a Restricted Boltzmann Machine with a novel
batch-level regularization scheme specifically designed for the purpose of
hashing (RBMH). A thorough empirical evaluation with state-of-the-art shows
that the results obtained both with the NIP descriptors and the NIP+RBMH hashes
are consistently outstanding across a wide range of datasets.</p>
</td>
    <td>
      
        Evaluation 
      
        Datasets 
      
        Hashing-Methods 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/cai2017revisit/">A Revisit on Deep Hashings for Large-scale Content Based Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Revisit on Deep Hashings for Large-scale Content Based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Revisit on Deep Hashings for Large-scale Content Based Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cai Deng, Gu Xiuye, Wang Chaoqi</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>13</td>
    <td><p>There is a growing trend in studying deep hashing methods for content-based
image retrieval (CBIR), where hash functions and binary codes are learnt using
deep convolutional neural networks and then the binary codes can be used to do
approximate nearest neighbor (ANN) search. All the existing deep hashing papers
report their methods’ superior performance over the traditional hashing methods
according to their experimental results. However, there are serious flaws in
the evaluations of existing deep hashing papers: (1) The datasets they used are
too small and simple to simulate the real CBIR situation. (2) They did not
correctly include the search time in their evaluation criteria, while the
search time is crucial in real CBIR systems. (3) The performance of some
unsupervised hashing algorithms (e.g., LSH) can easily be boosted if one uses
multiple hash tables, which is an important factor should be considered in the
evaluation while most of the deep hashing papers failed to do so.
  We re-evaluate several state-of-the-art deep hashing methods with a carefully
designed experimental setting. Empirical results reveal that the performance of
these deep hashing methods are inferior to multi-table IsoH, a very simple
unsupervised hashing method. Thus, the conclusions in all the deep hashing
papers should be carefully re-examined.</p>
</td>
    <td>
      
        Supervised 
      
        Locality-Sensitive-Hashing 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Unsupervised 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/calixto2017multilingual/">Multilingual Multi-modal Embeddings for Natural Language Processing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multilingual Multi-modal Embeddings for Natural Language Processing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multilingual Multi-modal Embeddings for Natural Language Processing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Calixto Iacer, Liu Qun, Campbell Nick</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>15</td>
    <td><p>We propose a novel discriminative model that learns embeddings from
multilingual and multi-modal data, meaning that our model can take advantage of
images and descriptions in multiple languages to improve embedding quality. To
that end, we introduce a modification of a pairwise contrastive estimation
optimisation function as our training objective. We evaluate our embeddings on
an image-sentence ranking (ISR), a semantic textual similarity (STS), and a
neural machine translation (NMT) task. We find that the additional multilingual
signals lead to improvements on both the ISR and STS tasks, and the
discriminative cost can also be used in re-ranking \(n\)-best lists produced by
NMT models, yielding strong improvements.</p>
</td>
    <td>
      
        Hybrid-ANN-Methods 
      
        Re-Ranking 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/li2017fast/">Fast k-Nearest Neighbour Search via Prioritized DCI</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast k-Nearest Neighbour Search via Prioritized DCI' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast k-Nearest Neighbour Search via Prioritized DCI' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Ke, Malik Jitendra</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>14</td>
    <td><p>Most exact methods for k-nearest neighbour search suffer from the curse of
dimensionality; that is, their query times exhibit exponential dependence on
either the ambient or the intrinsic dimensionality. Dynamic Continuous Indexing
(DCI) offers a promising way of circumventing the curse and successfully
reduces the dependence of query time on intrinsic dimensionality from
exponential to sublinear. In this paper, we propose a variant of DCI, which we
call Prioritized DCI, and show a remarkable improvement in the dependence of
query time on intrinsic dimensionality. In particular, a linear increase in
intrinsic dimensionality, or equivalently, an exponential increase in the
number of points near a query, can be mostly counteracted with just a linear
increase in space. We also demonstrate empirically that Prioritized DCI
significantly outperforms prior methods. In particular, relative to
Locality-Sensitive Hashing (LSH), Prioritized DCI reduces the number of
distance evaluations by a factor of 14 to 116 and the memory consumption by a
factor of 21.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Similarity-Search 
      
        Locality-Sensitive-Hashing 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/noh2016large/">Large-Scale Image Retrieval with Attentive Deep Local Features</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Large-Scale Image Retrieval with Attentive Deep Local Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Large-Scale Image Retrieval with Attentive Deep Local Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Noh Hyeonwoo, Araujo Andre, Sim Jack, Weyand Tobias, Han Bohyung</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Conference on Computer Vision (ICCV)</td>
    <td>812</td>
    <td><p>We propose an attentive local feature descriptor suitable for large-scale
image retrieval, referred to as DELF (DEep Local Feature). The new feature is
based on convolutional neural networks, which are trained only with image-level
annotations on a landmark image dataset. To identify semantically useful local
features for image retrieval, we also propose an attention mechanism for
keypoint selection, which shares most network layers with the descriptor. This
framework can be used for image retrieval as a drop-in replacement for other
keypoint detectors and descriptors, enabling more accurate feature matching and
geometric verification. Our system produces reliable confidence scores to
reject false positives—in particular, it is robust against queries that have
no correct match in the database. To evaluate the proposed descriptor, we
introduce a new large-scale dataset, referred to as Google-Landmarks dataset,
which involves challenges in both database and query such as background
clutter, partial occlusion, multiple landmarks, objects in variable scales,
etc. We show that DELF outperforms the state-of-the-art global and local
descriptors in the large-scale setting by significant margins. Code and dataset
can be found at the project webpage:
https://github.com/tensorflow/models/tree/master/research/delf .</p>
</td>
    <td>
      
        ICCV 
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
        Datasets 
      
        Scalability 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/vu2016search/">Search Personalization with Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Search Personalization with Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Search Personalization with Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Vu Thanh, Nguyen Dat Quoc, Johnson Mark, Song Dawei, Willis Alistair</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>48</td>
    <td><p>Recent research has shown that the performance of search personalization
depends on the richness of user profiles which normally represent the user’s
topical interests. In this paper, we propose a new embedding approach to
learning user profiles, where users are embedded on a topical interest space.
We then directly utilize the user profiles for search personalization.
Experiments on query logs from a major commercial web search engine demonstrate
that our embedding approach improves the performance of the search engine and
also achieves better search performance than other strong baselines.</p>
</td>
    <td>
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/ong2017siamese/">Siamese Network of Deep Fisher-Vector Descriptors for Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Siamese Network of Deep Fisher-Vector Descriptors for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Siamese Network of Deep Fisher-Vector Descriptors for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ong Eng-jon, Husain Sameed, Bober Miroslaw</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>37</td>
    <td><p>This paper addresses the problem of large scale image retrieval, with the aim
of accurately ranking the similarity of a large number of images to a given
query image. To achieve this, we propose a novel Siamese network. This network
consists of two computational strands, each comprising of a CNN component
followed by a Fisher vector component. The CNN component produces dense, deep
convolutional descriptors that are then aggregated by the Fisher Vector method.
Crucially, we propose to simultaneously learn both the CNN filter weights and
Fisher Vector model parameters. This allows us to account for the evolving
distribution of deep descriptors over the course of the learning process. We
show that the proposed approach gives significant improvements over the
state-of-the-art methods on the Oxford and Paris image retrieval datasets.
Additionally, we provide a baseline performance measure for both these datasets
with the inclusion of 1 million distractors.</p>
</td>
    <td>
      
        Datasets 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/oymak2017near/">Near-Optimal Sample Complexity Bounds for Circulant Binary Embedding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Near-Optimal Sample Complexity Bounds for Circulant Binary Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Near-Optimal Sample Complexity Bounds for Circulant Binary Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Oymak Samet</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</td>
    <td>13</td>
    <td><p>Binary embedding is the problem of mapping points from a high-dimensional
space to a Hamming cube in lower dimension while preserving pairwise distances.
An efficient way to accomplish this is to make use of fast embedding techniques
involving Fourier transform e.g.~circulant matrices. While binary embedding has
been studied extensively, theoretical results on fast binary embedding are
rather limited. In this work, we build upon the recent literature to obtain
significantly better dependencies on the problem parameters. A set of \(N\)
points in \(\mathbb{R}^n\) can be properly embedded into the Hamming cube \(\{\pm
1\}^k\) with \(\delta\) distortion, by using \(k\sim\delta^{-3}log N\) samples
which is optimal in the number of points \(N\) and compares well with the optimal
distortion dependency \(\delta^{-2}\). Our optimal embedding result applies in
the regime \(log N\lesssim n^{1/3}\). Furthermore, if the looser condition \(log
N\lesssim \sqrt{n}\) holds, we show that all but an arbitrarily small fraction
of the points can be optimally embedded. We believe our techniques can be
useful to obtain improved guarantees for other nonlinear embedding problems.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        ICASSP 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/oymak2016near/">Near-Optimal Sample Complexity Bounds for Circulant Binary Embedding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Near-Optimal Sample Complexity Bounds for Circulant Binary Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Near-Optimal Sample Complexity Bounds for Circulant Binary Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Oymak Samet</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</td>
    <td>13</td>
    <td><p>Binary embedding is the problem of mapping points from a high-dimensional
space to a Hamming cube in lower dimension while preserving pairwise distances.
An efficient way to accomplish this is to make use of fast embedding techniques
involving Fourier transform e.g.~circulant matrices. While binary embedding has
been studied extensively, theoretical results on fast binary embedding are
rather limited. In this work, we build upon the recent literature to obtain
significantly better dependencies on the problem parameters. A set of \(N\)
points in \(\mathbb{R}^n\) can be properly embedded into the Hamming cube \(\{\pm
1\}^k\) with \(\delta\) distortion, by using \(k\sim\delta^{-3}log N\) samples
which is optimal in the number of points \(N\) and compares well with the optimal
distortion dependency \(\delta^{-2}\). Our optimal embedding result applies in
the regime \(log N\lesssim n^{1/3}\). Furthermore, if the looser condition \(log
N\lesssim \sqrt{n}\) holds, we show that all but an arbitrarily small fraction
of the points can be optimally embedded. We believe our techniques can be
useful to obtain improved guarantees for other nonlinear embedding problems.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        ICASSP 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/appalaraju2017image/">Image similarity using Deep CNN and Curriculum Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Image similarity using Deep CNN and Curriculum Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Image similarity using Deep CNN and Curriculum Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Appalaraju Srikar, Chaoji Vineet</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>58</td>
    <td><p>Image similarity involves fetching similar looking images given a reference
image. Our solution called SimNet, is a deep siamese network which is trained
on pairs of positive and negative images using a novel online pair mining
strategy inspired by Curriculum learning. We also created a multi-scale CNN,
where the final image embedding is a joint representation of top as well as
lower layer embedding’s. We go on to show that this multi-scale siamese network
is better at capturing fine grained image similarities than traditional CNN’s.</p>
</td>
    <td>
      
        Uncategorized 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/andr%C3%A92017accelerated/">Accelerated Nearest Neighbor Search with Quick ADC</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Accelerated Nearest Neighbor Search with Quick ADC' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Accelerated Nearest Neighbor Search with Quick ADC' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>André Fabien Technicolor, Kermarrec Anne-marie Inria, Scouarnec Nicolas Le Technicolor</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval</td>
    <td>14</td>
    <td><p>Efficient Nearest Neighbor (NN) search in high-dimensional spaces is a
foundation of many multimedia retrieval systems. Because it offers low
responses times, Product Quantization (PQ) is a popular solution. PQ compresses
high-dimensional vectors into short codes using several sub-quantizers, which
enables in-RAM storage of large databases. This allows fast answers to NN
queries, without accessing the SSD or HDD. The key feature of PQ is that it can
compute distances between short codes and high-dimensional vectors using
cache-resident lookup tables. The efficiency of this technique, named
Asymmetric Distance Computation (ADC), remains limited because it performs many
cache accesses.
  In this paper, we introduce Quick ADC, a novel technique that achieves a 3 to
6 times speedup over ADC by exploiting Single Instruction Multiple Data (SIMD)
units available in current CPUs. Efficiently exploiting SIMD requires
algorithmic changes to the ADC procedure. Namely, Quick ADC relies on two key
modifications of ADC: (i) the use 4-bit sub-quantizers instead of the standard
8-bit sub-quantizers and (ii) the quantization of floating-point distances.
This allows Quick ADC to exceed the performance of state-of-the-art systems,
e.g., it achieves a Recall@100 of 0.94 in 3.4 ms on 1 billion SIFT descriptors
(128-bit codes).</p>
</td>
    <td>
      
        Quantization 
      
        Evaluation 
      
        Efficiency 
      
        Compact-Codes 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/andoni2016optimal/">Optimal Hashing-based Time-Space Trade-offs for Approximate Near Neighbors</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Optimal Hashing-based Time-Space Trade-offs for Approximate Near Neighbors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Optimal Hashing-based Time-Space Trade-offs for Approximate Near Neighbors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Andoni Alexandr, Laarhoven Thijs, Razenshteyn Ilya, Waingarten Erik</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>66</td>
    <td><p>[See the paper for the full abstract.]
  We show tight upper and lower bounds for time-space trade-offs for the
\(c\)-Approximate Near Neighbor Search problem. For the \(d\)-dimensional Euclidean
space and \(n\)-point datasets, we develop a data structure with space \(n^{1 +
\rho_u + o(1)} + O(dn)\) and query time \(n^{\rho_q + o(1)} + d n^{o(1)}\) for
every \(\rho_u, \rho_q \geq 0\) such that: \begin{equation} c^2 \sqrt{\rho_q} +
(c^2 - 1) \sqrt{\rho_u} = \sqrt{2c^2 - 1}. \end{equation}
  This is the first data structure that achieves sublinear query time and
near-linear space for every approximation factor \(c &gt; 1\), improving upon
[Kapralov, PODS 2015]. The data structure is a culmination of a long line of
work on the problem for all space regimes; it builds on Spherical
Locality-Sensitive Filtering [Becker, Ducas, Gama, Laarhoven, SODA 2016] and
data-dependent hashing [Andoni, Indyk, Nguyen, Razenshteyn, SODA 2014] [Andoni,
Razenshteyn, STOC 2015].
  Our matching lower bounds are of two types: conditional and unconditional.
First, we prove tightness of the whole above trade-off in a restricted model of
computation, which captures all known hashing-based approaches. We then show
unconditional cell-probe lower bounds for one and two probes that match the
above trade-off for \(\rho_q = 0\), improving upon the best known lower bounds
from [Panigrahy, Talwar, Wieder, FOCS 2010]. In particular, this is the first
space lower bound (for any static data structure) for two probes which is not
polynomially smaller than the one-probe bound. To show the result for two
probes, we establish and exploit a connection to locally-decodable codes.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/zhong2017re/">Re-ranking Person Re-identification with k-reciprocal Encoding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Re-ranking Person Re-identification with k-reciprocal Encoding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Re-ranking Person Re-identification with k-reciprocal Encoding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhong Zhun, Zheng Liang, Cao Donglin, Li Shaozi</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>1369</td>
    <td><p>When considering person re-identification (re-ID) as a retrieval process,
re-ranking is a critical step to improve its accuracy. Yet in the re-ID
community, limited effort has been devoted to re-ranking, especially those
fully automatic, unsupervised solutions. In this paper, we propose a
k-reciprocal encoding method to re-rank the re-ID results. Our hypothesis is
that if a gallery image is similar to the probe in the k-reciprocal nearest
neighbors, it is more likely to be a true match. Specifically, given an image,
a k-reciprocal feature is calculated by encoding its k-reciprocal nearest
neighbors into a single vector, which is used for re-ranking under the Jaccard
distance. The final distance is computed as the combination of the original
distance and the Jaccard distance. Our re-ranking method does not require any
human interaction or any labeled data, so it is applicable to large-scale
datasets. Experiments on the large-scale Market-1501, CUHK03, MARS, and PRW
datasets confirm the effectiveness of our method.</p>
</td>
    <td>
      
        Datasets 
      
        CVPR 
      
        Hybrid-ANN-Methods 
      
        Re-Ranking 
      
        Unsupervised 
      
        Scalability 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/ahle2017optimal/">Optimal Las Vegas Locality Sensitive Data Structures</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Optimal Las Vegas Locality Sensitive Data Structures' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Optimal Las Vegas Locality Sensitive Data Structures' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ahle Thomas Dybdahl</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS)</td>
    <td>13</td>
    <td><p>We show that approximate similarity (near neighbour) search can be solved in
high dimensions with performance matching state of the art (data independent)
Locality Sensitive Hashing, but with a guarantee of no false negatives.
  Specifically, we give two data structures for common problems.
  For \(c\)-approximate near neighbour in Hamming space we get query time
\(dn^{1/c+o(1)}\) and space \(dn^{1+1/c+o(1)}\) matching that of
\cite{indyk1998approximate} and answering a long standing open question
from~\cite{indyk2000dimensionality} and~\cite{pagh2016locality} in the
affirmative.
  By means of a new deterministic reduction from \(\ell_1\) to Hamming we also
solve \(\ell_1\) and \(ℓ₂\) with query time \(d^2n^{1/c+o(1)}\) and space \(d^2
n^{1+1/c+o(1)}\).
  For \((s_1,s_2)\)-approximate Jaccard similarity we get query time
\(dn^{\rho+o(1)}\) and space \(dn^{1+\rho+o(1)}\),
\(\rho=log\frac{1+s_1}{2s_1}\big/log\frac{1+s_2}{2s_2}\), when sets have equal
size, matching the performance of~\cite{tobias2016}.
  The algorithms are based on space partitions, as with classic LSH, but we
construct these using a combination of brute force, tensoring, perfect hashing
and splitter functions `a la~\cite{naor1995splitters}. We also show a new
dimensionality reduction lemma with 1-sided error.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Evaluation 
      
        Locality-Sensitive-Hashing 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/shrivastava2017optimal/">Optimal Densification for Fast and Accurate Minwise Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Optimal Densification for Fast and Accurate Minwise Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Optimal Densification for Fast and Accurate Minwise Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shrivastava Anshumali</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>27</td>
    <td><p>Minwise hashing is a fundamental and one of the most successful hashing
algorithm in the literature. Recent advances based on the idea of
densification~\cite{Proc:OneHashLSH_ICML14,Proc:Shrivastava_UAI14} have shown
that it is possible to compute \(k\) minwise hashes, of a vector with \(d\)
nonzeros, in mere \((d + k)\) computations, a significant improvement over the
classical \(O(dk)\). These advances have led to an algorithmic improvement in the
query complexity of traditional indexing algorithms based on minwise hashing.
Unfortunately, the variance of the current densification techniques is
unnecessarily high, which leads to significantly poor accuracy compared to
vanilla minwise hashing, especially when the data is sparse. In this paper, we
provide a novel densification scheme which relies on carefully tailored
2-universal hashes. We show that the proposed scheme is variance-optimal, and
without losing the runtime efficiency, it is significantly more accurate than
existing densification techniques. As a result, we obtain a significantly
efficient hashing scheme which has the same variance and collision probability
as minwise hashing. Experimental evaluations on real sparse and
high-dimensional datasets validate our claims. We believe that given the
significant advantages, our method will replace minwise hashing implementations
in practice.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/ahle2016parameter/">Parameter-free Locality Sensitive Hashing for Spherical Range Reporting</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Parameter-free Locality Sensitive Hashing for Spherical Range Reporting' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Parameter-free Locality Sensitive Hashing for Spherical Range Reporting' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ahle Thomas D., Aumüller Martin, Pagh Rasmus</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>19</td>
    <td><p>We present a data structure for <em>spherical range reporting</em> on a point set
\(S\), i.e., reporting all points in \(S\) that lie within radius \(r\) of a given
query point \(q\). Our solution builds upon the Locality-Sensitive Hashing (LSH)
framework of Indyk and Motwani, which represents the asymptotically best
solutions to near neighbor problems in high dimensions. While traditional LSH
data structures have several parameters whose optimal values depend on the
distance distribution from \(q\) to the points of \(S\), our data structure is
parameter-free, except for the space usage, which is configurable by the user.
Nevertheless, its expected query time basically matches that of an LSH data
structure whose parameters have been <em>optimally chosen for the data and query</em>
in question under the given space constraints. In particular, our data
structure provides a smooth trade-off between hard queries (typically addressed
by standard LSH) and easy queries such as those where the number of points to
report is a constant fraction of \(S\), or where almost all points in \(S\) are far
away from the query point. In contrast, known data structures fix LSH
parameters based on certain parameters of the input alone.
  The algorithm has expected query time bounded by \(O(t (n/t)^\rho)\), where \(t\)
is the number of points to report and \(\rho\in (0,1)\) depends on the data
distribution and the strength of the LSH family used. We further present a
parameter-free way of using multi-probing, for LSH families that support it,
and show that for many such families this approach allows us to get expected
query time close to \(O(n^\rho+t)\), which is the best we can hope to achieve
using LSH. The previously best running time in high dimensions was \(Ω(t
n^\rho)\). For many data distributions where the intrinsic dimensionality of the
point set close to \(q\) is low, we can give improved upper bounds on the
expected query time.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Locality-Sensitive-Hashing 
      
        Tools-&-Libraries 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/amato2016aggregating/">Aggregating Binary Local Descriptors for Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Aggregating Binary Local Descriptors for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Aggregating Binary Local Descriptors for Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Amato Giuseppe, Falchi Fabrizio, Vadicamo Lucia</td> <!-- 🔧 You were missing this -->
    <td>Multimedia Tools and Applications</td>
    <td>11</td>
    <td><p>Content-Based Image Retrieval based on local features is computationally
expensive because of the complexity of both extraction and matching of local
feature. On one hand, the cost for extracting, representing, and comparing
local visual descriptors has been dramatically reduced by recently proposed
binary local features. On the other hand, aggregation techniques provide a
meaningful summarization of all the extracted feature of an image into a single
descriptor, allowing us to speed up and scale up the image search. Only a few
works have recently mixed together these two research directions, defining
aggregation methods for binary local features, in order to leverage on the
advantage of both approaches. In this paper, we report an extensive comparison
among state-of-the-art aggregation methods applied to binary features. Then, we
mathematically formalize the application of Fisher Kernels to Bernoulli Mixture
Models. Finally, we investigate the combination of the aggregated binary
features with the emerging Convolutional Neural Network (CNN) features. Our
results show that aggregation methods on binary features are effective and
represent a worthwhile alternative to the direct matching. Moreover, the
combination of the CNN with the Fisher Vector (FV) built upon binary features
allowed us to obtain a relative improvement over the CNN results that is in
line with that recently obtained using the combination of the CNN with the FV
built upon SIFTs. The advantage of using the FV built upon binary features is
that the extraction process of binary features is about two order of magnitude
faster than SIFTs.</p>
</td>
    <td>
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/zhang2017hashgan/">HashGAN:Attention-aware Deep Adversarial Hashing for Cross Modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=HashGAN:Attention-aware Deep Adversarial Hashing for Cross Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=HashGAN:Attention-aware Deep Adversarial Hashing for Cross Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Xi, Zhou Siyu, Feng Jiashi, Lai Hanjiang, Li Bo, Pan Yan, Yin Jian, Yan Shuicheng</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>16</td>
    <td><p>As the rapid growth of multi-modal data, hashing methods for cross-modal
retrieval have received considerable attention. Deep-networks-based cross-modal
hashing methods are appealing as they can integrate feature learning and hash
coding into end-to-end trainable frameworks. However, it is still challenging
to find content similarities between different modalities of data due to the
heterogeneity gap. To further address this problem, we propose an adversarial
hashing network with attention mechanism to enhance the measurement of content
similarities by selectively focusing on informative parts of multi-modal data.
The proposed new adversarial network, HashGAN, consists of three building
blocks: 1) the feature learning module to obtain feature representations, 2)
the generative attention module to generate an attention mask, which is used to
obtain the attended (foreground) and the unattended (background) feature
representations, 3) the discriminative hash coding module to learn hash
functions that preserve the similarities between different modalities. In our
framework, the generative module and the discriminative module are trained in
an adversarial way: the generator is learned to make the discriminator cannot
preserve the similarities of multi-modal data w.r.t. the background feature
representations, while the discriminator aims to preserve the similarities of
multi-modal data w.r.t. both the foreground and the background feature
representations. Extensive evaluations on several benchmark datasets
demonstrate that the proposed HashGAN brings substantial improvements over
other state-of-the-art cross-modal hashing methods.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Datasets 
      
        Evaluation 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/wu2025deep/">Deep Supervised Hashing for Multi-Label and Large-Scale Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Supervised Hashing for Multi-Label and Large-Scale Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Supervised Hashing for Multi-Label and Large-Scale Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wu Dayan, Lin, Li, Ye, Wang</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval</td>
    <td>67</td>
    <td><p>One of the most challenging tasks in large-scale multi-label image retrieval is to map images into binary codes while preserving multilevel semantic similarity. Recently, several deep supervised hashing methods have been proposed to learn hash functions that preserve multilevel semantic similarity with deep convolutional neural networks. However, these triplet label based methods try to preserve the ranking order of images according to their similarity degrees to the queries while not putting direct constraints on the distance between the codes of very similar images. Besides, the current evaluation criteria are not able to measure the performance of existing hashing methods on preserving fine-grained multilevel semantic similarity. To tackle these issues, we propose a novel Deep Multilevel Semantic Similarity Preserving Hashing (DMSSPH) method to learn compact similarity-preserving binary codes for the huge body of multi-label image data with deep convolutional neural networks. In our approach, we make the best of the supervised information in the form of pairwise labels to maximize the discriminability of output binary codes. Extensive evaluations conducted on several benchmark datasets demonstrate that the proposed method significantly outperforms the state-of-the-art supervised and unsupervised hashing methods at the accuracies of top returned images, especially for shorter binary codes. Meanwhile, the proposed method shows better performance on preserving fine-grained multilevel semantic similarity according to the results under the Jaccard coefficient based evaluation criteria we propose.</p>
</td>
    <td>
      
        Scalability 
      
        Evaluation 
      
        Datasets 
      
        Unsupervised 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Multimodal-Retrieval 
      
        Neural-Hashing 
      
        Image-Retrieval 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/wang2025survey/">A Survey on Learning to Hash</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Survey on Learning to Hash' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Survey on Learning to Hash' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Jingdong, Zhang, Song, Sebe, Shen</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>929</td>
    <td><p>Nearest neighbor search is a problem of finding the data points from the database such that the distances from them to the
query point are the smallest. Learning to hash is one of the major solutions to this problem and has been widely studied recently. In this
paper, we present a comprehensive survey of the learning to hash algorithms, categorize them according to the manners of preserving
the similarities into: pairwise similarity preserving, multiwise similarity preserving, implicit similarity preserving, as well as quantization,
and discuss their relations. We separate quantization from pairwise similarity preserving as the objective function is very different
though quantization, as we show, can be derived from preserving the pairwise similarities. In addition, we present the evaluation
protocols, and the general performance analysis, and point out that the quantization algori</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Evaluation 
      
        Quantization 
      
        Survey-Paper 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/liu2016ordinal/">Ordinal Constrained Binary Code Learning for Nearest Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Ordinal Constrained Binary Code Learning for Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Ordinal Constrained Binary Code Learning for Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Hong, Ji Rongrong, Wu Yongjian, Huang Feiyue</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>23</td>
    <td><p>Recent years have witnessed extensive attention in binary code learning,
a.k.a. hashing, for nearest neighbor search problems. It has been seen that
high-dimensional data points can be quantized into binary codes to give an
efficient similarity approximation via Hamming distance. Among existing
schemes, ranking-based hashing is recent promising that targets at preserving
ordinal relations of ranking in the Hamming space to minimize retrieval loss.
However, the size of the ranking tuples, which shows the ordinal relations, is
quadratic or cubic to the size of training samples. By given a large-scale
training data set, it is very expensive to embed such ranking tuples in binary
code learning. Besides, it remains a dificulty to build ranking tuples
efficiently for most ranking-preserving hashing, which are deployed over an
ordinal graph-based setting. To handle these problems, we propose a novel
ranking-preserving hashing method, dubbed Ordinal Constraint Hashing (OCH),
which efficiently learns the optimal hashing functions with a graph-based
approximation to embed the ordinal relations. The core idea is to reduce the
size of ordinal graph with ordinal constraint projection, which preserves the
ordinal relations through a small data set (such as clusters or random
samples). In particular, to learn such hash functions effectively, we further
relax the discrete constraints and design a specific stochastic gradient decent
algorithm for optimization. Experimental results on three large-scale visual
search benchmark datasets, i.e. LabelMe, Tiny100K and GIST1M, show that the
proposed OCH method can achieve superior performance over the state-of-the-arts
approaches.</p>
</td>
    <td>
      
        Graph-Based-ANN 
      
        Hashing-Methods 
      
        Datasets 
      
        Compact-Codes 
      
        AAAI 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/jiang2017deep/">Deep Cross-Modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Cross-Modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Cross-Modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jiang Qing-yuan, Li</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>752</td>
    <td><p>Due to its low storage cost and fast query speed, crossmodal hashing (CMH) has been widely used for similarity
search in multimedia retrieval applications. However, most
existing CMH methods are based on hand-crafted features
which might not be optimally compatible with the hash-code
learning procedure. As a result, existing CMH methods
with hand-crafted features may not achieve satisfactory
performance. In this paper, we propose a novel CMH
method, called deep cross-modal hashing (DCMH), by
integrating feature learning and hash-code learning into
the same framework. DCMH is an end-to-end learning
framework with deep neural networks, one for each modality, to perform feature learning from scratch. Experiments
on three real datasets with image-text modalities show
that DCMH can outperform other baselines to achieve
the state-of-the-art performance in cross-modal retrieval
applications.</p>
</td>
    <td>
      
        Datasets 
      
        CVPR 
      
        Memory-Efficiency 
      
        Multimodal-Retrieval 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/jiang2016deep/">Deep Cross-Modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Cross-Modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Cross-Modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jiang Qing-yuan, Li Wu-jun</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>752</td>
    <td><p>Due to its low storage cost and fast query speed, cross-modal hashing (CMH)
has been widely used for similarity search in multimedia retrieval
applications. However, almost all existing CMH methods are based on
hand-crafted features which might not be optimally compatible with the
hash-code learning procedure. As a result, existing CMH methods with
handcrafted features may not achieve satisfactory performance. In this paper,
we propose a novel cross-modal hashing method, called deep crossmodal hashing
(DCMH), by integrating feature learning and hash-code learning into the same
framework. DCMH is an end-to-end learning framework with deep neural networks,
one for each modality, to perform feature learning from scratch. Experiments on
two real datasets with text-image modalities show that DCMH can outperform
other baselines to achieve the state-of-the-art performance in cross-modal
retrieval applications.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Datasets 
      
        Memory-Efficiency 
      
        CVPR 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/jimenez2017class/">Class-Weighted Convolutional Features for Visual Instance Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Class-Weighted Convolutional Features for Visual Instance Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Class-Weighted Convolutional Features for Visual Instance Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jimenez Albert, Alvarez Jose M., Giro-i-nieto Xavier</td> <!-- 🔧 You were missing this -->
    <td>Procedings of the British Machine Vision Conference 2017</td>
    <td>71</td>
    <td><p>Image retrieval in realistic scenarios targets large dynamic datasets of
unlabeled images. In these cases, training or fine-tuning a model every time
new images are added to the database is neither efficient nor scalable.
Convolutional neural networks trained for image classification over large
datasets have been proven effective feature extractors for image retrieval. The
most successful approaches are based on encoding the activations of
convolutional layers, as they convey the image spatial information. In this
paper, we go beyond this spatial information and propose a local-aware encoding
of convolutional features based on semantic information predicted in the target
image. To this end, we obtain the most discriminative regions of an image using
Class Activation Maps (CAMs). CAMs are based on the knowledge contained in the
network and therefore, our approach, has the additional advantage of not
requiring external information. In addition, we use CAMs to generate object
proposals during an unsupervised re-ranking stage after a first fast search.
Our experiments on two public available datasets for instance retrieval,
Oxford5k and Paris6k, demonstrate the competitiveness of our approach
outperforming the current state-of-the-art when using off-the-shelf models
trained on ImageNet. The source code and model used in this paper are publicly
available at http://imatge-upc.github.io/retrieval-2017-cam/.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Datasets 
      
        Hybrid-ANN-Methods 
      
        Re-Ranking 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/liong2017cross/">Cross-Modal Deep Variational Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-Modal Deep Variational Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-Modal Deep Variational Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liong Venice, Lu, Tan, Zhou</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Conference on Computer Vision (ICCV)</td>
    <td>98</td>
    <td><p>In this paper, we propose a cross-modal deep variational hashing (CMDVH) method for cross-modality multimedia retrieval. Unlike existing cross-modal hashing methods
which learn a single pair of projections to map each example as a binary vector, we design a couple of deep neural
network to learn non-linear transformations from imagetext input pairs, so that unified binary codes can be obtained. We then design the modality-specific neural networks in a probabilistic manner where we model a latent
variable as close as possible from the inferred binary codes,
which is approximated by a posterior distribution regularized by a known prior. Experimental results on three benchmark datasets show the efficacy of the proposed approach.</p>
</td>
    <td>
      
        ICCV 
      
        Datasets 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/jiang2025deep/">Deep Cross-Modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Cross-Modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Cross-Modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jiang Qing-yuan, Li</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>752</td>
    <td><p>Due to its low storage cost and fast query speed, crossmodal hashing (CMH) has been widely used for similarity
search in multimedia retrieval applications. However, most
existing CMH methods are based on hand-crafted features
which might not be optimally compatible with the hash-code
learning procedure. As a result, existing CMH methods
with hand-crafted features may not achieve satisfactory
performance. In this paper, we propose a novel CMH
method, called deep cross-modal hashing (DCMH), by
integrating feature learning and hash-code learning into
the same framework. DCMH is an end-to-end learning
framework with deep neural networks, one for each modality, to perform feature learning from scratch. Experiments
on three real datasets with image-text modalities show
that DCMH can outperform other baselines to achieve
the state-of-the-art performance in cross-modal retrieval
applications.</p>
</td>
    <td>
      
        Datasets 
      
        CVPR 
      
        Memory-Efficiency 
      
        Multimodal-Retrieval 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/razeghi2017privacy/">Privacy Preserving Identification Using Sparse Approximation with Ambiguization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Privacy Preserving Identification Using Sparse Approximation with Ambiguization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Privacy Preserving Identification Using Sparse Approximation with Ambiguization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Razeghi Behrooz, Voloshynovskiy Slava, Kostadinov Dimche, Taran Olga</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE Workshop on Information Forensics and Security (WIFS)</td>
    <td>25</td>
    <td><p>In this paper, we consider a privacy preserving encoding framework for
identification applications covering biometrics, physical object security and
the Internet of Things (IoT). The proposed framework is based on a sparsifying
transform, which consists of a trained linear map, an element-wise
nonlinearity, and privacy amplification. The sparsifying transform and privacy
amplification are not symmetric for the data owner and data user. We
demonstrate that the proposed approach is closely related to sparse ternary
codes (STC), a recent information-theoretic concept proposed for fast
approximate nearest neighbor (ANN) search in high dimensional feature spaces
that being machine learning in nature also offers significant benefits in
comparison to sparse approximation and binary embedding approaches. We
demonstrate that the privacy of the database outsourced to a server as well as
the privacy of the data user are preserved at a low computational cost, storage
and communication burdens.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Evaluation 
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/qiu2025deep/">Deep Semantic Hashing with Generative Adversarial Networks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Semantic Hashing with Generative Adversarial Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Semantic Hashing with Generative Adversarial Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Qiu Zhaofan, Pan, Yao, Mei</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>97</td>
    <td><p>Hashing has been a widely-adopted technique for nearest
neighbor search in large-scale image retrieval tasks. Recent research has shown that leveraging supervised information can
lead to high quality hashing. However, the cost of annotating
data is often an obstacle when applying supervised hashing
to a new domain. Moreover, the results can suffer from the
robustness problem as the data at training and test stage
may come from different distributions. This paper studies
the exploration of generating synthetic data through semisupervised generative adversarial networks (GANs), which
leverages largely unlabeled and limited labeled training data
to produce highly compelling data with intrinsic invariance
and global coherence, for better understanding statistical
structures of natural data. We demonstrate that the above
two limitations can be well mitigated by applying the synthetic data for hashing. Specifically, a novel deep semantic
hashing with GANs (DSH-GANs) is presented, which mainly
consists of four components: a deep convolution neural networks (CNN) for learning image representations, an adversary
stream to distinguish synthetic images from real ones, a hash
stream for encoding image representations to hash codes and
a classification stream. The whole architecture is trained endto-end by jointly optimizing three losses, i.e., adversarial loss
to correct label of synthetic or real for each sample, triplet
ranking loss to preserve the relative similarity ordering in the
input real-synthetic triplets and classification loss to classify
each sample accurately. Extensive experiments conducted on
both CIFAR-10 and NUS-WIDE image benchmarks validate the capability of exploiting synthetic images for hashing. Our
framework also achieves superior results when compared to
state-of-the-art deep hash models.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Neural-Hashing 
      
        Text-Retrieval 
      
        Tools-&-Libraries 
      
        SIGIR 
      
        Hashing-Methods 
      
        Supervised 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/wang2017attention/">An Attention-Based Deep Net for Learning to Rank</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=An Attention-Based Deep Net for Learning to Rank' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=An Attention-Based Deep Net for Learning to Rank' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Baiyang, Klabjan Diego</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>12</td>
    <td><p>In information retrieval, learning to rank constructs a machine-based ranking
model which given a query, sorts the search results by their degree of
relevance or importance to the query. Neural networks have been successfully
applied to this problem, and in this paper, we propose an attention-based deep
neural network which better incorporates different embeddings of the queries
and search results with an attention-based mechanism. This model also applies a
decoder mechanism to learn the ranks of the search results in a listwise
fashion. The embeddings are trained with convolutional neural networks or the
word2vec model. We demonstrate the performance of this model with image
retrieval and text querying data sets.</p>
</td>
    <td>
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/zhao2017scalable/">Scalable Nearest Neighbor Search based on kNN Graph</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Scalable Nearest Neighbor Search based on kNN Graph' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Scalable Nearest Neighbor Search based on kNN Graph' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhao Wan-lei, Yang Jie, Deng Cheng-hao</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>7</td>
    <td><p>Nearest neighbor search is known as a challenging issue that has been studied
for several decades. Recently, this issue becomes more and more imminent in
viewing that the big data problem arises from various fields. In this paper, a
scalable solution based on hill-climbing strategy with the support of k-nearest
neighbor graph (kNN) is presented. Two major issues have been considered in the
paper. Firstly, an efficient kNN graph construction method based on two means
tree is presented. For the nearest neighbor search, an enhanced hill-climbing
procedure is proposed, which sees considerable performance boost over original
procedure. Furthermore, with the support of inverted indexing derived from
residue vector quantization, our method achieves close to 100% recall with high
speed efficiency in two state-of-the-art evaluation benchmarks. In addition, a
comparative study on both the compressional and traditional nearest neighbor
search methods is presented. We show that our method achieves the best
trade-off between search quality, efficiency and memory complexity.</p>
</td>
    <td>
      
        Graph-Based-ANN 
      
        Quantization 
      
        Survey-Paper 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/liong2025cross/">Cross-Modal Deep Variational Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-Modal Deep Variational Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-Modal Deep Variational Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liong Venice, Lu, Tan, Zhou</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Conference on Computer Vision (ICCV)</td>
    <td>98</td>
    <td><p>In this paper, we propose a cross-modal deep variational hashing (CMDVH) method for cross-modality multimedia retrieval. Unlike existing cross-modal hashing methods
which learn a single pair of projections to map each example as a binary vector, we design a couple of deep neural
network to learn non-linear transformations from imagetext input pairs, so that unified binary codes can be obtained. We then design the modality-specific neural networks in a probabilistic manner where we model a latent
variable as close as possible from the inferred binary codes,
which is approximated by a posterior distribution regularized by a known prior. Experimental results on three benchmark datasets show the efficacy of the proposed approach.</p>
</td>
    <td>
      
        ICCV 
      
        Datasets 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/ji2017cross/">Cross-Domain Image Retrieval with Attention Modeling</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-Domain Image Retrieval with Attention Modeling' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-Domain Image Retrieval with Attention Modeling' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ji Xin, Wang Wei, Zhang Meihui, Yang Yang</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 25th ACM international conference on Multimedia</td>
    <td>79</td>
    <td><p>With the proliferation of e-commerce websites and the ubiquitousness of smart
phones, cross-domain image retrieval using images taken by smart phones as
queries to search products on e-commerce websites is emerging as a popular
application. One challenge of this task is to locate the attention of both the
query and database images. In particular, database images, e.g. of fashion
products, on e-commerce websites are typically displayed with other
accessories, and the images taken by users contain noisy background and large
variations in orientation and lighting. Consequently, their attention is
difficult to locate. In this paper, we exploit the rich tag information
available on the e-commerce websites to locate the attention of database
images. For query images, we use each candidate image in the database as the
context to locate the query attention. Novel deep convolutional neural network
architectures, namely TagYNet and CtxYNet, are proposed to learn the attention
weights and then extract effective representations of the images. Experimental
results on public datasets confirm that our approaches have significant
improvement over the existing methods in terms of the retrieval accuracy and
efficiency.</p>
</td>
    <td>
      
        Datasets 
      
        Image-Retrieval 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/jain2017compact/">Compact Environment-Invariant Codes for Robust Visual Place Recognition</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Compact Environment-Invariant Codes for Robust Visual Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Compact Environment-Invariant Codes for Robust Visual Place Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jain Unnat, Namboodiri Vinay P., Pandey Gaurav</td> <!-- 🔧 You were missing this -->
    <td>2017 14th Conference on Computer and Robot Vision (CRV)</td>
    <td>6</td>
    <td><p>Robust visual place recognition (VPR) requires scene representations that are
invariant to various environmental challenges such as seasonal changes and
variations due to ambient lighting conditions during day and night. Moreover, a
practical VPR system necessitates compact representations of environmental
features. To satisfy these requirements, in this paper we suggest a
modification to the existing pipeline of VPR systems to incorporate supervised
hashing. The modified system learns (in a supervised setting) compact binary
codes from image feature descriptors. These binary codes imbibe robustness to
the visual variations exposed to it during the training phase, thereby, making
the system adaptive to severe environmental changes. Also, incorporating
supervised hashing makes VPR computationally more efficient and easy to
implement on simple hardware. This is because binary embeddings can be learned
over simple-to-compute features and the distance computation is also in the
low-dimensional hamming space of binary codes. We have performed experiments on
several challenging data sets covering seasonal, illumination and viewpoint
variations. We also compare two widely used supervised hashing methods of
CCAITQ and MLH and show that this new pipeline out-performs or closely matches
the state-of-the-art deep learning VPR methods that are based on
high-dimensional features extracted from pre-trained deep convolutional neural
networks.</p>
</td>
    <td>
      
        Supervised 
      
        Hashing-Methods 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/rygl2017semantic/">Semantic Vector Encoding and Similarity Search Using Fulltext Search Engines</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semantic Vector Encoding and Similarity Search Using Fulltext Search Engines' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semantic Vector Encoding and Similarity Search Using Fulltext Search Engines' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Rygl Jan, Pomikálek Jan, Řehůřek Radim, Růžička Michal, Novotný Vít, Sojka Petr</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2nd Workshop on Representation Learning for NLP</td>
    <td>12</td>
    <td><p>Vector representations and vector space modeling (VSM) play a central role in
modern machine learning. We propose a novel approach to `vector similarity
searching’ over dense semantic representations of words and documents that can
be deployed on top of traditional inverted-index-based fulltext engines, taking
advantage of their robustness, stability, scalability and ubiquity.
  We show that this approach allows the indexing and querying of dense vectors
in text domains. This opens up exciting avenues for major efficiency gains,
along with simpler deployment, scaling and monitoring.
  The end result is a fast and scalable vector database with a tunable
trade-off between vector search performance and quality, backed by a standard
fulltext engine such as Elasticsearch.
  We empirically demonstrate its querying performance and quality by applying
this solution to the task of semantic searching over a dense vector
representation of the entire English Wikipedia.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Text-Retrieval 
      
        Robustness 
      
        Scalability 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/liu2017end/">End-to-end Binary Representation Learning via Direct Binary Embedding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=End-to-end Binary Representation Learning via Direct Binary Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=End-to-end Binary Representation Learning via Direct Binary Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Liu, Rahimpour Alireza, Taalimi Ali, Qi Hairong</td> <!-- 🔧 You were missing this -->
    <td>2017 IEEE International Conference on Image Processing (ICIP)</td>
    <td>7</td>
    <td><p>Learning binary representation is essential to large-scale computer vision
tasks. Most existing algorithms require a separate quantization constraint to
learn effective hashing functions. In this work, we present Direct Binary
Embedding (DBE), a simple yet very effective algorithm to learn binary
representation in an end-to-end fashion. By appending an ingeniously designed
DBE layer to the deep convolutional neural network (DCNN), DBE learns binary
code directly from the continuous DBE layer activation without quantization
error. By employing the deep residual network (ResNet) as DCNN component, DBE
captures rich semantics from images. Furthermore, in the effort of handling
multilabel images, we design a joint cross entropy loss that includes both
softmax cross entropy and weighted binary cross entropy in consideration of the
correlation and independence of labels, respectively. Extensive experiments
demonstrate the significant superiority of DBE over state-of-the-art methods on
tasks of natural object recognition, image retrieval and image annotation.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Scalability 
      
        Quantization 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2017</td>
    <td>
      <a href="/publications/zhu2017discrete/">Discrete Multi-modal Hashing with Canonical Views for Robust Mobile Landmark Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Discrete Multi-modal Hashing with Canonical Views for Robust Mobile Landmark Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Discrete Multi-modal Hashing with Canonical Views for Robust Mobile Landmark Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhu Lei, Huang Zi, Liu Xiaobai, He Xiangnan, Song Jingkuan, Zhou Xiaofang</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>9</td>
    <td><p>Mobile landmark search (MLS) recently receives increasing attention for its
great practical values. However, it still remains unsolved due to two important
challenges. One is high bandwidth consumption of query transmission, and the
other is the huge visual variations of query images sent from mobile devices.
In this paper, we propose a novel hashing scheme, named as canonical view based
discrete multi-modal hashing (CV-DMH), to handle these problems via a novel
three-stage learning procedure. First, a submodular function is designed to
measure visual representativeness and redundancy of a view set. With it,
canonical views, which capture key visual appearances of landmark with limited
redundancy, are efficiently discovered with an iterative mining strategy.
Second, multi-modal sparse coding is applied to transform visual features from
multiple modalities into an intermediate representation. It can robustly and
adaptively characterize visual contents of varied landmark images with certain
canonical views. Finally, compact binary codes are learned on intermediate
representation within a tailored discrete binary embedding model which
preserves visual relations of images measured with canonical views and removes
the involved noises. In this part, we develop a new augmented Lagrangian
multiplier (ALM) based optimization method to directly solve the discrete
binary codes. We can not only explicitly deal with the discrete constraint, but
also consider the bit-uncorrelated constraint and balance constraint together.
Experiments on real world landmark datasets demonstrate the superior
performance of CV-DMH over several state-of-the-art methods.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        Compact-Codes 
      
        Evaluation 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/szeto2016binary/">Binary Codes for Tagging X-Ray Images via Deep De-Noising Autoencoders</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Binary Codes for Tagging X-Ray Images via Deep De-Noising Autoencoders' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Binary Codes for Tagging X-Ray Images via Deep De-Noising Autoencoders' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sze-to Antonio, Tizhoosh Hamid R., Wong Andrew K. C.</td> <!-- 🔧 You were missing this -->
    <td>2016 International Joint Conference on Neural Networks (IJCNN)</td>
    <td>11</td>
    <td><p>A Content-Based Image Retrieval (CBIR) system which identifies similar
medical images based on a query image can assist clinicians for more accurate
diagnosis. The recent CBIR research trend favors the construction and use of
binary codes to represent images. Deep architectures could learn the non-linear
relationship among image pixels adaptively, allowing the automatic learning of
high-level features from raw pixels. However, most of them require class
labels, which are expensive to obtain, particularly for medical images. The
methods which do not need class labels utilize a deep autoencoder for binary
hashing, but the code construction involves a specific training algorithm and
an ad-hoc regularization technique. In this study, we explored using a deep
de-noising autoencoder (DDA), with a new unsupervised training scheme using
only backpropagation and dropout, to hash images into binary codes. We
conducted experiments on more than 14,000 x-ray images. By using class labels
only for evaluating the retrieval results, we constructed a 16-bit DDA and a
512-bit DDA independently. Comparing to other unsupervised methods, we
succeeded to obtain the lowest total error by using the 512-bit codes for
retrieval via exhaustive search, and speed up 9.27 times with the use of the
16-bit codes while keeping a comparable total error. We found that our new
training scheme could reduce the total retrieval error significantly by 21.9%.
To further boost the image retrieval performance, we developed Radon
Autoencoder Barcode (RABC) which are learned from the Radon projections of
images using a de-noising autoencoder. Experimental results demonstrated its
superior performance in retrieval when it was combined with DDA binary codes.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Compact-Codes 
      
        Unsupervised 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/wang2019supervised/">Supervised Quantization for Similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Quantization for Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Supervised Quantization for Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Xiaojuan, Zhang Ting, Q Guo-jun, Tang Jinhui, Wang Jingdong</td> <!-- 🔧 You were missing this -->
    <td>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>82</td>
    <td><p>In this paper, we address the problem of searching for semantically similar
images from a large database. We present a compact coding approach, supervised
quantization. Our approach simultaneously learns feature selection that
linearly transforms the database points into a low-dimensional discriminative
subspace, and quantizes the data points in the transformed space. The
optimization criterion is that the quantized points not only approximate the
transformed points accurately, but also are semantically separable: the points
belonging to a class lie in a cluster that is not overlapped with other
clusters corresponding to other classes, which is formulated as a
classification problem. The experiments on several standard datasets show the
superiority of our approach over the state-of-the art supervised hashing and
unsupervised quantization algorithms.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Supervised 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        CVPR 
      
        Unsupervised 
      
        Quantization 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/tizhoosh2016barcodes/">Barcodes for Medical Image Retrieval Using Autoencoded Radon Transform</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Barcodes for Medical Image Retrieval Using Autoencoded Radon Transform' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Barcodes for Medical Image Retrieval Using Autoencoded Radon Transform' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tizhoosh Hamid R., Mitcheltree Christopher, Zhu Shujin, Dutta Shamak</td> <!-- 🔧 You were missing this -->
    <td>2016 23rd International Conference on Pattern Recognition (ICPR)</td>
    <td>16</td>
    <td><p>Using content-based binary codes to tag digital images has emerged as a
promising retrieval technology. Recently, Radon barcodes (RBCs) have been
introduced as a new binary descriptor for image search. RBCs are generated by
binarization of Radon projections and by assembling them into a vector, namely
the barcode. A simple local thresholding has been suggested for binarization.
In this paper, we put forward the idea of “autoencoded Radon barcodes”. Using
images in a training dataset, we autoencode Radon projections to perform
binarization on outputs of hidden layers. We employed the mini-batch stochastic
gradient descent approach for the training. Each hidden layer of the
autoencoder can produce a barcode using a threshold determined based on the
range of the logistic function used. The compressing capability of autoencoders
apparently reduces the redundancies inherent in Radon projections leading to
more accurate retrieval results. The IRMA dataset with 14,410 x-ray images is
used to validate the performance of the proposed method. The experimental
results, containing comparison with RBCs, SURF and BRISK, show that autoencoded
Radon barcode (ARBC) has the capacity to capture important information and to
learn richer representations resulting in lower retrieval errors for image
retrieval measured with the accuracy of the first hit only.</p>
</td>
    <td>
      
        Datasets 
      
        Compact-Codes 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/sharma2016stacked/">Stacked Autoencoders for Medical Image Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Stacked Autoencoders for Medical Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Stacked Autoencoders for Medical Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sharma S., Umar I., Ospina L., Wong D., Tizhoosh H. R.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>32</td>
    <td><p>Medical images can be a valuable resource for reliable information to support
medical diagnosis. However, the large volume of medical images makes it
challenging to retrieve relevant information given a particular scenario. To
solve this challenge, content-based image retrieval (CBIR) attempts to
characterize images (or image regions) with invariant content information in
order to facilitate image search. This work presents a feature extraction
technique for medical images using stacked autoencoders, which encode images to
binary vectors. The technique is applied to the IRMA dataset, a collection of
14,410 x-ray images in order to demonstrate the ability of autoencoders to
retrieve similar x-rays given test queries. Using IRMA dataset as a benchmark,
it was found that stacked autoencoders gave excellent results with a retrieval
error of 376 for 1,733 test images with a compression of 74.61%.</p>
</td>
    <td>
      
        Datasets 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/liu2019query/">Query-Adaptive Hash Code Ranking for Large-Scale Multi-View Visual Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Query-Adaptive Hash Code Ranking for Large-Scale Multi-View Visual Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Query-Adaptive Hash Code Ranking for Large-Scale Multi-View Visual Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Xianglong, Huang Lei, Deng Cheng, Lang Bo, Tao Dacheng</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>74</td>
    <td><p>Hash based nearest neighbor search has become attractive in many
applications. However, the quantization in hashing usually degenerates the
discriminative power when using Hamming distance ranking. Besides, for
large-scale visual search, existing hashing methods cannot directly support the
efficient search over the data with multiple sources, and while the literature
has shown that adaptively incorporating complementary information from diverse
sources or views can significantly boost the search performance. To address the
problems, this paper proposes a novel and generic approach to building multiple
hash tables with multiple views and generating fine-grained ranking results at
bitwise and tablewise levels. For each hash table, a query-adaptive bitwise
weighting is introduced to alleviate the quantization loss by simultaneously
exploiting the quality of hash functions and their complement for nearest
neighbor search. From the tablewise aspect, multiple hash tables are built for
different data views as a joint index, over which a query-specific rank fusion
is proposed to rerank all results from the bitwise ranking by diffusing in a
graph. Comprehensive experiments on image search over three well-known
benchmarks show that the proposed method achieves up to 17.11% and 20.28%
performance gains on single and multiple table search over state-of-the-art
methods.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Quantization 
      
        Re-Ranking 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/liu2016generating/">Generating Binary Tags for Fast Medical Image Retrieval Based on Convolutional Nets and Radon Transform</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Generating Binary Tags for Fast Medical Image Retrieval Based on Convolutional Nets and Radon Transform' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Generating Binary Tags for Fast Medical Image Retrieval Based on Convolutional Nets and Radon Transform' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Xinran, Tizhoosh Hamid R., Kofman Jonathan</td> <!-- 🔧 You were missing this -->
    <td>2016 International Joint Conference on Neural Networks (IJCNN)</td>
    <td>62</td>
    <td><p>Content-based image retrieval (CBIR) in large medical image archives is a
challenging and necessary task. Generally, different feature extraction methods
are used to assign expressive and invariant features to each image such that
the search for similar images comes down to feature classification and/or
matching. The present work introduces a new image retrieval method for medical
applications that employs a convolutional neural network (CNN) with recently
introduced Radon barcodes. We combine neural codes for global classification
with Radon barcodes for the final retrieval. We also examine image search based
on regions of interest (ROI) matching after image retrieval. The IRMA dataset
with more than 14,000 x-rays images is used to evaluate the performance of our
method. Experimental results show that our approach is superior to many
published works.</p>
</td>
    <td>
      
        Datasets 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/liu2016supervised/">Supervised Matrix Factorization for Cross-Modality Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Matrix Factorization for Cross-Modality Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Supervised Matrix Factorization for Cross-Modality Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Hong, Ji Rongrong, Wu Yongjian, Hua Gang</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>65</td>
    <td><p>Matrix factorization has been recently utilized for the task of multi-modal
hashing for cross-modality visual search, where basis functions are learned to
map data from different modalities to the same Hamming embedding. In this
paper, we propose a novel cross-modality hashing algorithm termed Supervised
Matrix Factorization Hashing (SMFH) which tackles the multi-modal hashing
problem with a collective non-matrix factorization across the different
modalities. In particular, SMFH employs a well-designed binary code learning
algorithm to preserve the similarities among multi-modal original features
through a graph regularization. At the same time, semantic labels, when
available, are incorporated into the learning procedure. We conjecture that all
these would facilitate to preserve the most relevant information during the
binary quantization process, and hence improve the retrieval accuracy. We
demonstrate the superior performance of SMFH on three cross-modality visual
search benchmarks, i.e., the PASCAL-Sentence, Wiki, and NUS-WIDE, with
quantitative comparison to various state-of-the-art methods</p>
</td>
    <td>
      
        Supervised 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Compact-Codes 
      
        Quantization 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/lindgren2017leveraging/">Leveraging Sparsity for Efficient Submodular Data Summarization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Leveraging Sparsity for Efficient Submodular Data Summarization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Leveraging Sparsity for Efficient Submodular Data Summarization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lindgren Erik M., Wu Shanshan, Dimakis Alexandros G.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>12</td>
    <td><p>The facility location problem is widely used for summarizing large datasets
and has additional applications in sensor placement, image retrieval, and
clustering. One difficulty of this problem is that submodular optimization
algorithms require the calculation of pairwise benefits for all items in the
dataset. This is infeasible for large problems, so recent work proposed to only
calculate nearest neighbor benefits. One limitation is that several strong
assumptions were invoked to obtain provable approximation guarantees. In this
paper we establish that these extra assumptions are not necessary—solving the
sparsified problem will be almost optimal under the standard assumptions of the
problem. We then analyze a different method of sparsification that is a better
model for methods such as Locality Sensitive Hashing to accelerate the nearest
neighbor computations and extend the use of the problem to a broader family of
similarities. We validate our approach by demonstrating that it rapidly
generates interpretable summaries.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        Locality-Sensitive-Hashing 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/song2015deep/">Deep Metric Learning via Lifted Structured Feature Embedding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Metric Learning via Lifted Structured Feature Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Metric Learning via Lifted Structured Feature Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Song Hyun Oh, Xiang Yu, Jegelka Stefanie, Savarese Silvio</td> <!-- 🔧 You were missing this -->
    <td>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>1656</td>
    <td><p>Learning the distance metric between pairs of examples is of great importance
for learning and visual recognition. With the remarkable success from the state
of the art convolutional neural networks, recent works have shown promising
results on discriminatively training the networks to learn semantic feature
embeddings where similar examples are mapped close to each other and dissimilar
examples are mapped farther apart. In this paper, we describe an algorithm for
taking full advantage of the training batches in the neural network training by
lifting the vector of pairwise distances within the batch to the matrix of
pairwise distances. This step enables the algorithm to learn the state of the
art feature embedding by optimizing a novel structured prediction objective on
the lifted problem. Additionally, we collected Online Products dataset: 120k
images of 23k classes of online products for metric learning. Our experiments
on the CUB-200-2011, CARS196, and Online Products datasets demonstrate
significant improvement over existing deep feature embedding methods on all
experimented embedding sizes with the GoogLeNet network.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Datasets 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/koutaki2016fast/">Fast Supervised Discrete Hashing and its Analysis</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Supervised Discrete Hashing and its Analysis' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Supervised Discrete Hashing and its Analysis' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Koutaki Gou, Shirai Keiichiro, Ambai Mitsuru</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>7</td>
    <td><p>In this paper, we propose a learning-based supervised discrete hashing
method. Binary hashing is widely used for large-scale image retrieval as well
as video and document searches because the compact representation of binary
code is essential for data storage and reasonable for query searches using
bit-operations. The recently proposed Supervised Discrete Hashing (SDH)
efficiently solves mixed-integer programming problems by alternating
optimization and the Discrete Cyclic Coordinate descent (DCC) method. We show
that the SDH model can be simplified without performance degradation based on
some preliminary experiments; we call the approximate model for this the “Fast
SDH” (FSDH) model. We analyze the FSDH model and provide a mathematically exact
solution for it. In contrast to SDH, our model does not require an alternating
optimization algorithm and does not depend on initial values. FSDH is also
easier to implement than Iterative Quantization (ITQ). Experimental results
involving a large-scale database showed that FSDH outperforms conventional SDH
in terms of precision, recall, and computation time.</p>
</td>
    <td>
      
        Supervised 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Quantization 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/wieschollek2017efficient/">Efficient Large-scale Approximate Nearest Neighbor Search on the GPU</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Large-scale Approximate Nearest Neighbor Search on the GPU' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient Large-scale Approximate Nearest Neighbor Search on the GPU' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wieschollek Patrick, Wang Oliver, Sorkine-hornung Alexander, Lensch Hendrik P. A.</td> <!-- 🔧 You were missing this -->
    <td>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>55</td>
    <td><p>We present a new approach for efficient approximate nearest neighbor (ANN)
search in high dimensional spaces, extending the idea of Product Quantization.
We propose a two-level product and vector quantization tree that reduces the
number of vector comparisons required during tree traversal. Our approach also
includes a novel highly parallelizable re-ranking method for candidate vectors
by efficiently reusing already computed intermediate values. Due to its small
memory footprint during traversal, the method lends itself to an efficient,
parallel GPU implementation. This Product Quantization Tree (PQT) approach
significantly outperforms recent state of the art methods for high dimensional
nearest neighbor queries on standard reference datasets. Ours is the first work
that demonstrates GPU performance superior to CPU performance on high
dimensional, large scale ANN problems in time-critical real-world applications,
like loop-closing in videos.</p>
</td>
    <td>
      
        Datasets 
      
        Quantization 
      
        Memory-Efficiency 
      
        CVPR 
      
        Hybrid-ANN-Methods 
      
        Re-Ranking 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/yang2016zero/">Zero-Shot Hashing via Transferring Supervised Knowledge</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Zero-Shot Hashing via Transferring Supervised Knowledge' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Zero-Shot Hashing via Transferring Supervised Knowledge' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yang Yang, Chen Weilun, Luo Yadan, Shen Fumin, Shao Jie, Shen Heng Tao</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 24th ACM international conference on Multimedia</td>
    <td>139</td>
    <td><p>Hashing has shown its efficiency and effectiveness in facilitating
large-scale multimedia applications. Supervised knowledge e.g. semantic labels
or pair-wise relationship) associated to data is capable of significantly
improving the quality of hash codes and hash functions. However, confronted
with the rapid growth of newly-emerging concepts and multimedia data on the
Web, existing supervised hashing approaches may easily suffer from the scarcity
and validity of supervised information due to the expensive cost of manual
labelling. In this paper, we propose a novel hashing scheme, termed
<em>zero-shot hashing</em> (ZSH), which compresses images of “unseen” categories
to binary codes with hash functions learned from limited training data of
“seen” categories. Specifically, we project independent data labels i.e.
0/1-form label vectors) into semantic embedding space, where semantic
relationships among all the labels can be precisely characterized and thus seen
supervised knowledge can be transferred to unseen classes. Moreover, in order
to cope with the semantic shift problem, we rotate the embedded space to more
suitably align the embedded semantics with the low-level visual feature space,
thereby alleviating the influence of semantic gap. In the meantime, to exert
positive effects on learning high-quality hash functions, we further propose to
preserve local structural property and discrete nature in binary codes.
Besides, we develop an efficient alternating algorithm to solve the ZSH model.
Extensive experiments conducted on various real-life datasets show the superior
zero-shot image retrieval performance of ZSH as compared to several
state-of-the-art hashing methods.</p>
</td>
    <td>
      
        Few-Shot-&-Zero-Shot 
      
        Supervised 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Scalability 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/gui2019supervised/">Supervised Discrete Hashing with Relaxation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Discrete Hashing with Relaxation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Supervised Discrete Hashing with Relaxation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gui Jie, Liu Tongliang, Sun Zhenan, Tao Dacheng, Tan Tieniu</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Neural Networks and Learning Systems</td>
    <td>92</td>
    <td><p>Data-dependent hashing has recently attracted attention due to being able to
support efficient retrieval and storage of high-dimensional data such as
documents, images, and videos. In this paper, we propose a novel learning-based
hashing method called “Supervised Discrete Hashing with Relaxation” (SDHR)
based on “Supervised Discrete Hashing” (SDH). SDH uses ordinary least squares
regression and traditional zero-one matrix encoding of class label information
as the regression target (code words), thus fixing the regression target. In
SDHR, the regression target is instead optimized. The optimized regression
target matrix satisfies a large margin constraint for correct classification of
each example. Compared with SDH, which uses the traditional zero-one matrix,
SDHR utilizes the learned regression target matrix and, therefore, more
accurately measures the classification error of the regression model and is
more flexible. As expected, SDHR generally outperforms SDH. Experimental
results on two large-scale image datasets (CIFAR-10 and MNIST) and a
large-scale and challenging face dataset (FRGC) demonstrate the effectiveness
and efficiency of SDHR.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Supervised 
      
        Hashing-Methods 
      
        Datasets 
      
        Scalability 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/douze2016polysemous/">Polysemous codes</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Polysemous codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Polysemous codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Douze Matthijs, Jégou Hervé, Perronnin Florent</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>45</td>
    <td><p>This paper considers the problem of approximate nearest neighbor search in
the compressed domain. We introduce polysemous codes, which offer both the
distance estimation quality of product quantization and the efficient
comparison of binary codes with Hamming distance. Their design is inspired by
algorithms introduced in the 90’s to construct channel-optimized vector
quantizers. At search time, this dual interpretation accelerates the search.
Most of the indexed vectors are filtered out with Hamming distance, letting
only a fraction of the vectors to be ranked with an asymmetric distance
estimator.
  The method is complementary with a coarse partitioning of the feature space
such as the inverted multi-index. This is shown by our experiments performed on
several public benchmarks such as the BIGANN dataset comprising one billion
vectors, for which we report state-of-the-art results for query times below
0.3\,millisecond per core. Last but not least, our approach allows the
approximate computation of the k-NN graph associated with the Yahoo Flickr
Creative Commons 100M, described by CNN image descriptors, in less than 8 hours
on a single machine.</p>
</td>
    <td>
      
        Vector-Indexing 
      
        Datasets 
      
        Compact-Codes 
      
        Quantization 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/do2016binary/">Binary Hashing with Semidefinite Relaxation and Augmented Lagrangian</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Binary Hashing with Semidefinite Relaxation and Augmented Lagrangian' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Binary Hashing with Semidefinite Relaxation and Augmented Lagrangian' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Do Thanh-toan, Doan Anh-dzung, Nguyen Duc-thanh, Cheung Ngai-man</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>20</td>
    <td><p>This paper proposes two approaches for inferencing binary codes in two-step
(supervised, unsupervised) hashing. We first introduce an unified formulation
for both supervised and unsupervised hashing. Then, we cast the learning of one
bit as a Binary Quadratic Problem (BQP). We propose two approaches to solve
BQP. In the first approach, we relax BQP as a semidefinite programming problem
which its global optimum can be achieved. We theoretically prove that the
objective value of the binary solution achieved by this approach is well
bounded. In the second approach, we propose an augmented Lagrangian based
approach to solve BQP directly without relaxing the binary constraint.
Experimental results on three benchmark datasets show that our proposed methods
compare favorably with the state of the art.</p>
</td>
    <td>
      
        Supervised 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Unsupervised 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/do2016learning/">Learning to Hash with Binary Deep Neural Network</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning to Hash with Binary Deep Neural Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning to Hash with Binary Deep Neural Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Do Thanh-toan, Doan Anh-dzung, Cheung Ngai-man</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>169</td>
    <td><p>This work proposes deep network models and learning algorithms for
unsupervised and supervised binary hashing. Our novel network design constrains
one hidden layer to directly output the binary codes. This addresses a
challenging issue in some previous works: optimizing non-smooth objective
functions due to binarization. Moreover, we incorporate independence and
balance properties in the direct and strict forms in the learning. Furthermore,
we include similarity preserving property in our objective function. Our
resulting optimization with these binary, independence, and balance constraints
is difficult to solve. We propose to attack it with alternating optimization
and careful relaxation. Experimental results on three benchmark datasets show
that our proposed methods compare favorably with the state of the art.</p>
</td>
    <td>
      
        Supervised 
      
        Hashing-Methods 
      
        Datasets 
      
        Compact-Codes 
      
        Unsupervised 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/kang2025column/">Column Sampling Based Discrete Supervised Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Column Sampling Based Discrete Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Column Sampling Based Discrete Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kang Wang-cheng, Li, Zhou</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>299</td>
    <td><p>By leveraging semantic (label) information, supervised hashing has demonstrated better accuracy than unsupervised hashing in many real applications. Because the hashing-code learning problem is essentially a discrete optimization problem which is hard to solve, most existing supervised hashing methods try to solve a relaxed continuous optimization problem by dropping the discrete constraints.
However, these methods typically suffer from poor performance due to the errors caused by the relaxation. Some other methods try to directly solve the discrete optimization problem. However, they are typically time-consuming and unscalable. In this paper, we propose a novel method, called column sampling based discrete supervised hashing (COSDISH), to directly learn the discrete hashing code from semantic information.
COSDISH is an iterative method, in each iteration of which several columns are sampled from the semantic similarity matrix and then the hashing code is decomposed into two parts which can be alternately optimized in a discrete way. Theoretical analysis shows that the learning (optimization) algorithm of COSDISH has a constant-approximation bound in each step of the alternating optimization procedure. Empirical results on datasets with semantic labels illustrate that COSDISH can outperform the state-of-the-art methods in real applications like image retrieval.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Datasets 
      
        Neural-Hashing 
      
        Supervised 
      
        Hashing-Methods 
      
        AAAI 
      
        Evaluation 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/lai2016instance/">Instance-Aware Hashing for Multi-Label Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Instance-Aware Hashing for Multi-Label Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Instance-Aware Hashing for Multi-Label Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lai Hanjiang, Yan Pan, Shu Xiangbo, Wei Yunchao, Yan Shuicheng</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>78</td>
    <td><p>Similarity-preserving hashing is a commonly used method for nearest neighbour
search in large-scale image retrieval. For image retrieval, deep-networks-based
hashing methods are appealing since they can simultaneously learn effective
image representations and compact hash codes. This paper focuses on
deep-networks-based hashing for multi-label images, each of which may contain
objects of multiple categories. In most existing hashing methods, each image is
represented by one piece of hash code, which is referred to as semantic
hashing. This setting may be suboptimal for multi-label image retrieval. To
solve this problem, we propose a deep architecture that learns
\textbf{instance-aware} image representations for multi-label image data, which
are organized in multiple groups, with each group containing the features for
one category. The instance-aware representations not only bring advantages to
semantic hashing, but also can be used in category-aware hashing, in which an
image is represented by multiple pieces of hash codes and each piece of code
corresponds to a category. Extensive evaluations conducted on several benchmark
datasets demonstrate that, for both semantic hashing and category-aware
hashing, the proposed method shows substantial improvement over the
state-of-the-art supervised and unsupervised hashing methods.</p>
</td>
    <td>
      
        Supervised 
      
        Text-Retrieval 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Neural-Hashing 
      
        Unsupervised 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/hao2016what/">What Is the Best Practice for CNNs Applied to Visual Instance Retrieval?</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=What Is the Best Practice for CNNs Applied to Visual Instance Retrieval?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=What Is the Best Practice for CNNs Applied to Visual Instance Retrieval?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hao Jiedong, Dong Jing, Wang Wei, Tan Tieniu</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>9</td>
    <td><p>Previous work has shown that feature maps of deep convolutional neural
networks (CNNs) can be interpreted as feature representation of a particular
image region. Features aggregated from these feature maps have been exploited
for image retrieval tasks and achieved state-of-the-art performances in recent
years. The key to the success of such methods is the feature representation.
However, the different factors that impact the effectiveness of features are
still not explored thoroughly. There are much less discussion about the best
combination of them.
  The main contribution of our paper is the thorough evaluations of the various
factors that affect the discriminative ability of the features extracted from
CNNs. Based on the evaluation results, we also identify the best choices for
different factors and propose a new multi-scale image feature representation
method to encode the image effectively. Finally, we show that the proposed
method generalises well and outperforms the state-of-the-art methods on four
typical datasets used for visual instance retrieval.</p>
</td>
    <td>
      
        Datasets 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/salvador2016faster/">Faster R-CNN Features for Instance Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Faster R-CNN Features for Instance Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Faster R-CNN Features for Instance Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Salvador Amaia, Giro-i-nieto Xavier, Marques Ferran, Satoh Shin'ichi</td> <!-- 🔧 You were missing this -->
    <td>2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>121</td>
    <td><p>Image representations derived from pre-trained Convolutional Neural Networks
(CNNs) have become the new state of the art in computer vision tasks such as
instance retrieval. This work explores the suitability for instance retrieval
of image- and region-wise representations pooled from an object detection CNN
such as Faster R-CNN. We take advantage of the object proposals learned by a
Region Proposal Network (RPN) and their associated CNN features to build an
instance search pipeline composed of a first filtering stage followed by a
spatial reranking. We further investigate the suitability of Faster R-CNN
features when the network is fine-tuned for the same objects one wants to
retrieve. We assess the performance of our proposed system with the Oxford
Buildings 5k, Paris Buildings 6k and a subset of TRECVid Instance Search 2013,
achieving competitive results.</p>
</td>
    <td>
      
        CVPR 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/mitra2016dual/">A Dual Embedding Space Model for Document Ranking</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Dual Embedding Space Model for Document Ranking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Dual Embedding Space Model for Document Ranking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mitra Bhaskar, Nalisnick Eric, Craswell Nick, Caruana Rich</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>110</td>
    <td><p>A fundamental goal of search engines is to identify, given a query, documents
that have relevant text. This is intrinsically difficult because the query and
the document may use different vocabulary, or the document may contain query
words without being relevant. We investigate neural word embeddings as a source
of evidence in document ranking. We train a word2vec embedding model on a large
unlabelled query corpus, but in contrast to how the model is commonly used, we
retain both the input and the output projections, allowing us to leverage both
the embedding spaces to derive richer distributional relationships. During
ranking we map the query words into the input space and the document words into
the output space, and compute a query-document relevance score by aggregating
the cosine similarities across all the query-document word pairs.
  We postulate that the proposed Dual Embedding Space Model (DESM) captures
evidence on whether a document is about a query term in addition to what is
modelled by traditional term-frequency based approaches. Our experiments show
that the DESM can re-rank top documents returned by a commercial Web search
engine, like Bing, better than a term-matching based signal like TF-IDF.
However, when ranking a larger set of candidate documents, we find the
embeddings-based approach is prone to false positives, retrieving documents
that are only loosely related to the query. We demonstrate that this problem
can be solved effectively by ranking based on a linear mixture of the DESM and
the word counting features.</p>
</td>
    <td>
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/zhang2016efficient/">Efficient Training of Very Deep Neural Networks for Supervised Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Training of Very Deep Neural Networks for Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient Training of Very Deep Neural Networks for Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Ziming, Chen, Saligrama</td> <!-- 🔧 You were missing this -->
    <td>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>112</td>
    <td><p>In this paper, we propose training very deep neural networks (DNNs) for supervised learning of hash codes. Existing methods in this context train relatively “shallow” networks limited by the issues arising in back propagation (e.e. vanishing gradients) as well as computational efficiency. We propose a novel and efficient training algorithm inspired by alternating direction method of multipliers (ADMM) that overcomes some of these limitations. Our method decomposes the training process into independent layer-wise local updates through auxiliary variables. Empirically we observe that our training algorithm always converges and its computational complexity is linearly proportional to the number of edges in the networks. Empirically we manage to train DNNs with 64 hidden layers and 1024 nodes per layer for supervised hashing in about 3 hours using a single GPU. Our proposed very deep supervised hashing (VDSH) method significantly outperforms the state-of-the-art on several benchmark datasets.</p>
</td>
    <td>
      
        Efficiency 
      
        Datasets 
      
        CVPR 
      
        Neural-Hashing 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/moran2025enhancing/">Enhancing First Story Detection using Word Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Enhancing First Story Detection using Word Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Enhancing First Story Detection using Word Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Moran S., Mccreadie, Macdonald, Ounis</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</td>
    <td>29</td>
    <td><p>In this paper we show how word embeddings can be used to increase the effectiveness of a state-of-the art Locality Sensitive Hashing (LSH) based first story detection (FSD) system over a standard tweet corpus. Vocabulary mismatch, in which related tweets use different words, is a serious hindrance to the effectiveness of a modern FSD system. In this case, a tweet could be flagged as a first story even if a related tweet, which uses different but synonymous words, was already returned as a first story. In this work, we propose a novel approach to mitigate this problem of lexical variation, based on tweet expansion. In particular, we propose to expand tweets with semantically related paraphrases identified via automatically mined word embeddings over a background tweet corpus. Through experimentation on a large data stream comprised of 50 million tweets, we show that FSD effectiveness can be improved by 9.5% over a state-of-the-art FSD system.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Locality-Sensitive-Hashing 
      
        SIGIR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/srivastava20163d/">3D Binary Signatures</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=3D Binary Signatures' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=3D Binary Signatures' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Srivastava Siddharth, Lall Brejesh</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Tenth Indian Conference on Computer Vision, Graphics and Image Processing</td>
    <td>8</td>
    <td><p>In this paper, we propose a novel binary descriptor for 3D point clouds. The
proposed descriptor termed as 3D Binary Signature (3DBS) is motivated from the
matching efficiency of the binary descriptors for 2D images. 3DBS describes
keypoints from point clouds with a binary vector resulting in extremely fast
matching. The method uses keypoints from standard keypoint detectors. The
descriptor is built by constructing a Local Reference Frame and aligning a
local surface patch accordingly. The local surface patch constitutes of
identifying nearest neighbours based upon an angular constraint among them. The
points are ordered with respect to the distance from the keypoints. The normals
of the ordered pairs of these keypoints are projected on the axes and the
relative magnitude is used to assign a binary digit. The vector thus
constituted is used as a signature for representing the keypoints. The matching
is done by using hamming distance. We show that 3DBS outperforms state of the
art descriptors on various evaluation metrics.</p>
</td>
    <td>
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/moran2016enhancing/">Enhancing First Story Detection using Word Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Enhancing First Story Detection using Word Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Enhancing First Story Detection using Word Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Moran S., Mccreadie, Macdonald, Ounis</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</td>
    <td>29</td>
    <td><p>In this paper we show how word embeddings can be used to increase the effectiveness of a state-of-the art Locality Sensitive Hashing (LSH) based first story detection (FSD) system over a standard tweet corpus. Vocabulary mismatch, in which related tweets use different words, is a serious hindrance to the effectiveness of a modern FSD system. In this case, a tweet could be flagged as a first story even if a related tweet, which uses different but synonymous words, was already returned as a first story. In this work, we propose a novel approach to mitigate this problem of lexical variation, based on tweet expansion. In particular, we propose to expand tweets with semantically related paraphrases identified via automatically mined word embeddings over a background tweet corpus. Through experimentation on a large data stream comprised of 50 million tweets, we show that FSD effectiveness can be improved by 9.5% over a state-of-the-art FSD system.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Locality-Sensitive-Hashing 
      
        SIGIR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/zhu2016deep/">Deep Hashing Network for Efficient Similarity Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Hashing Network for Efficient Similarity Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Hashing Network for Efficient Similarity Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhu Han, Long, Wang, Cao</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>636</td>
    <td><p>Due to the storage and retrieval efficiency, hashing has been widely deployed to approximate nearest neighbor search for large-scale multimedia retrieval. Supervised hashing, which improves the quality of hash coding by exploiting the semantic similarity on data pairs, has received increasing attention recently. For most existing supervised hashing methods for image retrieval, an image is first represented as a vector of hand-crafted or machine-learned features, followed by another separate quantization step that generates binary codes.
However, suboptimal hash coding may be produced, because the quantization error is not statistically minimized and the feature representation is not optimally compatible with the binary coding. In this paper, we propose a novel Deep Hashing Network (DHN) architecture for supervised hashing, in which we jointly learn good image representation tailored to hash coding and formally control the quantization error.
The DHN model constitutes four key components: (1) a sub-network with multiple convolution-pooling layers to capture image representations; (2) a fully-connected hashing layer to generate compact binary hash codes; (3) a pairwise cross-entropy loss layer for similarity-preserving learning; and (4) a pairwise quantization loss for controlling hashing quality. Extensive experiments on standard image retrieval datasets show the proposed DHN model yields substantial boosts over latest state-of-the-art hashing methods.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Efficiency 
      
        Datasets 
      
        Neural-Hashing 
      
        Quantization 
      
        Compact-Codes 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        AAAI 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/sankaranarayanan2016triplet/">Triplet Similarity Embedding for Face Verification</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Triplet Similarity Embedding for Face Verification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Triplet Similarity Embedding for Face Verification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sankaranarayanan Swami, Alavi Azadeh, Chellappa Rama</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>47</td>
    <td><p>In this work, we present an unconstrained face verification algorithm and
evaluate it on the recently released IJB-A dataset that aims to push the
boundaries of face verification methods. The proposed algorithm couples a deep
CNN-based approach with a low-dimensional discriminative embedding learnt using
triplet similarity constraints in a large margin fashion. Aside from yielding
performance improvement, this embedding provides significant advantages in
terms of memory and post-processing operations like hashing and visualization.
Experiments on the IJB-A dataset show that the proposed algorithm outperforms
state of the art methods in verification and identification metrics, while
requiring less training time.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        Re-Ranking 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/khasanova2016multi/">Multi-modal image retrieval with random walk on multi-layer graphs</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multi-modal image retrieval with random walk on multi-layer graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multi-modal image retrieval with random walk on multi-layer graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Khasanova Renata, Dong Xiaowen, Frossard Pascal</td> <!-- 🔧 You were missing this -->
    <td>2016 IEEE International Symposium on Multimedia (ISM)</td>
    <td>7</td>
    <td><p>The analysis of large collections of image data is still a challenging
problem due to the difficulty of capturing the true concepts in visual data.
The similarity between images could be computed using different and possibly
multimodal features such as color or edge information or even text labels. This
motivates the design of image analysis solutions that are able to effectively
integrate the multi-view information provided by different feature sets. We
therefore propose a new image retrieval solution that is able to sort images
through a random walk on a multi-layer graph, where each layer corresponds to a
different type of information about the image data. We study in depth the
design of the image graph and propose in particular an effective method to
select the edge weights for the multi-layer graph, such that the image ranking
scores are optimised. We then provide extensive experiments in different
real-world photo collections, which confirm the high performance of our new
image retrieval algorithm that generally surpasses state-of-the-art solutions
due to a more meaningful image similarity computation.</p>
</td>
    <td>
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/jain2016approximate/">Approximate search with quantized sparse representations</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Approximate search with quantized sparse representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Approximate search with quantized sparse representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jain Himalaya, Pérez Patrick, Gribonval Rémi, Zepeda Joaquin, Jégou Hervé</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>17</td>
    <td><p>This paper tackles the task of storing a large collection of vectors, such as
visual descriptors, and of searching in it. To this end, we propose to
approximate database vectors by constrained sparse coding, where possible atom
weights are restricted to belong to a finite subset. This formulation
encompasses, as particular cases, previous state-of-the-art methods such as
product or residual quantization. As opposed to traditional sparse coding
methods, quantized sparse coding includes memory usage as a design constraint,
thereby allowing us to index a large collection such as the BIGANN
billion-sized benchmark. Our experiments, carried out on standard benchmarks,
show that our formulation leads to competitive solutions when considering
different trade-offs between learning/coding time, index size and search
quality.</p>
</td>
    <td>
      
        Memory-Efficiency 
      
        Quantization 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/castrejon2016learning/">Learning Aligned Cross-Modal Representations from Weakly Aligned Data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Aligned Cross-Modal Representations from Weakly Aligned Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Aligned Cross-Modal Representations from Weakly Aligned Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Castrejon Lluis, Aytar Yusuf, Vondrick Carl, Pirsiavash Hamed, Torralba Antonio</td> <!-- 🔧 You were missing this -->
    <td>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>199</td>
    <td><p>People can recognize scenes across many different modalities beyond natural
images. In this paper, we investigate how to learn cross-modal scene
representations that transfer across modalities. To study this problem, we
introduce a new cross-modal scene dataset. While convolutional neural networks
can categorize cross-modal scenes well, they also learn an intermediate
representation not aligned across modalities, which is undesirable for
cross-modal transfer applications. We present methods to regularize cross-modal
convolutional neural networks so that they have a shared representation that is
agnostic of the modality. Our experiments suggest that our scene representation
can help transfer representations across modalities for retrieval. Moreover,
our visualizations suggest that units emerge in the shared representation that
tend to activate on consistent concepts independently of the modality.</p>
</td>
    <td>
      
        Datasets 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/carreiraperpi%C3%B1%C3%A1n2016ensemble/">An ensemble diversity approach to supervised binary hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=An ensemble diversity approach to supervised binary hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=An ensemble diversity approach to supervised binary hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Carreira-perpiñán Miguel Á., Raziperchikolaei Ramin</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>8</td>
    <td><p>Binary hashing is a well-known approach for fast approximate nearest-neighbor
search in information retrieval. Much work has focused on affinity-based
objective functions involving the hash functions or binary codes. These
objective functions encode neighborhood information between data points and are
often inspired by manifold learning algorithms. They ensure that the hash
functions differ from each other through constraints or penalty terms that
encourage codes to be orthogonal or dissimilar across bits, but this couples
the binary variables and complicates the already difficult optimization. We
propose a much simpler approach: we train each hash function (or bit)
independently from each other, but introduce diversity among them using
techniques from classifier ensembles. Surprisingly, we find that not only is
this faster and trivially parallelizable, but it also improves over the more
complex, coupled objective function, and achieves state-of-the-art precision
and recall in experiments with image retrieval.</p>
</td>
    <td>
      
        Supervised 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Compact-Codes 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/cao2025correlation/">Correlation Autoencoder Hashing for Supervised Cross-Modal Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Correlation Autoencoder Hashing for Supervised Cross-Modal Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Correlation Autoencoder Hashing for Supervised Cross-Modal Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cao Yue, Long, Wang, Zhu</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval</td>
    <td>98</td>
    <td><p>Hashing is widely applied to approximate nearest neighbor search for large-scale multimodal retrieval with storage and computation efficiency. Cross-modal hashing improves the quality of hash coding by exploiting semantic correlations across different modalities. Existing cross-modal hashing methods first transform data into low-dimensional feature vectors, and then generate binary codes by another separate quantization step. However, suboptimal hash codes may be generated since the quantization error is not explicitly minimized and the feature representation is not jointly optimized with the binary codes.
This paper presents a Correlation Hashing Network (CHN) approach to cross-modal hashing, which jointly learns good data representation tailored to hash coding and formally controls the quantization error. The proposed CHN is a hybrid deep architecture that constitutes a convolutional neural network for learning good image representations, a multilayer perception for learning good text representations, two hashing layers for generating compact binary codes, and a structured max-margin loss that integrates all things together to enable learning similarity-preserving and high-quality hash codes. Extensive empirical study shows that CHN yields state of the art cross-modal retrieval performance on standard benchmarks.</p>
</td>
    <td>
      
        Scalability 
      
        Efficiency 
      
        Multimodal-Retrieval 
      
        Compact-Codes 
      
        Evaluation 
      
        Quantization 
      
        Hashing-Methods 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/cao2025deep/">Deep Visual-Semantic Hashing for Cross-Modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Visual-Semantic Hashing for Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Visual-Semantic Hashing for Cross-Modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cao Yue, Long, Wang, Yang, Yu</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</td>
    <td>264</td>
    <td><p>Due to the storage and retrieval efficiency, hashing has been
widely applied to approximate nearest neighbor search for
large-scale multimedia retrieval. Cross-modal hashing, which
enables efficient retrieval of images in response to text queries
or vice versa, has received increasing attention recently. Most
existing work on cross-modal hashing does not capture the
spatial dependency of images and temporal dynamics of text
sentences for learning powerful feature representations and
cross-modal embeddings that mitigate the heterogeneity of
different modalities. This paper presents a new Deep Visual Semantic Hashing (DVSH) model that generates compact
hash codes of images and sentences in an end-to-end deep
learning architecture, which capture the intrinsic cross-modal
correspondences between visual data and natural language.
DVSH is a hybrid deep architecture that constitutes a visual semantic fusion network for learning joint embedding space
of images and text sentences, and two modality-specific hashing networks for learning hash functions to generate compact
binary codes. Our architecture effectively unifies joint multimodal embedding and cross-modal hashing, which is based
on a novel combination of Convolutional Neural Networks
over images, Recurrent Neural Networks over sentences, and
a structured max-margin objective that integrates all things
together to enable learning of similarity-preserving and highquality hash codes. Extensive empirical evidence shows that
our DVSH approach yields state of the art results in crossmodal retrieval experiments on image-sentences datasets,
i.e. standard IAPR TC-12 and large-scale Microsoft COCO.</p>
</td>
    <td>
      
        Scalability 
      
        Efficiency 
      
        Datasets 
      
        Text-Retrieval 
      
        Multimodal-Retrieval 
      
        KDD 
      
        Compact-Codes 
      
        Similarity-Search 
      
        Hashing-Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/srivastava20173d/">3D Binary Signatures</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=3D Binary Signatures' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=3D Binary Signatures' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Srivastava Siddharth, Lall Brejesh</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the Tenth Indian Conference on Computer Vision, Graphics and Image Processing</td>
    <td>8</td>
    <td><p>In this paper, we propose a novel binary descriptor for 3D point clouds. The
proposed descriptor termed as 3D Binary Signature (3DBS) is motivated from the
matching efficiency of the binary descriptors for 2D images. 3DBS describes
keypoints from point clouds with a binary vector resulting in extremely fast
matching. The method uses keypoints from standard keypoint detectors. The
descriptor is built by constructing a Local Reference Frame and aligning a
local surface patch accordingly. The local surface patch constitutes of
identifying nearest neighbours based upon an angular constraint among them. The
points are ordered with respect to the distance from the keypoints. The normals
of the ordered pairs of these keypoints are projected on the axes and the
relative magnitude is used to assign a binary digit. The vector thus
constituted is used as a signature for representing the keypoints. The matching
is done by using hamming distance. We show that 3DBS outperforms state of the
art descriptors on various evaluation metrics.</p>
</td>
    <td>
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/pacuk2016locality/">Locality-Sensitive Hashing without False Negatives for l_p</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Locality-Sensitive Hashing without False Negatives for l_p' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Locality-Sensitive Hashing without False Negatives for l_p' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Pacuk Andrzej, Sankowski Piotr, Wegrzycki Karol, Wygocki Piotr</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>7</td>
    <td><p>In this paper, we show a construction of locality-sensitive hash functions
without false negatives, i.e., which ensure collision for every pair of points
within a given radius \(R\) in \(d\) dimensional space equipped with \(l_p\) norm
when \(p \in [1,\infty]\). Furthermore, we show how to use these hash functions
to solve the \(c\)-approximate nearest neighbor search problem without false
negatives. Namely, if there is a point at distance \(R\), we will certainly
report it and points at distance greater than \(cR\) will not be reported for
\(c=Ω(\sqrt{d},d^{1-\frac{1}{p}})\). The constructed algorithms work: - with
preprocessing time \(\mathcal{O}(n log(n))\) and sublinear expected query time,</p>
<ul>
  <li>with preprocessing time \(\mathcal{O}(\mathrm{poly}(n))\) and expected query
time \(\mathcal{O}(log(n))\). Our paper reports progress on answering the open
problem presented by Pagh [8] who considered the nearest neighbor search
without false negatives for the Hamming distance.</li>
</ul>
</td>
    <td>
      
        Hashing-Methods 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/cao2016correlation/">Correlation Autoencoder Hashing for Supervised Cross-Modal Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Correlation Autoencoder Hashing for Supervised Cross-Modal Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Correlation Autoencoder Hashing for Supervised Cross-Modal Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cao Yue, Long, Wang, Zhu</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval</td>
    <td>98</td>
    <td><p>Due to its storage and query efficiency, hashing has been widely
applied to approximate nearest neighbor search from large-scale
datasets. While there is increasing interest in cross-modal hashing
which facilitates cross-media retrieval by embedding data from different modalities into a common Hamming space, how to distill the
cross-modal correlation structure effectively remains a challenging
problem. In this paper, we propose a novel supervised cross-modal
hashing method, Correlation Autoencoder Hashing (CAH), to learn
discriminative and compact binary codes based on deep autoencoders. Specifically, CAH jointly maximizes the feature correlation
revealed by bimodal data and the semantic correlation conveyed in
similarity labels, while embeds them into hash codes by nonlinear
deep autoencoders. Extensive experiments clearly show the superior effectiveness and efficiency of CAH against the state-of-the-art
hashing methods on standard cross-modal retrieval benchmarks.</p>
</td>
    <td>
      
        Scalability 
      
        Efficiency 
      
        Datasets 
      
        Multimodal-Retrieval 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/cao2016deep/">Deep Cauchy Hashing for Hamming Space Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Cauchy Hashing for Hamming Space Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Cauchy Hashing for Hamming Space Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cao Yue, Long, Liu, Wang</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</td>
    <td>264</td>
    <td><p>Due to its computation efficiency and retrieval quality,
hashing has been widely applied to approximate nearest
neighbor search for large-scale image retrieval, while deep
hashing further improves the retrieval quality by end-toend representation learning and hash coding. With compact
hash codes, Hamming space retrieval enables the most efficient constant-time search that returns data points within a
given Hamming radius to each query, by hash table lookups
instead of linear scan. However, subject to the weak capability of concentrating relevant images to be within a small
Hamming ball due to mis-specified loss functions, existing deep hashing methods may underperform for Hamming
space retrieval.  This work presents Deep Cauchy Hashing
(DCH), a novel deep hashing model that generates compact
and concentrated binary hash codes to enable efficient and
effective Hamming space retrieval. The main idea is to design a pairwise cross-entropy loss based on Cauchy distribution, which penalizes significantly on similar image pairs
with Hamming distance larger than the given Hamming radius threshold. Comprehensive experiments demonstrate
that DCH can generate highly concentrated hash codes and
yield state-of-the-art Hamming space retrieval performance
on three datasets, NUS-WIDE, CIFAR-10, and MS-COCO.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Efficiency 
      
        Datasets 
      
        Neural-Hashing 
      
        KDD 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/roy2016representing/">Representing Documents and Queries as Sets of Word Embedded Vectors for Information Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Representing Documents and Queries as Sets of Word Embedded Vectors for Information Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Representing Documents and Queries as Sets of Word Embedded Vectors for Information Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Roy Dwaipayan, Ganguly Debasis, Mitra Mandar, Jones Gareth J. F.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>11</td>
    <td><p>A major difficulty in applying word vector embeddings in IR is in devising an
effective and efficient strategy for obtaining representations of compound
units of text, such as whole documents, (in comparison to the atomic words),
for the purpose of indexing and scoring documents. Instead of striving for a
suitable method for obtaining a single vector representation of a large
document of text, we rather aim for developing a similarity metric that makes
use of the similarities between the individual embedded word vectors in a
document and a query. More specifically, we represent a document and a query as
sets of word vectors, and use a standard notion of similarity measure between
these sets, computed as a function of the similarities between each constituent
word pair from these sets. We then make use of this similarity measure in
combination with standard IR based similarities for document ranking. The
results of our initial experimental investigations shows that our proposed
method improves MAP by up to \(5.77%\), in comparison to standard text-based
language model similarity, on the TREC ad-hoc dataset.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/pagh2016approximate/">Approximate Furthest Neighbor with Application to Annulus Query</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Approximate Furthest Neighbor with Application to Annulus Query' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Approximate Furthest Neighbor with Application to Annulus Query' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Pagh Rasmus, Silvestri Francesco, Sivertsen Johan, Skala Matthew</td> <!-- 🔧 You were missing this -->
    <td>Information Systems</td>
    <td>10</td>
    <td><p>Much recent work has been devoted to approximate nearest neighbor queries.
Motivated by applications in recommender systems, we consider approximate
furthest neighbor (AFN) queries and present a simple, fast, and highly
practical data structure for answering AFN queries in high- dimensional
Euclidean space. The method builds on the technique of In- dyk (SODA 2003),
storing random projections to provide sublinear query time for AFN. However, we
introduce a different query algorithm, improving on Indyk’s approximation
factor and reducing the running time by a logarithmic factor. We also present a
variation based on a query- independent ordering of the database points; while
this does not have the provable approximation factor of the query-dependent
data structure, it offers significant improvement in time and space complexity.
We give a theoretical analysis, and experimental results. As an application,
the query-dependent approach is used for deriving a data structure for the
approximate annulus query problem, which is defined as follows: given an input
set S and two parameters r &gt; 0 and w &gt;= 1, construct a data structure that
returns for each query point q a point p in S such that the distance between p
and q is at least r/w and at most wr.</p>
</td>
    <td>
      
        Recommender-Systems 
      
        Locality-Sensitive-Hashing 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/xia2016unsupervised/">Unsupervised Deep Hashing for Large-scale Visual Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Deep Hashing for Large-scale Visual Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Deep Hashing for Large-scale Visual Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xia Zhaoqiang, Feng Xiaoyi, Peng Jinye, Hadid Abdenour</td> <!-- 🔧 You were missing this -->
    <td>2016 Sixth International Conference on Image Processing Theory, Tools and Applications (IPTA)</td>
    <td>17</td>
    <td><p>Learning based hashing plays a pivotal role in large-scale visual search.
However, most existing hashing algorithms tend to learn shallow models that do
not seek representative binary codes. In this paper, we propose a novel hashing
approach based on unsupervised deep learning to hierarchically transform
features into hash codes. Within the heterogeneous deep hashing framework, the
autoencoder layers with specific constraints are considered to model the
nonlinear mapping between features and binary codes. Then, a Restricted
Boltzmann Machine (RBM) layer with constraints is utilized to reduce the
dimension in the hamming space. Extensive experiments on the problem of visual
search demonstrate the competitiveness of our proposed approach compared to
state-of-the-art.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Unsupervised 
      
        Scalability 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/zhuang2016fast/">Fast Training of Triplet-based Deep Binary Embedding Networks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Training of Triplet-based Deep Binary Embedding Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Training of Triplet-based Deep Binary Embedding Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhuang Bohan, Lin Guosheng, Shen Chunhua, Reid Ian</td> <!-- 🔧 You were missing this -->
    <td>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>136</td>
    <td><p>In this paper, we aim to learn a mapping (or embedding) from images to a
compact binary space in which Hamming distances correspond to a ranking measure
for the image retrieval task.
  We make use of a triplet loss because this has been shown to be most
effective for ranking problems.
  However, training in previous works can be prohibitively expensive due to the
fact that optimization is directly performed on the triplet space, where the
number of possible triplets for training is cubic in the number of training
examples.
  To address this issue, we propose to formulate high-order binary codes
learning as a multi-label classification problem by explicitly separating
learning into two interleaved stages.
  To solve the first stage, we design a large-scale high-order binary codes
inference algorithm to reduce the high-order objective to a standard binary
quadratic problem such that graph cuts can be used to efficiently infer the
binary code which serve as the label of each training datum.
  In the second stage we propose to map the original image to compact binary
codes via carefully designed deep convolutional neural networks (CNNs) and the
hashing function fitting can be solved by training binary CNN classifiers.
  An incremental/interleaved optimization strategy is proffered to ensure that
these two steps are interactive with each other during training for better
accuracy.
  We conduct experiments on several benchmark datasets, which demonstrate both
improved training time (by as much as two orders of magnitude) as well as
producing state-of-the-art hashing for various retrieval tasks.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Compact-Codes 
      
        CVPR 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/murray2016interferences/">Interferences in match kernels</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Interferences in match kernels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Interferences in match kernels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Murray Naila, Jégou Hervé, Perronnin Florent, Zisserman Andrew</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>30</td>
    <td><p>We consider the design of an image representation that embeds and aggregates
a set of local descriptors into a single vector. Popular representations of
this kind include the bag-of-visual-words, the Fisher vector and the VLAD. When
two such image representations are compared with the dot-product, the
image-to-image similarity can be interpreted as a match kernel. In match
kernels, one has to deal with interference, i.e. with the fact that even if two
descriptors are unrelated, their matching score may contribute to the overall
similarity.
  We formalise this problem and propose two related solutions, both aimed at
equalising the individual contributions of the local descriptors in the final
representation. These methods modify the aggregation stage by including a set
of per-descriptor weights. They differ by the objective function that is
optimised to compute those weights. The first is a “democratisation” strategy
that aims at equalising the relative importance of each descriptor in the set
comparison metric. The second one involves equalising the match of a single
descriptor to the aggregated vector.
  These concurrent methods give a substantial performance boost over the state
of the art in image search with short or mid-size vectors, as demonstrated by
our experiments on standard public image retrieval benchmarks.</p>
</td>
    <td>
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/ning2016scalable/">Scalable Image Retrieval by Sparse Product Quantization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Scalable Image Retrieval by Sparse Product Quantization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Scalable Image Retrieval by Sparse Product Quantization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ning Qingqun, Zhu Jianke, Zhong Zhiyuan, Hoi Steven C. H., Chen Chun</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>40</td>
    <td><p>Fast Approximate Nearest Neighbor (ANN) search technique for high-dimensional
feature indexing and retrieval is the crux of large-scale image retrieval. A
recent promising technique is Product Quantization, which attempts to index
high-dimensional image features by decomposing the feature space into a
Cartesian product of low dimensional subspaces and quantizing each of them
separately. Despite the promising results reported, their quantization approach
follows the typical hard assignment of traditional quantization methods, which
may result in large quantization errors and thus inferior search performance.
Unlike the existing approaches, in this paper, we propose a novel approach
called Sparse Product Quantization (SPQ) to encoding the high-dimensional
feature vectors into sparse representation. We optimize the sparse
representations of the feature vectors by minimizing their quantization errors,
making the resulting representation is essentially close to the original data
in practice. Experiments show that the proposed SPQ technique is not only able
to compress data, but also an effective encoding technique. We obtain
state-of-the-art results for ANN search on four public image datasets and the
promising results of content-based image retrieval further validate the
efficacy of our proposed method.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Image-Retrieval 
      
        Datasets 
      
        Scalability 
      
        Quantization 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/wang2016contextual/">Contextual Visual Similarity</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Contextual Visual Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Contextual Visual Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Xiaofang, Kitani Kris M., Hebert Martial</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>7</td>
    <td><p>Measuring visual similarity is critical for image understanding. But what
makes two images similar? Most existing work on visual similarity assumes that
images are similar because they contain the same object instance or category.
However, the reason why images are similar is much more complex. For example,
from the perspective of category, a black dog image is similar to a white dog
image. However, in terms of color, a black dog image is more similar to a black
horse image than the white dog image. This example serves to illustrate that
visual similarity is ambiguous but can be made precise when given an explicit
contextual perspective. Based on this observation, we propose the concept of
contextual visual similarity. To be concrete, we examine the concept of
contextual visual similarity in the application domain of image search. Instead
of providing only a single image for image similarity search (\eg, Google image
search), we require three images. Given a query image, a second positive image
and a third negative image, dissimilar to the first two images, we define a
contextualized similarity search criteria. In particular, we learn feature
weights over all the feature dimensions of each image such that the distance
between the query image and the positive image is small and their distances to
the negative image are large after reweighting their features. The learned
feature weights encode the contextualized visual similarity specified by the
user and can be used for attribute specific image search. We also show the
usefulness of our contextualized similarity weighting scheme for different
tasks, such as answering visual analogy questions and unsupervised attribute
discovery.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Unsupervised 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/zhang2019collaborative/">Collaborative Quantization for Cross-Modal Similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Collaborative Quantization for Cross-Modal Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Collaborative Quantization for Cross-Modal Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Ting, Wang Jingdong</td> <!-- 🔧 You were missing this -->
    <td>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>62</td>
    <td><p>Cross-modal similarity search is a problem about designing a search system
supporting querying across content modalities, e.g., using an image to search
for texts or using a text to search for images. This paper presents a compact
coding solution for efficient search, with a focus on the quantization approach
which has already shown the superior performance over the hashing solutions in
the single-modal similarity search. We propose a cross-modal quantization
approach, which is among the early attempts to introduce quantization into
cross-modal search. The major contribution lies in jointly learning the
quantizers for both modalities through aligning the quantized representations
for each pair of image and text belonging to a document. In addition, our
approach simultaneously learns the common space for both modalities in which
quantization is conducted to enable efficient and effective search using the
Euclidean distance computed in the common space with fast distance table
lookup. Experimental results compared with several competitive algorithms over
three benchmark datasets demonstrate that the proposed approach achieves the
state-of-the-art performance.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        CVPR 
      
        Quantization 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/pham2016scalability/">Scalability and Total Recall with Fast CoveringLSH</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Scalability and Total Recall with Fast CoveringLSH' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Scalability and Total Recall with Fast CoveringLSH' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Pham Ninh, Pagh Rasmus</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</td>
    <td>11</td>
    <td><p>Locality-sensitive hashing (LSH) has emerged as the dominant algorithmic
technique for similarity search with strong performance guarantees in
high-dimensional spaces. A drawback of traditional LSH schemes is that they may
have <em>false negatives</em>, i.e., the recall is less than 100%. This limits
the applicability of LSH in settings requiring precise performance guarantees.
Building on the recent theoretical “CoveringLSH” construction that eliminates
false negatives, we propose a fast and practical covering LSH scheme for
Hamming space called <em>Fast CoveringLSH (fcLSH)</em>. Inheriting the design
benefits of CoveringLSH our method avoids false negatives and always reports
all near neighbors. Compared to CoveringLSH we achieve an asymptotic
improvement to the hash function computation time from \(\mathcal{O}(dL)\) to
\(\mathcal{O}(d + Llog{L})\), where \(d\) is the dimensionality of data and \(L\) is
the number of hash tables. Our experiments on synthetic and real-world data
sets demonstrate that <em>fcLSH</em> is comparable (and often superior) to
traditional hashing-based approaches for search radius up to 20 in
high-dimensional Hamming space.</p>
</td>
    <td>
      
        Similarity-Search 
      
        CIKM 
      
        Locality-Sensitive-Hashing 
      
        Hashing-Methods 
      
        Scalability 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/salvi2016bloom/">Bloom Filters and Compact Hash Codes for Efficient and Distributed Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Bloom Filters and Compact Hash Codes for Efficient and Distributed Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Bloom Filters and Compact Hash Codes for Efficient and Distributed Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Salvi Andrea, Ercoli Simone, Bertini Marco, del Bimbo Alberto</td> <!-- 🔧 You were missing this -->
    <td>2016 IEEE International Symposium on Multimedia (ISM)</td>
    <td>5</td>
    <td><p>This paper presents a novel method for efficient image retrieval, based on a
simple and effective hashing of CNN features and the use of an indexing
structure based on Bloom filters. These filters are used as gatekeepers for the
database of image features, allowing to avoid to perform a query if the query
features are not stored in the database and speeding up the query process,
without affecting retrieval performance. Thanks to the limited memory
requirements the system is suitable for mobile applications and distributed
databases, associating each filter to a distributed portion of the database.
Experimental validation has been performed on three standard image retrieval
datasets, outperforming state-of-the-art hashing methods in terms of precision,
while the proposed indexing method obtains a \(2\times\) speedup.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Hashing-Methods 
      
        Datasets 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/zhang2025efficient/">Efficient Training of Very Deep Neural Networks for Supervised Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Training of Very Deep Neural Networks for Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient Training of Very Deep Neural Networks for Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Ziming, Chen, Saligrama</td> <!-- 🔧 You were missing this -->
    <td>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>112</td>
    <td><p>In this paper, we propose training very deep neural networks (DNNs) for supervised learning of hash codes. Existing methods in this context train relatively “shallow” networks limited by the issues arising in back propagation (e.e. vanishing gradients) as well as computational efficiency. We propose a novel and efficient training algorithm inspired by alternating direction method of multipliers (ADMM) that overcomes some of these limitations. Our method decomposes the training process into independent layer-wise local updates through auxiliary variables. Empirically we observe that our training algorithm always converges and its computational complexity is linearly proportional to the number of edges in the networks. Empirically we manage to train DNNs with 64 hidden layers and 1024 nodes per layer for supervised hashing in about 3 hours using a single GPU. Our proposed very deep supervised hashing (VDSH) method significantly outperforms the state-of-the-art on several benchmark datasets.</p>
</td>
    <td>
      
        Efficiency 
      
        Datasets 
      
        CVPR 
      
        Neural-Hashing 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/riazi2016sub/">Sub-Linear Privacy-Preserving Near-Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Sub-Linear Privacy-Preserving Near-Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Sub-Linear Privacy-Preserving Near-Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Riazi M. Sadegh, Chen Beidi, Shrivastava Anshumali, Wallach Dan, Koushanfar Farinaz</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>11</td>
    <td><p>In Near-Neighbor Search (NNS), a new client queries a database (held by a
server) for the most similar data (near-neighbors) given a certain similarity
metric. The Privacy-Preserving variant (PP-NNS) requires that neither server
nor the client shall learn information about the other party’s data except what
can be inferred from the outcome of NNS. The overwhelming growth in the size of
current datasets and the lack of a truly secure server in the online world
render the existing solutions impractical; either due to their high
computational requirements or non-realistic assumptions which potentially
compromise privacy. PP-NNS having query time {\it sub-linear} in the size of
the database has been suggested as an open research direction by Li et al.
(CCSW’15). In this paper, we provide the first such algorithm, called Secure
Locality Sensitive Indexing (SLSI) which has a sub-linear query time and the
ability to handle honest-but-curious parties. At the heart of our proposal lies
a secure binary embedding scheme generated from a novel probabilistic
transformation over locality sensitive hashing family. We provide information
theoretic bound for the privacy guarantees and support our theoretical claims
using substantial empirical evidence on real-world datasets.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        Locality-Sensitive-Hashing 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/zhu2025deep/">Deep Hashing Network for Efficient Similarity Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Hashing Network for Efficient Similarity Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Hashing Network for Efficient Similarity Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhu Han, Long, Wang, Cao</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>636</td>
    <td><p>Due to the storage and retrieval efficiency, hashing has been widely deployed to approximate nearest neighbor search for large-scale multimedia retrieval. Supervised hashing, which improves the quality of hash coding by exploiting the semantic similarity on data pairs, has received increasing attention recently. For most existing supervised hashing methods for image retrieval, an image is first represented as a vector of hand-crafted or machine-learned features, followed by another separate quantization step that generates binary codes.
However, suboptimal hash coding may be produced, because the quantization error is not statistically minimized and the feature representation is not optimally compatible with the binary coding. In this paper, we propose a novel Deep Hashing Network (DHN) architecture for supervised hashing, in which we jointly learn good image representation tailored to hash coding and formally control the quantization error.
The DHN model constitutes four key components: (1) a sub-network with multiple convolution-pooling layers to capture image representations; (2) a fully-connected hashing layer to generate compact binary hash codes; (3) a pairwise cross-entropy loss layer for similarity-preserving learning; and (4) a pairwise quantization loss for controlling hashing quality. Extensive experiments on standard image retrieval datasets show the proposed DHN model yields substantial boosts over latest state-of-the-art hashing methods.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Efficiency 
      
        Datasets 
      
        Neural-Hashing 
      
        Quantization 
      
        Compact-Codes 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        AAAI 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/mohedano2016bags/">Bags of Local Convolutional Features for Scalable Instance Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Bags of Local Convolutional Features for Scalable Instance Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Bags of Local Convolutional Features for Scalable Instance Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mohedano Eva, Salvador Amaia, Mcguinness Kevin, Marques Ferran, O'connor Noel E., Giro-i-nieto Xavier</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval</td>
    <td>154</td>
    <td><p>This work proposes a simple instance retrieval pipeline based on encoding the
convolutional features of CNN using the bag of words aggregation scheme (BoW).
Assigning each local array of activations in a convolutional layer to a visual
word produces an \textit{assignment map}, a compact representation that relates
regions of an image with a visual word. We use the assignment map for fast
spatial reranking, obtaining object localizations that are used for query
expansion. We demonstrate the suitability of the BoW representation based on
local CNN features for instance retrieval, achieving competitive performance on
the Oxford and Paris buildings benchmarks. We show that our proposed system for
CNN feature aggregation with BoW outperforms state-of-the-art techniques using
sum pooling at a subset of the challenging TRECVid INS benchmark.</p>
</td>
    <td>
      
        Evaluation 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/otani2016learning/">Learning Joint Representations of Videos and Sentences with Web Image Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Joint Representations of Videos and Sentences with Web Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Joint Representations of Videos and Sentences with Web Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Otani Mayu, Nakashima Yuta, Rahtu Esa, Heikkilä Janne, Yokoya Naokazu</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>92</td>
    <td><p>Our objective is video retrieval based on natural language queries. In
addition, we consider the analogous problem of retrieving sentences or
generating descriptions given an input video. Recent work has addressed the
problem by embedding visual and textual inputs into a common space where
semantic similarities correlate to distances. We also adopt the embedding
approach, and make the following contributions: First, we utilize web image
search in sentence embedding process to disambiguate fine-grained visual
concepts. Second, we propose embedding models for sentence, image, and video
inputs whose parameters are learned simultaneously. Finally, we show how the
proposed model can be applied to description generation. Overall, we observe a
clear improvement over the state-of-the-art methods in the video and sentence
retrieval tasks. In description generation, the performance level is comparable
to the current state-of-the-art, although our embeddings were trained for the
retrieval tasks.</p>
</td>
    <td>
      
        Video-Retrieval 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/mor%C3%A8re2016group/">Group Invariant Deep Representations for Image Instance Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Group Invariant Deep Representations for Image Instance Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Group Invariant Deep Representations for Image Instance Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Morère Olivier, Veillard Antoine, Lin Jie, Petta Julie, Chandrasekhar Vijay, Poggio Tomaso</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>6</td>
    <td><p>Most image instance retrieval pipelines are based on comparison of vectors
known as global image descriptors between a query image and the database
images. Due to their success in large scale image classification,
representations extracted from Convolutional Neural Networks (CNN) are quickly
gaining ground on Fisher Vectors (FVs) as state-of-the-art global descriptors
for image instance retrieval. While CNN-based descriptors are generally
remarked for good retrieval performance at lower bitrates, they nevertheless
present a number of drawbacks including the lack of robustness to common object
transformations such as rotations compared with their interest point based FV
counterparts.
  In this paper, we propose a method for computing invariant global descriptors
from CNNs. Our method implements a recently proposed mathematical theory for
invariance in a sensory cortex modeled as a feedforward neural network. The
resulting global descriptors can be made invariant to multiple arbitrary
transformation groups while retaining good discriminativeness.
  Based on a thorough empirical evaluation using several publicly available
datasets, we show that our method is able to significantly and consistently
improve retrieval results every time a new type of invariance is incorporated.
We also show that our method which has few parameters is not prone to
overfitting: improvements generalize well across datasets with different
properties with regard to invariances. Finally, we show that our descriptors
are able to compare favourably to other state-of-the-art compact descriptors in
similar bitranges, exceeding the highest retrieval results reported in the
literature on some datasets. A dedicated dimensionality reduction step
–quantization or hashing– may be able to further improve the competitiveness
of the descriptors.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        Quantization 
      
        Evaluation 
      
        Robustness 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/kalantidis2015cross/">Cross-dimensional Weighting for Aggregated Deep Convolutional Features</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Cross-dimensional Weighting for Aggregated Deep Convolutional Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Cross-dimensional Weighting for Aggregated Deep Convolutional Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kalantidis Yannis, Mellina Clayton, Osindero Simon</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>418</td>
    <td><p>We propose a simple and straightforward way of creating powerful image
representations via cross-dimensional weighting and aggregation of deep
convolutional neural network layer outputs. We first present a generalized
framework that encompasses a broad family of approaches and includes
cross-dimensional pooling and weighting steps. We then propose specific
non-parametric schemes for both spatial- and channel-wise weighting that boost
the effect of highly active spatial responses and at the same time regulate
burstiness effects. We experiment on different public datasets for image search
and show that our approach outperforms the current state-of-the-art for
approaches based on pre-trained networks. We also provide an easy-to-use, open
source implementation that reproduces our results.</p>
</td>
    <td>
      
        Datasets 
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/liang2016optimizing/">Optimizing Top Precision Performance Measure of Content-Based Image Retrieval by Learning Similarity Function</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Optimizing Top Precision Performance Measure of Content-Based Image Retrieval by Learning Similarity Function' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Optimizing Top Precision Performance Measure of Content-Based Image Retrieval by Learning Similarity Function' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liang Ru-ze, Shi Lihui, Wang Haoxiang, Meng Jiandong, Wang Jim Jing-yan, Sun Qingquan, Gu Yi</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>52</td>
    <td><p>In this paper we study the problem of content-based image retrieval. In this
problem, the most popular performance measure is the top precision measure, and
the most important component of a retrieval system is the similarity function
used to compare a query image against a database image. However, up to now,
there is no existing similarity learning method proposed to optimize the top
precision measure. To fill this gap, in this paper, we propose a novel
similarity learning method to maximize the top precision measure. We model this
problem as a minimization problem with an objective function as the combination
of the losses of the relevant images ranked behind the top-ranked irrelevant
image, and the squared Frobenius norm of the similarity function parameter.
This minimization problem is solved as a quadratic programming problem. The
experiments over two benchmark data sets show the advantages of the proposed
method over other similarity learning methods when the top precision is used as
the performance measure.</p>
</td>
    <td>
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/tizhoosh2016minmax/">MinMax Radon Barcodes for Medical Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=MinMax Radon Barcodes for Medical Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=MinMax Radon Barcodes for Medical Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tizhoosh H. R., Zhu Shujin, Lo Hanson, Chaudhari Varun, Mehdi Tahmid</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>38</td>
    <td><p>Content-based medical image retrieval can support diagnostic decisions by
clinical experts. Examining similar images may provide clues to the expert to
remove uncertainties in his/her final diagnosis. Beyond conventional feature
descriptors, binary features in different ways have been recently proposed to
encode the image content. A recent proposal is “Radon barcodes” that employ
binarized Radon projections to tag/annotate medical images with content-based
binary vectors, called barcodes. In this paper, MinMax Radon barcodes are
introduced which are superior to “local thresholding” scheme suggested in the
literature. Using IRMA dataset with 14,410 x-ray images from 193 different
classes, the advantage of using MinMax Radon barcodes over <em>thresholded</em>
Radon barcodes are demonstrated. The retrieval error for direct search drops by
more than 15%. As well, SURF, as a well-established non-binary approach, and
BRISK, as a recent binary method are examined to compare their results with
MinMax Radon barcodes when retrieving images from IRMA dataset. The results
demonstrate that MinMax Radon barcodes are faster and more accurate when
applied on IRMA images.</p>
</td>
    <td>
      
        Datasets 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/wang2016comprehensive/">A Comprehensive Survey on Cross-modal Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A Comprehensive Survey on Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A Comprehensive Survey on Cross-modal Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Kaiye, Yin Qiyue, Wang Wei, Wu Shu, Wang Liang</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>230</td>
    <td><p>In recent years, cross-modal retrieval has drawn much attention due to the
rapid growth of multimodal data. It takes one type of data as the query to
retrieve relevant data of another type. For example, a user can use a text to
retrieve relevant pictures or videos. Since the query and its retrieved results
can be of different modalities, how to measure the content similarity between
different modalities of data remains a challenge. Various methods have been
proposed to deal with such a problem. In this paper, we first review a number
of representative methods for cross-modal retrieval and classify them into two
main groups: 1) real-valued representation learning, and 2) binary
representation learning. Real-valued representation learning methods aim to
learn real-valued common representations for different modalities of data. To
speed up the cross-modal retrieval, a number of binary representation learning
methods are proposed to map different modalities of data into a common Hamming
space. Then, we introduce several multimodal datasets in the community, and
show the experimental results on two commonly used multimodal datasets. The
comparison reveals the characteristic of different kinds of cross-modal
retrieval methods, which is expected to benefit both practical applications and
future research. Finally, we discuss open problems and future research
directions.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        Survey-Paper 
      
        Evaluation 
      
        Multimodal-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/zhou2016transfer/">Transfer Hashing with Privileged Information</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Transfer Hashing with Privileged Information' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Transfer Hashing with Privileged Information' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhou Joey Tianyi, Xu Xinxing, Pan Sinno Jialin, Tsang Ivor W., Qin Zheng, Goh Rick Siow Mong</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>25</td>
    <td><p>Most existing learning to hash methods assume that there are sufficient data,
either labeled or unlabeled, on the domain of interest (i.e., the target
domain) for training. However, this assumption cannot be satisfied in some
real-world applications. To address this data sparsity issue in hashing,
inspired by transfer learning, we propose a new framework named Transfer
Hashing with Privileged Information (THPI). Specifically, we extend the
standard learning to hash method, Iterative Quantization (ITQ), in a transfer
learning manner, namely ITQ+. In ITQ+, a new slack function is learned from
auxiliary data to approximate the quantization error in ITQ. We developed an
alternating optimization approach to solve the resultant optimization problem
for ITQ+. We further extend ITQ+ to LapITQ+ by utilizing the geometry structure
among the auxiliary data for learning more precise binary codes in the target
domain. Extensive experiments on several benchmark datasets verify the
effectiveness of our proposed approaches through comparisons with several
state-of-the-art baselines.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Datasets 
      
        Compact-Codes 
      
        Quantization 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/tian2019global/">Global Hashing System for Fast Image Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Global Hashing System for Fast Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Global Hashing System for Fast Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Tian Dayong, Tao Dacheng</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>14</td>
    <td><p>Hashing methods have been widely investigated for fast approximate nearest
neighbor searching in large data sets. Most existing methods use binary vectors
in lower dimensional spaces to represent data points that are usually real
vectors of higher dimensionality. We divide the hashing process into two steps.
Data points are first embedded in a low-dimensional space, and the global
positioning system method is subsequently introduced but modified for binary
embedding. We devise dataindependent and data-dependent methods to distribute
the satellites at appropriate locations. Our methods are based on finding the
tradeoff between the information losses in these two steps. Experiments show
that our data-dependent method outperforms other methods in different-sized
data sets from 100k to 10M. By incorporating the orthogonality of the code
matrix, both our data-independent and data-dependent methods are particularly
impressive in experiments on longer bits.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/kennedy2016fast/">Fast Cross-Polytope Locality-Sensitive Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Cross-Polytope Locality-Sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Cross-Polytope Locality-Sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kennedy Christopher, Ward Rachel</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>9</td>
    <td><p>We provide a variant of cross-polytope locality sensitive hashing with
respect to angular distance which is provably optimal in asymptotic sensitivity
and enjoys \(\mathcal{O}(d \ln d )\) hash computation time. Building on a recent
result (by Andoni, Indyk, Laarhoven, Razenshteyn, Schmidt, 2015), we show that
optimal asymptotic sensitivity for cross-polytope LSH is retained even when the
dense Gaussian matrix is replaced by a fast Johnson-Lindenstrauss transform
followed by discrete pseudo-rotation, reducing the hash computation time from
\(\mathcal{O}(d^2)\) to \(\mathcal{O}(d \ln d )\). Moreover, our scheme achieves
the optimal rate of convergence for sensitivity. By incorporating a
low-randomness Johnson-Lindenstrauss transform, our scheme can be modified to
require only \(\mathcal{O}(\ln^9(d))\) random bits</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Locality-Sensitive-Hashing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/liu2016generalized/">Generalized residual vector quantization for large scale data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Generalized residual vector quantization for large scale data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Generalized residual vector quantization for large scale data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Shicong, Shao Junru, Lu Hongtao</td> <!-- 🔧 You were missing this -->
    <td>2016 IEEE International Conference on Multimedia and Expo (ICME)</td>
    <td>5</td>
    <td><p>Vector quantization is an essential tool for tasks involving large scale
data, for example, large scale similarity search, which is crucial for
content-based information retrieval and analysis. In this paper, we propose a
novel vector quantization framework that iteratively minimizes quantization
error. First, we provide a detailed review on a relevant vector quantization
method named \textit{residual vector quantization} (RVQ). Next, we propose
\textit{generalized residual vector quantization} (GRVQ) to further improve
over RVQ. Many vector quantization methods can be viewed as the special cases
of our proposed framework. We evaluate GRVQ on several large scale benchmark
datasets for large scale search, classification and object retrieval. We
compared GRVQ with existing methods in detail. Extensive experiments
demonstrate our GRVQ framework substantially outperforms existing methods in
term of quantization accuracy and computation efficiency.</p>
</td>
    <td>
      
        Similarity-Search 
      
        Tools-&-Libraries 
      
        Datasets 
      
        Quantization 
      
        Survey-Paper 
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/hoffer2016semi/">Semi-supervised deep learning by metric embedding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semi-supervised deep learning by metric embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semi-supervised deep learning by metric embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Hoffer Elad, Ailon Nir</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>28</td>
    <td><p>Deep networks are successfully used as classification models yielding
state-of-the-art results when trained on a large number of labeled samples.
These models, however, are usually much less suited for semi-supervised
problems because of their tendency to overfit easily when trained on small
amounts of data. In this work we will explore a new training objective that is
targeting a semi-supervised regime with only a small subset of labeled data.
This criterion is based on a deep metric embedding over distance relations
within the set of labeled samples, together with constraints over the
embeddings of the unlabeled set. The final learned representations are
discriminative in euclidean space, and hence can be used with subsequent
nearest-neighbor classification using the labeled samples.</p>
</td>
    <td>
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/huang2016local/">Local Similarity-Aware Deep Feature Embedding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Local Similarity-Aware Deep Feature Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Local Similarity-Aware Deep Feature Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Huang Chen, Loy Chen Change, Tang Xiaoou</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>124</td>
    <td><p>Existing deep embedding methods in vision tasks are capable of learning a
compact Euclidean space from images, where Euclidean distances correspond to a
similarity metric. To make learning more effective and efficient, hard sample
mining is usually employed, with samples identified through computing the
Euclidean feature distance. However, the global Euclidean distance cannot
faithfully characterize the true feature similarity in a complex visual feature
space, where the intraclass distance in a high-density region may be larger
than the interclass distance in low-density regions. In this paper, we
introduce a Position-Dependent Deep Metric (PDDM) unit, which is capable of
learning a similarity metric adaptive to local feature structure. The metric
can be used to select genuinely hard samples in a local neighborhood to guide
the deep embedding learning in an online and robust manner. The new layer is
appealing in that it is pluggable to any convolutional networks and is trained
end-to-end. Our local similarity-aware feature embedding not only demonstrates
faster convergence and boosted performance on two complex image retrieval
datasets, its large margin nature also leads to superior generalization results
under the large and open set scenarios of transfer learning and zero-shot
learning on ImageNet 2010 and ImageNet-10K datasets.</p>
</td>
    <td>
      
        Few-Shot-&-Zero-Shot 
      
        Image-Retrieval 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/fredriksson2016geometric/">Geometric Near-neighbor Access Tree (GNAT) revisited</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Geometric Near-neighbor Access Tree (GNAT) revisited' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Geometric Near-neighbor Access Tree (GNAT) revisited' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fredriksson Kimmo</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>6</td>
    <td><p>Geometric Near-neighbor Access Tree (GNAT) is a metric space indexing method
based on hierarchical hyperplane partitioning of the space. While GNAT is very
efficient in proximity searching, it has a bad reputation of being a memory
hog. We show that this is partially based on too coarse analysis, and that the
memory requirements can be lowered while at the same time improving the search
efficiency. We also show how to make GNAT memory adaptive in a smooth way, and
that the hyperplane partitioning can be replaced with ball partitioning, which
can further improve the search performance. We conclude with experimental
results showing the new methods can give significant performance boost.</p>
</td>
    <td>
      
        Evaluation 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/kang2016column/">Column Sampling Based Discrete Supervised Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Column Sampling Based Discrete Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Column Sampling Based Discrete Supervised Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kang Wang-cheng, Li, Zhou</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>299</td>
    <td><p>By leveraging semantic (label) information, supervised hashing has demonstrated better accuracy than unsupervised hashing in many real applications. Because the hashing-code learning problem is essentially a discrete optimization problem which is hard to solve, most existing supervised hashing methods try to solve a relaxed continuous optimization problem by dropping the discrete constraints.
However, these methods typically suffer from poor performance due to the errors caused by the relaxation. Some other methods try to directly solve the discrete optimization problem. However, they are typically time-consuming and unscalable. In this paper, we propose a novel method, called column sampling based discrete supervised hashing (COSDISH), to directly learn the discrete hashing code from semantic information.
COSDISH is an iterative method, in each iteration of which several columns are sampled from the semantic similarity matrix and then the hashing code is decomposed into two parts which can be alternately optimized in a discrete way. Theoretical analysis shows that the learning (optimization) algorithm of COSDISH has a constant-approximation bound in each step of the alternating optimization procedure. Empirical results on datasets with semantic labels illustrate that COSDISH can outperform the state-of-the-art methods in real applications like image retrieval.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Datasets 
      
        Neural-Hashing 
      
        Supervised 
      
        Hashing-Methods 
      
        AAAI 
      
        Evaluation 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/shrivastava2016exact/">Exact Weighted Minwise Hashing in Constant Time</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Exact Weighted Minwise Hashing in Constant Time' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Exact Weighted Minwise Hashing in Constant Time' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shrivastava Anshumali</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>7</td>
    <td><p>Weighted minwise hashing (WMH) is one of the fundamental subroutine, required
by many celebrated approximation algorithms, commonly adopted in industrial
practice for large scale-search and learning. The resource bottleneck of the
algorithms is the computation of multiple (typically a few hundreds to
thousands) independent hashes of the data. The fastest hashing algorithm is by
Ioffe \cite{Proc:Ioffe_ICDM10}, which requires one pass over the entire data
vector, \(O(d)\) (\(d\) is the number of non-zeros), for computing one hash.
However, the requirement of multiple hashes demands hundreds or thousands
passes over the data. This is very costly for modern massive dataset.
  In this work, we break this expensive barrier and show an expected constant
amortized time algorithm which computes \(k\) independent and unbiased WMH in
time \(O(k)\) instead of \(O(dk)\) required by Ioffe’s method. Moreover, our
proposal only needs a few bits (5 - 9 bits) of storage per hash value compared
to around \(64\) bits required by the state-of-art-methodologies. Experimental
evaluations, on real datasets, show that for computing 500 WMH, our proposal
can be 60000x faster than the Ioffe’s method without losing any accuracy. Our
method is also around 100x faster than approximate heuristics capitalizing on
the efficient “densified” one permutation hashing schemes
\cite{Proc:OneHashLSH_ICML14}. Given the simplicity of our approach and its
significant advantages, we hope that it will replace existing implementations
in practice.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/zhu2016radon/">Radon Features and Barcodes for Medical Image Retrieval via SVM</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Radon Features and Barcodes for Medical Image Retrieval via SVM' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Radon Features and Barcodes for Medical Image Retrieval via SVM' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhu Shujin, Tizhoosh H. R.</td> <!-- 🔧 You were missing this -->
    <td>2016 International Joint Conference on Neural Networks (IJCNN)</td>
    <td>12</td>
    <td><p>For more than two decades, research has been performed on content-based image
retrieval (CBIR). By combining Radon projections and the support vector
machines (SVM), a content-based medical image retrieval method is presented in
this work. The proposed approach employs the normalized Radon projections with
corresponding image category labels to build an SVM classifier, and the Radon
barcode database which encodes every image in a binary format is also generated
simultaneously to tag all images. To retrieve similar images when a query image
is given, Radon projections and the barcode of the query image are generated.
Subsequently, the k-nearest neighbor search method is applied to find the
images with minimum Hamming distance of the Radon barcode within the same class
predicted by the trained SVM classifier that uses Radon features. The
performance of the proposed method is validated by using the IRMA 2009 dataset
with 14,410 x-ray images in 57 categories. The results demonstrate that our
method has the capacity to retrieve similar responses for the correctly
identified query image and even for those mistakenly classified by SVM. The
approach further is very fast and has low memory requirement.</p>
</td>
    <td>
      
        Datasets 
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2016</td>
    <td>
      <a href="/publications/vasile2016meta/">Meta-Prod2Vec - Product Embeddings Using Side-Information for Recommendation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Meta-Prod2Vec - Product Embeddings Using Side-Information for Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Meta-Prod2Vec - Product Embeddings Using Side-Information for Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Vasile Flavian, Smirnova Elena, Conneau Alexis</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>120</td>
    <td><p>We propose Meta-Prod2vec, a novel method to compute item similarities for
recommendation that leverages existing item metadata. Such scenarios are
frequently encountered in applications such as content recommendation, ad
targeting and web search. Our method leverages past user interactions with
items and their attributes to compute low-dimensional embeddings of items.
Specifically, the item metadata is in- jected into the model as side
information to regularize the item embeddings. We show that the new item
representa- tions lead to better performance on recommendation tasks on an open
music dataset.</p>
</td>
    <td>
      
        Datasets 
      
        Recommender-Systems 
      
        Evaluation 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/moran2015regularised/">Regularised Cross-Modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Regularised Cross-Modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Regularised Cross-Modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Moran S., Lavrenko</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>18</td>
    <td><p>In this paper we propose Regularised Cross-Modal Hashing (RCMH) a new cross-modal hashing scheme that projects annotation and visual feature descriptors into a common Hamming space. RCMH optimises the intra-modality similarity of data-points in the annotation modality using an iterative three-step hashing algorithm: in the first step each training image is assigned a K-bit hashcode based on hyperplanes learnt at the previous iteration; in the second step the binary bits are smoothed by a formulation of graph regularisation so that similar data-points have similar bits; in the third step a set of binary classifiers are trained to predict the regularised bits with maximum margin. Visual descriptors are projected into the annotation Hamming space by a set of binary classifiers learnt using the bits of the corresponding annotations as labels. RCMH is shown to consistently improve retrieval effectiveness over state-of-the-art baselines.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        SIGIR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/song2025top/">Top Rank Supervised Binary Coding for Visual Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Top Rank Supervised Binary Coding for Visual Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Top Rank Supervised Binary Coding for Visual Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Song Dongjin, Liu, Ji, Meyer, Smith</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE International Conference on Computer Vision (ICCV)</td>
    <td>76</td>
    <td><p>In recent years, binary coding techniques are becoming
increasingly popular because of their high efficiency in handling large-scale computer vision applications. It has been
demonstrated that supervised binary coding techniques that
leverage supervised information can significantly enhance
the coding quality, and hence greatly benefit visual search
tasks. Typically, a modern binary coding method seeks
to learn a group of coding functions which compress data
samples into binary codes. However, few methods pursued
the coding functions such that the precision at the top of
a ranking list according to Hamming distances of the generated binary codes is optimized.
In this paper, we propose a novel supervised binary coding approach, namely
Top Rank Supervised Binary Coding (Top-RSBC), which
explicitly focuses on optimizing the precision of top positions in a Hamming-distance ranking list towards preserving the supervision information. The core idea is to train
the disciplined coding functions, by which the mistakes at
the top of a Hamming-distance ranking list are penalized
more than those at the bottom. To solve such coding functions, we relax the original discrete optimization objective
with a continuous surrogate, and derive a stochastic gradient descent to optimize the surrogate objective. To further reduce the training time cost, we also design an online
learning algorithm to optimize the surrogate objective more
efficiently. Empirical studies based upon three benchmark
image datasets demonstrate that the proposed binary coding approach achieves superior image search accuracy over
the state-of-the-arts.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Efficiency 
      
        ICCV 
      
        Datasets 
      
        Compact-Codes 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/jing2015visual/">Visual Search at Pinterest</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Visual Search at Pinterest' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Visual Search at Pinterest' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jing Yushi, Liu David, Kislyuk Dmitry, Zhai Andrew, Xu Jiajing, Donahue Jeff, Tavel Sarah</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</td>
    <td>144</td>
    <td><p>We demonstrate that, with the availability of distributed computation
platforms such as Amazon Web Services and open-source tools, it is possible for
a small engineering team to build, launch and maintain a cost-effective,
large-scale visual search system with widely available tools. We also
demonstrate, through a comprehensive set of live experiments at Pinterest, that
content recommendation powered by visual search improve user engagement. By
sharing our implementation details and the experiences learned from launching a
commercial visual search engines from scratch, we hope visual search are more
widely incorporated into today’s commercial applications.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
        Image-Retrieval 
      
        Recommender-Systems 
      
        KDD 
      
        Scalability 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/moran2025regularised/">Regularised Cross-Modal Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Regularised Cross-Modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Regularised Cross-Modal Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Moran S., Lavrenko</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</td>
    <td>18</td>
    <td><p>In this paper we propose Regularised Cross-Modal Hashing (RCMH) a new cross-modal hashing scheme that projects annotation and visual feature descriptors into a common Hamming space. RCMH optimises the intra-modality similarity of data-points in the annotation modality using an iterative three-step hashing algorithm: in the first step each training image is assigned a K-bit hashcode based on hyperplanes learnt at the previous iteration; in the second step the binary bits are smoothed by a formulation of graph regularisation so that similar data-points have similar bits; in the third step a set of binary classifiers are trained to predict the regularised bits with maximum margin. Visual descriptors are projected into the annotation Hamming space by a set of binary classifiers learnt using the bits of the corresponding annotations as labels. RCMH is shown to consistently improve retrieval effectiveness over state-of-the-art baselines.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        SIGIR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/leng2015hashing/">Hashing for Distributed Data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing for Distributed Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing for Distributed Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Leng Cong, Wu, Cheng, Lu</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>30</td>
    <td><p>Recently, hashing based approximate nearest
neighbors search has attracted much attention.
Extensive centralized hashing algorithms have
been proposed and achieved promising performance. However, due to the large scale of many
applications, the data is often stored or even collected in a distributed manner. Learning hash
functions by aggregating all the data into a fusion
center is infeasible because of the prohibitively
expensive communication and computation overhead.
In this paper, we develop a novel hashing
model to learn hash functions in a distributed setting. We cast a centralized hashing model as a
set of subproblems with consensus constraints.
We find these subproblems can be analytically
solved in parallel on the distributed compute nodes. Since no training data is transmitted across
the nodes in the learning process, the communication cost of our model is independent to the data size. Extensive experiments on several large
scale datasets containing up to 100 million samples demonstrate the efficacy of our method.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Evaluation 
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/lin2025semantics/">Semantics-Preserving Hashing for Cross-View Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semantics-Preserving Hashing for Cross-View Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semantics-Preserving Hashing for Cross-View Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin Zijia, Ding, Wang</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>540</td>
    <td><p>With benefits of low storage costs and high query speeds,
hashing methods are widely researched for efficiently retrieving large-scale data, which commonly contains multiple views, e.g. a news report with images, videos and texts.
In this paper, we study the problem of cross-view retrieval
and propose an effective Semantics-Preserving Hashing
method, termed SePH. Given semantic affinities of training data as supervised information, SePH transforms them
into a probability distribution and approximates it with tobe-learnt hash codes in Hamming space via minimizing the
Kullback-Leibler divergence. Then kernel logistic regression with a sampling strategy is utilized to learn the nonlinear projections from features in each view to the learnt
hash codes. And for any unseen instance, predicted hash
codes and their corresponding output probabilities from observed views are utilized to determine its unified hash code,
using a novel probabilistic approach. Extensive experiments conducted on three benchmark datasets well demonstrate the effectiveness and reasonableness of SePH.</p>
</td>
    <td>
      
        Scalability 
      
        Datasets 
      
        CVPR 
      
        Memory-Efficiency 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/wang2015semantic/">Semantic Topic Multimodal Hashing for Cross-Media Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semantic Topic Multimodal Hashing for Cross-Media Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semantic Topic Multimodal Hashing for Cross-Media Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang di, Gao, He</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>149</td>
    <td><p>Multimodal hashing is essential to cross-media
similarity search for its low storage cost and fast
query speed. Most existing multimodal hashing
methods embedded heterogeneous data into a common low-dimensional Hamming space, and then
rounded the continuous embeddings to obtain the
binary codes. Yet they usually neglect the inherent discrete nature of hashing for relaxing the discrete constraints, which will cause degraded retrieval performance especially for long codes. For
this purpose, a novel Semantic Topic Multimodal
Hashing (STMH) is developed by considering latent semantic information in coding procedure.
It
first discovers clustering patterns of texts and robust factorizes the matrix of images to obtain multiple semantic topics of texts and concepts of images.
Then the learned multimodal semantic features are
transformed into a common subspace by their correlations. Finally, each bit of unified hash code
can be generated directly by figuring out whether a
topic or concept is contained in a text or an image.
Therefore, the obtained model by STMH is more
suitable for hashing scheme as it directly learns discrete hash codes in the coding process. Experimental results demonstrate that the proposed method
outperforms several state-of-the-art methods.</p>
</td>
    <td>
      
        Memory-Efficiency 
      
        Compact-Codes 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/leng2025hashing/">Hashing for Distributed Data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing for Distributed Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing for Distributed Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Leng Cong, Wu, Cheng, Lu</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>30</td>
    <td><p>Recently, hashing based approximate nearest
neighbors search has attracted much attention.
Extensive centralized hashing algorithms have
been proposed and achieved promising performance. However, due to the large scale of many
applications, the data is often stored or even collected in a distributed manner. Learning hash
functions by aggregating all the data into a fusion
center is infeasible because of the prohibitively
expensive communication and computation overhead.
In this paper, we develop a novel hashing
model to learn hash functions in a distributed setting. We cast a centralized hashing model as a
set of subproblems with consensus constraints.
We find these subproblems can be analytically
solved in parallel on the distributed compute nodes. Since no training data is transmitted across
the nodes in the learning process, the communication cost of our model is independent to the data size. Extensive experiments on several large
scale datasets containing up to 100 million samples demonstrate the efficacy of our method.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Evaluation 
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/carreiraperpinan2015hashing/">Hashing with Binary Autoencoders</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing with Binary Autoencoders' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing with Binary Autoencoders' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Carreira-perpinan M., Raziperchikolaei</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>142</td>
    <td><p>An attractive approach for fast search in image
databases is binary hashing, where each high-dimensional,
real-valued image is mapped onto a low-dimensional, binary
vector and the search is done in this binary space.
Finding the optimal hash function is difficult because it involves
binary constraints, and most approaches approximate
the optimization by relaxing the constraints and then
binarizing the result. Here, we focus on the binary autoencoder
model, which seeks to reconstruct an image from the
binary code produced by the hash function. We show that
the optimization can be simplified with the method of auxiliary
coordinates. This reformulates the optimization as
alternating two easier steps: one that learns the encoder
and decoder separately, and one that optimizes the code for
each image. Image retrieval experiments show the resulting
hash function outperforms or is competitive with state-ofthe-art
methods for binary hashing.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Image-Retrieval 
      
        CVPR 
      
        Compact-Codes 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/carreiraperpinan2025hashing/">Hashing with Binary Autoencoders</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing with Binary Autoencoders' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing with Binary Autoencoders' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Carreira-perpinan M., Raziperchikolaei</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>142</td>
    <td><p>An attractive approach for fast search in image
databases is binary hashing, where each high-dimensional,
real-valued image is mapped onto a low-dimensional, binary
vector and the search is done in this binary space.
Finding the optimal hash function is difficult because it involves
binary constraints, and most approaches approximate
the optimization by relaxing the constraints and then
binarizing the result. Here, we focus on the binary autoencoder
model, which seeks to reconstruct an image from the
binary code produced by the hash function. We show that
the optimization can be simplified with the method of auxiliary
coordinates. This reformulates the optimization as
alternating two easier steps: one that learns the encoder
and decoder separately, and one that optimizes the code for
each image. Image retrieval experiments show the resulting
hash function outperforms or is competitive with state-ofthe-art
methods for binary hashing.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Image-Retrieval 
      
        CVPR 
      
        Compact-Codes 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/zhang2025bit/">Bit-Scalable Deep Hashing With Regularized Similarity Learning for Image Retrieval and Person Re-Identification</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Bit-Scalable Deep Hashing With Regularized Similarity Learning for Image Retrieval and Person Re-Identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Bit-Scalable Deep Hashing With Regularized Similarity Learning for Image Retrieval and Person Re-Identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang R., Lin, Zhang, Zuo, Zhang</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>418</td>
    <td><p>Extracting informative image features and learning
effective approximate hashing functions are two crucial steps in
image retrieval . Conventional methods often study these two
steps separately, e.g., learning hash functions from a predefined
hand-crafted feature space. Meanwhile, the bit lengths of output
hashing codes are preset in most previous methods, neglecting the
significance level of different bits and restricting their practical
flexibility. To address these issues, we propose a supervised
learning framework to generate compact and bit-scalable hashing
codes directly from raw images. We pose hashing learning as
a problem of regularized similarity learning. Specifically, we
organize the training images into a batch of triplet samples,
each sample containing two images with the same label and one
with a different label. With these triplet samples, we maximize
the margin between matched pairs and mismatched pairs in the
Hamming space. In addition, a regularization term is introduced
to enforce the adjacency consistency, i.e., images of similar
appearances should have similar codes. The deep convolutional
neural network is utilized to train the model in an end-to-end
fashion, where discriminative image features and hash functions
are simultaneously optimized. Furthermore, each bit of our
hashing codes is unequally weighted so that we can manipulate
the code lengths by truncating the insignificant bits. Our
framework outperforms state-of-the-arts on public benchmarks
of similar image search and also achieves promising results in
the application of person re-identification in surveillance. It is
also shown that the generated bit-scalable hashing codes well
preserve the discriminative powers with shorter code lengths.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Neural-Hashing 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/mukherjee2015nmf/">An NMF perspective on Binary Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=An NMF perspective on Binary Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=An NMF perspective on Binary Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mukherjee Lopamudra, Ravi, Ithapu, Singh</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE International Conference on Computer Vision (ICCV)</td>
    <td>14</td>
    <td><p>The pervasiveness of massive data repositories has led
to much interest in efficient methods for indexing, search,
and retrieval. For image data, a rapidly developing body of
work for these applications shows impressive performance
with methods that broadly fall under the umbrella term of
Binary Hashing. Given a distance matrix, a binary hashing
algorithm solves for a binary code for the given set of examples, whose Hamming distance nicely approximates the
original distances. The formulation is non-convex — so existing solutions adopt spectral relaxations or perform coordinate descent (or quantization) on a surrogate objective
that is numerically more tractable. In this paper, we first
derive an Augmented Lagrangian approach to optimize the
standard binary Hashing objective (i.e., maintain fidelity
with a given distance matrix). With appropriate step sizes,
we find that this scheme already yields results that match or
substantially outperform state of the art methods on most
benchmarks used in the literature. Then, to allow the model
to scale to large datasets, we obtain an interesting reformulation of the binary hashing objective as a non-negative matrix factorization. Later, this leads to a simple multiplicative updates algorithm — whose parallelization properties
are exploited to obtain a fast GPU based implementation.
We give a probabilistic analysis of our initialization scheme
and present a range of experiments to show that the method
is simple to implement and competes favorably with available methods (both for optimization and generalization).</p>
</td>
    <td>
      
        ICCV 
      
        Datasets 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Quantization 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/li2015feature/">Feature Learning based Deep Supervised Hashing with Pairwise Labels</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Feature Learning based Deep Supervised Hashing with Pairwise Labels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Feature Learning based Deep Supervised Hashing with Pairwise Labels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Wu-jun, Kang</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>510</td>
    <td><p>Recent years have witnessed wide application of
hashing for large-scale image retrieval. However,
most existing hashing methods are based on handcrafted features which might not be optimally compatible with the hashing procedure. Recently, deep
hashing methods have been proposed to perform simultaneous feature learning and hash-code learning with deep neural networks, which have shown
better performance than traditional hashing methods with hand-crafted features. Most of these deep
hashing methods are supervised whose supervised
information is given with triplet labels. For another common application scenario with pairwise labels, there have not existed methods for simultaneous feature learning and hash-code learning. In this
paper, we propose a novel deep hashing method,
called deep pairwise-supervised hashing (DPSH),
to perform simultaneous feature learning and hashcode learning for applications with pairwise labels.
Experiments on real datasets show that our DPSH
method can outperform other methods to achieve
the state-of-the-art performance in image retrieval
applications.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Datasets 
      
        Neural-Hashing 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/mukherjee2025nmf/">An NMF perspective on Binary Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=An NMF perspective on Binary Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=An NMF perspective on Binary Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mukherjee Lopamudra, Ravi, Ithapu, Singh</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE International Conference on Computer Vision (ICCV)</td>
    <td>14</td>
    <td><p>The pervasiveness of massive data repositories has led
to much interest in efficient methods for indexing, search,
and retrieval. For image data, a rapidly developing body of
work for these applications shows impressive performance
with methods that broadly fall under the umbrella term of
Binary Hashing. Given a distance matrix, a binary hashing
algorithm solves for a binary code for the given set of examples, whose Hamming distance nicely approximates the
original distances. The formulation is non-convex — so existing solutions adopt spectral relaxations or perform coordinate descent (or quantization) on a surrogate objective
that is numerically more tractable. In this paper, we first
derive an Augmented Lagrangian approach to optimize the
standard binary Hashing objective (i.e., maintain fidelity
with a given distance matrix). With appropriate step sizes,
we find that this scheme already yields results that match or
substantially outperform state of the art methods on most
benchmarks used in the literature. Then, to allow the model
to scale to large datasets, we obtain an interesting reformulation of the binary hashing objective as a non-negative matrix factorization. Later, this leads to a simple multiplicative updates algorithm — whose parallelization properties
are exploited to obtain a fast GPU based implementation.
We give a probabilistic analysis of our initialization scheme
and present a range of experiments to show that the method
is simple to implement and competes favorably with available methods (both for optimization and generalization).</p>
</td>
    <td>
      
        ICCV 
      
        Datasets 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Quantization 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/liu2015multi/">Multi-View Complementary Hash Tables for Nearest Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multi-View Complementary Hash Tables for Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multi-View Complementary Hash Tables for Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Xianglong, Huang, Deng, Land</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE International Conference on Computer Vision (ICCV)</td>
    <td>57</td>
    <td><p>Recent years have witnessed the success of hashing techniques in fast nearest neighbor search. In practice many
applications (e.g., visual search, object detection, image
matching, etc.) have enjoyed the benefits of complementary hash tables and information fusion over multiple views.
However, most of prior research mainly focused on compact hash code cleaning, and rare work studies how to build
multiple complementary hash tables, much less to adaptively integrate information stemming from multiple views.
In
this paper we first present a novel multi-view complementary hash table method that learns complementary hash tables from the data with multiple views. For single multiview table, using exemplar based feature fusion, we approximate the inherent data similarities with a low-rank matrix,
and learn discriminative hash functions in an efficient way.
To build complementary tables and meanwhile maintain scalable training and fast out-of-sample extension, an exemplar reweighting scheme is introduced to update the induced low-rank similarity in the sequential table construction framework, which indeed brings mutual benefits between tables by placing greater importance on exemplars
shared by mis-separated neighbors. Extensive experiments
on three large-scale image datasets demonstrate that the
proposed method significantly outperforms various naive
solutions and state-of-the-art multi-table methods.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        ICCV 
      
        Datasets 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/cakir2025adaptive/">Adaptive Hashing for Fast Similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Adaptive Hashing for Fast Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Adaptive Hashing for Fast Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cakir F., Sclaroff</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE International Conference on Computer Vision (ICCV)</td>
    <td>88</td>
    <td><p>With the staggering growth in image and video datasets,
algorithms that provide fast similarity search and compact
storage are crucial. Hashing methods that map the
data into Hamming space have shown promise; however,
many of these methods employ a batch-learning strategy
in which the computational cost and memory requirements
may become intractable and infeasible with larger and
larger datasets. To overcome these challenges, we propose
an online learning algorithm based on stochastic gradient
descent in which the hash functions are updated iteratively
with streaming data. In experiments with three image retrieval
benchmarks, our online algorithm attains retrieval
accuracy that is comparable to competing state-of-the-art
batch-learning solutions, while our formulation is orders
of magnitude faster and being online it is adaptable to the
variations of the data. Moreover, our formulation yields improved
retrieval performance over a recently reported online
hashing technique, Online Kernel Hashing.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        ICCV 
      
        Datasets 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/xu2025convolutional/">Convolutional Neural Networks for Text Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Convolutional Neural Networks for Text Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Convolutional Neural Networks for Text Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xu Jiaming, Pengwang, Tian, Xu, Zhao, Wang, Hao</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>33</td>
    <td><p>Hashing, as a popular approximate nearest neighbor
search, has been widely used for large-scale similarity search. Recently, a spectrum of machine learning
methods are utilized to learn similarity-preserving
binary codes. However, most of them directly encode the explicit features, keywords, which fail to
preserve the accurate semantic similarities in binary code beyond keyword matching, especially on
short texts. Here we propose a novel text hashing
framework with convolutional neural networks. In
particular, we first embed the keyword features into
compact binary code with a locality preserving constraint. Meanwhile word features and position features are together fed into a convolutional network to
learn the implicit features which are further incorporated with the explicit features to fit the pre-trained
binary code. Such base method can be successfully
accomplished without any external tags/labels, and
other three model variations are designed to integrate tags/labels. Experimental results show the
superiority of our proposed approach over several
state-of-the-art hashing methods when tested on one
short text dataset as well as one normal text dataset.</p>
</td>
    <td>
      
        Scalability 
      
        Datasets 
      
        Tools-&-Libraries 
      
        Compact-Codes 
      
        Similarity-Search 
      
        Hashing-Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/cakir2015adaptive/">Adaptive Hashing for Fast Similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Adaptive Hashing for Fast Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Adaptive Hashing for Fast Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Cakir F., Sclaroff</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE International Conference on Computer Vision (ICCV)</td>
    <td>88</td>
    <td><p>With the staggering growth in image and video datasets,
algorithms that provide fast similarity search and compact
storage are crucial. Hashing methods that map the
data into Hamming space have shown promise; however,
many of these methods employ a batch-learning strategy
in which the computational cost and memory requirements
may become intractable and infeasible with larger and
larger datasets. To overcome these challenges, we propose
an online learning algorithm based on stochastic gradient
descent in which the hash functions are updated iteratively
with streaming data. In experiments with three image retrieval
benchmarks, our online algorithm attains retrieval
accuracy that is comparable to competing state-of-the-art
batch-learning solutions, while our formulation is orders
of magnitude faster and being online it is adaptable to the
variations of the data. Moreover, our formulation yields improved
retrieval performance over a recently reported online
hashing technique, Online Kernel Hashing.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        ICCV 
      
        Datasets 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/wang2015hamming/">Hamming Compatible Quantization for Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hamming Compatible Quantization for Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hamming Compatible Quantization for Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Z., Duan, Lin, Wang, Gao</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>20</td>
    <td><p>Hashing is one of the effective techniques for fast
Approximate Nearest Neighbour (ANN) search.
Traditional single-bit quantization (SBQ) in most
hashing methods incurs lots of quantization error
which seriously degrades the search performance.
To address the limitation of SBQ, researchers have
proposed promising multi-bit quantization (MBQ)
methods to quantize each projection dimension
with multiple bits. However, some MBQ methods
need to adopt specific distance for binary code
matching instead of the original Hamming distance,
which would significantly decrease the retrieval
speed. Two typical MBQ methods Hierarchical
Quantization and Double Bit Quantization
retain the Hamming distance, but both of them only
consider the projection dimensions during quantization,
ignoring the neighborhood structure of raw
data inherent in Euclidean space. In this paper,
we propose a multi-bit quantization method named
Hamming Compatible Quantization (HCQ) to preserve
the capability of similarity metric between
Euclidean space and Hamming space by utilizing
the neighborhood structure of raw data. Extensive
experiment results have shown our approach significantly
improves the performance of various stateof-the-art
hashing methods while maintaining fast
retrieval speed.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Compact-Codes 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Evaluation 
      
        Quantization 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/xu2015convolutional/">Convolutional Neural Networks for Text Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Convolutional Neural Networks for Text Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Convolutional Neural Networks for Text Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xu Jiaming, Pengwang, Tian, Xu, Zhao, Wang, Hao</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>33</td>
    <td><p>Hashing, as a popular approximate nearest neighbor
search, has been widely used for large-scale similarity search. Recently, a spectrum of machine learning
methods are utilized to learn similarity-preserving
binary codes. However, most of them directly encode the explicit features, keywords, which fail to
preserve the accurate semantic similarities in binary code beyond keyword matching, especially on
short texts. Here we propose a novel text hashing
framework with convolutional neural networks. In
particular, we first embed the keyword features into
compact binary code with a locality preserving constraint. Meanwhile word features and position features are together fed into a convolutional network to
learn the implicit features which are further incorporated with the explicit features to fit the pre-trained
binary code. Such base method can be successfully
accomplished without any external tags/labels, and
other three model variations are designed to integrate tags/labels. Experimental results show the
superiority of our proposed approach over several
state-of-the-art hashing methods when tested on one
short text dataset as well as one normal text dataset.</p>
</td>
    <td>
      
        Scalability 
      
        Datasets 
      
        Tools-&-Libraries 
      
        Compact-Codes 
      
        Similarity-Search 
      
        Hashing-Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/liong2025deep/">Deep Hashing for Compact Binary Codes Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Hashing for Compact Binary Codes Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Hashing for Compact Binary Codes Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liong V., Lu, Wang, Moulin, Zhou</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>579</td>
    <td><p>In this paper, we propose a new deep hashing (DH) approach
to learn compact binary codes for large scale visual
search. Unlike most existing binary codes learning methods
which seek a single linear projection to map each sample
into a binary vector, we develop a deep neural network
to seek multiple hierarchical non-linear transformations to
learn these binary codes, so that the nonlinear relationship
of samples can be well exploited. Our model is learned under
three constraints at the top layer of the deep network:
1) the loss between the original real-valued feature descriptor
and the learned binary vector is minimized, 2) the binary
codes distribute evenly on each bit, and 3) different bits
are as independent as possible. To further improve the discriminative
power of the learned binary codes, we extend
DH into supervised DH (SDH) by including one discriminative
term into the objective function of DH which simultaneously
maximizes the inter-class variations and minimizes
the intra-class variations of the learned binary codes. Experimental
results show the superiority of the proposed approach
over the state-of-the-arts.</p>
</td>
    <td>
      
        CVPR 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/zhao2025deep/">Deep Semantic Ranking Based Hashing for Multi-Label Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Semantic Ranking Based Hashing for Multi-Label Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Semantic Ranking Based Hashing for Multi-Label Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhao F., Huang, Wang, Tan</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>463</td>
    <td><p>With the rapid growth of web images, hashing has received
increasing interests in large scale image retrieval.
Research efforts have been devoted to learning compact binary
codes that preserve semantic similarity based on labels.
However, most of these hashing methods are designed
to handle simple binary similarity. The complex multilevel
semantic structure of images associated with multiple labels
have not yet been well explored. Here we propose a deep
semantic ranking based method for learning hash functions
that preserve multilevel semantic similarity between multilabel
images. In our approach, deep convolutional neural
network is incorporated into hash functions to jointly
learn feature representations and mappings from them to
hash codes, which avoids the limitation of semantic representation
power of hand-crafted features. Meanwhile, a
ranking list that encodes the multilevel similarity information
is employed to guide the learning of such deep hash
functions. An effective scheme based on surrogate loss is
used to solve the intractable optimization problem of nonsmooth
and multivariate ranking measures involved in the
learning procedure. Experimental results show the superiority
of our proposed approach over several state-of-theart
hashing methods in term of ranking evaluation metrics
when tested on multi-label image datasets.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Datasets 
      
        CVPR 
      
        Neural-Hashing 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/liong2015deep/">Deep Variational and Structural Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Variational and Structural Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Variational and Structural Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liong Venice, Lu, Duan, Tan</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>579</td>
    <td><p>In this paper, we propose a deep variational and structural hashing (DVStH) method to learn compact binary codes for multimedia retrieval. Unlike most existing deep hashing methods which use a series of convolution and fully-connected layers to learn binary features, we develop a probabilistic framework to infer latent feature representation inside the network. Then, we design a struct layer rather than a bottleneck hash layer, to obtain binary codes through a simple encoding procedure. By doing these, we are able to obtain binary codes discriminatively and generatively. To make it applicable to cross-modal scalable multimedia retrieval, we extend our method to a cross-modal deep variational and structural hashing (CM-DVStH). We design a deep fusion network with a struct layer to maximize the correlation between image-text input pairs during the training stage so that a unified binary vector can be obtained. We then design modality-specific hashing networks to handle the out-of-sample extension scenario. Specifically, we train a network for each modality which outputs a latent representation that is as close as possible to the binary codes which are inferred from the fusion network. Experimental results on five benchmark datasets are presented to show the efficacy of the proposed approach.</p>
</td>
    <td>
      
        Datasets 
      
        CVPR 
      
        Neural-Hashing 
      
        Tools-&-Libraries 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/yu2023learning/">Learning cross space mapping via DNN using large scale click-through logs</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning cross space mapping via DNN using large scale click-through logs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning cross space mapping via DNN using large scale click-through logs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yu Wei, Yang Kuiyuan, Bai Yalong, Yao Hongxun, Rui Yong</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Multimedia</td>
    <td>8</td>
    <td><p>The gap between low-level visual signals and high-level semantics has been
progressively bridged by continuous development of deep neural network (DNN).
With recent progress of DNN, almost all image classification tasks have
achieved new records of accuracy. To extend the ability of DNN to image
retrieval tasks, we proposed a unified DNN model for image-query similarity
calculation by simultaneously modeling image and query in one network. The
unified DNN is named the cross space mapping (CSM) model, which contains two
parts, a convolutional part and a query-embedding part. The image and query are
mapped to a common vector space via these two parts respectively, and
image-query similarity is naturally defined as an inner product of their
mappings in the space. To ensure good generalization ability of the DNN, we
learn weights of the DNN from a large number of click-through logs which
consists of 23 million clicked image-query pairs between 1 million images and
11.7 million queries. Both the qualitative results and quantitative results on
an image retrieval evaluation task with 1000 queries demonstrate the
superiority of the proposed method.</p>
</td>
    <td>
      
        Evaluation 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/lin2015semantics/">Semantics-Preserving Hashing for Cross-View Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semantics-Preserving Hashing for Cross-View Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semantics-Preserving Hashing for Cross-View Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin Zijia, Ding, Wang</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>540</td>
    <td><p>With benefits of low storage costs and high query speeds,
hashing methods are widely researched for efficiently retrieving large-scale data, which commonly contains multiple views, e.g. a news report with images, videos and texts.
In this paper, we study the problem of cross-view retrieval
and propose an effective Semantics-Preserving Hashing
method, termed SePH. Given semantic affinities of training data as supervised information, SePH transforms them
into a probability distribution and approximates it with tobe-learnt hash codes in Hamming space via minimizing the
Kullback-Leibler divergence. Then kernel logistic regression with a sampling strategy is utilized to learn the nonlinear projections from features in each view to the learnt
hash codes. And for any unseen instance, predicted hash
codes and their corresponding output probabilities from observed views are utilized to determine its unified hash code,
using a novel probabilistic approach. Extensive experiments conducted on three benchmark datasets well demonstrate the effectiveness and reasonableness of SePH.</p>
</td>
    <td>
      
        Scalability 
      
        Datasets 
      
        CVPR 
      
        Memory-Efficiency 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/andoni2025practical/">Practical and Optimal LSH for Angular Distance</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Practical and Optimal LSH for Angular Distance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Practical and Optimal LSH for Angular Distance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Andoni A., Indyk, Laarhoven</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>239</td>
    <td><p>We show the existence of a Locality-Sensitive Hashing (LSH) family for the angular
distance that yields an approximate Near Neighbor Search algorithm with the
asymptotically optimal running time exponent. Unlike earlier algorithms with this
property (e.g., Spherical LSH [1, 2]), our algorithm is also practical, improving
upon the well-studied hyperplane LSH [3] in practice. We also introduce a multiprobe
version of this algorithm and conduct an experimental evaluation on real
and synthetic data sets.
We complement the above positive results with a fine-grained lower bound for the
quality of any LSH family for angular distance. Our lower bound implies that the
above LSH family exhibits a trade-off between evaluation time and quality that is
close to optimal for a natural class of LSH functions.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Locality-Sensitive-Hashing 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/jiang2025scalable/">Scalable Graph Hashing with Feature Transformation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Scalable Graph Hashing with Feature Transformation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Scalable Graph Hashing with Feature Transformation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jiang Q., Li</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>208</td>
    <td><p>Hashing has been widely used for approximate nearest
neighbor (ANN) search in big data applications
because of its low storage cost and fast retrieval
speed. The goal of hashing is to map the data
points from the original space into a binary-code
space where the similarity (neighborhood structure)
in the original space is preserved. By directly
exploiting the similarity to guide the hashing
code learning procedure, graph hashing has attracted
much attention. However, most existing graph
hashing methods cannot achieve satisfactory performance
in real applications due to the high complexity
for graph modeling. In this paper, we propose
a novel method, called scalable graph hashing
with feature transformation (SGH), for large-scale
graph hashing. Through feature transformation, we
can effectively approximate the whole graph without
explicitly computing the similarity graph matrix,
based on which a sequential learning method
is proposed to learn the hash functions in a bit-wise
manner. Experiments on two datasets with one million
data points show that our SGH method can
outperform the state-of-the-art methods in terms of
both accuracy and scalability.</p>
</td>
    <td>
      
        Scalability 
      
        Efficiency 
      
        Datasets 
      
        Memory-Efficiency 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/wang2025semantic/">Semantic Topic Multimodal Hashing for Cross-Media Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semantic Topic Multimodal Hashing for Cross-Media Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semantic Topic Multimodal Hashing for Cross-Media Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang di, Gao, He</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>149</td>
    <td><p>Multimodal hashing is essential to cross-media
similarity search for its low storage cost and fast
query speed. Most existing multimodal hashing
methods embedded heterogeneous data into a common low-dimensional Hamming space, and then
rounded the continuous embeddings to obtain the
binary codes. Yet they usually neglect the inherent discrete nature of hashing for relaxing the discrete constraints, which will cause degraded retrieval performance especially for long codes. For
this purpose, a novel Semantic Topic Multimodal
Hashing (STMH) is developed by considering latent semantic information in coding procedure.
It
first discovers clustering patterns of texts and robust factorizes the matrix of images to obtain multiple semantic topics of texts and concepts of images.
Then the learned multimodal semantic features are
transformed into a common subspace by their correlations. Finally, each bit of unified hash code
can be generated directly by figuring out whether a
topic or concept is contained in a text or an image.
Therefore, the obtained model by STMH is more
suitable for hashing scheme as it directly learns discrete hash codes in the coding process. Experimental results demonstrate that the proposed method
outperforms several state-of-the-art methods.</p>
</td>
    <td>
      
        Memory-Efficiency 
      
        Compact-Codes 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/andoni2015practical/">Practical and Optimal LSH for Angular Distance</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Practical and Optimal LSH for Angular Distance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Practical and Optimal LSH for Angular Distance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Andoni A., Indyk, Laarhoven</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>239</td>
    <td><p>We show the existence of a Locality-Sensitive Hashing (LSH) family for the angular
distance that yields an approximate Near Neighbor Search algorithm with the
asymptotically optimal running time exponent. Unlike earlier algorithms with this
property (e.g., Spherical LSH [1, 2]), our algorithm is also practical, improving
upon the well-studied hyperplane LSH [3] in practice. We also introduce a multiprobe
version of this algorithm and conduct an experimental evaluation on real
and synthetic data sets.
We complement the above positive results with a fine-grained lower bound for the
quality of any LSH family for angular distance. Our lower bound implies that the
above LSH family exhibits a trade-off between evaluation time and quality that is
close to optimal for a natural class of LSH functions.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Locality-Sensitive-Hashing 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/liu2025multi/">Multi-View Complementary Hash Tables for Nearest Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multi-View Complementary Hash Tables for Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multi-View Complementary Hash Tables for Nearest Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu Xianglong, Huang, Deng, Land</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE International Conference on Computer Vision (ICCV)</td>
    <td>57</td>
    <td><p>Recent years have witnessed the success of hashing techniques in fast nearest neighbor search. In practice many
applications (e.g., visual search, object detection, image
matching, etc.) have enjoyed the benefits of complementary hash tables and information fusion over multiple views.
However, most of prior research mainly focused on compact hash code cleaning, and rare work studies how to build
multiple complementary hash tables, much less to adaptively integrate information stemming from multiple views.
In
this paper we first present a novel multi-view complementary hash table method that learns complementary hash tables from the data with multiple views. For single multiview table, using exemplar based feature fusion, we approximate the inherent data similarities with a low-rank matrix,
and learn discriminative hash functions in an efficient way.
To build complementary tables and meanwhile maintain scalable training and fast out-of-sample extension, an exemplar reweighting scheme is introduced to update the induced low-rank similarity in the sequential table construction framework, which indeed brings mutual benefits between tables by placing greater importance on exemplars
shared by mis-separated neighbors. Extensive experiments
on three large-scale image datasets demonstrate that the
proposed method significantly outperforms various naive
solutions and state-of-the-art multi-table methods.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        ICCV 
      
        Datasets 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/lin2015deep/">Deep learning of binary hash codes for fast image retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep learning of binary hash codes for fast image retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep learning of binary hash codes for fast image retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin Kevin, Yang, Hsiao, Chen</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>626</td>
    <td><p>Approximate nearest neighbor search is an efficient strategy for large-scale image retrieval. Encouraged by the recent advances in convolutional neural networks (CNNs), we propose an effective deep learning framework to generate binary hash codes for fast image retrieval. Our idea is that when the data labels are available, binary codes can be learned by employing a hidden layer for representing the latent concepts that dominate the class labels.
he utilization of the CNN also allows for learning image representations. Unlike other supervised methods that require pair-wised inputs for binary code learning, our method learns hash codes and image representations in a point-wised manner, making it suitable for large-scale datasets. Experimental results show that our method outperforms several state-of-the-art hashing algorithms on the CIFAR-10 and MNIST datasets. We further demonstrate its scalability and efficacy on a large-scale dataset of 1 million clothing images.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Datasets 
      
        CVPR 
      
        Tools-&-Libraries 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/jiang2015scalable/">Scalable Graph Hashing with Feature Transformation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Scalable Graph Hashing with Feature Transformation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Scalable Graph Hashing with Feature Transformation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jiang Q., Li</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>208</td>
    <td><p>Hashing has been widely used for approximate nearest
neighbor (ANN) search in big data applications
because of its low storage cost and fast retrieval
speed. The goal of hashing is to map the data
points from the original space into a binary-code
space where the similarity (neighborhood structure)
in the original space is preserved. By directly
exploiting the similarity to guide the hashing
code learning procedure, graph hashing has attracted
much attention. However, most existing graph
hashing methods cannot achieve satisfactory performance
in real applications due to the high complexity
for graph modeling. In this paper, we propose
a novel method, called scalable graph hashing
with feature transformation (SGH), for large-scale
graph hashing. Through feature transformation, we
can effectively approximate the whole graph without
explicitly computing the similarity graph matrix,
based on which a sequential learning method
is proposed to learn the hash functions in a bit-wise
manner. Experiments on two datasets with one million
data points show that our SGH method can
outperform the state-of-the-art methods in terms of
both accuracy and scalability.</p>
</td>
    <td>
      
        Scalability 
      
        Efficiency 
      
        Datasets 
      
        Memory-Efficiency 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/fu2016improved/">An Improved System for Sentence-level Novelty Detection in Textual Streams</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=An Improved System for Sentence-level Novelty Detection in Textual Streams' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=An Improved System for Sentence-level Novelty Detection in Textual Streams' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fu Xinyu, Ch'ng Eugene, Aickelin Uwe, Zhang Lanyun</td> <!-- 🔧 You were missing this -->
    <td>2015 International Conference on Smart and Sustainable City and Big Data (ICSSC)</td>
    <td>5</td>
    <td><p>Novelty detection in news events has long been a difficult problem. A number
of models performed well on specific data streams but certain issues are far
from being solved, particularly in large data streams from the WWW where
unpredictability of new terms requires adaptation in the vector space model. We
present a novel event detection system based on the Incremental Term
Frequency-Inverse Document Frequency (TF-IDF) weighting incorporated with
Locality Sensitive Hashing (LSH). Our system could efficiently and effectively
adapt to the changes within the data streams of any new terms with continual
updates to the vector space model. Regarding miss probability, our proposed
novelty detection framework outperforms a recognised baseline system by
approximately 16% when evaluating a benchmark dataset from Google News.</p>
</td>
    <td>
      
        Locality-Sensitive-Hashing 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Datasets 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/zhang2015bit/">Bit-Scalable Deep Hashing With Regularized Similarity Learning for Image Retrieval and Person Re-Identification</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Bit-Scalable Deep Hashing With Regularized Similarity Learning for Image Retrieval and Person Re-Identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Bit-Scalable Deep Hashing With Regularized Similarity Learning for Image Retrieval and Person Re-Identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang R., Lin, Zhang, Zuo, Zhang</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Image Processing</td>
    <td>418</td>
    <td><p>Extracting informative image features and learning
effective approximate hashing functions are two crucial steps in
image retrieval . Conventional methods often study these two
steps separately, e.g., learning hash functions from a predefined
hand-crafted feature space. Meanwhile, the bit lengths of output
hashing codes are preset in most previous methods, neglecting the
significance level of different bits and restricting their practical
flexibility. To address these issues, we propose a supervised
learning framework to generate compact and bit-scalable hashing
codes directly from raw images. We pose hashing learning as
a problem of regularized similarity learning. Specifically, we
organize the training images into a batch of triplet samples,
each sample containing two images with the same label and one
with a different label. With these triplet samples, we maximize
the margin between matched pairs and mismatched pairs in the
Hamming space. In addition, a regularization term is introduced
to enforce the adjacency consistency, i.e., images of similar
appearances should have similar codes. The deep convolutional
neural network is utilized to train the model in an end-to-end
fashion, where discriminative image features and hash functions
are simultaneously optimized. Furthermore, each bit of our
hashing codes is unequally weighted so that we can manipulate
the code lengths by truncating the insignificant bits. Our
framework outperforms state-of-the-arts on public benchmarks
of similar image search and also achieves promising results in
the application of person re-identification in surveillance. It is
also shown that the generated bit-scalable hashing codes well
preserve the discriminative powers with shorter code lengths.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Neural-Hashing 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/lin2025deep/">Deep learning of binary hash codes for fast image retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep learning of binary hash codes for fast image retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep learning of binary hash codes for fast image retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin Kevin, Yang, Hsiao, Chen</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</td>
    <td>626</td>
    <td><p>Approximate nearest neighbor search is an efficient strategy for large-scale image retrieval. Encouraged by the recent advances in convolutional neural networks (CNNs), we propose an effective deep learning framework to generate binary hash codes for fast image retrieval. Our idea is that when the data labels are available, binary codes can be learned by employing a hidden layer for representing the latent concepts that dominate the class labels.
he utilization of the CNN also allows for learning image representations. Unlike other supervised methods that require pair-wised inputs for binary code learning, our method learns hash codes and image representations in a point-wised manner, making it suitable for large-scale datasets. Experimental results show that our method outperforms several state-of-the-art hashing algorithms on the CIFAR-10 and MNIST datasets. We further demonstrate its scalability and efficacy on a large-scale dataset of 1 million clothing images.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Datasets 
      
        CVPR 
      
        Tools-&-Libraries 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/song2015top/">Top Rank Supervised Binary Coding for Visual Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Top Rank Supervised Binary Coding for Visual Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Top Rank Supervised Binary Coding for Visual Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Song Dongjin, Liu, Ji, Meyer, Smith</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE International Conference on Computer Vision (ICCV)</td>
    <td>76</td>
    <td><p>In recent years, binary coding techniques are becoming
increasingly popular because of their high efficiency in handling large-scale computer vision applications. It has been
demonstrated that supervised binary coding techniques that
leverage supervised information can significantly enhance
the coding quality, and hence greatly benefit visual search
tasks. Typically, a modern binary coding method seeks
to learn a group of coding functions which compress data
samples into binary codes. However, few methods pursued
the coding functions such that the precision at the top of
a ranking list according to Hamming distances of the generated binary codes is optimized.
In this paper, we propose a novel supervised binary coding approach, namely
Top Rank Supervised Binary Coding (Top-RSBC), which
explicitly focuses on optimizing the precision of top positions in a Hamming-distance ranking list towards preserving the supervision information. The core idea is to train
the disciplined coding functions, by which the mistakes at
the top of a Hamming-distance ranking list are penalized
more than those at the bottom. To solve such coding functions, we relax the original discrete optimization objective
with a continuous surrogate, and derive a stochastic gradient descent to optimize the surrogate objective. To further reduce the training time cost, we also design an online
learning algorithm to optimize the surrogate objective more
efficiently. Empirical studies based upon three benchmark
image datasets demonstrate that the proposed binary coding approach achieves superior image search accuracy over
the state-of-the-arts.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Efficiency 
      
        ICCV 
      
        Datasets 
      
        Compact-Codes 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/ding2025knn/">kNN Hashing with Factorized Neighborhood Representation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=kNN Hashing with Factorized Neighborhood Representation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=kNN Hashing with Factorized Neighborhood Representation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ding Kun, Huo, Fan, Pan</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE International Conference on Computer Vision (ICCV)</td>
    <td>14</td>
    <td><p>Hashing is very effective for many tasks in reducing the
processing time and in compressing massive databases. Although lots of approaches have been developed to learn
data-dependent hash functions in recent years, how to learn
hash functions to yield good performance with acceptable
computational and memory cost is still a challenging problem. Based on the observation that retrieval precision is
highly related to the kNN classification accuracy, this paper
proposes a novel kNN-based supervised hashing method,
which learns hash functions by directly maximizing the kNN
accuracy of the Hamming-embedded training data. To make
it scalable well to large problem, we propose a factorized
neighborhood representation to parsimoniously model the
neighborhood relationships inherent in training data. Considering that real-world data are often linearly inseparable,
we further kernelize this basic model to improve its performance. As a result, the proposed method is able to learn
accurate hashing functions with tolerable computation and
storage cost. Experiments on four benchmarks demonstrate
that our method outperforms the state-of-the-arts.</p>
</td>
    <td>
      
        ICCV 
      
        Neural-Hashing 
      
        Memory-Efficiency 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/ding2015knn/">kNN Hashing with Factorized Neighborhood Representation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=kNN Hashing with Factorized Neighborhood Representation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=kNN Hashing with Factorized Neighborhood Representation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ding Kun, Huo, Fan, Pan</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE International Conference on Computer Vision (ICCV)</td>
    <td>14</td>
    <td><p>Hashing is very effective for many tasks in reducing the
processing time and in compressing massive databases. Although lots of approaches have been developed to learn
data-dependent hash functions in recent years, how to learn
hash functions to yield good performance with acceptable
computational and memory cost is still a challenging problem. Based on the observation that retrieval precision is
highly related to the kNN classification accuracy, this paper
proposes a novel kNN-based supervised hashing method,
which learns hash functions by directly maximizing the kNN
accuracy of the Hamming-embedded training data. To make
it scalable well to large problem, we propose a factorized
neighborhood representation to parsimoniously model the
neighborhood relationships inherent in training data. Considering that real-world data are often linearly inseparable,
we further kernelize this basic model to improve its performance. As a result, the proposed method is able to learn
accurate hashing functions with tolerable computation and
storage cost. Experiments on four benchmarks demonstrate
that our method outperforms the state-of-the-arts.</p>
</td>
    <td>
      
        ICCV 
      
        Neural-Hashing 
      
        Memory-Efficiency 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/li2025feature/">Feature Learning based Deep Supervised Hashing with Pairwise Labels</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Feature Learning based Deep Supervised Hashing with Pairwise Labels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Feature Learning based Deep Supervised Hashing with Pairwise Labels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Wu-jun, Kang</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>510</td>
    <td><p>Recent years have witnessed wide application of
hashing for large-scale image retrieval. However,
most existing hashing methods are based on handcrafted features which might not be optimally compatible with the hashing procedure. Recently, deep
hashing methods have been proposed to perform simultaneous feature learning and hash-code learning with deep neural networks, which have shown
better performance than traditional hashing methods with hand-crafted features. Most of these deep
hashing methods are supervised whose supervised
information is given with triplet labels. For another common application scenario with pairwise labels, there have not existed methods for simultaneous feature learning and hash-code learning. In this
paper, we propose a novel deep hashing method,
called deep pairwise-supervised hashing (DPSH),
to perform simultaneous feature learning and hashcode learning for applications with pairwise labels.
Experiments on real datasets show that our DPSH
method can outperform other methods to achieve
the state-of-the-art performance in image retrieval
applications.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Datasets 
      
        Neural-Hashing 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/zheng2015person/">Person Re-identification Meets Image Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Person Re-identification Meets Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Person Re-identification Meets Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zheng Liang, Shen Liyue, Tian Lu, Wang Shengjin, Bu Jiahao, Tian Qi</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>60</td>
    <td><p>For long time, person re-identification and image search are two separately
studied tasks. However, for person re-identification, the effectiveness of
local features and the “query-search” mode make it well posed for image search
techniques.
  In the light of recent advances in image search, this paper proposes to treat
person re-identification as an image search problem. Specifically, this paper
claims two major contributions. 1) By designing an unsupervised Bag-of-Words
representation, we are devoted to bridging the gap between the two tasks by
integrating techniques from image search in person re-identification. We show
that our system sets up an effective yet efficient baseline that is amenable to
further supervised/unsupervised improvements. 2) We contribute a new high
quality dataset which uses DPM detector and includes a number of distractor
images. Our dataset reaches closer to realistic settings, and new perspectives
are provided.
  Compared with approaches that rely on feature-feature match, our method is
faster by over two orders of magnitude. Moreover, on three datasets, we report
competitive results compared with the state-of-the-art methods.</p>
</td>
    <td>
      
        Unsupervised 
      
        Datasets 
      
        Supervised 
      
        Image-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/zhao2015deep/">Deep Semantic Ranking Based Hashing for Multi-Label Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Semantic Ranking Based Hashing for Multi-Label Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Semantic Ranking Based Hashing for Multi-Label Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhao F., Huang, Wang, Tan</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>463</td>
    <td><p>With the rapid growth of web images, hashing has received
increasing interests in large scale image retrieval.
Research efforts have been devoted to learning compact binary
codes that preserve semantic similarity based on labels.
However, most of these hashing methods are designed
to handle simple binary similarity. The complex multilevel
semantic structure of images associated with multiple labels
have not yet been well explored. Here we propose a deep
semantic ranking based method for learning hash functions
that preserve multilevel semantic similarity between multilabel
images. In our approach, deep convolutional neural
network is incorporated into hash functions to jointly
learn feature representations and mappings from them to
hash codes, which avoids the limitation of semantic representation
power of hand-crafted features. Meanwhile, a
ranking list that encodes the multilevel similarity information
is employed to guide the learning of such deep hash
functions. An effective scheme based on surrogate loss is
used to solve the intractable optimization problem of nonsmooth
and multivariate ranking measures involved in the
learning procedure. Experimental results show the superiority
of our proposed approach over several state-of-theart
hashing methods in term of ranking evaluation metrics
when tested on multi-label image datasets.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Datasets 
      
        CVPR 
      
        Neural-Hashing 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/lai2025simultaneous/">Simultaneous Feature Learning and Hash Coding with Deep Neural Networks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Simultaneous Feature Learning and Hash Coding with Deep Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Simultaneous Feature Learning and Hash Coding with Deep Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lai H., Pan, Liu, Yan</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>916</td>
    <td><p>Similarity-preserving hashing is a widely-used method
for nearest neighbour search in large-scale image retrieval
tasks. For most existing hashing methods, an image is
first encoded as a vector of hand-engineering visual features,
followed by another separate projection or quantization
step that generates binary codes. However, such visual
feature vectors may not be optimally compatible with the
coding process, thus producing sub-optimal hashing codes.
In this paper, we propose a deep architecture for supervised
hashing, in which images are mapped into binary codes via
carefully designed deep neural networks. The pipeline of
the proposed deep architecture consists of three building
blocks: 1) a sub-network with a stack of convolution layers
to produce the effective intermediate image features; 2)
a divide-and-encode module to divide the intermediate image
features into multiple branches, each encoded into one
hash bit; and 3) a triplet ranking loss designed to characterize
that one image is more similar to the second image than
to the third one. Extensive evaluations on several benchmark
image datasets show that the proposed simultaneous
feature learning and hash coding pipeline brings substantial
improvements over other state-of-the-art supervised or
unsupervised hashing methods.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Datasets 
      
        CVPR 
      
        Neural-Hashing 
      
        Supervised 
      
        Compact-Codes 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Quantization 
      
        Evaluation 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/lai2015simultaneous/">Simultaneous Feature Learning and Hash Coding with Deep Neural Networks</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Simultaneous Feature Learning and Hash Coding with Deep Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Simultaneous Feature Learning and Hash Coding with Deep Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lai H., Pan, Liu, Yan</td> <!-- 🔧 You were missing this -->
    <td>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
    <td>916</td>
    <td><p>Similarity-preserving hashing is a widely-used method
for nearest neighbour search in large-scale image retrieval
tasks. For most existing hashing methods, an image is
first encoded as a vector of hand-engineering visual features,
followed by another separate projection or quantization
step that generates binary codes. However, such visual
feature vectors may not be optimally compatible with the
coding process, thus producing sub-optimal hashing codes.
In this paper, we propose a deep architecture for supervised
hashing, in which images are mapped into binary codes via
carefully designed deep neural networks. The pipeline of
the proposed deep architecture consists of three building
blocks: 1) a sub-network with a stack of convolution layers
to produce the effective intermediate image features; 2)
a divide-and-encode module to divide the intermediate image
features into multiple branches, each encoded into one
hash bit; and 3) a triplet ranking loss designed to characterize
that one image is more similar to the second image than
to the third one. Extensive evaluations on several benchmark
image datasets show that the proposed simultaneous
feature learning and hash coding pipeline brings substantial
improvements over other state-of-the-art supervised or
unsupervised hashing methods.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Datasets 
      
        CVPR 
      
        Neural-Hashing 
      
        Supervised 
      
        Compact-Codes 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Quantization 
      
        Evaluation 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/wang2025hamming/">Hamming Compatible Quantization for Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hamming Compatible Quantization for Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hamming Compatible Quantization for Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Z., Duan, Lin, Wang, Gao</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>20</td>
    <td><p>Hashing is one of the effective techniques for fast
Approximate Nearest Neighbour (ANN) search.
Traditional single-bit quantization (SBQ) in most
hashing methods incurs lots of quantization error
which seriously degrades the search performance.
To address the limitation of SBQ, researchers have
proposed promising multi-bit quantization (MBQ)
methods to quantize each projection dimension
with multiple bits. However, some MBQ methods
need to adopt specific distance for binary code
matching instead of the original Hamming distance,
which would significantly decrease the retrieval
speed. Two typical MBQ methods Hierarchical
Quantization and Double Bit Quantization
retain the Hamming distance, but both of them only
consider the projection dimensions during quantization,
ignoring the neighborhood structure of raw
data inherent in Euclidean space. In this paper,
we propose a multi-bit quantization method named
Hamming Compatible Quantization (HCQ) to preserve
the capability of similarity metric between
Euclidean space and Hamming space by utilizing
the neighborhood structure of raw data. Extensive
experiment results have shown our approach significantly
improves the performance of various stateof-the-art
hashing methods while maintaining fast
retrieval speed.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Compact-Codes 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Evaluation 
      
        Quantization 
      
    </td>
    </tr>      
    
     <tr>
  <td>2015</td>
    <td>
      <a href="/publications/weissman2014identifying/">Identifying Duplicate and Contradictory Information in Wikipedia</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Identifying Duplicate and Contradictory Information in Wikipedia' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Identifying Duplicate and Contradictory Information in Wikipedia' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Weissman Sarah, Ayhan Samet, Bradley Joshua, Lin Jimmy</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 15th ACM/IEEE-CS Joint Conference on Digital Libraries</td>
    <td>10</td>
    <td><p>Our study identifies sentences in Wikipedia articles that are either
identical or highly similar by applying techniques for near-duplicate detection
of web pages. This is accomplished with a MapReduce implementation of minhash
to identify clusters of sentences with high Jaccard similarity. We show that
these clusters can be categorized into six different types, two of which are
particularly interesting: identical sentences quantify the extent to which
content in Wikipedia is copied and pasted, and near-duplicate sentences that
state contradictory facts point to quality issues in Wikipedia.</p>
</td>
    <td>
      
        Locality-Sensitive-Hashing 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/shrivastava2025densifying/">Densifying One Permutation Hashing via Rotation for Fast Near Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Densifying One Permutation Hashing via Rotation for Fast Near Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Densifying One Permutation Hashing via Rotation for Fast Near Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shrivastava A., Li</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>86</td>
    <td><p>The query complexity of locality sensitive hashing
(LSH) based similarity search is dominated
by the number of hash evaluations, and this number
grows with the data size (Indyk &amp; Motwani,
1998). In industrial applications such as search
where the data are often high-dimensional and
binary (e.g., text n-grams), minwise hashing is
widely adopted, which requires applying a large
number of permutations on the data. This is
costly in computation and energy-consumption.
In this paper, we propose a hashing technique
which generates all the necessary hash evaluations
needed for similarity search, using one
single permutation. The heart of the proposed
hash function is a “rotation” scheme which densifies
the sparse sketches of one permutation
hashing (Li et al., 2012) in an unbiased fashion
thereby maintaining the LSH property. This
makes the obtained sketches suitable for hash table
construction. This idea of rotation presented
in this paper could be of independent interest for
densifying other types of sparse sketches.
Using our proposed hashing method, the query
time of a (K, L)-parameterized LSH is reduced
from the typical O(dKL) complexity to merely
O(KL + dL), where d is the number of nonzeros
of the data vector, K is the number of hashes
in each hash table, and L is the number of hash
tables. Our experimental evaluation on real data
confirms that the proposed scheme significantly
reduces the query processing time over minwise
hashing without loss in retrieval accuracies.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Locality-Sensitive-Hashing 
      
        Evaluation 
      
        Similarity-Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/liu2025collaborative/">Collaborative Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Collaborative Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Collaborative Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu X., He, Deng, Lang</td> <!-- 🔧 You were missing this -->
    <td>2014 IEEE Conference on Computer Vision and Pattern Recognition</td>
    <td>130</td>
    <td><p>Hashing technique has become a promising approach for
fast similarity search. Most of existing hashing research
pursue the binary codes for the same type of entities by
preserving their similarities. In practice, there are many
scenarios involving nearest neighbor search on the data
given in matrix form, where two different types of, yet
naturally associated entities respectively correspond to its
two dimensions or views. To fully explore the duality
between the two views, we propose a collaborative hashing
scheme for the data in matrix form to enable fast search
in various applications such as image search using bag of
words and recommendation using user-item ratings. By
simultaneously preserving both the entity similarities in
each view and the interrelationship between views, our
collaborative hashing effectively learns the compact binary
codes and the explicit hash functions for out-of-sample
extension in an alternating optimization way. Extensive
evaluations are conducted on three well-known datasets
for search inside a single view and search across different
views, demonstrating that our proposed method outperforms
state-of-the-art baselines, with significant accuracy
gains ranging from 7.67% to 45.87% relatively.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Datasets 
      
        CVPR 
      
        Compact-Codes 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Recommender-Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/irie2014locally/">Locally Linear Hashing for Extracting Non-Linear Manifolds</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Locally Linear Hashing for Extracting Non-Linear Manifolds' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Locally Linear Hashing for Extracting Non-Linear Manifolds' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Irie G., Li, Wu, Chang</td> <!-- 🔧 You were missing this -->
    <td>2014 IEEE Conference on Computer Vision and Pattern Recognition</td>
    <td>94</td>
    <td><p>Previous efforts in hashing intend to preserve data variance
or pairwise affinity, but neither is adequate in capturing
the manifold structures hidden in most visual data. In
this paper, we tackle this problem by reconstructing the locally
linear structures of manifolds in the binary Hamming
space, which can be learned by locality-sensitive sparse
coding. We cast the problem as a joint minimization of
reconstruction error and quantization loss, and show that,
despite its NP-hardness, a local optimum can be obtained
efficiently via alternative optimization. Our method distinguishes
itself from existing methods in its remarkable ability
to extract the nearest neighbors of the query from the
same manifold, instead of from the ambient space. On extensive
experiments on various image benchmarks, our results
improve previous state-of-the-art by 28-74% typically,
and 627% on the Yale face data.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Quantization 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/zhang2025supervised/">Supervised Hashing with Latent Factor Models</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Hashing with Latent Factor Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Supervised Hashing with Latent Factor Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang P., Zhang, Li, Guo</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</td>
    <td>286</td>
    <td><p>Due to its low storage cost and fast query speed, hashing
has been widely adopted for approximate nearest neighbor
search in large-scale datasets. Traditional hashing methods
try to learn the hash codes in an unsupervised way where
the metric (Euclidean) structure of the training data is preserved.
Very recently, supervised hashing methods, which
try to preserve the semantic structure constructed from the
semantic labels of the training points, have exhibited higher
accuracy than unsupervised methods. In this paper, we
propose a novel supervised hashing method, called latent
factor hashing (LFH), to learn similarity-preserving binary
codes based on latent factor models. An algorithm with
convergence guarantee is proposed to learn the parameters
of LFH. Furthermore, a linear-time variant with stochastic
learning is proposed for training LFH on large-scale datasets.
Experimental results on two large datasets with semantic
labels show that LFH can achieve superior accuracy than
state-of-the-art methods with comparable training time.</p>
</td>
    <td>
      
        Scalability 
      
        Unsupervised 
      
        Datasets 
      
        Neural-Hashing 
      
        Memory-Efficiency 
      
        SIGIR 
      
        Hashing-Methods 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/shrivastava2014asymmetric/">Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS).</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS).' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS).' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shrivastava A., Li</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>268</td>
    <td><p>We present the first provably sublinear time hashing algorithm for approximate
Maximum Inner Product Search (MIPS). Searching with (un-normalized) inner
product as the underlying similarity measure is a known difficult problem and
finding hashing schemes for MIPS was considered hard. While the existing Locality
Sensitive Hashing (LSH) framework is insufficient for solving MIPS, in this
paper we extend the LSH framework to allow asymmetric hashing schemes. Our
proposal is based on a key observation that the problem of finding maximum inner
products, after independent asymmetric transformations, can be converted into
the problem of approximate near neighbor search in classical settings. This key
observation makes efficient sublinear hashing scheme for MIPS possible. Under
the extended asymmetric LSH (ALSH) framework, this paper provides an example
of explicit construction of provably fast hashing scheme for MIPS. Our proposed
algorithm is simple and easy to implement. The proposed hashing scheme
leads to significant computational savings over the two popular conventional LSH
schemes: (i) Sign Random Projection (SRP) and (ii) hashing based on p-stable
distributions for L2 norm (L2LSH), in the collaborative filtering task of item recommendations
on Netflix and Movielens (10M) datasets.</p>
</td>
    <td>
      
        Datasets 
      
        Locality-Sensitive-Hashing 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Recommender-Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/rippel2014learning/">Learning Ordered Representations with Nested Dropout</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Ordered Representations with Nested Dropout' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Ordered Representations with Nested Dropout' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Rippel Oren, Gelbart Michael A., Adams Ryan P.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>49</td>
    <td><p>In this paper, we study ordered representations of data in which different
dimensions have different degrees of importance. To learn these representations
we introduce nested dropout, a procedure for stochastically removing coherent
nested sets of hidden units in a neural network. We first present a sequence of
theoretical results in the simple case of a semi-linear autoencoder. We
rigorously show that the application of nested dropout enforces identifiability
of the units, which leads to an exact equivalence with PCA. We then extend the
algorithm to deep models and demonstrate the relevance of ordered
representations to a number of applications. Specifically, we use the ordered
property of the learned codes to construct hash-based data structures that
permit very fast retrieval, achieving retrieval in time logarithmic in the
database size and independent of the dimensionality of the representation. This
allows codes that are hundreds of times longer than currently feasible for
retrieval. We therefore avoid the diminished quality associated with short
codes, while still performing retrieval that is competitive in speed with
existing methods. We also show that ordered representations are a promising way
to learn adaptive compression for efficient online data reconstruction.</p>
</td>
    <td>
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/zhang2025large/">Large-scale supervised multimodal hashing with semantic correlation maximization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Large-scale supervised multimodal hashing with semantic correlation maximization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Large-scale supervised multimodal hashing with semantic correlation maximization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang D., Li</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>619</td>
    <td><p>Due to its low storage cost and fast query speed, hashing
has been widely adopted for similarity search in multimedia
data. In particular, more and more attentions
have been payed to multimodal hashing for search in
multimedia data with multiple modalities, such as images
with tags. Typically, supervised information of semantic
labels is also available for the data points in
many real applications. Hence, many supervised multimodal
hashing (SMH) methods have been proposed
to utilize such semantic labels to further improve the
search accuracy. However, the training time complexity
of most existing SMH methods is too high, which
makes them unscalable to large-scale datasets. In this
paper, a novel SMH method, called semantic correlation
maximization (SCM), is proposed to seamlessly integrate
semantic labels into the hashing learning procedure
for large-scale data modeling. Experimental results
on two real-world datasets show that SCM can signifi-
cantly outperform the state-of-the-art SMH methods, in
terms of both accuracy and scalability.</p>
</td>
    <td>
      
        Scalability 
      
        Datasets 
      
        Memory-Efficiency 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        AAAI 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/lin2014fast/">Fast supervised hashing with decision trees for high-dimensional data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast supervised hashing with decision trees for high-dimensional data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast supervised hashing with decision trees for high-dimensional data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin Guosheng, Shen, Shi, Hengel, Suter.</td> <!-- 🔧 You were missing this -->
    <td>2014 IEEE Conference on Computer Vision and Pattern Recognition</td>
    <td>453</td>
    <td><p>Supervised hashing aims to map the original features to
compact binary codes that are able to preserve label based
similarity in the Hamming space. Non-linear hash functions
have demonstrated their advantage over linear ones due to
their powerful generalization capability. In the literature,
kernel functions are typically used to achieve non-linearity
in hashing, which achieve encouraging retrieval performance at the price of slow evaluation and training time.
Here we propose to use boosted decision trees for achieving
non-linearity in hashing, which are fast to train and evaluate, hence more suitable for hashing with high dimensional
data. In our approach, we first propose sub-modular formulations for the hashing binary code inference problem
and an efficient GraphCut based block search method for
solving large-scale inference.
Then we learn hash functions by training boosted decision trees to fit the binary
codes. Experiments demonstrate that our proposed method
significantly outperforms most state-of-the-art methods in
retrieval precision and training time. Especially for highdimensional data, our method is orders of magnitude faster
than many methods in terms of training time.</p>
</td>
    <td>
      
        Scalability 
      
        CVPR 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/ge2014graph/">Graph Cuts for Supervised Binary Coding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Graph Cuts for Supervised Binary Coding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Graph Cuts for Supervised Binary Coding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ge T., He, Sun</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>69</td>
    <td><p>Learning short binary codes is challenged by the inherent discrete
nature of the problem. The graph cuts algorithm is a well-studied
discrete label assignment solution in computer vision, but has not yet
been applied to solve the binary coding problems. This is partially because
it was unclear how to use it to learn the encoding (hashing) functions
for out-of-sample generalization. In this paper, we formulate supervised
binary coding as a single optimization problem that involves both
the encoding functions and the binary label assignment. Then we apply
the graph cuts algorithm to address the discrete optimization problem
involved, with no continuous relaxation. This method, named as Graph
Cuts Coding (GCC), shows competitive results in various datasets.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Compact-Codes 
      
        Supervised 
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/yi2014deep/">Deep Metric Learning for Practical Person Re-Identification</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Deep Metric Learning for Practical Person Re-Identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Deep Metric Learning for Practical Person Re-Identification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yi Dong, Lei Zhen, Li Stan Z.</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>143</td>
    <td><p>Various hand-crafted features and metric learning methods prevail in the
field of person re-identification. Compared to these methods, this paper
proposes a more general way that can learn a similarity metric from image
pixels directly. By using a “siamese” deep neural network, the proposed method
can jointly learn the color feature, texture feature and metric in a unified
framework. The network has a symmetry structure with two sub-networks which are
connected by Cosine function. To deal with the big variations of person images,
binomial deviance is used to evaluate the cost between similarities and labels,
which is proved to be robust to outliers.
  Compared to existing researches, a more practical setting is studied in the
experiments that is training and test on different datasets (cross dataset
person re-identification). Both in “intra dataset” and “cross dataset”
settings, the superiorities of the proposed method are illustrated on VIPeR and
PRID.</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/zhang2014large/">Large-scale supervised multimodal hashing with semantic correlation maximization</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Large-scale supervised multimodal hashing with semantic correlation maximization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Large-scale supervised multimodal hashing with semantic correlation maximization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang D., Li</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the AAAI Conference on Artificial Intelligence</td>
    <td>619</td>
    <td><p>Due to its low storage cost and fast query speed, hashing
has been widely adopted for similarity search in multimedia
data. In particular, more and more attentions
have been payed to multimodal hashing for search in
multimedia data with multiple modalities, such as images
with tags. Typically, supervised information of semantic
labels is also available for the data points in
many real applications. Hence, many supervised multimodal
hashing (SMH) methods have been proposed
to utilize such semantic labels to further improve the
search accuracy. However, the training time complexity
of most existing SMH methods is too high, which
makes them unscalable to large-scale datasets. In this
paper, a novel SMH method, called semantic correlation
maximization (SCM), is proposed to seamlessly integrate
semantic labels into the hashing learning procedure
for large-scale data modeling. Experimental results
on two real-world datasets show that SCM can signifi-
cantly outperform the state-of-the-art SMH methods, in
terms of both accuracy and scalability.</p>
</td>
    <td>
      
        Scalability 
      
        Datasets 
      
        Memory-Efficiency 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        AAAI 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/zhang2014supervised/">Supervised Hashing with Latent Factor Models</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Hashing with Latent Factor Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Supervised Hashing with Latent Factor Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang P., Zhang, Li, Guo</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</td>
    <td>286</td>
    <td><p>Due to its low storage cost and fast query speed, hashing
has been widely adopted for approximate nearest neighbor
search in large-scale datasets. Traditional hashing methods
try to learn the hash codes in an unsupervised way where
the metric (Euclidean) structure of the training data is preserved.
Very recently, supervised hashing methods, which
try to preserve the semantic structure constructed from the
semantic labels of the training points, have exhibited higher
accuracy than unsupervised methods. In this paper, we
propose a novel supervised hashing method, called latent
factor hashing (LFH), to learn similarity-preserving binary
codes based on latent factor models. An algorithm with
convergence guarantee is proposed to learn the parameters
of LFH. Furthermore, a linear-time variant with stochastic
learning is proposed for training LFH on large-scale datasets.
Experimental results on two large datasets with semantic
labels show that LFH can achieve superior accuracy than
state-of-the-art methods with comparable training time.</p>
</td>
    <td>
      
        Scalability 
      
        Unsupervised 
      
        Datasets 
      
        Neural-Hashing 
      
        Memory-Efficiency 
      
        SIGIR 
      
        Hashing-Methods 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/yu2014circulant/">Circulant Binary Embedding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Circulant Binary Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Circulant Binary Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yu F., Kumar, Gong, Chang</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>132</td>
    <td><p>Binary embedding of high-dimensional data requires
long codes to preserve the discriminative
power of the input space. Traditional binary coding
methods often suffer from very high computation
and storage costs in such a scenario. To
address this problem, we propose Circulant Binary
Embedding (CBE) which generates binary
codes by projecting the data with a circulant matrix.
The circulant structure enables the use of
Fast Fourier Transformation to speed up the computation.
Compared to methods that use unstructured
matrices, the proposed method improves
the time complexity from O(d^2
) to O(d log d),
and the space complexity from O(d^2) to O(d)
where d is the input dimensionality. We also
propose a novel time-frequency alternating optimization
to learn data-dependent circulant projections,
which alternatively minimizes the objective
in original and Fourier domains. We show
by extensive experiments that the proposed approach
gives much better performance than the
state-of-the-art approaches for fixed time, and
provides much faster computation with no performance
degradation for fixed number of bits.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Evaluation 
      
        Memory-Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/liu2014collaborative/">Collaborative Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Collaborative Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Collaborative Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu X., He, Deng, Lang</td> <!-- 🔧 You were missing this -->
    <td>2014 IEEE Conference on Computer Vision and Pattern Recognition</td>
    <td>130</td>
    <td><p>Hashing technique has become a promising approach for
fast similarity search. Most of existing hashing research
pursue the binary codes for the same type of entities by
preserving their similarities. In practice, there are many
scenarios involving nearest neighbor search on the data
given in matrix form, where two different types of, yet
naturally associated entities respectively correspond to its
two dimensions or views. To fully explore the duality
between the two views, we propose a collaborative hashing
scheme for the data in matrix form to enable fast search
in various applications such as image search using bag of
words and recommendation using user-item ratings. By
simultaneously preserving both the entity similarities in
each view and the interrelationship between views, our
collaborative hashing effectively learns the compact binary
codes and the explicit hash functions for out-of-sample
extension in an alternating optimization way. Extensive
evaluations are conducted on three well-known datasets
for search inside a single view and search across different
views, demonstrating that our proposed method outperforms
state-of-the-art baselines, with significant accuracy
gains ranging from 7.67% to 45.87% relatively.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Datasets 
      
        CVPR 
      
        Compact-Codes 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Recommender-Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/lin2014optimizing/">Optimizing Ranking Measures for Compact Binary Code Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Optimizing Ranking Measures for Compact Binary Code Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Optimizing Ranking Measures for Compact Binary Code Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin Guosheng, Shen, Wu.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>23</td>
    <td><p>Hashing has proven a valuable tool for large-scale information retrieval. Despite much success, existing hashing methods optimize over simple objectives such as the reconstruction error or graph Laplacian related loss functions, instead of the performance evaluation criteria of interest—multivariate performance measures such as the AUC and NDCG. Here we present a general framework (termed StructHash) that allows one to directly optimize multivariate performance measures.
The resulting optimization problem can involve exponentially or infinitely many variables and constraints, which is more challenging than standard structured output learning. To solve the StructHash optimization problem, we use a combination of column generation and cutting-plane techniques. We demonstrate the generality of StructHash by applying it to ranking prediction and image retrieval, and show that it outperforms a few state-of-the-art hashing methods.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Tools-&-Libraries 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/shrivastava2025asymmetric/">Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS).</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS).' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS).' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shrivastava A., Li</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>268</td>
    <td><p>We present the first provably sublinear time hashing algorithm for approximate
Maximum Inner Product Search (MIPS). Searching with (un-normalized) inner
product as the underlying similarity measure is a known difficult problem and
finding hashing schemes for MIPS was considered hard. While the existing Locality
Sensitive Hashing (LSH) framework is insufficient for solving MIPS, in this
paper we extend the LSH framework to allow asymmetric hashing schemes. Our
proposal is based on a key observation that the problem of finding maximum inner
products, after independent asymmetric transformations, can be converted into
the problem of approximate near neighbor search in classical settings. This key
observation makes efficient sublinear hashing scheme for MIPS possible. Under
the extended asymmetric LSH (ALSH) framework, this paper provides an example
of explicit construction of provably fast hashing scheme for MIPS. Our proposed
algorithm is simple and easy to implement. The proposed hashing scheme
leads to significant computational savings over the two popular conventional LSH
schemes: (i) Sign Random Projection (SRP) and (ii) hashing based on p-stable
distributions for L2 norm (L2LSH), in the collaborative filtering task of item recommendations
on Netflix and Movielens (10M) datasets.</p>
</td>
    <td>
      
        Datasets 
      
        Locality-Sensitive-Hashing 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Recommender-Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/csurka2014unsupervised/">Unsupervised Visual and Textual Information Fusion in Multimedia Retrieval - A Graph-based Point of View</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Visual and Textual Information Fusion in Multimedia Retrieval - A Graph-based Point of View' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Unsupervised Visual and Textual Information Fusion in Multimedia Retrieval - A Graph-based Point of View' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Csurka Gabriela, Ah-pine Julien, Clinchant Stéphane</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>6</td>
    <td><p>Multimedia collections are more than ever growing in size and diversity.
Effective multimedia retrieval systems are thus critical to access these
datasets from the end-user perspective and in a scalable way. We are interested
in repositories of image/text multimedia objects and we study multimodal
information fusion techniques in the context of content based multimedia
information retrieval. We focus on graph based methods which have proven to
provide state-of-the-art performances. We particularly examine two of such
methods : cross-media similarities and random walk based scores. From a
theoretical viewpoint, we propose a unifying graph based framework which
encompasses the two aforementioned approaches. Our proposal allows us to
highlight the core features one should consider when using a graph based
technique for the combination of visual and textual information. We compare
cross-media and random walk based results using three different real-world
datasets. From a practical standpoint, our extended empirical analysis allow us
to provide insights and guidelines about the use of graph based methods for
multimodal information fusion in content based multimedia information
retrieval.</p>
</td>
    <td>
      
        Unsupervised 
      
        Datasets 
      
        Tools-&-Libraries 
      
        Graph-Based-ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/irie2025locally/">Locally Linear Hashing for Extracting Non-Linear Manifolds</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Locally Linear Hashing for Extracting Non-Linear Manifolds' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Locally Linear Hashing for Extracting Non-Linear Manifolds' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Irie G., Li, Wu, Chang</td> <!-- 🔧 You were missing this -->
    <td>2014 IEEE Conference on Computer Vision and Pattern Recognition</td>
    <td>94</td>
    <td><p>Previous efforts in hashing intend to preserve data variance
or pairwise affinity, but neither is adequate in capturing
the manifold structures hidden in most visual data. In
this paper, we tackle this problem by reconstructing the locally
linear structures of manifolds in the binary Hamming
space, which can be learned by locality-sensitive sparse
coding. We cast the problem as a joint minimization of
reconstruction error and quantization loss, and show that,
despite its NP-hardness, a local optimum can be obtained
efficiently via alternative optimization. Our method distinguishes
itself from existing methods in its remarkable ability
to extract the nearest neighbors of the query from the
same manifold, instead of from the ambient space. On extensive
experiments on various image benchmarks, our results
improve previous state-of-the-art by 28-74% typically,
and 627% on the Yale face data.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Quantization 
      
        CVPR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/shrivastava2014densifying/">Densifying One Permutation Hashing via Rotation for Fast Near Neighbor Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Densifying One Permutation Hashing via Rotation for Fast Near Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Densifying One Permutation Hashing via Rotation for Fast Near Neighbor Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Shrivastava A., Li</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>86</td>
    <td><p>The query complexity of locality sensitive hashing
(LSH) based similarity search is dominated
by the number of hash evaluations, and this number
grows with the data size (Indyk &amp; Motwani,
1998). In industrial applications such as search
where the data are often high-dimensional and
binary (e.g., text n-grams), minwise hashing is
widely adopted, which requires applying a large
number of permutations on the data. This is
costly in computation and energy-consumption.
In this paper, we propose a hashing technique
which generates all the necessary hash evaluations
needed for similarity search, using one
single permutation. The heart of the proposed
hash function is a “rotation” scheme which densifies
the sparse sketches of one permutation
hashing (Li et al., 2012) in an unbiased fashion
thereby maintaining the LSH property. This
makes the obtained sketches suitable for hash table
construction. This idea of rotation presented
in this paper could be of independent interest for
densifying other types of sparse sketches.
Using our proposed hashing method, the query
time of a (K, L)-parameterized LSH is reduced
from the typical O(dKL) complexity to merely
O(KL + dL), where d is the number of nonzeros
of the data vector, K is the number of hashes
in each hash table, and L is the number of hash
tables. Our experimental evaluation on real data
confirms that the proposed scheme significantly
reduces the query processing time over minwise
hashing without loss in retrieval accuracies.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Locality-Sensitive-Hashing 
      
        Evaluation 
      
        Similarity-Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/yu2025circulant/">Circulant Binary Embedding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Circulant Binary Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Circulant Binary Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Yu F., Kumar, Gong, Chang</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>132</td>
    <td><p>Binary embedding of high-dimensional data requires
long codes to preserve the discriminative
power of the input space. Traditional binary coding
methods often suffer from very high computation
and storage costs in such a scenario. To
address this problem, we propose Circulant Binary
Embedding (CBE) which generates binary
codes by projecting the data with a circulant matrix.
The circulant structure enables the use of
Fast Fourier Transformation to speed up the computation.
Compared to methods that use unstructured
matrices, the proposed method improves
the time complexity from O(d^2
) to O(d log d),
and the space complexity from O(d^2) to O(d)
where d is the input dimensionality. We also
propose a novel time-frequency alternating optimization
to learn data-dependent circulant projections,
which alternatively minimizes the objective
in original and Fourier domains. We show
by extensive experiments that the proposed approach
gives much better performance than the
state-of-the-art approaches for fixed time, and
provides much faster computation with no performance
degradation for fixed number of bits.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Evaluation 
      
        Memory-Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/liu2025discrete/">Discrete Graph Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Discrete Graph Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Discrete Graph Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu W., Mu, Kumar, Chang</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>496</td>
    <td><p>Hashing has emerged as a popular technique for fast nearest neighbor search in gigantic
databases. In particular, learning based hashing has received considerable
attention due to its appealing storage and search efficiency. However, the performance
of most unsupervised learning based hashing methods deteriorates rapidly
as the hash code length increases. We argue that the degraded performance is due
to inferior optimization procedures used to achieve discrete binary codes. This
paper presents a graph-based unsupervised hashing model to preserve the neighborhood
structure of massive data in a discrete code space. We cast the graph
hashing problem into a discrete optimization framework which directly learns the
binary codes. A tractable alternating maximization algorithm is then proposed to
explicitly deal with the discrete constraints, yielding high-quality codes to well
capture the local neighborhoods. Extensive experiments performed on four large
datasets with up to one million samples show that our discrete optimization based
graph hashing method obtains superior search accuracy over state-of-the-art unsupervised
hashing methods, especially for longer codes.</p>
</td>
    <td>
      
        Efficiency 
      
        Datasets 
      
        Neural-Hashing 
      
        Tools-&-Libraries 
      
        Supervised 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Unsupervised 
      
        Graph-Based-ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/ge2025graph/">Graph Cuts for Supervised Binary Coding</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Graph Cuts for Supervised Binary Coding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Graph Cuts for Supervised Binary Coding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ge T., He, Sun</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>69</td>
    <td><p>Learning short binary codes is challenged by the inherent discrete
nature of the problem. The graph cuts algorithm is a well-studied
discrete label assignment solution in computer vision, but has not yet
been applied to solve the binary coding problems. This is partially because
it was unclear how to use it to learn the encoding (hashing) functions
for out-of-sample generalization. In this paper, we formulate supervised
binary coding as a single optimization problem that involves both
the encoding functions and the binary label assignment. Then we apply
the graph cuts algorithm to address the discrete optimization problem
involved, with no continuous relaxation. This method, named as Graph
Cuts Coding (GCC), shows competitive results in various datasets.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Compact-Codes 
      
        Supervised 
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/ding2014collective/">Collective Matrix Factorization Hashing for Multimodal data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Collective Matrix Factorization Hashing for Multimodal data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Collective Matrix Factorization Hashing for Multimodal data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ding G., Guo, Zhou</td> <!-- 🔧 You were missing this -->
    <td>2014 IEEE Conference on Computer Vision and Pattern Recognition</td>
    <td>641</td>
    <td><p>Nearest neighbor search methods based on hashing have
attracted considerable attention for effective and efficient
large-scale similarity search in computer vision and information
retrieval community. In this paper, we study the
problems of learning hash functions in the context of multimodal
data for cross-view similarity search. We put forward
a novel hashing method, which is referred to Collective
Matrix Factorization Hashing (CMFH). CMFH learns unified
hash codes by collective matrix factorization with latent
factor model from different modalities of one instance,
which can not only supports cross-view search but also increases
the search accuracy by merging multiple view information
sources. We also prove that CMFH, a similaritypreserving
hashing learning method, has upper and lower
boundaries. Extensive experiments verify that CMFH significantly
outperforms several state-of-the-art methods on
three different datasets.</p>
</td>
    <td>
      
        Scalability 
      
        Datasets 
      
        CVPR 
      
        Similarity-Search 
      
        Hashing-Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/ding2025collective/">Collective Matrix Factorization Hashing for Multimodal data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Collective Matrix Factorization Hashing for Multimodal data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Collective Matrix Factorization Hashing for Multimodal data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Ding G., Guo, Zhou</td> <!-- 🔧 You were missing this -->
    <td>2014 IEEE Conference on Computer Vision and Pattern Recognition</td>
    <td>641</td>
    <td><p>Nearest neighbor search methods based on hashing have
attracted considerable attention for effective and efficient
large-scale similarity search in computer vision and information
retrieval community. In this paper, we study the
problems of learning hash functions in the context of multimodal
data for cross-view similarity search. We put forward
a novel hashing method, which is referred to Collective
Matrix Factorization Hashing (CMFH). CMFH learns unified
hash codes by collective matrix factorization with latent
factor model from different modalities of one instance,
which can not only supports cross-view search but also increases
the search accuracy by merging multiple view information
sources. We also prove that CMFH, a similaritypreserving
hashing learning method, has upper and lower
boundaries. Extensive experiments verify that CMFH significantly
outperforms several state-of-the-art methods on
three different datasets.</p>
</td>
    <td>
      
        Scalability 
      
        Datasets 
      
        CVPR 
      
        Similarity-Search 
      
        Hashing-Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/zhu2014personalized/">Personalized recommendation with corrected similarity</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Personalized recommendation with corrected similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Personalized recommendation with corrected similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhu Xuzhen, Tian Hui, Cai Shimin</td> <!-- 🔧 You were missing this -->
    <td>Journal of Statistical Mechanics: Theory and Experiment</td>
    <td>25</td>
    <td><p>Personalized recommendation attracts a surge of interdisciplinary researches.
Especially, similarity based methods in applications of real recommendation
systems achieve great success. However, the computations of similarities are
overestimated or underestimated outstandingly due to the defective strategy of
unidirectional similarity estimation. In this paper, we solve this drawback by
leveraging mutual correction of forward and backward similarity estimations,
and propose a new personalized recommendation index, i.e., corrected similarity
based inference (CSI). Through extensive experiments on four benchmark
datasets, the results show a greater improvement of CSI in comparison with
these mainstream baselines. And the detailed analysis is presented to unveil
and understand the origin of such difference between CSI and mainstream
indices.</p>
</td>
    <td>
      
        Datasets 
      
        Recommender-Systems 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/lin2025fast/">Fast supervised hashing with decision trees for high-dimensional data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast supervised hashing with decision trees for high-dimensional data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast supervised hashing with decision trees for high-dimensional data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin Guosheng, Shen, Shi, Hengel, Suter.</td> <!-- 🔧 You were missing this -->
    <td>2014 IEEE Conference on Computer Vision and Pattern Recognition</td>
    <td>453</td>
    <td><p>Supervised hashing aims to map the original features to
compact binary codes that are able to preserve label based
similarity in the Hamming space. Non-linear hash functions
have demonstrated their advantage over linear ones due to
their powerful generalization capability. In the literature,
kernel functions are typically used to achieve non-linearity
in hashing, which achieve encouraging retrieval performance at the price of slow evaluation and training time.
Here we propose to use boosted decision trees for achieving
non-linearity in hashing, which are fast to train and evaluate, hence more suitable for hashing with high dimensional
data. In our approach, we first propose sub-modular formulations for the hashing binary code inference problem
and an efficient GraphCut based block search method for
solving large-scale inference.
Then we learn hash functions by training boosted decision trees to fit the binary
codes. Experiments demonstrate that our proposed method
significantly outperforms most state-of-the-art methods in
retrieval precision and training time. Especially for highdimensional data, our method is orders of magnitude faster
than many methods in terms of training time.</p>
</td>
    <td>
      
        Scalability 
      
        CVPR 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/liu2014discrete/">Discrete Graph Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Discrete Graph Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Discrete Graph Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu W., Mu, Kumar, Chang</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>496</td>
    <td><p>Hashing has emerged as a popular technique for fast nearest neighbor search in gigantic
databases. In particular, learning based hashing has received considerable
attention due to its appealing storage and search efficiency. However, the performance
of most unsupervised learning based hashing methods deteriorates rapidly
as the hash code length increases. We argue that the degraded performance is due
to inferior optimization procedures used to achieve discrete binary codes. This
paper presents a graph-based unsupervised hashing model to preserve the neighborhood
structure of massive data in a discrete code space. We cast the graph
hashing problem into a discrete optimization framework which directly learns the
binary codes. A tractable alternating maximization algorithm is then proposed to
explicitly deal with the discrete constraints, yielding high-quality codes to well
capture the local neighborhoods. Extensive experiments performed on four large
datasets with up to one million samples show that our discrete optimization based
graph hashing method obtains superior search accuracy over state-of-the-art unsupervised
hashing methods, especially for longer codes.</p>
</td>
    <td>
      
        Efficiency 
      
        Datasets 
      
        Neural-Hashing 
      
        Tools-&-Libraries 
      
        Supervised 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Unsupervised 
      
        Graph-Based-ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2014</td>
    <td>
      <a href="/publications/lin2025optimizing/">Optimizing Ranking Measures for Compact Binary Code Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Optimizing Ranking Measures for Compact Binary Code Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Optimizing Ranking Measures for Compact Binary Code Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin Guosheng, Shen, Wu.</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>23</td>
    <td><p>Hashing has proven a valuable tool for large-scale information retrieval. Despite much success, existing hashing methods optimize over simple objectives such as the reconstruction error or graph Laplacian related loss functions, instead of the performance evaluation criteria of interest—multivariate performance measures such as the AUC and NDCG. Here we present a general framework (termed StructHash) that allows one to directly optimize multivariate performance measures.
The resulting optimization problem can involve exponentially or infinitely many variables and constraints, which is more challenging than standard structured output learning. To solve the StructHash optimization problem, we use a combination of column generation and cutting-plane techniques. We demonstrate the generality of StructHash by applying it to ranking prediction and image retrieval, and show that it outperforms a few state-of-the-art hashing methods.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Tools-&-Libraries 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/li2013learning/">Learning Hash Functions Using Column Generation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Hash Functions Using Column Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Hash Functions Using Column Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li X., Lin, Shen, Hengel, Dick</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>93</td>
    <td><p>Fast nearest neighbor searching is becoming
an increasingly important tool in solving
many large-scale problems. Recently
a number of approaches to learning datadependent
hash functions have been developed.
In this work, we propose a column
generation based method for learning datadependent
hash functions on the basis of
proximity comparison information. Given a
set of triplets that encode the pairwise proximity
comparison information, our method
learns hash functions that preserve the relative
comparison relationships in the data
as well as possible within the large-margin
learning framework. The learning procedure
is implemented using column generation and
hence is named CGHash. At each iteration
of the column generation procedure, the best
hash function is selected. Unlike most other
hashing methods, our method generalizes to
new data points naturally; and has a training
objective which is convex, thus ensuring
that the global optimum can be identi-
fied. Experiments demonstrate that the proposed
method learns compact binary codes
and that its retrieval performance compares
favorably with state-of-the-art methods when
tested on a few benchmark datasets.</p>
</td>
    <td>
      
        Scalability 
      
        Datasets 
      
        Tools-&-Libraries 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/zhang2025binary/">Binary Code Ranking with Weighted Hamming Distance</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Binary Code Ranking with Weighted Hamming Distance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Binary Code Ranking with Weighted Hamming Distance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Lei, Zhang, Tang, Lu, Tian</td> <!-- 🔧 You were missing this -->
    <td>2013 IEEE Conference on Computer Vision and Pattern Recognition</td>
    <td>98</td>
    <td><p>Binary hashing has been widely used for efficient similarity search due to its query and storage efficiency. In most
existing binary hashing methods, the high-dimensional data are embedded into Hamming space and the distance or
similarity of two points are approximated by the Hamming
distance between their binary codes. The Hamming distance calculation is efficient, however, in practice, there are
often lots of results sharing the same Hamming distance to
a query, which makes this distance measure ambiguous and
poses a critical issue for similarity search where ranking is
important. In this paper, we propose a weighted Hamming
distance ranking algorithm (WhRank) to rank the binary
codes of hashing methods. By assigning different bit-level
weights to different hash bits, the returned binary codes
are ranked at a finer-grained binary code level. We give
an algorithm to learn the data-adaptive and query-sensitive
weight for each hash bit. Evaluations on two large-scale
image data sets demonstrate the efficacy of our weighted
Hamming distance for binary code ranking.</p>
</td>
    <td>
      
        Scalability 
      
        Efficiency 
      
        CVPR 
      
        Compact-Codes 
      
        Similarity-Search 
      
        Hashing-Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/lin2013general/">A General Two-Step Approach to Learning-Based Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A General Two-Step Approach to Learning-Based Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A General Two-Step Approach to Learning-Based Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin G., Shen, Suter, Hengel</td> <!-- 🔧 You were missing this -->
    <td>2013 IEEE International Conference on Computer Vision</td>
    <td>199</td>
    <td><p>Most existing approaches to hashing apply a single form of hash function, and an optimization process which
is typically deeply coupled to this specific form. This tight coupling restricts the flexibility of the method to
respond to the data, and can result in complex optimization problems that are difficult to solve. Here we propose
a flexible yet simple framework that is able to accommodate different types of loss functions and hash functions.
This framework allows a number of existing approaches to hashing to be placed in context, and simplifies the
development of new problem-specific hashing methods. Our framework decomposes hashing learning problem
into two steps: hash bit learning and hash function learning based on the learned bits. The first step can typically
be formulated as binary quadratic problems, and the second step can be accomplished by training standard binary
classifiers. Both problems have been extensively studied in the literature. Our extensive experiments demonstrate
that the proposed framework is effective, flexible and outperforms the state-of-the-art.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Tools-&-Libraries 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/zadeh2012dimension/">Dimension Independent Similarity Computation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Dimension Independent Similarity Computation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Dimension Independent Similarity Computation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zadeh Reza Bosagh, Goel Ashish</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>53</td>
    <td><p>We present a suite of algorithms for Dimension Independent Similarity
Computation (DISCO) to compute all pairwise similarities between very high
dimensional sparse vectors. All of our results are provably independent of
dimension, meaning apart from the initial cost of trivially reading in the
data, all subsequent operations are independent of the dimension, thus the
dimension can be very large. We study Cosine, Dice, Overlap, and the Jaccard
similarity measures. For Jaccard similiarity we include an improved version of
MinHash. Our results are geared toward the MapReduce framework. We empirically
validate our theorems at large scale using data from the social networking site
Twitter. At time of writing, our algorithms are live in production at
twitter.com.</p>
</td>
    <td>
      
        Locality-Sensitive-Hashing 
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/moran2013variable/">Variable Bit Quantisation for LSH</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Variable Bit Quantisation for LSH' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Variable Bit Quantisation for LSH' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Moran S., Lavrenko, Osborne</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>21</td>
    <td><p>We introduce a scheme for optimally allocating
a variable number of bits per
LSH hyperplane. Previous approaches assign
a constant number of bits per hyperplane.
This neglects the fact that a subset
of hyperplanes may be more informative
than others. Our method, dubbed Variable
Bit Quantisation (VBQ), provides a datadriven
non-uniform bit allocation across
hyperplanes. Despite only using a fraction
of the available hyperplanes, VBQ outperforms
uniform quantisation by up to 168%
for retrieval across standard text and image
datasets.</p>
</td>
    <td>
      
        Locality-Sensitive-Hashing 
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/fan2013supervised/">Supervised binary hash code learning with jensen shannon divergence</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Supervised binary hash code learning with jensen shannon divergence' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Supervised binary hash code learning with jensen shannon divergence' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fan Lixin</td> <!-- 🔧 You were missing this -->
    <td>2013 IEEE International Conference on Computer Vision</td>
    <td>15</td>
    <td><p>This paper proposes to learn binary hash codes within
a statistical learning framework, in which an upper bound
of the probability of Bayes decision errors is derived for
different forms of hash functions and a rigorous proof of
the convergence of the upper bound is presented. Consequently, minimizing such an upper bound leads to consistent
performance improvements of existing hash code learning
algorithms, regardless of whether original algorithms are
unsupervised or supervised. This paper also illustrates a
fast hash coding method that exploits simple binary tests to
achieve orders of magnitude improvement in coding speed
as compared to projection based methods.</p>
</td>
    <td>
      
        ICCV 
      
        Unsupervised 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/song2025inter/">Inter-Media Hashing for Large-Scale Retrieval from Heterogeneous Data Sources</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Inter-Media Hashing for Large-Scale Retrieval from Heterogeneous Data Sources' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Inter-Media Hashing for Large-Scale Retrieval from Heterogeneous Data Sources' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Song J., Yang, Yang, Huang, Shen</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data</td>
    <td>619</td>
    <td><p>In this paper, we present a new multimedia retrieval paradigm to innovate large-scale search of heterogenous multimedia data. It is able to return results of different media types from heterogeneous data sources, e.g., using a query image to retrieve relevant text documents or images from different data sources. This utilizes the widely available data from different sources and caters for the current users’ demand of receiving a result list simultaneously containing multiple types of data to obtain a comprehensive understanding of the query’s results. To enable large-scale inter-media retrieval, we propose a novel inter-media hashing (IMH) model to explore the correlations among multiple media types from different data sources and tackle the scalability issue. To this end, multimedia data from heterogeneous data sources are transformed into a common Hamming space, in which fast search can be easily implemented by XOR and bit-count operations. Furthermore, we integrate a linear regression model to learn hashing functions so that the hash codes for new data points can be efficiently generated. Experiments conducted on real-world large-scale multimedia datasets demonstrate the superiority of our proposed method compared with state-of-the-art techniques.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Scalability 
      
        Datasets 
      
        Large-Scale-Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/xu2025harmonious/">Harmonious Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Harmonious Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Harmonious Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xu B., Bu, Chen, He, Cai</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>49</td>
    <td><p>Hashing-based fast nearest neighbor search technique
has attracted great attention in both research
and industry areas recently. Many existing hashing
approaches encode data with projection-based hash
functions and represent each projected dimension
by 1-bit. However, the dimensions with high variance
hold large energy or information of data but
treated equivalently as dimensions with low variance,
which leads to a serious information loss. In
this paper, we introduce a novel hashing algorithm
called Harmonious Hashing which aims at learning
hash functions with low information loss. Specifically,
we learn a set of optimized projections to
preserve the maximum cumulative energy and meet
the constraint of equivalent variance on each dimension
as much as possible. In this way, we could
minimize the information loss after binarization.
Despite the extreme simplicity, our method outperforms
superiorly to many state-of-the-art hashing
methods in large-scale and high-dimensional nearest
neighbor search experiments.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Scalability 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/neyshabur2025power/">The Power of Asymmetry in Binary Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=The Power of Asymmetry in Binary Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=The Power of Asymmetry in Binary Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Neyshabur B., Salakhutdinov, Srebro</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>57</td>
    <td><p>When approximating binary similarity using the hamming distance between short
binary hashes, we show that even if the similarity is symmetric, we can have
shorter and more accurate hashes by using two distinct code maps. I.e. by approximating the similarity between x and x
0
as the hamming distance between f(x)
and g(x0), for two distinct binary codes f, g, rather than as the hamming distance
between f(x) and f(x0).</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Compact-Codes 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/neyshabur2013power/">The Power of Asymmetry in Binary Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=The Power of Asymmetry in Binary Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=The Power of Asymmetry in Binary Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Neyshabur B., Salakhutdinov, Srebro</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>57</td>
    <td><p>When approximating binary similarity using the hamming distance between short
binary hashes, we show that even if the similarity is symmetric, we can have
shorter and more accurate hashes by using two distinct code maps. I.e. by approximating the similarity between x and x
0
as the hamming distance between f(x)
and g(x0), for two distinct binary codes f, g, rather than as the hamming distance
between f(x) and f(x0).</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Compact-Codes 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/song2013inter/">Inter-Media Hashing for Large-Scale Retrieval from Heterogeneous Data Sources</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Inter-Media Hashing for Large-Scale Retrieval from Heterogeneous Data Sources' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Inter-Media Hashing for Large-Scale Retrieval from Heterogeneous Data Sources' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Song J., Yang, Yang, Huang, Shen</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data</td>
    <td>619</td>
    <td><p>In this paper, we present a new multimedia retrieval paradigm to innovate large-scale search of heterogenous multimedia data. It is able to return results of different media types from heterogeneous data sources, e.g., using a query image to retrieve relevant text documents or images from different data sources. This utilizes the widely available data from different sources and caters for the current users’ demand of receiving a result list simultaneously containing multiple types of data to obtain a comprehensive understanding of the query’s results. To enable large-scale inter-media retrieval, we propose a novel inter-media hashing (IMH) model to explore the correlations among multiple media types from different data sources and tackle the scalability issue. To this end, multimedia data from heterogeneous data sources are transformed into a common Hamming space, in which fast search can be easily implemented by XOR and bit-count operations. Furthermore, we integrate a linear regression model to learn hashing functions so that the hash codes for new data points can be efficiently generated. Experiments conducted on real-world large-scale multimedia datasets demonstrate the superiority of our proposed method compared with state-of-the-art techniques.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Scalability 
      
        Datasets 
      
        Large-Scale-Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/li2025learning/">Learning Hash Functions Using Column Generation</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Hash Functions Using Column Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Hash Functions Using Column Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li X., Lin, Shen, Hengel, Dick</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>93</td>
    <td><p>Fast nearest neighbor searching is becoming
an increasingly important tool in solving
many large-scale problems. Recently
a number of approaches to learning datadependent
hash functions have been developed.
In this work, we propose a column
generation based method for learning datadependent
hash functions on the basis of
proximity comparison information. Given a
set of triplets that encode the pairwise proximity
comparison information, our method
learns hash functions that preserve the relative
comparison relationships in the data
as well as possible within the large-margin
learning framework. The learning procedure
is implemented using column generation and
hence is named CGHash. At each iteration
of the column generation procedure, the best
hash function is selected. Unlike most other
hashing methods, our method generalizes to
new data points naturally; and has a training
objective which is convex, thus ensuring
that the global optimum can be identi-
fied. Experiments demonstrate that the proposed
method learns compact binary codes
and that its retrieval performance compares
favorably with state-of-the-art methods when
tested on a few benchmark datasets.</p>
</td>
    <td>
      
        Scalability 
      
        Datasets 
      
        Tools-&-Libraries 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/lin2025general/">A General Two-Step Approach to Learning-Based Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=A General Two-Step Approach to Learning-Based Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=A General Two-Step Approach to Learning-Based Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Lin G., Shen, Suter, Hengel</td> <!-- 🔧 You were missing this -->
    <td>2013 IEEE International Conference on Computer Vision</td>
    <td>199</td>
    <td><p>Most existing approaches to hashing apply a single form of hash function, and an optimization process which
is typically deeply coupled to this specific form. This tight coupling restricts the flexibility of the method to
respond to the data, and can result in complex optimization problems that are difficult to solve. Here we propose
a flexible yet simple framework that is able to accommodate different types of loss functions and hash functions.
This framework allows a number of existing approaches to hashing to be placed in context, and simplifies the
development of new problem-specific hashing methods. Our framework decomposes hashing learning problem
into two steps: hash bit learning and hash function learning based on the learned bits. The first step can typically
be formulated as binary quadratic problems, and the second step can be accomplished by training standard binary
classifiers. Both problems have been extensively studied in the literature. Our extensive experiments demonstrate
that the proposed framework is effective, flexible and outperforms the state-of-the-art.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Tools-&-Libraries 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/jin2025complementary/">Complementary Projection Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Complementary Projection Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Complementary Projection Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jin Z., Hu, Lin, Zhang, Lin, Cai, Li</td> <!-- 🔧 You were missing this -->
    <td>2013 IEEE International Conference on Computer Vision</td>
    <td>59</td>
    <td><p>Recently, hashing techniques have been widely applied
to solve the approximate nearest neighbors search problem
in many vision applications. Generally, these hashing
approaches generate 2^c buckets, where c is the length
of the hash code. A good hashing method should satisfy
the following two requirements: 1) mapping the nearby
data points into the same bucket or nearby (measured by
the Hamming distance) buckets. 2) all the data points are
evenly distributed among all the buckets. In this paper,
we propose a novel algorithm named Complementary Projection
Hashing (CPH) to find the optimal hashing functions
which explicitly considers the above two requirements.
Specifically, CPH aims at sequentially finding a series of hyperplanes
(hashing functions) which cross the sparse region
of the data. At the same time, the data points are evenly distributed
in the hypercubes generated by these hyperplanes.
The experiments comparing with the state-of-the-art hashing
methods demonstrate the effectiveness of the proposed
method.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/moran2025neighbourhood/">Neighbourhood Preserving Quantisation for LSH</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Neighbourhood Preserving Quantisation for LSH' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Neighbourhood Preserving Quantisation for LSH' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Moran S., Lavrenko, Osborne</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval</td>
    <td>22</td>
    <td><p>We introduce a scheme for optimally allocating multiple bits per hyperplane for Locality Sensitive Hashing (LSH). Existing approaches binarise LSH projections by thresholding at zero yielding a single bit per dimension. We demonstrate that this is a sub-optimal bit allocation approach that can easily destroy the neighbourhood structure in the original feature space. Our proposed method, dubbed Neighbourhood Preserving Quantization (NPQ), assigns multiple bits per hyperplane based upon adaptively learned thresholds. NPQ exploits a pairwise affinity matrix to discretise each dimension such that nearest neighbours in the original feature space fall within the same quantisation thresholds and are therefore assigned identical bits. NPQ is not only applicable to LSH, but can also be applied to any low-dimensional projection scheme. Despite using half the number of hyperplanes, NPQ is shown to improve LSH-based retrieval accuracy by up to 65% compared to the state-of-the-art.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Locality-Sensitive-Hashing 
      
        Quantization 
      
        SIGIR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/zhang2013binary/">Binary Code Ranking with Weighted Hamming Distance</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Binary Code Ranking with Weighted Hamming Distance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Binary Code Ranking with Weighted Hamming Distance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang Lei, Zhang, Tang, Lu, Tian</td> <!-- 🔧 You were missing this -->
    <td>2013 IEEE Conference on Computer Vision and Pattern Recognition</td>
    <td>98</td>
    <td><p>Binary hashing has been widely used for efficient similarity search due to its query and storage efficiency. In most
existing binary hashing methods, the high-dimensional data are embedded into Hamming space and the distance or
similarity of two points are approximated by the Hamming
distance between their binary codes. The Hamming distance calculation is efficient, however, in practice, there are
often lots of results sharing the same Hamming distance to
a query, which makes this distance measure ambiguous and
poses a critical issue for similarity search where ranking is
important. In this paper, we propose a weighted Hamming
distance ranking algorithm (WhRank) to rank the binary
codes of hashing methods. By assigning different bit-level
weights to different hash bits, the returned binary codes
are ranked at a finer-grained binary code level. We give
an algorithm to learn the data-adaptive and query-sensitive
weight for each hash bit. Evaluations on two large-scale
image data sets demonstrate the efficacy of our weighted
Hamming distance for binary code ranking.</p>
</td>
    <td>
      
        Scalability 
      
        Efficiency 
      
        CVPR 
      
        Compact-Codes 
      
        Similarity-Search 
      
        Hashing-Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/zhu2013linear/">Linear cross-modal hashing for efficient multimedia search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Linear cross-modal hashing for efficient multimedia search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Linear cross-modal hashing for efficient multimedia search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhu Xiaofeng, Huang, Shen, Zhao</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 21st ACM international conference on Multimedia</td>
    <td>278</td>
    <td><p>Most existing cross-modal hashing methods suffer from the scalability issue in the training phase. In this paper, we propose a novel 
cross-modal hashing approach with a linear time complexity to the training data size, to enable scalable indexing for multimedia 
search across multiple modals. Taking both the intra-similarity in each modal and the inter-similarity across different modals 
into consideration, the proposed approach aims at effectively learning hash functions from large-scale training datasets. 
More specifically, for each modal, we first partition the training data into \(k\) clusters and then represent each training data 
point with its distances to \(k\) centroids of the clusters. Interestingly, such a k-dimensional data representation can reduce 
the time complexity of the training phase from traditional O(n2) or higher to O(n), where \(n\) is the training data size, leading to 
practical learning on large-scale datasets. We further prove that this new representation preserves the intra-similarity in each modal. 
To preserve the inter-similarity among data points across different modals, we transform the derived data representations into a 
common binary subspace in which binary codes from all the modals are “consistent” and comparable. The transformation simultaneously 
outputs the hash functions for all modals, which are used to convert unseen data into binary codes. Given a query of one modal, 
it is first mapped into the binary codes using the modal’s hash functions, followed by matching the database binary codes of any other 
modals. Experimental results on two benchmark datasets confirm the scalability and the effectiveness of the proposed approach in 
comparison with the state of the art.</p>
</td>
    <td>
      
        Scalability 
      
        Datasets 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/fan2025supervised/">Supervised binary hash code learning with jensen shannon divergence</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Supervised binary hash code learning with jensen shannon divergence' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Supervised binary hash code learning with jensen shannon divergence' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Fan Lixin</td> <!-- 🔧 You were missing this -->
    <td>2013 IEEE International Conference on Computer Vision</td>
    <td>15</td>
    <td><p>This paper proposes to learn binary hash codes within
a statistical learning framework, in which an upper bound
of the probability of Bayes decision errors is derived for
different forms of hash functions and a rigorous proof of
the convergence of the upper bound is presented. Consequently, minimizing such an upper bound leads to consistent
performance improvements of existing hash code learning
algorithms, regardless of whether original algorithms are
unsupervised or supervised. This paper also illustrates a
fast hash coding method that exploits simple binary tests to
achieve orders of magnitude improvement in coding speed
as compared to projection based methods.</p>
</td>
    <td>
      
        ICCV 
      
        Unsupervised 
      
        Tools-&-Libraries 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/jin2013complementary/">Complementary Projection Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Complementary Projection Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Complementary Projection Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jin Z., Hu, Lin, Zhang, Lin, Cai, Li</td> <!-- 🔧 You were missing this -->
    <td>2013 IEEE International Conference on Computer Vision</td>
    <td>59</td>
    <td><p>Recently, hashing techniques have been widely applied
to solve the approximate nearest neighbors search problem
in many vision applications. Generally, these hashing
approaches generate 2^c buckets, where c is the length
of the hash code. A good hashing method should satisfy
the following two requirements: 1) mapping the nearby
data points into the same bucket or nearby (measured by
the Hamming distance) buckets. 2) all the data points are
evenly distributed among all the buckets. In this paper,
we propose a novel algorithm named Complementary Projection
Hashing (CPH) to find the optimal hashing functions
which explicitly considers the above two requirements.
Specifically, CPH aims at sequentially finding a series of hyperplanes
(hashing functions) which cross the sparse region
of the data. At the same time, the data points are evenly distributed
in the hypercubes generated by these hyperplanes.
The experiments comparing with the state-of-the-art hashing
methods demonstrate the effectiveness of the proposed
method.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        ICCV 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/sundaram2013streaming/">Streaming Similarity Search over one Billion Tweets using Parallel Locality-Sensitive Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Streaming Similarity Search over one Billion Tweets using Parallel Locality-Sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Streaming Similarity Search over one Billion Tweets using Parallel Locality-Sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sundaram Narayanan, Turmukhametova, Satish, Mostak, Indyk, Dubey</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the VLDB Endowment</td>
    <td>109</td>
    <td><p>Finding nearest neighbors has become an important operation on databases, with applications to text search, multimedia indexing,
and many other areas. One popular algorithm for similarity search, especially for high dimensional data (where spatial indexes like kdtrees do not perform well) is Locality Sensitive Hashing (LSH), an
approximation algorithm for finding similar objects. In this paper, we describe a new variant of LSH, called Parallel
LSH (PLSH) designed to be extremely efficient, capable of scaling out on multiple nodes and multiple cores, and which supports highthroughput streaming of new data. Our approach employs several
novel ideas, including: cache-conscious hash table layout, using a 2-level merge algorithm for hash table construction; an efficient
algorithm for duplicate elimination during hash-table querying; an insert-optimized hash table structure and efficient data expiration
algorithm for streaming data; and a performance model that accurately estimates performance of the algorithm and can be used to
optimize parameter settings. We show that on a workload where we perform similarity search on a dataset of &gt; 1 Billion tweets, with
hundreds of millions of new tweets per day, we can achieve query times of 1–2.5 ms. We show that this is an order of magnitude faster
than existing indexing schemes, such as inverted indexes. To the best of our knowledge, this is the fastest implementation of LSH,
with table construction times up to 3.7x faster and query times that are 8.3x faster than a basic implementation.</p>
</td>
    <td>
      
        Efficiency 
      
        Datasets 
      
        Text-Retrieval 
      
        Locality-Sensitive-Hashing 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/xu2013harmonious/">Harmonious Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Harmonious Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Harmonious Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Xu B., Bu, Chen, He, Cai</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>49</td>
    <td><p>Hashing-based fast nearest neighbor search technique
has attracted great attention in both research
and industry areas recently. Many existing hashing
approaches encode data with projection-based hash
functions and represent each projected dimension
by 1-bit. However, the dimensions with high variance
hold large energy or information of data but
treated equivalently as dimensions with low variance,
which leads to a serious information loss. In
this paper, we introduce a novel hashing algorithm
called Harmonious Hashing which aims at learning
hash functions with low information loss. Specifically,
we learn a set of optimized projections to
preserve the maximum cumulative energy and meet
the constraint of equivalent variance on each dimension
as much as possible. In this way, we could
minimize the information loss after binarization.
Despite the extreme simplicity, our method outperforms
superiorly to many state-of-the-art hashing
methods in large-scale and high-dimensional nearest
neighbor search experiments.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Scalability 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/gong2013learning/">Learning Binary Codes for High-Dimensional Data Using Bilinear Projections</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Binary Codes for High-Dimensional Data Using Bilinear Projections' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Binary Codes for High-Dimensional Data Using Bilinear Projections' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gong Y., Kumar, Rowley, Lazebnik</td> <!-- 🔧 You were missing this -->
    <td>2013 IEEE Conference on Computer Vision and Pattern Recognition</td>
    <td>183</td>
    <td><p>Recent advances in visual recognition indicate that to
achieve good retrieval and classification accuracy on largescale
datasets like ImageNet, extremely high-dimensional
visual descriptors, e.g., Fisher Vectors, are needed. We
present a novel method for converting such descriptors to
compact similarity-preserving binary codes that exploits
their natural matrix structure to reduce their dimensionality
using compact bilinear projections instead of a single
large projection matrix. This method achieves comparable
retrieval and classification accuracy to the original descriptors
and to the state-of-the-art Product Quantization
approach while having orders of magnitude faster code generation
time and smaller memory footprint.</p>
</td>
    <td>
      
        Datasets 
      
        CVPR 
      
        Memory-Efficiency 
      
        Compact-Codes 
      
        Quantization 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/moran2013neighbourhood/">Neighbourhood Preserving Quantisation for LSH</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Neighbourhood Preserving Quantisation for LSH' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Neighbourhood Preserving Quantisation for LSH' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Moran S., Lavrenko, Osborne</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval</td>
    <td>22</td>
    <td><p>We introduce a scheme for optimally allocating multiple bits per hyperplane for Locality Sensitive Hashing (LSH). Existing approaches binarise LSH projections by thresholding at zero yielding a single bit per dimension. We demonstrate that this is a sub-optimal bit allocation approach that can easily destroy the neighbourhood structure in the original feature space. Our proposed method, dubbed Neighbourhood Preserving Quantization (NPQ), assigns multiple bits per hyperplane based upon adaptively learned thresholds. NPQ exploits a pairwise affinity matrix to discretise each dimension such that nearest neighbours in the original feature space fall within the same quantisation thresholds and are therefore assigned identical bits. NPQ is not only applicable to LSH, but can also be applied to any low-dimensional projection scheme. Despite using half the number of hyperplanes, NPQ is shown to improve LSH-based retrieval accuracy by up to 65% compared to the state-of-the-art.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Locality-Sensitive-Hashing 
      
        Quantization 
      
        SIGIR 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/zhu2025linear/">Linear cross-modal hashing for efficient multimedia search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Linear cross-modal hashing for efficient multimedia search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Linear cross-modal hashing for efficient multimedia search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhu Xiaofeng, Huang, Shen, Zhao</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 21st ACM international conference on Multimedia</td>
    <td>278</td>
    <td><p>Most existing cross-modal hashing methods suffer from the scalability issue in the training phase. In this paper, we propose a novel 
cross-modal hashing approach with a linear time complexity to the training data size, to enable scalable indexing for multimedia 
search across multiple modals. Taking both the intra-similarity in each modal and the inter-similarity across different modals 
into consideration, the proposed approach aims at effectively learning hash functions from large-scale training datasets. 
More specifically, for each modal, we first partition the training data into \(k\) clusters and then represent each training data 
point with its distances to \(k\) centroids of the clusters. Interestingly, such a k-dimensional data representation can reduce 
the time complexity of the training phase from traditional O(n2) or higher to O(n), where \(n\) is the training data size, leading to 
practical learning on large-scale datasets. We further prove that this new representation preserves the intra-similarity in each modal. 
To preserve the inter-similarity among data points across different modals, we transform the derived data representations into a 
common binary subspace in which binary codes from all the modals are “consistent” and comparable. The transformation simultaneously 
outputs the hash functions for all modals, which are used to convert unseen data into binary codes. Given a query of one modal, 
it is first mapped into the binary codes using the modal’s hash functions, followed by matching the database binary codes of any other 
modals. Experimental results on two benchmark datasets confirm the scalability and the effectiveness of the proposed approach in 
comparison with the state of the art.</p>
</td>
    <td>
      
        Scalability 
      
        Datasets 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/gong2025learning/">Learning Binary Codes for High-Dimensional Data Using Bilinear Projections</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Binary Codes for High-Dimensional Data Using Bilinear Projections' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Binary Codes for High-Dimensional Data Using Bilinear Projections' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gong Y., Kumar, Rowley, Lazebnik</td> <!-- 🔧 You were missing this -->
    <td>2013 IEEE Conference on Computer Vision and Pattern Recognition</td>
    <td>183</td>
    <td><p>Recent advances in visual recognition indicate that to
achieve good retrieval and classification accuracy on largescale
datasets like ImageNet, extremely high-dimensional
visual descriptors, e.g., Fisher Vectors, are needed. We
present a novel method for converting such descriptors to
compact similarity-preserving binary codes that exploits
their natural matrix structure to reduce their dimensionality
using compact bilinear projections instead of a single
large projection matrix. This method achieves comparable
retrieval and classification accuracy to the original descriptors
and to the state-of-the-art Product Quantization
approach while having orders of magnitude faster code generation
time and smaller memory footprint.</p>
</td>
    <td>
      
        Datasets 
      
        CVPR 
      
        Memory-Efficiency 
      
        Compact-Codes 
      
        Quantization 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/huang2017online/">Online Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Online Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Online Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Huang Long-kai, Yang Qiang, Zheng Wei-shi</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>51</td>
    <td><p>Although hash function learning algorithms have achieved great success in
recent years, most existing hash models are off-line, which are not suitable
for processing sequential or online data. To address this problem, this work
proposes an online hash model to accommodate data coming in stream for online
learning. Specifically, a new loss function is proposed to measure the
similarity loss between a pair of data samples in hamming space. Then, a
structured hash model is derived and optimized in a passive-aggressive way.
Theoretical analysis on the upper bound of the cumulative loss for the proposed
online hash model is provided. Furthermore, we extend our online hashing from a
single-model to a multi-model online hashing that trains multiple models so as
to retain diverse online hashing models in order to avoid biased update. The
competitive efficiency and effectiveness of the proposed online hash models are
verified through extensive experiments on several large-scale datasets as
compared to related hashing methods.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        Scalability 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/moran2025variable/">Variable Bit Quantisation for LSH</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Variable Bit Quantisation for LSH' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Variable Bit Quantisation for LSH' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Moran S., Lavrenko, Osborne</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>21</td>
    <td><p>We introduce a scheme for optimally allocating
a variable number of bits per
LSH hyperplane. Previous approaches assign
a constant number of bits per hyperplane.
This neglects the fact that a subset
of hyperplanes may be more informative
than others. Our method, dubbed Variable
Bit Quantisation (VBQ), provides a datadriven
non-uniform bit allocation across
hyperplanes. Despite only using a fraction
of the available hyperplanes, VBQ outperforms
uniform quantisation by up to 168%
for retrieval across standard text and image
datasets.</p>
</td>
    <td>
      
        Locality-Sensitive-Hashing 
      
        Datasets 
      
    </td>
    </tr>      
    
     <tr>
  <td>2013</td>
    <td>
      <a href="/publications/sundaram2025streaming/">Streaming Similarity Search over one Billion Tweets using Parallel Locality-Sensitive Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Streaming Similarity Search over one Billion Tweets using Parallel Locality-Sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Streaming Similarity Search over one Billion Tweets using Parallel Locality-Sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Sundaram Narayanan, Turmukhametova, Satish, Mostak, Indyk, Dubey</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the VLDB Endowment</td>
    <td>109</td>
    <td><p>Finding nearest neighbors has become an important operation on databases, with applications to text search, multimedia indexing,
and many other areas. One popular algorithm for similarity search, especially for high dimensional data (where spatial indexes like kdtrees do not perform well) is Locality Sensitive Hashing (LSH), an
approximation algorithm for finding similar objects. In this paper, we describe a new variant of LSH, called Parallel
LSH (PLSH) designed to be extremely efficient, capable of scaling out on multiple nodes and multiple cores, and which supports highthroughput streaming of new data. Our approach employs several
novel ideas, including: cache-conscious hash table layout, using a 2-level merge algorithm for hash table construction; an efficient
algorithm for duplicate elimination during hash-table querying; an insert-optimized hash table structure and efficient data expiration
algorithm for streaming data; and a performance model that accurately estimates performance of the algorithm and can be used to
optimize parameter settings. We show that on a workload where we perform similarity search on a dataset of &gt; 1 Billion tweets, with
hundreds of millions of new tweets per day, we can achieve query times of 1–2.5 ms. We show that this is an order of magnitude faster
than existing indexing schemes, such as inverted indexes. To the best of our knowledge, this is the fastest implementation of LSH,
with table construction times up to 3.7x faster and query times that are 8.3x faster than a basic implementation.</p>
</td>
    <td>
      
        Efficiency 
      
        Datasets 
      
        Text-Retrieval 
      
        Locality-Sensitive-Hashing 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2012</td>
    <td>
      <a href="/publications/heo2025spherical/">Spherical Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Spherical Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Spherical Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Heo J., Lee, He, Chang, Yoon</td> <!-- 🔧 You were missing this -->
    <td>2012 IEEE Conference on Computer Vision and Pattern Recognition</td>
    <td>380</td>
    <td><p>Many binary code encoding schemes based on hashing
have been actively studied recently, since they can provide
efficient similarity search, especially nearest neighbor
search, and compact data representations suitable for handling
large scale image databases in many computer vision
problems. Existing hashing techniques encode highdimensional
data points by using hyperplane-based hashing
functions. In this paper we propose a novel hyperspherebased
hashing function, spherical hashing, to map more
spatially coherent data points into a binary code compared
to hyperplane-based hashing functions. Furthermore, we
propose a new binary code distance function, spherical
Hamming distance, that is tailored to our hyperspherebased
binary coding scheme, and design an efficient iterative
optimization process to achieve balanced partitioning
of data points for each hash function and independence between
hashing functions. Our extensive experiments show
that our spherical hashing technique significantly outperforms
six state-of-the-art hashing techniques based on hyperplanes
across various image benchmarks of sizes ranging
from one to 75 million of GIST descriptors. The performance
gains are consistent and large, up to 100% improvements.
The excellent results confirm the unique merits of
the proposed idea in using hyperspheres to encode proximity
regions in high-dimensional spaces. Finally, our method
is intuitive and easy to implement.</p>
</td>
    <td>
      
        CVPR 
      
        Compact-Codes 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2012</td>
    <td>
      <a href="/publications/norouzi2025hamming/">Hamming Distance Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hamming Distance Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hamming Distance Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Norouzi M., Fleet, Salakhutdinov</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>540</td>
    <td><p>Motivated by large-scale multimedia applications we propose to learn mappings
from high-dimensional data to binary codes that preserve semantic similarity.
Binary codes are well suited to large-scale applications as they are storage efficient and permit exact sub-linear kNN search. The framework is applicable
to broad families of mappings, and uses a flexible form of triplet ranking loss.
We overcome discontinuous optimization of the discrete mappings by minimizing
a piecewise-smooth upper bound on empirical loss, inspired by latent structural
SVMs. We develop a new loss-augmented inference algorithm that is quadratic in
the code length. We show strong retrieval performance on CIFAR-10 and MNIST,
with promising classification results using no more than kNN on the binary codes.</p>
</td>
    <td>
      
        Scalability 
      
        Distance-Metric-Learning 
      
        Tools-&-Libraries 
      
        Compact-Codes 
      
        Similarity-Search 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2012</td>
    <td>
      <a href="/publications/norouzi2012hamming/">Hamming Distance Metric Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hamming Distance Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hamming Distance Metric Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Norouzi M., Fleet, Salakhutdinov</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>540</td>
    <td><p>Motivated by large-scale multimedia applications we propose to learn mappings
from high-dimensional data to binary codes that preserve semantic similarity.
Binary codes are well suited to large-scale applications as they are storage efficient and permit exact sub-linear kNN search. The framework is applicable
to broad families of mappings, and uses a flexible form of triplet ranking loss.
We overcome discontinuous optimization of the discrete mappings by minimizing
a piecewise-smooth upper bound on empirical loss, inspired by latent structural
SVMs. We develop a new loss-augmented inference algorithm that is quadratic in
the code length. We show strong retrieval performance on CIFAR-10 and MNIST,
with promising classification results using no more than kNN on the binary codes.</p>
</td>
    <td>
      
        Scalability 
      
        Distance-Metric-Learning 
      
        Tools-&-Libraries 
      
        Compact-Codes 
      
        Similarity-Search 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2012</td>
    <td>
      <a href="/publications/kong2012isotropic/">Isotropic Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Isotropic Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Isotropic Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kong W., Li</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>260</td>
    <td><p>Most existing hashing methods adopt some projection functions to project the original data into several dimensions of real values, and then each of these projected dimensions is quantized into one bit (zero or one) by thresholding. Typically, the variances of different projected dimensions are different for existing projection functions such as principal component analysis (PCA). Using the same number of bits for different projected dimensions is unreasonable because larger-variance dimensions will carry more information. Although this viewpoint has been widely accepted by many researchers, it is still not verified by either theory or experiment because no methods have been proposed to find a projection with equal variances for different dimensions. In this paper, we propose a novel method, called isotropic hashing (IsoHash), to learn projection functions which can produce projected dimensions with isotropic variances (equal variances). Experimental results on real data sets show that IsoHash can outperform its counterpart with different variances for different dimensions, which verifies the viewpoint that projections with isotropic variances will be better than those with anisotropic variances.</p>
</td>
    <td>
      
        Hashing-Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2012</td>
    <td>
      <a href="/publications/zhen2025co/">Co-Regularized Hashing for Multimodal Data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Co-Regularized Hashing for Multimodal Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Co-Regularized Hashing for Multimodal Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhen Y., Yeung</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>195</td>
    <td><p>Hashing-based methods provide a very promising approach to large-scale similarity
search. To obtain compact hash codes, a recent trend seeks to learn the hash
functions from data automatically. In this paper, we study hash function learning
in the context of multimodal data. We propose a novel multimodal hash function
learning method, called Co-Regularized Hashing (CRH), based on a boosted coregularization
framework. The hash functions for each bit of the hash codes are
learned by solving DC (difference of convex functions) programs, while the learning
for multiple bits proceeds via a boosting procedure so that the bias introduced
by the hash functions can be sequentially minimized. We empirically compare
CRH with two state-of-the-art multimodal hash function learning methods on two
publicly available data sets.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Scalability 
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2012</td>
    <td>
      <a href="/publications/liu2025supervised/">Supervised Hashing with Kernels</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Hashing with Kernels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Supervised Hashing with Kernels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu W., Wang, Ji, Jiang, Chang</td> <!-- 🔧 You were missing this -->
    <td>2012 IEEE Conference on Computer Vision and Pattern Recognition</td>
    <td>1447</td>
    <td><p>Recent years have witnessed the growing popularity of
hashing in large-scale vision problems. It has been shown
that the hashing quality could be boosted by leveraging supervised
information into hash function learning. However,
the existing supervised methods either lack adequate performance
or often incur cumbersome model training. In this
paper, we propose a novel kernel-based supervised hashing
model which requires a limited amount of supervised information,
i.e., similar and dissimilar data pairs, and a feasible
training cost in achieving high quality hashing. The idea
is to map the data to compact binary codes whose Hamming
distances are minimized on similar pairs and simultaneously
maximized on dissimilar pairs. Our approach is
distinct from prior works by utilizing the equivalence between
optimizing the code inner products and the Hamming
distances. This enables us to sequentially and efficiently
train the hash functions one bit at a time, yielding very
short yet discriminative codes. We carry out extensive experiments
on two image benchmarks with up to one million
samples, demonstrating that our approach significantly outperforms
the state-of-the-arts in searching both metric distance
neighbors and semantically similar neighbors, with
accuracy gains ranging from 13% to 46%.</p>
</td>
    <td>
      
        Scalability 
      
        CVPR 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2012</td>
    <td>
      <a href="/publications/petrovic2012using/">Using paraphrases for improving first story detection in news and Twitter</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Using paraphrases for improving first story detection in news and Twitter' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Using paraphrases for improving first story detection in news and Twitter' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Petrovic S., Osborne, Lavrenko</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>91</td>
    <td><p>First story detection (FSD) involves identifying
first stories about events from a continuous
stream of documents. A major problem in this
task is the high degree of lexical variation in
documents which makes it very difficult to detect
stories that talk about the same event but
expressed using different words. We suggest
using paraphrases to alleviate this problem,
making this the first work to use paraphrases
for FSD. We show a novel way of integrating
paraphrases with locality sensitive hashing
(LSH) in order to obtain an efficient FSD system
that can scale to very large datasets. Our
system achieves state-of-the-art results on the
first story detection task, beating both the best
supervised and unsupervised systems. To test
our approach on large data, we construct a corpus
of events for Twitter, consisting of 50 million
documents, and show that paraphrasing is
also beneficial in this domain.</p>
</td>
    <td>
      
        Datasets 
      
        Locality-Sensitive-Hashing 
      
        Supervised 
      
        Hashing-Methods 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2012</td>
    <td>
      <a href="/publications/liu2012supervised/">Supervised Hashing with Kernels</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Supervised Hashing with Kernels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Supervised Hashing with Kernels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu W., Wang, Ji, Jiang, Chang</td> <!-- 🔧 You were missing this -->
    <td>2012 IEEE Conference on Computer Vision and Pattern Recognition</td>
    <td>1447</td>
    <td><p>Recent years have witnessed the growing popularity of
hashing in large-scale vision problems. It has been shown
that the hashing quality could be boosted by leveraging supervised
information into hash function learning. However,
the existing supervised methods either lack adequate performance
or often incur cumbersome model training. In this
paper, we propose a novel kernel-based supervised hashing
model which requires a limited amount of supervised information,
i.e., similar and dissimilar data pairs, and a feasible
training cost in achieving high quality hashing. The idea
is to map the data to compact binary codes whose Hamming
distances are minimized on similar pairs and simultaneously
maximized on dissimilar pairs. Our approach is
distinct from prior works by utilizing the equivalence between
optimizing the code inner products and the Hamming
distances. This enables us to sequentially and efficiently
train the hash functions one bit at a time, yielding very
short yet discriminative codes. We carry out extensive experiments
on two image benchmarks with up to one million
samples, demonstrating that our approach significantly outperforms
the state-of-the-arts in searching both metric distance
neighbors and semantically similar neighbors, with
accuracy gains ranging from 13% to 46%.</p>
</td>
    <td>
      
        Scalability 
      
        CVPR 
      
        Neural-Hashing 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2012</td>
    <td>
      <a href="/publications/grauman2012learning/">Learning Binary Hash Codes for Large-Scale Image Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Binary Hash Codes for Large-Scale Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Binary Hash Codes for Large-Scale Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Grauman Kristen, Fergus</td> <!-- 🔧 You were missing this -->
    <td>Studies in Computational Intelligence</td>
    <td>102</td>
    <td><p>Algorithms to rapidly search massive image or video collections are critical for many vision applications, including visual search, content-based retrieval, and non-parametric models for object recognition. Recent work shows that learned binary projections are a powerful way to index large collections according to their content. The basic idea is to formulate the projections so as to approximately preserve a given similarity function of interest. Having done so, one can then search the data efficiently using hash tables, or by exploring the Hamming ball volume around a novel query. Both enable sub-linear time retrieval with respect to the database size. Further, depending on the design of the projections, in some cases it is possible to bound the number of database examples that must be searched in order to achieve a given level of accuracy.</p>

<p>This chapter overviews data structures for fast search with binary codes, and then describes several supervised and unsupervised strategies for generating the codes. In particular, we review supervised methods that integrate metric learning, boosting, and neural networks into the hash key construction, and unsupervised methods based on spectral analysis or kernelized random projections that compute affinity-preserving binary codes.Whether learning from explicit semantic supervision or exploiting the structure among unlabeled data, these methods make scalable retrieval possible for a variety of robust visual similarity measures.We focus on defining the algorithms, and illustrate the main points with results using millions of images.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Distance-Metric-Learning 
      
        Survey-Paper 
      
        Large-Scale-Search 
      
        Locality-Sensitive-Hashing 
      
        Supervised 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2012</td>
    <td>
      <a href="/publications/kong2025isotropic/">Isotropic Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Isotropic Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Isotropic Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kong W., Li</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>260</td>
    <td><p>Most existing hashing methods adopt some projection functions to project the original data into several dimensions of real values, and then each of these projected dimensions is quantized into one bit (zero or one) by thresholding. Typically, the variances of different projected dimensions are different for existing projection functions such as principal component analysis (PCA). Using the same number of bits for different projected dimensions is unreasonable because larger-variance dimensions will carry more information. Although this viewpoint has been widely accepted by many researchers, it is still not verified by either theory or experiment because no methods have been proposed to find a projection with equal variances for different dimensions. In this paper, we propose a novel method, called isotropic hashing (IsoHash), to learn projection functions which can produce projected dimensions with isotropic variances (equal variances). Experimental results on real data sets show that IsoHash can outperform its counterpart with different variances for different dimensions, which verifies the viewpoint that projections with isotropic variances will be better than those with anisotropic variances.</p>
</td>
    <td>
      
        Hashing-Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2012</td>
    <td>
      <a href="/publications/kong2012manhattan/">Manhattan Hashing for Large-Scale Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Manhattan Hashing for Large-Scale Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Manhattan Hashing for Large-Scale Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kong W., Li, Guo</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval</td>
    <td>105</td>
    <td><p>Hashing is used to learn binary-code representation for data with
expectation of preserving the neighborhood structure in the original
feature space. Due to its fast query speed and reduced storage
cost, hashing has been widely used for efficient nearest neighbor
search in a large variety of applications like text and image retrieval.
Most existing hashing methods adopt Hamming distance to
measure the similarity (neighborhood) between points in the hashcode
space. However, one problem with Hamming distance is that
it may destroy the neighborhood structure in the original feature
space, which violates the essential goal of hashing. In this paper,
Manhattan hashing (MH), which is based on Manhattan distance, is
proposed to solve the problem of Hamming distance based hashing.
The basic idea of MH is to encode each projected dimension with
multiple bits of natural binary code (NBC), based on which the
Manhattan distance between points in the hashcode space is calculated
for nearest neighbor search. MH can effectively preserve the
neighborhood structure in the data to achieve the goal of hashing.
To the best of our knowledge, this is the first work to adopt Manhattan
distance with NBC for hashing. Experiments on several largescale
image data sets containing up to one million points show that
our MH method can significantly outperform other state-of-the-art
methods.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        SIGIR 
      
        Compact-Codes 
      
        Hashing-Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2012</td>
    <td>
      <a href="/publications/weiss2012multidimensional/">Multidimensional Spectral Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multidimensional Spectral Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multidimensional Spectral Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Weiss Y., Fergus, Torralba</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>162</td>
    <td><p>en a surge of interest in methods based on “semantic hashing”,
i.e. compact binary codes of data-points so that the Hamming distance
between codewords correlates with similarity. In reviewing and
comparing existing methods, we show that their relative performance can
change drastically depending on the definition of ground-truth neighbors.
Motivated by this finding, we propose a new formulation for learning binary
codes which seeks to reconstruct the affinity between datapoints,
rather than their distances. We show that this criterion is intractable
to solve exactly, but a spectral relaxation gives an algorithm where the
bits correspond to thresholded eigenvectors of the affinity matrix, and
as the number of datapoints goes to infinity these eigenvectors converge
to eigenfunctions of Laplace-Beltrami operators, similar to the recently
proposed Spectral Hashing (SH) method. Unlike SH whose performance
may degrade as the number of bits increases, the optimal code using
our formulation is guaranteed to faithfully reproduce the affinities as
the number of bits increases. We show that the number of eigenfunctions
needed may increase exponentially with dimension, but introduce a “kernel
trick” to allow us to compute with an exponentially large number of
bits but using only memory and computation that grows linearly with
dimension. Experiments shows that MDSH outperforms the state-of-the
art, especially in the challenging regime of small distance thresholds.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Evaluation 
      
        Compact-Codes 
      
        Text-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2012</td>
    <td>
      <a href="/publications/kong2025manhattan/">Manhattan Hashing for Large-Scale Image Retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Manhattan Hashing for Large-Scale Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Manhattan Hashing for Large-Scale Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kong W., Li, Guo</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval</td>
    <td>105</td>
    <td><p>Hashing is used to learn binary-code representation for data with
expectation of preserving the neighborhood structure in the original
feature space. Due to its fast query speed and reduced storage
cost, hashing has been widely used for efficient nearest neighbor
search in a large variety of applications like text and image retrieval.
Most existing hashing methods adopt Hamming distance to
measure the similarity (neighborhood) between points in the hashcode
space. However, one problem with Hamming distance is that
it may destroy the neighborhood structure in the original feature
space, which violates the essential goal of hashing. In this paper,
Manhattan hashing (MH), which is based on Manhattan distance, is
proposed to solve the problem of Hamming distance based hashing.
The basic idea of MH is to encode each projected dimension with
multiple bits of natural binary code (NBC), based on which the
Manhattan distance between points in the hashcode space is calculated
for nearest neighbor search. MH can effectively preserve the
neighborhood structure in the data to achieve the goal of hashing.
To the best of our knowledge, this is the first work to adopt Manhattan
distance with NBC for hashing. Experiments on several largescale
image data sets containing up to one million points show that
our MH method can significantly outperform other state-of-the-art
methods.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        SIGIR 
      
        Compact-Codes 
      
        Hashing-Methods 
      
    </td>
    </tr>      
    
     <tr>
  <td>2012</td>
    <td>
      <a href="/publications/petrovic2025using/">Using paraphrases for improving first story detection in news and Twitter</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Using paraphrases for improving first story detection in news and Twitter' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Using paraphrases for improving first story detection in news and Twitter' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Petrovic S., Osborne, Lavrenko</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>91</td>
    <td><p>First story detection (FSD) involves identifying
first stories about events from a continuous
stream of documents. A major problem in this
task is the high degree of lexical variation in
documents which makes it very difficult to detect
stories that talk about the same event but
expressed using different words. We suggest
using paraphrases to alleviate this problem,
making this the first work to use paraphrases
for FSD. We show a novel way of integrating
paraphrases with locality sensitive hashing
(LSH) in order to obtain an efficient FSD system
that can scale to very large datasets. Our
system achieves state-of-the-art results on the
first story detection task, beating both the best
supervised and unsupervised systems. To test
our approach on large data, we construct a corpus
of events for Twitter, consisting of 50 million
documents, and show that paraphrasing is
also beneficial in this domain.</p>
</td>
    <td>
      
        Datasets 
      
        Locality-Sensitive-Hashing 
      
        Supervised 
      
        Hashing-Methods 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2012</td>
    <td>
      <a href="/publications/zhen2012co/">Co-Regularized Hashing for Multimodal Data</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Co-Regularized Hashing for Multimodal Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Co-Regularized Hashing for Multimodal Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhen Y., Yeung</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>195</td>
    <td><p>Hashing-based methods provide a very promising approach to large-scale similarity
search. To obtain compact hash codes, a recent trend seeks to learn the hash
functions from data automatically. In this paper, we study hash function learning
in the context of multimodal data. We propose a novel multimodal hash function
learning method, called Co-Regularized Hashing (CRH), based on a boosted coregularization
framework. The hash functions for each bit of the hash codes are
learned by solving DC (difference of convex functions) programs, while the learning
for multiple bits proceeds via a boosting procedure so that the bias introduced
by the hash functions can be sequentially minimized. We empirically compare
CRH with two state-of-the-art multimodal hash function learning methods on two
publicly available data sets.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Scalability 
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2012</td>
    <td>
      <a href="/publications/heo2012spherical/">Spherical Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Spherical Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Spherical Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Heo J., Lee, He, Chang, Yoon</td> <!-- 🔧 You were missing this -->
    <td>2012 IEEE Conference on Computer Vision and Pattern Recognition</td>
    <td>380</td>
    <td><p>Many binary code encoding schemes based on hashing
have been actively studied recently, since they can provide
efficient similarity search, especially nearest neighbor
search, and compact data representations suitable for handling
large scale image databases in many computer vision
problems. Existing hashing techniques encode highdimensional
data points by using hyperplane-based hashing
functions. In this paper we propose a novel hyperspherebased
hashing function, spherical hashing, to map more
spatially coherent data points into a binary code compared
to hyperplane-based hashing functions. Furthermore, we
propose a new binary code distance function, spherical
Hamming distance, that is tailored to our hyperspherebased
binary coding scheme, and design an efficient iterative
optimization process to achieve balanced partitioning
of data points for each hash function and independence between
hashing functions. Our extensive experiments show
that our spherical hashing technique significantly outperforms
six state-of-the-art hashing techniques based on hyperplanes
across various image benchmarks of sizes ranging
from one to 75 million of GIST descriptors. The performance
gains are consistent and large, up to 100% improvements.
The excellent results confirm the unique merits of
the proposed idea in using hyperspheres to encode proximity
regions in high-dimensional spaces. Finally, our method
is intuitive and easy to implement.</p>
</td>
    <td>
      
        CVPR 
      
        Compact-Codes 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2012</td>
    <td>
      <a href="/publications/weiss2025multidimensional/">Multidimensional Spectral Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Multidimensional Spectral Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Multidimensional Spectral Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Weiss Y., Fergus, Torralba</td> <!-- 🔧 You were missing this -->
    <td>Lecture Notes in Computer Science</td>
    <td>162</td>
    <td><p>en a surge of interest in methods based on “semantic hashing”,
i.e. compact binary codes of data-points so that the Hamming distance
between codewords correlates with similarity. In reviewing and
comparing existing methods, we show that their relative performance can
change drastically depending on the definition of ground-truth neighbors.
Motivated by this finding, we propose a new formulation for learning binary
codes which seeks to reconstruct the affinity between datapoints,
rather than their distances. We show that this criterion is intractable
to solve exactly, but a spectral relaxation gives an algorithm where the
bits correspond to thresholded eigenvectors of the affinity matrix, and
as the number of datapoints goes to infinity these eigenvectors converge
to eigenfunctions of Laplace-Beltrami operators, similar to the recently
proposed Spectral Hashing (SH) method. Unlike SH whose performance
may degrade as the number of bits increases, the optimal code using
our formulation is guaranteed to faithfully reproduce the affinities as
the number of bits increases. We show that the number of eigenfunctions
needed may increase exponentially with dimension, but introduce a “kernel
trick” to allow us to compute with an exponentially large number of
bits but using only memory and computation that grows linearly with
dimension. Experiments shows that MDSH outperforms the state-of-the
art, especially in the challenging regime of small distance thresholds.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Evaluation 
      
        Compact-Codes 
      
        Text-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2012</td>
    <td>
      <a href="/publications/grauman2025learning/">Learning Binary Hash Codes for Large-Scale Image Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Binary Hash Codes for Large-Scale Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Binary Hash Codes for Large-Scale Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Grauman Kristen, Fergus</td> <!-- 🔧 You were missing this -->
    <td>Studies in Computational Intelligence</td>
    <td>102</td>
    <td><p>Algorithms to rapidly search massive image or video collections are critical for many vision applications, including visual search, content-based retrieval, and non-parametric models for object recognition. Recent work shows that learned binary projections are a powerful way to index large collections according to their content. The basic idea is to formulate the projections so as to approximately preserve a given similarity function of interest. Having done so, one can then search the data efficiently using hash tables, or by exploring the Hamming ball volume around a novel query. Both enable sub-linear time retrieval with respect to the database size. Further, depending on the design of the projections, in some cases it is possible to bound the number of database examples that must be searched in order to achieve a given level of accuracy.</p>

<p>This chapter overviews data structures for fast search with binary codes, and then describes several supervised and unsupervised strategies for generating the codes. In particular, we review supervised methods that integrate metric learning, boosting, and neural networks into the hash key construction, and unsupervised methods based on spectral analysis or kernelized random projections that compute affinity-preserving binary codes.Whether learning from explicit semantic supervision or exploiting the structure among unlabeled data, these methods make scalable retrieval possible for a variety of robust visual similarity measures.We focus on defining the algorithms, and illustrate the main points with results using millions of images.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Distance-Metric-Learning 
      
        Survey-Paper 
      
        Large-Scale-Search 
      
        Locality-Sensitive-Hashing 
      
        Supervised 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Unsupervised 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2011</td>
    <td>
      <a href="/publications/kumar2011learning/">Learning hash functions for cross-view similarity search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning hash functions for cross-view similarity search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning hash functions for cross-view similarity search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kumar S., Udupa</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>438</td>
    <td><p>Many applications in Multilingual and Multimodal
Information Access involve searching large
databases of high dimensional data objects with
multiple (conditionally independent) views. In this
work we consider the problem of learning hash
functions for similarity search across the views
for such applications. We propose a principled
method for learning a hash function for each view
given a set of multiview training data objects. The
hash functions map similar objects to similar codes
across the views thus enabling cross-view similarity
search. We present results from an extensive
empirical study of the proposed approach
which demonstrate its effectiveness on Japanese
language People Search and Multilingual People
Search problems.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Evaluation 
      
        Similarity-Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2011</td>
    <td>
      <a href="/publications/mcfee2010learning/">Learning Multi-modal Similarity</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning Multi-modal Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning Multi-modal Similarity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Mcfee Brian, Lanckriet Gert</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>122</td>
    <td><p>In many applications involving multi-media data, the definition of similarity
between items is integral to several key tasks, e.g., nearest-neighbor
retrieval, classification, and recommendation. Data in such regimes typically
exhibits multiple modalities, such as acoustic and visual content of video.
Integrating such heterogeneous data to form a holistic similarity space is
therefore a key challenge to be overcome in many real-world applications.
  We present a novel multiple kernel learning technique for integrating
heterogeneous data into a single, unified similarity space. Our algorithm
learns an optimal ensemble of kernel transfor- mations which conform to
measurements of human perceptual similarity, as expressed by relative
comparisons. To cope with the ubiquitous problems of subjectivity and
inconsistency in multi- media similarity, we develop graph-based techniques to
filter similarity measurements, resulting in a simplified and robust training
procedure.</p>
</td>
    <td>
      
        Recommender-Systems 
      
        Graph-Based-ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2011</td>
    <td>
      <a href="/publications/liu2011hashing/">Hashing with Graphs</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing with Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing with Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu W., Wang, Kumar, Chang</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>861</td>
    <td><p>Hashing is becoming increasingly popular for
efficient nearest neighbor search in massive
databases. However, learning short codes
that yield good search performance is still
a challenge. Moreover, in many cases realworld
data lives on a low-dimensional manifold,
which should be taken into account
to capture meaningful nearest neighbors. In
this paper, we propose a novel graph-based
hashing method which automatically discovers
the neighborhood structure inherent in
the data to learn appropriate compact codes.
To make such an approach computationally
feasible, we utilize Anchor Graphs to obtain
tractable low-rank adjacency matrices. Our
formulation allows constant time hashing of a
new data point by extrapolating graph Laplacian
eigenvectors to eigenfunctions. Finally,
we describe a hierarchical threshold learning
procedure in which each eigenfunction yields
multiple bits, leading to higher search accuracy.
Experimental comparison with the
other state-of-the-art methods on two large
datasets demonstrates the efficacy of the proposed
method.</p>
</td>
    <td>
      
        Datasets 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Graph-Based-ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2011</td>
    <td>
      <a href="/publications/joly2025random/">Random Maximum Margin Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Random Maximum Margin Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Random Maximum Margin Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Joly A., Buisson</td> <!-- 🔧 You were missing this -->
    <td>CVPR 2011</td>
    <td>151</td>
    <td><p>Following the success of hashing methods for multidimensional
indexing, more and more works are interested
in embedding visual feature space in compact hash codes.
Such approaches are not an alternative to using index structures
but a complementary way to reduce both the memory
usage and the distance computation cost. Several data
dependent hash functions have notably been proposed to
closely fit data distribution and provide better selectivity
than usual random projections such as LSH. However, improvements
occur only for relatively small hash code sizes
up to 64 or 128 bits. As discussed in the paper, this is mainly
due to the lack of independence between the produced hash
functions. We introduce a new hash function family that
attempts to solve this issue in any kernel space. Rather
than boosting the collision probability of close points, our
method focus on data scattering. By training purely random
splits of the data, regardless the closeness of the training
samples, it is indeed possible to generate consistently
more independent hash functions. On the other side, the
use of large margin classifiers allows to maintain good generalization
performances. Experiments show that our new
Random Maximum Margin Hashing scheme (RMMH) outperforms
four state-of-the-art hashing methods, notably in
kernel spaces.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Locality-Sensitive-Hashing 
      
        CVPR 
      
        Vector-Indexing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2011</td>
    <td>
      <a href="/publications/zhang2011composite/">Composite Hashing with Multiple Information Sources</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Composite Hashing with Multiple Information Sources' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Composite Hashing with Multiple Information Sources' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang D., Wang, Si</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval</td>
    <td>221</td>
    <td><p>Similarity search applications with a large amount of text
and image data demands an efficient and effective solution.
One useful strategy is to represent the examples in databases
as compact binary codes through semantic hashing, which
has attracted much attention due to its fast query/search
speed and drastically reduced storage requirement. All of
the current semantic hashing methods only deal with the
case when each example is represented by one type of features.
However, examples are often described from several
different information sources in many real world applications.
For example, the characteristics of a webpage can be
derived from both its content part and its associated links.
To address the problem of learning good hashing codes in
this scenario, we propose a novel research problem – Composite
Hashing with Multiple Information Sources (CHMIS).
The focus of the new research problem is to design an algorithm
for incorporating the features from different information
sources into the binary hashing codes efficiently and
effectively. In particular, we propose an algorithm CHMISAW
(CHMIS with Adjusted Weights) for learning the codes.
The proposed algorithm integrates information from several
different sources into the binary hashing codes by adjusting
the weights on each individual source for maximizing
the coding performance, and enables fast conversion from
query examples to their binary hashing codes. Experimental
results on five different datasets demonstrate the superior
performance of the proposed method against several other
state-of-the-art semantic hashing techniques.</p>
</td>
    <td>
      
        Datasets 
      
        Text-Retrieval 
      
        SIGIR 
      
        Compact-Codes 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2011</td>
    <td>
      <a href="/publications/kumar2025learning/">Learning hash functions for cross-view similarity search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning hash functions for cross-view similarity search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning hash functions for cross-view similarity search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kumar S., Udupa</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>438</td>
    <td><p>Many applications in Multilingual and Multimodal
Information Access involve searching large
databases of high dimensional data objects with
multiple (conditionally independent) views. In this
work we consider the problem of learning hash
functions for similarity search across the views
for such applications. We propose a principled
method for learning a hash function for each view
given a set of multiview training data objects. The
hash functions map similar objects to similar codes
across the views thus enabling cross-view similarity
search. We present results from an extensive
empirical study of the proposed approach
which demonstrate its effectiveness on Japanese
language People Search and Multilingual People
Search problems.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Evaluation 
      
        Similarity-Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2011</td>
    <td>
      <a href="/publications/liu2025hashing/">Hashing with Graphs</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing with Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing with Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Liu W., Wang, Kumar, Chang</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>861</td>
    <td><p>Hashing is becoming increasingly popular for
efficient nearest neighbor search in massive
databases. However, learning short codes
that yield good search performance is still
a challenge. Moreover, in many cases realworld
data lives on a low-dimensional manifold,
which should be taken into account
to capture meaningful nearest neighbors. In
this paper, we propose a novel graph-based
hashing method which automatically discovers
the neighborhood structure inherent in
the data to learn appropriate compact codes.
To make such an approach computationally
feasible, we utilize Anchor Graphs to obtain
tractable low-rank adjacency matrices. Our
formulation allows constant time hashing of a
new data point by extrapolating graph Laplacian
eigenvectors to eigenfunctions. Finally,
we describe a hierarchical threshold learning
procedure in which each eigenfunction yields
multiple bits, leading to higher search accuracy.
Experimental comparison with the
other state-of-the-art methods on two large
datasets demonstrates the efficacy of the proposed
method.</p>
</td>
    <td>
      
        Datasets 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Graph-Based-ANN 
      
    </td>
    </tr>      
    
     <tr>
  <td>2011</td>
    <td>
      <a href="/publications/joly2011random/">Random Maximum Margin Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Random Maximum Margin Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Random Maximum Margin Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Joly A., Buisson</td> <!-- 🔧 You were missing this -->
    <td>CVPR 2011</td>
    <td>151</td>
    <td><p>Following the success of hashing methods for multidimensional
indexing, more and more works are interested
in embedding visual feature space in compact hash codes.
Such approaches are not an alternative to using index structures
but a complementary way to reduce both the memory
usage and the distance computation cost. Several data
dependent hash functions have notably been proposed to
closely fit data distribution and provide better selectivity
than usual random projections such as LSH. However, improvements
occur only for relatively small hash code sizes
up to 64 or 128 bits. As discussed in the paper, this is mainly
due to the lack of independence between the produced hash
functions. We introduce a new hash function family that
attempts to solve this issue in any kernel space. Rather
than boosting the collision probability of close points, our
method focus on data scattering. By training purely random
splits of the data, regardless the closeness of the training
samples, it is indeed possible to generate consistently
more independent hash functions. On the other side, the
use of large margin classifiers allows to maintain good generalization
performances. Experiments show that our new
Random Maximum Margin Hashing scheme (RMMH) outperforms
four state-of-the-art hashing methods, notably in
kernel spaces.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Locality-Sensitive-Hashing 
      
        CVPR 
      
        Vector-Indexing 
      
    </td>
    </tr>      
    
     <tr>
  <td>2011</td>
    <td>
      <a href="/publications/zhang2025composite/">Composite Hashing with Multiple Information Sources</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Composite Hashing with Multiple Information Sources' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Composite Hashing with Multiple Information Sources' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang D., Wang, Si</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval</td>
    <td>221</td>
    <td><p>Similarity search applications with a large amount of text
and image data demands an efficient and effective solution.
One useful strategy is to represent the examples in databases
as compact binary codes through semantic hashing, which
has attracted much attention due to its fast query/search
speed and drastically reduced storage requirement. All of
the current semantic hashing methods only deal with the
case when each example is represented by one type of features.
However, examples are often described from several
different information sources in many real world applications.
For example, the characteristics of a webpage can be
derived from both its content part and its associated links.
To address the problem of learning good hashing codes in
this scenario, we propose a novel research problem – Composite
Hashing with Multiple Information Sources (CHMIS).
The focus of the new research problem is to design an algorithm
for incorporating the features from different information
sources into the binary hashing codes efficiently and
effectively. In particular, we propose an algorithm CHMISAW
(CHMIS with Adjusted Weights) for learning the codes.
The proposed algorithm integrates information from several
different sources into the binary hashing codes by adjusting
the weights on each individual source for maximizing
the coding performance, and enables fast conversion from
query examples to their binary hashing codes. Experimental
results on five different datasets demonstrate the superior
performance of the proposed method against several other
state-of-the-art semantic hashing techniques.</p>
</td>
    <td>
      
        Datasets 
      
        Text-Retrieval 
      
        SIGIR 
      
        Compact-Codes 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2010</td>
    <td>
      <a href="/publications/wang2010semi/">Semi-supervised Deep Quantization for Cross-modal Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semi-supervised Deep Quantization for Cross-modal Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semi-supervised Deep Quantization for Cross-modal Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang Xin, Zhu, Liu</td> <!-- 🔧 You were missing this -->
    <td>2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</td>
    <td>626</td>
    <td><p>The problem of cross-modal similarity search, which aims at making efficient and accurate queries across multiple domains, has become a significant and important research topic. Composite quantization, a compact coding solution superior to hashing techniques, has shown its effectiveness for similarity search. However, most existing works utilizing composite quantization to search multi-domain content only consider either pairwise similarity information or class label information across different domains, which fails to tackle the semi-supervised problem in composite quantization. In this paper, we address the semi-supervised quantization problem by considering: (i) pairwise similarity information (without class label information) across different domains, which captures the intra-document relation, (ii) cross-domain data with class label which can help capture inter-document relation, and (iii) cross-domain data with neither pairwise similarity nor class label which enables the full use of abundant unlabelled information. To the best of our knowledge, we are the first to consider both supervised information (pairwise similarity + class label) and unsupervised information (neither pairwise similarity nor class label) simultaneously in composite quantization. A challenging problem arises: how can we jointly handle these three sorts of information across multiple domains in an efficient way? To tackle this challenge, we propose a novel semi-supervised deep quantization (SSDQ) model that takes both supervised and unsupervised information into account. The proposed SSDQ model is capable of incorporating the above three kinds of information into one single framework when utilizing composite quantization for accurate and efficient queries across different domains. More specifically, we employ a modified deep autoencoder for better latent representation and formulate pairwise similarity loss, supervised quantization loss as well as unsupervised distribution match loss to handle all three types of information. The extensive experiments demonstrate the significant improvement of SSDQ over several state-of-the-art methods on various datasets.</p>
</td>
    <td>
      
        Datasets 
      
        CVPR 
      
        Tools-&-Libraries 
      
        Supervised 
      
        Similarity-Search 
      
        Quantization 
      
        Hashing-Methods 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2010</td>
    <td>
      <a href="/publications/wang2010sequential/">Sequential projection learning for hashing with compact codes</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Sequential projection learning for hashing with compact codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Sequential projection learning for hashing with compact codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang J., Kumar, Chang</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>328</td>
    <td><p>Hashing based Approximate Nearest Neighbor
(ANN) search has attracted much attention
due to its fast query time and drastically
reduced storage. However, most of the hashing
methods either use random projections or
extract principal directions from the data to
derive hash functions. The resulting embedding
suffers from poor discrimination when
compact codes are used. In this paper, we
propose a novel data-dependent projection
learning method such that each hash function
is designed to correct the errors made by
the previous one sequentially. The proposed
method easily adapts to both unsupervised
and semi-supervised scenarios and shows significant
performance gains over the state-ofthe-art
methods on two large datasets containing
up to 1 million points.</p>
</td>
    <td>
      
        Efficiency 
      
        Unsupervised 
      
        Datasets 
      
        Locality-Sensitive-Hashing 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2010</td>
    <td>
      <a href="/publications/kato2009solving/">Solving $k$-Nearest Neighbor Problem on Multiple Graphics Processors</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Solving $k$-Nearest Neighbor Problem on Multiple Graphics Processors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Solving $k$-Nearest Neighbor Problem on Multiple Graphics Processors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kato Kimikazu, Hosino Tikara</td> <!-- 🔧 You were missing this -->
    <td>2010 10th IEEE/ACM International Conference on Cluster, Cloud and Grid Computing</td>
    <td>37</td>
    <td><p>The recommendation system is a software system to predict customers’ unknown
preferences from known preferences. In the recommendation system, customers’
preferences are encoded into vectors, and finding the nearest vectors to each
vector is an essential part. This vector-searching part of the problem is
called a \(k\)-nearest neighbor problem. We give an effective algorithm to solve
this problem on multiple graphics processor units (GPUs).
  Our algorithm consists of two parts: an \(N\)-body problem and a partial sort.
For a algorithm of the \(N\)-body problem, we applied the idea of a known
algorithm for the \(N\)-body problem in physics, although another trick is need
to overcome the problem of small sized shared memory. For the partial sort, we
give a novel GPU algorithm which is effective for small \(k\). In our partial
sort algorithm, a heap is accessed in parallel by threads with a low cost of
synchronization. Both of these two parts of our algorithm utilize maximal power
of coalesced memory access, so that a full bandwidth is achieved.
  By an experiment, we show that when the size of the problem is large, an
implementation of the algorithm on two GPUs runs more than 330 times faster
than a single core implementation on a latest CPU. We also show that our
algorithm scales well with respect to the number of GPUs.</p>
</td>
    <td>
      
        Recommender-Systems 
      
    </td>
    </tr>      
    
     <tr>
  <td>2010</td>
    <td>
      <a href="/publications/jain2025hashing/">Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jain P., Vijayanarasimhan, Grauman</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>69</td>
    <td><p>We consider the problem of retrieving the database points nearest to a given hyperplane query without exhaustively scanning the 
database. We propose two hashing-based solutions. Our first approach maps the data to two-bit binary keys that are locality-sensitive for the angle between the hyperplane normal and a database point. Our second approach embeds the data into a vector space where the Euclidean norm reflects the desired distance between the original points and hyperplane query. Both use hashing to retrieve near points in sub-linear time. Our first method’s preprocessing stage is more efficient, while the second has stronger accuracy guarantees. We apply both to pool-based active learning: taking the current hyperplane classifier as a query, our algorithm identifies those points (approximately) satisfying the well-known minimal distance-to-hyperplane selection criterion. We empirically demonstrate our methods’ tradeoffs, and show that they make it practical to perform active selection with millions 
of unlabeled points.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Scalability 
      
    </td>
    </tr>      
    
     <tr>
  <td>2010</td>
    <td>
      <a href="/publications/petrovic2025streaming/">Streaming First Story Detection with application to Twitter</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Streaming First Story Detection with application to Twitter' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Streaming First Story Detection with application to Twitter' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Petrovic S., Osborne, Lavrenko</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>565</td>
    <td><p>With the recent rise in popularity and size of
social media, there is a growing need for systems
that can extract useful information from
this amount of data. We address the problem
of detecting new events from a stream of
Twitter posts. To make event detection feasible
on web-scale corpora, we present an algorithm
based on locality-sensitive hashing which
is able overcome the limitations of traditional
approaches, while maintaining competitive results.
In particular, a comparison with a stateof-the-art
system on the first story detection
task shows that we achieve over an order of
magnitude speedup in processing time, while
retaining comparable performance. Event detection
experiments on a collection of 160 million
Twitter posts show that celebrity deaths
are the fastest spreading news on Twitter.</p>
</td>
    <td>
      
        Scalability 
      
        Efficiency 
      
        Large-Scale-Search 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2010</td>
    <td>
      <a href="/publications/petrovic2010streaming/">Streaming First Story Detection with application to Twitter</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Streaming First Story Detection with application to Twitter' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Streaming First Story Detection with application to Twitter' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Petrovic S., Osborne, Lavrenko</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>565</td>
    <td><p>With the recent rise in popularity and size of
social media, there is a growing need for systems
that can extract useful information from
this amount of data. We address the problem
of detecting new events from a stream of
Twitter posts. To make event detection feasible
on web-scale corpora, we present an algorithm
based on locality-sensitive hashing which
is able overcome the limitations of traditional
approaches, while maintaining competitive results.
In particular, a comparison with a stateof-the-art
system on the first story detection
task shows that we achieve over an order of
magnitude speedup in processing time, while
retaining comparable performance. Event detection
experiments on a collection of 160 million
Twitter posts show that celebrity deaths
are the fastest spreading news on Twitter.</p>
</td>
    <td>
      
        Scalability 
      
        Efficiency 
      
        Large-Scale-Search 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2010</td>
    <td>
      <a href="/publications/wang2025sequential/">Sequential projection learning for hashing with compact codes</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Sequential projection learning for hashing with compact codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Sequential projection learning for hashing with compact codes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang J., Kumar, Chang</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>328</td>
    <td><p>Hashing based Approximate Nearest Neighbor
(ANN) search has attracted much attention
due to its fast query time and drastically
reduced storage. However, most of the hashing
methods either use random projections or
extract principal directions from the data to
derive hash functions. The resulting embedding
suffers from poor discrimination when
compact codes are used. In this paper, we
propose a novel data-dependent projection
learning method such that each hash function
is designed to correct the errors made by
the previous one sequentially. The proposed
method easily adapts to both unsupervised
and semi-supervised scenarios and shows significant
performance gains over the state-ofthe-art
methods on two large datasets containing
up to 1 million points.</p>
</td>
    <td>
      
        Efficiency 
      
        Unsupervised 
      
        Datasets 
      
        Locality-Sensitive-Hashing 
      
        Compact-Codes 
      
        Hashing-Methods 
      
        Evaluation 
      
        Supervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2010</td>
    <td>
      <a href="/publications/jain2010hashing/">Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jain P., Vijayanarasimhan, Grauman</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>69</td>
    <td><p>We consider the problem of retrieving the database points nearest to a given hyperplane query without exhaustively scanning the 
database. We propose two hashing-based solutions. Our first approach maps the data to two-bit binary keys that are locality-sensitive for the angle between the hyperplane normal and a database point. Our second approach embeds the data into a vector space where the Euclidean norm reflects the desired distance between the original points and hyperplane query. Both use hashing to retrieve near points in sub-linear time. Our first method’s preprocessing stage is more efficient, while the second has stronger accuracy guarantees. We apply both to pool-based active learning: taking the current hyperplane classifier as a query, our algorithm identifies those points (approximately) satisfying the well-known minimal distance-to-hyperplane selection criterion. We empirically demonstrate our methods’ tradeoffs, and show that they make it practical to perform active selection with millions 
of unlabeled points.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Scalability 
      
    </td>
    </tr>      
    
     <tr>
  <td>2010</td>
    <td>
      <a href="/publications/wang2025semi/">Semi-supervised hashing for scalable image retrieval</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semi-supervised hashing for scalable image retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semi-supervised hashing for scalable image retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Wang J., Kumar, Chang</td> <!-- 🔧 You were missing this -->
    <td>2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</td>
    <td>626</td>
    <td><p>Large scale image search has recently attracted considerable
attention due to easy availability of huge amounts of
data. Several hashing methods have been proposed to allow
approximate but highly efficient search. Unsupervised
hashing methods show good performance with metric distances
but, in image search, semantic similarity is usually
given in terms of labeled pairs of images. There exist supervised
hashing methods that can handle such semantic similarity
but they are prone to overfitting when labeled data
is small or noisy. Moreover, these methods are usually very
slow to train. In this work, we propose a semi-supervised
hashing method that is formulated as minimizing empirical
error on the labeled data while maximizing variance
and independence of hash bits over the labeled and unlabeled
data. The proposed method can handle both metric as
well as semantic similarity. The experimental results on two
large datasets (up to one million samples) demonstrate its
superior performance over state-of-the-art supervised and
unsupervised methods.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Datasets 
      
        CVPR 
      
        Neural-Hashing 
      
        Supervised 
      
        Hashing-Methods 
      
        Evaluation 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2010</td>
    <td>
      <a href="/publications/zhang2010self/">Self-Taught Hashing for Fast Similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Self-Taught Hashing for Fast Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Self-Taught Hashing for Fast Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang D., Wang, Cai, Lu</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval</td>
    <td>354</td>
    <td><p>The ability of fast similarity search at large scale is of great
importance to many Information Retrieval (IR) applications.
A promising way to accelerate similarity search is semantic
hashing which designs compact binary codes for a large number
of documents so that semantically similar documents
are mapped to similar codes (within a short Hamming distance).
Although some recently proposed techniques are
able to generate high-quality codes for documents known
in advance, obtaining the codes for previously unseen documents
remains to be a very challenging problem. In this
paper, we emphasise this issue and propose a novel SelfTaught
Hashing (STH) approach to semantic hashing: we
first find the optimal l-bit binary codes for all documents in
the given corpus via unsupervised learning, and then train
l classifiers via supervised learning to predict the l-bit code
for any query document unseen before. Our experiments on
three real-world text datasets show that the proposed approach
using binarised Laplacian Eigenmap (LapEig) and
linear Support Vector Machine (SVM) outperforms stateof-the-art
techniques significantly.</p>
</td>
    <td>
      
        Datasets 
      
        Text-Retrieval 
      
        SIGIR 
      
        Supervised 
      
        Compact-Codes 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Unsupervised 
      
    </td>
    </tr>      
    
     <tr>
  <td>2010</td>
    <td>
      <a href="/publications/zhang2025self/">Self-Taught Hashing for Fast Similarity Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Self-Taught Hashing for Fast Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Self-Taught Hashing for Fast Similarity Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Zhang D., Wang, Cai, Lu</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval</td>
    <td>354</td>
    <td><p>The ability of fast similarity search at large scale is of great
importance to many Information Retrieval (IR) applications.
A promising way to accelerate similarity search is semantic
hashing which designs compact binary codes for a large number
of documents so that semantically similar documents
are mapped to similar codes (within a short Hamming distance).
Although some recently proposed techniques are
able to generate high-quality codes for documents known
in advance, obtaining the codes for previously unseen documents
remains to be a very challenging problem. In this
paper, we emphasise this issue and propose a novel SelfTaught
Hashing (STH) approach to semantic hashing: we
first find the optimal l-bit binary codes for all documents in
the given corpus via unsupervised learning, and then train
l classifiers via supervised learning to predict the l-bit code
for any query document unseen before. Our experiments on
three real-world text datasets show that the proposed approach
using binarised Laplacian Eigenmap (LapEig) and
linear Support Vector Machine (SVM) outperforms stateof-the-art
techniques significantly.</p>
</td>
    <td>
      
        Datasets 
      
        Text-Retrieval 
      
        SIGIR 
      
        Supervised 
      
        Compact-Codes 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Unsupervised 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2009</td>
    <td>
      <a href="/publications/kulis2009learning/">Learning to Hash with Binary Reconstructive Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning to Hash with Binary Reconstructive Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning to Hash with Binary Reconstructive Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kulis B., Darrell</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>841</td>
    <td><p>Fast retrieval methods are increasingly critical for many large-scale analysis tasks, and there have been
several recent methods that attempt to learn hash functions for fast and accurate nearest neighbor searches.
In this paper, we develop an algorithm for learning hash functions based on explicitly minimizing the
reconstruction error between the original distances and the Hamming distances of the corresponding binary
embeddings. We develop a scalable coordinate-descent algorithm for our proposed hashing objective that
is able to efficiently learn hash functions in a variety of settings. Unlike existing methods such as semantic
hashing and spectral hashing, our method is easily kernelized and does not require restrictive assumptions
about the underlying distribution of the data. We present results over several domains to demonstrate that
our method outperforms existing state-of-the-art techniques.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Scalability 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2009</td>
    <td>
      <a href="/publications/uno2009efficient/">Efficient Construction of Neighborhood Graphs by the Multiple Sorting Method</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Construction of Neighborhood Graphs by the Multiple Sorting Method' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Efficient Construction of Neighborhood Graphs by the Multiple Sorting Method' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Uno Takeaki, Sugiyama Masashi, Tsuda Koji</td> <!-- 🔧 You were missing this -->
    <td>Arxiv</td>
    <td>10</td>
    <td><p>Neighborhood graphs are gaining popularity as a concise data representation
in machine learning. However, naive graph construction by pairwise distance
calculation takes \(O(n^2)\) runtime for \(n\) data points and this is
prohibitively slow for millions of data points. For strings of equal length,
the multiple sorting method (Uno, 2008) can construct an \(\epsilon\)-neighbor
graph in \(O(n+m)\) time, where \(m\) is the number of \(\epsilon\)-neighbor pairs in
the data. To introduce this remarkably efficient algorithm to continuous
domains such as images, signals and texts, we employ a random projection method
to convert vectors to strings. Theoretical results are presented to elucidate
the trade-off between approximation quality and computation time. Empirical
results show the efficiency of our method in comparison to fast nearest
neighbor alternatives.</p>
</td>
    <td>
      
        Graph-Based-ANN 
      
        Evaluation 
      
        Locality-Sensitive-Hashing 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2009</td>
    <td>
      <a href="/publications/kulis2025learning/">Learning to Hash with Binary Reconstructive Embeddings</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Learning to Hash with Binary Reconstructive Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Learning to Hash with Binary Reconstructive Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kulis B., Darrell</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>841</td>
    <td><p>Fast retrieval methods are increasingly critical for many large-scale analysis tasks, and there have been
several recent methods that attempt to learn hash functions for fast and accurate nearest neighbor searches.
In this paper, we develop an algorithm for learning hash functions based on explicitly minimizing the
reconstruction error between the original distances and the Hamming distances of the corresponding binary
embeddings. We develop a scalable coordinate-descent algorithm for our proposed hashing objective that
is able to efficiently learn hash functions in a variety of settings. Unlike existing methods such as semantic
hashing and spectral hashing, our method is easily kernelized and does not require restrictive assumptions
about the underlying distribution of the data. We present results over several domains to demonstrate that
our method outperforms existing state-of-the-art techniques.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Scalability 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2009</td>
    <td>
      <a href="/publications/kulis2025kernelized/">Kernelized Locality-Sensitive Hashing for Scalable Image Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Kernelized Locality-Sensitive Hashing for Scalable Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Kernelized Locality-Sensitive Hashing for Scalable Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kulis B., Grauman</td> <!-- 🔧 You were missing this -->
    <td>2009 IEEE 12th International Conference on Computer Vision</td>
    <td>908</td>
    <td><p>Fast retrieval methods are critical for large-scale and
data-driven vision applications. Recent work has explored
ways to embed high-dimensional features or complex distance
functions into a low-dimensional Hamming space
where items can be efficiently searched. However, existing
methods do not apply for high-dimensional kernelized
data when the underlying feature embedding for the kernel
is unknown. We show how to generalize locality-sensitive
hashing to accommodate arbitrary kernel functions, making
it possible to preserve the algorithm’s sub-linear time similarity
search guarantees for a wide class of useful similarity
functions. Since a number of successful image-based kernels
have unknown or incomputable embeddings, this is especially
valuable for image retrieval tasks. We validate our
technique on several large-scale datasets, and show that it
enables accurate and fast performance for example-based
object classification, feature matching, and content-based
retrieval.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Efficiency 
      
        ICCV 
      
        Datasets 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
     <tr>
  <td>2009</td>
    <td>
      <a href="/publications/jain2025fast/">Fast Similarity Search for Learned Metrics</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Similarity Search for Learned Metrics' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Similarity Search for Learned Metrics' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jain P., Kulis, Grauman</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>265</td>
    <td><p>We propose a method to efficiently index into a large database of examples according to a learned metric.
Given a collection of examples, we learn a Mahalanobis distance using an information-theoretic metric
learning technique that adapts prior knowledge about pairwise distances to incorporate similarity and dissimilarity
constraints. To enable sub-linear time similarity search under the learned metric, we show how
to encode a learned Mahalanobis parameterization into randomized locality-sensitive hash functions. We
further formulate an indirect solution that enables metric learning and hashing for sparse input vector spaces
whose high dimensionality make it infeasible to learn an explicit weighting over the feature dimensions.
We demonstrate the approach applied to systems and image datasets, and show that our learned metrics
improve accuracy relative to commonly-used metric baselines, while our hashing construction permits effi-
cient indexing with a learned distance and very large databases.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Similarity-Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2009</td>
    <td>
      <a href="/publications/raginsky2009locality/">Locality-sensitive binary codes from shift-invariant kernels</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Locality-sensitive binary codes from shift-invariant kernels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Locality-sensitive binary codes from shift-invariant kernels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Raginsky M., Lazebnik</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>633</td>
    <td><p>This paper addresses the problem of designing binary codes for high-dimensional
data such that vectors that are similar in the original space map to similar binary
strings. We introduce a simple distribution-free encoding scheme based on
random projections, such that the expected Hamming distance between the binary
codes of two vectors is related to the value of a shift-invariant kernel (e.g., a
Gaussian kernel) between the vectors. We present a full theoretical analysis of the
convergence properties of the proposed scheme, and report favorable experimental
performance as compared to a recent state-of-the-art method, spectral hashing.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Locality-Sensitive-Hashing 
      
        Evaluation 
      
        Compact-Codes 
      
    </td>
    </tr>      
    
     <tr>
  <td>2009</td>
    <td>
      <a href="/publications/raginsky2025locality/">Locality-sensitive binary codes from shift-invariant kernels</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Locality-sensitive binary codes from shift-invariant kernels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Locality-sensitive binary codes from shift-invariant kernels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Raginsky M., Lazebnik</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>633</td>
    <td><p>This paper addresses the problem of designing binary codes for high-dimensional
data such that vectors that are similar in the original space map to similar binary
strings. We introduce a simple distribution-free encoding scheme based on
random projections, such that the expected Hamming distance between the binary
codes of two vectors is related to the value of a shift-invariant kernel (e.g., a
Gaussian kernel) between the vectors. We present a full theoretical analysis of the
convergence properties of the proposed scheme, and report favorable experimental
performance as compared to a recent state-of-the-art method, spectral hashing.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Locality-Sensitive-Hashing 
      
        Evaluation 
      
        Compact-Codes 
      
    </td>
    </tr>      
    
     <tr>
  <td>2009</td>
    <td>
      <a href="/publications/jain2009fast/">Fast Similarity Search for Learned Metrics</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Fast Similarity Search for Learned Metrics' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Fast Similarity Search for Learned Metrics' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Jain P., Kulis, Grauman</td> <!-- 🔧 You were missing this -->
    <td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
    <td>265</td>
    <td><p>We propose a method to efficiently index into a large database of examples according to a learned metric.
Given a collection of examples, we learn a Mahalanobis distance using an information-theoretic metric
learning technique that adapts prior knowledge about pairwise distances to incorporate similarity and dissimilarity
constraints. To enable sub-linear time similarity search under the learned metric, we show how
to encode a learned Mahalanobis parameterization into randomized locality-sensitive hash functions. We
further formulate an indirect solution that enables metric learning and hashing for sparse input vector spaces
whose high dimensionality make it infeasible to learn an explicit weighting over the feature dimensions.
We demonstrate the approach applied to systems and image datasets, and show that our learned metrics
improve accuracy relative to commonly-used metric baselines, while our hashing construction permits effi-
cient indexing with a learned distance and very large databases.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Distance-Metric-Learning 
      
        Datasets 
      
        Similarity-Search 
      
    </td>
    </tr>      
    
     <tr>
  <td>2009</td>
    <td>
      <a href="/publications/kulis2009kernelized/">Kernelized Locality-Sensitive Hashing for Scalable Image Search</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Kernelized Locality-Sensitive Hashing for Scalable Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Kernelized Locality-Sensitive Hashing for Scalable Image Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Kulis B., Grauman</td> <!-- 🔧 You were missing this -->
    <td>2009 IEEE 12th International Conference on Computer Vision</td>
    <td>908</td>
    <td><p>Fast retrieval methods are critical for large-scale and
data-driven vision applications. Recent work has explored
ways to embed high-dimensional features or complex distance
functions into a low-dimensional Hamming space
where items can be efficiently searched. However, existing
methods do not apply for high-dimensional kernelized
data when the underlying feature embedding for the kernel
is unknown. We show how to generalize locality-sensitive
hashing to accommodate arbitrary kernel functions, making
it possible to preserve the algorithm’s sub-linear time similarity
search guarantees for a wide class of useful similarity
functions. Since a number of successful image-based kernels
have unknown or incomputable embeddings, this is especially
valuable for image retrieval tasks. We validate our
technique on several large-scale datasets, and show that it
enables accurate and fast performance for example-based
object classification, feature matching, and content-based
retrieval.</p>
</td>
    <td>
      
        Image-Retrieval 
      
        Scalability 
      
        Efficiency 
      
        ICCV 
      
        Datasets 
      
        Hashing-Methods 
      
        Evaluation 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2008</td>
    <td>
      <a href="/publications/weiss2008spectral/">Spectral Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Spectral Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Spectral Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Weiss Y., Torralba, Fergus</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>2154</td>
    <td><p>Semantic hashing seeks compact binary codes of data-points so that the
Hamming distance between codewords correlates with semantic similarity.
In this paper, we show that the problem of finding a best code for a given
dataset is closely related to the problem of graph partitioning and can
be shown to be NP hard. By relaxing the original problem, we obtain a
spectral method whose solutions are simply a subset of thresholded eigenvectors
of the graph Laplacian. By utilizing recent results on convergence
of graph Laplacian eigenvectors to the Laplace-Beltrami eigenfunctions of
manifolds, we show how to efficiently calculate the code of a novel datapoint.
Taken together, both learning the code and applying it to a novel
point are extremely simple. Our experiments show that our codes outperform
the state-of-the art.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        Compact-Codes 
      
        Text-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2008</td>
    <td>
      <a href="/publications/salakhutdinov2008semantic/">Semantic Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semantic Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semantic Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Salakhutdinov R., Hinton</td> <!-- 🔧 You were missing this -->
    <td>International Journal of Approximate Reasoning</td>
    <td>1272</td>
    <td><p>We show how to learn a deep graphical model of the word-count
vectors obtained from a large set of documents. The values of the
latent variables in the deepest layer are easy to infer and give a
much better representation of each document than Latent Semantic
Analysis. When the deepest layer is forced to use a small number of
binary variables (e.g. 32), the graphical model performs “semantic
hashing”: Documents are mapped to memory addresses in such a
way that semantically similar documents are located at nearby addresses.
Documents similar to a query document can then be found
by simply accessing all the addresses that differ by only a few bits
from the address of the query document. This way of extending the
efficiency of hash-coding to approximate matching is much faster
than locality sensitive hashing, which is the fastest current method.
By using semantic hashing to filter the documents given to TF-IDF,
we achieve higher accuracy than applying TF-IDF to the entire document
set.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Locality-Sensitive-Hashing 
      
        Efficiency 
      
        Text-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2008</td>
    <td>
      <a href="/publications/andoni2025near/">Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Andoni A., Indyk</td> <!-- 🔧 You were missing this -->
    <td>Communications of the ACM</td>
    <td>1420</td>
    <td><p>We present an algorithm for the c-approximate nearest neighbor problem in a d-dimensional Euclidean space, achieving query time of O(dn 1c2/+o(1)) and space O(dn + n1+1c2/+o(1)). This almost matches the lower bound for hashing-based algorithm recently obtained in (R. Motwani et al., 2006). We also obtain a space-efficient version of the algorithm, which uses dn+n logO(1) n space, with a query time of dnO(1/c2). Finally, we discuss practical variants of the algorithms that utilize fast bounded-distance decoders for the Leech lattice</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2008</td>
    <td>
      <a href="/publications/andoni2008near/">Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Andoni A., Indyk</td> <!-- 🔧 You were missing this -->
    <td>Communications of the ACM</td>
    <td>1420</td>
    <td><p>We present an algorithm for the c-approximate nearest neighbor problem in a d-dimensional Euclidean space, achieving query time of O(dn 1c2/+o(1)) and space O(dn + n1+1c2/+o(1)). This almost matches the lower bound for hashing-based algorithm recently obtained in (R. Motwani et al., 2006). We also obtain a space-efficient version of the algorithm, which uses dn+n logO(1) n space, with a query time of dnO(1/c2). Finally, we discuss practical variants of the algorithms that utilize fast bounded-distance decoders for the Leech lattice</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2008</td>
    <td>
      <a href="/publications/salakhutdinov2025semantic/">Semantic Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Semantic Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Semantic Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Salakhutdinov R., Hinton</td> <!-- 🔧 You were missing this -->
    <td>International Journal of Approximate Reasoning</td>
    <td>1272</td>
    <td><p>We show how to learn a deep graphical model of the word-count
vectors obtained from a large set of documents. The values of the
latent variables in the deepest layer are easy to infer and give a
much better representation of each document than Latent Semantic
Analysis. When the deepest layer is forced to use a small number of
binary variables (e.g. 32), the graphical model performs “semantic
hashing”: Documents are mapped to memory addresses in such a
way that semantically similar documents are located at nearby addresses.
Documents similar to a query document can then be found
by simply accessing all the addresses that differ by only a few bits
from the address of the query document. This way of extending the
efficiency of hash-coding to approximate matching is much faster
than locality sensitive hashing, which is the fastest current method.
By using semantic hashing to filter the documents given to TF-IDF,
we achieve higher accuracy than applying TF-IDF to the entire document
set.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Locality-Sensitive-Hashing 
      
        Efficiency 
      
        Text-Retrieval 
      
    </td>
    </tr>      
    
     <tr>
  <td>2008</td>
    <td>
      <a href="/publications/weiss2025spectral/">Spectral Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Spectral Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Spectral Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Weiss Y., Torralba, Fergus</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>2154</td>
    <td><p>Semantic hashing seeks compact binary codes of data-points so that the
Hamming distance between codewords correlates with semantic similarity.
In this paper, we show that the problem of finding a best code for a given
dataset is closely related to the problem of graph partitioning and can
be shown to be NP hard. By relaxing the original problem, we obtain a
spectral method whose solutions are simply a subset of thresholded eigenvectors
of the graph Laplacian. By utilizing recent results on convergence
of graph Laplacian eigenvectors to the Laplace-Beltrami eigenfunctions of
manifolds, we show how to efficiently calculate the code of a novel datapoint.
Taken together, both learning the code and applying it to a novel
point are extremely simple. Our experiments show that our codes outperform
the state-of-the art.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Datasets 
      
        Compact-Codes 
      
        Text-Retrieval 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2006</td>
    <td>
      <a href="/publications/godil2011retrieval/">Retrieval and Clustering from a 3D Human Database based on Body and Head Shape</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Retrieval and Clustering from a 3D Human Database based on Body and Head Shape' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Retrieval and Clustering from a 3D Human Database based on Body and Head Shape' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Godil Afzal, Ressler Sandy</td> <!-- 🔧 You were missing this -->
    <td>SAE Technical Paper Series</td>
    <td>19</td>
    <td><p>In this paper, we describe a framework for similarity based retrieval and
clustering from a 3D human database. Our technique is based on both body and
head shape representation and the retrieval is based on similarity of both of
them. The 3D human database used in our study is the CAESAR anthropometric
database which contains approximately 5000 bodies. We have developed a
web-based interface for specifying the queries to interact with the retrieval
system. Our approach performs the similarity based retrieval in a reasonable
amount of time and is a practical approach.</p>
</td>
    <td>
      
        Tools-&-Libraries 
      
    </td>
    </tr>      
    
     <tr>
  <td>2006</td>
    <td>
      <a href="/publications/li2025very/">Very Sparse Random Projections</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Very Sparse Random Projections' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Very Sparse Random Projections' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Ping, Hastie, Church</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</td>
    <td>634</td>
    <td><p>There has been considerable interest in random projections, an approximate algorithm for estimating distances between pairs of points in a high-dimensional vector space. Let A in Rn x D be our n points in D dimensions. The method multiplies A by a random matrix R in RD x k, reducing the D dimensions down to just k for speeding up the computation. R typically consists of entries of standard normal N(0,1). It is well known that random projections preserve pairwise distances (in the expectation). Achlioptas proposed sparse random projections by replacing the N(0,1) entries in R with entries in -1,0,1 with probabilities 1/6, 2/3, 1/6, achieving a threefold speedup in processing time.We recommend using R of entries in -1,0,1 with probabilities 1/2√D, 1-1√D, 1/2√D for achieving a significant √D-fold speedup, with little loss in accuracy.</p>
</td>
    <td>
      
        Locality-Sensitive-Hashing 
      
        Efficiency 
      
        KDD 
      
    </td>
    </tr>      
    
     <tr>
  <td>2006</td>
    <td>
      <a href="/publications/li2006very/">Very Sparse Random Projections</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Very Sparse Random Projections' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Very Sparse Random Projections' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Li Ping, Hastie, Church</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</td>
    <td>634</td>
    <td><p>There has been considerable interest in random projections, an approximate algorithm for estimating distances between pairs of points in a high-dimensional vector space. Let A in Rn x D be our n points in D dimensions. The method multiplies A by a random matrix R in RD x k, reducing the D dimensions down to just k for speeding up the computation. R typically consists of entries of standard normal N(0,1). It is well known that random projections preserve pairwise distances (in the expectation). Achlioptas proposed sparse random projections by replacing the N(0,1) entries in R with entries in -1,0,1 with probabilities 1/6, 2/3, 1/6, achieving a threefold speedup in processing time.We recommend using R of entries in -1,0,1 with probabilities 1/2√D, 1-1√D, 1/2√D for achieving a significant √D-fold speedup, with little loss in accuracy.</p>
</td>
    <td>
      
        Locality-Sensitive-Hashing 
      
        Efficiency 
      
        KDD 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>2004</td>
    <td>
      <a href="/publications/datar2025locality/">Locality-sensitive hashing scheme based on p-stable distributions</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Locality-sensitive hashing scheme based on p-stable distributions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Locality-sensitive hashing scheme based on p-stable distributions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Datar M., Immorlica, Indyk, Mirrokni</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the twentieth annual symposium on Computational geometry</td>
    <td>2887</td>
    <td><p>We present a novel Locality-Sensitive Hashing scheme for the Approximate Nearest Neighbor Problem under lp norm, based on p-stable distributions.Our scheme improves the running time of the earlier algorithm for the case of the lp norm. It also yields the first known provably efficient approximate NN algorithm for the case p&lt;1. We also show that the algorithm finds the exact near neigbhor in O(log n) time for data satisfying certain “bounded growth” condition.Unlike earlier schemes, our LSH scheme works directly on points in the Euclidean space without embeddings. Consequently, the resulting query time bound is free of large factors and is simple and easy to implement. Our experiments (on synthetic data sets) show that the our data structure is up to 40 times faster than kd-tree.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Locality-Sensitive-Hashing 
      
        Tree-Based-ANN 
      
        Efficiency 
      
    </td>
    </tr>      
    
     <tr>
  <td>2004</td>
    <td>
      <a href="/publications/datar2004locality/">Locality-sensitive hashing scheme based on p-stable distributions</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Locality-sensitive hashing scheme based on p-stable distributions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Locality-sensitive hashing scheme based on p-stable distributions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Datar M., Immorlica, Indyk, Mirrokni</td> <!-- 🔧 You were missing this -->
    <td>Proceedings of the twentieth annual symposium on Computational geometry</td>
    <td>2887</td>
    <td><p>We present a novel Locality-Sensitive Hashing scheme for the Approximate Nearest Neighbor Problem under lp norm, based on p-stable distributions.Our scheme improves the running time of the earlier algorithm for the case of the lp norm. It also yields the first known provably efficient approximate NN algorithm for the case p&lt;1. We also show that the algorithm finds the exact near neigbhor in O(log n) time for data satisfying certain “bounded growth” condition.Unlike earlier schemes, our LSH scheme works directly on points in the Euclidean space without embeddings. Consequently, the resulting query time bound is free of large factors and is simple and easy to implement. Our experiments (on synthetic data sets) show that the our data structure is up to 40 times faster than kd-tree.</p>
</td>
    <td>
      
        Hashing-Methods 
      
        Locality-Sensitive-Hashing 
      
        Tree-Based-ANN 
      
        Efficiency 
      
    </td>
    </tr>      
    
    
      
     <tr>
  <td>1999</td>
    <td>
      <a href="/publications/gionis1999similarity/">Similarity Search in High Dimensions via Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Similarity Search in High Dimensions via Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Similarity Search in High Dimensions via Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gionis A., Indyk, Motwani</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>3205</td>
    <td><p>The nearest- or near-neighbor query problems arise in a large variety of database applications, usually in the context of similarity searching. Of late, there has been increasing interest in building search/index structures for performing similarity search over high-dimensional data, e.g., image databases, document collections, time-series databases, and genome databases. Unfortunately,
all known techniques for solving this problem fall prey to the curse of dimensionality. That is, the data structures scale poorly with data dimensionality;
in fact, if the number of dimensions exceeds 10 to 20, searching in k-d trees and related structures involves the inspection of a large fraction of the database, thereby doing no better than brute-force linear search. It has been suggested that since the selection of features and the choice of a distance metric in typical applications is rather heuristic, determining an approximate nearest neighbor should suffice for most practical purposes. In this paper, we examine a novel scheme for approximate similarity search based on hashing. The basic idea is to hash the points from the database so as to ensure that the probability of collision is much higher for objects that are close to each other than for those that are far apart. We provide experimental evidence that our
method gives significant improvement in running time over other methods for searching in highdimensional spaces based on hierarchical tree decomposition.
Experimental results also indicate that our scheme scales well even for a relatively large number of dimensions (more than 50).</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Tree-Based-ANN 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Vector-Indexing 
      
    </td>
    </tr>      
    
     <tr>
  <td>1999</td>
    <td>
      <a href="/publications/gionis2025similarity/">Similarity Search in High Dimensions via Hashing</a>
      <span class="externallinks">
        &nbsp;<a href='http://scholar.google.com/scholar?q=Similarity Search in High Dimensions via Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
        <a href='https://www.semanticscholar.org/search?q=Similarity Search in High Dimensions via Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
      </span>
    </td>
    <td>Gionis A., Indyk, Motwani</td> <!-- 🔧 You were missing this -->
    <td>No Venue</td>
    <td>3205</td>
    <td><p>The nearest- or near-neighbor query problems arise in a large variety of database applications, usually in the context of similarity searching. Of late, there has been increasing interest in building search/index structures for performing similarity search over high-dimensional data, e.g., image databases, document collections, time-series databases, and genome databases. Unfortunately,
all known techniques for solving this problem fall prey to the curse of dimensionality. That is, the data structures scale poorly with data dimensionality;
in fact, if the number of dimensions exceeds 10 to 20, searching in k-d trees and related structures involves the inspection of a large fraction of the database, thereby doing no better than brute-force linear search. It has been suggested that since the selection of features and the choice of a distance metric in typical applications is rather heuristic, determining an approximate nearest neighbor should suffice for most practical purposes. In this paper, we examine a novel scheme for approximate similarity search based on hashing. The basic idea is to hash the points from the database so as to ensure that the probability of collision is much higher for objects that are close to each other than for those that are far apart. We provide experimental evidence that our
method gives significant improvement in running time over other methods for searching in highdimensional spaces based on hierarchical tree decomposition.
Experimental results also indicate that our scheme scales well even for a relatively large number of dimensions (more than 50).</p>
</td>
    <td>
      
        Distance-Metric-Learning 
      
        Tree-Based-ANN 
      
        Similarity-Search 
      
        Hashing-Methods 
      
        Vector-Indexing 
      
    </td>
    </tr>      
    
    
  </tbody>
</table>

<!-- CSS Styles -->
<style>
  /* Hide the table initially */

  #allPapers {
  display: none;
  table-layout: fixed;
  width: 100%;
  }
  /* Style the loading indicator */
  #loading {
    position: fixed;
    top: 50%;
    left: 50%;
    transform: translate(-50%, -50%);
    font-size: 1.5em;
    text-align: center;
  }
</style>

<!-- JavaScript -->
<script>
  var datatable;
  var searchInitialized = false;

  function searchTable() {
    // Check if datatable is initialized
    if (datatable && searchInitialized) {
      var hash = decodeURIComponent(window.location.hash.substr(1));
      if (hash) {
        datatable.search(hash).draw();
      } else {
        datatable.search('').draw();  // Clear search if no hash is present
      }
    } else {
      // Retry if datatable is not yet initialized
      setTimeout(searchTable, 500);
    }
  }

  $(document).ready(function() {
    // Show the loading indicator
    $('#loading').show();

    // Initialize the DataTable
    datatable = $('#allPapers').DataTable({
      paging: false,
      pageLength: 100,
      searching: true,
      order: [[0, 'desc'], [4, 'desc']],  // Default sort: Year desc, then Citations desc
      columnDefs: [
        {
          targets: [5, 6],  // Adjusted for new column indices
          visible: false,
          searchable: true
        },
        {
          targets: 4,  // Citation count column
          type: 'num'
        }
      ],
      // Callback when DataTable is initialized
      initComplete: function(settings, json) {
        // Hide the loading indicator
        $('#loading').hide();
        // Show the table
        $('#allPapers').show();
        // Set searchInitialized to true after initialization
        searchInitialized = true;
        // Perform the initial search based on the hash (if any)
        searchTable();
      }
    });
  });

  // Update search whenever the hash changes
  $(window).on('hashchange', function() {
    searchTable();
  });
</script>


    </div>

  </body>
</html>
