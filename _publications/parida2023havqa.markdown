---
layout: publication
title: 'Havqa: A Dataset For Visual Question Answering And Multimodal Research In
  Hausa Language'
authors: "Shantipriya Parida, Idris Abdulmumin, Shamsuddeen Hassan Muhammad, Aneesh\
  \ Bose, Guneet Singh Kohli, Ibrahim Said Ahmad, Ketan Kotwal, Sayan Deb Sarkar,\
  \ Ond\u0159ej Bojar, Habeebah Adamu Kakudi"
conference: 'Findings of the Association for Computational Linguistics: ACL 2023'
year: 2023
bibkey: parida2023havqa
citations: 2
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2305.17690'}]
tags: ["Datasets"]
short_authors: Parida et al.
---
This paper presents HaVQA, the first multimodal dataset for visual
question-answering (VQA) tasks in the Hausa language. The dataset was created
by manually translating 6,022 English question-answer pairs, which are
associated with 1,555 unique images from the Visual Genome dataset. As a
result, the dataset provides 12,044 gold standard English-Hausa parallel
sentences that were translated in a fashion that guarantees their semantic
match with the corresponding visual information. We conducted several baseline
experiments on the dataset, including visual question answering, visual
question elicitation, text-only and multimodal machine translation.