---
layout: publication
title: 'Look, Imagine And Match: Improving Textual-visual Cross-modal Retrieval With
  Generative Models'
authors: Gu et al.
conference: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition
year: 2017
bibkey: gu2017look
citations: 283
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1711.06420'}]
tags: ["Multimodal-Retrieval", "CVPR"]
---
Textual-visual cross-modal retrieval has been a hot research topic in both
computer vision and natural language processing communities. Learning
appropriate representations for multi-modal data is crucial for the cross-modal
retrieval performance. Unlike existing image-text retrieval approaches that
embed image-text pairs as single feature vectors in a common representational
space, we propose to incorporate generative processes into the cross-modal
feature embedding, through which we are able to learn not only the global
abstract features but also the local grounded features. Extensive experiments
show that our framework can well match images and sentences with complex
content, and achieve the state-of-the-art cross-modal retrieval results on
MSCOCO dataset.