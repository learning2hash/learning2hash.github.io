---
layout: publication
title: Check Your Facts And Try Again Improving Large Language Models With External Knowledge And Automated Feedback
authors: Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, Jianfeng Gao
conference: "Arxiv"
year: 2023
bibkey: peng2023check
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2302.12813v3"}
tags: ['ARXIV']
---
Large language models (LLMs) such as ChatGPT are able to generate human-like fluent responses for many downstream tasks e.g. task-oriented dialog and question answering. However applying LLMs to real-world mission-critical applications remains challenging mainly due to their tendency to generate hallucinations and their inability to use external knowledge. This paper proposes a LLM-Augmenter system which augments a black-box LLM with a set of plug-and-play modules. Our system makes the LLM generate responses grounded in external knowledge e.g. stored in task-specific databases. It also iteratively revises LLM prompts to improve model responses using feedback generated by utility functions e.g. the factuality score of a LLM-generated response. The effectiveness of LLM-Augmenter is empirically validated on two types of scenarios task-oriented dialog and open-domain question answering. LLM-Augmenter significantly reduces ChatGPTs hallucinations without sacrificing the fluency and informativeness of its responses. We make the source code and models publicly available.
