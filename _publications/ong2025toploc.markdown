---
layout: publication
title: 'TOPLOC: A Locality Sensitive Hashing Scheme For Trustless Verifiable Inference'
authors: Jack Min Ong, Matthew di Ferrante, Aaron Pazdera, Ryan Garner, Sami Jaghouar,
  Manveer Basra, Max Ryabinin, Johannes Hagemann
conference: Arxiv
year: 2025
bibkey: ong2025toploc
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2501.16007'}]
tags: [Hashing Methods, Locality Sensitive Hashing, Evaluation]
short_authors: Ong et al.
---
Large language models (LLMs) have proven to be very capable, but access to frontier models currently relies on inference providers. This introduces trust challenges: how can we be sure that the provider is using the model configuration they claim? We propose TOPLOC, a novel method for verifiable inference that addresses this problem. TOPLOC leverages a compact locality-sensitive hashing mechanism for intermediate activations, which can detect unauthorized modifications to models, prompts, or precision with 100% accuracy, achieving no false positives or negatives in our empirical evaluations. Our approach is robust across diverse hardware configurations, GPU types, and algebraic reorderings, which allows for validation speeds significantly faster than the original inference. By introducing a polynomial encoding scheme, TOPLOC minimizes the memory overhead of the generated proofs by \(1000\times\), requiring only 258 bytes of storage per 32 new tokens, compared to the 262 KB requirement of storing the token embeddings directly for Llama 3.1-8B-Instruct. Our method empowers users to verify LLM inference computations efficiently, fostering greater trust and transparency in open ecosystems and laying a foundation for decentralized, verifiable and trustless AI services.