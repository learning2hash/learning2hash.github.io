---
layout: publication
title: Deep Semantic Multimodal Hashing Network For Scalable Image45;text And Video45;text Retrievals
authors: Jin Lu, Li Zechao, Tang Jinhui
conference: "Arxiv"
year: 2019
bibkey: jin2019deep
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1901.02662"}
tags: ['ARXIV', 'CNN', 'Cross Modal', 'Supervised', 'Text Retrieval']
---
Hashing has been widely applied to multimodal retrieval on large45;scale multimedia data due to its efficiency in computation and storage. In this article we propose a novel deep semantic multimodal hashing network (DSMHN) for scalable image45;text and video45;text retrieval. The proposed deep hashing framework leverages 245;D convolutional neural networks (CNN) as the backbone network to capture the spatial information for image45;text retrieval while the 345;D CNN as the backbone network to capture the spatial and temporal information for video45;text retrieval. In the DSMHN two sets of modality45;specific hash functions are jointly learned by explicitly preserving both intermodality similarities and intramodality semantic labels. Specifically with the assumption that the learned hash codes should be optimal for the classification task two stream networks are jointly trained to learn the hash functions by embedding the semantic labels on the resultant hash codes. Moreover a unified deep multimodal hashing framework is proposed to learn compact and high45;quality hash codes by exploiting the feature representation learning intermodality similarity45;preserving learning semantic label45;preserving learning and hash function learning with different types of loss functions simultaneously. The proposed DSMHN method is a generic and scalable deep hashing framework for both image45;text and video45;text retrievals which can be flexibly integrated with different types of loss functions. We conduct extensive experiments for both single modal45; and cross45;modal45;retrieval tasks on four widely used multimodal45;retrieval data sets. Experimental results on both image45;text45; and video45;text45;retrieval tasks demonstrate that the DSMHN significantly outperforms the state45;of45;the45;art methods.
