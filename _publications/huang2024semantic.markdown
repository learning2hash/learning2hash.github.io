---
layout: publication
title: Semantic Residual For Multimodal Unified Discrete Representation
authors: Hai Huang, Shulei Wang, Yan Xia
conference: ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech
  and Signal Processing (ICASSP)
year: 2025
bibkey: huang2024semantic
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2412.19128'}]
tags: [Quantization, ICASSP, Few-shot & Zero-shot, Tools & Libraries, Evaluation]
short_authors: Hai Huang, Shulei Wang, Yan Xia
---
Recent research in the domain of multimodal unified representations
predominantly employs codebook as representation forms, utilizing Vector
Quantization(VQ) for quantization, yet there has been insufficient exploration
of other quantization representation forms. Our work explores more precise
quantization methods and introduces a new framework, Semantic Residual
Cross-modal Information Disentanglement (SRCID), inspired by the numerical
residual concept inherent to Residual Vector Quantization (RVQ). SRCID employs
semantic residual-based information disentanglement for multimodal data to
better handle the inherent discrepancies between different modalities. Our
method enhances the capabilities of unified multimodal representations and
demonstrates exceptional performance in cross-modal generalization and
cross-modal zero-shot retrieval. Its average results significantly surpass
existing state-of-the-art models, as well as previous attempts with RVQ and
Finite Scalar Quantization (FSQ) based on these modals.