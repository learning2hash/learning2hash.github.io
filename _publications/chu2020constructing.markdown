---
layout: publication
title: Constructing A Visual Relationship Authenticity Dataset
authors: Chenhui Chu, Yuto Takebayashi, Mishra Vipul, Yuta Nakashima
conference: Arxiv
year: 2020
bibkey: chu2020constructing
citations: 0
additional_links: [{name: Code, url: 'https://github.com/codecreator2053/VR_ClassifiedDataset'},
  {name: Paper, url: 'https://arxiv.org/abs/2010.05185'}]
tags: ["Datasets"]
short_authors: Chu et al.
---
A visual relationship denotes a relationship between two objects in an image,
which can be represented as a triplet of (subject; predicate; object). Visual
relationship detection is crucial for scene understanding in images. Existing
visual relationship detection datasets only contain true relationships that
correctly describe the content in an image. However, distinguishing false
visual relationships from true ones is also crucial for image understanding and
grounded natural language processing. In this paper, we construct a visual
relationship authenticity dataset, where both true and false relationships
among all objects appeared in the captions in the Flickr30k entities image
caption dataset are annotated. The dataset is available at
https://github.com/codecreator2053/VR_ClassifiedDataset. We hope that this
dataset can promote the study on both vision and language understanding.