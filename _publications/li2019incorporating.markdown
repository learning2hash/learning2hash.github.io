---
layout: publication
title: Incorporating Textual Evidence In Visual Storytelling
authors: Tianyi Li, Sujian Li
conference: Proceedings of the 1st Workshop on Discourse Structure in Neural NLG
year: 2019
bibkey: li2019incorporating
citations: 2
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1911.09334'}]
tags: []
short_authors: Tianyi Li, Sujian Li
---
Previous work on visual storytelling mainly focused on exploring image
sequence as evidence for storytelling and neglected textual evidence for
guiding story generation. Motivated by human storytelling process which recalls
stories for familiar images, we exploit textual evidence from similar images to
help generate coherent and meaningful stories. To pick the images which may
provide textual experience, we propose a two-step ranking method based on image
object recognition techniques. To utilize textual information, we design an
extended Seq2Seq model with two-channel encoder and attention. Experiments on
the VIST dataset show that our method outperforms state-of-the-art baseline
models without heavy engineering.