---
layout: publication
title: Deep Cross-modal Hashing
authors: Qing-yuan Jiang, Wu-jun Li
conference: Arxiv
year: 2016
citations: 589
bibkey: jiang2016deep
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1602.02255'}]
tags: [Applications, Multi-Modal Hashing, Hashing Methods, Deep Hashing]
---
Due to its low storage cost and fast query speed, cross-modal hashing (CMH)
has been widely used for similarity search in multimedia retrieval
applications. However, almost all existing CMH methods are based on
hand-crafted features which might not be optimally compatible with the
hash-code learning procedure. As a result, existing CMH methods with
handcrafted features may not achieve satisfactory performance. In this paper,
we propose a novel cross-modal hashing method, called deep crossmodal hashing
(DCMH), by integrating feature learning and hash-code learning into the same
framework. DCMH is an end-to-end learning framework with deep neural networks,
one for each modality, to perform feature learning from scratch. Experiments on
two real datasets with text-image modalities show that DCMH can outperform
other baselines to achieve the state-of-the-art performance in cross-modal
retrieval applications.