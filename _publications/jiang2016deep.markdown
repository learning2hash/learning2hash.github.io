---
layout: publication
title: Deep Cross-Modal Hashing
authors: Jiang Qing-Yuan, Li Wu-Jun
conference: "Arxiv"
year: 2016
bibkey: jiang2016deep
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1602.02255"}
tags: ['ARXIV', 'Cross Modal']
---
Due to its low storage cost and fast query speed cross-modal hashing (CMH) has been widely used for similarity search in multimedia retrieval applications. However almost all existing CMH methods are based on hand-crafted features which might not be optimally compatible with the hash-code learning procedure. As a result existing CMH methods with handcrafted features may not achieve satisfactory performance. In this paper we propose a novel cross-modal hashing method called deep crossmodal hashing (DCMH) by integrating feature learning and hash-code learning into the same framework. DCMH is an end-to-end learning framework with deep neural networks one for each modality to perform feature learning from scratch. Experiments on two real datasets with text-image modalities show that DCMH can outperform other baselines to achieve the state-of-the-art performance in cross-modal retrieval applications.
