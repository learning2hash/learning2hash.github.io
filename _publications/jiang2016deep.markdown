---
layout: publication
title: Deep Cross-modal Hashing
authors: Qing-Yuan Jiang, Wu-Jun Li
conference: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
year: 2017
bibkey: jiang2016deep
citations: 761
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1602.02255'}]
tags: ["CVPR", "Datasets", "Evaluation", "Hashing Methods", "Memory Efficiency", "Similarity Search", "Tools & Libraries"]
short_authors: Qing-Yuan Jiang, Wu-Jun Li
---
Due to its low storage cost and fast query speed, cross-modal hashing (CMH)
has been widely used for similarity search in multimedia retrieval
applications. However, almost all existing CMH methods are based on
hand-crafted features which might not be optimally compatible with the
hash-code learning procedure. As a result, existing CMH methods with
handcrafted features may not achieve satisfactory performance. In this paper,
we propose a novel cross-modal hashing method, called deep crossmodal hashing
(DCMH), by integrating feature learning and hash-code learning into the same
framework. DCMH is an end-to-end learning framework with deep neural networks,
one for each modality, to perform feature learning from scratch. Experiments on
two real datasets with text-image modalities show that DCMH can outperform
other baselines to achieve the state-of-the-art performance in cross-modal
retrieval applications.