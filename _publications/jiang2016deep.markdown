---
layout: publication
title: Deep Cross45;modal Hashing
authors: Jiang Qing-yuan, Li Wu-jun
conference: "Arxiv"
year: 2016
bibkey: jiang2016deep
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1602.02255"}
tags: ['ARXIV', 'Cross Modal', 'Supervised']
---
Due to its low storage cost and fast query speed cross45;modal hashing (CMH) has been widely used for similarity search in multimedia retrieval applications. However almost all existing CMH methods are based on hand45;crafted features which might not be optimally compatible with the hash45;code learning procedure. As a result existing CMH methods with handcrafted features may not achieve satisfactory performance. In this paper we propose a novel cross45;modal hashing method called deep crossmodal hashing (DCMH) by integrating feature learning and hash45;code learning into the same framework. DCMH is an end45;to45;end learning framework with deep neural networks one for each modality to perform feature learning from scratch. Experiments on two real datasets with text45;image modalities show that DCMH can outperform other baselines to achieve the state45;of45;the45;art performance in cross45;modal retrieval applications.
