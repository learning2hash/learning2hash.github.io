---
layout: publication
title: Normalized Information Distance
authors: Paul M. B. Vitanyi, Frank J. Balbach, Rudi L. Cilibrasi, Ming Li
conference: Information Theory and Statistical Learning
year: 2008
bibkey: vitanyi2008normalized
citations: 87
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/0809.2553'}]
tags: ["Distance Metric Learning"]
short_authors: Vitanyi et al.
---
The normalized information distance is a universal distance measure for
objects of all kinds. It is based on Kolmogorov complexity and thus
uncomputable, but there are ways to utilize it. First, compression algorithms
can be used to approximate the Kolmogorov complexity if the objects have a
string representation. Second, for names and abstract concepts, page count
statistics from the World Wide Web can be used. These practical realizations of
the normalized information distance can then be applied to machine learning
tasks, expecially clustering, to perform feature-free and parameter-free data
mining. This chapter discusses the theoretical foundations of the normalized
information distance and both practical realizations. It presents numerous
examples of successful real-world applications based on these distance
measures, ranging from bioinformatics to music clustering to machine
translation.