---
layout: publication
title: 'Visual Semantic Parsing: From Images To Abstract Meaning Representation'
authors: Mohamed Ashraf Abdelsalam, Zhan Shi, Federico Fancellu, Kalliopi Basioti,
  Dhaivat J. Bhatt, Vladimir Pavlovic, Afsaneh Fazly
conference: Proceedings of the 26th Conference on Computational Natural Language Learning
  (CoNLL)
year: 2022
bibkey: abdelsalam2022visual
citations: 2
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2210.14862'}]
tags: ["Image Retrieval", "Multimodal Retrieval"]
short_authors: Abdelsalam et al.
---
The success of scene graphs for visual scene understanding has brought
attention to the benefits of abstracting a visual input (e.g., image) into a
structured representation, where entities (people and objects) are nodes
connected by edges specifying their relations. Building these representations,
however, requires expensive manual annotation in the form of images paired with
their scene graphs or frames. These formalisms remain limited in the nature of
entities and relations they can capture. In this paper, we propose to leverage
a widely-used meaning representation in the field of natural language
processing, the Abstract Meaning Representation (AMR), to address these
shortcomings. Compared to scene graphs, which largely emphasize spatial
relationships, our visual AMR graphs are more linguistically informed, with a
focus on higher-level semantic concepts extrapolated from visual input.
Moreover, they allow us to generate meta-AMR graphs to unify information
contained in multiple image descriptions under one representation. Through
extensive experimentation and analysis, we demonstrate that we can re-purpose
an existing text-to-AMR parser to parse images into AMRs. Our findings point to
important future research directions for improved scene understanding.