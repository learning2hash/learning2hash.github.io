---
layout: publication
title: Interpreting And Analysing Clip's Zero-shot Image Classification Via Mutual
  Knowledge
authors: Fawaz Sammani, Nikos Deligiannis
conference: Arxiv
year: 2024
bibkey: sammani2024interpreting
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2410.13016'}]
tags: [Few-shot & Zero-shot, Datasets]
short_authors: Fawaz Sammani, Nikos Deligiannis
---
Contrastive Language-Image Pretraining (CLIP) performs zero-shot image
classification by mapping images and textual class representation into a shared
embedding space, then retrieving the class closest to the image. This work
provides a new approach for interpreting CLIP models for image classification
from the lens of mutual knowledge between the two modalities. Specifically, we
ask: what concepts do both vision and language CLIP encoders learn in common
that influence the joint embedding space, causing points to be closer or
further apart? We answer this question via an approach of textual concept-based
explanations, showing their effectiveness, and perform an analysis encompassing
a pool of 13 CLIP models varying in architecture, size and pretraining
datasets. We explore those different aspects in relation to mutual knowledge,
and analyze zero-shot predictions. Our approach demonstrates an effective and
human-friendly way of understanding zero-shot classification decisions with
CLIP.