---
layout: publication
title: Jointly Embedding Entities And Text With Distant Supervision
authors: Denis Newman-Griffis, Albert M. Lai, Eric Fosler-Lussier
conference: Proceedings of The Third Workshop on Representation Learning for NLP
year: 2018
bibkey: newmangriffis2018jointly
citations: 37
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1807.03399'}]
tags: []
short_authors: Denis Newman-Griffis, Albert M. Lai, Eric Fosler-Lussier
---
Learning representations for knowledge base entities and concepts is becoming
increasingly important for NLP applications. However, recent entity embedding
methods have relied on structured resources that are expensive to create for
new domains and corpora. We present a distantly-supervised method for jointly
learning embeddings of entities and text from an unnanotated corpus, using only
a list of mappings between entities and surface forms. We learn embeddings from
open-domain and biomedical corpora, and compare against prior methods that rely
on human-annotated text or large knowledge graph structure. Our embeddings
capture entity similarity and relatedness better than prior work, both in
existing biomedical datasets and a new Wikipedia-based dataset that we release
to the community. Results on analogy completion and entity sense disambiguation
indicate that entities and words capture complementary information that can be
effectively combined for downstream use.