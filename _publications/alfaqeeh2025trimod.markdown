---
layout: publication
title: Trimod Fusion For Multimodal Named Entity Recognition In Social Media
authors: Mosab Alfaqeeh
conference: Arxiv
year: 2025
bibkey: alfaqeeh2025trimod
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2501.08267'}]
tags: ["Datasets", "Evaluation"]
short_authors: Mosab Alfaqeeh
---
Social media platforms serve as invaluable sources of user-generated content,
offering insights into various aspects of human behavior. Named Entity
Recognition (NER) plays a crucial role in analyzing such content by identifying
and categorizing named entities into predefined classes. However, traditional
NER models often struggle with the informal, contextually sparse, and ambiguous
nature of social media language. To address these challenges, recent research
has focused on multimodal approaches that leverage both textual and visual cues
for enhanced entity recognition. Despite advances, existing methods face
limitations in capturing nuanced mappings between visual objects and textual
entities and addressing distributional disparities between modalities. In this
paper, we propose a novel approach that integrates textual, visual, and hashtag
features (TriMod), utilizing Transformer-attention for effective modality
fusion. The improvements exhibited by our model suggest that named entities can
greatly benefit from the auxiliary context provided by multiple modalities,
enabling more accurate recognition. Through the experiments on a multimodal
social media dataset, we demonstrate the superiority of our approach over
existing state-of-the-art methods, achieving significant improvements in
precision, recall, and F1 score.