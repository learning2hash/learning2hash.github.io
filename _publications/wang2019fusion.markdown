---
layout: publication
title: Fusion-supervised Deep Cross-modal Hashing
authors: Li Wang, Lei Zhu, En Yu, Jiande Sun, Huaxiang Zhang
conference: Arxiv
year: 2019
citations: 13
bibkey: wang2019fusion
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1904.11171'}]
tags: [Deep Hashing, Tools and Libraries, ANN Search, Multi-Modal Hashing, Evaluation
    Metrics, Supervised, Benchmarks and Datasets, Hashing Methods]
---
Deep hashing has recently received attention in cross-modal retrieval for its
impressive advantages. However, existing hashing methods for cross-modal
retrieval cannot fully capture the heterogeneous multi-modal correlation and
exploit the semantic information. In this paper, we propose a novel
*Fusion-supervised Deep Cross-modal Hashing* (FDCH) approach. Firstly,
FDCH learns unified binary codes through a fusion hash network with paired
samples as input, which effectively enhances the modeling of the correlation of
heterogeneous multi-modal data. Then, these high-quality unified hash codes
further supervise the training of the modality-specific hash networks for
encoding out-of-sample queries. Meanwhile, both pair-wise similarity
information and classification information are embedded in the hash networks
under one stream framework, which simultaneously preserves cross-modal
similarity and keeps semantic consistency. Experimental results on two
benchmark datasets demonstrate the state-of-the-art performance of FDCH.