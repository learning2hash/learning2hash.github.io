---
layout: publication
title: Improvements And Extensions On Metaphor Detection
authors: Weicheng Ma, Ruibo Liu, Lili Wang, Soroush Vosoughi
conference: Arxiv
year: 2020
bibkey: ma2020improvements
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2012.04540'}]
tags: []
short_authors: Ma et al.
---
Metaphors are ubiquitous in human language. The metaphor detection task (MD)
aims at detecting and interpreting metaphors from written language, which is
crucial in natural language understanding (NLU) research. In this paper, we
introduce a pre-trained Transformer-based model into MD. Our model outperforms
the previous state-of-the-art models by large margins in our evaluations, with
relative improvements on the F-1 score from 5.33% to 28.39%. Second, we extend
MD to a classification task about the metaphoricity of an entire piece of text
to make MD applicable in more general NLU scenes. Finally, we clean up the
improper or outdated annotations in one of the MD benchmark datasets and
re-benchmark it with our Transformer-based model. This approach could be
applied to other existing MD datasets as well, since the metaphoricity
annotations in these benchmark datasets may be outdated. Future research
efforts are also necessary to build an up-to-date and well-annotated dataset
consisting of longer and more complex texts.