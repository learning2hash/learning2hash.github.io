---
layout: publication
title: "Deep Cross-Modal Hashing"
authors: Qing-Yuan Jiang, Wu-Jun Li
conference: CVPR
year: 2017
bibkey: jiang2017deep
additional_links:
   - {name: "PDF", url: "https://cs.nju.edu.cn/lwj/paper/CVPR17_DCMH.pdf"}
   - {name: "Code", url: "https://cs.nju.edu.cn/lwj/code/DCMH_tensorflow.zip"}   
---
Due to its low storage cost and fast query speed, crossmodal hashing (CMH) has been widely used for similarity
search in multimedia retrieval applications. However, most
existing CMH methods are based on hand-crafted features
which might not be optimally compatible with the hash-code
learning procedure. As a result, existing CMH methods
with hand-crafted features may not achieve satisfactory
performance. In this paper, we propose a novel CMH
method, called deep cross-modal hashing (DCMH), by
integrating feature learning and hash-code learning into
the same framework. DCMH is an end-to-end learning
framework with deep neural networks, one for each modality, to perform feature learning from scratch. Experiments
on three real datasets with image-text modalities show
that DCMH can outperform other baselines to achieve
the state-of-the-art performance in cross-modal retrieval
applications.
