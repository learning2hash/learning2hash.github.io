---
layout: publication
title: Connecting Vision And Language With Localized Narratives
authors: Jordi Pont-tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, Vittorio
  Ferrari
conference: Lecture Notes in Computer Science
year: 2020
bibkey: ponttuset2019connecting
citations: 155
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1912.03098'}]
tags: ["Datasets"]
short_authors: Pont-tuset et al.
---
We propose Localized Narratives, a new form of multimodal image annotations
connecting vision and language. We ask annotators to describe an image with
their voice while simultaneously hovering their mouse over the region they are
describing. Since the voice and the mouse pointer are synchronized, we can
localize every single word in the description. This dense visual grounding
takes the form of a mouse trace segment per word and is unique to our data. We
annotated 849k images with Localized Narratives: the whole COCO, Flickr30k, and
ADE20K datasets, and 671k images of Open Images, all of which we make publicly
available. We provide an extensive analysis of these annotations showing they
are diverse, accurate, and efficient to produce. We also demonstrate their
utility on the application of controlled image captioning.