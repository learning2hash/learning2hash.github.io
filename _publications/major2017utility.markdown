---
layout: publication
title: Utility Of General And Specific Word Embeddings For Classifying Translational
  Stages Of Research
authors: Vincent Major, Alisa Surkis, Yindalon Aphinyanaphongs
conference: Arxiv
year: 2018
bibkey: major2017utility
citations: 9
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1705.06262'}]
tags: []
short_authors: Vincent Major, Alisa Surkis, Yindalon Aphinyanaphongs
---
Conventional text classification models make a bag-of-words assumption
reducing text into word occurrence counts per document. Recent algorithms such
as word2vec are capable of learning semantic meaning and similarity between
words in an entirely unsupervised manner using a contextual window and doing so
much faster than previous methods. Each word is projected into vector space
such that similar meaning words such as "strong" and "powerful" are projected
into the same general Euclidean space. Open questions about these embeddings
include their utility across classification tasks and the optimal properties
and source of documents to construct broadly functional embeddings. In this
work, we demonstrate the usefulness of pre-trained embeddings for
classification in our task and demonstrate that custom word embeddings, built
in the domain and for the tasks, can improve performance over word embeddings
learnt on more general data including news articles or Wikipedia.