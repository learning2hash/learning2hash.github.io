---
layout: publication
title: Overview Of The TREC 2020 Deep Learning Track
authors: Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos
conference: Arxiv
year: 2021
bibkey: craswell2021overview
citations: 116
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2102.07662'}]
tags: ["Evaluation", "Neural Hashing", "Survey Paper", "Text Retrieval"]
short_authors: Craswell et al.
---
This is the second year of the TREC Deep Learning Track, with the goal of
studying ad hoc ranking in the large training data regime. We again have a
document retrieval task and a passage retrieval task, each with hundreds of
thousands of human-labeled training queries. We evaluate using single-shot
TREC-style evaluation, to give us a picture of which ranking methods work best
when large data is available, with much more comprehensive relevance labeling
on the small number of test queries. This year we have further evidence that
rankers with BERT-style pretraining outperform other rankers in the large data
regime.