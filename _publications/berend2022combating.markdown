---
layout: publication
title: Combating The Curse Of Multilinguality In Cross-lingual WSD By Aligning Sparse
  Contextualized Word Representations
authors: "G\xE1bor Berend"
conference: 'Proceedings of the 2022 Conference of the North American Chapter of the
  Association for Computational Linguistics: Human Language Technologies'
year: 2022
bibkey: berend2022combating
citations: 4
additional_links: [{name: Code, url: 'https://github.com/begab/sparsity_makes_sense'},
  {name: Paper, url: 'https://arxiv.org/abs/2307.13776'}]
tags: ["Few Shot & Zero Shot", "NAACL"]
short_authors: "G\xE1bor Berend"
---
In this paper, we advocate for using large pre-trained monolingual language
models in cross lingual zero-shot word sense disambiguation (WSD) coupled with
a contextualized mapping mechanism. We also report rigorous experiments that
illustrate the effectiveness of employing sparse contextualized word
representations obtained via a dictionary learning procedure. Our experimental
results demonstrate that the above modifications yield a significant
improvement of nearly 6.5 points of increase in the average F-score (from 62.0
to 68.5) over a collection of 17 typologically diverse set of target languages.
We release our source code for replicating our experiments at
https://github.com/begab/sparsity_makes_sense.