---
layout: publication
title: Multi-task Contrastive Learning For 8192-token Bilingual Text Embeddings
authors: "Isabelle Mohr, Markus Krimmel, Saba Sturua, Mohammad Kalim Akram, Andreas\
  \ Koukounas, Michael G\xFCnther, Georgios Mastrapas, Vinit Ravishankar, Joan Fontanals\
  \ Mart\xEDnez, Feng Wang, Qi Liu, Ziniu Yu, Jie Fu, Saahil Ognawala, Susana Guzman,\
  \ Bo Wang, Maximilian Werk, Nan Wang, Han Xiao"
conference: Arxiv
year: 2024
bibkey: mohr2024multi
citations: 3
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2402.17016'}]
tags: ["Evaluation", "Self-Supervised", "Text Retrieval"]
short_authors: Mohr et al.
---
We introduce a novel suite of state-of-the-art bilingual text embedding
models that are designed to support English and another target language. These
models are capable of processing lengthy text inputs with up to 8192 tokens,
making them highly versatile for a range of natural language processing tasks
such as text retrieval, clustering, and semantic textual similarity (STS)
calculations.
  By focusing on bilingual models and introducing a unique multi-task learning
objective, we have significantly improved the model performance on STS tasks,
which outperforms the capabilities of existing multilingual models in both
target language understanding and cross-lingual evaluation tasks. Moreover, our
bilingual models are more efficient, requiring fewer parameters and less memory
due to their smaller vocabulary needs. Furthermore, we have expanded the
Massive Text Embedding Benchmark (MTEB) to include benchmarks for German and
Spanish embedding models. This integration aims to stimulate further research
and advancement in text embedding technologies for these languages.