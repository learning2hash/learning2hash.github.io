---
layout: publication
title: Pb-hash Partitioned B-bit Hashing
authors: Li Ping, Zhao Weijie
conference: "Arxiv"
year: 2023
bibkey: li2023pb
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2306.15944"}
tags: ['ARXIV', 'Deep Learning', 'Supervised']
---
<p>Many hashing algorithms including minwise hashing (MinHash), one
permutation hashing (OPH), and consistent weighted sampling (CWS)
generate integers of <span class="math inline">\(B\)</span> bits. With
<span class="math inline">\(k\)</span> hashes for each data vector, the
storage would be <span class="math inline">\(B\times k\)</span> bits;
and when used for large-scale learning, the model size would be <span
class="math inline">\(2^B\times k\)</span>, which can be expensive. A
standard strategy is to use only the lowest <span
class="math inline">\(b\)</span> bits out of the <span
class="math inline">\(B\)</span> bits and somewhat increase <span
class="math inline">\(k\)</span>, the number of hashes. In this study,
we propose to re-use the hashes by partitioning the <span
class="math inline">\(B\)</span> bits into <span
class="math inline">\(m\)</span> chunks, e.g., <span
class="math inline">\(b\times m =B\)</span>. Correspondingly, the model
size becomes <span class="math inline">\(m\times 2^b \times k\)</span>,
which can be substantially smaller than the original <span
class="math inline">\(2^B\times k\)</span>. Our theoretical analysis
reveals that by partitioning the hash values into <span
class="math inline">\(m\)</span> chunks, the accuracy would drop. In
other words, using <span class="math inline">\(m\)</span> chunks of
<span class="math inline">\(B/m\)</span> bits would not be as accurate
as directly using <span class="math inline">\(B\)</span> bits. This is
due to the correlation from re-using the same hash. On the other hand,
our analysis also shows that the accuracy would not drop much for
(e.g.,) <span class="math inline">\(m=2\sim 4\)</span>. In some regions,
Pb-Hash still works well even for <span class="math inline">\(m\)</span>
much larger than 4. We expect Pb-Hash would be a good addition to the
family of hashing methods/applications and benefit industrial
practitioners. We verify the effectiveness of Pb-Hash in machine
learning tasks, for linear SVM models as well as deep learning models.
Since the hashed data are essentially categorical (ID) features, we
follow the standard practice of using embedding tables for each hash.
With Pb-Hash, we need to design an effective strategy to combine <span
class="math inline">\(m\)</span> embeddings. Our study provides an
empirical evaluation on four pooling schemes: concatenation, max
pooling, mean pooling, and product pooling. There is no definite answer
which pooling would be always better and we leave that for future
study.</p>
