---
    layout: publication
    title: "Coupled CycleGAN: Unsupervised Hashing Network for Cross-Modal Retrieval"
    authors: Li Chao, Deng Cheng, Wang Lei, Xie De, Liu Xianglong
    conference: Arxiv
    year: 2019
    bibkey: li2019coupled
    additional_links:
       - {name: "License", url: "http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}
   - {name: "Paper", url: "https://arxiv.org/abs/1903.02149"}
    tags: ['Deep Learning', 'Arxiv', 'Unsupervised', 'GAN', 'Cross-Modal', 'Supervised']
    ---
    In recent years, hashing has attracted more and more attention owing to its superior capacity of low storage cost and high query efficiency in large-scale cross-modal retrieval. Benefiting from deep leaning, continuously compelling results in cross-modal retrieval community have been achieved. However, existing deep cross-modal hashing methods either rely on amounts of labeled information or have no ability to learn an accuracy correlation between different modalities. In this paper, we proposed Unsupervised coupled Cycle generative adversarial Hashing networks (UCH), for cross-modal retrieval, where outer-cycle network is used to learn powerful common representation, and inner-cycle network is explained to generate reliable hash codes. Specifically, our proposed UCH seamlessly couples these two networks with generative adversarial mechanism, which can be optimized simultaneously to learn representation and hash codes. Extensive experiments on three popular benchmark datasets show that the proposed UCH outperforms the state-of-the-art unsupervised cross-modal hashing methods.