---
layout: publication
title: 'Coupled Cyclegan: Unsupervised Hashing Network For Cross-modal Retrieval'
authors: Chao Li, Cheng Deng, Lei Wang, De Xie, Xianglong Liu
conference: Arxiv
year: 2019
citations: 88
bibkey: li2019coupled
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1903.02149'}]
tags: [Unsupervised, Deep Hashing, Multi-Modal Hashing, Hashing Methods, Evaluation
    Metrics]
---
In recent years, hashing has attracted more and more attention owing to its
superior capacity of low storage cost and high query efficiency in large-scale
cross-modal retrieval. Benefiting from deep leaning, continuously compelling
results in cross-modal retrieval community have been achieved. However,
existing deep cross-modal hashing methods either rely on amounts of labeled
information or have no ability to learn an accuracy correlation between
different modalities. In this paper, we proposed Unsupervised coupled Cycle
generative adversarial Hashing networks (UCH), for cross-modal retrieval, where
outer-cycle network is used to learn powerful common representation, and
inner-cycle network is explained to generate reliable hash codes. Specifically,
our proposed UCH seamlessly couples these two networks with generative
adversarial mechanism, which can be optimized simultaneously to learn
representation and hash codes. Extensive experiments on three popular benchmark
datasets show that the proposed UCH outperforms the state-of-the-art
unsupervised cross-modal hashing methods.