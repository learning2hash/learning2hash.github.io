---
layout: publication
title: Uncertainty-based Cross-modal Retrieval With Probabilistic Representations
authors: Leila Pishdad, Ran Zhang, Konstantinos G. Derpanis, Allan Jepson, Afsaneh
  Fazly
conference: Arxiv
year: 2022
bibkey: pishdad2022uncertainty
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2204.09268'}]
tags: ["Evaluation", "Image Retrieval", "Multimodal Retrieval"]
short_authors: Pishdad et al.
---
Probabilistic embeddings have proven useful for capturing polysemous word
meanings, as well as ambiguity in image matching. In this paper, we study the
advantages of probabilistic embeddings in a cross-modal setting (i.e., text and
images), and propose a simple approach that replaces the standard vector point
embeddings in extant image-text matching models with probabilistic
distributions that are parametrically learned. Our guiding hypothesis is that
the uncertainty encoded in the probabilistic embeddings captures the
cross-modal ambiguity in the input instances, and that it is through capturing
this uncertainty that the probabilistic models can perform better at downstream
tasks, such as image-to-text or text-to-image retrieval. Through extensive
experiments on standard and new benchmarks, we show a consistent advantage for
probabilistic representations in cross-modal retrieval, and validate the
ability of our embeddings to capture uncertainty.