---
layout: publication
title: Is Multimodal Vision Supervision Beneficial To Language?
authors: Avinash Madasu, Vasudev Lal
conference: 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops
  (CVPRW)
year: 2023
bibkey: madasu2023is
citations: 1
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2302.05016'}]
tags: [Video Retrieval, Evaluation, Supervised, Image Retrieval, CVPR, Unsupervised]
short_authors: Avinash Madasu, Vasudev Lal
---
Vision (image and video) - Language (VL) pre-training is the recent popular
paradigm that achieved state-of-the-art results on multi-modal tasks like
image-retrieval, video-retrieval, visual question answering etc. These models
are trained in an unsupervised way and greatly benefit from the complementary
modality supervision. In this paper, we explore if the language representations
trained using vision supervision perform better than vanilla language
representations on Natural Language Understanding and commonsense reasoning
benchmarks. We experiment with a diverse set of image-text models such as
ALBEF, BLIP, METER and video-text models like ALPRO, Frozen-in-Time (FiT),
VIOLET. We compare the performance of language representations of stand-alone
text encoders of these models to the language representations of text encoders
learnt through vision supervision. Our experiments suggest that vanilla
language representations show superior performance on most of the tasks. These
results shed light on the current drawbacks of the vision-language models.