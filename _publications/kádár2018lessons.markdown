---
layout: publication
title: Lessons Learned In Multilingual Grounded Language Learning
authors: "\xC1kos K\xE1d\xE1r, Desmond Elliott, Marc-Alexandre C\xF4t\xE9, Grzegorz\
  \ Chrupa\u0142a, Afra Alishahi"
conference: Proceedings of the 22nd Conference on Computational Natural Language Learning
year: 2018
bibkey: "k\xE1d\xE1r2018lessons"
citations: 22
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1809.07615'}]
tags: ["Evaluation"]
short_authors: "K\xE1d\xE1r et al."
---
Recent work has shown how to learn better visual-semantic embeddings by
leveraging image descriptions in more than one language. Here, we investigate
in detail which conditions affect the performance of this type of grounded
language learning model. We show that multilingual training improves over
bilingual training, and that low-resource languages benefit from training with
higher-resource languages. We demonstrate that a multilingual model can be
trained equally well on either translations or comparable sentence pairs, and
that annotating the same set of images in multiple language enables further
improvements via an additional caption-caption ranking objective.