---
layout: publication
title: BERT Pre-training Of Deep Bidirectional Transformers For Language Understanding
authors: Jacob Devlin, Ming-wei Chang, Kenton Lee, Kristina Toutanova
conference: "Arxiv"
year: 2018
bibkey: devlin2018bert
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/1810.04805v2"}
tags: ['ARXIV']
---
We introduce a new language representation model called BERT which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks such as question answering and language inference without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks including pushing the GLUE score to 80.537; (7.737; point absolute improvement) MultiNLI accuracy to 86.737; (4.637; absolute improvement) SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).
