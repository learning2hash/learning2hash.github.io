---
layout: publication
title: 'Universal Perceptual Grouping'
authors: Ke Li et al.
conference: "Arxiv"
year: 2018
citations: 0
bibkey: li2018universal
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/1808.02312'}
tags: ['Cross-Modal', 'Deep', 'Retrieval Models', 'Datasets', 'Vector Indexing', 'Supervised', 'Applications']
---
In this work we aim to develop a universal sketch grouper. That is, a grouper
that can be applied to sketches of any category in any domain to group
constituent strokes/segments into semantically meaningful object parts. The
first obstacle to this goal is the lack of large-scale datasets with grouping
annotation. To overcome this, we contribute the largest sketch perceptual
grouping (SPG) dataset to date, consisting of 20,000 unique sketches evenly
distributed over 25 object categories. Furthermore, we propose a novel deep
universal perceptual grouping model. The model is learned with both generative
and discriminative losses. The generative losses improve the generalisation
ability of the model to unseen object categories and datasets. The
discriminative losses include a local grouping loss and a novel global grouping
loss to enforce global grouping consistency. We show that the proposed model
significantly outperforms the state-of-the-art groupers. Further, we show that
our grouper is useful for a number of sketch analysis tasks including sketch
synthesis and fine-grained sketch-based image retrieval (FG-SBIR).
