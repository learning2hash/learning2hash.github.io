---
layout: publication
title: Working Hard To Know Your Neighbors Margins Local Descriptor Learning Loss
authors: Anastasiia Mishchuk, Dmytro Mishkin, Filip Radenovic, Jiri Matas
conference: "Neural Information Processing Systems"
year: 2017
bibkey: mishchuk2017working
additional_links:
  - {name: "Paper", url: "https://papers.nips.cc/paper/2017/hash/831caa1b600f852b7844499430ecac17-Abstract.html"}
tags: ['CNN', 'NEURIPS']
---
We introduce a loss for metric learning which is inspired by the Lowes matching criterion for SIFT. We show that the proposed loss that maximizes the distance between the closest positive and closest negative example in the batch is better than complex regularization methods; it works well for both shallow and deep convolution network architectures. Applying the novel loss to the L2Net CNN architecture results in a compact descriptor named HardNet. It has the same dimensionality as SIFT (128) and shows state45;of45;art performance in wide baseline stereo patch verification and instance retrieval benchmarks.
