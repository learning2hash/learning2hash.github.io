---
layout: publication
title: Tiny Transformers Excel At Sentence Compression
authors: Peter Belcak, Roger Wattenhofer
conference: Arxiv
year: 2024
bibkey: belcak2024tiny
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2410.23510'}]
tags: ["Efficiency", "Transformer Based ANN"]
short_authors: Peter Belcak, Roger Wattenhofer
---
It is staggering that words of the English language, which are on average
represented by 5--6 bytes of ASCII, require as much as 24 kilobytes when served
to large language models. We show that there is room for more information in
every token embedding. We demonstrate that 1--3-layer transformers are capable
of encoding and subsequently decoding standard English sentences into as little
as a single 3-kilobyte token. Our work implies that even small networks can
learn to construct valid English sentences and suggests the possibility of
optimising large language models by moving from sub-word token embeddings
towards larger fragments of text.