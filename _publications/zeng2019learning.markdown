---
layout: publication
title: Learning Joint Embedding For Cross-modal Retrieval
authors: Donghuo Zeng
conference: 2019 International Conference on Data Mining Workshops (ICDMW)
year: 2019
bibkey: zeng2019learning
citations: 6
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1908.07673'}]
tags: [Supervised, Multimodal Retrieval]
short_authors: Donghuo Zeng
---
A cross-modal retrieval process is to use a query in one modality to obtain
relevant data in another modality. The challenging issue of cross-modal
retrieval lies in bridging the heterogeneous gap for similarity computation,
which has been broadly discussed in image-text, audio-text, and video-text
cross-modal multimedia data mining and retrieval. However, the gap in temporal
structures of different data modalities is not well addressed due to the lack
of alignment relationship between temporal cross-modal structures. Our research
focuses on learning the correlation between different modalities for the task
of cross-modal retrieval. We have proposed an architecture: Supervised-Deep
Canonical Correlation Analysis (S-DCCA), for cross-modal retrieval. In this
forum paper, we will talk about how to exploit triplet neural networks (TNN) to
enhance the correlation learning for cross-modal retrieval. The experimental
result shows the proposed TNN-based supervised correlation learning
architecture can get the best result when the data representation extracted by
supervised learning.