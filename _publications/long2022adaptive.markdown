---
layout: publication
title: Adaptive Asymmetric Label-guided Hashing For Multimedia Search
authors: Long Yitian
conference: "Arxiv"
year: 2022
bibkey: long2022adaptive
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2207.12625"}
tags: ['ARXIV', 'Cross Modal', 'Quantisation', 'Supervised']
---
<p>With the rapid growth of multimodal media data on the Web in recent
years, hash learning methods as a way to achieve efficient and flexible
cross-modal retrieval of massive multimedia data have received a lot of
attention from the current Web resource retrieval research community.
Existing supervised hashing methods simply transform label information
into pairwise similarity information to guide hash learning, leading to
a potential risk of semantic error in the face of multi-label data. In
addition, most existing hash optimization methods solve NP-hard
optimization problems by employing approximate approximation strategies
based on relaxation strategies, leading to a large quantization error.
In order to address above obstacles, we present a simple yet efficient
Adaptive Asymmetric Label-guided Hashing, named A2LH, for Multimedia
Search. Specifically, A2LH is a two-step hashing method. In the first
step, we design an association representation model between the
different modality representations and semantic label representation
separately, and use the semantic label representation as an intermediate
bridge to solve the semantic gap existing between different modalities.
In addition, we present an efficient discrete optimization algorithm for
solving the quantization error problem caused by relaxation-based
optimization algorithms. In the second step, we leverage the generated
hash codes to learn the hash mapping functions. The experimental results
show that our proposed method achieves optimal performance on all
compared baseline methods.</p>
