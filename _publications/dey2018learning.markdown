---
layout: publication
title: Learning Cross-modal Deep Embeddings For Multi-object Image Retrieval Using
  Text And Sketch
authors: "Sounak Dey, Anjan Dutta, Suman K. Ghosh, Ernest Valveny, Josep Llad\xF3\
  s, Umapada Pal"
conference: 2018 24th International Conference on Pattern Recognition (ICPR)
year: 2018
bibkey: dey2018learning
citations: 3
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1804.10819'}]
tags: ["Datasets", "Image Retrieval"]
short_authors: Dey et al.
---
In this work we introduce a cross modal image retrieval system that allows
both text and sketch as input modalities for the query. A cross-modal deep
network architecture is formulated to jointly model the sketch and text input
modalities as well as the the image output modality, learning a common
embedding between text and images and between sketches and images. In addition,
an attention model is used to selectively focus the attention on the different
objects of the image, allowing for retrieval with multiple objects in the
query. Experiments show that the proposed method performs the best in both
single and multiple object image retrieval in standard datasets.