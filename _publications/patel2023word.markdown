---
layout: publication
title: Word Embeddings For Banking Industry
authors: Avnish Patel
conference: Arxiv
year: 2023
bibkey: patel2023word
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2306.01807'}]
tags: []
short_authors: Avnish Patel
---
Applications of Natural Language Processing (NLP) are plentiful, from
sentiment analysis to text classification. Practitioners rely on static word
embeddings (e.g. Word2Vec or GloVe) or static word representation from
contextual models (e.g. BERT or ELMo) to perform many of these NLP tasks. These
widely available word embeddings are built from large amount of text, so they
are likely to have captured most of the vocabulary in different context.
However, how well would they capture domain-specific semantics and word
relatedness? This paper explores this idea by creating a bank-specific word
embeddings and evaluates them against other sources of word embeddings such as
GloVe and BERT. Not surprising that embeddings built from bank-specific corpora
does a better job of capturing the bank-specific semantics and word
relatedness. This finding suggests that bank-specific word embeddings could be
a good stand-alone source or a complement to other widely available embeddings
when performing NLP tasks specific to the banking industry.