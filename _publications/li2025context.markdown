---
layout: publication
title: Context-efficient Retrieval With Factual Decomposition
authors: Yanhong Li, David Yunis, David McAllester, Jiawei Zhou
conference: 'Proceedings of the 2025 Conference of the Nations of the Americas Chapter
  of the Association for Computational Linguistics: Human Language Technologies (Volume
  2: Short Papers)'
year: 2025
bibkey: li2025context
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2503.19574'}]
tags: ["Efficiency", "Similarity Search"]
short_authors: Li et al.
---
There has recently been considerable interest in incorporating information
retrieval into large language models (LLMs). Retrieval from a dynamically
expanding external corpus of text allows a model to incorporate current events
and can be viewed as a form of episodic memory. Here we demonstrate that
pre-processing the external corpus into semi-structured ''atomic facts'' makes
retrieval more efficient. More specifically, we demonstrate that our particular
form of atomic facts improves performance on various question answering tasks
when the amount of retrieved text is limited. Limiting the amount of retrieval
reduces the size of the context and improves inference efficiency.