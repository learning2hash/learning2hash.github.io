---
layout: publication
title: 'NFT1000: A Cross-modal Dataset For Non-fungible Token Retrieval'
authors: Shuxun Wang, Yunfei Lei, Ziqi Zhang, Wei Liu, Haowei Liu, Li Yang, Wenjuan
  Li, Bing Li, Weiming Hu
conference: Proceedings of the 32nd ACM International Conference on Multimedia
year: 2024
bibkey: wang2024nft1000
citations: 0
additional_links: [{name: Code, url: 'https://github.com/ShuxunoO/NFT-Net.git'}, {
    name: Paper, url: 'https://arxiv.org/abs/2402.16872'}]
tags: ["Datasets", "Evaluation"]
short_authors: Wang et al.
---
With the rise of "Metaverse" and "Web 3.0", Non-Fungible Token (NFT) has
emerged as a kind of pivotal digital asset, garnering significant attention. By
the end of March 2024, more than 1.7 billion NFTs have been minted across
various blockchain platforms. To effectively locate a desired NFT, conducting
searches within a vast array of NFTs is essential. The challenge in NFT
retrieval is heightened due to the high degree of similarity among different
NFTs, regarding regional and semantic aspects. In this paper, we will introduce
a benchmark dataset named "NFT Top1000 Visual-Text Dataset" (NFT1000),
containing 7.56 million image-text pairs, and being collected from 1000 most
famous PFP1 NFT collections2 by sales volume on the Ethereum blockchain. Based
on this dataset and leveraging the CLIP series of pre-trained models as our
foundation, we propose the dynamic masking fine-tuning scheme. This innovative
approach results in a 7.4% improvement in the top1 accuracy rate, while
utilizing merely 13% of the total training data (0.79 million vs. 6.1
million). We also propose a robust metric Comprehensive Variance Index (CVI) to
assess the similarity and retrieval difficulty of visual-text pairs data. The
dataset will be released as an open-source resource. For more details, please
refer to: https://github.com/ShuxunoO/NFT-Net.git.