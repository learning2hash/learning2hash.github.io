---
layout: publication
title: Answering Visual-relational Queries In Web-extracted Knowledge Graphs
authors: "Daniel O\xF1oro-Rubio, Mathias Niepert, Alberto Garc\xEDa-Dur\xE1n, Roberto\
  \ Gonz\xE1lez, Roberto J. L\xF3pez-Sastre"
conference: AKBC2019
year: 2017
bibkey: "o\xF1ororubio2017answering"
citations: 10
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1709.02314'}]
tags: []
short_authors: "O\xF1oro-Rubio et al."
---
A visual-relational knowledge graph (KG) is a multi-relational graph whose
entities are associated with images. We explore novel machine learning
approaches for answering visual-relational queries in web-extracted knowledge
graphs. To this end, we have created ImageGraph, a KG with 1,330 relation
types, 14,870 entities, and 829,931 images crawled from the web. With
visual-relational KGs such as ImageGraph one can introduce novel probabilistic
query types in which images are treated as first-class citizens. Both the
prediction of relations between unseen images as well as multi-relational image
retrieval can be expressed with specific families of visual-relational queries.
We introduce novel combinations of convolutional networks and knowledge graph
embedding methods to answer such queries. We also explore a zero-shot learning
scenario where an image of an entirely new entity is linked with multiple
relations to entities of an existing KG. The resulting multi-relational
grounding of unseen entity images into a knowledge graph serves as a semantic
entity representation. We conduct experiments to demonstrate that the proposed
methods can answer these visual-relational queries efficiently and accurately.