---
layout: publication
title: 'Svt-net: Super Light-weight Sparse Voxel Transformer For Large Scale Place
  Recognition'
authors: Zhaoxin Fan, Zhenbo Song, Hongyan Liu, Zhiwu Lu, Jun He, Xiaoyong Du
conference: Proceedings of the AAAI Conference on Artificial Intelligence
year: 2022
bibkey: fan2021svt
citations: 54
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2105.00149'}]
tags: ["AAAI"]
short_authors: Fan et al.
---
Point cloud-based large scale place recognition is fundamental for many
applications like Simultaneous Localization and Mapping (SLAM). Although many
models have been proposed and have achieved good performance by learning
short-range local features, long-range contextual properties have often been
neglected. Moreover, the model size has also become a bottleneck for their wide
applications. To overcome these challenges, we propose a super light-weight
network model termed SVT-Net for large scale place recognition. Specifically,
on top of the highly efficient 3D Sparse Convolution (SP-Conv), an Atom-based
Sparse Voxel Transformer (ASVT) and a Cluster-based Sparse Voxel Transformer
(CSVT) are proposed to learn both short-range local features and long-range
contextual features in this model. Consisting of ASVT and CSVT, SVT-Net can
achieve state-of-the-art on benchmark datasets in terms of both accuracy and
speed with a super-light model size (0.9M). Meanwhile, two simplified versions
of SVT-Net are introduced, which also achieve state-of-the-art and further
reduce the model size to 0.8M and 0.4M respectively.