---
layout: publication
title: 'SCRATCH: A Scalable Discrete Matrix Factorization Hashing For Cross-modal
  Retrieval'
authors: Chuan-xiang, Chen, Zhang, Luo, Nie, Zhang, Xu
conference: IEEE Transactions on Circuits and Systems for Video Technology
year: 2019
bibkey: 2019scratch
citations: 110
additional_links: [{name: Paper, url: 'https://dl.acm.org/citation.cfm?id=3240547'}]
tags: ["Compact Codes", "Evaluation", "Hashing Methods", "Multimodal Retrieval", "Quantization", "Scalability", "Supervised", "Unsupervised"]
short_authors: Chuan-xiang et al.
---
In recent years, many hashing methods have been proposed for the cross-modal retrieval task. However, there are still some issues that need to be further explored. For example, some of them relax the binary constraints to generate the hash codes, which may generate large quantization error. Although some discrete schemes have been proposed, most of them are time-consuming. In addition, most of the existing supervised hashing methods use an n x n similarity matrix during the optimization, making them unscalable. To address these issues, in this paper, we present a novel supervised cross-modal hashing method---Scalable disCRete mATrix faCtorization Hashing, SCRATCH for short. It leverages the collective matrix factorization on the kernelized features and the semantic embedding with labels to find a latent semantic space to preserve the intra- and inter-modality similarities. In addition, it incorporates the label matrix instead of the similarity matrix into the loss function. Based on the proposed loss function and the iterative optimization algorithm, it can learn the hash functions and binary codes simultaneously. Moreover, the binary codes can be generated discretely, reducing the quantization error generated by the relaxation scheme. Its time complexity is linear to the size of the dataset, making it scalable to large-scale datasets. Extensive experiments on three benchmark datasets, namely, Wiki, MIRFlickr-25K, and NUS-WIDE, have verified that our proposed SCRATCH model outperforms several state-of-the-art unsupervised and supervised hashing methods for cross-modal retrieval.