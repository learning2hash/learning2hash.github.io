---
layout: publication
title: 'Sketchtriplet: Self-supervised Scenarized Sketch-text-image Triplet Generation'
authors: Zhenbei Wu, Qiang Wang, Jie Yang
conference: Arxiv
year: 2024
bibkey: wu2024sketchtriplet
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2405.18801'}]
tags: ["Datasets", "Few Shot & Zero Shot", "Image Retrieval", "Self-Supervised", "Supervised"]
short_authors: Zhenbei Wu, Qiang Wang, Jie Yang
---
The scarcity of free-hand sketch presents a challenging problem. Despite the
emergence of some large-scale sketch datasets, these datasets primarily consist
of sketches at the single-object level. There continues to be a lack of
large-scale paired datasets for scene sketches. In this paper, we propose a
self-supervised method for scene sketch generation that does not rely on any
existing scene sketch, enabling the transformation of single-object sketches
into scene sketches. To accomplish this, we introduce a method for vector
sketch captioning and sketch semantic expansion. Additionally, we design a
sketch generation network that incorporates a fusion of multi-modal perceptual
constraints, suitable for application in zero-shot image-to-sketch downstream
task, demonstrating state-of-the-art performance through experimental
validation. Finally, leveraging our proposed sketch-to-sketch generation
method, we contribute a large-scale dataset centered around scene sketches,
comprising highly semantically consistent "text-sketch-image" triplets. Our
research confirms that this dataset can significantly enhance the capabilities
of existing models in sketch-based image retrieval and sketch-controlled image
synthesis tasks. We will make our dataset and code publicly available.