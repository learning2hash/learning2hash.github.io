---
layout: publication
title: A Light-weight Transformer-based Self-supervised Matching Network For Heterogeneous
  Images
authors: Wang Zhang, Tingting Li, Yuntian Zhang, Gensheng Pei, Xiruo Jiang, Yazhou
  Yao
conference: Arxiv
year: 2024
bibkey: zhang2024light
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2404.19311'}]
tags: ["Self-Supervised"]
short_authors: Zhang et al.
---
Matching visible and near-infrared (NIR) images remains a significant
challenge in remote sensing image fusion. The nonlinear radiometric differences
between heterogeneous remote sensing images make the image matching task even
more difficult. Deep learning has gained substantial attention in computer
vision tasks in recent years. However, many methods rely on supervised learning
and necessitate large amounts of annotated data. Nevertheless, annotated data
is frequently limited in the field of remote sensing image matching. To address
this challenge, this paper proposes a novel keypoint descriptor approach that
obtains robust feature descriptors via a self-supervised matching network. A
light-weight transformer network, termed as LTFormer, is designed to generate
deep-level feature descriptors. Furthermore, we implement an innovative triplet
loss function, LT Loss, to enhance the matching performance further. Our
approach outperforms conventional hand-crafted local feature descriptors and
proves equally competitive compared to state-of-the-art deep learning-based
methods, even amidst the shortage of annotated data.