---
layout: publication
title: Unsupervised Multilingual Word Embeddings
authors: Xilun Chen, Claire Cardie
conference: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
  Processing
year: 2018
bibkey: chen2018unsupervised
citations: 153
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1808.08933'}]
tags: ["EMNLP", "Unsupervised"]
short_authors: Xilun Chen, Claire Cardie
---
Multilingual Word Embeddings (MWEs) represent words from multiple languages
in a single distributional vector space. Unsupervised MWE (UMWE) methods
acquire multilingual embeddings without cross-lingual supervision, which is a
significant advantage over traditional supervised approaches and opens many new
possibilities for low-resource languages. Prior art for learning UMWEs,
however, merely relies on a number of independently trained Unsupervised
Bilingual Word Embeddings (UBWEs) to obtain multilingual embeddings. These
methods fail to leverage the interdependencies that exist among many languages.
To address this shortcoming, we propose a fully unsupervised framework for
learning MWEs that directly exploits the relations between all language pairs.
Our model substantially outperforms previous approaches in the experiments on
multilingual word translation and cross-lingual word similarity. In addition,
our model even beats supervised approaches trained with cross-lingual
resources.