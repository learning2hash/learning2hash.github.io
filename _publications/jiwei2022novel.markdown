---
layout: publication
title: A Novel Image Descriptor With Aggregated Semantic Skeleton Representation For
  Long-term Visual Place Recognition
authors: Nie Jiwei, Feng Joe-Mei, Xue Dingyu, Pan Feng, Liu Wei, Hu Jun, Cheng Shuai
conference: 2022 26th International Conference on Pattern Recognition (ICPR)
year: 2022
bibkey: jiwei2022novel
citations: 1
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2202.03677'}]
tags: []
short_authors: Jiwei et al.
---
In a Simultaneous Localization and Mapping (SLAM) system, a loop-closure can
eliminate accumulated errors, which is accomplished by Visual Place Recognition
(VPR), a task that retrieves the current scene from a set of pre-stored
sequential images through matching specific scene-descriptors. In urban scenes,
the appearance variation caused by seasons and illumination has brought great
challenges to the robustness of scene descriptors. Semantic segmentation images
can not only deliver the shape information of objects but also their categories
and spatial relations that will not be affected by the appearance variation of
the scene. Innovated by the Vector of Locally Aggregated Descriptor (VLAD), in
this paper, we propose a novel image descriptor with aggregated semantic
skeleton representation (SSR), dubbed SSR-VLAD, for the VPR under drastic
appearance-variation of environments. The SSR-VLAD of one image aggregates the
semantic skeleton features of each category and encodes the spatial-temporal
distribution information of the image semantic information. We conduct a series
of experiments on three public datasets of challenging urban scenes. Compared
with four state-of-the-art VPR methods- CoHOG, NetVLAD, LOST-X, and
Region-VLAD, VPR by matching SSR-VLAD outperforms those methods and maintains
competitive real-time performance at the same time.