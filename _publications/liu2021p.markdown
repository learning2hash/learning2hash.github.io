---
layout: publication
title: P-tuning V2 Prompt Tuning Can Be Comparable To Fine-tuning Universally Across Scales And Tasks
authors: Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, Jie Tang
conference: "Arxiv"
year: 2021
bibkey: liu2021p
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2110.07602v3"}
  - {name: "Code", url: "https://github.com/THUDM/P-tuning-v2"}
tags: ['ARXIV', 'Has Code']
---
Prompt tuning which only tunes continuous prompts with a frozen language model substantially reduces per-task storage and memory usage at training. However in the context of NLU prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.137;-337; tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning citeli2021prefixqin2021learning optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2 we believe it can serve as an alternative to finetuning and a strong baseline for future research.Our code and data are released at https://github.com/THUDM/P-tuning-v2.
