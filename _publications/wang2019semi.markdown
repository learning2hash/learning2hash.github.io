---
layout: publication
title: Semi-supervised Learning With Contrastive Predicative Coding
authors: Jiaxing Wang, Yin Zheng, Xiaoshuang Chen, Junzhou Huang, Jian Cheng
conference: Arxiv
year: 2019
bibkey: wang2019semi
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1905.10514'}]
tags: [Evaluation, Supervised, Neural Hashing, Datasets, Scalability, Tools & Libraries]
short_authors: Wang et al.
---
Semi-supervised learning (SSL) provides a powerful framework for leveraging
unlabeled data when labels are limited or expensive to obtain. SSL algorithms
based on deep neural networks have recently proven successful on standard
benchmark tasks. However, many of them have thus far been either inflexible,
inefficient or non-scalable. This paper explores recently developed contrastive
predictive coding technique to improve discriminative power of deep learning
models when a large portion of labels are absent. Two models, cpc-SSL and a
class conditional variant~(ccpc-SSL) are presented. They effectively exploit
the unlabeled data by extracting shared information between different parts of
the (high-dimensional) data. The proposed approaches are inductive, and scale
well to very large datasets like ImageNet, making them good candidates in
real-world large scale applications.