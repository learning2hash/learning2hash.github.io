---
layout: publication
title: CLIP Multi45;modal Hashing A New Baseline CLIPMH
authors: Zhu Jian, Sheng Mingkai, Ke Mingda, Huang Zhangmin, Chang Jingfei
conference: "Arxiv"
year: 2023
bibkey: zhu2023clip
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2308.11797"}
tags: ['ARXIV', 'Unsupervised']
---
The multi45;modal hashing method is widely used in multimedia retrieval. It can fuse multi45;source data to generate binary hash code. However the current multi45;modal methods have the problem of low retrieval accuracy. The reason is that the individual backbone networks have limited feature expression capabilities and are not jointly pre45;trained on large45;scale unsupervised multi45;modal data. To solve this problem we propose a new baseline CLIP Multi45;modal Hashing (CLIPMH) method. It uses CLIP model to extract text and image features and then fuse to generate hash code. CLIP improves the expressiveness of each modal feature. In this way it can greatly improve the retrieval performance of multi45;modal hashing methods. In comparison to state45;of45;the45;art unsupervised and supervised multi45;modal hashing methods experiments reveal that the proposed CLIPMH can significantly enhance performance (Maximum increase of 8.3837;). CLIP also has great advantages over the text and visual backbone networks commonly used before.
