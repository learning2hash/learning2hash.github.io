---
layout: publication
title: Cross-modal Contrastive Learning With Asymmetric Co-attention Network For Video
  Moment Retrieval
authors: Love Panta, Prashant Shrestha, Brabeem Sapkota, Amrita Bhattarai, Suresh
  Manandhar, Anand Kumar Sah
conference: Arxiv
year: 2023
bibkey: panta2023cross
citations: 1
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2312.07435'}]
tags: ["Datasets", "Distance Metric Learning", "Evaluation", "Self-Supervised"]
short_authors: Panta et al.
---
Video moment retrieval is a challenging task requiring fine-grained
interactions between video and text modalities. Recent work in image-text
pretraining has demonstrated that most existing pretrained models suffer from
information asymmetry due to the difference in length between visual and
textual sequences. We question whether the same problem also exists in the
video-text domain with an auxiliary need to preserve both spatial and temporal
information. Thus, we evaluate a recently proposed solution involving the
addition of an asymmetric co-attention network for video grounding tasks.
Additionally, we incorporate momentum contrastive loss for robust,
discriminative representation learning in both modalities. We note that the
integration of these supplementary modules yields better performance compared
to state-of-the-art models on the TACoS dataset and comparable results on
ActivityNet Captions, all while utilizing significantly fewer parameters with
respect to baseline.