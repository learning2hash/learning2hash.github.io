---
layout: publication
title: The Nt-xent Loss Upper Bound
authors: "Wilhelm \xC5gren"
conference: Arxiv
year: 2022
bibkey: "\xE5gren2022the"
citations: 2
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2205.03169'}]
tags: ["Evaluation", "Self-Supervised", "Supervised", "Tools & Libraries"]
short_authors: "Wilhelm \xC5gren"
---
Self-supervised learning is a growing paradigm in deep representation
learning, showing great generalization capabilities and competitive performance
in low-labeled data regimes. The SimCLR framework proposes the NT-Xent loss for
contrastive representation learning. The objective of the loss function is to
maximize agreement, similarity, between sampled positive pairs. This short
paper derives and proposes an upper bound for the loss and average similarity.
An analysis of the implications is however not provided, but we strongly
encourage anyone in the field to conduct this.