---
layout: publication
title: CLIP Multi-modal Hashing For Multimedia Retrieval
authors: Zhu Jian, Sheng Mingkai, Huang Zhangmin, Chang Jingfei, Jiang Jinling, Long
  Jian, Luo Cheng, Liu Lei
conference: IEEE Transactions on Circuits and Systems for Video Technology
year: 2024
bibkey: zhu2024clip
citations: 2
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2410.07783'}]
tags: ["Evaluation", "Hashing Methods", "Image Retrieval", "Multimodal Retrieval", "Scalability", "Video Retrieval"]
short_authors: Zhu et al.
---
Multi-modal hashing methods are widely used in multimedia retrieval, which
can fuse multi-source data to generate binary hash code. However, the
individual backbone networks have limited feature expression capabilities and
are not jointly pre-trained on large-scale unsupervised multi-modal data,
resulting in low retrieval accuracy. To address this issue, we propose a novel
CLIP Multi-modal Hashing (CLIPMH) method. Our method employs the CLIP framework
to extract both text and vision features and then fuses them to generate hash
code. Due to enhancement on each modal feature, our method has great
improvement in the retrieval performance of multi-modal hashing methods.
Compared with state-of-the-art unsupervised and supervised multi-modal hashing
methods, experiments reveal that the proposed CLIPMH can significantly improve
performance (a maximum increase of 8.38% in mAP).