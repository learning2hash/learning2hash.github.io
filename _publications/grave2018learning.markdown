---
layout: publication
title: Learning Word Vectors For 157 Languages
authors: Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, Tomas Mikolov
conference: Arxiv
year: 2018
bibkey: grave2018learning
citations: 608
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1802.06893'}]
tags: ["Datasets", "Evaluation"]
short_authors: Grave et al.
---
Distributed word representations, or word vectors, have recently been applied
to many tasks in natural language processing, leading to state-of-the-art
performance. A key ingredient to the successful application of these
representations is to train them on very large corpora, and use these
pre-trained models in downstream tasks. In this paper, we describe how we
trained such high quality word representations for 157 languages. We used two
sources of data to train these models: the free online encyclopedia Wikipedia
and data from the common crawl project. We also introduce three new word
analogy datasets to evaluate these word vectors, for French, Hindi and Polish.
Finally, we evaluate our pre-trained word vectors on 10 languages for which
evaluation datasets exists, showing very strong performance compared to
previous models.