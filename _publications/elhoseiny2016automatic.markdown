---
layout: publication
title: Automatic Annotation Of Structured Facts In Images
authors: Mohamed Elhoseiny, Scott Cohen, Walter Chang, Brian Price, Ahmed Elgammal
conference: Proceedings of the 5th Workshop on Vision and Language
year: 2016
bibkey: elhoseiny2016automatic
citations: 5
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1604.00466'}]
tags: ["Datasets"]
short_authors: Elhoseiny et al.
---
Motivated by the application of fact-level image understanding, we present an
automatic method for data collection of structured visual facts from images
with captions. Example structured facts include attributed objects (e.g.,
<flower, red>), actions (e.g., <baby, smile>), interactions (e.g., <man,
walking, dog>), and positional information (e.g., <vase, on, table>). The
collected annotations are in the form of fact-image pairs (e.g.,<man, walking,
dog> and an image region containing this fact). With a language approach, the
proposed method is able to collect hundreds of thousands of visual fact
annotations with accuracy of 83% according to human judgment. Our method
automatically collected more than 380,000 visual fact annotations and more than
110,000 unique visual facts from images with captions and localized them in
images in less than one day of processing time on standard CPU platforms.