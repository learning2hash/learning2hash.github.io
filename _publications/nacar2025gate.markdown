---
layout: publication
title: 'GATE: General Arabic Text Embedding For Enhanced Semantic Textual Similarity
  With Matryoshka Representation Learning And Hybrid Loss Training'
authors: Omer Nacar, Anis Koubaa, Serry Sibaee, Yasser Al-Habashi, Adel Ammar, Wadii
  Boulila
conference: Arxiv
year: 2025
bibkey: nacar2025gate
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2505.24581'}]
tags: ["Datasets", "Evaluation", "Similarity Search"]
short_authors: Nacar et al.
---
Semantic textual similarity (STS) is a critical task in natural language processing (NLP), enabling applications in retrieval, clustering, and understanding semantic relationships between texts. However, research in this area for the Arabic language remains limited due to the lack of high-quality datasets and pre-trained models. This scarcity of resources has restricted the accurate evaluation and advance of semantic similarity in Arabic text. This paper introduces General Arabic Text Embedding (GATE) models that achieve state-of-the-art performance on the Semantic Textual Similarity task within the MTEB benchmark. GATE leverages Matryoshka Representation Learning and a hybrid loss training approach with Arabic triplet datasets for Natural Language Inference, which are essential for enhancing model performance in tasks that demand fine-grained semantic understanding. GATE outperforms larger models, including OpenAI, with a 20-25% performance improvement on STS benchmarks, effectively capturing the unique semantic nuances of Arabic.