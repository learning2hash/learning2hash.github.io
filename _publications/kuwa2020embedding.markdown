---
layout: publication
title: Embedding Meta-textual Information For Improved Learning To Rank
authors: Toshitaka Kuwa, Shigehiko Schamoni, Stefan Riezler
conference: Proceedings of the 28th International Conference on Computational Linguistics
year: 2020
bibkey: kuwa2020embedding
citations: 1
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2010.16313'}]
tags: ["COLING"]
short_authors: Toshitaka Kuwa, Shigehiko Schamoni, Stefan Riezler
---
Neural approaches to learning term embeddings have led to improved
computation of similarity and ranking in information retrieval (IR). So far
neural representation learning has not been extended to meta-textual
information that is readily available for many IR tasks, for example, patent
classes in prior-art retrieval, topical information in Wikipedia articles, or
product categories in e-commerce data. We present a framework that learns
embeddings for meta-textual categories, and optimizes a pairwise ranking
objective for improved matching based on combined embeddings of textual and
meta-textual information. We show considerable gains in an experimental
evaluation on cross-lingual retrieval in the Wikipedia domain for three
language pairs, and in the Patent domain for one language pair. Our results
emphasize that the mode of combining different types of information is crucial
for model improvement.