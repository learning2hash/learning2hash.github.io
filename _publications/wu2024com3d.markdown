---
layout: publication
title: 'COM3D: Leveraging Cross-view Correspondence And Cross-modal Mining For 3D
  Retrieval'
authors: Hao Wu, Ruochong Li, Hao Wang, Hui Xiong
conference: Arxiv
year: 2024
bibkey: wu2024com3d
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2405.04103'}]
tags: ["Multimodal Retrieval"]
short_authors: Wu et al.
---
In this paper, we investigate an open research task of cross-modal retrieval
between 3D shapes and textual descriptions. Previous approaches mainly rely on
point cloud encoders for feature extraction, which may ignore key inherent
features of 3D shapes, including depth, spatial hierarchy, geometric
continuity, etc. To address this issue, we propose COM3D, making the first
attempt to exploit the cross-view correspondence and cross-modal mining to
enhance the retrieval performance. Notably, we augment the 3D features through
a scene representation transformer, to generate cross-view correspondence
features of 3D shapes, which enrich the inherent features and enhance their
compatibility with text matching. Furthermore, we propose to optimize the
cross-modal matching process based on the semi-hard negative example mining
method, in an attempt to improve the learning efficiency. Extensive
quantitative and qualitative experiments demonstrate the superiority of our
proposed COM3D, achieving state-of-the-art results on the Text2Shape dataset.