---
layout: publication
title: Word Embedding Based On Low-rank Doubly Stochastic Matrix Decomposition
authors: Denis Sedov, Zhirong Yang
conference: Lecture Notes in Computer Science
year: 2018
bibkey: sedov2018word
citations: 4
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1812.10401'}]
tags: []
short_authors: Denis Sedov, Zhirong Yang
---
Word embedding, which encodes words into vectors, is an important starting
point in natural language processing and commonly used in many text-based
machine learning tasks. However, in most current word embedding approaches, the
similarity in embedding space is not optimized in the learning. In this paper
we propose a novel neighbor embedding method which directly learns an embedding
simplex where the similarities between the mapped words are optimal in terms of
minimal discrepancy to the input neighborhoods. Our method is built upon
two-step random walks between words via topics and thus able to better reveal
the topics among the words. Experiment results indicate that our method,
compared with another existing word embedding approach, is more favorable for
various queries.