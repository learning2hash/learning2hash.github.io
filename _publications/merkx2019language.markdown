---
layout: publication
title: Language Learning Using Speech To Image Retrieval
authors: Danny Merkx, Stefan L. Frank, Mirjam Ernestus
conference: Interspeech 2019
year: 2019
bibkey: merkx2019language
citations: 50
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1909.03795'}]
tags: [Evaluation, INTERSPEECH, Image Retrieval]
short_authors: Danny Merkx, Stefan L. Frank, Mirjam Ernestus
---
Humans learn language by interaction with their environment and listening to
other humans. It should also be possible for computational models to learn
language directly from speech but so far most approaches require text. We
improve on existing neural network approaches to create visually grounded
embeddings for spoken utterances. Using a combination of a multi-layer GRU,
importance sampling, cyclic learning rates, ensembling and vectorial
self-attention our results show a remarkable increase in image-caption
retrieval performance over previous work. Furthermore, we investigate which
layers in the model learn to recognise words in the input. We find that deeper
network layers are better at encoding word presence, although the final layer
has slightly lower performance. This shows that our visually grounded sentence
encoder learns to recognise words from the input even though it is not
explicitly trained for word recognition.