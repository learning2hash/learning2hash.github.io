---
layout: publication
title: MM-REACT Prompting Chatgpt For Multimodal Reasoning And Action
authors: Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang
conference: "Arxiv"
year: 2023
bibkey: yang2023mm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2303.11381v1"}
  - {name: "Code", url: "https://multimodal-react.github.io/"}
tags: ['ARXIV', 'Cross Modal', 'Has Code', 'MM']
---
We propose MM-REACT a system paradigm that integrates ChatGPT with a pool of vision experts to achieve multimodal reasoning and action. In this paper we define and explore a comprehensive list of advanced vision tasks that are intriguing to solve but may exceed the capabilities of existing vision and vision-language models. To achieve such advanced visual intelligence MM-REACT introduces a textual prompt design that can represent text descriptions textualized spatial coordinates and aligned file names for dense visual signals such as images and videos. MM-REACTs prompt design allows language models to accept associate and process multimodal information thereby facilitating the synergetic combination of ChatGPT and various vision experts. Zero-shot experiments demonstrate MM-REACTs effectiveness in addressing the specified capabilities of interests and its wide application in different scenarios that require advanced visual understanding. Furthermore we discuss and compare MM-REACTs system paradigm with an alternative approach that extends language models for multimodal scenarios through joint finetuning. Code demo video and visualization are available at https://multimodal-react.github.io/
