---
    layout: publication
    title: "A Benchmark on Tricks for Large-scale Image Retrieval"
    authors: Ko Byungsoo, Shin Minchul, Gu Geonmo, Jun HeeJae, Lee Tae Kwan, Kim Youngjoon
    conference: Arxiv
    year: 2019
    bibkey: ko2019a
    additional_links:
       - {name: "License", url: "http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}
   - {name: "Paper", url: "https://arxiv.org/abs/1907.11854"}
    tags: ['Arxiv', 'Deep Learning', 'Image Retrieval']
    ---
    {% raw %}
    Many studies have been performed on metric learning, which has become a key ingredient in top-performing methods of instance-level image retrieval. Meanwhile, less attention has been paid to pre-processing and post-processing tricks that can significantly boost performance. Furthermore, we found that most previous studies used small scale datasets to simplify processing. Because the behavior of a feature representation in a deep learning model depends on both domain and data, it is important to understand how model behave in large-scale environments when a proper combination of retrieval tricks is used. In this paper, we extensively analyze the effect of well-known pre-processing, post-processing tricks, and their combination for large-scale image retrieval. We found that proper use of these tricks can significantly improve model performance without necessitating complex architecture or introducing loss, as confirmed by achieving a competitive result on the Google Landmark Retrieval Challenge 2019.
    {% endraw %}