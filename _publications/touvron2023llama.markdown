---
layout: publication
title: Llama Open And Efficient Foundation Language Models
authors: Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample
conference: "Arxiv"
year: 2023
bibkey: touvron2023llama
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2302.13971v1"}
tags: ['ARXIV']
---
We introduce LLaMA a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens and show that it is possible to train state-of-the-art models using publicly available datasets exclusively without resorting to proprietary and inaccessible datasets. In particular LLaMA-13B outperforms GPT-3 (175B) on most benchmarks and LLaMA-65B is competitive with the best models Chinchilla-70B and PaLM-540B. We release all our models to the research community.
