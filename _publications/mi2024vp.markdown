---
layout: publication
title: 'VP-MEL: Visual Prompts Guided Multimodal Entity Linking'
authors: Hongze Mi, Jinyuan Li, Xuying Zhang, Haoran Cheng, Jiahao Wang, di Sun, Gang
  Pan
conference: Arxiv
year: 2024
bibkey: mi2024vp
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2412.06720'}]
tags: []
short_authors: Mi et al.
---
Multimodal entity linking (MEL), a task aimed at linking mentions within
multimodal contexts to their corresponding entities in a knowledge base (KB),
has attracted much attention due to its wide applications in recent years.
However, existing MEL methods often rely on mention words as retrieval cues,
which limits their ability to effectively utilize information from both images
and text. This reliance causes MEL to struggle with accurately retrieving
entities in certain scenarios, especially when the focus is on image objects or
mention words are missing from the text. To solve these issues, we introduce a
Visual Prompts guided Multimodal Entity Linking (VP-MEL) task. Given a
text-image pair, VP-MEL aims to link a marked region (i.e., visual prompt) in
an image to its corresponding entities in the knowledge base. To facilitate
this task, we present a new dataset, VPWiki, specifically designed for VP-MEL.
Furthermore, we propose a framework named IIER, which enhances visual feature
extraction using visual prompts and leverages the pretrained Detective-VLM
model to capture latent information. Experimental results on the VPWiki dataset
demonstrate that IIER outperforms baseline methods across multiple benchmarks
for the VP-MEL task.