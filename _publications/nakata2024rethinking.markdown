---
layout: publication
title: 'Rethinking Sparse Lexical Representations For Image Retrieval In The Age Of Rising Multi-modal Large Language Models'
authors: Kengo Nakata, Daisuke Miyashita, Youyang Ng, Yasuto Hoshi, Jun Deguchi
conference: "Arxiv"
year: 2024
citations: 0
bibkey: nakata2024rethinking
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2408.16296'}
tags: ['Benchmarks and Datasets', 'Tools and Libraries', 'Evaluation Metrics', 'Applications']
---
In this paper, we rethink sparse lexical representations for image retrieval.
By utilizing multi-modal large language models (M-LLMs) that support visual
prompting, we can extract image features and convert them into textual data,
enabling us to utilize efficient sparse retrieval algorithms employed in
natural language processing for image retrieval tasks. To assist the LLM in
extracting image features, we apply data augmentation techniques for key
expansion and analyze the impact with a metric for relevance between images and
textual data. We empirically show the superior precision and recall performance
of our image retrieval method compared to conventional vision-language
model-based methods on the MS-COCO, PASCAL VOC, and NUS-WIDE datasets in a
keyword-based image retrieval scenario, where keywords serve as search queries.
We also demonstrate that the retrieval performance can be improved by
iteratively incorporating keywords into search queries.
