---
layout: publication
title: Learning To Learn Image Classifiers With Visual Analogy
authors: Linjun Zhou, Peng Cui, Shiqiang Yang, Wenwu Zhu, Qi Tian
conference: Remote Sensing
year: 2017
bibkey: zhou2017learning
citations: 139
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1710.06177'}]
tags: []
short_authors: Zhou et al.
---
Humans are far better learners who can learn a new concept very fast with
only a few samples compared with machines. The plausible mystery making the
difference is two fundamental learning mechanisms: learning to learn and
learning by analogy. In this paper, we attempt to investigate a new human-like
learning method by organically combining these two mechanisms. In particular,
we study how to generalize the classification parameters from previously
learned concepts to a new concept. we first propose a novel Visual Analogy
Graph Embedded Regression (VAGER) model to jointly learn a low-dimensional
embedding space and a linear mapping function from the embedding space to
classification parameters for base classes. We then propose an out-of-sample
embedding method to learn the embedding of a new class represented by a few
samples through its visual analogy with base classes and derive the
classification parameters for the new class. We conduct extensive experiments
on ImageNet dataset and the results show that our method could consistently and
significantly outperform state-of-the-art baselines.