---
layout: publication
title: 'Bagformer: Better Cross-modal Retrieval Via Bag-wise Interaction'
authors: Haowen Hou, Xiaopeng Yan, Yigeng Zhang, Fengzong Lian, Zhanhui Kang
conference: Arxiv
year: 2022
bibkey: hou2022bagformer
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2212.14322'}]
tags: [Evaluation, Multimodal Retrieval]
short_authors: Hou et al.
---
In the field of cross-modal retrieval, single encoder models tend to perform
better than dual encoder models, but they suffer from high latency and low
throughput. In this paper, we present a dual encoder model called BagFormer
that utilizes a cross modal interaction mechanism to improve recall performance
without sacrificing latency and throughput. BagFormer achieves this through the
use of bag-wise interactions, which allow for the transformation of text to a
more appropriate granularity and the incorporation of entity knowledge into the
model. Our experiments demonstrate that BagFormer is able to achieve results
comparable to state-of-the-art single encoder models in cross-modal retrieval
tasks, while also offering efficient training and inference with 20.72 times
lower latency and 25.74 times higher throughput.