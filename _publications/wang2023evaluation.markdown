---
layout: publication
title: Evaluation And Analysis Of Hallucination In Large Vision-language Models
authors: Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji Zhang, Jihua Zhu, Jitao Sang, Haoyu Tang
conference: "Arxiv"
year: 2023
bibkey: wang2023evaluation
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2308.15126v3"}
tags: ['ARXIV', 'Cross Modal', 'Supervised']
---
Large Vision-Language Models (LVLMs) have recently achieved remarkable success. However LVLMs are still plagued by the hallucination problem which limits the practicality in many scenarios. Hallucination refers to the information of LVLMs responses that does not exist in the visual input which poses potential risks of substantial consequences. There has been limited work studying hallucination evaluation in LVLMs. In this paper we propose Hallucination Evaluation based on Large Language Models (HaELM) an LLM-based hallucination evaluation framework. HaELM achieves an approximate 9537; performance comparable to ChatGPT and has additional advantages including low cost reproducibility privacy preservation and local deployment. Leveraging the HaELM we evaluate the hallucination in current LVLMs. Furthermore we analyze the factors contributing to hallucination in LVLMs and offer helpful suggestions to mitigate the hallucination problem. Our training data and human annotation hallucination data will be made public soon.
