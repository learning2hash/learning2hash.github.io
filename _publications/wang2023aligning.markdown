---
layout: publication
title: Aligning Large Language Models With Human A Survey
authors: Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, Qun Liu
conference: "Arxiv"
year: 2023
bibkey: wang2023aligning
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2307.12966v1"}
  - {name: "Code", url: "https://github.com/GaryYufei/AlignLLMHumanSurvey"}
tags: ['ARXIV', 'Has Code', 'Supervised', 'Survey Paper']
---
Large Language Models (LLMs) trained on extensive textual corpora have emerged as leading solutions for a broad array of Natural Language Processing (NLP) tasks. Despite their notable performance these models are prone to certain limitations such as misunderstanding human instructions generating potentially biased content or factually incorrect (hallucinated) information. Hence aligning LLMs with human expectations has become an active area of interest within the research community. This survey presents a comprehensive overview of these alignment technologies including the following aspects. (1) Data collection the methods for effectively collecting high-quality instructions for LLM alignment including the use of NLP benchmarks human annotations and leveraging strong LLMs. (2) Training methodologies a detailed review of the prevailing training methods employed for LLM alignment. Our exploration encompasses Supervised Fine-tuning both Online and Offline human preference training along with parameter-efficient training mechanisms. (3) Model Evaluation the methods for evaluating the effectiveness of these human-aligned LLMs presenting a multifaceted approach towards their assessment. In conclusion we collate and distill our findings shedding light on several promising future research avenues in the field. This survey therefore serves as a valuable resource for anyone invested in understanding and advancing the alignment of LLMs to better suit human-oriented tasks and expectations. An associated GitHub link collecting the latest papers is available at https://github.com/GaryYufei/AlignLLMHumanSurvey.
