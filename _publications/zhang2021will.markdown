---
layout: publication
title: Will Multi-modal Data Improves Few-shot Learning?
authors: Zilun Zhang, Shihao Ma, Yichun Zhang
conference: Arxiv
year: 2021
bibkey: zhang2021will
citations: 1
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2107.11853'}]
tags: ["Few Shot & Zero Shot"]
short_authors: Zilun Zhang, Shihao Ma, Yichun Zhang
---
Most few-shot learning models utilize only one modality of data. We would
like to investigate qualitatively and quantitatively how much will the model
improve if we add an extra modality (i.e. text description of the image), and
how it affects the learning procedure. To achieve this goal, we propose four
types of fusion method to combine the image feature and text feature. To verify
the effectiveness of improvement, we test the fusion methods with two classical
few-shot learning models - ProtoNet and MAML, with image feature extractors
such as ConvNet and ResNet12. The attention-based fusion method works best,
which improves the classification accuracy by a large margin around 30%
comparing to the baseline result.