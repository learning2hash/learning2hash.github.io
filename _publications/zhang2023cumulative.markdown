---
layout: publication
title: Cumulative Reasoning With Large Language Models
authors: Yifan Zhang, Jingqin Yang, Yang Yuan, Andrew Chi-chih Yao
conference: "Arxiv"
year: 2023
bibkey: zhang2023cumulative
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2308.04371v6"}
  - {name: "Code", url: "https://github.com/iiis-ai/cumulative-reasoning"}
tags: ['ARXIV', 'Has Code']
---
Despite the recent advancements in language models (LMs) their ability to solve complex problems remains limited. This paper introduces Cumulative Reasoning (CR) a novel approach that utilizes LMs cumulatively and iteratively mirroring human thought processes for problem-solving. CR decomposes tasks into smaller manageable components and leverages previous propositions for effective composition significantly enhancing problem-solving capabilities. We demonstrate CRs superiority through several complex reasoning tasks it outperforms existing methods in logical inference tasks with up to a 9.337; improvement achieving 98.0437; accuracy on the curated FOLIO wiki dataset. In the Game of 24 it achieves 9837; accuracy marking a 2437; improvement over the prior state-of-the-art. Additionally CR sets new state-of-the-art on the MATH dataset achieving a 4.237; increase from previous methods and a 4337; relative improvement in the most challenging problems. By extending CR to incorporate a code environment without external aids like retrieval or web browsing we further harness the computational and logical reasoning capabilities of LMs achieving a remarkable 72.237; accuracy on the MATH dataset and outperforming the PAL/PoT method by 38.837;. Our work not only sets new state-of-the-art but also paves the way toward more sophisticated AI reasoning methods. The code is available at https://github.com/iiis-ai/cumulative-reasoning.
