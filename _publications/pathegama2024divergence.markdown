---
layout: publication
title: 'R\''enyi Divergence-based Uniformity Guarantees For \(k\)-universal Hash Functions'
authors: Madhura Pathegama, Alexander Barg
conference: "Arxiv"
year: 2024
citations: 0
bibkey: pathegama2024divergence
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2410.16459'}
tags: ['Hashing Fundamentals', 'Evaluation Metrics', 'Hashing Methods']
---
Universal hash functions map the output of a source to random strings over a
finite alphabet, aiming to approximate the uniform distribution on the set of
strings. A classic result on these functions, called the Leftover Hash Lemma,
gives an estimate of the distance from uniformity based on the assumptions
about the min-entropy of the source. We prove several results concerning
extensions of this lemma to a class of functions that are \\(k^\ast\\)-universal,
i.e., \\(l\\)-universal for all \\(2\le l\le k\\). As a common distinctive feature, our
results provide estimates of closeness to uniformity in terms of the
\\(\alpha\\)-R\'enyi divergence for all \\(\alpha\in (1,\infty]\\). For \\(1\le \alpha\le
k\\) we show that it is possible to convert all the randomness of the source
measured in \\(\alpha\\)-R\'enyi entropy into approximately uniform bits with
nearly the same amount of randomness. For large enough \\(k\\) we show that it is
possible to distill random bits that are nearly uniform, as measured by
min-entropy. We also extend these results to hashing with side information.
