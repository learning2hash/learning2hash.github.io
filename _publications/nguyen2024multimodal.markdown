---
layout: publication
title: Multimodal Learned Sparse Retrieval For Image Suggestion
authors: Thong Nguyen, Mariya Hendriksen, Andrew Yates
conference: Arxiv
year: 2024
bibkey: nguyen2024multimodal
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2402.07736'}]
tags: ["Evaluation", "Image Retrieval", "Multimodal Retrieval"]
short_authors: Thong Nguyen, Mariya Hendriksen, Andrew Yates
---
Learned Sparse Retrieval (LSR) is a group of neural methods designed to
encode queries and documents into sparse lexical vectors. These vectors can be
efficiently indexed and retrieved using an inverted index. While LSR has shown
promise in text retrieval, its potential in multi-modal retrieval remains
largely unexplored. Motivated by this, in this work, we explore the application
of LSR in the multi-modal domain, i.e., we focus on Multi-Modal Learned Sparse
Retrieval (MLSR). We conduct experiments using several MLSR model
configurations and evaluate the performance on the image suggestion task. We
find that solving the task solely based on the image content is challenging.
Enriching the image content with its caption improves the model performance
significantly, implying the importance of image captions to provide
fine-grained concepts and context information of images. Our approach presents
a practical and effective solution for training LSR retrieval models in
multi-modal settings.