---
layout: publication
title: A Deeper Look Into Dependency-based Word Embeddings
authors: Sean MacAvaney, Amir Zeldes
conference: 'Proceedings of the 2018 Conference of the North American Chapter of the
  Association for Computational Linguistics: Student Research Workshop'
year: 2018
bibkey: macavaney2018deeper
citations: 12
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1804.05972'}]
tags: ["NAACL"]
short_authors: Sean MacAvaney, Amir Zeldes
---
We investigate the effect of various dependency-based word embeddings on
distinguishing between functional and domain similarity, word similarity
rankings, and two downstream tasks in English. Variations include word
embeddings trained using context windows from Stanford and Universal
dependencies at several levels of enhancement (ranging from unlabeled, to
Enhanced++ dependencies). Results are compared to basic linear contexts and
evaluated on several datasets. We found that embeddings trained with Universal
and Stanford dependency contexts excel at different tasks, and that enhanced
dependencies often improve performance.