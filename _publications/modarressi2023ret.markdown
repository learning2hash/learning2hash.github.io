---
layout: publication
title: RET-LLM Towards A General Read-write Memory For Large Language Models
authors: Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, Hinrich Sch√ºtze
conference: "Arxiv"
year: 2023
bibkey: modarressi2023ret
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2305.14322v1"}
tags: ['ARXIV']
---
Large language models (LLMs) have significantly advanced the field of natural language processing (NLP) through their extensive parameters and comprehensive data utilization. However existing LLMs lack a dedicated memory unit limiting their ability to explicitly store and retrieve knowledge for various tasks. In this paper we propose RET-LLM a novel framework that equips LLMs with a general write-read memory unit allowing them to extract store and recall knowledge from the text as needed for task performance. Inspired by Davidsonian semantics theory we extract and save knowledge in the form of triplets. The memory unit is designed to be scalable aggregatable updatable and interpretable. Through qualitative evaluations we demonstrate the superiority of our proposed framework over baseline approaches in question answering tasks. Moreover our framework exhibits robust performance in handling temporal-based question answering tasks showcasing its ability to effectively manage time-dependent information.
