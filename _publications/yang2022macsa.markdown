---
layout: publication
title: 'MACSA: A Multimodal Aspect-category Sentiment Analysis Dataset With Multimodal
  Fine-grained Aligned Annotations'
authors: Hao Yang, Yanyan Zhao, Jianwei Liu, Yang Wu, Bing Qin
conference: Multimedia Tools and Applications
year: 2024
bibkey: yang2022macsa
citations: 5
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2206.13969'}]
tags: ["Datasets"]
short_authors: Yang et al.
---
Multimodal fine-grained sentiment analysis has recently attracted increasing
attention due to its broad applications. However, the existing multimodal
fine-grained sentiment datasets most focus on annotating the fine-grained
elements in text but ignore those in images, which leads to the fine-grained
elements in visual content not receiving the full attention they deserve. In
this paper, we propose a new dataset, the Multimodal Aspect-Category Sentiment
Analysis (MACSA) dataset, which contains more than 21K text-image pairs. The
dataset provides fine-grained annotations for both textual and visual content
and firstly uses the aspect category as the pivot to align the fine-grained
elements between the two modalities. Based on our dataset, we propose the
Multimodal ACSA task and a multimodal graph-based aligned model (MGAM), which
adopts a fine-grained cross-modal fusion method. Experimental results show that
our method can facilitate the baseline comparison for future research on this
corpus. We will make the dataset and code publicly available.