---
layout: publication
title: Occ-mllm:empowering Multimodal Large Language Model For The Understanding Of
  Occluded Objects
authors: Wenmo Qiu, Xinhan di
conference: Arxiv
year: 2024
bibkey: qiu2024occ
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2410.01261'}]
tags: []
short_authors: Wenmo Qiu, Xinhan di
---
There is a gap in the understanding of occluded objects in existing
large-scale visual language multi-modal models. Current state-of-the-art
multimodal models fail to provide satisfactory results in describing occluded
objects for visual-language multimodal models through universal visual
encoders. Another challenge is the limited number of datasets containing
image-text pairs with a large number of occluded objects. Therefore, we
introduce a novel multimodal model that applies a newly designed visual encoder
to understand occluded objects in RGB images. We also introduce a large-scale
visual-language pair dataset for training large-scale visual-language
multimodal models and understanding occluded objects. We start our experiments
comparing with the state-of-the-art models.