---
layout: publication
title: Advances In Pre-training Distributed Word Representations
authors: Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, Armand
  Joulin
conference: Arxiv
year: 2017
bibkey: mikolov2017advances
citations: 530
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1712.09405'}]
tags: ["Self-Supervised"]
short_authors: Mikolov et al.
---
Many Natural Language Processing applications nowadays rely on pre-trained
word representations estimated from large text corpora such as news
collections, Wikipedia and Web Crawl. In this paper, we show how to train
high-quality word vector representations by using a combination of known tricks
that are however rarely used together. The main result of our work is the new
set of publicly available pre-trained models that outperform the current state
of the art by a large margin on a number of tasks.