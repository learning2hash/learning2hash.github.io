---
layout: publication
title: 'Neural Fashion Image Captioning : Accounting For Data Diversity'
authors: Gilles Hacheme, Noureini Sayouti
conference: ML4D 35th Conference on Neural Information Processing Systems (NeurIPS
  2021) Sydney Australia
year: 2021
bibkey: hacheme2021neural
citations: 9
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2106.12154'}]
tags: ["Datasets", "NEURIPS"]
short_authors: Gilles Hacheme, Noureini Sayouti
---
Image captioning has increasingly large domains of application, and fashion
is not an exception. Having automatic item descriptions is of great interest
for fashion web platforms, sometimes hosting hundreds of thousands of images.
This paper is one of the first to tackle image captioning for fashion images.
To address dataset diversity issues, we introduced the InFashAIv1 dataset
containing almost 16.000 African fashion item images with their titles, prices,
and general descriptions. We also used the well-known DeepFashion dataset in
addition to InFashAIv1. Captions are generated using the Show and Tell model
made of CNN encoder and RNN Decoder. We showed that jointly training the model
on both datasets improves captions quality for African style fashion images,
suggesting a transfer learning from Western style data. The InFashAIv1 dataset
is released on Github to encourage works with more diversity inclusion.