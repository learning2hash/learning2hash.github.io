---
layout: publication
title: Constructing Multimodal Datasets From Scratch For Rapid Development Of A Japanese
  Visual Language Model
authors: Keito Sasagawa, Koki Maeda, Issa Sugiura, Shuhei Kurita, Naoaki Okazaki,
  Daisuke Kawahara
conference: Arxiv
year: 2024
bibkey: sasagawa2024constructing
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2410.22736'}]
tags: ["Datasets"]
short_authors: Sasagawa et al.
---
To develop high-performing Visual Language Models (VLMs), it is essential to
prepare multimodal resources, such as image-text pairs, interleaved data, and
instruction data. While multimodal resources for English are abundant, there is
a significant lack of corresponding resources for non-English languages, such
as Japanese. To address this problem, we take Japanese as a non-English
language and propose a method for rapidly creating Japanese multimodal datasets
from scratch. We collect Japanese image-text pairs and interleaved data from
web archives and generate Japanese instruction data directly from images using
an existing VLM. Our experimental results show that a VLM trained on these
native datasets outperforms those relying on machine-translated content.