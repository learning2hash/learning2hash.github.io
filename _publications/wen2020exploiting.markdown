---
layout: publication
title: 'Exploiting Weight Redundancy In Cnns: Beyond Pruning And Quantization'
authors: Yuan Wen, David Gregg
conference: Arxiv
year: 2020
bibkey: wen2020exploiting
citations: 2
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2006.11967'}]
tags: ["Efficiency", "Quantization"]
short_authors: Yuan Wen, David Gregg
---
Pruning and quantization are proven methods for improving the performance and
storage efficiency of convolutional neural networks (CNNs). Pruning removes
near-zero weights in tensors and masks weak connections between neurons in
neighbouring layers. Quantization reduces the precision of weights by replacing
them with numerically similar values that require less storage. In this paper,
we identify another form of redundancy in CNN weight tensors, in the form of
repeated patterns of similar values. We observe that pruning and quantization
both tend to drastically increase the number of repeated patterns in the weight
tensors.
  We investigate several compression schemes to take advantage of this
structure in CNN weight data, including multiple forms of Huffman coding, and
other approaches inspired by block sparse matrix formats. We evaluate our
approach on several well-known CNNs and find that we can achieve compaction
ratios of 1.4x to 3.1x in addition to the saving from pruning and quantization.