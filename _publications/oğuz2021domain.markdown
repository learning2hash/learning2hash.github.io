---
layout: publication
title: Domain-matched Pre-training Tasks For Dense Retrieval
authors: "Barlas O\u011Fuz, Kushal Lakhotia, Anchit Gupta, Patrick Lewis, Vladimir\
  \ Karpukhin, Aleksandra Piktus, Xilun Chen, Sebastian Riedel, Wen-Tau Yih, Sonal\
  \ Gupta, Yashar Mehdad"
conference: Arxiv
year: 2021
bibkey: "o\u011Fuz2021domain"
citations: 1
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2107.13602'}]
tags: ["Datasets", "Evaluation", "Supervised"]
short_authors: "O\u011Fuz et al."
---
Pre-training on larger datasets with ever increasing model size is now a
proven recipe for increased performance across almost all NLP tasks. A notable
exception is information retrieval, where additional pre-training has so far
failed to produce convincing results. We show that, with the right pre-training
setup, this barrier can be overcome. We demonstrate this by pre-training large
bi-encoder models on 1) a recently released set of 65 million synthetically
generated questions, and 2) 200 million post-comment pairs from a preexisting
dataset of Reddit conversations made available by pushshift.io. We evaluate on
a set of information retrieval and dialogue retrieval benchmarks, showing
substantial improvements over supervised baselines.