---
layout: publication
title: 'Image Search Using Multilingual Texts: A Cross-modal Learning Approach Between
  Image And Text'
authors: Maxime Portaz, Hicham Randrianarivo, Adrien Nivaggioli, Estelle Maudet, Christophe
  Servan, Sylvain Peyronnet
conference: Arxiv
year: 2019
bibkey: portaz2019image
citations: 10
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1903.11299'}]
tags: [Efficiency, Tools & Libraries, Datasets, Image Retrieval]
short_authors: Portaz et al.
---
Multilingual (or cross-lingual) embeddings represent several languages in a
unique vector space. Using a common embedding space enables for a shared
semantic between words from different languages. In this paper, we propose to
embed images and texts into a unique distributional vector space, enabling to
search images by using text queries expressing information needs related to the
(visual) content of images, as well as using image similarity. Our framework
forces the representation of an image to be similar to the representation of
the text that describes it. Moreover, by using multilingual embeddings we
ensure that words from two different languages have close descriptors and thus
are attached to similar images. We provide experimental evidence of the
efficiency of our approach by experimenting it on two datasets: Common Objects
in COntext (COCO) [19] and Multi30K [7].