---
layout: publication
title: 'M3P: Learning Universal Representations Via Multitask Multilingual Multimodal
  Pre-training'
authors: Minheng Ni, Haoyang Huang, Lin Su, Edward Cui, Taroon Bharti, Lijuan Wang,
  Jianfeng Gao, Dongdong Zhang, Nan Duan
conference: Arxiv
year: 2020
bibkey: ni2020m3p
citations: 40
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2006.02635'}]
tags: ["Datasets"]
short_authors: Ni et al.
---
We present M3P, a Multitask Multilingual Multimodal Pre-trained model that
combines multilingual pre-training and multimodal pre-training into a unified
framework via multitask pre-training. Our goal is to learn universal
representations that can map objects occurred in different modalities or texts
expressed in different languages into a common semantic space. In addition, to
explicitly encourage fine-grained alignment between images and non-English
languages, we also propose Multimodal Code-switched Training (MCT) to combine
monolingual pre-training and multimodal pre-training via a code-switch
strategy. Experiments are performed on the multilingual image retrieval task
across two benchmark datasets, including MSCOCO and Multi30K. M3P can achieve
comparable results for English and new state-of-the-art results for non-English
languages.