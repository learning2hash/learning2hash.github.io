---
layout: publication
title: Llm-eval Unified Multi-dimensional Automatic Evaluation For Open-domain Conversations With Large Language Models
authors: Yen-ting Lin, Yun-nung Chen
conference: "Arxiv"
year: 2023
bibkey: lin2023llm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2305.13711v1"}
tags: ['ARXIV']
---
We propose LLM-Eval a unified multi-dimensional automatic evaluation method for open-domain conversations with large language models (LLMs). Existing evaluation methods often rely on human annotations ground-truth responses or multiple LLM prompts which can be expensive and time-consuming. To address these issues we design a single prompt-based evaluation method that leverages a unified evaluation schema to cover multiple dimensions of conversation quality in a single model call. We extensively evaluate the performance of LLM-Eval on various benchmark datasets demonstrating its effectiveness efficiency and adaptability compared to state-of-the-art evaluation methods. Our analysis also highlights the importance of choosing suitable LLMs and decoding strategies for accurate evaluation results. LLM-Eval offers a versatile and robust solution for evaluating open-domain conversation systems streamlining the evaluation process and providing consistent performance across diverse scenarios.
