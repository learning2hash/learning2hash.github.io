---
layout: publication
title: Building Russian Benchmark For Evaluation Of Information Retrieval Models
authors: Grigory Kovalev, Mikhail Tikhomirov, Evgeny Kozhevnikov, Max Kornilov, Natalia
  Loukachevitch
conference: Computational Linguistics and Intellectual Technologies
year: 2025
bibkey: kovalev2025building
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2504.12879'}]
tags: [Evaluation, Few-shot & Zero-shot, Datasets, Tools & Libraries, Text Retrieval]
short_authors: Kovalev et al.
---
We introduce RusBEIR, a comprehensive benchmark designed for zero-shot
evaluation of information retrieval (IR) models in the Russian language.
Comprising 17 datasets from various domains, it integrates adapted, translated,
and newly created datasets, enabling systematic comparison of lexical and
neural models. Our study highlights the importance of preprocessing for lexical
models in morphologically rich languages and confirms BM25 as a strong baseline
for full-document retrieval. Neural models, such as mE5-large and BGE-M3,
demonstrate superior performance on most datasets, but face challenges with
long-document retrieval due to input size constraints. RusBEIR offers a
unified, open-source framework that promotes research in Russian-language
information retrieval.