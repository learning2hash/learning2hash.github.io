---
layout: publication
title: Chain-of-thought Prompting Under Streaming Batch A Case Study
authors: Yuxin Tang
conference: "Arxiv"
year: 2023
bibkey: tang2023chain
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2306.00550v1"}
tags: ['ARXIV', 'Case Study']
---
Recently Large Language Models (LLMs) have demonstrated remarkable capabilities. Chain-of-Thought (CoT) has been proposed as a way of assisting LLMs in performing complex reasoning. However developing effective prompts can be a challenging and labor-intensive task. Many studies come out of some way to automatically construct CoT from test data. Most of them assume that all test data is visible before testing and only select a small subset to generate rationales which is an unrealistic assumption. In this paper we present a case study on how to construct and optimize chain-of-thought prompting using batch data in streaming settings.
