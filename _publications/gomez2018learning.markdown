---
layout: publication
title: Learning To Learn From Web Data Through Deep Semantic Embeddings
authors: Raul Gomez, Lluis Gomez, Jaume Gibert, Dimosthenis Karatzas
conference: Lecture Notes in Computer Science
year: 2019
bibkey: gomez2018learning
citations: 23
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1808.06368'}]
tags: ["Datasets", "Evaluation", "Image Retrieval", "Supervised"]
short_authors: Gomez et al.
---
In this paper we propose to learn a multimodal image and text embedding from
Web and Social Media data, aiming to leverage the semantic knowledge learnt in
the text domain and transfer it to a visual model for semantic image retrieval.
We demonstrate that the pipeline can learn from images with associated text
without supervision and perform a thourough analysis of five different text
embeddings in three different benchmarks. We show that the embeddings learnt
with Web and Social Media data have competitive performances over supervised
methods in the text based image retrieval task, and we clearly outperform state
of the art in the MIRFlickr dataset when training in the target data. Further
we demonstrate how semantic multimodal image retrieval can be performed using
the learnt embeddings, going beyond classical instance-level retrieval
problems. Finally, we present a new dataset, InstaCities1M, composed by
Instagram images and their associated texts that can be used for fair
comparison of image-text embeddings.