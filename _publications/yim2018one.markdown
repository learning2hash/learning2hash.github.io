---
layout: publication
title: One-shot Item Search With Multimodal Data
authors: Jonghwa Yim, Junghun James Kim, Daekyu Shin
conference: Arxiv
year: 2018
bibkey: yim2018one
citations: 2
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1811.10969'}]
tags: ["Datasets", "Evaluation", "Image Retrieval"]
short_authors: Jonghwa Yim, Junghun James Kim, Daekyu Shin
---
In the task of near similar image search, features from Deep Neural Network
is often used to compare images and measure similarity. In the past, we only
focused visual search in image dataset without text data. However, since deep
neural network emerged, the performance of visual search becomes high enough to
apply it in many industries from 3D data to multimodal data. Compared to the
needs of multimodal search, there has not been sufficient researches.
  In this paper, we present a method of near similar search with image and text
multimodal dataset. Earlier time, similar image search, especially when
searching shopping items, treated image and text separately to search similar
items and reorder the results. This regards two tasks of image search and text
matching as two different tasks. Our method, however, explore the vast data to
compute k-nearest neighbors using both image and text.
  In our experiment of similar item search, our system using multimodal data
shows better performance than single data while it only increases minute
computing time. For the experiment, we collected more than 15 million of
accessory and six million of digital product items from online shopping
websites, in which the product item comprises item images, titles, categories,
and descriptions. Then we compare the performance of multimodal searching to
single space searching in these datasets.