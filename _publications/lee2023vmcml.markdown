---
layout: publication
title: 'VMCML: Video And Music Matching Via Cross-modality Lifting'
authors: Yi-Shan Lee, Wei-Cheng Tseng, Fu-En Wang, Min Sun
conference: Arxiv
year: 2023
bibkey: lee2023vmcml
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2303.12379'}]
tags: ["Datasets", "Distance Metric Learning", "Evaluation", "Scalability", "Tools & Libraries"]
short_authors: Lee et al.
---
We propose a content-based system for matching video and background music.
The system aims to address the challenges in music recommendation for new users
or new music give short-form videos. To this end, we propose a cross-modal
framework VMCML that finds a shared embedding space between video and music
representations. To ensure the embedding space can be effectively shared by
both representations, we leverage CosFace loss based on margin-based cosine
similarity loss. Furthermore, we establish a large-scale dataset called MSVD,
in which we provide 390 individual music and the corresponding matched 150,000
videos. We conduct extensive experiments on Youtube-8M and our MSVD datasets.
Our quantitative and qualitative results demonstrate the effectiveness of our
proposed framework and achieve state-of-the-art video and music matching
performance.