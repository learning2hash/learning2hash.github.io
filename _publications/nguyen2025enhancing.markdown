---
layout: publication
title: Enhancing Multimodal Entity Linking With Jaccard Distance-based Conditional
  Contrastive Learning And Contextual Visual Augmentation
authors: Cong-Duy Nguyen, Xiaobao Wu, Thong Nguyen, Shuai Zhao, Khoi Le, Viet-Anh
  Nguyen, Feng Yichao, Anh Tuan Luu
conference: 'Proceedings of the 2025 Conference of the Nations of the Americas Chapter
  of the Association for Computational Linguistics: Human Language Technologies (Volume
  1: Long Papers)'
year: 2025
bibkey: nguyen2025enhancing
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2501.14166'}]
tags: []
short_authors: Nguyen et al.
---
Previous research on multimodal entity linking (MEL) has primarily employed
contrastive learning as the primary objective. However, using the rest of the
batch as negative samples without careful consideration, these studies risk
leveraging easy features and potentially overlook essential details that make
entities unique. In this work, we propose JD-CCL (Jaccard Distance-based
Conditional Contrastive Learning), a novel approach designed to enhance the
ability to match multimodal entity linking models. JD-CCL leverages
meta-information to select negative samples with similar attributes, making the
linking task more challenging and robust. Additionally, to address the
limitations caused by the variations within the visual modality among mentions
and entities, we introduce a novel method, CVaCPT (Contextual Visual-aid
Controllable Patch Transform). It enhances visual representations by
incorporating multi-view synthetic images and contextual textual
representations to scale and shift patch representations. Experimental results
on benchmark MEL datasets demonstrate the strong effectiveness of our approach.