---
layout: publication
title: Image Representations Learned With Unsupervised Pre-training Contain Human-like
  Biases
authors: Ryan Steed, Aylin Caliskan
conference: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and
  Transparency
year: 2021
bibkey: steed2020image
citations: 40
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2010.15052'}]
tags: ["Unsupervised"]
short_authors: Ryan Steed, Aylin Caliskan
---
Recent advances in machine learning leverage massive datasets of unlabeled
images from the web to learn general-purpose image representations for tasks
from image classification to face recognition. But do unsupervised computer
vision models automatically learn implicit patterns and embed social biases
that could have harmful downstream effects? We develop a novel method for
quantifying biased associations between representations of social concepts and
attributes in images. We find that state-of-the-art unsupervised models trained
on ImageNet, a popular benchmark image dataset curated from internet images,
automatically learn racial, gender, and intersectional biases. We replicate 8
previously documented human biases from social psychology, from the innocuous,
as with insects and flowers, to the potentially harmful, as with race and
gender. Our results closely match three hypotheses about intersectional bias
from social psychology. For the first time in unsupervised computer vision, we
also quantify implicit human biases about weight, disabilities, and several
ethnicities. When compared with statistical patterns in online image datasets,
our findings suggest that machine learning models can automatically learn bias
from the way people are stereotypically portrayed on the web.