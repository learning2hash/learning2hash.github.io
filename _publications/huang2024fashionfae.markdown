---
layout: publication
title: 'Fashionfae: Fine-grained Attributes Enhanced Fashion Vision-language Pre-training'
authors: Jiale Huang, Dehong Gao, Jinxia Zhang, Zechao Zhan, Yang Hu, Xin Wang
conference: Arxiv
year: 2024
bibkey: huang2024fashionfae
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2412.19997'}]
tags: ["Scalability"]
short_authors: Huang et al.
---
Large-scale Vision-Language Pre-training (VLP) has demonstrated remarkable
success in the general domain. However, in the fashion domain, items are
distinguished by fine-grained attributes like texture and material, which are
crucial for tasks such as retrieval. Existing models often fail to leverage
these fine-grained attributes from both text and image modalities. To address
the above issues, we propose a novel approach for the fashion domain,
Fine-grained Attributes Enhanced VLP (FashionFAE), which focuses on the
detailed characteristics of fashion data. An attribute-emphasized text
prediction task is proposed to predict fine-grained attributes of the items.
This forces the model to focus on the salient attributes from the text
modality. Additionally, a novel attribute-promoted image reconstruction task is
proposed, which further enhances the fine-grained ability of the model by
leveraging the representative attributes from the image modality. Extensive
experiments show that FashionFAE significantly outperforms State-Of-The-Art
(SOTA) methods, achieving 2.9% and 5.2% improvements in retrieval on sub-test
and full test sets, respectively, and a 1.6% average improvement in recognition
tasks.