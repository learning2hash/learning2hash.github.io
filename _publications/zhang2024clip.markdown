---
layout: publication
title: CLIP Model For Images To Textual Prompts Based On Top-k Neighbors
authors: Xin Zhang, Xin Zhang, Yeming Cai, Tianzhi Jia
conference: Arxiv
year: 2024
bibkey: zhang2024clip
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2401.09763'}]
tags: [Uncategorized]
short_authors: Zhang et al.
---
Text-to-image synthesis, a subfield of multimodal generation, has gained
significant attention in recent years. We propose a cost-effective approach for
image-to-prompt generation that leverages generative models to generate textual
prompts without the need for large amounts of annotated data. We divide our
method into two stages: online stage and offline stage. We use a combination of
the CLIP model and K-nearest neighbors (KNN) algorithm. The proposed system
consists of two main parts: an offline task and an online task. Our method owns
the highest metric 0.612 among these models, which is 0.013, 0.055, 0.011
higher than Clip, Clip + KNN(top 10) respectively.