---
layout: publication
title: Leveraging Large Language Models For Pre-trained Recommender Systems
authors: Zhixuan Chu, Hongyan Hao, Xin Ouyang, Simeng Wang, Yan Wang, Yue Shen, Jinjie Gu, Qing Cui, Longfei Li, Siqiao Xue, James Y Zhang, Sheng Li
conference: "Arxiv"
year: 2023
bibkey: chu2023leveraging
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2308.10837v1"}
tags: ['ARXIV']
---
Recent advancements in recommendation systems have shifted towards more comprehensive and personalized recommendations by utilizing large language models (LLM). However effectively integrating LLMs commonsense knowledge and reasoning abilities into recommendation systems remains a challenging problem. In this paper we propose RecSysLLM a novel pre-trained recommendation model based on LLMs. RecSysLLM retains LLM reasoning and knowledge while integrating recommendation domain knowledge through unique designs of data training and inference. This allows RecSysLLM to leverage LLMs capabilities for recommendation tasks in an efficient unified framework. We demonstrate the effectiveness of RecSysLLM on benchmarks and real-world scenarios. RecSysLLM provides a promising approach to developing unified recommendation systems by fully exploiting the power of pre-trained language models.
