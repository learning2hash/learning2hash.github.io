---
layout: publication
title: \(\beta\)-vaes Can Retain Label Information Even At High Compression
authors: Emily Fertig, Aryan Arbabi, Alexander A. Alemi
conference: Arxiv
year: 2018
bibkey: fertig2018beta
citations: 3
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1812.02682'}]
tags: ["Unsupervised"]
short_authors: Emily Fertig, Aryan Arbabi, Alexander A. Alemi
---
In this paper, we investigate the degree to which the encoding of a
\(\beta\)-VAE captures label information across multiple architectures on Binary
Static MNIST and Omniglot. Even though they are trained in a completely
unsupervised manner, we demonstrate that a \(\beta\)-VAE can retain a large
amount of label information, even when asked to learn a highly compressed
representation.