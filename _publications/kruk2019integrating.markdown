---
layout: publication
title: 'Integrating Text And Image: Determining Multimodal Document Intent In Instagram
  Posts'
authors: Julia Kruk, Jonah Lubin, Karan Sikka, Xiao Lin, Dan Jurafsky, Ajay Divakaran
conference: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
  Processing and the 9th International Joint Conference on Natural Language Processing
  (EMNLP-IJCNLP)
year: 2019
bibkey: kruk2019integrating
citations: 93
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1904.09073'}]
tags: ["Datasets"]
short_authors: Kruk et al.
---
Computing author intent from multimodal data like Instagram posts requires
modeling a complex relationship between text and image. For example, a caption
might evoke an ironic contrast with the image, so neither caption nor image is
a mere transcript of the other. Instead they combine -- via what has been
called meaning multiplication -- to create a new meaning that has a more
complex relation to the literal meanings of text and image. Here we introduce a
multimodal dataset of 1299 Instagram posts labeled for three orthogonal
taxonomies: the authorial intent behind the image-caption pair, the contextual
relationship between the literal meanings of the image and caption, and the
semiotic relationship between the signified meanings of the image and caption.
We build a baseline deep multimodal classifier to validate the taxonomy,
showing that employing both text and image improves intent detection by 9.6%
compared to using only the image modality, demonstrating the commonality of
non-intersective meaning multiplication. The gain with multimodality is
greatest when the image and caption diverge semiotically. Our dataset offers a
new resource for the study of the rich meanings that result from pairing text
and image.