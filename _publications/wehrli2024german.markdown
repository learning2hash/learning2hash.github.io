---
layout: publication
title: German Text Embedding Clustering Benchmark
authors: Silvan Wehrli, Bert Arnrich, Christopher Irrgang
conference: Arxiv
year: 2024
bibkey: wehrli2024german
citations: 1
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2401.02709'}]
tags: ["Datasets", "Evaluation"]
short_authors: Silvan Wehrli, Bert Arnrich, Christopher Irrgang
---
This work introduces a benchmark assessing the performance of clustering
German text embeddings in different domains. This benchmark is driven by the
increasing use of clustering neural text embeddings in tasks that require the
grouping of texts (such as topic modeling) and the need for German resources in
existing benchmarks. We provide an initial analysis for a range of pre-trained
mono- and multilingual models evaluated on the outcome of different clustering
algorithms. Results include strong performing mono- and multilingual models.
Reducing the dimensions of embeddings can further improve clustering.
Additionally, we conduct experiments with continued pre-training for German
BERT models to estimate the benefits of this additional training. Our
experiments suggest that significant performance improvements are possible for
short text. All code and datasets are publicly available.