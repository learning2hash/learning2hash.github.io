---
layout: publication
title: Sentence Meta-embeddings For Unsupervised Semantic Textual Similarity
authors: "Nina Poerner, Ulli Waltinger, Hinrich Sch\xFCtze"
conference: Proceedings of the 58th Annual Meeting of the Association for Computational
  Linguistics
year: 2020
bibkey: poerner2019sentence
citations: 24
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1911.03700'}]
tags: ["Datasets", "Evaluation", "Unsupervised"]
short_authors: "Nina Poerner, Ulli Waltinger, Hinrich Sch\xFCtze"
---
We address the task of unsupervised Semantic Textual Similarity (STS) by
ensembling diverse pre-trained sentence encoders into sentence meta-embeddings.
We apply, extend and evaluate different meta-embedding methods from the word
embedding literature at the sentence level, including dimensionality reduction
(Yin and Sch\"utze, 2016), generalized Canonical Correlation Analysis (Rastogi
et al., 2015) and cross-view auto-encoders (Bollegala and Bao, 2018). Our
sentence meta-embeddings set a new unsupervised State of The Art (SoTA) on the
STS Benchmark and on the STS12-STS16 datasets, with gains of between 3.7% and
6.4% Pearson's r over single-source systems.