---
layout: publication
title: Learning Video Embedding Space With Natural Language Supervision
authors: Phani Krishna Uppala, Abhishek Bamotra, Shriti Priya, Vaidehi Joshi
conference: Arxiv
year: 2023
bibkey: uppala2023learning
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2303.14584'}]
tags: []
short_authors: Uppala et al.
---
The recent success of the CLIP model has shown its potential to be applied to
a wide range of vision and language tasks. However this only establishes
embedding space relationship of language to images, not to the video domain. In
this paper, we propose a novel approach to map video embedding space to natural
langugage. We propose a two-stage approach that first extracts visual features
from each frame of a video using a pre-trained CNN, and then uses the CLIP
model to encode the visual features for the video domain, along with the
corresponding text descriptions. We evaluate our method on two benchmark
datasets, UCF101 and HMDB51, and achieve state-of-the-art performance on both
tasks.