---
layout: publication
title: Direction-oriented Visual-semantic Embedding Model For Remote Sensing Image-text
  Retrieval
authors: Qing Ma, Jiancheng Pan, Cong Bai
conference: IEEE Transactions on Geoscience and Remote Sensing
year: 2024
bibkey: ma2023direction
citations: 11
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2310.08276'}]
tags: [Datasets, Text Retrieval, Evaluation]
short_authors: Qing Ma, Jiancheng Pan, Cong Bai
---
Image-text retrieval has developed rapidly in recent years. However, it is
still a challenge in remote sensing due to visual-semantic imbalance, which
leads to incorrect matching of non-semantic visual and textual features. To
solve this problem, we propose a novel Direction-Oriented Visual-semantic
Embedding Model (DOVE) to mine the relationship between vision and language.
Our highlight is to conduct visual and textual representations in latent space,
directing them as close as possible to a redundancy-free regional visual
representation. Concretely, a Regional-Oriented Attention Module (ROAM)
adaptively adjusts the distance between the final visual and textual embeddings
in the latent semantic space, oriented by regional visual features. Meanwhile,
a lightweight Digging Text Genome Assistant (DTGA) is designed to expand the
range of tractable textual representation and enhance global word-level
semantic connections using less attention operations. Ultimately, we exploit a
global visual-semantic constraint to reduce single visual dependency and serve
as an external constraint for the final visual and textual representations. The
effectiveness and superiority of our method are verified by extensive
experiments including parameter evaluation, quantitative comparison, ablation
studies and visual analysis, on two benchmark datasets, RSICD and RSITMD.