---
layout: publication
title: 'Onlineanyseg: Online Zero-shot 3D Segmentation By Visual Foundation Model
  Guided 2D Mask Merging'
authors: Yijie Tang, Jiazhao Zhang, Yuqing Lan, Yulan Guo, Dezun Dong, Chenyang Zhu,
  Kai Xu
conference: 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
year: 2025
bibkey: tang2025onlineanyseg
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2503.01309'}]
tags: ["CVPR", "Efficiency", "Evaluation", "Few Shot & Zero Shot", "Hashing Methods"]
short_authors: Tang et al.
---
Online zero-shot 3D instance segmentation of a progressively reconstructed
scene is both a critical and challenging task for embodied applications. With
the success of visual foundation models (VFMs) in the image domain, leveraging
2D priors to address 3D online segmentation has become a prominent research
focus. Since segmentation results provided by 2D priors often require spatial
consistency to be lifted into final 3D segmentation, an efficient method for
identifying spatial overlap among 2D masks is essential - yet existing methods
rarely achieve this in real time, mainly limiting its use to offline
approaches. To address this, we propose an efficient method that lifts 2D masks
generated by VFMs into a unified 3D instance using a hashing technique. By
employing voxel hashing for efficient 3D scene querying, our approach reduces
the time complexity of costly spatial overlap queries from \(O(n^2)\) to \(O(n)\).
Accurate spatial associations further enable 3D merging of 2D masks through
simple similarity-based filtering in a zero-shot manner, making our approach
more robust to incomplete and noisy data. Evaluated on the ScanNet and SceneNN
benchmarks, our approach achieves state-of-the-art performance in online,
zero-shot 3D instance segmentation with leading efficiency.