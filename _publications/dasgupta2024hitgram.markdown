---
layout: publication
title: 'Hitgram: A Platform For Experimenting With N-gram Language Models'
authors: Shibaranjani Dasgupta, Chandan Maity, Somdip Mukherjee, Rohan Singh, Diptendu
  Dutta, Debasish Jana
conference: Lecture Notes in Computer Science
year: 2025
bibkey: dasgupta2024hitgram
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2412.10717'}]
tags: []
short_authors: Dasgupta et al.
---
Large language models (LLMs) are powerful but resource intensive, limiting
accessibility. HITgram addresses this gap by offering a lightweight platform
for n-gram model experimentation, ideal for resource-constrained environments.
It supports unigrams to 4-grams and incorporates features like context
sensitive weighting, Laplace smoothing, and dynamic corpus management to
e-hance prediction accuracy, even for unseen word sequences. Experiments
demonstrate HITgram's efficiency, achieving 50,000 tokens/second and generating
2-grams from a 320MB corpus in 62 seconds. HITgram scales efficiently,
constructing 4-grams from a 1GB file in under 298 seconds on an 8 GB RAM
system. Planned enhancements include multilingual support, advanced smoothing,
parallel processing, and model saving, further broadening its utility.