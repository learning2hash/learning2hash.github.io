---
layout: publication
title: 'Tricolo: Trimodal Contrastive Loss For Text To Shape Retrieval'
authors: Ruan Yue, Lee Han-hung, Zhang Yiming, Zhang Ke, Chang Angel X.
conference: 2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)
year: 2024
bibkey: ruan2022tricolo
citations: 4
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2201.07366'}]
tags: ["Self-Supervised", "Evaluation", "Distance-Metric-Learning"]
short_authors: Ruan et al.
---
Text-to-shape retrieval is an increasingly relevant problem with the growth
of 3D shape data. Recent work on contrastive losses for learning joint
embeddings over multimodal data has been successful at tasks such as retrieval
and classification. Thus far, work on joint representation learning for 3D
shapes and text has focused on improving embeddings through modeling of complex
attention between representations, or multi-task learning. We propose a
trimodal learning scheme over text, multi-view images and 3D shape voxels, and
show that with large batch contrastive learning we achieve good performance on
text-to-shape retrieval without complex attention mechanisms or losses. Our
experiments serve as a foundation for follow-up work on building trimodal
embeddings for text-image-shape.