---
layout: publication
title: Pyramid Vector Quantization And Bit Level Sparsity In Weights For Efficient
  Neural Networks Inference
authors: Vincenzo Liguori
conference: Arxiv
year: 2019
bibkey: liguori2019pyramid
citations: 1
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1911.10636'}]
tags: ["Quantization"]
short_authors: Vincenzo Liguori
---
This paper discusses three basic blocks for the inference of convolutional
neural networks (CNNs). Pyramid Vector Quantization (PVQ) is discussed as an
effective quantizer for CNNs weights resulting in highly sparse and
compressible networks. Properties of PVQ are exploited for the elimination of
multipliers during inference while maintaining high performance. The result is
then extended to any other quantized weights. The Tiny Yolo v3 CNN is used to
compare such basic blocks.