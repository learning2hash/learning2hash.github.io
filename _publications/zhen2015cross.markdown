---
layout: publication
title: "Cross-Modal Similarity Learning via Pairs, Preferences, and Active Supervision"
authors: Yi Zhen, Piyush Rai, Hongyuan Zha, and Lawrence Carin
conference: AAAI
year: 2015
bibkey: zhen2015cross
additional_links:
   - {name: "PDF", url: "http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9648/10001/"}
tags: ["Cross-Modal", "AAAI"]
---
We present a probabilistic framework for learning pairwise similarities between objects belonging to different modalities, such as drugs and proteins, or text and
images. Our framework is based on learning a binary
code based representation for objects in each modality, and has the following key properties: (i) it can
leverage both pairwise as well as easy-to-obtain relative
preference based cross-modal constraints, (ii) the probabilistic framework naturally allows querying for the
most useful/informative constraints, facilitating an active learning setting (existing methods for cross-modal
similarity learning do not have such a mechanism), and
(iii) the binary code length is learned from the data. We
demonstrate the effectiveness of the proposed approach
on two problems that require computing pairwise similarities between cross-modal object pairs: cross-modal
link prediction in bipartite graphs, and hashing based
cross-modal similarity search.
