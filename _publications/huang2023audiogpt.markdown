---
layout: publication
title: Audiogpt Understanding And Generating Speech Music Sound And Talking Head
authors: Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, Yi Ren, Zhou Zhao, Shinji Watanabe
conference: "Arxiv"
year: 2023
bibkey: huang2023audiogpt
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2304.12995v1"}
  - {name: "Code", url: "https://github.com/AIGC-Audio/AudioGPT}"}
tags: ['ARXIV', 'Has Code']
---
Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks challenging our understanding of learning and cognition. Despite the recent success current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work we propose a multi-modal AI system named AudioGPT which complements LLMs (i.e. ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models we outline the principles and processes and test AudioGPT in terms of consistency capability and robustness. Experimental results demonstrate the capabilities of AudioGPT in solving AI tasks with speech music sound and talking head understanding and generation in multi-round dialogues which empower humans to create rich and diverse audio content with unprecedented ease. Our system is publicly available at urlhttps://github.com/AIGC-Audio/AudioGPT\}.
