---
layout: publication
title: Improving Deep Visual Representation For Person Re-identification By Global
  And Local Image-language Association
authors: Dapeng Chen, Hongsheng Li, Xihui Liu, Yantao Shen, Zejian Yuan, Xiaogang
  Wang
conference: Lecture Notes in Computer Science
year: 2018
bibkey: chen2018improving
citations: 152
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1808.01571'}]
tags: ["Compact Codes", "Datasets", "Distance Metric Learning", "Evaluation", "Hashing Methods", "Image Retrieval"]
short_authors: Chen et al.
---
Person re-identification is an important task that requires learning
discriminative visual features for distinguishing different person identities.
Diverse auxiliary information has been utilized to improve the visual feature
learning. In this paper, we propose to exploit natural language description as
additional training supervisions for effective visual features. Compared with
other auxiliary information, language can describe a specific person from more
compact and semantic visual aspects, thus is complementary to the pixel-level
image data. Our method not only learns better global visual feature with the
supervision of the overall description but also enforces semantic consistencies
between local visual and linguistic features, which is achieved by building
global and local image-language associations. The global image-language
association is established according to the identity labels, while the local
association is based upon the implicit correspondences between image regions
and noun phrases. Extensive experiments demonstrate the effectiveness of
employing language as training supervisions with the two association schemes.
Our method achieves state-of-the-art performance without utilizing any
auxiliary information during testing and shows better performance than other
joint embedding methods for the image-language association.