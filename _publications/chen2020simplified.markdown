---
layout: publication
title: 'Simplified Tinybert: Knowledge Distillation For Document Retrieval'
authors: Xuanang Chen, Ben He, Kai Hui, Le Sun, Yingfei Sun
conference: Arxiv
year: 2020
bibkey: chen2020simplified
citations: 7
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2009.07531'}]
tags: ["Efficiency", "Evaluation", "Text Retrieval"]
short_authors: Chen et al.
---
Despite the effectiveness of utilizing the BERT model for document ranking,
the high computational cost of such approaches limits their uses. To this end,
this paper first empirically investigates the effectiveness of two knowledge
distillation models on the document ranking task. In addition, on top of the
recently proposed TinyBERT model, two simplifications are proposed. Evaluations
on two different and widely-used benchmarks demonstrate that Simplified
TinyBERT with the proposed simplifications not only boosts TinyBERT, but also
significantly outperforms BERT-Base when providing 15\(\times\) speedup.