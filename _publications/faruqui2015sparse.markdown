---
layout: publication
title: Sparse Overcomplete Word Vector Representations
authors: Manaal Faruqui, Yulia Tsvetkov, Dani Yogatama, Chris Dyer, Noah Smith
conference: 'Proceedings of the 53rd Annual Meeting of the Association for Computational
  Linguistics and the 7th International Joint Conference on Natural Language Processing
  (Volume 1: Long Papers)'
year: 2015
bibkey: faruqui2015sparse
citations: 165
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1506.02004'}]
tags: []
short_authors: Faruqui et al.
---
Current distributed representations of words show little resemblance to
theories of lexical semantics. The former are dense and uninterpretable, the
latter largely based on familiar, discrete classes (e.g., supersenses) and
relations (e.g., synonymy and hypernymy). We propose methods that transform
word vectors into sparse (and optionally binary) vectors. The resulting
representations are more similar to the interpretable features typically used
in NLP, though they are discovered automatically from raw corpora. Because the
vectors are highly sparse, they are computationally easy to work with. Most
importantly, we find that they outperform the original vectors on benchmark
tasks.