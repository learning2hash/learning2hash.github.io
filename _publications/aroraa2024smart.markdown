---
layout: publication
title: 'Smart Multi-modal Search: Contextual Sparse And Dense Embedding Integration
  In Adobe Express'
authors: Cherag Aroraa, Tracy Holloway King, Jayant Kumar, Yi Lu, Sanat Sharma, Arvind
  Srikantan, David Uvalle, Josep Valls-Vargas, Harsha Vardhan
conference: Arxiv
year: 2024
bibkey: aroraa2024smart
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2408.14698'}]
tags: ["Evaluation", "Image Retrieval", "Large Scale Search", "Multimodal Retrieval", "Robustness", "Scalability", "Similarity Search", "Text Retrieval"]
short_authors: Aroraa et al.
---
As user content and queries become increasingly multi-modal, the need for
effective multi-modal search systems has grown. Traditional search systems
often rely on textual and metadata annotations for indexed images, while
multi-modal embeddings like CLIP enable direct search using text and image
embeddings. However, embedding-based approaches face challenges in integrating
contextual features such as user locale and recency. Building a scalable
multi-modal search system requires fine-tuning several components. This paper
presents a multi-modal search architecture and a series of AB tests that
optimize embeddings and multi-modal technologies in Adobe Express template
search. We address considerations such as embedding model selection, the roles
of embeddings in matching and ranking, and the balance between dense and sparse
embeddings. Our iterative approach demonstrates how utilizing sparse, dense,
and contextual features enhances short and long query search, significantly
reduces null rates (over 70%), and increases click-through rates (CTR). Our
findings provide insights into developing robust multi-modal search systems,
thereby enhancing relevance for complex queries.