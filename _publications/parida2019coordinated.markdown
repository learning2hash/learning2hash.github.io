---
layout: publication
title: Coordinated Joint Multimodal Embeddings For Generalized Audio-visual Zeroshot
  Classification And Retrieval Of Videos
authors: Kranti Kumar Parida, Neeraj Matiyali, Tanaya Guha, Gaurav Sharma
conference: Arxiv
year: 2019
bibkey: parida2019coordinated
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1910.08732'}]
tags: ["Datasets", "Few Shot & Zero Shot", "Multimodal Retrieval"]
short_authors: Parida et al.
---
We present an audio-visual multimodal approach for the task of zeroshot
learning (ZSL) for classification and retrieval of videos. ZSL has been studied
extensively in the recent past but has primarily been limited to visual
modality and to images. We demonstrate that both audio and visual modalities
are important for ZSL for videos. Since a dataset to study the task is
currently not available, we also construct an appropriate multimodal dataset
with 33 classes containing 156,416 videos, from an existing large scale audio
event dataset. We empirically show that the performance improves by adding
audio modality for both tasks of zeroshot classification and retrieval, when
using multimodal extensions of embedding learning methods. We also propose a
novel method to predict the `dominant' modality using a jointly learned
modality attention network. We learn the attention in a semi-supervised setting
and thus do not require any additional explicit labelling for the modalities.
We provide qualitative validation of the modality specific attention, which
also successfully generalizes to unseen test classes.