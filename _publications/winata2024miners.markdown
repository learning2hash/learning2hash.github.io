---
layout: publication
title: 'MINERS: Multilingual Language Models As Semantic Retrievers'
authors: Genta Indra Winata, Ruochen Zhang, David Ifeoluwa Adelani
conference: Arxiv
year: 2024
bibkey: winata2024miners
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2406.07424'}]
tags: [Tools & Libraries, Robustness, Evaluation]
short_authors: Genta Indra Winata, Ruochen Zhang, David Ifeoluwa Adelani
---
Words have been represented in a high-dimensional vector space that encodes
their semantic similarities, enabling downstream applications such as
retrieving synonyms, antonyms, and relevant contexts. However, despite recent
advances in multilingual language models (LMs), the effectiveness of these
models' representations in semantic retrieval contexts has not been
comprehensively explored. To fill this gap, this paper introduces the MINERS, a
benchmark designed to evaluate the ability of multilingual LMs in semantic
retrieval tasks, including bitext mining and classification via
retrieval-augmented contexts. We create a comprehensive framework to assess the
robustness of LMs in retrieving samples across over 200 diverse languages,
including extremely low-resource languages in challenging cross-lingual and
code-switching settings. Our results demonstrate that by solely retrieving
semantically similar embeddings yields performance competitive with
state-of-the-art approaches, without requiring any fine-tuning.