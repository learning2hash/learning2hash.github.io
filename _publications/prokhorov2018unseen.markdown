---
layout: publication
title: Unseen Word Representation By Aligning Heterogeneous Lexical Semantic Spaces
authors: Victor Prokhorov, Mohammad Taher Pilehvar, Dimitri Kartsaklis, Pietro Lio,
  Nigel Collier
conference: Proceedings of the AAAI Conference on Artificial Intelligence
year: 2019
bibkey: prokhorov2018unseen
citations: 12
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1811.04983'}]
tags: ["AAAI"]
short_authors: Prokhorov et al.
---
Word embedding techniques heavily rely on the abundance of training data for
individual words. Given the Zipfian distribution of words in natural language
texts, a large number of words do not usually appear frequently or at all in
the training data. In this paper we put forward a technique that exploits the
knowledge encoded in lexical resources, such as WordNet, to induce embeddings
for unseen words. Our approach adapts graph embedding and cross-lingual vector
space transformation techniques in order to merge lexical knowledge encoded in
ontologies with that derived from corpus statistics. We show that the approach
can provide consistent performance improvements across multiple evaluation
benchmarks: in-vitro, on multiple rare word similarity datasets, and in-vivo,
in two downstream text classification tasks.