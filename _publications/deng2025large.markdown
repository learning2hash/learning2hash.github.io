---
layout: publication
title: Large Vision-language Models For Knowledge-grounded Data Annotation Of Memes
authors: Deng Shiling, Belongie Serge, Christensen Peter Ebert
conference: Arxiv
year: 2025
bibkey: deng2025large
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2501.13851'}]
tags: ["Text-Retrieval", "Scalability", "Tools-&-Libraries", "Datasets", "Evaluation"]
short_authors: Deng Shiling, Belongie Serge, Christensen Peter Ebert
---
Memes have emerged as a powerful form of communication, integrating visual
and textual elements to convey humor, satire, and cultural messages. Existing
research has focused primarily on aspects such as emotion classification, meme
generation, propagation, interpretation, figurative language, and
sociolinguistics, but has often overlooked deeper meme comprehension and
meme-text retrieval. To address these gaps, this study introduces
ClassicMemes-50-templates (CM50), a large-scale dataset consisting of over
33,000 memes, centered around 50 popular meme templates. We also present an
automated knowledge-grounded annotation pipeline leveraging large
vision-language models to produce high-quality image captions, meme captions,
and literary device labels overcoming the labor intensive demands of manual
annotation. Additionally, we propose a meme-text retrieval CLIP model (mtrCLIP)
that utilizes cross-modal embedding to enhance meme analysis, significantly
improving retrieval performance. Our contributions include:(1) a novel dataset
for large-scale meme study, (2) a scalable meme annotation framework, and (3) a
fine-tuned CLIP for meme-text retrieval, all aimed at advancing the
understanding and analysis of memes at scale.