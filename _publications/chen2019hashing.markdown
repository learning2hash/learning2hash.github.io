---
layout: publication
title: Locality45;sensitive Hashing For F45;divergences Mutual Information Loss And Beyond
authors: Lin Chen, Hossein Esfandiari, Gang Fu, Vahab Mirrokni
conference: "Neural Information Processing Systems"
year: 2019
bibkey: chen2019hashing
additional_links:
  - {name: "Paper", url: "https://papers.nips.cc/paper/2019/hash/21b29648a47a45ad16bb0da0c004dfba-Abstract.html"}
tags: ['Independent', 'LSH', 'NEURIPS']
---
Computing approximate nearest neighbors in high dimensional spaces is a central problem in large45;scale data mining with a wide range of applications in machine learning and data science. A popular and effective technique in computing nearest neighbors approximately is the locality45;sensitive hashing (LSH) scheme. In this paper we aim to develop LSH schemes for distance functions that measure the distance between two probability distributions particularly for f45;divergences as well as a generalization to capture mutual information loss. First we provide a general framework to design LHS schemes for f45;divergence distance functions and develop LSH schemes for the generalized Jensen45;Shannon divergence and triangular discrimination in this framework. We show a two45;sided approximation result for approximation of the generalized Jensen45;Shannon divergence by the Hellinger distance which may be of independent interest. Next we show a general method of reducing the problem of designing an LSH scheme for a Krein kernel (which can be expressed as the difference of two positive definite kernels) to the problem of maximum inner product search. We exemplify this method by applying it to the mutual information loss due to its several important applications such as model compression.
