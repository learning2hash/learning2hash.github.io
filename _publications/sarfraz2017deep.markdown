---
layout: publication
title: Deep View-sensitive Pedestrian Attribute Inference In An End-to-end Model
authors: M. Saquib Sarfraz, Arne Schumann, Yan Wang, Rainer Stiefelhagen
conference: Procedings of the British Machine Vision Conference 2017
year: 2017
bibkey: sarfraz2017deep
citations: 51
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1707.06089'}]
tags: []
short_authors: Sarfraz et al.
---
Pedestrian attribute inference is a demanding problem in visual surveillance
that can facilitate person retrieval, search and indexing. To exploit semantic
relations between attributes, recent research treats it as a multi-label image
classification task. The visual cues hinting at attributes can be strongly
localized and inference of person attributes such as hair, backpack, shorts,
etc., are highly dependent on the acquired view of the pedestrian. In this
paper we assert this dependence in an end-to-end learning framework and show
that a view-sensitive attribute inference is able to learn better attribute
predictions. Our proposed model jointly predicts the coarse pose (view) of the
pedestrian and learns specialized view-specific multi-label attribute
predictions. We show in an extensive evaluation on three challenging datasets
(PETA, RAP and WIDER) that our proposed end-to-end view-aware attribute
prediction model provides competitive performance and improves on the published
state-of-the-art on these datasets.