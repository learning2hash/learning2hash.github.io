---
layout: publication
title: 'Uni-mlip: Unified Self-supervision For Medical Vision Language Pre-training'
authors: Ameera Bawazir, Kebin Wu, Wenbin Li
conference: Arxiv
year: 2024
bibkey: bawazir2024uni
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2411.15207'}]
tags: [Text Retrieval, Datasets, Self-Supervised, Evaluation, Tools & Libraries]
short_authors: Ameera Bawazir, Kebin Wu, Wenbin Li
---
Recent advancements in vision-language pre-training via contrastive learning
have significantly improved performance across computer vision tasks. However,
in the medical domain, obtaining multimodal data is often costly and
challenging due to privacy, sensitivity, and annotation complexity. To mitigate
data scarcity while boosting model performance, we introduce \textbf\{Uni-Mlip\},
a unified self-supervision framework specifically designed to enhance medical
vision-language pre-training. Uni-Mlip seamlessly integrates cross-modality,
uni-modality, and fused-modality self-supervision techniques at the data-level
and the feature-level. Additionally, Uni-Mlip tailors uni-modal image
self-supervision to accommodate the unique characteristics of medical images.
Our experiments across datasets of varying scales demonstrate that Uni-Mlip
significantly surpasses current state-of-the-art methods in three key
downstream tasks: image-text retrieval, image classification, and visual
question answering (VQA).