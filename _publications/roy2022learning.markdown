---
layout: publication
title: Learning Deep Optimal Embeddings With Sinkhorn Divergences
authors: Soumava Kumar Roy, Yan Han, Mehrtash Harandi, Lars Petersson
conference: SSRN Electronic Journal
year: 2022
bibkey: roy2022learning
citations: 0
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2209.06469'}]
tags: ["Distance Metric Learning"]
short_authors: Roy et al.
---
Deep Metric Learning algorithms aim to learn an efficient embedding space to
preserve the similarity relationships among the input data. Whilst these
algorithms have achieved significant performance gains across a wide plethora
of tasks, they have also failed to consider and increase comprehensive
similarity constraints; thus learning a sub-optimal metric in the embedding
space. Moreover, up until now; there have been few studies with respect to
their performance in the presence of noisy labels. Here, we address the concern
of learning a discriminative deep embedding space by designing a novel, yet
effective Deep Class-wise Discrepancy Loss (DCDL) function that segregates the
underlying similarity distributions (thus introducing class-wise discrepancy)
of the embedding points between each and every class. Our empirical results
across three standard image classification datasets and two fine-grained image
recognition datasets in the presence and absence of noise clearly demonstrate
the need for incorporating such class-wise similarity relationships along with
traditional algorithms while learning a discriminative embedding space.