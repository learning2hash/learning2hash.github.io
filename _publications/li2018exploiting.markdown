---
layout: publication
title: Exploiting Kernel Sparsity And Entropy For Interpretable CNN Compression
authors: Yuchao Li, Shaohui Lin, Baochang Zhang, Jianzhuang Liu, David Doermann, Yongjian
  Wu, Feiyue Huang, Rongrong Ji
conference: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
year: 2019
bibkey: li2018exploiting
citations: 131
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1812.04368'}]
tags: ["CVPR", "Efficiency"]
short_authors: Li et al.
---
Compressing convolutional neural networks (CNNs) has received ever-increasing
research focus. However, most existing CNN compression methods do not interpret
their inherent structures to distinguish the implicit redundancy. In this
paper, we investigate the problem of CNN compression from a novel interpretable
perspective. The relationship between the input feature maps and 2D kernels is
revealed in a theoretical framework, based on which a kernel sparsity and
entropy (KSE) indicator is proposed to quantitate the feature map importance in
a feature-agnostic manner to guide model compression. Kernel clustering is
further conducted based on the KSE indicator to accomplish high-precision CNN
compression. KSE is capable of simultaneously compressing each layer in an
efficient way, which is significantly faster compared to previous data-driven
feature map pruning methods. We comprehensively evaluate the compression and
speedup of the proposed method on CIFAR-10, SVHN and ImageNet 2012. Our method
demonstrates superior performance gains over previous ones. In particular, it
achieves 4.7 \times FLOPs reduction and 2.9 \times compression on ResNet-50
with only a Top-5 accuracy drop of 0.35% on ImageNet 2012, which significantly
outperforms state-of-the-art methods.