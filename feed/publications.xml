<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://learning2hash.github.io/feed/publications.xml" rel="self" type="application/atom+xml" /><link href="https://learning2hash.github.io/" rel="alternate" type="text/html" /><updated>2026-01-24T07:06:35-06:00</updated><id>https://learning2hash.github.io/feed/publications.xml</id><title type="html">Awesome Learning to Hash | Publications</title><subtitle>A Webpage dedicated to the latest research on Hash Function Learning. Maintained by &lt;a href=&quot;https://www.buymeacoffee.com/sjmoran&quot;&gt;Sean Moran&lt;/a&gt;.</subtitle><entry><title type="html">Estimating And Abstracting The 3D Structure Of Bones Using Neural Networks On X-ray (2D) Images</title><link href="https://learning2hash.github.io/publications/%C4%8Davojsk%C3%A12020estimating/" rel="alternate" type="text/html" title="Estimating And Abstracting The 3D Structure Of Bones Using Neural Networks On X-ray (2D) Images" /><published>2026-01-24T07:06:35-06:00</published><updated>2026-01-24T07:06:35-06:00</updated><id>https://learning2hash.github.io/publications/%C4%8Davojsk%C3%A12020estimating</id><content type="html" xml:base="https://learning2hash.github.io/publications/%C4%8Davojsk%C3%A12020estimating/"><![CDATA[<p>In this paper, we present a deep-learning based method for estimating the 3D
structure of a bone from a pair of 2D X-ray images. Our triplet loss-trained
neural network selects the most closely matching 3D bone shape from a
predefined set of shapes. Our predictions have an average root mean square
(RMS) distance of 1.08 mm between the predicted and true shapes, making it more
accurate than the average error achieved by eight other examined 3D bone
reconstruction approaches. The prediction process that we use is fully
automated and unlike many competing approaches, it does not rely on any
previous knowledge about bone geometry. Additionally, our neural network can
determine the identity of a bone based only on its X-ray image. It computes a
low-dimensional representation (“embedding”) of each 2D X-ray image and
henceforth compares different X-ray images based only on their embeddings. An
embedding holds enough information to uniquely identify the bone CT belonging
to the input X-ray image with a 100% accuracy and can therefore serve as a kind
of fingerprint for that bone. Possible applications include faster, image
content-based bone database searches for forensic purposes.</p>]]></content><author><name></name></author><category term="Distance Metric Learning" /><category term="Neural Hashing" /><summary type="html"><![CDATA[In this paper, we present a deep-learning based method for estimating the 3D structure of a bone from a pair of 2D X-ray images. Our triplet loss-trained neural network selects the most closely matching 3D bone shape from a predefined set of shapes. Our predictions have an average root mean square (RMS) distance of 1.08 mm between the predicted and true shapes, making it more accurate than the average error achieved by eight other examined 3D bone reconstruction approaches. The prediction process that we use is fully automated and unlike many competing approaches, it does not rely on any previous knowledge about bone geometry. Additionally, our neural network can determine the identity of a bone based only on its X-ray image. It computes a low-dimensional representation (“embedding”) of each 2D X-ray image and henceforth compares different X-ray images based only on their embeddings. An embedding holds enough information to uniquely identify the bone CT belonging to the input X-ray image with a 100% accuracy and can therefore serve as a kind of fingerprint for that bone. Possible applications include faster, image content-based bone database searches for forensic purposes.]]></summary></entry><entry><title type="html">Pattern Spotting In Historical Documents Using Convolutional Models</title><link href="https://learning2hash.github.io/publications/%C3%BAbeda2019pattern/" rel="alternate" type="text/html" title="Pattern Spotting In Historical Documents Using Convolutional Models" /><published>2026-01-24T07:06:35-06:00</published><updated>2026-01-24T07:06:35-06:00</updated><id>https://learning2hash.github.io/publications/%C3%BAbeda2019pattern</id><content type="html" xml:base="https://learning2hash.github.io/publications/%C3%BAbeda2019pattern/"><![CDATA[<p>Pattern spotting consists of searching in a collection of historical document
images for occurrences of a graphical object using an image query. Contrary to
object detection, no prior information nor predefined class is given about the
query so training a model of the object is not feasible. In this paper, a
convolutional neural network approach is proposed to tackle this problem. We
use RetinaNet as a feature extractor to obtain multiscale embeddings of the
regions of the documents and also for the queries. Experiments conducted on the
DocExplore dataset show that our proposal is better at locating patterns and
requires less storage for indexing images than the state-of-the-art system, but
fails at retrieving some pages containing multiple instances of the query.</p>]]></content><author><name></name></author><category term="Datasets" /><summary type="html"><![CDATA[Pattern spotting consists of searching in a collection of historical document images for occurrences of a graphical object using an image query. Contrary to object detection, no prior information nor predefined class is given about the query so training a model of the object is not feasible. In this paper, a convolutional neural network approach is proposed to tackle this problem. We use RetinaNet as a feature extractor to obtain multiscale embeddings of the regions of the documents and also for the queries. Experiments conducted on the DocExplore dataset show that our proposal is better at locating patterns and requires less storage for indexing images than the state-of-the-art system, but fails at retrieving some pages containing multiple instances of the query.]]></summary></entry><entry><title type="html">Content-based Medical Image Retrieval With Opponent Class Adaptive Margin Loss</title><link href="https://learning2hash.github.io/publications/%C3%B6zt%C3%BCrk2022content/" rel="alternate" type="text/html" title="Content-based Medical Image Retrieval With Opponent Class Adaptive Margin Loss" /><published>2026-01-24T07:06:35-06:00</published><updated>2026-01-24T07:06:35-06:00</updated><id>https://learning2hash.github.io/publications/%C3%B6zt%C3%BCrk2022content</id><content type="html" xml:base="https://learning2hash.github.io/publications/%C3%B6zt%C3%BCrk2022content/"><![CDATA[<p>Broadspread use of medical imaging devices with digital storage has paved the
way for curation of substantial data repositories. Fast access to image samples
with similar appearance to suspected cases can help establish a consulting
system for healthcare professionals, and improve diagnostic procedures while
minimizing processing delays. However, manual querying of large data
repositories is labor intensive. Content-based image retrieval (CBIR) offers an
automated solution based on dense embedding vectors that represent image
features to allow quantitative similarity assessments. Triplet learning has
emerged as a powerful approach to recover embeddings in CBIR, albeit
traditional loss functions ignore the dynamic relationship between opponent
image classes. Here, we introduce a triplet-learning method for automated
querying of medical image repositories based on a novel Opponent Class Adaptive
Margin (OCAM) loss. OCAM uses a variable margin value that is updated
continually during the course of training to maintain optimally discriminative
representations. CBIR performance of OCAM is compared against state-of-the-art
loss functions for representational learning on three public databases
(gastrointestinal disease, skin lesion, lung disease). Comprehensive
experiments in each application domain demonstrate the superior performance of
OCAM against baselines.</p>]]></content><author><name></name></author><category term="Evaluation" /><category term="Image Retrieval" /><category term="Tools &amp; Libraries" /><summary type="html"><![CDATA[Broadspread use of medical imaging devices with digital storage has paved the way for curation of substantial data repositories. Fast access to image samples with similar appearance to suspected cases can help establish a consulting system for healthcare professionals, and improve diagnostic procedures while minimizing processing delays. However, manual querying of large data repositories is labor intensive. Content-based image retrieval (CBIR) offers an automated solution based on dense embedding vectors that represent image features to allow quantitative similarity assessments. Triplet learning has emerged as a powerful approach to recover embeddings in CBIR, albeit traditional loss functions ignore the dynamic relationship between opponent image classes. Here, we introduce a triplet-learning method for automated querying of medical image repositories based on a novel Opponent Class Adaptive Margin (OCAM) loss. OCAM uses a variable margin value that is updated continually during the course of training to maintain optimally discriminative representations. CBIR performance of OCAM is compared against state-of-the-art loss functions for representational learning on three public databases (gastrointestinal disease, skin lesion, lung disease). Comprehensive experiments in each application domain demonstrate the superior performance of OCAM against baselines.]]></summary></entry><entry><title type="html">A Comparison Of CNN And Classic Features For Image Retrieval</title><link href="https://learning2hash.github.io/publications/%C3%B6zayd%C4%B1n2019a/" rel="alternate" type="text/html" title="A Comparison Of CNN And Classic Features For Image Retrieval" /><published>2026-01-24T07:06:35-06:00</published><updated>2026-01-24T07:06:35-06:00</updated><id>https://learning2hash.github.io/publications/%C3%B6zayd%C4%B1n2019a</id><content type="html" xml:base="https://learning2hash.github.io/publications/%C3%B6zayd%C4%B1n2019a/"><![CDATA[<p>Feature detectors and descriptors have been successfully used for various
computer vision tasks, such as video object tracking and content-based image
retrieval. Many methods use image gradients in different stages of the
detection-description pipeline to describe local image structures. Recently,
some, or all, of these stages have been replaced by convolutional neural
networks (CNNs), in order to increase their performance. A detector is defined
as a selection problem, which makes it more challenging to implement as a CNN.
They are therefore generally defined as regressors, converting input images to
score maps and keypoints can be selected with non-maximum suppression. This
paper discusses and compares several recent methods that use CNNs for keypoint
detection. Experiments are performed both on the CNN based approaches, as well
as a selection of conventional methods. In addition to qualitative measures
defined on keypoints and descriptors, the bag-of-words (BoW) model is used to
implement an image retrieval application, in order to determine how the methods
perform in practice. The results show that each type of features are best in
different contexts.</p>]]></content><author><name></name></author><category term="Evaluation" /><category term="Image Retrieval" /><summary type="html"><![CDATA[Feature detectors and descriptors have been successfully used for various computer vision tasks, such as video object tracking and content-based image retrieval. Many methods use image gradients in different stages of the detection-description pipeline to describe local image structures. Recently, some, or all, of these stages have been replaced by convolutional neural networks (CNNs), in order to increase their performance. A detector is defined as a selection problem, which makes it more challenging to implement as a CNN. They are therefore generally defined as regressors, converting input images to score maps and keypoints can be selected with non-maximum suppression. This paper discusses and compares several recent methods that use CNNs for keypoint detection. Experiments are performed both on the CNN based approaches, as well as a selection of conventional methods. In addition to qualitative measures defined on keypoints and descriptors, the bag-of-words (BoW) model is used to implement an image retrieval application, in order to determine how the methods perform in practice. The results show that each type of features are best in different contexts.]]></summary></entry><entry><title type="html">Self-supervised Bernoulli Autoencoders For Semi-supervised Hashing</title><link href="https://learning2hash.github.io/publications/%C3%B1anculef2020self/" rel="alternate" type="text/html" title="Self-supervised Bernoulli Autoencoders For Semi-supervised Hashing" /><published>2026-01-24T07:06:35-06:00</published><updated>2026-01-24T07:06:35-06:00</updated><id>https://learning2hash.github.io/publications/%C3%B1anculef2020self</id><content type="html" xml:base="https://learning2hash.github.io/publications/%C3%B1anculef2020self/"><![CDATA[<p>Semantic hashing is an emerging technique for large-scale similarity search
based on representing high-dimensional data using similarity-preserving binary
codes used for efficient indexing and search. It has recently been shown that
variational autoencoders, with Bernoulli latent representations parametrized by
neural nets, can be successfully trained to learn such codes in supervised and
unsupervised scenarios, improving on more traditional methods thanks to their
ability to handle the binary constraints architecturally. However, the scenario
where labels are scarce has not been studied yet.
  This paper investigates the robustness of hashing methods based on
variational autoencoders to the lack of supervision, focusing on two
semi-supervised approaches currently in use. The first augments the variational
autoencoder’s training objective to jointly model the distribution over the
data and the class labels. The second approach exploits the annotations to
define an additional pairwise loss that enforces consistency between the
similarity in the code (Hamming) space and the similarity in the label space.
Our experiments show that both methods can significantly increase the hash
codes’ quality. The pairwise approach can exhibit an advantage when the number
of labelled points is large. However, we found that this method degrades
quickly and loses its advantage when labelled samples decrease. To circumvent
this problem, we propose a novel supervision method in which the model uses its
label distribution predictions to implement the pairwise objective. Compared to
the best baseline, this procedure yields similar performance in fully
supervised settings but improves the results significantly when labelled data
is scarce. Our code is made publicly available at
https://github.com/amacaluso/SSB-VAE.</p>]]></content><author><name></name></author><category term="Hashing Methods" /><category term="Self-Supervised" /><category term="Similarity Search" /><category term="Supervised" /><category term="Unsupervised" /><summary type="html"><![CDATA[Semantic hashing is an emerging technique for large-scale similarity search based on representing high-dimensional data using similarity-preserving binary codes used for efficient indexing and search. It has recently been shown that variational autoencoders, with Bernoulli latent representations parametrized by neural nets, can be successfully trained to learn such codes in supervised and unsupervised scenarios, improving on more traditional methods thanks to their ability to handle the binary constraints architecturally. However, the scenario where labels are scarce has not been studied yet. This paper investigates the robustness of hashing methods based on variational autoencoders to the lack of supervision, focusing on two semi-supervised approaches currently in use. The first augments the variational autoencoder’s training objective to jointly model the distribution over the data and the class labels. The second approach exploits the annotations to define an additional pairwise loss that enforces consistency between the similarity in the code (Hamming) space and the similarity in the label space. Our experiments show that both methods can significantly increase the hash codes’ quality. The pairwise approach can exhibit an advantage when the number of labelled points is large. However, we found that this method degrades quickly and loses its advantage when labelled samples decrease. To circumvent this problem, we propose a novel supervision method in which the model uses its label distribution predictions to implement the pairwise objective. Compared to the best baseline, this procedure yields similar performance in fully supervised settings but improves the results significantly when labelled data is scarce. Our code is made publicly available at https://github.com/amacaluso/SSB-VAE.]]></summary></entry><entry><title type="html">The Nt-xent Loss Upper Bound</title><link href="https://learning2hash.github.io/publications/%C3%A5gren2022the/" rel="alternate" type="text/html" title="The Nt-xent Loss Upper Bound" /><published>2026-01-24T07:06:35-06:00</published><updated>2026-01-24T07:06:35-06:00</updated><id>https://learning2hash.github.io/publications/%C3%A5gren2022the</id><content type="html" xml:base="https://learning2hash.github.io/publications/%C3%A5gren2022the/"><![CDATA[<p>Self-supervised learning is a growing paradigm in deep representation
learning, showing great generalization capabilities and competitive performance
in low-labeled data regimes. The SimCLR framework proposes the NT-Xent loss for
contrastive representation learning. The objective of the loss function is to
maximize agreement, similarity, between sampled positive pairs. This short
paper derives and proposes an upper bound for the loss and average similarity.
An analysis of the implications is however not provided, but we strongly
encourage anyone in the field to conduct this.</p>]]></content><author><name></name></author><category term="Evaluation" /><category term="Self-Supervised" /><category term="Supervised" /><category term="Tools &amp; Libraries" /><summary type="html"><![CDATA[Self-supervised learning is a growing paradigm in deep representation learning, showing great generalization capabilities and competitive performance in low-labeled data regimes. The SimCLR framework proposes the NT-Xent loss for contrastive representation learning. The objective of the loss function is to maximize agreement, similarity, between sampled positive pairs. This short paper derives and proposes an upper bound for the loss and average similarity. An analysis of the implications is however not provided, but we strongly encourage anyone in the field to conduct this.]]></summary></entry><entry><title type="html">Angle Tree: Nearest Neighbor Search In High Dimensions With Low Intrinsic Dimensionality</title><link href="https://learning2hash.github.io/publications/zvedeniouk2010angle/" rel="alternate" type="text/html" title="Angle Tree: Nearest Neighbor Search In High Dimensions With Low Intrinsic Dimensionality" /><published>2026-01-24T07:06:35-06:00</published><updated>2026-01-24T07:06:35-06:00</updated><id>https://learning2hash.github.io/publications/zvedeniouk2010angle</id><content type="html" xml:base="https://learning2hash.github.io/publications/zvedeniouk2010angle/"><![CDATA[<p>We propose an extension of tree-based space-partitioning indexing structures
for data with low intrinsic dimensionality embedded in a high dimensional
space. We call this extension an Angle Tree. Our extension can be applied to
both classical kd-trees as well as the more recent rp-trees. The key idea of
our approach is to store the angle (the “dihedral angle”) between the data
region (which is a low dimensional manifold) and the random hyperplane that
splits the region (the “splitter”). We show that the dihedral angle can be used
to obtain a tight lower bound on the distance between the query point and any
point on the opposite side of the splitter. This in turn can be used to
efficiently prune the search space. We introduce a novel randomized strategy to
efficiently calculate the dihedral angle with a high degree of accuracy.
Experiments and analysis on real and synthetic data sets shows that the Angle
Tree is the most efficient known indexing structure for nearest neighbor
queries in terms of preprocessing and space usage while achieving high accuracy
and fast search time.</p>]]></content><author><name></name></author><category term="Tree Based ANN" /><summary type="html"><![CDATA[We propose an extension of tree-based space-partitioning indexing structures for data with low intrinsic dimensionality embedded in a high dimensional space. We call this extension an Angle Tree. Our extension can be applied to both classical kd-trees as well as the more recent rp-trees. The key idea of our approach is to store the angle (the “dihedral angle”) between the data region (which is a low dimensional manifold) and the random hyperplane that splits the region (the “splitter”). We show that the dihedral angle can be used to obtain a tight lower bound on the distance between the query point and any point on the opposite side of the splitter. This in turn can be used to efficiently prune the search space. We introduce a novel randomized strategy to efficiently calculate the dihedral angle with a high degree of accuracy. Experiments and analysis on real and synthetic data sets shows that the Angle Tree is the most efficient known indexing structure for nearest neighbor queries in terms of preprocessing and space usage while achieving high accuracy and fast search time.]]></summary></entry><entry><title type="html">Lib-sibgmu – A University Library Circulation Dataset For Recommender Systems Developmen</title><link href="https://learning2hash.github.io/publications/zubchuk2022lib/" rel="alternate" type="text/html" title="Lib-sibgmu – A University Library Circulation Dataset For Recommender Systems Developmen" /><published>2026-01-24T07:06:35-06:00</published><updated>2026-01-24T07:06:35-06:00</updated><id>https://learning2hash.github.io/publications/zubchuk2022lib</id><content type="html" xml:base="https://learning2hash.github.io/publications/zubchuk2022lib/"><![CDATA[<p>We opensource under CC BY 4.0 license Lib-SibGMU - a university library
circulation dataset - for a wide research community, and benchmark major
algorithms for recommender systems on this dataset. For a recommender
architecture that consists of a vectorizer that turns the history of the books
borrowed into a vector, and a neighborhood-based recommender, trained
separately, we show that using the fastText model as a vectorizer delivers
competitive results.</p>]]></content><author><name></name></author><category term="Datasets" /><category term="Evaluation" /><category term="Recommender Systems" /><category term="Tools &amp; Libraries" /><summary type="html"><![CDATA[We opensource under CC BY 4.0 license Lib-SibGMU - a university library circulation dataset - for a wide research community, and benchmark major algorithms for recommender systems on this dataset. For a recommender architecture that consists of a vectorizer that turns the history of the books borrowed into a vector, and a neighborhood-based recommender, trained separately, we show that using the fastText model as a vectorizer delivers competitive results.]]></summary></entry><entry><title type="html">Prompthash: Affinity-prompted Collaborative Cross-modal Learning For Adaptive Hashing Retrieval</title><link href="https://learning2hash.github.io/publications/zou2025prompthash/" rel="alternate" type="text/html" title="Prompthash: Affinity-prompted Collaborative Cross-modal Learning For Adaptive Hashing Retrieval" /><published>2026-01-24T07:06:35-06:00</published><updated>2026-01-24T07:06:35-06:00</updated><id>https://learning2hash.github.io/publications/zou2025prompthash</id><content type="html" xml:base="https://learning2hash.github.io/publications/zou2025prompthash/"><![CDATA[<p>Cross-modal hashing is a promising approach for efficient data retrieval and
storage optimization. However, contemporary methods exhibit significant
limitations in semantic preservation, contextual integrity, and information
redundancy, which constrains retrieval efficacy. We present PromptHash, an
innovative framework leveraging affinity prompt-aware collaborative learning
for adaptive cross-modal hashing. We propose an end-to-end framework for
affinity-prompted collaborative hashing, with the following fundamental
technical contributions: (i) a text affinity prompt learning mechanism that
preserves contextual information while maintaining parameter efficiency, (ii)
an adaptive gated selection fusion architecture that synthesizes State Space
Model with Transformer network for precise cross-modal feature integration, and
(iii) a prompt affinity alignment strategy that bridges modal heterogeneity
through hierarchical contrastive learning. To the best of our knowledge, this
study presents the first investigation into affinity prompt awareness within
collaborative cross-modal adaptive hash learning, establishing a paradigm for
enhanced semantic consistency across modalities. Through comprehensive
evaluation on three benchmark multi-label datasets, PromptHash demonstrates
substantial performance improvements over existing approaches. Notably, on the
NUS-WIDE dataset, our method achieves significant gains of 18.22% and 18.65% in
image-to-text and text-to-image retrieval tasks, respectively. The code is
publicly available at https://github.com/ShiShuMo/PromptHash.</p>]]></content><author><name></name></author><category term="Datasets" /><category term="Efficiency" /><category term="Evaluation" /><category term="Hashing Methods" /><category term="Image Retrieval" /><summary type="html"><![CDATA[Cross-modal hashing is a promising approach for efficient data retrieval and storage optimization. However, contemporary methods exhibit significant limitations in semantic preservation, contextual integrity, and information redundancy, which constrains retrieval efficacy. We present PromptHash, an innovative framework leveraging affinity prompt-aware collaborative learning for adaptive cross-modal hashing. We propose an end-to-end framework for affinity-prompted collaborative hashing, with the following fundamental technical contributions: (i) a text affinity prompt learning mechanism that preserves contextual information while maintaining parameter efficiency, (ii) an adaptive gated selection fusion architecture that synthesizes State Space Model with Transformer network for precise cross-modal feature integration, and (iii) a prompt affinity alignment strategy that bridges modal heterogeneity through hierarchical contrastive learning. To the best of our knowledge, this study presents the first investigation into affinity prompt awareness within collaborative cross-modal adaptive hash learning, establishing a paradigm for enhanced semantic consistency across modalities. Through comprehensive evaluation on three benchmark multi-label datasets, PromptHash demonstrates substantial performance improvements over existing approaches. Notably, on the NUS-WIDE dataset, our method achieves significant gains of 18.22% and 18.65% in image-to-text and text-to-image retrieval tasks, respectively. The code is publicly available at https://github.com/ShiShuMo/PromptHash.]]></summary></entry><entry><title type="html">Retrieval Robust To Object Motion Blur</title><link href="https://learning2hash.github.io/publications/zou2024retrieval/" rel="alternate" type="text/html" title="Retrieval Robust To Object Motion Blur" /><published>2026-01-24T07:06:35-06:00</published><updated>2026-01-24T07:06:35-06:00</updated><id>https://learning2hash.github.io/publications/zou2024retrieval</id><content type="html" xml:base="https://learning2hash.github.io/publications/zou2024retrieval/"><![CDATA[<p>Moving objects are frequently seen in daily life and usually appear blurred
in images due to their motion. While general object retrieval is a widely
explored area in computer vision, it primarily focuses on sharp and static
objects, and retrieval of motion-blurred objects in large image collections
remains unexplored. We propose a method for object retrieval in images that are
affected by motion blur. The proposed method learns a robust representation
capable of matching blurred objects to their deblurred versions and vice versa.
To evaluate our approach, we present the first large-scale datasets for blurred
object retrieval, featuring images with objects exhibiting varying degrees of
blur in various poses and scales. We conducted extensive experiments, showing
that our method outperforms state-of-the-art retrieval methods on the new
blur-retrieval datasets, which validates the effectiveness of the proposed
approach. Code, data, and model are available at
https://github.com/Rong-Zou/Retrieval-Robust-to-Object-Motion-Blur.</p>]]></content><author><name></name></author><category term="Datasets" /><category term="Scalability" /><summary type="html"><![CDATA[Moving objects are frequently seen in daily life and usually appear blurred in images due to their motion. While general object retrieval is a widely explored area in computer vision, it primarily focuses on sharp and static objects, and retrieval of motion-blurred objects in large image collections remains unexplored. We propose a method for object retrieval in images that are affected by motion blur. The proposed method learns a robust representation capable of matching blurred objects to their deblurred versions and vice versa. To evaluate our approach, we present the first large-scale datasets for blurred object retrieval, featuring images with objects exhibiting varying degrees of blur in various poses and scales. We conducted extensive experiments, showing that our method outperforms state-of-the-art retrieval methods on the new blur-retrieval datasets, which validates the effectiveness of the proposed approach. Code, data, and model are available at https://github.com/Rong-Zou/Retrieval-Robust-to-Object-Motion-Blur.]]></summary></entry></feed>