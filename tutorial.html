<!DOCTYPE html>
<html lang="en-us">

  <head>
<!-- Begin Web-Stat code v 7.0 -->
<span id="wts2185304"></span>
<script>var wts=document.createElement('script');wts.async=true;
wts.src='https://app.ardalio.com/log7.js';document.head.appendChild(wts);
wts.onload = function(){ wtslog7(2185304,4); };
</script><noscript><a href="https://www.web-stat.com">
<img src="https://app.ardalio.com/7/4/2185304.png" 
alt="Web-Stat web statistics"></a></noscript>
<!-- End Web-Stat code v 7.0 -->
  <!-- Hotjar Tracking Code for https://learning2hash.github.io/ -->
<script>
    (function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:1843243,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109544763-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-109544763-1');
</script>

  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="keywords" content="machine learning, hashing, approximate nearest neighbour search, lsh, learning-to-hash">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Tutorial | Awesome Learning to Hash</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Tutorial" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A Webpage dedicated to the latest research on Hash Function Learning. Maintained by Sean Moran." />
<meta property="og:description" content="A Webpage dedicated to the latest research on Hash Function Learning. Maintained by Sean Moran." />
<link rel="canonical" href="https://learning2hash.github.io/https://learning2hash.github.io/tutorial.html" />
<meta property="og:url" content="https://learning2hash.github.io/https://learning2hash.github.io/tutorial.html" />
<meta property="og:site_name" content="Awesome Learning to Hash" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Tutorial" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"A Webpage dedicated to the latest research on Hash Function Learning. Maintained by Sean Moran.","headline":"Tutorial","url":"https://learning2hash.github.io/https://learning2hash.github.io/tutorial.html"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="https://learning2hash.github.io/public/css/poole.css">
  <link rel="stylesheet" href="https://learning2hash.github.io/public/css/syntax.css">
  <link rel="stylesheet" href="https://learning2hash.github.io/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link rel="search" href="/public/opensearchdescription.xml" 
      type="application/opensearchdescription+xml" 
      title="learning2hash" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
  integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
  crossorigin="anonymous"></script>
  
  <link rel="stylesheet" type="text/css" href="//cdn.datatables.net/1.10.16/css/jquery.dataTables.min.css">
  <script type="text/javascript" charset="utf8" src="//cdn.datatables.net/1.10.16/js/jquery.dataTables.min.js"></script>
</head>


  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <body class="theme-base-0c layout-reverse">

    <a href='/contributing.html' class='ribbon'>Add your paper to Learning2Hash</a>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Awesome Learning to Hash
        </a>
      </h1>
      <p class="lead">A Webpage dedicated to the latest research on Hash Function Learning. Maintained by <a href="http://sjmoran.github.io/">Sean Moran</a>.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="sidebar-item">
        <p style="font-size: 12px">
          Search related work 
          <input type='text' id='searchTarget' size="16"/> 
          <button onClick="search();">Go</button>
        </p>
      </div>
      <a class="sidebar-nav-item" href="/papers.html">All Papers</a>
      <a class="sidebar-nav-item" href="/https://learning2hash.github.io/tags.html">Papers by Tag</a>
      <a class="sidebar-nav-item" href="/https://learning2hash.github.io/tsne-viz.html">2D Map of Papers</a>
      <a class="sidebar-nav-item" href="/https://learning2hash.github.io/topic-viz.html">Topic-based Explorer</a>
      <a class="sidebar-nav-item" href="/https://learning2hash.github.io/base-taxonomy/">Core Taxonomy</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/datasets.html">Datasets</a>
      <a class="sidebar-nav-item" href="/tutorial.html">Tutorial</a>
      <a class="sidebar-nav-item" href="/https://learning2hash.github.io/resources.html">Resources, Courses &#38; Events</a>
      <a class="sidebar-nav-item" href="/https://learning2hash.github.io/contributing.html">Contributing</a>
    </nav>

    <div class="sidebar-item">
      <p style="font-size: 12px">
        Contact <a href="http://www.seanjmoran.com">Sean Moran</a> about this survey or website.
        <span style="font-size: 9px">
          Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
        </span>
      </p>
    </div>
  </div>
</div>

<script>
$("#searchTarget").keydown(function (e) {	
  if (e.keyCode == 13) {
    search();
  }
});

function search() {
  try {
    ga('send', 'event', 'search', 'search', $("#searchTarget").val());
  } finally {
    window.location = "/https://learning2hash.github.io/papers.html#" + $("#searchTarget").val();
  }
}
</script>


    <div class="content container">
      <h1 id="learning-to-hash-tutorial">Learning To Hash Tutorial</h1>

<h3 id="overview">Overview</h3>

<p>In this tutorial, we dive into a <a href="https://learning2hash.github.io/publications/moran2015agraph/">published learning to hash model</a> and compare its image retrieval performance against Locality Sensitive Hashing (LSH).</p>

<p>We will specifically explore the <a href="https://learning2hash.github.io/publications/moran2015agraph/">Graph Regularized Hashing (GRH)</a> model by Moran and Lavrenko, a simple yet highly effective supervised hashing method for learning to hash. This model was later <a href="https://dl.acm.org/doi/abs/10.1145/2766462.2767816">extended for cross-modal hashing</a>.</p>

<h3 id="preliminaries">Preliminaries</h3>

<p>The original MATLAB code for the GRH model by Moran and Lavrenko is available <a href="https://github.com/sjmoran/GRH">here</a>, but we’ll re-implement it in Python 3. We’ll train the model on the CIFAR-10 dataset and evaluate its performance using precision at 10 and semantic nearest neighbor evaluation, comparing it to LSH.</p>

<p>To get started, set up a virtual environment:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 <span class="nt">-m</span> venv ./hashing_tutorial
<span class="nb">source </span>hashing_tutorial/bin/activate
pip3 <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
</code></pre></div></div>

<p>Next, we retrieve and preprocess the CIFAR-10 dataset. We’ll use pre-processed <a href="http://people.csail.mit.edu/torralba/code/spatialenvelope/">GIST</a> features for this task.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">scipy.io</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">requests</span>

<span class="c1"># Step 1: Download and save the pre-processed CIFAR-10 dataset
</span><span class="n">url</span><span class="o">=</span><span class="s">'https://www.dropbox.com/s/875u1rkva9iffpj/Gist512CIFAR10.mat?dl=1'</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="s">"./"</span><span class="p">,</span> <span class="s">"Gist512CIFAR10.mat"</span><span class="p">),</span> <span class="s">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">content</span><span class="p">)</span>

<span class="c1"># Step 2: Load the dataset
</span><span class="n">mat</span> <span class="o">=</span> <span class="n">scipy</span><span class="p">.</span><span class="n">io</span><span class="p">.</span><span class="n">loadmat</span><span class="p">(</span><span class="s">'./Gist512CIFAR10.mat'</span><span class="p">)</span>

<span class="c1"># Extract the data and class labels from the dataset
</span><span class="n">data</span> <span class="o">=</span> <span class="n">mat</span><span class="p">[</span><span class="s">'X'</span><span class="p">]</span>
<span class="n">classes</span> <span class="o">=</span> <span class="n">mat</span><span class="p">[</span><span class="s">'X_class'</span><span class="p">]</span>

<span class="c1"># Step 3: Normalize the data to ensure it's ready for indexing
</span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">Normalizer</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">Normalizer</span><span class="p">(</span><span class="n">norm</span><span class="o">=</span><span class="s">'l2'</span><span class="p">).</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>  <span class="c1"># L2 normalization
</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span> <span class="o">-</span> <span class="n">data</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Mean centering the data
</span></code></pre></div></div>
<p>The code above does three things:</p>

<ol>
  <li>Downloads the pre-processed CIFAR-10 dataset.</li>
  <li>Loads the data into memory using scipy.io.loadmat.</li>
  <li>Normalizes the data using L2 normalization and mean-centers it. This step is crucial to ensure consistent indexing later.</li>
</ol>

<h3 id="quick-start">Quick Start</h3>
<p>If you want to skip ahead, you can run the entire tutorial using this command:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">python3</span> <span class="n">hashing_tutorial</span><span class="p">.</span><span class="n">py</span> 
</code></pre></div></div>

<p>This will automatically download the dataset, train the GRH model, and evaluate its performance.</p>

<h3 id="implementation-lsh">Implementation (LSH)</h3>
<p>Now, let’s jump into Locality Sensitive Hashing (LSH). We’ll generate 16 random hyperplanes (which translates into a 16-bit hashcode), and project the first image onto them to generate its LSH hashcode.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Parameters for LSH
</span><span class="n">n_vectors</span> <span class="o">=</span> <span class="mi">32</span>  <span class="c1"># Number of random hyperplanes
</span><span class="n">dim</span> <span class="o">=</span> <span class="mi">512</span>  <span class="c1"># Dimensions in GIST features
</span>
<span class="c1"># Step 1: Generate random hyperplanes
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">random_vectors</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">n_vectors</span><span class="p">)</span>

<span class="c1"># Step 2: Project the first image onto these hyperplanes to generate its hashcode
</span><span class="n">bin_indices_bits</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:].</span><span class="n">dot</span><span class="p">(</span><span class="n">random_vectors</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">0</span>  <span class="c1"># Binary hashcode based on sign
</span>
<span class="k">print</span><span class="p">(</span><span class="n">bin_indices_bits</span><span class="p">)</span>
<span class="c1"># Output: [False  True False  True False  True False False False False  True  True True False False False]
</span></code></pre></div></div>

<p>Here’s what’s happening:</p>

<p>We generate random hyperplanes and project the first image onto them. The result is a binary hashcode that represents the image. Any two images with the same hashcode will land in the same bucket in our hashtable.</p>

<p>Next, we’ll convert this binary hashcode into an integer for easier indexing.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># https://wiki.python.org/moin/BitwiseOperators
# x &lt;&lt; y (bitwise left shift) is the same as multiplying x by 2 ** y.
# This is a bitwise operation that effectively gives us the powers of 2.
</span>
<span class="c1"># Create an array where each entry is a power of 2, starting from 2^(n_vectors-1) down to 2^0
# 'n_vectors' represents the number of dimensions (or bits) in the binary hash codes.
# This array is used to convert binary numbers (hash codes) into decimal (integer) values.
</span><span class="n">powers_of_two</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_vectors</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">step</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  
<span class="c1"># Example: If n_vectors is 16, this creates an array like [32768, 16384, 8192, ..., 2, 1], corresponding to powers of 2.
</span><span class="k">print</span><span class="p">(</span><span class="n">powers_of_two</span><span class="p">)</span>  <span class="c1"># Print the array of powers of 2
</span>
<span class="c1"># Sample output:
# [32768 16384  8192  4096  2048  1024   512   256   128    64    32    16    8     4     2     1]
</span>
<span class="c1"># Convert binary hash codes into decimal (bin indices)
# 'bin_indices_bits' contains the binary hash codes as arrays of 1s and 0s (or True/False).
# By doing a dot product between 'bin_indices_bits' and 'powers_of_two', we convert each binary number into a unique integer.
# Each binary code is interpreted as a binary number, and 'powers_of_two' acts as the weights for each bit.
</span><span class="n">bin_indices</span> <span class="o">=</span> <span class="n">bin_indices_bits</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">powers_of_two</span><span class="p">)</span>

<span class="c1"># Example: If bin_indices_bits contains a binary code like [1, 0, 1, 1], this will convert it into its decimal equivalent (e.g., 21560).
</span><span class="k">print</span><span class="p">(</span><span class="n">bin_indices</span><span class="p">)</span>  <span class="c1"># Print the resulting bin index (the integer representation of the binary hash code)
</span>
<span class="c1"># Sample output:
# 21560  # This represents the decimal equivalent of a specific binary hash code.
</span></code></pre></div></div>

<p>We’ve now hashed the image into bucket 21560. Now let’s extend this process to the entire dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Step 3: Hash the entire dataset
</span>
<span class="c1"># Compute binary hash codes for the dataset using random hyperplanes
# For each data point in 'data', compute the dot product with 'random_vectors' (random hyperplanes).
# If the dot product is &gt;= 0, assign a binary value of 1, otherwise 0 (True/False).
</span><span class="n">bin_indices_bits</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">random_vectors</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">0</span>  <span class="c1"># This generates a binary code (True/False) for each data point.
</span>
<span class="c1"># Print the shape of 'bin_indices_bits' to verify the dimensions of the binary hash codes
</span><span class="k">print</span><span class="p">(</span><span class="n">bin_indices_bits</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># Should print something like (60000, n_vectors) where '60000' is the number of data points.
</span>
<span class="c1"># Convert the binary hash codes into bin indices (integer values)
# The binary hash code is converted into an integer using 'powers_of_two'.
# This process maps each binary code to a unique bin index.
</span><span class="n">bin_indices</span> <span class="o">=</span> <span class="n">bin_indices_bits</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">powers_of_two</span><span class="p">)</span>  <span class="c1"># Each binary code is converted into a unique integer (bin index).
</span>
<span class="c1"># The expected output shape of 'bin_indices' should be (60000,), meaning there are 60,000 bin indices,
# one for each data point in the dataset.
</span><span class="n">bin_indices</span><span class="p">.</span><span class="n">shape</span>  <span class="c1"># Check the shape of the resulting bin indices array.
</span>
</code></pre></div></div>
<p>At this point, every image in the CIFAR-10 dataset has been hashed. Now, we will create a hashtable to group images that land in the same bucket and check for duplicates.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>  <span class="c1"># Import defaultdict to create a hash table where each bin index stores a list of image indices
</span>
<span class="c1"># Step 4: Insert the images into a hash table based on their bin indices
</span><span class="n">table</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>  <span class="c1"># Initialize a hash table where each bin (key) stores a list of image indices (values)
</span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">bin_index</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bin_indices</span><span class="p">):</span>
    <span class="c1"># For each image, append its index to the corresponding bin in the hash table
</span>    <span class="n">table</span><span class="p">[</span><span class="n">bin_index</span><span class="p">].</span><span class="n">append</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>  <span class="c1"># 'bin_index' is the key (bin), 'idx' is the image index being stored
</span>
<span class="c1"># Step 5: Inspect the buckets (bins) that have at least two colliding images
</span><span class="k">for</span> <span class="n">bucket</span><span class="p">,</span> <span class="n">images</span> <span class="ow">in</span> <span class="n">table</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">images</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># If a bucket contains more than one image (collision)
</span>        <span class="k">print</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>  <span class="c1"># Print the list of image indices that collide (i.e., are hashed to the same bin)
</span>
</code></pre></div></div>

<p>This code builds a hashtable where images that share the same hashcode (i.e., collide) are placed in the same bucket. Let’s inspect some of these collisions to see how well LSH groups semantically similar images.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Inspect one of the buckets
# Example bucket: [39378, 39502, 41761, 42070, 50364]
</span><span class="k">print</span><span class="p">(</span><span class="n">classes</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">classes</span><span class="p">[:,</span> <span class="mi">39378</span><span class="p">])</span>  <span class="c1"># Output: 7 (horse)
</span><span class="k">print</span><span class="p">(</span><span class="n">classes</span><span class="p">[:,</span> <span class="mi">39502</span><span class="p">])</span>  <span class="c1"># Output: 8 (ship)
</span><span class="k">print</span><span class="p">(</span><span class="n">classes</span><span class="p">[:,</span> <span class="mi">41761</span><span class="p">])</span>  <span class="c1"># Output: 8 (ship)
</span><span class="k">print</span><span class="p">(</span><span class="n">classes</span><span class="p">[:,</span> <span class="mi">42070</span><span class="p">])</span>  <span class="c1"># Output: 4 (deer)
</span><span class="k">print</span><span class="p">(</span><span class="n">classes</span><span class="p">[:,</span> <span class="mi">50364</span><span class="p">])</span>  <span class="c1"># Output: 9 (truck)
</span></code></pre></div></div>

<p>In this case, LSH performs poorly, as two different categories (horse, truck) collide in the same bucket. Let’s inspect another bucket for better results.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># We take this bucket and inspect the images:
# [42030, 42486, 43090, 47535, 50134, 50503]
</span>
<span class="k">print</span><span class="p">(</span><span class="n">classes</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">classes</span><span class="p">[:,</span><span class="mi">42030</span><span class="p">])</span>   <span class="c1"># 4
</span><span class="k">print</span><span class="p">(</span><span class="n">classes</span><span class="p">[:,</span><span class="mi">42486</span><span class="p">])</span>   <span class="c1"># 4
</span><span class="k">print</span><span class="p">(</span><span class="n">classes</span><span class="p">[:,</span><span class="mi">43090</span><span class="p">])</span>   <span class="c1"># 4
</span><span class="k">print</span><span class="p">(</span><span class="n">classes</span><span class="p">[:,</span><span class="mi">47535</span><span class="p">])</span>   <span class="c1"># 1
</span><span class="k">print</span><span class="p">(</span><span class="n">classes</span><span class="p">[:,</span><span class="mi">50134</span><span class="p">])</span>   <span class="c1"># 1
</span><span class="k">print</span><span class="p">(</span><span class="n">classes</span><span class="p">[:,</span><span class="mi">50503</span><span class="p">])</span>   <span class="c1"># 4
</span></code></pre></div></div>

<p>In this bucket, LSH performs better—grouping similar categories (mostly deer).</p>

<h3 id="evaluation-lsh">Evaluation (LSH)</h3>

<p>Next, let’s evaluate LSH more rigorously using precision at 10, which measures how many of the 10 nearest neighbors for a query share the same class label.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Step 1: Split the dataset into query, training, and database sets for nearest neighbor search
</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>  <span class="c1"># Import train_test_split to split the dataset
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Set a seed for reproducibility
</span>
<span class="c1"># First split: Separate out a small portion of the data to serve as the query set
# 'data' contains the entire dataset, and 'classes[0, :]' contains the corresponding labels
# We split off 0.2% of the data for queries (test_size=0.002)
</span><span class="n">data_temp</span><span class="p">,</span> <span class="n">data_query</span><span class="p">,</span> <span class="n">labels_temp</span><span class="p">,</span> <span class="n">labels_query</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">classes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.002</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>

<span class="c1"># Second split: From the remaining data, split into the database set and the training set
# We split off 2% of the remaining data for training, leaving the rest for the database
</span><span class="n">data_database</span><span class="p">,</span> <span class="n">data_train</span><span class="p">,</span> <span class="n">labels_database</span><span class="p">,</span> <span class="n">labels_train</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">data_temp</span><span class="p">,</span> <span class="n">labels_temp</span><span class="p">[:],</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>

</code></pre></div></div>

<p>This code will give 120 random queries that we will use alongside the LSH search index to find nearest neighbours. The database consists of 58682 images, and the training dataset contains 1198 images.</p>

<p><img src="./tutorial/lsh_dataset.png" alt="Dataset" /></p>

<p>To prevent overfitting we maintain a held-out <em>database</em> that we perform retrieval against using the set of 120 queries. The training dataset is used to learn any parameters and hyperparameters required by the models.</p>

<p>We now index the database portion with LSH creating our hashtable:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Step 1: Compute binary hash codes for the entire database
# Project the data points onto the random hyperplanes (random_vectors) and convert to binary hash codes
# If the dot product of a data point and a hyperplane is &gt;= 0, assign a value of 1, else 0
</span><span class="n">bin_indices_bits</span> <span class="o">=</span> <span class="n">data_database</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">random_vectors</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">0</span>  <span class="c1"># Each data point gets a binary code (True/False)
</span>
<span class="c1"># Step 2: Convert binary hash codes to bin indices (integer representation)
# The binary code is treated as bits and converted to an integer using powers_of_two
</span><span class="n">bin_indices</span> <span class="o">=</span> <span class="n">bin_indices_bits</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">powers_of_two</span><span class="p">)</span>  <span class="c1"># Each binary code is converted into an integer index
</span>
<span class="c1"># Step 3: Build the hash table
</span><span class="n">table</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>  <span class="c1"># Initialize the hash table as a defaultdict of lists
</span>
<span class="c1"># Step 4: Populate the hash table with data points
</span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">bin_index</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bin_indices</span><span class="p">):</span>
    <span class="n">table</span><span class="p">[</span><span class="n">bin_index</span><span class="p">].</span><span class="n">append</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>  <span class="c1"># For each data point, add its index to the corresponding bin in the hash table
</span></code></pre></div></div>

<p>To search for nearest neighbours we apply a <em>Hamming radius based search</em>:</p>

<p><img src="./tutorial/lsh_evaluation.png" alt="Dataset" /></p>

<p>In Figure (b) above, we see an example of a Hamming radius-based search with a radius of zero, as illustrated in <a href="https://era.ed.ac.uk/handle/1842/20390">Sean Moran’s PhD thesis</a>.</p>

<p>To break it down, this search method not only looks within the same bin but also explores nearby bins that differ by a specific number of bits, up to a defined radius. Using the <code class="language-plaintext highlighter-rouge">_itertools combinations_</code> function, we can generate all possible bins that differ by up to 10 bits from the current one. This allows us to retrieve neighbors not just from the original bin but from surrounding bins as well, significantly broadening the search results.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">combinations</span>  <span class="c1"># Used to generate combinations of bit positions for Hamming radius search
</span><span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">pairwise_distances</span>  <span class="c1"># Used to compute distances between query and candidates
</span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>  <span class="c1"># Pandas for managing candidate neighbors and their labels/distances
</span><span class="kn">import</span> <span class="nn">time</span>  <span class="c1"># For measuring the time spent on each search radius
</span>
<span class="c1"># Hamming radius-based search for nearest neighbors
</span><span class="n">max_search_radius</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># Maximum Hamming radius to consider in the search
</span><span class="n">topn</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># We will evaluate precision@10 (how many correct results are in the top 10 nearest neighbors)
</span><span class="n">precision_history</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_search_radius</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)}</span>  <span class="c1"># Dictionary to track precision at each radius
</span><span class="n">time_history</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_search_radius</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)}</span>  <span class="c1"># Dictionary to track time taken for each radius
</span>
<span class="c1"># Iterate over each query image and its corresponding label
</span><span class="k">for</span> <span class="n">query_image</span><span class="p">,</span> <span class="n">query_label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">data_query</span><span class="p">,</span> <span class="n">labels_query</span><span class="p">):</span>
    
    <span class="c1"># Step 1: Compute binary hash code for the query image using random hyperplanes
</span>    <span class="n">bin_index_bits</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">query_image</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">random_vectors</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># Convert query image to binary hash code
</span>    <span class="n">candidate_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>  <span class="c1"># Set to collect candidate neighbors found within the Hamming radius
</span>
    <span class="c1"># Step 2: Search over various Hamming radii (from 0 to max_search_radius)
</span>    <span class="k">for</span> <span class="n">search_radius</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_search_radius</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>  <span class="c1"># Start timer to measure the search time for this radius
</span>
        <span class="c1"># Search for nearby bins by flipping bits up to 'search_radius' positions
</span>        <span class="k">for</span> <span class="n">different_bits</span> <span class="ow">in</span> <span class="n">combinations</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_vectors</span><span class="p">),</span> <span class="n">search_radius</span><span class="p">):</span>
            <span class="n">alternate_bits</span> <span class="o">=</span> <span class="n">bin_index_bits</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>  <span class="c1"># Copy original binary hash code of the query
</span>            <span class="n">alternate_bits</span><span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">different_bits</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">logical_not</span><span class="p">(</span><span class="n">alternate_bits</span><span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">different_bits</span><span class="p">)])</span>  <span class="c1"># Flip bits at positions defined by 'different_bits'
</span>            <span class="n">nearby_bin</span> <span class="o">=</span> <span class="n">alternate_bits</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">powers_of_two</span><span class="p">)</span>  <span class="c1"># Convert binary code to an integer to look up in the hash table
</span>            <span class="k">if</span> <span class="n">nearby_bin</span> <span class="ow">in</span> <span class="n">table</span><span class="p">:</span>  <span class="c1"># Check if the altered binary hash corresponds to a bin in the hash table
</span>                <span class="n">candidate_set</span><span class="p">.</span><span class="n">update</span><span class="p">(</span><span class="n">table</span><span class="p">[</span><span class="n">nearby_bin</span><span class="p">])</span>  <span class="c1"># Add the candidates from this bin to the candidate set
</span>
        <span class="c1"># Step 3: If candidate neighbors are found, rank them by their actual distances (using cosine similarity)
</span>        <span class="k">if</span> <span class="n">candidate_set</span><span class="p">:</span>
            <span class="n">candidates</span> <span class="o">=</span> <span class="n">data_database</span><span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">candidate_set</span><span class="p">)]</span>  <span class="c1"># Retrieve the actual data points for the candidates
</span>            <span class="n">ground_truth</span> <span class="o">=</span> <span class="n">labels_database</span><span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">candidate_set</span><span class="p">)]</span>  <span class="c1"># Retrieve the true labels for the candidates
</span>            <span class="n">distances</span> <span class="o">=</span> <span class="n">pairwise_distances</span><span class="p">(</span><span class="n">candidates</span><span class="p">,</span> <span class="n">query_image</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">metric</span><span class="o">=</span><span class="s">'cosine'</span><span class="p">).</span><span class="n">flatten</span><span class="p">()</span>  <span class="c1"># Compute cosine distances between candidates and query
</span>            <span class="n">nearest_neighbors</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">'id'</span><span class="p">:</span> <span class="nb">list</span><span class="p">(</span><span class="n">candidate_set</span><span class="p">),</span> <span class="s">'class'</span><span class="p">:</span> <span class="n">ground_truth</span><span class="p">,</span> <span class="s">'distance'</span><span class="p">:</span> <span class="n">distances</span><span class="p">})</span>  <span class="c1"># Store candidates and their distances
</span>            <span class="c1"># Sort candidates by their distances and take the top 10 closest
</span>            <span class="n">candidate_set_labels</span> <span class="o">=</span> <span class="n">nearest_neighbors</span><span class="p">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s">'distance'</span><span class="p">).</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="s">'class'</span><span class="p">][:</span><span class="mi">10</span><span class="p">]</span>
            <span class="c1"># Calculate precision@10 (how many of the top 10 closest candidates have the correct label)
</span>            <span class="n">precision</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">candidate_set_labels</span><span class="p">).</span><span class="n">count</span><span class="p">(</span><span class="n">query_label</span><span class="p">)</span> <span class="o">/</span> <span class="n">topn</span>
            <span class="n">precision_history</span><span class="p">[</span><span class="n">search_radius</span><span class="p">].</span><span class="n">append</span><span class="p">(</span><span class="n">precision</span><span class="p">)</span>  <span class="c1"># Store the precision for this radius
</span>
        <span class="n">time_history</span><span class="p">[</span><span class="n">search_radius</span><span class="p">].</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>  <span class="c1"># Store the time taken for this search radius
</span>
<span class="c1"># Step 4: Calculate average precision and time across all query images for each search radius
</span><span class="n">mean_time</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">time_history</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">time_history</span><span class="p">))]</span>  <span class="c1"># Compute average time for each search radius
</span><span class="n">mean_precision</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">precision_history</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">precision_history</span><span class="p">))]</span>  <span class="c1"># Compute average precision for each search radius
</span>
<span class="c1"># Output the results
</span><span class="k">print</span><span class="p">(</span><span class="n">mean_time</span><span class="p">)</span>  <span class="c1"># Print the average time spent for each search radius
</span><span class="k">print</span><span class="p">(</span><span class="n">mean_precision</span><span class="p">)</span>  <span class="c1"># Print the average precision@10 for each search radius
</span></code></pre></div></div>
<p>The code above generates a mean precision@10 of 0.30 for a Hamming radius of 2. As we increase the radius, the retrieval quality improves, but it comes at the cost of checking a larger number of candidate nearest neighbors. In simpler terms, when using a Hamming radius of 2, around 30% of the images returned in a list of 10 will, on average, be relevant to the query. We’ll also demonstrate how we can boost this to a mean precision@10 of 0.40 by <em>learning the hashing hyperplanes</em> rather than using random hyperplanes, as is done in traditional Locality-Sensitive Hashing (LSH).</p>

<p><img src="./tutorial/lsh_precision10.png" alt="LSH Precision@10" /></p>

<p>As the Hamming radius increases from 0 to 10, we begin retrieving more images from the database in our candidate set. This leads to a noticeable spike in query time, which can eventually approach the time required for a standard brute force search (~53 seconds) when the candidate set includes the entire database.</p>

<p><img src="./tutorial/lsh_time.png" alt="LSH Time" /></p>

<h2 id="implementation-grh">Implementation (GRH)</h2>

<p>Next, we explore how learning the hyperplanes (i.e., learning to hash) can significantly enhance retrieval effectiveness. Specifically, we’ll be working with the supervised learning to hash model, <a href="https://learning2hash.github.io/publications/moran2015agraph/">Graph Regularised Hashing</a>.</p>

<p>Our first step involves using the training dataset to build an <em>adjacency matrix</em>, which GRH will use as a supervisory signal for learning the hashing hyperplanes. In this matrix, if two images share the same class label, then <em>adjacency_matrix[i,j] = 1</em>; otherwise, <em>adjacency_matrix[i,j] = 0</em>. In Python, we can construct this adjacency matrix using the class label vector:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">adjacency_matrix</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">equal</span><span class="p">.</span><span class="n">outer</span><span class="p">(</span><span class="n">labels_train</span><span class="p">,</span> <span class="n">labels_train</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="n">row_sums</span> <span class="o">=</span> <span class="n">adjacency_matrix</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">adjacency_matrix</span> <span class="o">=</span> <span class="n">adjacency_matrix</span> <span class="o">/</span> <span class="n">row_sums</span><span class="p">[:,</span> <span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">]</span>
</code></pre></div></div>

<p>Next, we explore how learning the hyperplanes (i.e., learning to hash) can significantly enhance retrieval effectiveness. Specifically, we’ll be working with the supervised learning to hash model, <a href="https://learning2hash.github.io/publications/moran2015agraph/">Graph Regularised Hashing</a>.</p>

<p>Our first step involves using the training dataset to build an <em>adjacency matrix</em>, which GRH will use as a supervisory signal for learning the hashing hyperplanes. In this matrix, if two images share the same class label, then <em>adjacency_matrix[i,j] = 1</em>; otherwise, <em>adjacency_matrix[i,j] = 0</em>. In Python, we can construct this adjacency matrix using the class label vector:</p>

<p><img src="./tutorial/grh.png" alt="GRH" /></p>

<p>The first step is <em>Graph Regularisation</em>:</p>

<p><img src="./tutorial/grh_step1.png" alt="GRH" /></p>

<p>(The paper by Fernando Diaz, referenced in the <a href="https://www.slideshare.net/sjmoran1/graph-regularised-hashing-ecir15-talk">slidedeck</a>, is highly recommended and can be accessed <a href="https://fernando.diaz.nyc/LSR-IR.pdf">here</a>.)</p>

<p>In the first step, the adjacency matrix is multiplied by the hashcodes of the training dataset images. This operation adjusts the hashcodes so that semantically similar images have more similar hashcodes.</p>

<p>The second step involves <em>Data Space Partitioning</em>:</p>

<p><img src="./tutorial/grh_step2.png" alt="GRH" /></p>

<p>In the second step, the refined hashcodes are used to update the hyperplanes. This is done by training an SVM for each hash bit, using the bits as targets. GRH starts with the LSH hyperplanes in <em>random_vector</em> as an initial point and then iteratively refines them to make the hashing process more effective. The complete GRH model is implemented below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_iter</span> <span class="o">=</span> <span class="mi">2</span>   <span class="c1"># Number of iterations of the Generalized Randomized Hashing (GRH) process
</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span>  <span class="c1"># Weighting factor to update the hashcodes based on supervisory information (supervised learning)
</span>
<span class="c1"># Loop through each iteration of GRH
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_iter</span><span class="p">):</span>
    
    <span class="c1"># Step 1: Compute initial binary hash codes based on random hyperplanes
</span>    <span class="c1"># Data is projected onto random vectors (hyperplanes) and hashed to binary values (1 or -1)
</span>    <span class="n">bin_indices_bits</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_train</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">random_vectors</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>  <span class="c1"># Binary hash codes (0 or 1)
</span>    <span class="n">bin_indices_bits</span><span class="p">[</span><span class="n">bin_indices_bits</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>  <span class="c1"># Convert 0s to -1 to get [-1, 1] binary representation
</span>    
    <span class="c1"># Step 2: Refine hash codes using adjacency matrix (leveraging graph-based relationships)
</span>    <span class="n">bin_indices_bits_refined</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">adjacency_matrix</span><span class="p">,</span> <span class="n">bin_indices_bits</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">))</span>  <span class="c1"># Refine codes using neighborhood structure
</span>    <span class="n">bin_indices_bits_refined</span> <span class="o">=</span> <span class="p">(</span><span class="n">bin_indices_bits_refined</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>  <span class="c1"># Convert to binary
</span>    <span class="n">bin_indices_bits_refined</span><span class="p">[</span><span class="n">bin_indices_bits_refined</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>  <span class="c1"># Ensure the binary codes are in {-1, 1}
</span>    
    <span class="c1"># Step 3: Smooth hash codes by combining original and refined codes
</span>    <span class="n">bin_indices_bits</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.25</span> <span class="o">*</span> <span class="n">bin_indices_bits_refined</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.75</span> <span class="o">*</span> <span class="n">bin_indices_bits</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">))</span>  <span class="c1"># Smooth out refined hash codes with original codes
</span>    <span class="n">bin_indices_bits</span> <span class="o">=</span> <span class="p">(</span><span class="n">bin_indices_bits</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>  <span class="c1"># Convert back to binary form
</span>    <span class="n">bin_indices_bits</span><span class="p">[</span><span class="n">bin_indices_bits</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>  <span class="c1"># Ensure binary values are in {-1, 1}
</span>    
    <span class="c1"># Step 4: Update hyperplanes based on supervised learning (using SVM)
</span>    <span class="n">grh_hyperplanes</span> <span class="o">=</span> <span class="n">random_vectors</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>  <span class="c1"># Copy the current set of random hyperplanes
</span>    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_vectors</span><span class="p">):</span>
        
        <span class="c1"># Check if all binary codes for the j-th vector are the same (either all 1 or all -1)
</span>        <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">bin_indices_bits</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]))</span> <span class="o">==</span> <span class="n">data_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="c1"># If all bits are the same, generate a new random hyperplane
</span>            <span class="n">random_vector</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">grh_hyperplanes</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">random_vector</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># Replace the j-th hyperplane with a new random vector
</span>        
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Otherwise, fit an SVM classifier to the data using the binary codes as labels
</span>            <span class="n">hyperplane</span> <span class="o">=</span> <span class="n">svclassifier</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_train</span><span class="p">,</span> <span class="n">bin_indices_bits</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]).</span><span class="n">coef_</span>  <span class="c1"># Fit SVM to get the hyperplane
</span>            <span class="n">hyperplane</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">hyperplane</span><span class="p">)</span>  <span class="c1"># Convert the hyperplane coefficients to a NumPy array
</span>            <span class="n">grh_hyperplanes</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">hyperplane</span>  <span class="c1"># Update the j-th hyperplane with the SVM-trained hyperplane
</span>    
    <span class="c1"># Step 5: Update the random vectors for the next iteration
</span>    <span class="n">random_vectors</span> <span class="o">=</span> <span class="n">grh_hyperplanes</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>  <span class="c1"># Copy the updated hyperplanes for the next iteration of GRH
</span></code></pre></div></div>

<p>In the code above, we configure GRH with 2 iterations and an alpha of 0.25. The <em>iterations</em> parameter controls how many times we repeat the two steps of GRH: refining the hashcodes using the adjacency matrix, followed by updating the hyperplanes based on the refined hashcodes. After 2 iterations, the <em>random_vectors</em> matrix contains a set of hyperplanes that have been refined, making them more effective for hashing-based nearest neighbor search. These hyperplanes, influenced by the supervisory information in the training dataset (encoded in the adjacency matrix), can now be used as shown earlier in this tutorial to evaluate their performance through a hash table-based evaluation across different Hamming radii.</p>

<p>Next, we demonstrate how GRH works using a toy image retrieval example. The diagram below shows the nearest neighbor relationships, represented by nodes and edges (with edges connecting semantically similar images). The LSH-generated hashcodes are displayed next to each image.</p>

<p><img src="./tutorial/grh_toy1.png" alt="GRH" /></p>

<p>The diagram below shows how the adjacency matrix is used to refine the hashcodes. For example, the first bit of image <em>C</em> flips to -1, making it more similar to the images above and to its left. Meanwhile, image <em>E</em> has its second bit flipped, aligning more closely with the hashcodes of the images below and to its left.</p>

<p><img src="./tutorial/grh_toy2.png" alt="GRH" /></p>

<p>Next, a hyperplane is learned for the first bit by using each image’s first hash bit as the target. In the toy example below, the hyperplane divides the data space horizontally, assigning a -1 to the first bit of the hashcode for images above the line and a 1 for those below the line.</p>

<p><img src="./tutorial/grh_toy3.png" alt="GRH" /></p>

<h3 id="evaluation-grh">Evaluation (GRH)</h3>

<p>Now we have gained an understanding of how the GRH model works we will evaluate the GRH hashcodes using the same methodology as we did for LSH. We find an improved retrieval effectiveness, particularly at low Hamming radii:</p>

<p><img src="./tutorial/grh_precision10.png" alt="GRH Time" /></p>

<p>The advantages of GRH on this dataset, using a 16-bit hashcode, are most noticeable in the low Hamming radius range (≤5). For instance, at a Hamming radius of 0, GRH achieves around 0.25 mean precision@10, compared to just 0.1 for LSH, while query times remain roughly the same for both methods (~0.5 seconds). The query time curve for GRH at increasing Hamming radii is shown below. As you can see, while we experience a slight increase in query time at some Hamming radii, the boost in retrieval effectiveness is well worth the trade-off.</p>

<p><img src="./tutorial/grh_time.png" alt="GRH Time" /></p>

<h3 id="conclusions">Conclusions</h3>

<p>In this tutorial, we demonstrate how to use a Support Vector Machine (SVM) to learn the hyperplanes for GRH. One of the key advantages of GRH, beyond its simplicity and effectiveness, is its flexibility—it’s agnostic to the learning algorithm. This means you can easily swap in a deep network to achieve more precise data-space partitioning or opt for a <a href="https://www.youtube.com/watch?v=uxGDwyPWNkU">passive aggressive classifier</a> for a lightweight, online-adaptable method, ideal for streaming scenarios. Additionally, we explored a single hashtable implementation of LSH and GRH, and boosted the retrieval of relevant items by using multiple buckets through a multi-probing mechanism. Other LSH implementations take a different approach, utilizing multiple independent hashtables rather than probing buckets within the same one to increase the number of relevant results.</p>

<h3 id="contact--feedback">Contact &amp; Feedback</h3>

<p>Got thoughts on this tutorial? I’d love to hear them! You can reach out to the author, <a href="https://sjmoran.github.io/">Sean Moran</a>, anytime. Want to dive into the code? Check it out <a href="./tutorial/hashing_tutorial.py">here</a>, and don’t forget to grab the dependencies from the [requirements.txt]</p>

<p>Copyright © <a href="https://sjmoran.github.io/">Sean Moran</a> 2024. All opinions are my own.</p>

    </div>

  </body>
</html>
