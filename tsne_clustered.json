[{"key": "2024scratch", "year": "2024", "citations": "108", "title": "SCRATCH A Scalable Discrete Matrix Factorization Hashing For Cross-modal Retrieval", "abstract": "<p>In recent years, many hashing methods have been proposed for the cross-modal retrieval task. However, there are still some issues that need to be further explored. For example, some of them relax the binary constraints to generate the hash codes, which may generate large quantization error. Although some discrete schemes have been proposed, most of them are time-consuming. In addition, most of the existing supervised hashing methods use an n x n similarity matrix during the optimization, making them unscalable. To address these issues, in this paper, we present a novel supervised cross-modal hashing method\u2014Scalable disCRete mATrix faCtorization Hashing, SCRATCH for short. It leverages the collective matrix factorization on the kernelized features and the semantic embedding with labels to find a latent semantic space to preserve the intra- and inter-modality similarities. In addition, it incorporates the label matrix instead of the similarity matrix into the loss function. Based on the proposed loss function and the iterative optimization algorithm, it can learn the hash functions and binary codes simultaneously. Moreover, the binary codes can be generated discretely, reducing the quantization error generated by the relaxation scheme. Its time complexity is linear to the size of the dataset, making it scalable to large-scale datasets. Extensive experiments on three benchmark datasets, namely, Wiki, MIRFlickr-25K, and NUS-WIDE, have verified that our proposed SCRATCH model outperforms several state-of-the-art unsupervised and supervised hashing methods for cross-modal retrieval.</p>\n", "tags": ["Cross Modal", "Quantization", "Supervised", "Efficient Learning", "Benchmarks and Datasets"], "tsne_embedding": [3.0246329307556152, 6.404141902923584], "cluster": 5}, {"key": "a2016ensemble", "year": "2016", "citations": "8", "title": "An Ensemble Diversity Approach To Supervised Binary Hashing", "abstract": "<p>Binary hashing is a well-known approach for fast approximate nearest-neighbor search in information retrieval. Much work has focused on affinity-based objective functions involving the hash functions or binary codes. These objective functions encode neighborhood information between data points and are often inspired by manifold learning algorithms. They ensure that the hash functions differ from each other through constraints or penalty terms that encourage codes to be orthogonal or dissimilar across bits, but this couples the binary variables and complicates the already difficult optimization. We propose a much simpler approach: we train each hash function (or bit) independently from each other, but introduce diversity among them using techniques from classifier ensembles. Surprisingly, we find that not only is this faster and trivially parallelizable, but it also improves over the more complex, coupled objective function, and achieves state-of-the-art precision and recall in experiments with image retrieval.</p>\n", "tags": ["Image Retrieval", "Supervised"], "tsne_embedding": [-1.2594932317733765, 2.7392020225524902], "cluster": 0}, {"key": "aamand2020no", "year": "2020", "citations": "1", "title": "No Repetition: Fast Streaming With Highly Concentrated Hashing", "abstract": "<p>To get estimators that work within a certain error bound with high\nprobability, a common strategy is to design one that works with constant\nprobability, and then boost the probability using independent repetitions.\nImportant examples of this approach are small space algorithms for estimating\nthe number of distinct elements in a stream, or estimating the set similarity\nbetween large sets. Using standard strongly universal hashing to process each\nelement, we get a sketch based estimator where the probability of a too large\nerror is, say, 1/4. By performing \\(r\\) independent repetitions and taking the\nmedian of the estimators, the error probability falls exponentially in \\(r\\).\nHowever, running \\(r\\) independent experiments increases the processing time by a\nfactor \\(r\\).\n  Here we make the point that if we have a hash function with strong\nconcentration bounds, then we get the same high probability bounds without any\nneed for repetitions. Instead of \\(r\\) independent sketches, we have a single\nsketch that is \\(r\\) times bigger, so the total space is the same. However, we\nonly apply a single hash function, so we save a factor \\(r\\) in time, and the\noverall algorithms just get simpler.\n  Fast practical hash functions with strong concentration bounds were recently\nproposed by Aamand em et al. (to appear in STOC 2020). Using their hashing\nschemes, the algorithms thus become very fast and practical, suitable for\nonline processing of high volume data streams.</p>\n", "tags": ["Hashing Methods", "Efficient Learning"], "tsne_embedding": [-15.283333778381348, -1.1561938524246216], "cluster": 2}, {"key": "adimoolam2023efficient", "year": "2023", "citations": "1", "title": "Efficient Deduplication And Leakage Detection In Large Scale Image Datasets With A Focus On The Crowdai Mapping Challenge Dataset", "abstract": "<p>Recent advancements in deep learning and computer vision have led to\nwidespread use of deep neural networks to extract building footprints from\nremote-sensing imagery. The success of such methods relies on the availability\nof large databases of high-resolution remote sensing images with high-quality\nannotations. The CrowdAI Mapping Challenge Dataset is one of these datasets\nthat has been used extensively in recent years to train deep neural networks.\nThis dataset consists of \\( \\sim\\ \\)280k training images and \\( \\sim\\ \\)60k testing\nimages, with polygonal building annotations for all images. However, issues\nsuch as low-quality and incorrect annotations, extensive duplication of image\nsamples, and data leakage significantly reduce the utility of deep neural\nnetworks trained on the dataset. Therefore, it is an imperative pre-condition\nto adopt a data validation pipeline that evaluates the quality of the dataset\nprior to its use. To this end, we propose a drop-in pipeline that employs\nperceptual hashing techniques for efficient de-duplication of the dataset and\nidentification of instances of data leakage between training and testing\nsplits. In our experiments, we demonstrate that nearly 250k(\\( \\sim\\ \\)90%)\nimages in the training split were identical. Moreover, our analysis on the\nvalidation split demonstrates that roughly 56k of the 60k images also appear in\nthe training split, resulting in a data leakage of 93%. The source code used\nfor the analysis and de-duplication of the CrowdAI Mapping Challenge dataset is\npublicly available at https://github.com/yeshwanth95/CrowdAI_Hash_and_search .</p>\n", "tags": ["Hashing Methods", "ANN Search", "Evaluation Metrics", "Tools and Libraries"], "tsne_embedding": [2.497382879257202, 11.092884063720703], "cluster": 5}, {"key": "agarwal2021dynamic", "year": "2021", "citations": "2", "title": "Dynamic Enumeration Of Similarity Joins", "abstract": "<p>This paper considers enumerating answers to similarity-join queries under\ndynamic updates: Given two sets of \\(n\\) points \\(A,B\\) in \\(\\mathbb{R}^d\\), a metric\n\\(\\phi(\\cdot)\\), and a distance threshold \\(r &gt; 0\\), report all pairs of points\n\\((a, b) \\in A \\times B\\) with \\(\\phi(a,b) \\le r\\). Our goal is to store \\(A,B\\) into\na dynamic data structure that, whenever asked, can enumerate all result pairs\nwith worst-case delay guarantee, i.e., the time between enumerating two\nconsecutive pairs is bounded. Furthermore, the data structure can be\nefficiently updated when a point is inserted into or deleted from \\(A\\) or \\(B\\).\n  We propose several efficient data structures for answering similarity-join\nqueries in low dimension. For exact enumeration of similarity join, we present\nnear-linear-size data structures for \\(\\ell_1, \\ell_\\infty\\) metrics with\n\\(log^{O(1)} n\\) update time and delay. We show that such a data structure is\nnot feasible for the \\(\u2113\u2082\\) metric for \\(d \\ge 4\\). For approximate enumeration\nof similarity join, where the distance threshold is a soft constraint, we\nobtain a unified linear-size data structure for \\(\\ell_p\\) metric, with\n\\(log^{O(1)} n\\) delay and update time. In high dimensions, we present an\nefficient data structure with worst-case delay-guarantee using locality\nsensitive hashing (LSH).</p>\n", "tags": ["Hashing Methods", "ANN Search", "Efficient Learning"], "tsne_embedding": [-12.085944175720215, -5.969161033630371], "cluster": 2}, {"key": "aghazadeh2016near", "year": "2016", "citations": "0", "title": "Near-isometric Binary Hashing For Large-scale Datasets", "abstract": "<p>We develop a scalable algorithm to learn binary hash codes for indexing\nlarge-scale datasets. Near-isometric binary hashing (NIBH) is a data-dependent\nhashing scheme that quantizes the output of a learned low-dimensional embedding\nto obtain a binary hash code. In contrast to conventional hashing schemes,\nwhich typically rely on an \\(\u2113\u2082\\)-norm (i.e., average distortion)\nminimization, NIBH is based on a \\(\\ell_{\\infty}\\)-norm (i.e., worst-case\ndistortion) minimization that provides several benefits, including superior\ndistance, ranking, and near-neighbor preservation performance. We develop a\npractical and efficient algorithm for NIBH based on column generation that\nscales well to large datasets. A range of experimental evaluations demonstrate\nthe superiority of NIBH over ten state-of-the-art binary hashing schemes.</p>\n", "tags": ["Hashing Methods", "Indexing", "Efficient Learning"], "tsne_embedding": [-6.00195837020874, -0.44756364822387695], "cluster": 7}, {"key": "aguerrebere2023similarity", "year": "2023", "citations": "12", "title": "Similarity Search In The Blink Of An Eye With Compressed Indices", "abstract": "<p>Nowadays, data is represented by vectors. Retrieving those vectors, among\nmillions and billions, that are similar to a given query is a ubiquitous\nproblem, known as similarity search, of relevance for a wide range of\napplications. Graph-based indices are currently the best performing techniques\nfor billion-scale similarity search. However, their random-access memory\npattern presents challenges to realize their full potential. In this work, we\npresent new techniques and systems for creating faster and smaller graph-based\nindices. To this end, we introduce a novel vector compression method,\nLocally-adaptive Vector Quantization (LVQ), that uses per-vector scaling and\nscalar quantization to improve search performance with fast similarity\ncomputations and a reduced effective bandwidth, while decreasing memory\nfootprint and barely impacting accuracy. LVQ, when combined with a new\nhigh-performance computing system for graph-based similarity search,\nestablishes the new state of the art in terms of performance and memory\nfootprint. For billions of vectors, LVQ outcompetes the second-best\nalternatives: (1) in the low-memory regime, by up to 20.7x in throughput with\nup to a 3x memory footprint reduction, and (2) in the high-throughput regime by\n5.8x with 1.4x less memory.</p>\n", "tags": ["Applications", "Quantization", "ANN Search"], "tsne_embedding": [1.9389135837554932, -9.972498893737793], "cluster": 1}, {"key": "aguerrebere2024locally", "year": "2024", "citations": "0", "title": "Locally-adaptive Quantization For Streaming Vector Search", "abstract": "<p>Retrieving the most similar vector embeddings to a given query among a\nmassive collection of vectors has long been a key component of countless\nreal-world applications. The recently introduced Retrieval-Augmented Generation\nis one of the most prominent examples. For many of these applications, the\ndatabase evolves over time by inserting new data and removing outdated data. In\nthese cases, the retrieval problem is known as streaming similarity search.\nWhile Locally-Adaptive Vector Quantization (LVQ), a highly efficient vector\ncompression method, yields state-of-the-art search performance for non-evolving\ndatabases, its usefulness in the streaming setting has not been yet\nestablished. In this work, we study LVQ in streaming similarity search. In\nsupport of our evaluation, we introduce two improvements of LVQ: Turbo LVQ and\nmulti-means LVQ that boost its search performance by up to 28% and 27%,\nrespectively. Our studies show that LVQ and its new variants enable blazing\nfast vector search, outperforming its closest competitor by up to 9.4x for\nidentically distributed data and by up to 8.8x under the challenging scenario\nof data distribution shifts (i.e., where the statistical distribution of the\ndata changes over time). We release our contributions as part of Scalable\nVector Search, an open-source library for high-performance similarity search.</p>\n", "tags": ["Quantization", "Applications", "Tools and Libraries"], "tsne_embedding": [12.415735244750977, 3.0292716026306152], "cluster": 3}, {"key": "ahle2015complexity", "year": "2015", "citations": "22", "title": "On The Complexity Of Inner Product Similarity Join", "abstract": "<p>A number of tasks in classification, information retrieval, recommendation\nsystems, and record linkage reduce to the core problem of inner product\nsimilarity join (IPS join): identifying pairs of vectors in a collection that\nhave a sufficiently large inner product. IPS join is well understood when\nvectors are normalized and some approximation of inner products is allowed.\nHowever, the general case where vectors may have any length appears much more\nchallenging. Recently, new upper bounds based on asymmetric locality-sensitive\nhashing (ALSH) and asymmetric embeddings have emerged, but little has been\nknown on the lower bound side. In this paper we initiate a systematic study of\ninner product similarity join, showing new lower and upper bounds. Our main\nresults are:</p>\n<ul>\n  <li>Approximation hardness of IPS join in subquadratic time, assuming the\nstrong exponential time hypothesis.</li>\n  <li>New upper and lower bounds for (A)LSH-based algorithms. In particular, we\nshow that asymmetry can be avoided by relaxing the LSH definition to only\nconsider the collision probability of distinct elements.</li>\n  <li>A new indexing method for IPS based on linear sketches, implying that our\nhardness results are not far from being tight.\n  Our technical contributions include new asymmetric embeddings that may be of\nindependent interest. At the conceptual level we strive to provide greater\nclarity, for example by distinguishing among signed and unsigned variants of\nIPS join and shedding new light on the effect of asymmetry.</li>\n</ul>\n", "tags": ["Indexing", "Hashing Methods"], "tsne_embedding": [-9.936849594116211, -3.221987247467041], "cluster": 7}, {"key": "ahle2016parameter", "year": "2016", "citations": "12", "title": "Parameter-free Locality Sensitive Hashing For Spherical Range Reporting", "abstract": "<p>We present a data structure for <em>spherical range reporting</em> on a point set\n\\(S\\), i.e., reporting all points in \\(S\\) that lie within radius \\(r\\) of a given\nquery point \\(q\\). Our solution builds upon the Locality-Sensitive Hashing (LSH)\nframework of Indyk and Motwani, which represents the asymptotically best\nsolutions to near neighbor problems in high dimensions. While traditional LSH\ndata structures have several parameters whose optimal values depend on the\ndistance distribution from \\(q\\) to the points of \\(S\\), our data structure is\nparameter-free, except for the space usage, which is configurable by the user.\nNevertheless, its expected query time basically matches that of an LSH data\nstructure whose parameters have been <em>optimally chosen for the data and query</em>\nin question under the given space constraints. In particular, our data\nstructure provides a smooth trade-off between hard queries (typically addressed\nby standard LSH) and easy queries such as those where the number of points to\nreport is a constant fraction of \\(S\\), or where almost all points in \\(S\\) are far\naway from the query point. In contrast, known data structures fix LSH\nparameters based on certain parameters of the input alone.\n  The algorithm has expected query time bounded by \\(O(t (n/t)^\\rho)\\), where \\(t\\)\nis the number of points to report and \\(\\rho\\in (0,1)\\) depends on the data\ndistribution and the strength of the LSH family used. We further present a\nparameter-free way of using multi-probing, for LSH families that support it,\nand show that for many such families this approach allows us to get expected\nquery time close to \\(O(n^\\rho+t)\\), which is the best we can hope to achieve\nusing LSH. The previously best running time in high dimensions was \\(\u03a9(t\nn^\\rho)\\). For many data distributions where the intrinsic dimensionality of the\npoint set close to \\(q\\) is low, we can give improved upper bounds on the\nexpected query time.</p>\n", "tags": ["LSH", "ANN Search"], "tsne_embedding": [-12.063882827758789, -7.660384654998779], "cluster": 2}, {"key": "ahle2017optimal", "year": "2017", "citations": "7", "title": "Optimal Las Vegas Locality Sensitive Data Structures", "abstract": "<p>We show that approximate similarity (near neighbour) search can be solved in\nhigh dimensions with performance matching state of the art (data independent)\nLocality Sensitive Hashing, but with a guarantee of no false negatives.\n  Specifically, we give two data structures for common problems.\n  For \\(c\\)-approximate near neighbour in Hamming space we get query time\n\\(dn^{1/c+o(1)}\\) and space \\(dn^{1+1/c+o(1)}\\) matching that of\n\\cite{indyk1998approximate} and answering a long standing open question\nfrom~\\cite{indyk2000dimensionality} and~\\cite{pagh2016locality} in the\naffirmative.\n  By means of a new deterministic reduction from \\(\\ell_1\\) to Hamming we also\nsolve \\(\\ell_1\\) and \\(\u2113\u2082\\) with query time \\(d^2n^{1/c+o(1)}\\) and space \\(d^2\nn^{1+1/c+o(1)}\\).\n  For \\((s_1,s_2)\\)-approximate Jaccard similarity we get query time\n\\(dn^{\\rho+o(1)}\\) and space \\(dn^{1+\\rho+o(1)}\\),\n\\(\\rho=log\\frac{1+s_1}{2s_1}\\big/log\\frac{1+s_2}{2s_2}\\), when sets have equal\nsize, matching the performance of~\\cite{tobias2016}.\n  The algorithms are based on space partitions, as with classic LSH, but we\nconstruct these using a combination of brute force, tensoring, perfect hashing\nand splitter functions `a la~\\cite{naor1995splitters}. We also show a new\ndimensionality reduction lemma with 1-sided error.</p>\n", "tags": ["Hashing Methods", "ANN Search"], "tsne_embedding": [-13.379870414733887, -7.310064792633057], "cluster": 2}, {"key": "ahle2020power", "year": "2020", "citations": "0", "title": "The Power Of Hashing With Mersenne Primes", "abstract": "<p>The classic way of computing a \\(k\\)-universal hash function is to use a random\ndegree-\\((k-1)\\) polynomial over a prime field \\(\\mathbb Z_p\\). For a fast\ncomputation of the polynomial, the prime \\(p\\) is often chosen as a Mersenne\nprime \\(p=2^b-1\\).\n  In this paper, we show that there are other nice advantages to using Mersenne\nprimes. Our view is that the hash function\u2019s output is a \\(b\\)-bit integer that\nis uniformly distributed in \\(\\{0, \\dots, 2^b-1\\}\\), except that \\(p\\) (the all\n\\texttt1s value in binary) is missing. Uniform bit strings have many nice\nproperties, such as splitting into substrings which gives us two or more hash\nfunctions for the cost of one, while preserving strong theoretical qualities.\nWe call this trick \u201cTwo for one\u201d hashing, and we demonstrate it on 4-universal\nhashing in the classic Count Sketch algorithm for second-moment estimation.\n  We also provide a new fast branch-free code for division and modulus with\nMersenne primes. Contrasting our analytic work, this code generalizes to any\nPseudo-Mersenne primes \\(p=2^b-c\\) for small \\(c\\).</p>\n", "tags": ["Hashing Methods"], "tsne_embedding": [-16.343414306640625, -4.505526542663574], "cluster": 2}, {"key": "ahle2020problem", "year": "2020", "citations": "2", "title": "On The Problem Of \\(p_1^{-1}\\) In Locality-sensitive Hashing", "abstract": "<p>A Locality-Sensitive Hash (LSH) function is called\n\\((r,cr,p_1,p_2)\\)-sensitive, if two data-points with a distance less than \\(r\\)\ncollide with probability at least \\(p_1\\) while data points with a distance\ngreater than \\(cr\\) collide with probability at most \\(p_2\\). These functions form\nthe basis of the successful Indyk-Motwani algorithm (STOC 1998) for nearest\nneighbour problems. In particular one may build a \\(c\\)-approximate nearest\nneighbour data structure with query time \\(\\tilde O(n^\\rho/p_1)\\) where\n\\(\\rho=\\frac{log1/p_1}{log1/p_2}\\in(0,1)\\). That is, sub-linear time, as long\nas \\(p_1\\) is not too small. This is significant since most high dimensional\nnearest neighbour problems suffer from the curse of dimensionality, and can\u2019t\nbe solved exact, faster than a brute force linear-time scan of the database.\n  Unfortunately, the best LSH functions tend to have very low collision\nprobabilities, \\(p_1\\) and \\(p_2\\). Including the best functions for Cosine and\nJaccard Similarity. This means that the \\(n^\\rho/p_1\\) query time of LSH is often\nnot sub-linear after all, even for approximate nearest neighbours!\n  In this paper, we improve the general Indyk-Motwani algorithm to reduce the\nquery time of LSH to \\(\\tilde O(n^\\rho/p_1^{1-\\rho})\\) (and the space usage\ncorrespondingly.) Since \\(n^\\rho p_1^{\\rho-1} &lt; n \\Leftrightarrow p_1 &gt; n^{-1}\\),\nour algorithm always obtains sublinear query time, for any collision\nprobabilities at least \\(1/n\\). For \\(p_1\\) and \\(p_2\\) small enough, our improvement\nover all previous methods can be <em>up to a factor \\(n\\)</em> in both query time\nand space.\n  The improvement comes from a simple change to the Indyk-Motwani algorithm,\nwhich can easily be implemented in existing software packages.</p>\n", "tags": ["Hashing Methods", "ANN Search", "Efficient Learning"], "tsne_embedding": [-12.642950057983398, -7.046259880065918], "cluster": 2}, {"key": "aiger2023we", "year": "2023", "citations": "2", "title": "Yes, We CANN: Constrained Approximate Nearest Neighbors For Local Feature-based Visual Localization", "abstract": "<p>Large-scale visual localization systems continue to rely on 3D point clouds\nbuilt from image collections using structure-from-motion. While the 3D points\nin these models are represented using local image features, directly matching a\nquery image\u2019s local features against the point cloud is challenging due to the\nscale of the nearest-neighbor search problem. Many recent approaches to visual\nlocalization have thus proposed a hybrid method, where first a global (per\nimage) embedding is used to retrieve a small subset of database images, and\nlocal features of the query are matched only against those. It seems to have\nbecome common belief that global embeddings are critical for said\nimage-retrieval in visual localization, despite the significant downside of\nhaving to compute two feature types for each query image. In this paper, we\ntake a step back from this assumption and propose Constrained Approximate\nNearest Neighbors (CANN), a joint solution of k-nearest-neighbors across both\nthe geometry and appearance space using only local features. We first derive\nthe theoretical foundation for k-nearest-neighbor retrieval across multiple\nmetrics and then showcase how CANN improves visual localization. Our\nexperiments on public localization benchmarks demonstrate that our method\nsignificantly outperforms both state-of-the-art global feature-based retrieval\nand approaches using local feature aggregation schemes. Moreover, it is an\norder of magnitude faster in both index and query time than feature aggregation\nschemes for these datasets. Code:\n\\url{https://github.com/google-research/google-research/tree/master/cann}</p>\n", "tags": ["ANN Search", "Evaluation Metrics", "Tools and Libraries"], "tsne_embedding": [2.240233898162842, -3.0536015033721924], "cluster": 9}, {"key": "ak2021fashionsearchnet", "year": "2021", "citations": "4", "title": "Fashionsearchnet-v2: Learning Attribute Representations With Localization For Image Retrieval With Attribute Manipulation", "abstract": "<p>The focus of this paper is on the problem of image retrieval with attribute\nmanipulation. Our proposed work is able to manipulate the desired attributes of\nthe query image while maintaining its other attributes. For example, the collar\nattribute of the query image can be changed from round to v-neck to retrieve\nsimilar images from a large dataset. A key challenge in e-commerce is that\nimages have multiple attributes where users would like to manipulate and it is\nimportant to estimate discriminative feature representations for each of these\nattributes. The proposed FashionSearchNet-v2 architecture is able to learn\nattribute specific representations by leveraging on its weakly-supervised\nlocalization module, which ignores the unrelated features of attributes in the\nfeature space, thus improving the similarity learning. The network is jointly\ntrained with the combination of attribute classification and triplet ranking\nloss to estimate local representations. These local representations are then\nmerged into a single global representation based on the instructed attribute\nmanipulation where desired images can be retrieved with a distance metric. The\nproposed method also provides explainability for its retrieval process to help\nprovide additional information on the attention of the network. Experiments\nperformed on several datasets that are rich in terms of the number of\nattributes show that FashionSearchNet-v2 outperforms the other state-of-the-art\nattribute manipulation techniques. Different than our earlier work\n(FashionSearchNet), we propose several improvements in the learning procedure\nand show that the proposed FashionSearchNet-v2 can be generalized to different\ndomains other than fashion.</p>\n", "tags": ["Applications", "Supervised", "Loss Functions"], "tsne_embedding": [3.9031074047088623, -0.5961673855781555], "cluster": 9}, {"key": "aksoy2022satellite", "year": "2022", "citations": "6", "title": "Satellite Image Search In Agoraeo", "abstract": "<p>The growing operational capability of global Earth Observation (EO) creates\nnew opportunities for data-driven approaches to understand and protect our\nplanet. However, the current use of EO archives is very restricted due to the\nhuge archive sizes and the limited exploration capabilities provided by EO\nplatforms. To address this limitation, we have recently proposed MiLaN, a\ncontent-based image retrieval approach for fast similarity search in satellite\nimage archives. MiLaN is a deep hashing network based on metric learning that\nencodes high-dimensional image features into compact binary hash codes. We use\nthese codes as keys in a hash table to enable real-time nearest neighbor search\nand highly accurate retrieval. In this demonstration, we showcase the\nefficiency of MiLaN by integrating it with EarthQube, a browser and search\nengine within AgoraEO. EarthQube supports interactive visual exploration and\nQuery-by-Example over satellite image repositories. Demo visitors will interact\nwith EarthQube playing the role of different users that search images in a\nlarge-scale remote sensing archive by their semantic content and apply other\nfilters.</p>\n", "tags": ["Applications", "ANN Search", "Hashing Methods", "Deep Hashing"], "tsne_embedding": [10.319018363952637, 0.28071722388267517], "cluster": 8}, {"key": "ali2017content", "year": "2017", "citations": "4", "title": "Content-based Image Retrieval Based On Late Fusion Of Binary And Local Descriptors", "abstract": "<p>One of the challenges in Content-Based Image Retrieval (CBIR) is to reduce\nthe semantic gaps between low-level features and high-level semantic concepts.\nIn CBIR, the images are represented in the feature space and the performance of\nCBIR depends on the type of selected feature representation. Late fusion also\nknown as visual words integration is applied to enhance the performance of\nimage retrieval. The recent advances in image retrieval diverted the focus of\nresearch towards the use of binary descriptors as they are reported\ncomputationally efficient. In this paper, we aim to investigate the late fusion\nof Fast Retina Keypoint (FREAK) and Scale Invariant Feature Transform (SIFT).\nThe late fusion of binary and local descriptor is selected because among binary\ndescriptors, FREAK has shown good results in classification-based problems\nwhile SIFT is robust to translation, scaling, rotation and small distortions.\nThe late fusion of FREAK and SIFT integrates the performance of both feature\ndescriptors for an effective image retrieval. Experimental results and\ncomparisons show that the proposed late fusion enhances the performances of\nimage retrieval.</p>\n", "tags": ["Applications"], "tsne_embedding": [3.6041831970214844, -3.3889718055725098], "cluster": 9}, {"key": "alomari2019scalable", "year": "2019", "citations": "1", "title": "Scalable Source Code Similarity Detection In Large Code Repositories", "abstract": "<p>Source code similarity are increasingly used in application development to\nidentify clones, isolate bugs, and find copy-rights violations. Similar code\nfragments can be very problematic due to the fact that errors in the original\ncode must be fixed in every copy. Other maintenance changes, such as extensions\nor patches, must be applied multiple times. Furthermore, the diversity of\ncoding styles and flexibility of modern languages makes it difficult and cost\nineffective to manually inspect large code repositories. Therefore, detection\nis only feasible by automatic techniques. We present an efficient and scalable\napproach for similar code fragment identification based on source code control\nflow graphs fingerprinting. The source code is processed to generate control\nflow graphs that are then hashed to create a unique fingerprint of the code\ncapturing semantics as well as syntax similarity. The fingerprints can then be\nefficiently stored and retrieved to perform similarity search between code\nfragments. Experimental results from our prototype implementation supports the\nvalidity of our approach and show its effectiveness and efficiency in\ncomparison with other solutions.</p>\n", "tags": ["Graph", "Hashing Methods", "ANN Search", "Efficient Learning"], "tsne_embedding": [4.915955543518066, 5.875078201293945], "cluster": 5}, {"key": "alparslan2020towards", "year": "2020", "citations": "4", "title": "Towards Evaluating Gaussian Blurring In Perceptual Hashing As A Facial Image Filter", "abstract": "<p>With the growth in social media, there is a huge amount of images of faces\navailable on the internet. Often, people use other people\u2019s pictures on their\nown profile. Perceptual hashing is often used to detect whether two images are\nidentical. Therefore, it can be used to detect whether people are misusing\nothers\u2019 pictures. In perceptual hashing, a hash is calculated for a given\nimage, and a new test image is mapped to one of the existing hashes if\nduplicate features are present. Therefore, it can be used as an image filter to\nflag banned image content or adversarial attacks \u2013which are modifications that\nare made on purpose to deceive the filter\u2013 even though the content might be\nchanged to deceive the filters. For this reason, it is critical for perceptual\nhashing to be robust enough to take transformations such as resizing, cropping,\nand slight pixel modifications into account. In this paper, we would like to\npropose to experiment with effect of gaussian blurring in perceptual hashing\nfor detecting misuse of personal images specifically for face images. We\nhypothesize that use of gaussian blurring on the image before calculating its\nhash will increase the accuracy of our filter that detects adversarial attacks\nwhich consist of image cropping, adding text annotation, and image rotation.</p>\n", "tags": ["Hashing Methods", "Evaluation Metrics", "ANN Search"], "tsne_embedding": [-5.0527238845825195, 6.1330084800720215], "cluster": 0}, {"key": "amara2021nearest", "year": "2021", "citations": "1", "title": "Nearest Neighbor Search With Compact Codes: A Decoder Perspective", "abstract": "<p>Modern approaches for fast retrieval of similar vectors on billion-scaled\ndatasets rely on compressed-domain approaches such as binary sketches or\nproduct quantization. These methods minimize a certain loss, typically the mean\nsquared error or other objective functions tailored to the retrieval problem.\nIn this paper, we re-interpret popular methods such as binary hashing or\nproduct quantizers as auto-encoders, and point out that they implicitly make\nsuboptimal assumptions on the form of the decoder. We design\nbackward-compatible decoders that improve the reconstruction of the vectors\nfrom the same codes, which translates to a better performance in nearest\nneighbor search. Our method significantly improves over binary hashing methods\nor product quantization on popular benchmarks.</p>\n", "tags": ["Quantization", "ANN Search", "Hashing Methods", "Evaluation Metrics"], "tsne_embedding": [-3.4956133365631104, 2.1643259525299072], "cluster": 0}, {"key": "amato2016aggregating", "year": "2016", "citations": "6", "title": "Aggregating Binary Local Descriptors For Image Retrieval", "abstract": "<p>Content-Based Image Retrieval based on local features is computationally\nexpensive because of the complexity of both extraction and matching of local\nfeature. On one hand, the cost for extracting, representing, and comparing\nlocal visual descriptors has been dramatically reduced by recently proposed\nbinary local features. On the other hand, aggregation techniques provide a\nmeaningful summarization of all the extracted feature of an image into a single\ndescriptor, allowing us to speed up and scale up the image search. Only a few\nworks have recently mixed together these two research directions, defining\naggregation methods for binary local features, in order to leverage on the\nadvantage of both approaches. In this paper, we report an extensive comparison\namong state-of-the-art aggregation methods applied to binary features. Then, we\nmathematically formalize the application of Fisher Kernels to Bernoulli Mixture\nModels. Finally, we investigate the combination of the aggregated binary\nfeatures with the emerging Convolutional Neural Network (CNN) features. Our\nresults show that aggregation methods on binary features are effective and\nrepresent a worthwhile alternative to the direct matching. Moreover, the\ncombination of the CNN with the Fisher Vector (FV) built upon binary features\nallowed us to obtain a relative improvement over the CNN results that is in\nline with that recently obtained using the combination of the CNN with the FV\nbuilt upon SIFTs. The advantage of using the FV built upon binary features is\nthat the extraction process of binary features is about two order of magnitude\nfaster than SIFTs.</p>\n", "tags": ["Applications", "Hashing Methods", "Deep Hashing", "Efficient Learning"], "tsne_embedding": [5.291273593902588, -2.7742393016815186], "cluster": 8}, {"key": "amrouche2021hashing", "year": "2021", "citations": "5", "title": "Hashing And Metric Learning For Charged Particle Tracking", "abstract": "<p>We propose a novel approach to charged particle tracking at high intensity\nparticle colliders based on Approximate Nearest Neighbors search. With hundreds\nof thousands of measurements per collision to be reconstructed e.g. at the High\nLuminosity Large Hadron Collider, the currently employed combinatorial track\nfinding approaches become inadequate. Here, we use hashing techniques to\nseparate measurements into buckets of 20-50 hits and increase their purity\nusing metric learning. Two different approaches are studied to further resolve\ntracks inside buckets: Local Fisher Discriminant Analysis and Neural Networks\nfor triplet similarity learning. We demonstrate the proposed approach on\nsimulated collisions and show significant speed improvement with bucket\ntracking efficiency of 96% and a fake rate of 8% on unseen particle events.</p>\n", "tags": ["ANN Search", "Hashing Methods", "Loss Functions"], "tsne_embedding": [-4.759254455566406, -9.993099212646484], "cluster": 6}, {"key": "an2023universal", "year": "2023", "citations": "10", "title": "Unicom: Universal And Compact Representation Learning For Image Retrieval", "abstract": "<p>Modern image retrieval methods typically rely on fine-tuning pre-trained\nencoders to extract image-level descriptors. However, the most widely used\nmodels are pre-trained on ImageNet-1K with limited classes. The pre-trained\nfeature representation is therefore not universal enough to generalize well to\nthe diverse open-world classes. In this paper, we first cluster the large-scale\nLAION400M into one million pseudo classes based on the joint textual and visual\nfeatures extracted by the CLIP model. Due to the confusion of label\ngranularity, the automatically clustered dataset inevitably contains heavy\ninter-class conflict. To alleviate such conflict, we randomly select partial\ninter-class prototypes to construct the margin-based softmax loss. To further\nenhance the low-dimensional feature representation, we randomly select partial\nfeature dimensions when calculating the similarities between embeddings and\nclass-wise prototypes. The dual random partial selections are with respect to\nthe class dimension and the feature dimension of the prototype matrix, making\nthe classification conflict-robust and the feature embedding compact. Our\nmethod significantly outperforms state-of-the-art unsupervised and supervised\nimage retrieval approaches on multiple benchmarks. The code and pre-trained\nmodels are released to facilitate future research\nhttps://github.com/deepglint/unicom.</p>\n", "tags": ["Applications", "Unsupervised", "Supervised", "Benchmarks and Datasets", "Tools and Libraries"], "tsne_embedding": [1.088690161705017, -0.3996076285839081], "cluster": 9}, {"key": "andoni2013beyond", "year": "2013", "citations": "62", "title": "Beyond Locality-sensitive Hashing", "abstract": "<p>We present a new data structure for the c-approximate near neighbor problem\n(ANN) in the Euclidean space. For n points in R^d, our algorithm achieves\nO(n^{\\rho} + d log n) query time and O(n^{1 + \\rho} + d log n) space, where\n\\rho &lt;= 7/(8c^2) + O(1 / c^3) + o(1). This is the first improvement over the\nresult by Andoni and Indyk (FOCS 2006) and the first data structure that\nbypasses a locality-sensitive hashing lower bound proved by O\u2019Donnell, Wu and\nZhou (ICS 2011). By a standard reduction we obtain a data structure for the\nHamming space and \\ell_1 norm with \\rho &lt;= 7/(8c) + O(1/c^{3/2}) + o(1), which\nis the first improvement over the result of Indyk and Motwani (STOC 1998).</p>\n", "tags": ["ANN Search", "Hashing Methods"], "tsne_embedding": [-12.724266052246094, -2.2834067344665527], "cluster": 2}, {"key": "andoni2015optimal", "year": "2015", "citations": "111", "title": "Optimal Data-dependent Hashing For Approximate Near Neighbors", "abstract": "<p>We show an optimal data-dependent hashing scheme for the approximate near\nneighbor problem. For an \\(n\\)-point data set in a \\(d\\)-dimensional space our data\nstructure achieves query time \\(O(d n^{\\rho+o(1)})\\) and space \\(O(n^{1+\\rho+o(1)}</p>\n<ul>\n  <li>dn)\\), where \\(\\rho=\\tfrac{1}{2c^2-1}\\) for the Euclidean space and\napproximation \\(c&gt;1\\). For the Hamming space, we obtain an exponent of\n\\(\\rho=\\tfrac{1}{2c-1}\\).\nOur result completes the direction set forth in [AINR14] who gave a\nproof-of-concept that data-dependent hashing can outperform classical Locality\nSensitive Hashing (LSH). In contrast to [AINR14], the new bound is not only\noptimal, but in fact improves over the best (optimal) LSH data structures\n[IM98,AI06] for all approximation factors \\(c&gt;1\\).\nFrom the technical perspective, we proceed by decomposing an arbitrary\ndataset into several subsets that are, in a certain sense, pseudo-random.</li>\n</ul>\n", "tags": ["Hashing Methods", "ANN Search"], "tsne_embedding": [-13.116626739501953, -4.252466201782227], "cluster": 2}, {"key": "andoni2015practical", "year": "2015", "citations": "238", "title": "Practical And Optimal LSH For Angular Distance", "abstract": "<p>We show the existence of a Locality-Sensitive Hashing (LSH) family for the\nangular distance that yields an approximate Near Neighbor Search algorithm with\nthe asymptotically optimal running time exponent. Unlike earlier algorithms\nwith this property (e.g., Spherical LSH [Andoni, Indyk, Nguyen, Razenshteyn\n2014], [Andoni, Razenshteyn 2015]), our algorithm is also practical, improving\nupon the well-studied hyperplane LSH [Charikar, 2002] in practice. We also\nintroduce a multiprobe version of this algorithm, and conduct experimental\nevaluation on real and synthetic data sets.\n  We complement the above positive results with a fine-grained lower bound for\nthe quality of any LSH family for angular distance. Our lower bound implies\nthat the above LSH family exhibits a trade-off between evaluation time and\nquality that is close to optimal for a natural class of LSH functions.</p>\n", "tags": ["Hashing Methods", "ANN Search"], "tsne_embedding": [-9.776257514953613, 0.49837878346443176], "cluster": 4}, {"key": "andoni2015tight", "year": "2015", "citations": "21", "title": "Tight Lower Bounds For Data-dependent Locality-sensitive Hashing", "abstract": "<p>We prove a tight lower bound for the exponent \\(\\rho\\) for data-dependent\nLocality-Sensitive Hashing schemes, recently used to design efficient solutions\nfor the \\(c\\)-approximate nearest neighbor search. In particular, our lower bound\nmatches the bound of \\(\\rho\\le \\frac{1}{2c-1}+o(1)\\) for the \\(\\ell_1\\) space,\nobtained via the recent algorithm from [Andoni-Razenshteyn, STOC\u201915].\n  In recent years it emerged that data-dependent hashing is strictly superior\nto the classical Locality-Sensitive Hashing, when the hash function is\ndata-independent. In the latter setting, the best exponent has been already\nknown: for the \\(\\ell_1\\) space, the tight bound is \\(\\rho=1/c\\), with the upper\nbound from [Indyk-Motwani, STOC\u201998] and the matching lower bound from\n[O\u2019Donnell-Wu-Zhou, ITCS\u201911].\n  We prove that, even if the hashing is data-dependent, it must hold that\n\\(\\rho\\ge \\frac{1}{2c-1}-o(1)\\). To prove the result, we need to formalize the\nexact notion of data-dependent hashing that also captures the complexity of the\nhash functions (in addition to their collision properties). Without restricting\nsuch complexity, we would allow for obviously infeasible solutions such as the\nVoronoi diagram of a dataset. To preclude such solutions, we require our hash\nfunctions to be succinct. This condition is satisfied by all the known\nalgorithmic results.</p>\n", "tags": ["ANN Search", "Hashing Methods"], "tsne_embedding": [-11.060362815856934, -6.870145320892334], "cluster": 2}, {"key": "andoni2016lower", "year": "2016", "citations": "4", "title": "Lower Bounds On Time-space Trade-offs For Approximate Near Neighbors", "abstract": "<p>We show tight lower bounds for the entire trade-off between space and query\ntime for the Approximate Near Neighbor search problem. Our lower bounds hold in\na restricted model of computation, which captures all hashing-based approaches.\nIn articular, our lower bound matches the upper bound recently shown in\n[Laarhoven 2015] for the random instance on a Euclidean sphere (which we show\nin fact extends to the entire space \\(\\mathbb{R}^d\\) using the techniques from\n[Andoni, Razenshteyn 2015]).\n  We also show tight, unconditional cell-probe lower bounds for one and two\nprobes, improving upon the best known bounds from [Panigrahy, Talwar, Wieder\n2010]. In particular, this is the first space lower bound (for any static data\nstructure) for two probes which is not polynomially smaller than for one probe.\nTo show the result for two probes, we establish and exploit a connection to\nlocally-decodable codes.</p>\n", "tags": ["Hashing Methods", "ANN Search"], "tsne_embedding": [-12.576038360595703, -0.7435770034790039], "cluster": 4}, {"key": "andoni2016optimal", "year": "2016", "citations": "30", "title": "Optimal Hashing-based Time-space Trade-offs For Approximate Near Neighbors", "abstract": "<p>[See the paper for the full abstract.]\n  We show tight upper and lower bounds for time-space trade-offs for the\n\\(c\\)-Approximate Near Neighbor Search problem. For the \\(d\\)-dimensional Euclidean\nspace and \\(n\\)-point datasets, we develop a data structure with space \\(n^{1 +\n\\rho_u + o(1)} + O(dn)\\) and query time \\(n^{\\rho_q + o(1)} + d n^{o(1)}\\) for\nevery \\(\\rho_u, \\rho_q \\geq 0\\) such that: \\begin{equation} c^2 \\sqrt{\\rho_q} +\n(c^2 - 1) \\sqrt{\\rho_u} = \\sqrt{2c^2 - 1}. \\end{equation}\n  This is the first data structure that achieves sublinear query time and\nnear-linear space for every approximation factor \\(c &gt; 1\\), improving upon\n[Kapralov, PODS 2015]. The data structure is a culmination of a long line of\nwork on the problem for all space regimes; it builds on Spherical\nLocality-Sensitive Filtering [Becker, Ducas, Gama, Laarhoven, SODA 2016] and\ndata-dependent hashing [Andoni, Indyk, Nguyen, Razenshteyn, SODA 2014] [Andoni,\nRazenshteyn, STOC 2015].\n  Our matching lower bounds are of two types: conditional and unconditional.\nFirst, we prove tightness of the whole above trade-off in a restricted model of\ncomputation, which captures all known hashing-based approaches. We then show\nunconditional cell-probe lower bounds for one and two probes that match the\nabove trade-off for \\(\\rho_q = 0\\), improving upon the best known lower bounds\nfrom [Panigrahy, Talwar, Wieder, FOCS 2010]. In particular, this is the first\nspace lower bound (for any static data structure) for two probes which is not\npolynomially smaller than the one-probe bound. To show the result for two\nprobes, we establish and exploit a connection to locally-decodable codes.</p>\n", "tags": ["Hashing Methods", "ANN Search"], "tsne_embedding": [-12.887547492980957, -7.862214088439941], "cluster": 2}, {"key": "andoni2021from", "year": "2021", "citations": "0", "title": "From Average Embeddings To Nearest Neighbor Search", "abstract": "<p>In this note, we show that one can use average embeddings, introduced\nrecently in [Naor\u201920, arXiv:1905.01280], to obtain efficient algorithms for\napproximate nearest neighbor search. In particular, a metric \\(X\\) embeds into\n\\(\u2113\u2082\\) on average, with distortion \\(D\\), if, for any distribution \\(\\mu\\) on\n\\(X\\), the embedding is \\(D\\) Lipschitz and the (square of) distance does not\ndecrease on average (wrt \\(\\mu\\)). In particular existence of such an embedding\n(assuming it is efficient) implies a \\(O(D^3)\\) approximate nearest neighbor\nsearch under \\(X\\). This can be seen as a strengthening of the classic\n(bi-Lipschitz) embedding approach to nearest neighbor search, and is another\napplication of data-dependent hashing paradigm.</p>\n", "tags": ["ANN Search", "Hashing Methods"], "tsne_embedding": [-10.677424430847168, 0.7931158542633057], "cluster": 4}, {"key": "andoni2021learning", "year": "2021", "citations": "1", "title": "Learning To Hash Robustly, Guaranteed", "abstract": "<p>The indexing algorithms for the high-dimensional nearest neighbor search\n(NNS) with the best worst-case guarantees are based on the randomized Locality\nSensitive Hashing (LSH), and its derivatives. In practice, many heuristic\napproaches exist to \u201clearn\u201d the best indexing method in order to speed-up NNS,\ncrucially adapting to the structure of the given dataset.\n  Oftentimes, these heuristics outperform the LSH-based algorithms on real\ndatasets, but, almost always, come at the cost of losing the guarantees of\neither correctness or robust performance on adversarial queries, or apply to\ndatasets with an assumed extra structure/model. In this paper, we design an NNS\nalgorithm for the Hamming space that has worst-case guarantees essentially\nmatching that of theoretical algorithms, while optimizing the hashing to the\nstructure of the dataset (think instance-optimal algorithms) for performance on\nthe minimum-performing query. We evaluate the algorithm\u2019s ability to optimize\nfor a given dataset both theoretically and practically. On the theoretical\nside, we exhibit a natural setting (dataset model) where our algorithm is much\nbetter than the standard theoretical one. On the practical side, we run\nexperiments that show that our algorithm has a 1.8x and 2.1x better recall on\nthe worst-performing queries to the MNIST and ImageNet datasets.</p>\n", "tags": ["Hashing Methods", "Indexing", "ANN Search", "Evaluation Metrics", "Benchmarks and Datasets"], "tsne_embedding": [-4.786643981933594, -2.0458860397338867], "cluster": 7}, {"key": "andoni2024learning", "year": "2024", "citations": "0", "title": "Learning To Hash Robustly With Guarantees", "abstract": "<p>The indexing algorithms for the high-dimensional nearest neighbor search (NNS) with the best worst-case guarantees are based on the randomized Locality Sensitive Hashing (LSH), and its derivatives. In practice, many heuristic approaches exist to \u201clearn\u201d the best indexing method in order to speed-up NNS, crucially adapting to the structure of the given dataset. Oftentimes, these heuristics outperform the LSH-based algorithms on real datasets, but, almost always, come at the cost of losing the guarantees of either correctness or robust performance on adversarial queries, or apply to datasets with an assumed extra structure/model. In this paper, we design an NNS algorithm for the Hamming space that has worst-case guarantees essentially matching that of theoretical algorithms, while optimizing the hashing to the structure of the dataset (think instance-optimal algorithms) for performance on the minimum-performing query. We evaluate the algorithm\u2019s ability to optimize for a given dataset both theoretically and practically. On the theoretical side, we exhibit a natural setting (dataset model) where our algorithm is much better than the standard theoretical one. On the practical side, we run experiments that show that our algorithm has a 1.8x and 2.1x better recall on the worst-performing queries to the MNIST and ImageNet datasets.</p>\n", "tags": ["Hashing Methods", "ANN Search", "Evaluation Metrics", "Applications", "Supervision Type"], "tsne_embedding": [-4.786678791046143, -2.045931100845337], "cluster": 7}, {"key": "andoni2024near", "year": "2024", "citations": "707", "title": "Near-optimal Hashing Algorithms For Approximate Nearest Neighbor In High Dimensions", "abstract": "<p>We present an algorithm for the c-approximate nearest neighbor problem in a d-dimensional Euclidean space, achieving query time of O(dn 1c2/+o(1)) and space O(dn + n1+1c2/+o(1)). This almost matches the lower bound for hashing-based algorithm recently obtained in (R. Motwani et al., 2006). We also obtain a space-efficient version of the algorithm, which uses dn+n logO(1) n space, with a query time of dnO(1/c2). Finally, we discuss practical variants of the algorithms that utilize fast bounded-distance decoders for the Leech lattice</p>\n", "tags": ["ANN Search", "Hashing Methods"], "tsne_embedding": [-12.703683853149414, -0.683865487575531], "cluster": 4}, {"key": "andrecut2021additive", "year": "2021", "citations": "0", "title": "Additive Feature Hashing", "abstract": "<p>The hashing trick is a machine learning technique used to encode categorical\nfeatures into a numerical vector representation of pre-defined fixed length. It\nworks by using the categorical hash values as vector indices, and updating the\nvector values at those indices. Here we discuss a different approach based on\nadditive-hashing and the \u201calmost orthogonal\u201d property of high-dimensional\nrandom vectors. That is, we show that additive feature hashing can be performed\ndirectly by adding the hash values and converting them into high-dimensional\nnumerical vectors. We show that the performance of additive feature hashing is\nsimilar to the hashing trick, and we illustrate the results numerically using\nsynthetic, language recognition, and SMS spam detection data.</p>\n", "tags": ["Hashing Methods"], "tsne_embedding": [6.547476291656494, 10.221634864807129], "cluster": 5}, {"key": "andr\u00e92017accelerated", "year": "2017", "citations": "17", "title": "Accelerated Nearest Neighbor Search With Quick ADC", "abstract": "<p>Efficient Nearest Neighbor (NN) search in high-dimensional spaces is a\nfoundation of many multimedia retrieval systems. Because it offers low\nresponses times, Product Quantization (PQ) is a popular solution. PQ compresses\nhigh-dimensional vectors into short codes using several sub-quantizers, which\nenables in-RAM storage of large databases. This allows fast answers to NN\nqueries, without accessing the SSD or HDD. The key feature of PQ is that it can\ncompute distances between short codes and high-dimensional vectors using\ncache-resident lookup tables. The efficiency of this technique, named\nAsymmetric Distance Computation (ADC), remains limited because it performs many\ncache accesses.\n  In this paper, we introduce Quick ADC, a novel technique that achieves a 3 to\n6 times speedup over ADC by exploiting Single Instruction Multiple Data (SIMD)\nunits available in current CPUs. Efficiently exploiting SIMD requires\nalgorithmic changes to the ADC procedure. Namely, Quick ADC relies on two key\nmodifications of ADC: (i) the use 4-bit sub-quantizers instead of the standard\n8-bit sub-quantizers and (ii) the quantization of floating-point distances.\nThis allows Quick ADC to exceed the performance of state-of-the-art systems,\ne.g., it achieves a Recall@100 of 0.94 in 3.4 ms on 1 billion SIFT descriptors\n(128-bit codes).</p>\n", "tags": ["ANN Search", "Efficient Learning", "Quantization", "Evaluation Metrics"], "tsne_embedding": [-1.71604585647583, -9.285642623901367], "cluster": 6}, {"key": "andr\u00e92017exploiting", "year": "2017", "citations": "0", "title": "Exploiting Modern Hardware For High-dimensional Nearest Neighbor Search", "abstract": "<p>Many multimedia information retrieval or machine learning problems require\nefficient high-dimensional nearest neighbor search techniques. For instance,\nmultimedia objects (images, music or videos) can be represented by\nhigh-dimensional feature vectors. Finding two similar multimedia objects then\ncomes down to finding two objects that have similar feature vectors. In the\ncurrent context of mass use of social networks, large scale multimedia\ndatabases or large scale machine learning applications are more and more\ncommon, calling for efficient nearest neighbor search approaches.\n  This thesis builds on product quantization, an efficient nearest neighbor\nsearch technique that compresses high-dimensional vectors into short codes.\nThis makes it possible to store very large databases entirely in RAM, enabling\nlow response times. We propose several contributions that exploit the\ncapabilities of modern CPUs, especially SIMD and the cache hierarchy, to\nfurther decrease response times offered by product quantization.</p>\n", "tags": ["ANN Search", "Quantization", "Applications"], "tsne_embedding": [-1.3407039642333984, -9.330545425415039], "cluster": 6}, {"key": "andr\u00e92019derived", "year": "2019", "citations": "1", "title": "Derived Codebooks For High-accuracy Nearest Neighbor Search", "abstract": "<p>High-dimensional Nearest Neighbor (NN) search is central in multimedia search\nsystems. Product Quantization (PQ) is a widespread NN search technique which\nhas a high performance and good scalability. PQ compresses high-dimensional\nvectors into compact codes thanks to a combination of quantizers. Large\ndatabases can, therefore, be stored entirely in RAM, enabling fast responses to\nNN queries. In almost all cases, PQ uses 8-bit quantizers as they offer low\nresponse times. In this paper, we advocate the use of 16-bit quantizers.\nCompared to 8-bit quantizers, 16-bit quantizers boost accuracy but they\nincrease response time by a factor of 3 to 10. We propose a novel approach that\nallows 16-bit quantizers to offer the same response time as 8-bit quantizers,\nwhile still providing a boost of accuracy. Our approach builds on two key\nideas: (i) the construction of derived codebooks that allow a fast and\napproximate distance evaluation, and (ii) a two-pass NN search procedure which\nbuilds a candidate set using the derived codebooks, and then refines it using\n16-bit quantizers. On 1 billion SIFT vectors, with an inverted index, our\napproach offers a Recall@100 of 0.85 in 5.2 ms. By contrast, 16-bit quantizers\nalone offer a Recall@100 of 0.85 in 39 ms, and 8-bit quantizers a Recall@100 of\n0.82 in 3.8 ms.</p>\n", "tags": ["Indexing", "ANN Search", "Quantization", "Evaluation Metrics"], "tsne_embedding": [-2.1975865364074707, -10.233285903930664], "cluster": 6}, {"key": "annaji2009parallelization", "year": "2009", "citations": "0", "title": "Parallelization Of The LBG Vector Quantization Algorithm For Shared Memory Systems", "abstract": "<p>This paper proposes a parallel approach for the Vector Quantization (VQ)\nproblem in image processing. VQ deals with codebook generation from the input\ntraining data set and replacement of any arbitrary data with the nearest\ncodevector. Most of the efforts in VQ have been directed towards designing\nparallel search algorithms for the codebook, and little has hitherto been done\nin evolving a parallelized procedure to obtain an optimum codebook. This\nparallel algorithm addresses the problem of designing an optimum codebook using\nthe traditional LBG type of vector quantization algorithm for shared memory\nsystems and for the efficient usage of parallel processors. Using the codebook\nformed from a training set, any arbitrary input data is replaced with the\nnearest codevector from the codebook. The effectiveness of the proposed\nalgorithm is indicated.</p>\n", "tags": ["Quantization"], "tsne_embedding": [2.7971653938293457, 13.775630950927734], "cluster": 5}, {"key": "argerich2017generic", "year": "2017", "citations": "1", "title": "Generic LSH Families For The Angular Distance Based On Johnson-lindenstrauss Projections And Feature Hashing LSH", "abstract": "<p>In this paper we propose the creation of generic LSH families for the angular\ndistance based on Johnson-Lindenstrauss projections. We show that feature\nhashing is a valid J-L projection and propose two new LSH families based on\nfeature hashing. These new LSH families are tested on both synthetic and real\ndatasets with very good results and a considerable performance improvement over\nother LSH families. While the theoretical analysis is done for the angular\ndistance, these families can also be used in practice for the euclidean\ndistance with excellent results [2]. Our tests using real datasets show that\nthe proposed LSH functions work well for the euclidean distance.</p>\n", "tags": ["Hashing Methods", "Tools and Libraries"], "tsne_embedding": [-9.525679588317871, 0.04323197528719902], "cluster": 4}, {"key": "arponen2019semantic", "year": "2019", "citations": "2", "title": "SHREWD: Semantic Hierarchy-based Relational Embeddings For Weakly-supervised Deep Hashing", "abstract": "<p>Using class labels to represent class similarity is a typical approach to\ntraining deep hashing systems for retrieval; samples from the same or different\nclasses take binary 1 or 0 similarity values. This similarity does not model\nthe full rich knowledge of semantic relations that may be present between data\npoints. In this work we build upon the idea of using semantic hierarchies to\nform distance metrics between all available sample labels; for example cat to\ndog has a smaller distance than cat to guitar. We combine this type of semantic\ndistance into a loss function to promote similar distances between the deep\nneural network embeddings. We also introduce an empirical Kullback-Leibler\ndivergence loss term to promote binarization and uniformity of the embeddings.\nWe test the resulting SHREWD method and demonstrate improvements in\nhierarchical retrieval scores using compact, binary hash codes instead of real\nvalued ones, and show that in a weakly supervised hashing setting we are able\nto learn competitively without explicitly relying on class labels, but instead\non similarities between labels.</p>\n", "tags": ["Hashing Methods", "Deep Hashing", "Supervised", "Loss Functions"], "tsne_embedding": [3.6733720302581787, 1.7374979257583618], "cluster": 9}, {"key": "arponen2020learning", "year": "2020", "citations": "0", "title": "Learning To Hash With Semantic Similarity Metrics And Empirical KL Divergence", "abstract": "<p>Learning to hash is an efficient paradigm for exact and approximate nearest\nneighbor search from massive databases. Binary hash codes are typically\nextracted from an image by rounding output features from a CNN, which is\ntrained on a supervised binary similar/ dissimilar task. Drawbacks of this\napproach are: (i) resulting codes do not necessarily capture semantic\nsimilarity of the input data (ii) rounding results in information loss,\nmanifesting as decreased retrieval performance and (iii) Using only class-wise\nsimilarity as a target can lead to trivial solutions, simply encoding\nclassifier outputs rather than learning more intricate relations, which is not\ndetected by most performance metrics. We overcome (i) via a novel loss function\nencouraging the relative hash code distances of learned features to match those\nderived from their targets. We address (ii) via a differentiable estimate of\nthe KL divergence between network outputs and a binary target distribution,\nresulting in minimal information loss when the features are rounded to binary.\nFinally, we resolve (iii) by focusing on a hierarchical precision metric.\nEfficiency of the methods is demonstrated with semantic image retrieval on the\nCIFAR-100, ImageNet and Conceptual Captions datasets, using similarities\ninferred from the WordNet label hierarchy or sentence embeddings.</p>\n", "tags": ["Applications", "Evaluation Metrics", "Supervised", "Benchmarks and Datasets", "Hashing Methods", "Loss Functions"], "tsne_embedding": [1.0891951322555542, 2.317878484725952], "cluster": 9}, {"key": "arthur2010reverse", "year": "2010", "citations": "3", "title": "Reverse Nearest Neighbors Search In High Dimensions Using Locality-sensitive Hashing", "abstract": "<p>We investigate the problem of finding reverse nearest neighbors efficiently.\nAlthough provably good solutions exist for this problem in low or fixed\ndimensions, to this date the methods proposed in high dimensions are mostly\nheuristic. We introduce a method that is both provably correct and efficient in\nall dimensions, based on a reduction of the problem to one instance of\n\\(\\e\\)-nearest neighbor search plus a controlled number of instances of {\\em\nexhaustive \\(r\\)-\\pleb}, a variant of {\\em Point Location among Equal Balls}\nwhere all the \\(r\\)-balls centered at the data points that contain the query\npoint are sought for, not just one. The former problem has been extensively\nstudied and elegantly solved in high dimensions using Locality-Sensitive\nHashing (LSH) techniques. By contrast, the latter problem has a complexity that\nis still not fully understood. We revisit the analysis of the LSH scheme for\nexhaustive \\(r\\)-\\pleb using a somewhat refined notion of locality-sensitive\nfamily of hash function, which brings out a meaningful output-sensitive term in\nthe complexity of the problem. Our analysis, combined with a non-isometric\nlifting of the data, enables us to answer exhaustive \\(r\\)-\\pleb queries (and\ndown the road reverse nearest neighbors queries) efficiently. Along the way, we\nobtain a simple algorithm for answering exact nearest neighbor queries, whose\ncomplexity is parametrized by some {\\em condition number} measuring the\ninherent difficulty of a given instance of the problem.</p>\n", "tags": ["ANN Search", "Hashing Methods"], "tsne_embedding": [-10.424407958984375, -5.518123149871826], "cluster": 2}, {"key": "asghar2025property", "year": "2025", "citations": "0", "title": "Property-preserving Hashing For \\(\\ell_1\\)-distance Predicates: Applications To Countering Adversarial Input Attacks", "abstract": "<p>Perceptual hashing is used to detect whether an input image is similar to a\nreference image with a variety of security applications. Recently, they have\nbeen shown to succumb to adversarial input attacks which make small\nimperceptible changes to the input image yet the hashing algorithm does not\ndetect its similarity to the original image. Property-preserving hashing (PPH)\nis a recent construct in cryptography, which preserves some property\n(predicate) of its inputs in the hash domain. Researchers have so far shown\nconstructions of PPH for Hamming distance predicates, which, for instance,\noutputs 1 if two inputs are within Hamming distance \\(t\\). A key feature of PPH\nis its strong correctness guarantee, i.e., the probability that the predicate\nwill not be correctly evaluated in the hash domain is negligible. Motivated by\nthe use case of detecting similar images under adversarial setting, we propose\nthe first PPH construction for an \\(\\ell_1\\)-distance predicate. Roughly, this\npredicate checks if the two one-sided \\(\\ell_1\\)-distances between two images are\nwithin a threshold \\(t\\). Since many adversarial attacks use \\(\u2113\u2082\\)-distance\n(related to \\(\\ell_1\\)-distance) as the objective function to perturb the input\nimage, by appropriately choosing the threshold \\(t\\), we can force the attacker\nto add considerable noise to evade detection, and hence significantly\ndeteriorate the image quality. Our proposed scheme is highly efficient, and\nruns in time \\(O(t^2)\\). For grayscale images of size \\(28 \\times 28\\), we can\nevaluate the predicate in \\(0.0784\\) seconds when pixel values are perturbed by\nup to \\(1 %\\). For larger RGB images of size \\(224 \\times 224\\), by dividing the\nimage into 1,000 blocks, we achieve times of \\(0.0128\\) seconds per block for \\(1\n%\\) change, and up to \\(0.2641\\) seconds per block for \\(14%\\) change.</p>\n", "tags": ["Hashing Methods", "Applications", "Evaluation Metrics"], "tsne_embedding": [-8.916706085205078, -8.924371719360352], "cluster": 7}, {"key": "ashutosh2024ai", "year": "2024", "citations": "0", "title": "Ai-based Copyright Detection Of An Image In A Video Using Degree Of Similarity And Image Hashing", "abstract": "<p>The expanse of information available over the internet makes it difficult to\nidentify whether a specific work is a replica or a duplication of a protected\nwork, especially if we talk about visual representations. Strategies are\nplanned to identify the utilization of the copyrighted image in a report.\nStill, we want to resolve the issue of involving a copyrighted image in a video\nand a calculation that could recognize the degree of similarity of the\ncopyrighted picture utilized in the video, even for the pieces of the video\nthat are not featured a lot and in the end perform characterization errands on\nthose edges. Machine learning (ML) and artificial intelligence (AI) are vital\nto address this problem. Numerous associations have been creating different\ncalculations to screen the identification of copyrighted work. This work means\nconcentrating on those calculations, recognizing designs inside the\ninformation, and fabricating a more reasonable model for copyrighted image\nclassification and detection. We have used different algorithms like- Image\nProcessing, Convolutional Neural Networks (CNN), Image hashing, etc. Keywords-\nCopyright, Artificial Intelligence(AI), Copyrighted Image, Convolutional Neural\nNetwork(CNN), Image processing, Degree of similarity, Image Hashing.</p>\n", "tags": ["Hashing Methods", "ANN Search", "Deep Hashing", "Applications"], "tsne_embedding": [-1.6961982250213623, 12.75938892364502], "cluster": 5}, {"key": "aum\u00fcller2016simple", "year": "2016", "citations": "4", "title": "A Simple Hash Class With Strong Randomness Properties In Graphs And Hypergraphs", "abstract": "<p>We study randomness properties of graphs and hypergraphs generated by simple\nhash functions. Several hashing applications can be analyzed by studying the\nstructure of \\(d\\)-uniform random (\\(d\\)-partite) hypergraphs obtained from a set\n\\(S\\) of \\(n\\) keys and \\(d\\) randomly chosen hash functions \\(h_1,\\dots,h_d\\) by\nassociating each key \\(x\\in S\\) with a hyperedge \\(\\{h_1(x),\\dots, h_d(x)\\}\\).\nOften it is assumed that \\(h_1,\\dots,h_d\\) exhibit a high degree of independence.\nWe present a simple construction of a hash class whose hash functions have\nsmall constant evaluation time and can be stored in sublinear space. We devise\ngeneral techniques to analyze the randomness properties of the graphs and\nhypergraphs generated by these hash functions, and we show that they can\nreplace other, less efficient constructions in cuckoo hashing (with and without\nstash), the simulation of a uniform hash function, the construction of a\nperfect hash function, generalized cuckoo hashing and different load balancing\nscenarios.</p>\n", "tags": ["Hashing Methods", "Applications"], "tsne_embedding": [-17.35036277770996, -5.280623435974121], "cluster": 2}, {"key": "aum\u00fcller2017distance", "year": "2017", "citations": "13", "title": "Distance-sensitive Hashing", "abstract": "<p>Locality-sensitive hashing (LSH) is an important tool for managing\nhigh-dimensional noisy or uncertain data, for example in connection with data\ncleaning (similarity join) and noise-robust search (similarity search).\nHowever, for a number of problems the LSH framework is not known to yield good\nsolutions, and instead ad hoc solutions have been designed for particular\nsimilarity and distance measures. For example, this is true for\noutput-sensitive similarity search/join, and for indexes supporting annulus\nqueries that aim to report a point close to a certain given distance from the\nquery point.\n  In this paper we initiate the study of distance-sensitive hashing (DSH), a\ngeneralization of LSH that seeks a family of hash functions such that the\nprobability of two points having the same hash value is a given function of the\ndistance between them. More precisely, given a distance space \\((X,\n\\text{dist})\\) and a \u201ccollision probability function\u201d (CPF) \\(f\\colon\n\\mathbb{R}\\rightarrow [0,1]\\) we seek a distribution over pairs of functions\n\\((h,g)\\) such that for every pair of points \\(x, y \\in X\\) the collision\nprobability is \\(\\Pr[h(x)=g(y)] = f(\\text{dist}(x,y))\\). Locality-sensitive\nhashing is the study of how fast a CPF can decrease as the distance grows. For\nmany spaces, \\(f\\) can be made exponentially decreasing even if we restrict\nattention to the symmetric case where \\(g=h\\). We show that the asymmetry\nachieved by having a pair of functions makes it possible to achieve CPFs that\nare, for example, increasing or unimodal, and show how this leads to principled\nsolutions to problems not addressed by the LSH framework. This includes a novel\napplication to privacy-preserving distance estimation. We believe that the DSH\nframework will find further applications in high-dimensional data management.</p>\n", "tags": ["Hashing Methods", "ANN Search", "Applications"], "tsne_embedding": [-12.301553726196289, -5.9773688316345215], "cluster": 2}, {"key": "aum\u00fcller2018ann", "year": "2018", "citations": "153", "title": "Ann-benchmarks A Benchmarking Tool For Approximate Nearest Neighbor Algorithms", "abstract": "<p>This paper describes ANN-Benchmarks, a tool for evaluating the performance of\nin-memory approximate nearest neighbor algorithms. It provides a standard\ninterface for measuring the performance and quality achieved by nearest\nneighbor algorithms on different standard data sets. It supports several\ndifferent ways of integrating \\(k\\)-NN algorithms, and its configuration system\nautomatically tests a range of parameter settings for each algorithm.\nAlgorithms are compared with respect to many different (approximate) quality\nmeasures, and adding more is easy and fast; the included plotting front-ends\ncan visualise these as images, \\(\\LaTeX\\) plots, and websites with interactive\nplots. ANN-Benchmarks aims to provide a constantly updated overview of the\ncurrent state of the art of \\(k\\)-NN algorithms. In the short term, this overview\nallows users to choose the correct \\(k\\)-NN algorithm and parameters for their\nsimilarity search task; in the longer term, algorithm designers will be able to\nuse this overview to test and refine automatic parameter tuning. The paper\ngives an overview of the system, evaluates the results of the benchmark, and\npoints out directions for future work. Interestingly, very different approaches\nto \\(k\\)-NN search yield comparable quality-performance trade-offs. The system is\navailable at http://ann-benchmarks.com .</p>\n", "tags": ["ANN Search", "Evaluation Metrics", "Tools and Libraries", "Benchmarks and Datasets"], "tsne_embedding": [-11.039557456970215, 3.4894092082977295], "cluster": 4}, {"key": "aum\u00fcller2019fair", "year": "2019", "citations": "11", "title": "Fair Near Neighbor Search: Independent Range Sampling In High Dimensions", "abstract": "<p>Similarity search is a fundamental algorithmic primitive, widely used in many\ncomputer science disciplines. There are several variants of the similarity\nsearch problem, and one of the most relevant is the \\(r\\)-near neighbor (\\(r\\)-NN)\nproblem: given a radius \\(r&gt;0\\) and a set of points \\(S\\), construct a data\nstructure that, for any given query point \\(q\\), returns a point \\(p\\) within\ndistance at most \\(r\\) from \\(q\\). In this paper, we study the \\(r\\)-NN problem in\nthe light of fairness. We consider fairness in the sense of equal opportunity:\nall points that are within distance \\(r\\) from the query should have the same\nprobability to be returned. In the low-dimensional case, this problem was first\nstudied by Hu, Qiao, and Tao (PODS 2014). Locality sensitive hashing (LSH), the\ntheoretically strongest approach to similarity search in high dimensions, does\nnot provide such a fairness guarantee. To address this, we propose efficient\ndata structures for \\(r\\)-NN where all points in \\(S\\) that are near \\(q\\) have the\nsame probability to be selected and returned by the query. Specifically, we\nfirst propose a black-box approach that, given any LSH scheme, constructs a\ndata structure for uniformly sampling points in the neighborhood of a query.\nThen, we develop a data structure for fair similarity search under inner\nproduct that requires nearly-linear space and exploits locality sensitive\nfilters. The paper concludes with an experimental evaluation that highlights\n(un)fairness in a recommendation setting on real-world datasets and discusses\nthe inherent unfairness introduced by solving other variants of the problem.</p>\n", "tags": ["Hashing Methods", "ANN Search", "Applications", "Evaluation Metrics"], "tsne_embedding": [-9.359074592590332, -5.814606666564941], "cluster": 7}, {"key": "aum\u00fcller2019parameterless", "year": "2019", "citations": "4", "title": "PUFFINN: Parameterless And Universally Fast Finding Of Nearest Neighbors", "abstract": "<p>We present PUFFINN, a parameterless LSH-based index for solving the\n\\(k\\)-nearest neighbor problem with probabilistic guarantees. By parameterless we\nmean that the user is only required to specify the amount of memory the index\nis supposed to use and the result quality that should be achieved. The index\ncombines several heuristic ideas known in the literature. By small adaptions to\nthe query algorithm, we make heuristics rigorous. We perform experiments on\nreal-world and synthetic inputs to evaluate implementation choices and show\nthat the implementation satisfies the quality guarantees while being\ncompetitive with other state-of-the-art approaches to nearest neighbor search.\n  We describe a novel synthetic data set that is difficult to solve for almost\nall existing nearest neighbor search approaches, and for which PUFFINN\nsignificantly outperform previous methods.</p>\n", "tags": ["ANN Search"], "tsne_embedding": [-8.009638786315918, 2.1690757274627686], "cluster": 4}, {"key": "aum\u00fcller2021sampling", "year": "2021", "citations": "4", "title": "Sampling A Near Neighbor In High Dimensions -- Who Is The Fairest Of Them All?", "abstract": "<p>Similarity search is a fundamental algorithmic primitive, widely used in many\ncomputer science disciplines. Given a set of points \\(S\\) and a radius parameter\n\\(r&gt;0\\), the \\(r\\)-near neighbor (\\(r\\)-NN) problem asks for a data structure that,\ngiven any query point \\(q\\), returns a point \\(p\\) within distance at most \\(r\\) from\n\\(q\\). In this paper, we study the \\(r\\)-NN problem in the light of individual\nfairness and providing equal opportunities: all points that are within distance\n\\(r\\) from the query should have the same probability to be returned. In the\nlow-dimensional case, this problem was first studied by Hu, Qiao, and Tao (PODS\n2014). Locality sensitive hashing (LSH), the theoretically strongest approach\nto similarity search in high dimensions, does not provide such a fairness\nguarantee. In this work, we show that LSH based algorithms can be made fair,\nwithout a significant loss in efficiency. We propose several efficient data\nstructures for the exact and approximate variants of the fair NN problem. Our\napproach works more generally for sampling uniformly from a sub-collection of\nsets of a given collection and can be used in a few other applications. We also\ndevelop a data structure for fair similarity search under inner product that\nrequires nearly-linear space and exploits locality sensitive filters. The paper\nconcludes with an experimental evaluation that highlights the inherent\nunfairness of NN data structures and shows the performance of our algorithms on\nreal-world datasets.</p>\n", "tags": ["Hashing Methods", "Applications", "ANN Search"], "tsne_embedding": [-9.433462142944336, -5.681700229644775], "cluster": 7}, {"key": "auvolat2015clustering", "year": "2015", "citations": "29", "title": "Clustering Is Efficient For Approximate Maximum Inner Product Search", "abstract": "<p>Efficient Maximum Inner Product Search (MIPS) is an important task that has a\nwide applicability in recommendation systems and classification with a large\nnumber of classes. Solutions based on locality-sensitive hashing (LSH) as well\nas tree-based solutions have been investigated in the recent literature, to\nperform approximate MIPS in sublinear time. In this paper, we compare these to\nanother extremely simple approach for solving approximate MIPS, based on\nvariants of the k-means clustering algorithm. Specifically, we propose to train\na spherical k-means, after having reduced the MIPS problem to a Maximum Cosine\nSimilarity Search (MCSS). Experiments on two standard recommendation system\nbenchmarks as well as on large vocabulary word embeddings, show that this\nsimple approach yields much higher speedups, for the same retrieval precision,\nthan current state-of-the-art hashing-based and tree-based methods. This simple\nmethod also yields more robust retrievals when the query is corrupted by noise.</p>\n", "tags": ["Efficient Learning", "Hashing Methods", "Evaluation Metrics", "Applications"], "tsne_embedding": [-7.8096208572387695, -0.26263195276260376], "cluster": 7}, {"key": "babenko2014improving", "year": "2014", "citations": "19", "title": "Improving Bilayer Product Quantization For Billion-scale Approximate Nearest Neighbors In High Dimensions", "abstract": "<p>The top-performing systems for billion-scale high-dimensional approximate\nnearest neighbor (ANN) search are all based on two-layer architectures that\ninclude an indexing structure and a compressed datapoints layer. An indexing\nstructure is crucial as it allows to avoid exhaustive search, while the lossy\ndata compression is needed to fit the dataset into RAM. Several of the most\nsuccessful systems use product quantization (PQ) for both the indexing and the\ndataset compression layers. These systems are however limited in the way they\nexploit the interaction of product quantization processes that happen at\ndifferent stages of these systems.\n  Here we introduce and evaluate two approximate nearest neighbor search\nsystems that both exploit the synergy of product quantization processes in a\nmore efficient way. The first system, called Fast Bilayer Product Quantization\n(FBPQ), speeds up the runtime of the baseline system (Multi-D-ADC) by several\ntimes, while achieving the same accuracy. The second system, Hierarchical\nBilayer Product Quantization (HBPQ) provides a significantly better recall for\nthe same runtime at a cost of small memory footprint increase. For the BIGANN\ndataset of billion SIFT descriptors, the 10% increase in Recall@1 and the 17%\nincrease in Recall@10 is observed.</p>\n", "tags": ["Quantization", "ANN Search", "Efficiency and Optimization"], "tsne_embedding": [-1.9571141004562378, -10.453516960144043], "cluster": 6}, {"key": "babenko2014neural", "year": "2014", "citations": "618", "title": "Neural Codes For Image Retrieval", "abstract": "<p>It has been shown that the activations invoked by an image within the top\nlayers of a large convolutional neural network provide a high-level descriptor\nof the visual content of the image. In this paper, we investigate the use of\nsuch descriptors (neural codes) within the image retrieval application. In the\nexperiments with several standard retrieval benchmarks, we establish that\nneural codes perform competitively even when the convolutional neural network\nhas been trained for an unrelated classification task (e.g.\\ Image-Net). We\nalso evaluate the improvement in the retrieval performance of neural codes,\nwhen the network is retrained on a dataset of images that are similar to images\nencountered at test time.\n  We further evaluate the performance of the compressed neural codes and show\nthat a simple PCA compression provides very good short codes that give\nstate-of-the-art accuracy on a number of datasets. In general, neural codes\nturn out to be much more resilient to such compression in comparison other\nstate-of-the-art descriptors. Finally, we show that discriminative\ndimensionality reduction trained on a dataset of pairs of matched photographs\nimproves the performance of PCA-compressed neural codes even further. Overall,\nour quantitative experiments demonstrate the promise of neural codes as visual\ndescriptors for image retrieval.</p>\n", "tags": ["Applications", "Evaluation Metrics", "Quantization"], "tsne_embedding": [0.8285097479820251, -3.946173906326294], "cluster": 9}, {"key": "babenko2015aggregating", "year": "2015", "citations": "235", "title": "Aggregating Deep Convolutional Features For Image Retrieval", "abstract": "<p>Several recent works have shown that image descriptors produced by deep\nconvolutional neural networks provide state-of-the-art performance for image\nclassification and retrieval problems. It has also been shown that the\nactivations from the convolutional layers can be interpreted as local features\ndescribing particular image regions. These local features can be aggregated\nusing aggregation approaches developed for local features (e.g. Fisher\nvectors), thus providing new powerful global descriptors.\n  In this paper we investigate possible ways to aggregate local deep features\nto produce compact global descriptors for image retrieval. First, we show that\ndeep features and traditional hand-engineered features have quite different\ndistributions of pairwise similarities, hence existing aggregation methods have\nto be carefully re-evaluated. Such re-evaluation reveals that in contrast to\nshallow features, the simple aggregation method based on sum pooling provides\narguably the best performance for deep convolutional features. This method is\nefficient, has few parameters, and bears little risk of overfitting when e.g.\nlearning the PCA matrix. Overall, the new compact global descriptor improves\nthe state-of-the-art on four common benchmarks considerably.</p>\n", "tags": ["Applications", "Evaluation Metrics"], "tsne_embedding": [3.945554256439209, -2.208012342453003], "cluster": 9}, {"key": "bai2020targeted", "year": "2020", "citations": "55", "title": "Targeted Attack For Deep Hashing Based Retrieval", "abstract": "<p>The deep hashing based retrieval method is widely adopted in large-scale\nimage and video retrieval. However, there is little investigation on its\nsecurity. In this paper, we propose a novel method, dubbed deep hashing\ntargeted attack (DHTA), to study the targeted attack on such retrieval.\nSpecifically, we first formulate the targeted attack as a point-to-set\noptimization, which minimizes the average distance between the hash code of an\nadversarial example and those of a set of objects with the target label. Then\nwe design a novel component-voting scheme to obtain an anchor code as the\nrepresentative of the set of hash codes of objects with the target label, whose\noptimality guarantee is also theoretically derived. To balance the performance\nand perceptibility, we propose to minimize the Hamming distance between the\nhash code of the adversarial example and the anchor code under the\n\\(\\ell^\\infty\\) restriction on the perturbation. Extensive experiments verify\nthat DHTA is effective in attacking both deep hashing based image retrieval and\nvideo retrieval.</p>\n", "tags": ["Applications", "Hashing Methods", "Deep Hashing", "Evaluation Metrics", "Privacy and Security"], "tsne_embedding": [1.754815697669983, 2.8452019691467285], "cluster": 9}, {"key": "bakhtiary2015speeding", "year": "2015", "citations": "2", "title": "Speeding Up Neural Networks For Large Scale Classification Using WTA Hashing", "abstract": "<p>In this paper we propose to use the Winner Takes All hashing technique to\nspeed up forward propagation and backward propagation in fully connected layers\nin convolutional neural networks. The proposed technique reduces significantly\nthe computational complexity, which in turn, allows us to train layers with a\nlarge number of kernels with out the associated time penalty.\n  As a consequence we are able to train convolutional neural network on a very\nlarge number of output classes with only a small increase in the computational\ncost. To show the effectiveness of the technique we train a new output layer on\na pretrained network using both the regular multiplicative approach and our\nproposed hashing methodology. Our results showed no drop in performance and\ndemonstrate, with our implementation, a 7 fold speed up during the training.</p>\n", "tags": ["Hashing Methods", "Deep Hashing", "Efficient Learning"], "tsne_embedding": [11.833117485046387, 9.82823371887207], "cluster": 3}, {"key": "ballard2015diamond", "year": "2015", "citations": "16", "title": "Diamond Sampling For Approximate Maximum All-pairs Dot-product (MAD) Search", "abstract": "<p>Given two sets of vectors, \\(A = \\{{a_1}, \\dots, {a_m}\\}\\) and\n\\(B=\\{{b_1},\\dots,{b_n}\\}\\), our problem is to find the top-\\(t\\) dot products,\ni.e., the largest \\(|{a_i}\\cdot{b_j}|\\) among all possible pairs. This is a\nfundamental mathematical problem that appears in numerous data applications\ninvolving similarity search, link prediction, and collaborative filtering. We\npropose a sampling-based approach that avoids direct computation of all \\(mn\\)\ndot products. We select diamonds (i.e., four-cycles) from the weighted\ntripartite representation of \\(A\\) and \\(B\\). The probability of selecting a\ndiamond corresponding to pair \\((i,j)\\) is proportional to \\(({a_i}\\cdot{b_j})^2\\),\namplifying the focus on the largest-magnitude entries. Experimental results\nindicate that diamond sampling is orders of magnitude faster than direct\ncomputation and requires far fewer samples than any competing approach. We also\napply diamond sampling to the special case of maximum inner product search, and\nget significantly better results than the state-of-the-art hashing methods.</p>\n", "tags": ["Applications", "Hashing Methods", "ANN Search"], "tsne_embedding": [-16.793865203857422, 2.306422233581543], "cluster": 4}, {"key": "bansal2016extraction", "year": "2016", "citations": "0", "title": "Extraction Of Layout Entities And Sub-layout Query-based Retrieval Of Document Images", "abstract": "<p>Layouts and sub-layouts constitute an important clue while searching a\ndocument on the basis of its structure, or when textual content is\nunknown/irrelevant. A sub-layout specifies the arrangement of document entities\nwithin a smaller portion of the document. We propose an efficient graph-based\nmatching algorithm, integrated with hash-based indexing, to prune a possibly\nlarge search space. A user can specify a combination of sub-layouts of interest\nusing sketch-based queries. The system supports partial matching for\nunspecified layout entities. We handle cases of segmentation pre-processing\nerrors (for text/non-text blocks) with a symmetry maximization-based strategy,\nand accounting for multiple domain-specific plausible segmentation hypotheses.\nWe show promising results of our system on a database of unstructured entities,\ncontaining 4776 newspaper images.</p>\n", "tags": ["Indexing"], "tsne_embedding": [8.420522689819336, -6.384868621826172], "cluster": 1}, {"key": "baranchuk2018revisiting", "year": "2018", "citations": "47", "title": "Revisiting The Inverted Indices For Billion-scale Approximate Nearest Neighbors", "abstract": "<p>This work addresses the problem of billion-scale nearest neighbor search. The\nstate-of-the-art retrieval systems for billion-scale databases are currently\nbased on the inverted multi-index, the recently proposed generalization of the\ninverted index structure. The multi-index provides a very fine-grained\npartition of the feature space that allows extracting concise and accurate\nshort-lists of candidates for the search queries. In this paper, we argue that\nthe potential of the simple inverted index was not fully exploited in previous\nworks and advocate its usage both for the highly-entangled deep descriptors and\nrelatively disentangled SIFT descriptors. We introduce a new retrieval system\nthat is based on the inverted index and outperforms the multi-index by a large\nmargin for the same memory consumption and construction complexity. For\nexample, our system achieves the state-of-the-art recall rates several times\nfaster on the dataset of one billion deep descriptors compared to the efficient\nimplementation of the inverted multi-index from the FAISS library.</p>\n", "tags": ["Indexing", "ANN Search", "Evaluation Metrics"], "tsne_embedding": [-0.6059495806694031, -12.097373008728027], "cluster": 6}, {"key": "baranchuk2019learning", "year": "2019", "citations": "8", "title": "Learning To Route In Similarity Graphs", "abstract": "<p>Recently similarity graphs became the leading paradigm for efficient nearest\nneighbor search, outperforming traditional tree-based and LSH-based methods.\nSimilarity graphs perform the search via greedy routing: a query traverses the\ngraph and in each vertex moves to the adjacent vertex that is the closest to\nthis query. In practice, similarity graphs are often susceptible to local\nminima, when queries do not reach its nearest neighbors, getting stuck in\nsuboptimal vertices. In this paper we propose to learn the routing function\nthat overcomes local minima via incorporating information about the graph\nglobal structure. In particular, we augment the vertices of a given graph with\nadditional representations that are learned to provide the optimal routing from\nthe start vertex to the query nearest neighbor. By thorough experiments, we\ndemonstrate that the proposed learnable routing successfully diminishes the\nlocal minima problem and significantly improves the overall search performance.</p>\n", "tags": ["ANN Search", "Efficient Learning"], "tsne_embedding": [3.845869541168213, -10.2304105758667], "cluster": 1}, {"key": "baranchuk2023dedrift", "year": "2023", "citations": "2", "title": "Dedrift Robust Similarity Search Under Content Drift", "abstract": "<p>The statistical distribution of content uploaded and searched on media\nsharing sites changes over time due to seasonal, sociological and technical\nfactors. We investigate the impact of this \u201ccontent drift\u201d for large-scale\nsimilarity search tools, based on nearest neighbor search in embedding space.\nUnless a costly index reconstruction is performed frequently, content drift\ndegrades the search accuracy and efficiency. The degradation is especially\nsevere since, in general, both the query and database distributions change.\n  We introduce and analyze real-world image and video datasets for which\ntemporal information is available over a long time period. Based on the\nlearnings, we devise DeDrift, a method that updates embedding quantizers to\ncontinuously adapt large-scale indexing structures on-the-fly. DeDrift almost\neliminates the accuracy degradation due to the query and database content drift\nwhile being up to 100x faster than a full index reconstruction.</p>\n", "tags": ["ANN Search", "Quantization", "Indexing", "Efficient Learning"], "tsne_embedding": [13.605432510375977, 5.253762245178223], "cluster": 3}, {"key": "baranchuk2023robust", "year": "2023", "citations": "2", "title": "Dedrift: Robust Similarity Search Under Content Drift", "abstract": "<p>The statistical distribution of content uploaded and searched on media\nsharing sites changes over time due to seasonal, sociological and technical\nfactors. We investigate the impact of this \u201ccontent drift\u201d for large-scale\nsimilarity search tools, based on nearest neighbor search in embedding space.\nUnless a costly index reconstruction is performed frequently, content drift\ndegrades the search accuracy and efficiency. The degradation is especially\nsevere since, in general, both the query and database distributions change.\n  We introduce and analyze real-world image and video datasets for which\ntemporal information is available over a long time period. Based on the\nlearnings, we devise DeDrift, a method that updates embedding quantizers to\ncontinuously adapt large-scale indexing structures on-the-fly. DeDrift almost\neliminates the accuracy degradation due to the query and database content drift\nwhile being up to 100x faster than a full index reconstruction.</p>\n", "tags": ["Indexing", "ANN Search", "Quantization"], "tsne_embedding": [13.605488777160645, 5.253612995147705], "cluster": 3}, {"key": "bawa2024lsh", "year": "2024", "citations": "0", "title": "LSH Forest Self-tuning Indexes For Similarity Search", "abstract": "<p>We consider the problem of indexing high-dimensional data for answering (approximate) similarity-search queries. Similarity indexes prove to be important in a wide variety of settings: Web search\nengines desire fast, parallel, main-memory-based indexes for similarity search on text data; database systems desire disk-based similarity indexes for high-dimensional data, including text and images;\npeer-to-peer systems desire distributed similarity indexes with low\ncommunication cost. We propose an indexing scheme called LSH\nForest which is applicable in all the above contexts. Our index uses the well-known technique of locality-sensitive hashing (LSH),\nbut improves upon previous designs by (a) eliminating the different data-dependent parameters for which LSH must be constantly hand-tuned, and (b) improving on LSH\u2019s performance guarantees for skewed data distributions while retaining the same storage\nand query overhead. We show how to construct this index in main\nmemory, on disk, in parallel systems, and in peer-to-peer systems.\nWe evaluate the design with experiments on multiple text corpora\nand demonstrate both the self-tuning nature and the superior performance of LSH Forest.</p>\n", "tags": ["Hashing Methods", "ANN Search", "Indexing", "Efficient Learning"], "tsne_embedding": [5.058077812194824, -5.4819254875183105], "cluster": 1}, {"key": "beame2016massively", "year": "2016", "citations": "1", "title": "Massively-parallel Similarity Join, Edge-isoperimetry, And Distance Correlations On The Hypercube", "abstract": "<p>We study distributed protocols for finding all pairs of similar vectors in a\nlarge dataset. Our results pertain to a variety of discrete metrics, and we\ngive concrete instantiations for Hamming distance. In particular, we give\nimproved upper bounds on the overhead required for similarity defined by\nHamming distance \\(r&gt;1\\) and prove a lower bound showing qualitative optimality\nof the overhead required for similarity over any Hamming distance \\(r\\). Our main\nconceptual contribution is a connection between similarity search algorithms\nand certain graph-theoretic quantities. For our upper bounds, we exhibit a\ngeneral method for designing one-round protocols using edge-isoperimetric\nshapes in similarity graphs. For our lower bounds, we define a new\ncombinatorial optimization problem, which can be stated in purely\ngraph-theoretic terms yet also captures the core of the analysis in previous\ntheoretical work on distributed similarity joins. As one of our main technical\nresults, we prove new bounds on distance correlations in subsets of the Hamming\ncube.</p>\n", "tags": ["Evaluation Metrics"], "tsne_embedding": [4.881824493408203, -10.102767944335938], "cluster": 1}, {"key": "bera2021node", "year": "2021", "citations": "4", "title": "QUINT: Node Embedding Using Network Hashing", "abstract": "<p>Representation learning using network embedding has received tremendous\nattention due to its efficacy to solve downstream tasks. Popular embedding\nmethods (such as deepwalk, node2vec, LINE) are based on a neural architecture,\nthus unable to scale on large networks both in terms of time and space usage.\nRecently, we proposed BinSketch, a sketching technique for compressing binary\nvectors to binary vectors. In this paper, we show how to extend BinSketch and\nuse it for network hashing. Our proposal named QUINT is built upon BinSketch,\nand it embeds nodes of a sparse network onto a low-dimensional space using\nsimple bi-wise operations. QUINT is the first of its kind that provides\ntremendous gain in terms of speed and space usage without compromising much on\nthe accuracy of the downstream tasks. Extensive experiments are conducted to\ncompare QUINT with seven state-of-the-art network embedding methods for two end\ntasks - link prediction and node classification. We observe huge performance\ngain for QUINT in terms of speedup (up to 7000x) and space saving (up to 80x)\ndue to its bit-wise nature to obtain node embedding. Moreover, QUINT is a\nconsistent top-performer for both the tasks among the baselines across all the\ndatasets. Our empirical observations are backed by rigorous theoretical\nanalysis to justify the effectiveness of QUINT. In particular, we prove that\nQUINT retains enough structural information which can be used further to\napproximate many topological properties of networks with high confidence.</p>\n", "tags": ["Hashing Methods", "Efficient Learning"], "tsne_embedding": [1.6506298780441284, -7.10964298248291], "cluster": 1}, {"key": "berman2018supermodular", "year": "2018", "citations": "1", "title": "Supermodular Locality Sensitive Hashes", "abstract": "<p>In this work, we show deep connections between Locality Sensitive Hashability\nand submodular analysis. We show that the LSHablility of the most commonly\nanalyzed set similarities is in one-to-one correspondance with the\nsupermodularity of these similarities when taken with respect to the symmetric\ndifference of their arguments. We find that the supermodularity of equivalent\nLSHable similarities can be dependent on the set encoding. While monotonicity\nand supermodularity does not imply the metric condition necessary for\nsupermodularity, this condition is guaranteed for the more restricted class of\nsupermodular Hamming similarities that we introduce. We show moreover that LSH\npreserving transformations are also supermodular-preserving, yielding a way to\ngenerate families of similarities both LSHable and supermodular. Finally, we\nshow that even the more restricted family of cardinality-based supermodular\nHamming similarities presents promising aspects for the study of the link\nbetween LSHability and supermodularity. We hope that the several bridges that\nwe introduce between LSHability and supermodularity paves the way to a better\nunderstanding both of supermodular analysis and LSHability, notably in the\ncontext of large-scale supermodular optimization.</p>\n", "tags": ["Hashing Methods", "ANN Search", "Efficient Learning"], "tsne_embedding": [-9.223807334899902, -3.0474252700805664], "cluster": 7}, {"key": "berriche2024leveraging", "year": "2024", "citations": "1", "title": "Leveraging High-resolution Features For Improved Deep Hashing-based Image Retrieval", "abstract": "<p>Deep hashing techniques have emerged as the predominant approach for\nefficient image retrieval. Traditionally, these methods utilize pre-trained\nconvolutional neural networks (CNNs) such as AlexNet and VGG-16 as feature\nextractors. However, the increasing complexity of datasets poses challenges for\nthese backbone architectures in capturing meaningful features essential for\neffective image retrieval. In this study, we explore the efficacy of employing\nhigh-resolution features learned through state-of-the-art techniques for image\nretrieval tasks. Specifically, we propose a novel methodology that utilizes\nHigh-Resolution Networks (HRNets) as the backbone for the deep hashing task,\ntermed High-Resolution Hashing Network (HHNet). Our approach demonstrates\nsuperior performance compared to existing methods across all tested benchmark\ndatasets, including CIFAR-10, NUS-WIDE, MS COCO, and ImageNet. This performance\nimprovement is more pronounced for complex datasets, which highlights the need\nto learn high-resolution features for intricate image retrieval tasks.\nFurthermore, we conduct a comprehensive analysis of different HRNet\nconfigurations and provide insights into the optimal architecture for the deep\nhashing task</p>\n", "tags": ["Applications", "Deep Hashing", "Evaluation Metrics", "Benchmarks and Datasets", "Hashing Methods"], "tsne_embedding": [9.778611183166504, 3.203315019607544], "cluster": 3}, {"key": "berton2025all", "year": "2025", "citations": "0", "title": "All You Need To Know About Training Image Retrieval Models", "abstract": "<p>Image retrieval is the task of finding images in a database that are most\nsimilar to a given query image. The performance of an image retrieval pipeline\ndepends on many training-time factors, including the embedding model\narchitecture, loss function, data sampler, mining function, learning rate(s),\nand batch size. In this work, we run tens of thousands of training runs to\nunderstand the effect each of these factors has on retrieval accuracy. We also\ndiscover best practices that hold across multiple datasets. The code is\navailable at https://github.com/gmberton/image-retrieval</p>\n", "tags": ["Applications", "Evaluation Metrics", "Tools and Libraries"], "tsne_embedding": [11.32431697845459, -2.963054656982422], "cluster": 8}, {"key": "bessa2023weighted", "year": "2023", "citations": "2", "title": "Weighted Minwise Hashing Beats Linear Sketching For Inner Product Estimation", "abstract": "<p>We present a new approach for computing compact sketches that can be used to\napproximate the inner product between pairs of high-dimensional vectors. Based\non the Weighted MinHash algorithm, our approach admits strong accuracy\nguarantees that improve on the guarantees of popular linear sketching\napproaches for inner product estimation, such as CountSketch and\nJohnson-Lindenstrauss projection. Specifically, while our method admits\nguarantees that exactly match linear sketching for dense vectors, it yields\nsignificantly lower error for sparse vectors with limited overlap between\nnon-zero entries. Such vectors arise in many applications involving sparse\ndata. They are also important in increasingly popular dataset search\napplications, where inner product sketches are used to estimate data\ncovariance, conditional means, and other quantities involving columns in\nunjoined tables. We complement our theoretical results by showing that our\napproach empirically outperforms existing linear sketches and unweighted\nhashing-based sketches for sparse vectors.</p>\n", "tags": ["Hashing Methods", "Applications"], "tsne_embedding": [-9.334110260009766, -1.6216901540756226], "cluster": 7}, {"key": "bhatnagar2024piecewise", "year": "2024", "citations": "0", "title": "Piecewise-linear Manifolds For Deep Metric Learning", "abstract": "<p>Unsupervised deep metric learning (UDML) focuses on learning a semantic\nrepresentation space using only unlabeled data. This challenging problem\nrequires accurately estimating the similarity between data points, which is\nused to supervise a deep network. For this purpose, we propose to model the\nhigh-dimensional data manifold using a piecewise-linear approximation, with\neach low-dimensional linear piece approximating the data manifold in a small\nneighborhood of a point. These neighborhoods are used to estimate similarity\nbetween data points. We empirically show that this similarity estimate\ncorrelates better with the ground truth than the similarity estimates of\ncurrent state-of-the-art techniques. We also show that proxies, commonly used\nin supervised metric learning, can be used to model the piecewise-linear\nmanifold in an unsupervised setting, helping improve performance. Our method\noutperforms existing unsupervised metric learning approaches on standard\nzero-shot image retrieval benchmarks.</p>\n", "tags": ["Applications", "Unsupervised", "Evaluation Metrics", "Supervised"], "tsne_embedding": [3.2668585777282715, 0.4082289934158325], "cluster": 9}, {"key": "bhunia2018texture", "year": "2018", "citations": "12", "title": "Texture Synthesis Guided Deep Hashing For Texture Image Retrieval", "abstract": "<p>With the large-scale explosion of images and videos over the internet,\nefficient hashing methods have been developed to facilitate memory and time\nefficient retrieval of similar images. However, none of the existing works uses\nhashing to address texture image retrieval mostly because of the lack of\nsufficiently large texture image databases. Our work addresses this problem by\ndeveloping a novel deep learning architecture that generates binary hash codes\nfor input texture images. For this, we first pre-train a Texture Synthesis\nNetwork (TSN) which takes a texture patch as input and outputs an enlarged view\nof the texture by injecting newer texture content. Thus it signifies that the\nTSN encodes the learnt texture specific information in its intermediate layers.\nIn the next stage, a second network gathers the multi-scale feature\nrepresentations from the TSN\u2019s intermediate layers using channel-wise\nattention, combines them in a progressive manner to a dense continuous\nrepresentation which is finally converted into a binary hash code with the help\nof individual and pairwise label information. The new enlarged texture patches\nalso help in data augmentation to alleviate the problem of insufficient texture\ndata and are used to train the second stage of the network. Experiments on\nthree public texture image retrieval datasets indicate the superiority of our\ntexture synthesis guided hashing approach over current state-of-the-art\nmethods.</p>\n", "tags": ["Applications", "Deep Hashing", "ANN Search", "Hashing Methods", "Benchmarks and Datasets"], "tsne_embedding": [0.9288680553436279, 7.405342102050781], "cluster": 5}, {"key": "billings2018gradient", "year": "2018", "citations": "0", "title": "Gradient Augmented Information Retrieval With Autoencoders And Semantic Hashing", "abstract": "<p>This paper will explore the use of autoencoders for semantic hashing in the\ncontext of Information Retrieval. This paper will summarize how to efficiently\ntrain an autoencoder in order to create meaningful and low-dimensional\nencodings of data. This paper will demonstrate how computing and storing the\nclosest encodings to an input query can help speed up search time and improve\nthe quality of our search results. The novel contributions of this paper\ninvolve using the representation of the data learned by an auto-encoder in\norder to augment our search query in various ways. I present and evaluate the\nnew gradient search augmentation (GSA) approach, as well as the more well-known\npseudo-relevance-feedback (PRF) adjustment. I find that GSA helps to improve\nthe performance of the TF-IDF based information retrieval system, and PRF\ncombined with GSA works best overall for the systems compared in this paper.</p>\n", "tags": ["Hashing Methods", "Deep Hashing", "ANN Search", "Evaluation Metrics"], "tsne_embedding": [7.129510879516602, 8.657147407531738], "cluster": 3}, {"key": "biswas2020perceptual", "year": "2020", "citations": "0", "title": "Perceptual Hashing Applied To Tor Domains Recognition", "abstract": "<p>The Tor darknet hosts different types of illegal content, which are monitored\nby cybersecurity agencies. However, manually classifying Tor content can be\nslow and error-prone. To support this task, we introduce Frequency-Dominant\nNeighborhood Structure (F-DNS), a new perceptual hashing method for\nautomatically classifying domains by their screenshots. First, we evaluated\nF-DNS using images subject to various content preserving operations. We\ncompared them with their original images, achieving better correlation\ncoefficients than other state-of-the-art methods, especially in the case of\nrotation. Then, we applied F-DNS to categorize Tor domains using the Darknet\nUsage Service Images-2K (DUSI-2K), a dataset with screenshots of active Tor\nservice domains. Finally, we measured the performance of F-DNS against an image\nclassification approach and a state-of-the-art hashing method. Our proposal\nobtained 98.75% accuracy in Tor images, surpassing all other methods compared.</p>\n", "tags": ["Hashing Methods", "Applications", "Evaluation Metrics"], "tsne_embedding": [-2.3477416038513184, 10.419260025024414], "cluster": 5}, {"key": "biswas2021state", "year": "2021", "citations": "4", "title": "State Of The Art: Image Hashing", "abstract": "<p>Perceptual image hashing methods are often applied in various objectives,\nsuch as image retrieval, finding duplicate or near-duplicate images, and\nfinding similar images from large-scale image content. The main challenge in\nimage hashing techniques is robust feature extraction, which generates the same\nor similar hashes in images that are visually identical. In this article, we\npresent a short review of the state-of-the-art traditional perceptual hashing\nand deep learning-based perceptual hashing methods, identifying the best\napproaches.</p>\n", "tags": ["Applications", "Hashing Methods", "Survey Paper"], "tsne_embedding": [11.47673225402832, -1.994760274887085], "cluster": 8}, {"key": "black2021compositional", "year": "2021", "citations": "1", "title": "Compositional Sketch Search", "abstract": "<p>We present an algorithm for searching image collections using free-hand\nsketches that describe the appearance and relative positions of multiple\nobjects. Sketch based image retrieval (SBIR) methods predominantly match\nqueries containing a single, dominant object invariant to its position within\nan image. Our work exploits drawings as a concise and intuitive representation\nfor specifying entire scene compositions. We train a convolutional neural\nnetwork (CNN) to encode masked visual features from sketched objects, pooling\nthese into a spatial descriptor encoding the spatial relationships and\nappearances of objects in the composition. Training the CNN backbone as a\nSiamese network under triplet loss yields a metric search embedding for\nmeasuring compositional similarity which may be efficiently leveraged for\nvisual search by applying product quantization.</p>\n", "tags": ["Applications", "Quantization", "Loss Functions", "Deep Hashing"], "tsne_embedding": [9.168450355529785, -4.176178932189941], "cluster": 8}, {"key": "blumenstiel2024multi", "year": "2024", "citations": "0", "title": "Multi-spectral Remote Sensing Image Retrieval Using Geospatial Foundation Models", "abstract": "<p>Image retrieval enables an efficient search through vast amounts of satellite\nimagery and returns similar images to a query. Deep learning models can\nidentify images across various semantic concepts without the need for\nannotations. This work proposes to use Geospatial Foundation Models, like\nPrithvi, for remote sensing image retrieval with multiple benefits: i) the\nmodels encode multi-spectral satellite data and ii) generalize without further\nfine-tuning. We introduce two datasets to the retrieval task and observe a\nstrong performance: Prithvi processes six bands and achieves a mean Average\nPrecision of 97.62% on BigEarthNet-43 and 44.51% on ForestNet-12, outperforming\nother RGB-based models. Further, we evaluate three compression methods with\nbinarized embeddings balancing retrieval speed and accuracy. They match the\nretrieval speed of much shorter hash codes while maintaining the same accuracy\nas floating-point embeddings but with a 32-fold compression. The code is\navailable at https://github.com/IBM/remote-sensing-image-retrieval.</p>\n", "tags": ["Applications", "ANN Search", "Efficient Learning", "Evaluation Metrics", "Hashing Methods"], "tsne_embedding": [8.648646354675293, -0.6377372741699219], "cluster": 8}, {"key": "bondugula2015shoe", "year": "2015", "citations": "0", "title": "SHOE Supervised Hashing With Output Embeddings", "abstract": "<p>We present a supervised binary encoding scheme for image retrieval that\nlearns projections by taking into account similarity between classes obtained\nfrom output embeddings. Our motivation is that binary hash codes learned in\nthis way improve both the visual quality of retrieval results and existing\nsupervised hashing schemes. We employ a sequential greedy optimization that\nlearns relationship aware projections by minimizing the difference between\ninner products of binary codes and output embedding vectors. We develop a joint\noptimization framework to learn projections which improve the accuracy of\nsupervised hashing over the current state of the art with respect to standard\nand sibling evaluation metrics. We further boost performance by applying the\nsupervised dimensionality reduction technique on kernelized input CNN features.\nExperiments are performed on three datasets: CUB-2011, SUN-Attribute and\nImageNet ILSVRC 2010. As a by-product of our method, we show that using a\nsimple k-nn pooling classifier with our discriminative codes improves over the\ncomplex classification models on fine grained datasets like CUB and offer an\nimpressive compression ratio of 1024 on CNN features.</p>\n", "tags": ["Deep Hashing", "Supervised", "Image Retrieval", "Loss Functions"], "tsne_embedding": [6.815265655517578, -2.307175397872925], "cluster": 8}, {"key": "bondugula2015supervised", "year": "2015", "citations": "0", "title": "SHOE: Supervised Hashing With Output Embeddings", "abstract": "<p>We present a supervised binary encoding scheme for image retrieval that\nlearns projections by taking into account similarity between classes obtained\nfrom output embeddings. Our motivation is that binary hash codes learned in\nthis way improve both the visual quality of retrieval results and existing\nsupervised hashing schemes. We employ a sequential greedy optimization that\nlearns relationship aware projections by minimizing the difference between\ninner products of binary codes and output embedding vectors. We develop a joint\noptimization framework to learn projections which improve the accuracy of\nsupervised hashing over the current state of the art with respect to standard\nand sibling evaluation metrics. We further boost performance by applying the\nsupervised dimensionality reduction technique on kernelized input CNN features.\nExperiments are performed on three datasets: CUB-2011, SUN-Attribute and\nImageNet ILSVRC 2010. As a by-product of our method, we show that using a\nsimple k-nn pooling classifier with our discriminative codes improves over the\ncomplex classification models on fine grained datasets like CUB and offer an\nimpressive compression ratio of 1024 on CNN features.</p>\n", "tags": ["Applications", "Efficient Learning", "Evaluation Metrics", "Supervised", "Benchmarks and Datasets", "Hashing Methods"], "tsne_embedding": [6.815480709075928, -2.307281732559204], "cluster": 8}, {"key": "borthwick2020scalable", "year": "2020", "citations": "1", "title": "Scalable Blocking For Very Large Databases", "abstract": "<p>In the field of database deduplication, the goal is to find approximately\nmatching records within a database. Blocking is a typical stage in this process\nthat involves cheaply finding candidate pairs of records that are potential\nmatches for further processing. We present here Hashed Dynamic Blocking, a new\napproach to blocking designed to address datasets larger than those studied in\nmost prior work. Hashed Dynamic Blocking (HDB) extends Dynamic Blocking, which\nleverages the insight that rare matching values and rare intersections of\nvalues are predictive of a matching relationship. We also present a novel use\nof Locality Sensitive Hashing (LSH) to build blocking key values for huge\ndatabases with a convenient configuration to control the trade-off between\nprecision and recall. HDB achieves massive scale by minimizing data movement,\nusing compact block representation, and greedily pruning ineffective candidate\nblocks using a Count-min Sketch approximate counting data structure. We\nbenchmark the algorithm by focusing on real-world datasets in excess of one\nmillion rows, demonstrating that the algorithm displays linear time complexity\nscaling in this range. Furthermore, we execute HDB on a 530 million row\nindustrial dataset, detecting 68 billion candidate pairs in less than three\nhours at a cost of $307 on a major cloud service.</p>\n", "tags": ["Hashing Methods", "Evaluation Metrics", "Tools and Libraries"], "tsne_embedding": [-3.0126142501831055, -8.092555046081543], "cluster": 6}, {"key": "boytsov2013learning", "year": "2013", "citations": "19", "title": "Learning To Prune In Metric And Non-metric Spaces", "abstract": "<p>Our focus is on approximate nearest neighbor retrieval in metric and non-metric spaces. We employ a VP-tree and explore two simple yet effective learning-to prune approaches: density estimation through sampling and \u201cstretching\u201d of the triangle inequality. Both methods are evaluated using data sets with metric (Euclidean) and non-metric (KL-divergence and Itakura-Saito) distance functions. Conditions on spaces where the VP-tree is applicable are discussed. The VP-tree with a learned pruner is compared against the recently proposed state-of-the-art approaches: the bbtree, the multi-probe locality sensitive hashing (LSH), and permutation methods. Our method was competitive against state-of-the-art methods and, in most cases, was more efficient for the same rank approximation quality.</p>\n", "tags": ["ANN Search", "LSH", "NEURIPS"], "tsne_embedding": [-7.366878032684326, 3.0224382877349854], "cluster": 4}, {"key": "breznik2022cross", "year": "2022", "citations": "2", "title": "Cross-modality Sub-image Retrieval Using Contrastive Multimodal Image Representations", "abstract": "<p>In tissue characterization and cancer diagnostics, multimodal imaging has\nemerged as a powerful technique. Thanks to computational advances, large\ndatasets can be exploited to discover patterns in pathologies and improve\ndiagnosis. However, this requires efficient and scalable image retrieval\nmethods. Cross-modality image retrieval is particularly challenging, since\nimages of similar (or even the same) content captured by different modalities\nmight share few common structures. We propose a new application-independent\ncontent-based image retrieval (CBIR) system for reverse (sub-)image search\nacross modalities, which combines deep learning to generate representations\n(embedding the different modalities in a common space) with classical feature\nextraction and bag-of-words models for efficient and reliable retrieval. We\nillustrate its advantages through a replacement study, exploring a number of\nfeature extractors and learned representations, as well as through comparison\nto recent (cross-modality) CBIR methods. For the task of (sub-)image retrieval\non a (publicly available) dataset of brightfield and second harmonic generation\nmicroscopy images, the results show that our approach is superior to all tested\nalternatives. We discuss the shortcomings of the compared methods and observe\nthe importance of equivariance and invariance properties of the learned\nrepresentations and feature extractors in the CBIR pipeline. Code is available\nat: \\url{https://github.com/MIDA-group/CrossModal_ImgRetrieval}.</p>\n", "tags": ["Applications", "Deep Hashing", "Tools and Libraries"], "tsne_embedding": [7.772650241851807, 1.262852668762207], "cluster": 8}, {"key": "bronstein2011kernel", "year": "2011", "citations": "1", "title": "Kernel Diff-hash", "abstract": "<p>This paper presents a kernel formulation of the recently introduced diff-hash\nalgorithm for the construction of similarity-sensitive hash functions. Our\nkernel diff-hash algorithm that shows superior performance on the problem of\nimage feature descriptor matching.</p>\n", "tags": ["Hashing Methods", "ANN Search", "Applications"], "tsne_embedding": [12.275296211242676, -1.277047872543335], "cluster": 8}, {"key": "bronstein2011multimodal", "year": "2011", "citations": "0", "title": "Multimodal Diff-hash", "abstract": "<p>Many applications require comparing multimodal data with different structure\nand dimensionality that cannot be compared directly. Recently, there has been\nincreasing interest in methods for learning and efficiently representing such\nmultimodal similarity. In this paper, we present a simple algorithm for\nmultimodal similarity-preserving hashing, trying to map multimodal data into\nthe Hamming space while preserving the intra- and inter-modal similarities. We\nshow that our method significantly outperforms the state-of-the-art method in\nthe field.</p>\n", "tags": ["Applications", "ANN Search", "Hashing Methods", "Multi-Modal Hashing"], "tsne_embedding": [5.80418586730957, 1.5486441850662231], "cluster": 9}, {"key": "brooks2017multi", "year": "2017", "citations": "1", "title": "Multi-level Spherical Locality Sensitive Hashing For Approximate Near Neighbors", "abstract": "<p>This paper introduces \u201cMulti-Level Spherical LSH\u201d: parameter-free, a\nmulti-level, data-dependant Locality Sensitive Hashing data structure for\nsolving the Approximate Near Neighbors Problem (ANN). This data structure uses\na modified version of a multi-probe adaptive querying algorithm, with the\npotential of achieving a \\(O(n^p + t)\\) query run time, for all inputs n where \\(t\n&lt;= n\\).</p>\n", "tags": ["ANN Search", "Hashing Methods"], "tsne_embedding": [-12.269940376281738, 0.5792602896690369], "cluster": 4}, {"key": "bury2016efficient", "year": "2016", "citations": "0", "title": "Efficient Similarity Search In Dynamic Data Streams", "abstract": "<p>The Jaccard index is an important similarity measure for item sets and\nBoolean data. On large datasets, an exact similarity computation is often\ninfeasible for all item pairs both due to time and space constraints, giving\nrise to faster approximate methods. The algorithm of choice used to quickly\ncompute the Jaccard index \\(\\frac{\\vert A \\cap B \\vert}{\\vert A\\cup B\\vert}\\) of\ntwo item sets \\(A\\) and \\(B\\) is usually a form of min-hashing. Most min-hashing\nschemes are maintainable in data streams processing only additions, but none\nare known to work when facing item-wise deletions. In this paper, we\ninvestigate scalable approximation algorithms for rational set similarities, a\nbroad class of similarity measures including Jaccard. Motivated by a result of\nChierichetti and Kumar [J. ACM 2015] who showed any rational set similarity \\(S\\)\nadmits a locality sensitive hashing (LSH) scheme if and only if the\ncorresponding distance \\(1-S\\) is a metric, we can show that there exists a space\nefficient summary maintaining a \\((1\\pm \\epsilon)\\) multiplicative\napproximation to \\(1-S\\) in dynamic data streams. This in turn also yields a\n\\(\\epsilon\\) additive approximation of the similarity. The existence of these\napproximations hints at, but does not directly imply a LSH scheme in dynamic\ndata streams. Our second and main contribution now lies in the design of such a\nLSH scheme maintainable in dynamic data streams. The scheme is space efficient,\neasy to implement and to the best of our knowledge the first of its kind able\nto process deletions.</p>\n", "tags": ["Hashing Methods", "ANN Search", "Efficient Learning"], "tsne_embedding": [-8.576360702514648, -4.186795711517334], "cluster": 7}, {"key": "busolin2024early", "year": "2024", "citations": "2", "title": "Early Exit Strategies For Approximate K-nn Search In Dense Retrieval", "abstract": "<p>Learned dense representations are a popular family of techniques for encoding\nqueries and documents using high-dimensional embeddings, which enable retrieval\nby performing approximate k nearest-neighbors search (A-kNN). A popular\ntechnique for making A-kNN search efficient is based on a two-level index,\nwhere the embeddings of documents are clustered offline and, at query\nprocessing, a fixed number N of clusters closest to the query is visited\nexhaustively to compute the result set. In this paper, we build upon\nstate-of-the-art for early exit A-kNN and propose an unsupervised method based\non the notion of patience, which can reach competitive effectiveness with large\nefficiency gains. Moreover, we discuss a cascade approach where we first\nidentify queries that find their nearest neighbor within the closest t \u00ab\u00a0N\nclusters, and then we decide how many more to visit based on our patience\napproach or other state-of-the-art strategies. Reproducible experiments\nemploying state-of-the-art dense retrieval models and publicly available\nresources show that our techniques improve the A-kNN efficiency with up to 5x\nspeedups while achieving negligible effectiveness losses. All the code used is\navailable at https://github.com/francescobusolin/faiss_pEE</p>\n", "tags": ["Efficient Learning", "Unsupervised", "Has Code"], "tsne_embedding": [-2.9307632446289062, -5.451927185058594], "cluster": 6}, {"key": "cai2016revisit", "year": "2016", "citations": "23", "title": "A Revisit Of Hashing Algorithms For Approximate Nearest Neighbor Search", "abstract": "<p>Approximate Nearest Neighbor Search (ANNS) is a fundamental problem in many\nareas of machine learning and data mining. During the past decade, numerous\nhashing algorithms are proposed to solve this problem. Every proposed algorithm\nclaims outperform other state-of-the-art hashing methods. However, the\nevaluation of these hashing papers was not thorough enough, and those claims\nshould be re-examined. The ultimate goal of an ANNS method is returning the\nmost accurate answers (nearest neighbors) in the shortest time. If implemented\ncorrectly, almost all the hashing methods will have their performance improved\nas the code length increases. However, many existing hashing papers only report\nthe performance with the code length shorter than 128. In this paper, we\ncarefully revisit the problem of search with a hash index, and analyze the pros\nand cons of two popular hash index search procedures. Then we proposed a very\nsimple but effective two level index structures and make a thorough comparison\nof eleven popular hashing algorithms. Surprisingly, the random-projection-based\nLocality Sensitive Hashing (LSH) is the best performed algorithm, which is in\ncontradiction to the claims in all the other ten hashing papers. Despite the\nextreme simplicity of random-projection-based LSH, our results show that the\ncapability of this algorithm has been far underestimated. For the sake of\nreproducibility, all the codes used in the paper are released on GitHub, which\ncan be used as a testing platform for a fair comparison between various hashing\nalgorithms.</p>\n", "tags": ["Indexing", "Hashing Methods", "ANN Search", "KDD"], "tsne_embedding": [-5.028505325317383, -3.8288331031799316], "cluster": 7}, {"key": "cai2017revisit", "year": "2017", "citations": "13", "title": "A Revisit On Deep Hashings For Large-scale Content Based Image Retrieval", "abstract": "<p>There is a growing trend in studying deep hashing methods for content-based\nimage retrieval (CBIR), where hash functions and binary codes are learnt using\ndeep convolutional neural networks and then the binary codes can be used to do\napproximate nearest neighbor (ANN) search. All the existing deep hashing papers\nreport their methods\u2019 superior performance over the traditional hashing methods\naccording to their experimental results. However, there are serious flaws in\nthe evaluations of existing deep hashing papers: (1) The datasets they used are\ntoo small and simple to simulate the real CBIR situation. (2) They did not\ncorrectly include the search time in their evaluation criteria, while the\nsearch time is crucial in real CBIR systems. (3) The performance of some\nunsupervised hashing algorithms (e.g., LSH) can easily be boosted if one uses\nmultiple hash tables, which is an important factor should be considered in the\nevaluation while most of the deep hashing papers failed to do so.\n  We re-evaluate several state-of-the-art deep hashing methods with a carefully\ndesigned experimental setting. Empirical results reveal that the performance of\nthese deep hashing methods are inferior to multi-table IsoH, a very simple\nunsupervised hashing method. Thus, the conclusions in all the deep hashing\npapers should be carefully re-examined.</p>\n", "tags": ["Applications", "Unsupervised", "Deep Hashing", "ANN Search", "Hashing Methods"], "tsne_embedding": [2.3176467418670654, 9.698375701904297], "cluster": 5}, {"key": "cakir2015online", "year": "2015", "citations": "0", "title": "Online Supervised Hashing For Ever-growing Datasets", "abstract": "<p>Supervised hashing methods are widely-used for nearest neighbor search in\ncomputer vision applications. Most state-of-the-art supervised hashing\napproaches employ batch-learners. Unfortunately, batch-learning strategies can\nbe inefficient when confronted with large training datasets. Moreover, with\nbatch-learners, it is unclear how to adapt the hash functions as a dataset\ncontinues to grow and diversify over time. Yet, in many practical scenarios the\ndataset grows and diversifies; thus, both the hash functions and the indexing\nmust swiftly accommodate these changes. To address these issues, we propose an\nonline hashing method that is amenable to changes and expansions of the\ndatasets. Since it is an online algorithm, our approach offers linear\ncomplexity with the dataset size. Our solution is supervised, in that we\nincorporate available label information to preserve the semantic neighborhood.\nSuch an adaptive hashing method is attractive; but it requires recomputing the\nhash table as the hash functions are updated. If the frequency of update is\nhigh, then recomputing the hash table entries may cause inefficiencies in the\nsystem, especially for large indexes. Thus, we also propose a framework to\nreduce hash table updates. We compare our method to state-of-the-art solutions\non two benchmarks and demonstrate significant improvements over previous work.</p>\n", "tags": ["Applications", "Indexing", "ANN Search", "Evaluation Metrics", "Supervised", "Hashing Methods"], "tsne_embedding": [6.50640344619751, 6.872338771820068], "cluster": 3}, {"key": "cakir2017mihash", "year": "2017", "citations": "71", "title": "Mihash Online Hashing With Mutual Information", "abstract": "<p>Learning-based hashing methods are widely used for nearest neighbor\nretrieval, and recently, online hashing methods have demonstrated good\nperformance-complexity trade-offs by learning hash functions from streaming\ndata. In this paper, we first address a key challenge for online hashing: the\nbinary codes for indexed data must be recomputed to keep pace with updates to\nthe hash functions. We propose an efficient quality measure for hash functions,\nbased on an information-theoretic quantity, mutual information, and use it\nsuccessfully as a criterion to eliminate unnecessary hash table updates. Next,\nwe also show how to optimize the mutual information objective using stochastic\ngradient descent. We thus develop a novel hashing method, MIHash, that can be\nused in both online and batch settings. Experiments on image retrieval\nbenchmarks (including a 2.5M image dataset) confirm the effectiveness of our\nformulation, both in reducing hash table recomputations and in learning\nhigh-quality hash functions.</p>\n", "tags": ["Hashing Methods", "Deep Hashing", "ANN Search", "Applications", "Loss Functions", "Benchmarks and Datasets"], "tsne_embedding": [8.75218677520752, 5.734844207763672], "cluster": 3}, {"key": "cakir2017online", "year": "2017", "citations": "71", "title": "Mihash: Online Hashing With Mutual Information", "abstract": "<p>Learning-based hashing methods are widely used for nearest neighbor\nretrieval, and recently, online hashing methods have demonstrated good\nperformance-complexity trade-offs by learning hash functions from streaming\ndata. In this paper, we first address a key challenge for online hashing: the\nbinary codes for indexed data must be recomputed to keep pace with updates to\nthe hash functions. We propose an efficient quality measure for hash functions,\nbased on an information-theoretic quantity, mutual information, and use it\nsuccessfully as a criterion to eliminate unnecessary hash table updates. Next,\nwe also show how to optimize the mutual information objective using stochastic\ngradient descent. We thus develop a novel hashing method, MIHash, that can be\nused in both online and batch settings. Experiments on image retrieval\nbenchmarks (including a 2.5M image dataset) confirm the effectiveness of our\nformulation, both in reducing hash table recomputations and in learning\nhigh-quality hash functions.</p>\n", "tags": ["Applications", "Hashing Methods", "Evaluation Metrics"], "tsne_embedding": [8.75218677520752, 5.734844207763672], "cluster": 3}, {"key": "cakir2018hashing", "year": "2018", "citations": "13", "title": "Hashing With Binary Matrix Pursuit", "abstract": "<p>We propose theoretical and empirical improvements for two-stage hashing\nmethods. We first provide a theoretical analysis on the quality of the binary\ncodes and show that, under mild assumptions, a residual learning scheme can\nconstruct binary codes that fit any neighborhood structure with arbitrary\naccuracy. Secondly, we show that with high-capacity hash functions such as\nCNNs, binary code inference can be greatly simplified for many standard\nneighborhood definitions, yielding smaller optimization problems and more\nrobust codes. Incorporating our findings, we propose a novel two-stage hashing\nmethod that significantly outperforms previous hashing studies on widely used\nimage retrieval benchmarks.</p>\n", "tags": ["Applications", "Hashing Methods", "Evaluation Metrics"], "tsne_embedding": [-2.53291654586792, 3.97747802734375], "cluster": 0}, {"key": "cakir2024adaptive", "year": "2024", "citations": "60", "title": "Adaptive Hashing For Fast Similarity Search", "abstract": "<p>With the staggering growth in image and video datasets,\nalgorithms that provide fast similarity search and compact\nstorage are crucial. Hashing methods that map the\ndata into Hamming space have shown promise; however,\nmany of these methods employ a batch-learning strategy\nin which the computational cost and memory requirements\nmay become intractable and infeasible with larger and\nlarger datasets. To overcome these challenges, we propose\nan online learning algorithm based on stochastic gradient\ndescent in which the hash functions are updated iteratively\nwith streaming data. In experiments with three image retrieval\nbenchmarks, our online algorithm attains retrieval\naccuracy that is comparable to competing state-of-the-art\nbatch-learning solutions, while our formulation is orders\nof magnitude faster and being online it is adaptable to the\nvariations of the data. Moreover, our formulation yields improved\nretrieval performance over a recently reported online\nhashing technique, Online Kernel Hashing.</p>\n", "tags": ["Hashing Methods", "ANN Search", "Applications", "Efficient Learning", "Supervision Type"], "tsne_embedding": [10.32017993927002, 5.570010662078857], "cluster": 3}, {"key": "cao2016correlation", "year": "2016", "citations": "32", "title": "Correlation Hashing Network For Efficient Cross-modal Retrieval", "abstract": "<p>Hashing is widely applied to approximate nearest neighbor search for\nlarge-scale multimodal retrieval with storage and computation efficiency.\nCross-modal hashing improves the quality of hash coding by exploiting semantic\ncorrelations across different modalities. Existing cross-modal hashing methods\nfirst transform data into low-dimensional feature vectors, and then generate\nbinary codes by another separate quantization step. However, suboptimal hash\ncodes may be generated since the quantization error is not explicitly minimized\nand the feature representation is not jointly optimized with the binary codes.\nThis paper presents a Correlation Hashing Network (CHN) approach to cross-modal\nhashing, which jointly learns good data representation tailored to hash coding\nand formally controls the quantization error. The proposed CHN is a hybrid deep\narchitecture that constitutes a convolutional neural network for learning good\nimage representations, a multilayer perception for learning good text\nrepresentations, two hashing layers for generating compact binary codes, and a\nstructured max-margin loss that integrates all things together to enable\nlearning similarity-preserving and high-quality hash codes. Extensive empirical\nstudy shows that CHN yields state of the art cross-modal retrieval performance\non standard benchmarks.</p>\n", "tags": ["Deep Hashing", "Quantization", "ANN Search", "Multi-Modal Hashing", "Loss Functions", "Evaluation Metrics", "Hashing Methods"], "tsne_embedding": [4.3144307136535645, 3.38881516456604], "cluster": 9}, {"key": "cao2016transitive", "year": "2016", "citations": "11", "title": "Transitive Hashing Network For Heterogeneous Multimedia Retrieval", "abstract": "<p>Hashing has been widely applied to large-scale multimedia retrieval due to\nthe storage and retrieval efficiency. Cross-modal hashing enables efficient\nretrieval from database of one modality in response to a query of another\nmodality. Existing work on cross-modal hashing assumes heterogeneous\nrelationship across modalities for hash function learning. In this paper, we\nrelax the strong assumption by only requiring such heterogeneous relationship\nin an auxiliary dataset different from the query/database domain. We craft a\nhybrid deep architecture to simultaneously learn the cross-modal correlation\nfrom the auxiliary dataset, and align the dataset distributions between the\nauxiliary dataset and the query/database domain, which generates transitive\nhash codes for heterogeneous multimedia retrieval. Extensive experiments\nexhibit that the proposed approach yields state of the art multimedia retrieval\nperformance on public datasets, i.e. NUS-WIDE, ImageNet-YahooQA.</p>\n", "tags": ["Deep Hashing", "Multi-Modal Hashing", "Benchmarks and Datasets", "Hashing Methods"], "tsne_embedding": [7.528520107269287, 4.0214409828186035], "cluster": 3}, {"key": "cao2017deep", "year": "2017", "citations": "510", "title": "Hashnet: Deep Learning To Hash By Continuation", "abstract": "<p>Learning to hash has been widely applied to approximate nearest neighbor\nsearch for large-scale multimedia retrieval, due to its computation efficiency\nand retrieval quality. Deep learning to hash, which improves retrieval quality\nby end-to-end representation learning and hash encoding, has received\nincreasing attention recently. Subject to the ill-posed gradient difficulty in\nthe optimization with sign activations, existing deep learning to hash methods\nneed to first learn continuous representations and then generate binary hash\ncodes in a separated binarization step, which suffer from substantial loss of\nretrieval quality. This work presents HashNet, a novel deep architecture for\ndeep learning to hash by continuation method with convergence guarantees, which\nlearns exactly binary hash codes from imbalanced similarity data. The key idea\nis to attack the ill-posed gradient problem in optimizing deep networks with\nnon-smooth binary activations by continuation method, in which we begin from\nlearning an easier network with smoothed activation function and let it evolve\nduring the training, until it eventually goes back to being the original,\ndifficult to optimize, deep network with the sign activation function.\nComprehensive empirical evidence shows that HashNet can generate exactly binary\nhash codes and yield state-of-the-art multimedia retrieval performance on\nstandard benchmarks.</p>\n", "tags": ["ANN Search", "Hashing Methods", "Deep Hashing", "Evaluation Metrics"], "tsne_embedding": [3.3956003189086914, 5.611512184143066], "cluster": 5}, {"key": "cao2017transfer", "year": "2017", "citations": "10", "title": "Transfer Adversarial Hashing For Hamming Space Retrieval", "abstract": "<p>Hashing is widely applied to large-scale image retrieval due to the storage\nand retrieval efficiency. Existing work on deep hashing assumes that the\ndatabase in the target domain is identically distributed with the training set\nin the source domain. This paper relaxes this assumption to a transfer\nretrieval setting, which allows the database and the training set to come from\ndifferent but relevant domains. However, the transfer retrieval setting will\nintroduce two technical difficulties: first, the hash model trained on the\nsource domain cannot work well on the target domain due to the large\ndistribution gap; second, the domain gap makes it difficult to concentrate the\ndatabase points to be within a small Hamming ball. As a consequence, transfer\nretrieval performance within Hamming Radius 2 degrades significantly in\nexisting hashing methods. This paper presents Transfer Adversarial Hashing\n(TAH), a new hybrid deep architecture that incorporates a pairwise\n\\(t\\)-distribution cross-entropy loss to learn concentrated hash codes and an\nadversarial network to align the data distributions between the source and\ntarget domains. TAH can generate compact transfer hash codes for efficient\nimage retrieval on both source and target domains. Comprehensive experiments\nvalidate that TAH yields state of the art Hamming space retrieval performance\non standard datasets.</p>\n", "tags": ["Applications", "Deep Hashing", "ANN Search", "Hashing Methods", "Loss Functions"], "tsne_embedding": [1.8601493835449219, 1.8761948347091675], "cluster": 9}, {"key": "cao2018deep", "year": "2018", "citations": "33", "title": "Deep Priority Hashing", "abstract": "<p>Deep hashing enables image retrieval by end-to-end learning of deep\nrepresentations and hash codes from training data with pairwise similarity\ninformation. Subject to the distribution skewness underlying the similarity\ninformation, most existing deep hashing methods may underperform for imbalanced\ndata due to misspecified loss functions. This paper presents Deep Priority\nHashing (DPH), an end-to-end architecture that generates compact and balanced\nhash codes in a Bayesian learning framework. The main idea is to reshape the\nstandard cross-entropy loss for similarity-preserving learning such that it\ndown-weighs the loss associated to highly-confident pairs. This idea leads to a\nnovel priority cross-entropy loss, which prioritizes the training on uncertain\npairs over confident pairs. Also, we propose another priority quantization\nloss, which prioritizes hard-to-quantize examples for generation of nearly\nlossless hash codes. Extensive experiments demonstrate that DPH can generate\nhigh-quality hash codes and yield state-of-the-art image retrieval results on\nthree datasets, ImageNet, NUS-WIDE, and MS-COCO.</p>\n", "tags": ["Applications", "Deep Hashing", "Quantization", "Loss Functions", "Benchmarks and Datasets", "Hashing Methods"], "tsne_embedding": [3.325863838195801, 3.302114248275757], "cluster": 9}, {"key": "cao2019enhancing", "year": "2019", "citations": "79", "title": "Enhancing Remote Sensing Image Retrieval With Triplet Deep Metric Learning Network", "abstract": "<p>With the rapid growing of remotely sensed imagery data, there is a high\ndemand for effective and efficient image retrieval tools to manage and exploit\nsuch data. In this letter, we present a novel content-based remote sensing\nimage retrieval method based on Triplet deep metric learning convolutional\nneural network (CNN). By constructing a Triplet network with metric learning\nobjective function, we extract the representative features of the images in a\nsemantic space in which images from the same class are close to each other\nwhile those from different classes are far apart. In such a semantic space,\nsimple metric measures such as Euclidean distance can be used directly to\ncompare the similarity of images and effectively retrieve images of the same\nclass. We also investigate a supervised and an unsupervised learning methods\nfor reducing the dimensionality of the learned semantic features. We present\ncomprehensive experimental results on two publicly available remote sensing\nimage retrieval datasets and show that our method significantly outperforms\nstate-of-the-art.</p>\n", "tags": ["Applications", "Supervised", "Unsupervised", "Loss Functions", "Benchmarks and Datasets"], "tsne_embedding": [7.228809833526611, -1.0984858274459839], "cluster": 8}, {"key": "cao2020unifying", "year": "2020", "citations": "226", "title": "Unifying Deep Local And Global Features For Image Search", "abstract": "<p>Image retrieval is the problem of searching an image database for items that\nare similar to a query image. To address this task, two main types of image\nrepresentations have been studied: global and local image features. In this\nwork, our key contribution is to unify global and local features into a single\ndeep model, enabling accurate retrieval with efficient feature extraction. We\nrefer to the new model as DELG, standing for DEep Local and Global features. We\nleverage lessons from recent feature learning work and propose a model that\ncombines generalized mean pooling for global features and attentive selection\nfor local features. The entire network can be learned end-to-end by carefully\nbalancing the gradient flow between two heads \u2013 requiring only image-level\nlabels. We also introduce an autoencoder-based dimensionality reduction\ntechnique for local features, which is integrated into the model, improving\ntraining efficiency and matching performance. Comprehensive experiments show\nthat our model achieves state-of-the-art image retrieval on the Revisited\nOxford and Paris datasets, and state-of-the-art single-model instance-level\nrecognition on the Google Landmarks dataset v2. Code and models are available\nat https://github.com/tensorflow/models/tree/master/research/delf .</p>\n", "tags": ["Applications", "Tools and Libraries"], "tsne_embedding": [4.804967403411865, -0.9488939642906189], "cluster": 9}, {"key": "cao2024collective", "year": "2024", "citations": "46", "title": "Collective Deep Quantization For Efficient Cross-modal Retrieval", "abstract": "<p>Cross-modal similarity retrieval is a problem about designing a retrieval system that supports querying across\ncontent modalities, e.g., using an image to retrieve for\ntexts. This paper presents a compact coding solution for\nefficient cross-modal retrieval, with a focus on the quantization approach which has already shown the superior\nperformance over the hashing solutions in single-modal\nsimilarity retrieval. We propose a collective deep quantization (CDQ) approach, which is the first attempt to\nintroduce quantization in end-to-end deep architecture\nfor cross-modal retrieval. The major contribution lies in\njointly learning deep representations and the quantizers\nfor both modalities using carefully-crafted hybrid networks and well-specified loss functions. In addition, our\napproach simultaneously learns the common quantizer\ncodebook for both modalities through which the crossmodal correlation can be substantially enhanced. CDQ\nenables efficient and effective cross-modal retrieval using inner product distance computed based on the common codebook with fast distance table lookup. Extensive experiments show that CDQ yields state of the art\ncross-modal retrieval results on standard benchmarks.</p>\n", "tags": ["Quantization", "Cross Modal", "Efficient Learning"], "tsne_embedding": [6.177674293518066, 2.830369472503662], "cluster": 9}, {"key": "cao2024correlation", "year": "2024", "citations": "74", "title": "Correlation Autoencoder Hashing For Supervised Cross-modal Search", "abstract": "<p>Due to its storage and query efficiency, hashing has been widely\napplied to approximate nearest neighbor search from large-scale\ndatasets. While there is increasing interest in cross-modal hashing\nwhich facilitates cross-media retrieval by embedding data from different modalities into a common Hamming space, how to distill the\ncross-modal correlation structure effectively remains a challenging\nproblem. In this paper, we propose a novel supervised cross-modal\nhashing method, Correlation Autoencoder Hashing (CAH), to learn\ndiscriminative and compact binary codes based on deep autoencoders. Specifically, CAH jointly maximizes the feature correlation\nrevealed by bimodal data and the semantic correlation conveyed in\nsimilarity labels, while embeds them into hash codes by nonlinear\ndeep autoencoders. Extensive experiments clearly show the superior effectiveness and efficiency of CAH against the state-of-the-art\nhashing methods on standard cross-modal retrieval benchmarks.</p>\n", "tags": ["Cross Modal", "Supervised", "Deep Hashing", "ANN Search", "Evaluation Metrics", "Applications"], "tsne_embedding": [5.152684211730957, 2.9016199111938477], "cluster": 9}, {"key": "cao2024deep", "year": "2024", "citations": "278", "title": "Deep Cauchy Hashing For Hamming Space Retrieval", "abstract": "<p>Due to its computation efficiency and retrieval quality,\nhashing has been widely applied to approximate nearest\nneighbor search for large-scale image retrieval, while deep\nhashing further improves the retrieval quality by end-toend representation learning and hash coding. With compact\nhash codes, Hamming space retrieval enables the most efficient constant-time search that returns data points within a\ngiven Hamming radius to each query, by hash table lookups\ninstead of linear scan. However, subject to the weak capability of concentrating relevant images to be within a small\nHamming ball due to mis-specified loss functions, existing deep hashing methods may underperform for Hamming\nspace retrieval.  This work presents Deep Cauchy Hashing\n(DCH), a novel deep hashing model that generates compact\nand concentrated binary hash codes to enable efficient and\neffective Hamming space retrieval. The main idea is to design a pairwise cross-entropy loss based on Cauchy distribution, which penalizes significantly on similar image pairs\nwith Hamming distance larger than the given Hamming radius threshold. Comprehensive experiments demonstrate\nthat DCH can generate highly concentrated hash codes and\nyield state-of-the-art Hamming space retrieval performance\non three datasets, NUS-WIDE, CIFAR-10, and MS-COCO.</p>\n", "tags": ["Deep Hashing", "ANN Search", "Loss Functions", "Benchmarks and Datasets", "Applications"], "tsne_embedding": [2.638073444366455, 2.5149385929107666], "cluster": 9}, {"key": "cao2024hashgan", "year": "2024", "citations": "79", "title": "Hashgan Deep Learning To Hash With Pair Conditional Wasserstein GAN", "abstract": "<p>Deep learning to hash improves image retrieval performance by end-to-end representation learning and hash coding from training data with pairwise similarity information.\nSubject to the scarcity of similarity information that is often\nexpensive to collect for many application domains, existing\ndeep learning to hash methods may overfit the training data\nand result in substantial loss of retrieval quality. This paper\npresents HashGAN, a novel architecture for deep learning\nto hash, which learns compact binary hash codes from both\nreal images and diverse images synthesized by generative\nmodels. The main idea is to augment the training data with\nnearly real images synthesized from a new Pair Conditional\nWasserstein GAN (PC-WGAN) conditioned on the pairwise\nsimilarity information. Extensive experiments demonstrate\nthat HashGAN can generate high-quality binary hash codes\nand yield state-of-the-art image retrieval performance on\nthree benchmarks, NUS-WIDE, CIFAR-10, and MS-COCO.</p>\n", "tags": ["Deep Hashing", "ANN Search", "Supervised", "Benchmarks and Datasets"], "tsne_embedding": [3.3202152252197266, 4.1473388671875], "cluster": 9}, {"key": "cappellari2007trellis", "year": "2007", "citations": "0", "title": "Trellis-coded Quantization Based On Maximum-hamming-distance Binary Codes", "abstract": "<p>Most design approaches for trellis-coded quantization take advantage of the\nduality of trellis-coded quantization with trellis-coded modulation, and use\nthe same empirically-found convolutional codes to label the trellis branches.\nThis letter presents an alternative approach that instead takes advantage of\nmaximum-Hamming-distance convolutional codes. The proposed source codes are\nshown to be competitive with the best in the literature for the same\ncomputational complexity.</p>\n", "tags": ["Quantization"], "tsne_embedding": [5.549432277679443, 12.188119888305664], "cluster": 5}, {"key": "carreiraperpinan2024hashing", "year": "2024", "citations": "66", "title": "Hashing With Binary Autoencoders", "abstract": "<p>An attractive approach for fast search in image\ndatabases is binary hashing, where each high-dimensional,\nreal-valued image is mapped onto a low-dimensional, binary\nvector and the search is done in this binary space.\nFinding the optimal hash function is difficult because it involves\nbinary constraints, and most approaches approximate\nthe optimization by relaxing the constraints and then\nbinarizing the result. Here, we focus on the binary autoencoder\nmodel, which seeks to reconstruct an image from the\nbinary code produced by the hash function. We show that\nthe optimization can be simplified with the method of auxiliary\ncoordinates. This reformulates the optimization as\nalternating two easier steps: one that learns the encoder\nand decoder separately, and one that optimizes the code for\neach image. Image retrieval experiments show the resulting\nhash function outperforms or is competitive with state-ofthe-art\nmethods for binary hashing.</p>\n", "tags": ["Hashing Methods", "Unsupervised", "Image Retrieval"], "tsne_embedding": [-2.048078775405884, 1.6476095914840698], "cluster": 0}, {"key": "carreiraperpi\u00f1\u00e1n2015hashing", "year": "2015", "citations": "66", "title": "Hashing With Binary Autoencoders", "abstract": "<p>An attractive approach for fast search in image databases is binary hashing,\nwhere each high-dimensional, real-valued image is mapped onto a\nlow-dimensional, binary vector and the search is done in this binary space.\nFinding the optimal hash function is difficult because it involves binary\nconstraints, and most approaches approximate the optimization by relaxing the\nconstraints and then binarizing the result. Here, we focus on the binary\nautoencoder model, which seeks to reconstruct an image from the binary code\nproduced by the hash function. We show that the optimization can be simplified\nwith the method of auxiliary coordinates. This reformulates the optimization as\nalternating two easier steps: one that learns the encoder and decoder\nseparately, and one that optimizes the code for each image. Image retrieval\nexperiments, using precision/recall and a measure of code utilization, show the\nresulting hash function outperforms or is competitive with state-of-the-art\nmethods for binary hashing.</p>\n", "tags": ["Applications", "Hashing Methods", "Evaluation Metrics", "Deep Hashing", "Loss Functions"], "tsne_embedding": [-2.076094388961792, 1.653878927230835], "cluster": 0}, {"key": "carreiraperpi\u00f1\u00e1n2016ensemble", "year": "2016", "citations": "8", "title": "An Ensemble Diversity Approach To Supervised Binary Hashing", "abstract": "<p>Binary hashing is a well-known approach for fast approximate nearest-neighbor\nsearch in information retrieval. Much work has focused on affinity-based\nobjective functions involving the hash functions or binary codes. These\nobjective functions encode neighborhood information between data points and are\noften inspired by manifold learning algorithms. They ensure that the hash\nfunctions differ from each other through constraints or penalty terms that\nencourage codes to be orthogonal or dissimilar across bits, but this couples\nthe binary variables and complicates the already difficult optimization. We\npropose a much simpler approach: we train each hash function (or bit)\nindependently from each other, but introduce diversity among them using\ntechniques from classifier ensembles. Surprisingly, we find that not only is\nthis faster and trivially parallelizable, but it also improves over the more\ncomplex, coupled objective function, and achieves state-of-the-art precision\nand recall in experiments with image retrieval.</p>\n", "tags": ["Applications", "Hashing Methods", "Evaluation Metrics", "Supervised"], "tsne_embedding": [-1.2594927549362183, 2.7392024993896484], "cluster": 0}, {"key": "cartis2021hashing", "year": "2021", "citations": "3", "title": "Hashing Embeddings Of Optimal Dimension, With Applications To Linear Least Squares", "abstract": "<p>The aim of this paper is two-fold: firstly, to present subspace embedding\nproperties for \\(s\\)-hashing sketching matrices, with \\(s\\geq 1\\), that are optimal\nin the projection dimension \\(m\\) of the sketch, namely, \\(m=\\mathcal{O}(d)\\),\nwhere \\(d\\) is the dimension of the subspace. A diverse set of results are\npresented that address the case when the input matrix has sufficiently low\ncoherence (thus removing the \\(log^2 d\\) factor dependence in \\(m\\), in the\nlow-coherence result of Bourgain et al (2015) at the expense of a smaller\ncoherence requirement); how this coherence changes with the number \\(s\\) of\ncolumn nonzeros (allowing a scaling of \\(\\sqrt{s}\\) of the coherence bound), or\nis reduced through suitable transformations (when considering hashed \u2013 instead\nof subsampled \u2013 coherence reducing transformations such as randomised\nHadamard). Secondly, we apply these general hashing sketching results to the\nspecial case of Linear Least Squares (LLS), and develop Ski-LLS, a generic\nsoftware package for these problems, that builds upon and improves the\nBlendenpik solver on dense input and the (sequential) LSRN performance on\nsparse problems. In addition to the hashing sketching improvements, we add\nsuitable linear algebra tools for rank-deficient and for sparse problems that\nlead Ski-LLS to outperform not only sketching-based routines on randomly\ngenerated input, but also state of the art direct solver SPQR and iterative\ncode HSL on certain subsets of the sparse Florida matrix collection; namely, on\nleast squares problems that are significantly overdetermined, or moderately\nsparse, or difficult.</p>\n", "tags": ["Hashing Methods", "Applications"], "tsne_embedding": [-12.25601577758789, -9.520877838134766], "cluster": 2}, {"key": "cayton2007learning", "year": "2007", "citations": "28", "title": "A Learning Framework For Nearest Neighbor Search", "abstract": "<p>Can we leverage learning techniques to build a fast nearest-neighbor (NN) retrieval data structure? We present a general learning framework for the NN problem in which sample queries are used to learn the parameters of a data structure that minimize the retrieval time and/or the miss rate. We explore the potential of this novel framework through two popular NN data structures: KD-trees and the rectilinear structures employed by locality sensitive hashing. We derive a generalization theory for these data structure classes and present simple learning algorithms for both. Experimental results reveal that learning often improves on the already strong performance of these data structures.</p>\n", "tags": ["ANN Search", "Hashing Methods", "Theory"], "tsne_embedding": [5.028934955596924, 7.625290870666504], "cluster": 5}, {"key": "ceccarello2018fresh", "year": "2018", "citations": "6", "title": "FRESH Frechet Similarity With Hashing", "abstract": "<p>This paper studies the \\(r\\)-range search problem for curves under the\ncontinuous Fr'echet distance: given a dataset \\(S\\) of \\(n\\) polygonal curves and\na threshold \\(r&gt;0\\), construct a data structure that, for any query curve \\(q\\),\nefficiently returns all entries in \\(S\\) with distance at most \\(r\\) from \\(q\\). We\npropose FRESH, an approximate and randomized approach for \\(r\\)-range search,\nthat leverages on a locality sensitive hashing scheme for detecting candidate\nnear neighbors of the query curve, and on a subsequent pruning step based on a\ncascade of curve simplifications. We experimentally compare \\fresh to exact and\ndeterministic solutions, and we show that high performance can be reached by\nsuitably relaxing precision and recall.</p>\n", "tags": ["Hashing Methods", "ANN Search", "Efficient Learning"], "tsne_embedding": [-11.98320198059082, 2.6404855251312256], "cluster": 4}, {"key": "ceccarello2018similarity", "year": "2018", "citations": "6", "title": "FRESH: Fr\\'echet Similarity With Hashing", "abstract": "<p>This paper studies the \\(r\\)-range search problem for curves under the\ncontinuous Fr'echet distance: given a dataset \\(S\\) of \\(n\\) polygonal curves and\na threshold \\(r&gt;0\\), construct a data structure that, for any query curve \\(q\\),\nefficiently returns all entries in \\(S\\) with distance at most \\(r\\) from \\(q\\). We\npropose FRESH, an approximate and randomized approach for \\(r\\)-range search,\nthat leverages on a locality sensitive hashing scheme for detecting candidate\nnear neighbors of the query curve, and on a subsequent pruning step based on a\ncascade of curve simplifications. We experimentally compare \\fresh to exact and\ndeterministic solutions, and we show that high performance can be reached by\nsuitably relaxing precision and recall.</p>\n", "tags": ["Hashing Methods", "Evaluation Metrics", "ANN Search"], "tsne_embedding": [-11.983200073242188, 2.6404809951782227], "cluster": 4}, {"key": "chadha2016voronoi", "year": "2016", "citations": "27", "title": "Voronoi-based Compact Image Descriptors: Efficient Region-of-interest Retrieval With VLAD And Deep-learning-based Descriptors", "abstract": "<p>We investigate the problem of image retrieval based on visual queries when\nthe latter comprise arbitrary regions-of-interest (ROI) rather than entire\nimages. Our proposal is a compact image descriptor that combines the\nstate-of-the-art in content-based descriptor extraction with a multi-level,\nVoronoi-based spatial partitioning of each dataset image. The proposed\nmulti-level Voronoi-based encoding uses a spatial hierarchical K-means over\ninterest-point locations, and computes a content-based descriptor over each\ncell. In order to reduce the matching complexity with minimal or no sacrifice\nin retrieval performance: (i) we utilize the tree structure of the spatial\nhierarchical K-means to perform a top-to-bottom pruning for local similarity\nmaxima; (ii) we propose a new image similarity score that combines relevant\ninformation from all partition levels into a single measure for similarity;\n(iii) we combine our proposal with a novel and efficient approach for optimal\nbit allocation within quantized descriptor representations. By deriving both a\nVoronoi-based VLAD descriptor (termed as Fast-VVLAD) and a Voronoi-based deep\nconvolutional neural network (CNN) descriptor (termed as Fast-VDCNN), we\ndemonstrate that our Voronoi-based framework is agnostic to the descriptor\nbasis, and can easily be slotted into existing frameworks. Via a range of ROI\nqueries in two standard datasets, it is shown that the Voronoi-based\ndescriptors achieve comparable or higher mean Average Precision against\nconventional grid-based spatial search, while offering more than two-fold\nreduction in complexity. Finally, beyond ROI queries, we show that Voronoi\npartitioning improves the geometric invariance of compact CNN descriptors,\nthereby resulting in competitive performance to the current state-of-the-art on\nwhole image retrieval.</p>\n", "tags": ["Applications", "Evaluation Metrics", "Quantization"], "tsne_embedding": [2.483464002609253, -1.9108569622039795], "cluster": 9}, {"key": "chaidaroon2017variational", "year": "2017", "citations": "38", "title": "Variational Deep Semantic Hashing For Text Documents", "abstract": "<p>As the amount of textual data has been rapidly increasing over the past\ndecade, efficient similarity search methods have become a crucial component of\nlarge-scale information retrieval systems. A popular strategy is to represent\noriginal data samples by compact binary codes through hashing. A spectrum of\nmachine learning methods have been utilized, but they often lack expressiveness\nand flexibility in modeling to learn effective representations. The recent\nadvances of deep learning in a wide range of applications has demonstrated its\ncapability to learn robust and powerful feature representations for complex\ndata. Especially, deep generative models naturally combine the expressiveness\nof probabilistic generative models with the high capacity of deep neural\nnetworks, which is very suitable for text modeling. However, little work has\nleveraged the recent progress in deep learning for text hashing.\n  In this paper, we propose a series of novel deep document generative models\nfor text hashing. The first proposed model is unsupervised while the second one\nis supervised by utilizing document labels/tags for hashing. The third model\nfurther considers document-specific factors that affect the generation of\nwords. The probabilistic generative formulation of the proposed models provides\na principled framework for model extension, uncertainty estimation, simulation,\nand interpretability. Based on variational inference and reparameterization,\nthe proposed models can be interpreted as encoder-decoder deep neural networks\nand thus they are capable of learning complex nonlinear distributed\nrepresentations of the original documents. We conduct a comprehensive set of\nexperiments on four public testbeds. The experimental results have demonstrated\nthe effectiveness of the proposed supervised learning models for text hashing.</p>\n", "tags": ["Applications", "Unsupervised", "Supervised", "Hashing Methods"], "tsne_embedding": [3.97977614402771, 9.434271812438965], "cluster": 5}, {"key": "chaidaroon2024deep", "year": "2024", "citations": "14", "title": "Deep Semantic Text Hashing With Weak Supervision", "abstract": "<p>With an ever increasing amount of data available on the web, fast similarity search has become the critical component for large-scale information retrieval systems. One solution is semantic hashing which designs binary codes to accelerate similarity search. Recently, deep learning has been successfully applied to the semantic hashing problem and produces high-quality compact binary codes compared to traditional methods. However, most state-of-the-art semantic hashing approaches require large amounts of hand-labeled training data which are often expensive and time consuming to collect. The cost of getting labeled data is the key bottleneck in deploying these hashing methods. Motivated by the recent success in machine learning that makes use of weak supervision, we employ unsupervised ranking methods such as BM25 to extract weak signals from training data. We further introduce two deep generative semantic hashing models to leverage weak signals for text hashing. The experimental results on four public datasets show that our models can generate high-quality binary codes without using hand-labeled training data and significantly outperform the competitive unsupervised semantic hashing baselines.</p>\n", "tags": ["Deep Hashing", "Unsupervised", "Supervised", "ANN Search"], "tsne_embedding": [4.127273082733154, 8.843064308166504], "cluster": 5}, {"key": "chakrabarti2014sequential", "year": "2014", "citations": "8", "title": "Sequential Hypothesis Tests For Adaptive Locality Sensitive Hashing", "abstract": "<p>All pairs similarity search is a problem where a set of data objects is given\nand the task is to find all pairs of objects that have similarity above a\ncertain threshold for a given similarity measure-of-interest. When the number\nof points or dimensionality is high, standard solutions fail to scale\ngracefully. Approximate solutions such as Locality Sensitive Hashing (LSH) and\nits Bayesian variants (BayesLSH and BayesLSHLite) alleviate the problem to some\nextent and provides substantial speedup over traditional index based\napproaches. BayesLSH is used for pruning the candidate space and computation of\napproximate similarity, whereas BayesLSHLite can only prune the candidates, but\nsimilarity needs to be computed exactly on the original data. Thus where ever\nthe explicit data representation is available and exact similarity computation\nis not too expensive, BayesLSHLite can be used to aggressively prune candidates\nand provide substantial speedup without losing too much on quality. However,\nthe loss in quality is higher in the BayesLSH variant, where explicit data\nrepresentation is not available, rather only a hash sketch is available and\nsimilarity has to be estimated approximately. In this work we revisit the LSH\nproblem from a Frequentist setting and formulate sequential tests for composite\nhypothesis (similarity greater than or less than threshold) that can be\nleveraged by such LSH algorithms for adaptively pruning candidates\naggressively. We propose a vanilla sequential probability ration test (SPRT)\napproach based on this idea and two novel variants. We extend these variants to\nthe case where approximate similarity needs to be computed using fixed-width\nsequential confidence interval generation technique.</p>\n", "tags": ["Efficient Learning", "Hashing Methods", "ANN Search"], "tsne_embedding": [-8.353813171386719, -4.206161022186279], "cluster": 7}, {"key": "chakrabarti2020efficient", "year": "2020", "citations": "0", "title": "Efficient Image Retrieval Using Multi Neural Hash Codes And Bloom Filters", "abstract": "<p>This paper aims to deliver an efficient and modified approach for image\nretrieval using multiple neural hash codes and limiting the number of queries\nusing bloom filters by identifying false positives beforehand. Traditional\napproaches involving neural networks for image retrieval tasks tend to use\nhigher layers for feature extraction. But it has been seen that the activations\nof lower layers have proven to be more effective in a number of scenarios. In\nour approach, we have leveraged the use of local deep convolutional neural\nnetworks which combines the powers of both the features of lower and higher\nlayers for creating feature maps which are then compressed using PCA and fed to\na bloom filter after binary sequencing using a modified multi k-means approach.\nThe feature maps obtained are further used in the image retrieval process in a\nhierarchical coarse-to-fine manner by first comparing the images in the higher\nlayers for semantically similar images and then gradually moving towards the\nlower layers searching for structural similarities. While searching, the neural\nhashes for the query image are again calculated and queried in the bloom filter\nwhich tells us whether the query image is absent in the set or maybe present.\nIf the bloom filter doesn\u2019t necessarily rule out the query, then it goes into\nthe image retrieval process. This approach can be particularly helpful in cases\nwhere the image store is distributed since the approach supports parallel\nquerying.</p>\n", "tags": ["Applications", "ANN Search", "Hashing Methods", "Efficient Learning"], "tsne_embedding": [2.9279255867004395, -1.472101092338562], "cluster": 9}, {"key": "chakraborty2017improved", "year": "2017", "citations": "0", "title": "An Improved Video Analysis Using Context Based Extension Of LSH", "abstract": "<p>Locality Sensitive Hashing (LSH) based algorithms have already shown their\npromise in finding approximate nearest neighbors in high dimen- sional data\nspace. However, there are certain scenarios, as in sequential data, where the\nproximity of a pair of points cannot be captured without considering their\nsurroundings or context. In videos, as for example, a particular frame is\nmeaningful only when it is seen in the context of its preceding and following\nframes. LSH has no mechanism to handle the con- texts of the data points. In\nthis article, a novel scheme of Context based Locality Sensitive Hashing\n(conLSH) has been introduced, in which points are hashed together not only\nbased on their closeness, but also because of similar context. The contribution\nmade in this article is three fold. First, conLSH is integrated with a recently\nproposed fast optimal sequence alignment algorithm (FOGSAA) using a layered\napproach. The resultant method is applied to video retrieval for extracting\nsimilar sequences. The pro- posed algorithm yields more than 80% accuracy on an\naverage in different datasets. It has been found to save 36.3% of the total\ntime, consumed by the exhaustive search. conLSH reduces the search space to\napproximately 42% of the entire dataset, when compared with an exhaustive\nsearch by the aforementioned FOGSAA, Bag of Words method and the standard LSH\nimplementations. Secondly, the effectiveness of conLSH is demon- strated in\naction recognition of the video clips, which yields an average gain of 12.83%\nin terms of classification accuracy over the state of the art methods using\nSTIP descriptors. The last but of great significance is that this article\nprovides a way of automatically annotating long and composite real life videos.\nThe source code of conLSH is made available at\nhttp://www.isical.ac.in/~bioinfo_miu/conLSH/conLSH.html</p>\n", "tags": ["Applications", "ANN Search", "Hashing Methods"], "tsne_embedding": [0.36583900451660156, 11.724190711975098], "cluster": 5}, {"key": "chandrasekaran2017lattice", "year": "2017", "citations": "1", "title": "Lattice-based Locality Sensitive Hashing Is Optimal", "abstract": "<p>Locality sensitive hashing (LSH) was introduced by Indyk and Motwani (STOC\n<code class=\"language-plaintext highlighter-rouge\">98) to give the first sublinear time algorithm for the c-approximate nearest\nneighbor (ANN) problem using only polynomial space. At a high level, an LSH\nfamily hashes \"nearby\" points to the same bucket and \"far away\" points to\ndifferent buckets. The quality of measure of an LSH family is its LSH exponent,\nwhich helps determine both query time and space usage.\n  In a seminal work, Andoni and Indyk (FOCS </code>06) constructed an LSH family\nbased on random ball partitioning of space that achieves an LSH exponent of\n1/c^2 for the l_2 norm, which was later shown to be optimal by Motwani, Naor\nand Panigrahy (SIDMA <code class=\"language-plaintext highlighter-rouge\">07) and O'Donnell, Wu and Zhou (TOCT </code>14). Although\noptimal in the LSH exponent, the ball partitioning approach is computationally\nexpensive. So, in the same work, Andoni and Indyk proposed a simpler and more\npractical hashing scheme based on Euclidean lattices and provided computational\nresults using the 24-dimensional Leech lattice. However, no theoretical\nanalysis of the scheme was given, thus leaving open the question of finding the\nexponent of lattice based LSH.\n  In this work, we resolve this question by showing the existence of lattices\nachieving the optimal LSH exponent of 1/c^2 using techniques from the geometry\nof numbers. At a more conceptual level, our results show that optimal LSH space\npartitions can have periodic structure. Understanding the extent to which\nadditional structure can be imposed on these partitions, e.g. to yield low\nspace and query complexity, remains an important open problem.</p>\n", "tags": ["ANN Search", "Hashing Methods"], "tsne_embedding": [-10.854536056518555, -7.904104709625244], "cluster": 2}, {"key": "chandrasekhar2017compression", "year": "2017", "citations": "14", "title": "Compression Of Deep Neural Networks For Image Instance Retrieval", "abstract": "<p>Image instance retrieval is the problem of retrieving images from a database\nwhich contain the same object. Convolutional Neural Network (CNN) based\ndescriptors are becoming the dominant approach for generating {\\it global image\ndescriptors} for the instance retrieval problem. One major drawback of\nCNN-based {\\it global descriptors} is that uncompressed deep neural network\nmodels require hundreds of megabytes of storage making them inconvenient to\ndeploy in mobile applications or in custom hardware. In this work, we study the\nproblem of neural network model compression focusing on the image instance\nretrieval task. We study quantization, coding, pruning and weight sharing\ntechniques for reducing model size for the instance retrieval problem. We\nprovide extensive experimental results on the trade-off between retrieval\nperformance and model size for different types of networks on several data sets\nproviding the most comprehensive study on this topic. We compress models to the\norder of a few MBs: two orders of magnitude smaller than the uncompressed\nmodels while achieving negligible loss in retrieval performance.</p>\n", "tags": ["Applications", "Quantization"], "tsne_embedding": [-2.7726798057556152, 7.752506732940674], "cluster": 0}, {"key": "charikar2018hashing", "year": "2018", "citations": "19", "title": "Hashing-based-estimators For Kernel Density In High Dimensions", "abstract": "<p>Given a set of points \\(P\\subset \\mathbb{R}^{d}\\) and a kernel \\(k\\), the Kernel\nDensity Estimate at a point \\(x\\in\\mathbb{R}^{d}\\) is defined as\n\\(\\mathrm{KDE}<em>{P}(x)=\\frac{1}{|P|}\\sum</em>{y\\in P} k(x,y)\\). We study the problem\nof designing a data structure that given a data set \\(P\\) and a kernel function,\nreturns <em>approximations to the kernel density</em> of a query point in <em>sublinear\ntime</em>. We introduce a class of unbiased estimators for kernel density\nimplemented through locality-sensitive hashing, and give general theorems\nbounding the variance of such estimators. These estimators give rise to\nefficient data structures for estimating the kernel density in high dimensions\nfor a variety of commonly used kernels. Our work is the first to provide\ndata-structures with theoretical guarantees that improve upon simple random\nsampling in high dimensions.</p>\n", "tags": ["Hashing Methods", "ANN Search", "Efficient Learning"], "tsne_embedding": [-14.359965324401855, -3.866320848464966], "cluster": 2}, {"key": "charikar2018multi", "year": "2018", "citations": "2", "title": "Multi-resolution Hashing For Fast Pairwise Summations", "abstract": "<p>A basic computational primitive in the analysis of massive datasets is\nsumming simple functions over a large number of objects. Modern applications\npose an additional challenge in that such functions often depend on a parameter\nvector \\(y\\) (query) that is unknown a priori. Given a set of points \\(X\\subset\n\\mathbb{R}^{d}\\) and a pairwise function \\(w:\\mathbb{R}^{d}\\times\n\\mathbb{R}^{d}\\to [0,1]\\), we study the problem of designing a data-structure\nthat enables sublinear-time approximation of the summation\n\\(Z_{w}(y)=\\frac{1}{|X|}\\sum_{x\\in X}w(x,y)\\) for any query \\(y\\in\n\\mathbb{R}^{d}\\). By combining ideas from Harmonic Analysis (partitions of unity\nand approximation theory) with Hashing-Based-Estimators [Charikar, Siminelakis\nFOCS\u201917], we provide a general framework for designing such data structures\nthrough hashing that reaches far beyond what previous techniques allowed.\n  A key design principle is a collection of \\(T\\geq 1\\) hashing schemes with\ncollision probabilities \\(p_{1},\\ldots, p_{T}\\) such that \\(\\sup_{t\\in\n[T]}\\{p_{t}(x,y)\\} = \\Theta(\\sqrt{w(x,y)})\\). This leads to a data-structure\nthat approximates \\(Z_{w}(y)\\) using a sub-linear number of samples from each\nhash family. Using this new framework along with Distance Sensitive Hashing\n[Aumuller, Christiani, Pagh, Silvestri PODS\u201918], we show that such a collection\ncan be constructed and evaluated efficiently for any log-convex function\n\\(w(x,y)=e^{\\phi(\\langle x,y\\rangle)}\\) of the inner product on the unit sphere\n\\(x,y\\in \\mathcal{S}^{d-1}\\).\n  Our method leads to data structures with sub-linear query time that\nsignificantly improve upon random sampling and can be used for Kernel Density\nor Partition Function Estimation. We provide extensions of our result from the\nsphere to \\(\\mathbb{R}^{d}\\) and from scalar functions to vector functions.</p>\n", "tags": ["Hashing Methods", "Applications"], "tsne_embedding": [-14.223904609680176, -7.938551425933838], "cluster": 2}, {"key": "chatfield2014efficient", "year": "2014", "citations": "4", "title": "Efficient On-the-fly Category Retrieval Using Convnets And Gpus", "abstract": "<p>We investigate the gains in precision and speed, that can be obtained by\nusing Convolutional Networks (ConvNets) for on-the-fly retrieval - where\nclassifiers are learnt at run time for a textual query from downloaded images,\nand used to rank large image or video datasets.\n  We make three contributions: (i) we present an evaluation of state-of-the-art\nimage representations for object category retrieval over standard benchmark\ndatasets containing 1M+ images; (ii) we show that ConvNets can be used to\nobtain features which are incredibly performant, and yet much lower dimensional\nthan previous state-of-the-art image representations, and that their\ndimensionality can be reduced further without loss in performance by\ncompression using product quantization or binarization. Consequently, features\nwith the state-of-the-art performance on large-scale datasets of millions of\nimages can fit in the memory of even a commodity GPU card; (iii) we show that\nan SVM classifier can be learnt within a ConvNet framework on a GPU in parallel\nwith downloading the new training images, allowing for a continuous refinement\nof the model as more images become available, and simultaneous training and\nranking. The outcome is an on-the-fly system that significantly outperforms its\npredecessors in terms of: precision of retrieval, memory requirements, and\nspeed, facilitating accurate on-the-fly learning and ranking in under a second\non a single GPU.</p>\n", "tags": ["Quantization", "Evaluation Metrics", "Tools and Libraries"], "tsne_embedding": [0.016808778047561646, -5.104784965515137], "cluster": 6}, {"key": "chen2016revisiting", "year": "2016", "citations": "3", "title": "Revisiting Winner Take All (WTA) Hashing For Sparse Datasets", "abstract": "<p>WTA (Winner Take All) hashing has been successfully applied in many large\nscale vision applications. This hashing scheme was tailored to take advantage\nof the comparative reasoning (or order based information), which showed\nsignificant accuracy improvements. In this paper, we identify a subtle issue\nwith WTA, which grows with the sparsity of the datasets. This issue limits the\ndiscriminative power of WTA. We then propose a solution for this problem based\non the idea of Densification which provably fixes the issue. Our experiments\nshow that Densified WTA Hashing outperforms Vanilla WTA both in image\nclassification and retrieval tasks consistently and significantly.</p>\n", "tags": ["Applications", "Hashing Methods"], "tsne_embedding": [9.184860229492188, 11.206235885620117], "cluster": 3}, {"key": "chen2018deep", "year": "2018", "citations": "4", "title": "ALMN: Deep Embedding Learning With Geometrical Virtual Point Generating", "abstract": "<p>Deep embedding learning becomes more attractive for discriminative feature\nlearning, but many methods still require hard-class mining, which is\ncomputationally complex and performance-sensitive. To this end, we propose\nAdaptive Large Margin N-Pair loss (ALMN) to address the aforementioned issues.\nInstead of exploring hard example-mining strategy, we introduce the concept of\nlarge margin constraint. This constraint aims at encouraging local-adaptive\nlarge angular decision margin among dissimilar samples in multimodal feature\nspace so as to significantly encourage intraclass compactness and interclass\nseparability. And it is mainly achieved by a simple yet novel geometrical\nVirtual Point Generating (VPG) method, which converts artificially setting a\nfixed margin into automatically generating a boundary training sample in\nfeature space and is an open question. We demonstrate the effectiveness of our\nmethod on several popular datasets for image retrieval and clustering tasks.</p>\n", "tags": ["Applications", "Loss Functions"], "tsne_embedding": [1.131939172744751, -1.8128999471664429], "cluster": 9}, {"key": "chen2018distributed", "year": "2018", "citations": "5", "title": "Distributed Collaborative Hashing And Its Applications In Ant Financial", "abstract": "<p>Collaborative filtering, especially latent factor model, has been popularly\nused in personalized recommendation. Latent factor model aims to learn user and\nitem latent factors from user-item historic behaviors. To apply it into real\nbig data scenarios, efficiency becomes the first concern, including offline\nmodel training efficiency and online recommendation efficiency. In this paper,\nwe propose a Distributed Collaborative Hashing (DCH) model which can\nsignificantly improve both efficiencies. Specifically, we first propose a\ndistributed learning framework, following the state-of-the-art parameter server\nparadigm, to learn the offline collaborative model. Our model can be learnt\nefficiently by distributedly computing subgradients in minibatches on workers\nand updating model parameters on servers asynchronously. We then adopt hashing\ntechnique to speedup the online recommendation procedure. Recommendation can be\nquickly made through exploiting lookup hash tables. We conduct thorough\nexperiments on two real large-scale datasets. The experimental results\ndemonstrate that, comparing with the classic and state-of-the-art (distributed)\nlatent factor models, DCH has comparable performance in terms of recommendation\naccuracy but has both fast convergence speed in offline model training\nprocedure and realtime efficiency in online recommendation procedure.\nFurthermore, the encouraging performance of DCH is also shown for several\nreal-world applications in Ant Financial.</p>\n", "tags": ["Applications", "Efficient Learning", "Hashing Methods"], "tsne_embedding": [7.601784706115723, 6.674361228942871], "cluster": 3}, {"key": "chen2019hadamard", "year": "2019", "citations": "3", "title": "Hadamard Codebook Based Deep Hashing", "abstract": "<p>As an approximate nearest neighbor search technique, hashing has been widely\napplied in large-scale image retrieval due to its excellent efficiency. Most\nsupervised deep hashing methods have similar loss designs with embedding\nlearning, while quantizing the continuous high-dim feature into compact binary\nspace. We argue that the existing deep hashing schemes are defective in two\nissues that seriously affect the performance, i.e., bit independence and bit\nbalance. The former refers to hash codes of different classes should be\nindependent of each other, while the latter means each bit should have a\nbalanced distribution of +1s and -1s. In this paper, we propose a novel\nsupervised deep hashing method, termed Hadamard Codebook based Deep Hashing\n(HCDH), which solves the above two problems in a unified formulation.\nSpecifically, we utilize an off-the-shelf algorithm to generate a binary\nHadamard codebook to satisfy the requirement of bit independence and bit\nbalance, which subsequently serves as the desired outputs of the hash functions\nlearning. We also introduce a projection matrix to solve the inconsistency\nbetween the order of Hadamard matrix and the number of classes. Besides, the\nproposed HCDH further exploits the supervised labels by constructing a\nclassifier on top of the outputs of hash functions. Extensive experiments\ndemonstrate that HCDH can yield discriminative and balanced binary codes, which\nwell outperforms many state-of-the-arts on three widely-used benchmarks.</p>\n", "tags": ["Applications", "Deep Hashing", "ANN Search", "Evaluation Metrics", "Supervised", "Hashing Methods"], "tsne_embedding": [0.48850277066230774, 2.7866592407226562], "cluster": 0}, {"key": "chen2019locality", "year": "2019", "citations": "4", "title": "Locality-sensitive Hashing For F-divergences: Mutual Information Loss And Beyond", "abstract": "<p>Computing approximate nearest neighbors in high dimensional spaces is a\ncentral problem in large-scale data mining with a wide range of applications in\nmachine learning and data science. A popular and effective technique in\ncomputing nearest neighbors approximately is the locality-sensitive hashing\n(LSH) scheme. In this paper, we aim to develop LSH schemes for distance\nfunctions that measure the distance between two probability distributions,\nparticularly for f-divergences as well as a generalization to capture mutual\ninformation loss. First, we provide a general framework to design LHS schemes\nfor f-divergence distance functions and develop LSH schemes for the generalized\nJensen-Shannon divergence and triangular discrimination in this framework. We\nshow a two-sided approximation result for approximation of the generalized\nJensen-Shannon divergence by the Hellinger distance, which may be of\nindependent interest. Next, we show a general method of reducing the problem of\ndesigning an LSH scheme for a Krein kernel (which can be expressed as the\ndifference of two positive definite kernels) to the problem of maximum inner\nproduct search. We exemplify this method by applying it to the mutual\ninformation loss, due to its several important applications such as model\ncompression.</p>\n", "tags": ["Hashing Methods", "ANN Search", "Applications"], "tsne_embedding": [-10.671573638916016, -0.4179084599018097], "cluster": 4}, {"key": "chen2019vector", "year": "2019", "citations": "5", "title": "Vector And Line Quantization For Billion-scale Similarity Search On Gpus", "abstract": "<p>Billion-scale high-dimensional approximate nearest neighbour (ANN) search has\nbecome an important problem for searching similar objects among the vast amount\nof images and videos available online. The existing ANN methods are usually\ncharacterized by their specific indexing structures, including the inverted\nindex and the inverted multi-index structure. The inverted index structure is\namenable to GPU-based implementations, and the state-of-the-art systems such as\nFaiss are able to exploit the massive parallelism offered by GPUs. However, the\ninverted index requires high memory overhead to index the dataset effectively.\nThe inverted multi-index structure is difficult to implement for GPUs, and also\nineffective in dealing with database with different data distributions. In this\npaper we propose a novel hierarchical inverted index structure generated by\nvector and line quantization methods. Our quantization method improves both\nsearch efficiency and accuracy, while maintaining comparable memory\nconsumption. This is achieved by reducing search space and increasing the\nnumber of indexed regions. We introduce a new ANN search system, VLQ-ADC, that\nis based on the proposed inverted index, and perform extensive evaluation on\ntwo public billion-scale benchmark datasets SIFT1B and DEEP1B. Our evaluation\nshows that VLQ-ADC significantly outperforms the state-of-the-art GPU- and\nCPU-based systems in terms of both accuracy and search speed. The source code\nof VLQ-ADC is available at\nhttps://github.com/zjuchenwei/vector-line-quantization.</p>\n", "tags": ["Quantization", "Indexing", "ANN Search", "Evaluation Metrics", "Benchmarks and Datasets", "Tools and Libraries"], "tsne_embedding": [-0.6363056302070618, -10.372032165527344], "cluster": 6}, {"key": "chen2020making", "year": "2020", "citations": "13", "title": "Making Online Sketching Hashing Even Faster", "abstract": "<p>Data-dependent hashing methods have demonstrated good performance in various\nmachine learning applications to learn a low-dimensional representation from\nthe original data. However, they still suffer from several obstacles: First,\nmost of existing hashing methods are trained in a batch mode, yielding\ninefficiency for training streaming data. Second, the computational cost and\nthe memory consumption increase extraordinarily in the big data setting, which\nperplexes the training procedure. Third, the lack of labeled data hinders the\nimprovement of the model performance. To address these difficulties, we utilize\nonline sketching hashing (OSH) and present a FasteR Online Sketching Hashing\n(FROSH) algorithm to sketch the data in a more compact form via an independent\ntransformation. We provide theoretical justification to guarantee that our\nproposed FROSH consumes less time and achieves a comparable sketching precision\nunder the same memory cost of OSH. We also extend FROSH to its distributed\nimplementation, namely DFROSH, to further reduce the training time cost of\nFROSH while deriving the theoretical bound of the sketching precision. Finally,\nwe conduct extensive experiments on both synthetic and real datasets to\ndemonstrate the attractive merits of FROSH and DFROSH.</p>\n", "tags": ["Hashing Methods", "Applications", "Evaluation Metrics", "Efficient Learning"], "tsne_embedding": [1.4846457242965698, 8.633498191833496], "cluster": 5}, {"key": "chen2021deep", "year": "2021", "citations": "3", "title": "DVHN: A Deep Hashing Framework For Large-scale Vehicle Re-identification", "abstract": "<p>In this paper, we make the very first attempt to investigate the integration\nof deep hash learning with vehicle re-identification. We propose a deep\nhash-based vehicle re-identification framework, dubbed DVHN, which\nsubstantially reduces memory usage and promotes retrieval efficiency while\nreserving nearest neighbor search accuracy. Concretely,~DVHN directly learns\ndiscrete compact binary hash codes for each image by jointly optimizing the\nfeature learning network and the hash code generating module. Specifically, we\ndirectly constrain the output from the convolutional neural network to be\ndiscrete binary codes and ensure the learned binary codes are optimal for\nclassification. To optimize the deep discrete hashing framework, we further\npropose an alternating minimization method for learning binary\nsimilarity-preserved hashing codes. Extensive experiments on two widely-studied\nvehicle re-identification datasets- \\textbf{VehicleID} and \\textbf{VeRi}-~have\ndemonstrated the superiority of our method against the state-of-the-art deep\nhash methods. \\textbf{DVHN} of \\(2048\\) bits can achieve 13.94% and 10.21%\naccuracy improvement in terms of \\textbf{mAP} and \\textbf{Rank@1} for\n\\textbf{VehicleID (800)} dataset. For \\textbf{VeRi}, we achieve 35.45% and\n32.72% performance gains for \\textbf{Rank@1} and \\textbf{mAP}, respectively.</p>\n", "tags": ["Deep Hashing", "ANN Search", "Evaluation Metrics", "Hashing Methods"], "tsne_embedding": [-0.5800521969795227, 6.135335445404053], "cluster": 0}, {"key": "chen2021highly", "year": "2021", "citations": "4", "title": "SPANN: Highly-efficient Billion-scale Approximate Nearest Neighbor Search", "abstract": "<p>The in-memory algorithms for approximate nearest neighbor search (ANNS) have\nachieved great success for fast high-recall search, but are extremely expensive\nwhen handling very large scale database. Thus, there is an increasing request\nfor the hybrid ANNS solutions with small memory and inexpensive solid-state\ndrive (SSD). In this paper, we present a simple but efficient memory-disk\nhybrid indexing and search system, named SPANN, that follows the inverted index\nmethodology. It stores the centroid points of the posting lists in the memory\nand the large posting lists in the disk. We guarantee both disk-access\nefficiency (low latency) and high recall by effectively reducing the\ndisk-access number and retrieving high-quality posting lists. In the\nindex-building stage, we adopt a hierarchical balanced clustering algorithm to\nbalance the length of posting lists and augment the posting list by adding the\npoints in the closure of the corresponding clusters. In the search stage, we\nuse a query-aware scheme to dynamically prune the access of unnecessary posting\nlists. Experiment results demonstrate that SPANN is 2\\(\\times\\) faster than the\nstate-of-the-art ANNS solution DiskANN to reach the same recall quality \\(90%\\)\nwith same memory cost in three billion-scale datasets. It can reach \\(90%\\)\nrecall@1 and recall@10 in just around one millisecond with only 32GB memory\ncost. Code is available at:\n{\\footnotesize\\color{blue}{\\url{https://github.com/microsoft/SPTAG}}}.</p>\n", "tags": ["ANN Search", "Indexing", "Evaluation Metrics", "Tools and Libraries"], "tsne_embedding": [-2.0799477100372314, -8.304314613342285], "cluster": 6}, {"key": "chen2021spann", "year": "2021", "citations": "0", "title": "SPANN Highly-efficient Billion-scale Approximate Nearest Neighborhood Search", "abstract": "<p>The in-memory algorithms for approximate nearest neighbor search (ANNS) have achieved great success for fast high-recall search, but are extremely expensive when handling very large scale database. Thus, there is an increasing request for the hybrid ANNS solutions with small memory and inexpensive solid-state drive (SSD). In this paper, we present a simple but efficient memory-disk hybrid indexing and search system, named SPANN, that follows the inverted index methodology. It stores the centroid points of the posting lists in the memory and the large posting lists in the disk. We guarantee both disk-access efficiency (low  latency) and high recall by effectively reducing the disk-access number and retrieving high-quality posting lists. In the index-building stage, we adopt a hierarchical balanced clustering algorithm to balance the length of posting lists and augment the posting list by adding the points in the closure of the corresponding clusters. In the search stage, we use a query-aware scheme to dynamically prune the access of unnecessary posting lists.  Experiment results demonstrate that SPANN is 2X faster than the state-of-the-art ANNS solution DiskANN to reach the same recall quality 90% with same memory cost in three billion-scale datasets. It can reach 90% recall@1 and recall@10 in just around one millisecond with only 32GB memory cost.  Code is available at: https://github.com/microsoft/SPTAG.</p>\n", "tags": ["ANN Search", "Unsupervised", "Has Code", "NEURIPS"], "tsne_embedding": [-2.082489490509033, -8.424436569213867], "cluster": 6}, {"key": "chen2021towards", "year": "2021", "citations": "1", "title": "Towards Low-loss 1-bit Quantization Of User-item Representations For Top-k Recommendation", "abstract": "<p>Due to the promising advantages in space compression and inference\nacceleration, quantized representation learning for recommender systems has\nbecome an emerging research direction recently. As the target is to embed\nlatent features in the discrete embedding space, developing quantization for\nuser-item representations with a few low-precision integers confronts the\nchallenge of high information loss, thus leading to unsatisfactory performance\nin Top-K recommendation.\n  In this work, we study the problem of representation learning for\nrecommendation with 1-bit quantization. We propose a model named Low-loss\nQuantized Graph Convolutional Network (L^2Q-GCN). Different from previous work\nthat plugs quantization as the final encoder of user-item embeddings, L^2Q-GCN\nlearns the quantized representations whilst capturing the structural\ninformation of user-item interaction graphs at different semantic levels. This\nachieves the substantial retention of intermediate interactive information,\nalleviating the feature smoothing issue for ranking caused by numerical\nquantization. To further improve the model performance, we also present an\nadvanced solution named L^2Q-GCN-anl with quantization approximation and\nannealing training strategy. We conduct extensive experiments on four\nbenchmarks over Top-K recommendation task. The experimental results show that,\nwith nearly 9x representation storage compression, L^2Q-GCN-anl attains about\n90~99% performance recovery compared to the state-of-the-art model.</p>\n", "tags": ["Quantization", "Efficiency and Optimization", "Applications"], "tsne_embedding": [3.5408191680908203, -7.280214309692383], "cluster": 1}, {"key": "chen2021transformer", "year": "2021", "citations": "40", "title": "Transhash: Transformer-based Hamming Hashing For Efficient Image Retrieval", "abstract": "<p>Deep hamming hashing has gained growing popularity in approximate nearest\nneighbour search for large-scale image retrieval. Until now, the deep hashing\nfor the image retrieval community has been dominated by convolutional neural\nnetwork architectures, e.g. \\texttt{Resnet}\\cite{he2016deep}. In this paper,\ninspired by the recent advancements of vision transformers, we present\n\\textbf{Transhash}, a pure transformer-based framework for deep hashing\nlearning. Concretely, our framework is composed of two major modules: (1) Based\non \\textit{Vision Transformer} (ViT), we design a siamese vision transformer\nbackbone for image feature extraction. To learn fine-grained features, we\ninnovate a dual-stream feature learning on top of the transformer to learn\ndiscriminative global and local features. (2) Besides, we adopt a Bayesian\nlearning scheme with a dynamically constructed similarity matrix to learn\ncompact binary hash codes. The entire framework is jointly trained in an\nend-to-end manner.~To the best of our knowledge, this is the first work to\ntackle deep hashing learning problems without convolutional neural networks\n(\\textit{CNNs}). We perform comprehensive experiments on three widely-studied\ndatasets: \\textbf{CIFAR-10}, \\textbf{NUSWIDE} and \\textbf{IMAGENET}. The\nexperiments have evidenced our superiority against the existing\nstate-of-the-art deep hashing methods. Specifically, we achieve 8.2%, 2.6%,\n12.7% performance gains in terms of average \\textit{mAP} for different hash\nbit lengths on three public datasets, respectively.</p>\n", "tags": ["Applications", "Deep Hashing", "ANN Search", "Evaluation Metrics", "Benchmarks and Datasets", "Hashing Methods"], "tsne_embedding": [8.026028633117676, 2.5915610790252686], "cluster": 3}, {"key": "chen2022approximate", "year": "2022", "citations": "10", "title": "Approximate Nearest Neighbor Search Under Neural Similarity Metric For Large-scale Recommendation", "abstract": "<p>Model-based methods for recommender systems have been studied extensively for\nyears. Modern recommender systems usually resort to 1) representation learning\nmodels which define user-item preference as the distance between their\nembedding representations, and 2) embedding-based Approximate Nearest Neighbor\n(ANN) search to tackle the efficiency problem introduced by large-scale corpus.\nWhile providing efficient retrieval, the embedding-based retrieval pattern also\nlimits the model capacity since the form of user-item preference measure is\nrestricted to the distance between their embedding representations. However,\nfor other more precise user-item preference measures, e.g., preference scores\ndirectly derived from a deep neural network, they are computationally\nintractable because of the lack of an efficient retrieval method, and an\nexhaustive search for all user-item pairs is impractical. In this paper, we\npropose a novel method to extend ANN search to arbitrary matching functions,\ne.g., a deep neural network. Our main idea is to perform a greedy walk with a\nmatching function in a similarity graph constructed from all items. To solve\nthe problem that the similarity measures of graph construction and user-item\nmatching function are heterogeneous, we propose a pluggable adversarial\ntraining task to ensure the graph search with arbitrary matching function can\nachieve fairly high precision. Experimental results in both open source and\nindustry datasets demonstrate the effectiveness of our method. The proposed\nmethod has been fully deployed in the Taobao display advertising platform and\nbrings a considerable advertising revenue increase. We also summarize our\ndetailed experiences in deployment in this paper.</p>\n", "tags": ["ANN Search", "Graph", "Supervised", "Applications"], "tsne_embedding": [-2.892854928970337, -3.7608656883239746], "cluster": 7}, {"key": "chen2022fast", "year": "2022", "citations": "11", "title": "FINGER: Fast Inference For Graph-based Approximate Nearest Neighbor Search", "abstract": "<p>Approximate K-Nearest Neighbor Search (AKNNS) has now become ubiquitous in\nmodern applications, for example, as a fast search procedure with two tower\ndeep learning models. Graph-based methods for AKNNS in particular have received\ngreat attention due to their superior performance. These methods rely on greedy\ngraph search to traverse the data points as embedding vectors in a database.\nUnder this greedy search scheme, we make a key observation: many distance\ncomputations do not influence search updates so these computations can be\napproximated without hurting performance. As a result, we propose FINGER, a\nfast inference method to achieve efficient graph search. FINGER approximates\nthe distance function by estimating angles between neighboring residual vectors\nwith low-rank bases and distribution matching. The approximated distance can be\nused to bypass unnecessary computations, which leads to faster searches.\nEmpirically, accelerating a popular graph-based method named HNSW by FINGER is\nshown to outperform existing graph-based methods by 20%-60% across different\nbenchmark datasets.</p>\n", "tags": ["ANN Search", "Applications", "Evaluation Metrics", "Benchmarks and Datasets"], "tsne_embedding": [2.6198925971984863, -10.549349784851074], "cluster": 1}, {"key": "chen2022learning", "year": "2022", "citations": "21", "title": "Learning Binarized Graph Representations With Multi-faceted Quantization Reinforcement For Top-k Recommendation", "abstract": "<p>Learning vectorized embeddings is at the core of various recommender systems\nfor user-item matching. To perform efficient online inference, representation\nquantization, aiming to embed the latent features by a compact sequence of\ndiscrete numbers, recently shows the promising potentiality in optimizing both\nmemory and computation overheads. However, existing work merely focuses on\nnumerical quantization whilst ignoring the concomitant information loss issue,\nwhich, consequently, leads to conspicuous performance degradation. In this\npaper, we propose a novel quantization framework to learn Binarized Graph\nRepresentations for Top-K Recommendation (BiGeaR). BiGeaR introduces\nmulti-faceted quantization reinforcement at the pre-, mid-, and post-stage of\nbinarized representation learning, which substantially retains the\nrepresentation informativeness against embedding binarization. In addition to\nsaving the memory footprint, BiGeaR further develops solid online inference\nacceleration with bitwise operations, providing alternative flexibility for the\nrealistic deployment. The empirical results over five large real-world\nbenchmarks show that BiGeaR achieves about 22%~40% performance improvement over\nthe state-of-the-art quantization-based recommender system, and recovers about\n95%~102% of the performance capability of the best full-precision counterpart\nwith over 8x time and space reduction.</p>\n", "tags": ["Quantization", "Evaluation Metrics", "Applications"], "tsne_embedding": [3.741896390914917, -7.467222690582275], "cluster": 1}, {"key": "chen2022locality", "year": "2022", "citations": "4", "title": "Locality-sensitive Bucketing Functions For The Edit Distance", "abstract": "<p>Many bioinformatics applications involve bucketing a set of sequences where\neach sequence is allowed to be assigned into multiple buckets. To achieve both\nhigh sensitivity and precision, bucketing methods are desired to assign similar\nsequences into the same bucket while assigning dissimilar sequences into\ndistinct buckets. Existing \\(k\\)-mer-based bucketing methods have been efficient\nin processing sequencing data with low error rate, but encounter much reduced\nsensitivity on data with high error rate. Locality-sensitive hashing (LSH)\nschemes are able to mitigate this issue through tolerating the edits in similar\nsequences, but state-of-the-art methods still have large gaps. Here we\ngeneralize the LSH function by allowing it to hash one sequence into multiple\nbuckets. Formally, a bucketing function, which maps a sequence (of fixed\nlength) into a subset of buckets, is defined to be \\((d_1, d_2)\\)-sensitive if\nany two sequences within an edit distance of \\(d_1\\) are mapped into at least one\nshared bucket, and any two sequences with distance at least \\(d_2\\) are mapped\ninto disjoint subsets of buckets. We construct locality-sensitive bucketing\n(LSB) functions with a variety of values of \\((d_1,d_2)\\) and analyze their\nefficiency with respect to the total number of buckets needed as well as the\nnumber of buckets that a specific sequence is mapped to. We also prove lower\nbounds of these two parameters in different settings and show that some of our\nconstructed LSB functions are optimal. These results provide theoretical\nfoundations for their practical use in analyzing sequences with high error rate\nwhile also providing insights for the hardness of designing ungapped LSH\nfunctions.</p>\n", "tags": ["Hashing Methods", "Evaluation Metrics", "Applications"], "tsne_embedding": [-7.149776458740234, -6.536997318267822], "cluster": 7}, {"key": "chen2023bipartite", "year": "2023", "citations": "10", "title": "Bipartite Graph Convolutional Hashing For Effective And Efficient Top-n Search In Hamming Space", "abstract": "<p>Searching on bipartite graphs is basal and versatile to many real-world Web\napplications, e.g., online recommendation, database retrieval, and\nquery-document searching. Given a query node, the conventional approaches rely\non the similarity matching with the vectorized node embeddings in the\ncontinuous Euclidean space. To efficiently manage intensive similarity\ncomputation, developing hashing techniques for graph structured data has\nrecently become an emerging research direction. Despite the retrieval\nefficiency in Hamming space, prior work is however confronted with catastrophic\nperformance decay. In this work, we investigate the problem of hashing with\nGraph Convolutional Network on bipartite graphs for effective Top-N search. We\npropose an end-to-end Bipartite Graph Convolutional Hashing approach, namely\nBGCH, which consists of three novel and effective modules: (1) adaptive graph\nconvolutional hashing, (2) latent feature dispersion, and (3) Fourier\nserialized gradient estimation. Specifically, the former two modules achieve\nthe substantial retention of the structural information against the inevitable\ninformation loss in hash encoding; the last module develops Fourier Series\ndecomposition to the hashing function in the frequency domain mainly for more\naccurate gradient estimation. The extensive experiments on six real-world\ndatasets not only show the performance superiority over the competing\nhashing-based counterparts, but also demonstrate the effectiveness of all\nproposed model components contained therein.</p>\n", "tags": ["Hashing Methods", "Applications", "Efficient Learning"], "tsne_embedding": [4.618910312652588, -9.131377220153809], "cluster": 1}, {"key": "chen2023supervised", "year": "2023", "citations": "0", "title": "Supervised Auto-encoding Twin-bottleneck Hashing", "abstract": "<p>Deep hashing has shown to be a complexity-efficient solution for the\nApproximate Nearest Neighbor search problem in high dimensional space. Many\nmethods usually build the loss function from pairwise or triplet data points to\ncapture the local similarity structure. Other existing methods construct the\nsimilarity graph and consider all points simultaneously. Auto-encoding\nTwin-bottleneck Hashing is one such method that dynamically builds the graph.\nSpecifically, each input data is encoded into a binary code and a continuous\nvariable, or the so-called twin bottlenecks. The similarity graph is then\ncomputed from these binary codes, which get updated consistently during the\ntraining. In this work, we generalize the original model into a supervised deep\nhashing network by incorporating the label information. In addition, we examine\nthe differences of codes structure between these two networks and consider the\nclass imbalance problem especially in multi-labeled datasets. Experiments on\nthree datasets yield statistically significant improvement against the original\nmodel. Results are also comparable and competitive to other supervised methods.</p>\n", "tags": ["Deep Hashing", "ANN Search", "Supervised", "Hashing Methods"], "tsne_embedding": [4.68628454208374, 5.1907453536987305], "cluster": 5}, {"key": "chen2024deep", "year": "2024", "citations": "1", "title": "Deep Class-guided Hashing For Multi-label Cross-modal Retrieval", "abstract": "<p>Deep hashing, due to its low cost and efficient retrieval advantages, is\nwidely valued in cross-modal retrieval. However, existing cross-modal hashing\nmethods either explore the relationships between data points, which inevitably\nleads to intra-class dispersion, or explore the relationships between data\npoints and categories while ignoring the preservation of inter-class structural\nrelationships, resulting in the generation of suboptimal hash codes. How to\nmaintain both intra-class aggregation and inter-class structural relationships,\nIn response to this issue, this paper proposes a DCGH method. Specifically, we\nuse proxy loss as the mainstay to maintain intra-class aggregation of data,\ncombined with pairwise loss to maintain inter-class structural relationships,\nand on this basis, further propose a variance constraint to address the\nsemantic bias issue caused by the combination. A large number of comparative\nexperiments on three benchmark datasets show that the DCGH method has\ncomparable or even better performance compared to existing cross-modal\nretrieval methods. The code for the implementation of our DCGH framework is\navailable at https://github.com/donnotnormal/DCGH.</p>\n", "tags": ["Deep Hashing", "Multi-Modal Hashing", "Loss Functions", "Benchmarks and Datasets", "Hashing Methods"], "tsne_embedding": [5.2809576988220215, 3.4738755226135254], "cluster": 9}, {"key": "chen2024efficient", "year": "2024", "citations": "0", "title": "Efficient Ternary Weight Embedding Model: Bridging Scalability And Performance", "abstract": "<p>Embedding models have become essential tools in both natural language\nprocessing and computer vision, enabling efficient semantic search,\nrecommendation, clustering, and more. However, the high memory and\ncomputational demands of full-precision embeddings pose challenges for\ndeployment in resource-constrained environments, such as real-time\nrecommendation systems. In this work, we propose a novel finetuning framework\nto ternary-weight embedding models, which reduces memory and computational\noverhead while maintaining high performance. To apply ternarization to\npre-trained embedding models, we introduce self-taught knowledge distillation\nto finalize the ternary-weights of the linear layers. With extensive\nexperiments on public text and vision datasets, we demonstrated that without\nsacrificing effectiveness, the ternarized model consumes low memory usage and\nhas low latency in the inference stage with great efficiency. In practical\nimplementations, embedding models are typically integrated with Approximate\nNearest Neighbor (ANN) search. Our experiments combining ternary embedding with\nANN search yielded impressive improvement in both accuracy and computational\nefficiency. The repository is available at here.</p>\n", "tags": ["ANN Search", "Evaluation Metrics", "Tools and Libraries"], "tsne_embedding": [3.460198402404785, -5.586162090301514], "cluster": 1}, {"key": "chen2024enhanced", "year": "2024", "citations": "29", "title": "Enhanced Discrete Multi-modal Hashing More Constraints Yet Less Time To Learn", "abstract": "<p>Due to the exponential growth of multimedia data, multi-modal hashing as a promising technique to make cross-view retrieval scalable is attracting more and more attention. However, most of the existing multi-modal hashing methods either divide the learning process unnaturally into two separate stages or treat the discrete optimization problem simplistically as a continuous one, which leads to suboptimal results. Recently, a few discrete multi-modal hashing methods that try to address such issues have emerged, but they still ignore several important discrete constraints (such as the balance and decorrelation of hash bits). In this paper, we overcome those limitations by proposing a novel method named \u201cEnhanced Discrete Multi-modal Hashing (EDMH)\u201d which learns binary codes and hashing functions simultaneously from the pairwise similarity matrix of data, under the aforementioned discrete constraints. Although the model of EDMH looks a lot more complex than the other models for multi-modal hashing, we are actually able to develop a fast iterative learning algorithm for it, since the subproblems of its optimization all have closed-form solutions after introducing two auxiliary variables. Our experimental results on three real-world datasets have demonstrated that EDMH not only performs much better than state-of-the-art competitors but also runs much faster than them.</p>\n", "tags": ["Multi-Modal Hashing", "Hashing Methods", "Efficient Learning"], "tsne_embedding": [2.7943406105041504, 8.319357872009277], "cluster": 5}, {"key": "chen2024long", "year": "2024", "citations": "8", "title": "Long-tail Hashing", "abstract": "<p>Hashing, which represents data items as compact binary codes, has\nbeen becoming a more and more popular technique, e.g., for large-scale image retrieval, owing to its super fast search speed as well\nas its extremely economical memory consumption. However, existing hashing methods all try to learn binary codes from artificially\nbalanced datasets which are not commonly available in real-world\nscenarios. In this paper, we propose Long-Tail Hashing Network\n(LTHNet), a novel two-stage deep hashing approach that addresses\nthe problem of learning to hash for more realistic datasets where\nthe data labels roughly exhibit a long-tail distribution. Specifically,\nthe first stage is to learn relaxed embeddings of the given dataset\nwith its long-tail characteristic taken into account via an end-to-end deep neural network; the second stage is to binarize those\nobtained embeddings. A critical part of LTHNet is its extended dynamic meta-embedding module which can adaptively realize visual\nknowledge transfer between head and tail classes, and thus enrich\nimage representations for hashing. Our experiments have shown\nthat LTHNet achieves dramatic performance improvements over all\nstate-of-the-art competitors on long-tail datasets, with no or little\nsacrifice on balanced datasets. Further analyses reveal that while to\nour surprise directly manipulating class weights in the loss function\nhas little effect, the extended dynamic meta-embedding module, the\nusage of cross-entropy loss instead of square loss, and the relatively\nsmall batch-size for training all contribute to LTHNet\u2019s success.</p>\n", "tags": ["Deep Hashing", "Supervised", "Image Retrieval", "Loss Functions"], "tsne_embedding": [0.6095582246780396, 4.288059711456299], "cluster": 0}, {"key": "chen2024strongly", "year": "2024", "citations": "48", "title": "Strongly Constrained Discrete Hashing", "abstract": "<p>Learning to hash is a fundamental technique widely used in large-scale image retrieval. Most existing methods for learning to hash address the involved discrete optimization problem by the continuous relaxation of the binary constraint, which usually leads to large quantization errors and consequently suboptimal binary codes. A few discrete hashing methods have emerged recently. However, they either completely ignore some useful constraints (specifically the balance and decorrelation of hash bits) or just turn those constraints into regularizers that would make the optimization easier but less accurate. In this paper, we propose a novel supervised hashing method named Strongly Constrained Discrete Hashing (SCDH) which overcomes such limitations. It can learn the binary codes for all examples in the training set, and meanwhile obtain a hash function for unseen samples with the above mentioned constraints preserved. Although the model of SCDH is fairly sophisticated, we are able to find closed-form solutions to all of its optimization subproblems and thus design an efficient algorithm that converges quickly. In addition, we extend SCDH to a kernelized version SCDH K . Our experiments on three large benchmark datasets have demonstrated that not only can SCDH and SCDH K achieve substantially higher MAP scores than state-of-the-art baselines, but they train much faster than those that are also supervised as well.</p>\n", "tags": ["Quantisation", "Supervised", "Efficiency and Optimization", "Image Retrieval"], "tsne_embedding": [2.4830617904663086, 7.142846584320068], "cluster": 5}, {"key": "chen2024supervised", "year": "2024", "citations": "2", "title": "Supervised Consensus Anchor Graph Hashing For Cross Modal Retrieval", "abstract": "<p>The target of cross-modal hashing is to embed heterogeneous multimedia data into a common low-dimensional Hamming space, which plays a pivotal part in multimedia retrieval due to the emergence of big multimodal data. Recently, matrix factorization has achieved great success in cross-modal hashing. However, how to effectively use label information and local geometric structure is still a challenging problem for these approaches. To address this issue, we propose a cross-modal hashing method based on collective matrix factorization, which considers both the label consistency across different modalities and the local geometric consistency in each modality. These two elements are formulated as a graph Laplacian term in the objective function, leading to a substantial improvement on the discriminative power of latent semantic features obtained by collective matrix factorization. Moreover, the proposed method learns unified hash codes for different modalities of an instance to facilitate cross-modal search, and the objective function is solved using an iterative strategy. The experimental results on two benchmark data sets show the effectiveness of the proposed method and its superiority over state-of-the-art cross-modal hashing methods.</p>\n", "tags": ["Cross Modal", "Graph", "Supervised"], "tsne_embedding": [5.503479480743408, -7.064657211303711], "cluster": 1}, {"key": "chen2024towards", "year": "2024", "citations": "0", "title": "Towards Effective Top-n Hamming Search Via Bipartite Graph Contrastive Hashing", "abstract": "<p>Searching on bipartite graphs serves as a fundamental task for various\nreal-world applications, such as recommendation systems, database retrieval,\nand document querying. Conventional approaches rely on similarity matching in\ncontinuous Euclidean space of vectorized node embeddings. To handle intensive\nsimilarity computation efficiently, hashing techniques for graph-structured\ndata have emerged as a prominent research direction. However, despite the\nretrieval efficiency in Hamming space, previous studies have encountered\ncatastrophic performance decay. To address this challenge, we investigate the\nproblem of hashing with Graph Convolutional Network for effective Top-N search.\nOur findings indicate the learning effectiveness of incorporating hashing\ntechniques within the exploration of bipartite graph reception fields, as\nopposed to simply treating hashing as post-processing to output embeddings. To\nfurther enhance the model performance, we advance upon these findings and\npropose Bipartite Graph Contrastive Hashing (BGCH+). BGCH+ introduces a novel\ndual augmentation approach to both intermediate information and hash code\noutputs in the latent feature spaces, thereby producing more expressive and\nrobust hash codes within a dual self-supervised learning paradigm.\nComprehensive empirical analyses on six real-world benchmarks validate the\neffectiveness of our dual feature contrastive learning in boosting the\nperformance of BGCH+ compared to existing approaches.</p>\n", "tags": ["Applications", "Evaluation Metrics", "Supervised", "Hashing Methods"], "tsne_embedding": [4.555278778076172, -8.91562271118164], "cluster": 1}, {"key": "chen2024two", "year": "2024", "citations": "43", "title": "A Two-step Cross-modal Hashing By Exploiting Label Correlations And Preserving Similarity In Both Steps", "abstract": "<p>In this paper, we present a novel Two-stEp Cross-modal Hashing method, TECH for short, for cross-modal retrieval tasks. As a two-step method, it first learns hash codes based on semantic labels, while preserving the similarity in the original space and exploiting the label correlations in the label space. In the light of this, it is able to make better use of label information and generate better binary codes. In addition, different from other two-step methods that mainly focus on the hash codes learning, TECH adopts a new hash function learning strategy in the second step, which also preserves the similarity in the original space. Moreover, with the help of well designed objective function and optimization scheme, it is able to generate hash codes discretely and scalable for large scale data. To the best of our knowledge, it is the first cross-modal hashing method exploiting label correlations, and also the first two-step hashing model preserving the similarity while leaning hash function. Extensive experiments demonstrate that the proposed approach outperforms some state-of-the-art cross-modal hashing methods.</p>\n", "tags": ["Cross Modal", "Hashing Methods", "Supervision Type", "Loss Functions", "Efficient Learning"], "tsne_embedding": [0.4276047646999359, 5.706540584564209], "cluster": 0}, {"key": "chen2025learning", "year": "2025", "citations": "0", "title": "Learning Binarized Representations With Pseudo-positive Sample Enhancement For Efficient Graph Collaborative Filtering", "abstract": "<p>Learning vectorized embeddings is fundamental to many recommender systems for user-item matching. To enable efficient online inference, representation binarization, which embeds latent features into compact binary sequences, has recently shown significant promise in optimizing both memory usage and computational overhead. However, existing approaches primarily focus on numerical quantization, neglecting the associated information loss, which often results in noticeable performance degradation. To address these issues, we study the problem of graph representation binarization for efficient collaborative filtering. Our findings indicate that explicitly mitigating information loss at various stages of embedding binarization has a significant positive impact on performance. Building on these insights, we propose an enhanced framework, BiGeaR++, which specifically leverages supervisory signals from pseudo-positive samples, incorporating both real item data and latent embedding samples. Compared to its predecessor BiGeaR, BiGeaR++ introduces a fine-grained inference distillation mechanism and an effective embedding sample synthesis approach. Empirical evaluations across five real-world datasets demonstrate that the new designs in BiGeaR++ work seamlessly well with other modules, delivering substantial improvements of around 1%-10% over BiGeaR and thus achieving state-of-the-art performance compared to the competing methods. Our implementation is available at https://github.com/QueYork/BiGeaR-SS.</p>\n", "tags": ["Quantization", "Tools and Libraries", "Has Code"], "tsne_embedding": [4.065821647644043, -7.319435119628906], "cluster": 1}, {"key": "cheng2018crh", "year": "2018", "citations": "2", "title": "CRH A Simple Benchmark Approach To Continuous Hashing", "abstract": "<p>In recent years, the distinctive advancement of handling huge data promotes\nthe evolution of ubiquitous computing and analysis technologies. With the\nconstantly upward system burden and computational complexity, adaptive coding\nhas been a fascinating topic for pattern analysis, with outstanding\nperformance. In this work, a continuous hashing method, termed continuous\nrandom hashing (CRH), is proposed to encode sequential data stream, while\nignorance of previously hashing knowledge is possible. Instead, a random\nselection idea is adopted to adaptively approximate the differential encoding\npatterns of data stream, e.g., streaming media, and iteration is avoided for\nstepwise learning. Experimental results demonstrate our method is able to\nprovide outstanding performance, as a benchmark approach to continuous hashing.</p>\n", "tags": ["Hashing Methods", "Efficient Learning", "Evaluation Metrics"], "tsne_embedding": [11.775029182434082, 7.19081449508667], "cluster": 3}, {"key": "cheng2018simple", "year": "2018", "citations": "2", "title": "CRH: A Simple Benchmark Approach To Continuous Hashing", "abstract": "<p>In recent years, the distinctive advancement of handling huge data promotes\nthe evolution of ubiquitous computing and analysis technologies. With the\nconstantly upward system burden and computational complexity, adaptive coding\nhas been a fascinating topic for pattern analysis, with outstanding\nperformance. In this work, a continuous hashing method, termed continuous\nrandom hashing (CRH), is proposed to encode sequential data stream, while\nignorance of previously hashing knowledge is possible. Instead, a random\nselection idea is adopted to adaptively approximate the differential encoding\npatterns of data stream, e.g., streaming media, and iteration is avoided for\nstepwise learning. Experimental results demonstrate our method is able to\nprovide outstanding performance, as a benchmark approach to continuous hashing.</p>\n", "tags": ["Hashing Methods", "Evaluation Metrics"], "tsne_embedding": [11.775029182434082, 7.19081449508667], "cluster": 3}, {"key": "cheng2024robust", "year": "2024", "citations": "46", "title": "Robust Unsupervised Cross-modal Hashing For Multimedia Retrieval", "abstract": "<p>With the quick development of social websites, there are more opportunities to have different media types (such as text, image, video, etc.) describing the same topic from large-scale heterogeneous data sources. To efficiently identify the inter-media correlations for multimedia retrieval, unsupervised cross-modal hashing (UCMH) has gained increased interest due to the significant reduction in computation and storage. However, most UCMH methods assume that the data from different modalities are well paired. As a result, existing UCMH methods may not achieve satisfactory performance when partially paired data are given only. In this article, we propose a new-type of UCMH method called robust unsupervised cross-modal hashing (RUCMH). The major contribution lies in jointly learning modal-specific hash function, exploring the correlations among modalities with partial or even without any pairwise correspondence, and preserving the information of original features as much as possible. The learning process can be modeled via a joint minimization problem, and the corresponding optimization algorithm is presented. A series of experiments is conducted on four real-world datasets (Wiki, MIRFlickr, NUS-WIDE, and MS-COCO). The results demonstrate that RUCMH can significantly outperform the state-of-the-art unsupervised cross-modal hashing methods, especially for the partially paired case, which validates the effectiveness of RUCMH.</p>\n", "tags": ["Cross Modal", "Unsupervised"], "tsne_embedding": [6.2292022705078125, 5.049871444702148], "cluster": 3}, {"key": "choromanska2015binary", "year": "2015", "citations": "27", "title": "Binary Embeddings With Structured Hashed Projections", "abstract": "<p>We consider the hashing mechanism for constructing binary embeddings, that\ninvolves pseudo-random projections followed by nonlinear (sign function)\nmappings. The pseudo-random projection is described by a matrix, where not all\nentries are independent random variables but instead a fixed \u201cbudget of\nrandomness\u201d is distributed across the matrix. Such matrices can be efficiently\nstored in sub-quadratic or even linear space, provide reduction in randomness\nusage (i.e. number of required random values), and very often lead to\ncomputational speed ups. We prove several theoretical results showing that\nprojections via various structured matrices followed by nonlinear mappings\naccurately preserve the angular distance between input high-dimensional\nvectors. To the best of our knowledge, these results are the first that give\ntheoretical ground for the use of general structured matrices in the nonlinear\nsetting. In particular, they generalize previous extensions of the\nJohnson-Lindenstrauss lemma and prove the plausibility of the approach that was\nso far only heuristically confirmed for some special structured matrices.\nConsequently, we show that many structured matrices can be used as an efficient\ninformation compression mechanism. Our findings build a better understanding of\ncertain deep architectures, which contain randomly weighted and untrained\nlayers, and yet achieve high performance on different learning tasks. We\nempirically verify our theoretical findings and show the dependence of learning\nvia structured hashed projections on the performance of neural network as well\nas nearest neighbor classifier.</p>\n", "tags": ["Hashing Methods", "Evaluation Metrics", "Deep Hashing", "Efficient Learning", "ANN Search"], "tsne_embedding": [-1.1481577157974243, 4.655131816864014], "cluster": 0}, {"key": "choromanski2015efficient", "year": "2015", "citations": "0", "title": "Efficient Data Hashing With Structured Binary Embeddings", "abstract": "<p>We present here new mechanisms for hashing data via binary embeddings.\nContrary to most of the techniques presented before, the embedding matrix of\nour mechanism is highly structured. That enables us to perform hashing more\nefficiently and use less memory. What is crucial and nonintuitive is the fact\nthat imposing structured mechanism does not affect the quality of the produced\nhash. To the best of our knowledge, we are the first to give strong theoretical\nguarantees of the proposed binary hashing method by proving the efficiency of\nthe mechanism for several classes of structured projection matrices. As a\ncorollary, we obtain binary hashing mechanisms with strong concentration\nresults for circulant and Topelitz matrices. Our approach is however much more\ngeneral.</p>\n", "tags": ["Hashing Methods", "Efficient Learning"], "tsne_embedding": [-2.1123111248016357, 5.865850448608398], "cluster": 0}, {"key": "christiani2016framework", "year": "2016", "citations": "12", "title": "A Framework For Similarity Search With Space-time Tradeoffs Using Locality-sensitive Filtering", "abstract": "<p>We present a framework for similarity search based on Locality-Sensitive\nFiltering (LSF), generalizing the Indyk-Motwani (STOC 1998) Locality-Sensitive\nHashing (LSH) framework to support space-time tradeoffs. Given a family of\nfilters, defined as a distribution over pairs of subsets of space with certain\nlocality-sensitivity properties, we can solve the approximate near neighbor\nproblem in \\(d\\)-dimensional space for an \\(n\\)-point data set with query time\n\\(dn^{\\rho_q+o(1)}\\), update time \\(dn^{\\rho_u+o(1)}\\), and space usage \\(dn + n^{1</p>\n<ul>\n  <li>\\rho_u + o(1)}\\). The space-time tradeoff is tied to the tradeoff between\nquery time and update time, controlled by the exponents \\(\\rho_q, \\rho_u\\) that\nare determined by the filter family. Locality-sensitive filtering was\nintroduced by Becker et al. (SODA 2016) together with a framework yielding a\nsingle, balanced, tradeoff between query time and space, further relying on the\nassumption of an efficient oracle for the filter evaluation algorithm. We\nextend the LSF framework to support space-time tradeoffs and through a\ncombination of existing techniques we remove the oracle assumption.\nBuilding on a filter family for the unit sphere by Laarhoven (arXiv 2015) we\nuse a kernel embedding technique by Rahimi &amp; Recht (NIPS 2007) to show a\nsolution to the \\((r,cr)\\)-near neighbor problem in \\(\\ell_s^d\\)-space for \\(0 &lt; s\n\\leq 2\\) with query and update exponents\n\\(\\rho_q=\\frac{c^s(1+\\lambda)^2}{(c^s+\\lambda)^2}\\) and\n\\(\\rho_u=\\frac{c^s(1-\\lambda)^2}{(c^s+\\lambda)^2}\\) where \\(\\lambda\\in[-1,1]\\) is a\ntradeoff parameter. This result improves upon the space-time tradeoff of\nKapralov (PODS 2015) and is shown to be optimal in the case of a balanced\ntradeoff. Finally, we show a lower bound for the space-time tradeoff on the\nunit sphere that matches Laarhoven\u2019s and our own upper bound in the case of\nrandom data.</li>\n</ul>\n", "tags": ["Hashing Methods", "ANN Search"], "tsne_embedding": [-15.004936218261719, -7.123409748077393], "cluster": 2}, {"key": "christiani2016set", "year": "2016", "citations": "24", "title": "Set Similarity Search Beyond Minhash", "abstract": "<p>We consider the problem of approximate set similarity search under\nBraun-Blanquet similarity \\(B(\\mathbf{x}, \\mathbf{y}) = |\\mathbf{x} \\cap\n\\mathbf{y}| / \\max(|\\mathbf{x}|, |\\mathbf{y}|)\\). The \\((b_2, b_2)\\)-approximate\nBraun-Blanquet similarity search problem is to preprocess a collection of sets\n\\(P\\) such that, given a query set \\(\\mathbf{q}\\), if there exists \\(\\mathbf{x} \\in\nP\\) with \\(B(\\mathbf{q}, \\mathbf{x}) \\geq b_1\\), then we can efficiently return\n\\(\\mathbf{x}\u2019 \\in P\\) with \\(B(\\mathbf{q}, \\mathbf{x}\u2019) &gt; b_2\\).\n  We present a simple data structure that solves this problem with space usage\n\\(O(n^{1+\\rho}log n + \\sum_{\\mathbf{x} \\in P}|\\mathbf{x}|)\\) and query time\n\\(O(|\\mathbf{q}|n^{\\rho} log n)\\) where \\(n = |P|\\) and \\(\\rho =\nlog(1/b_1)/log(1/b_2)\\). Making use of existing lower bounds for\nlocality-sensitive hashing by O\u2019Donnell et al. (TOCT 2014) we show that this\nvalue of \\(\\rho\\) is tight across the parameter space, i.e., for every choice of\nconstants \\(0 &lt; b_2 &lt; b_1 &lt; 1\\).\n  In the case where all sets have the same size our solution strictly improves\nupon the value of \\(\\rho\\) that can be obtained through the use of\nstate-of-the-art data-independent techniques in the Indyk-Motwani\nlocality-sensitive hashing framework (STOC 1998) such as Broder\u2019s MinHash (CCS\n1997) for Jaccard similarity and Andoni et al.\u2019s cross-polytope LSH (NIPS 2015)\nfor cosine similarity. Surprisingly, even though our solution is\ndata-independent, for a large part of the parameter space we outperform the\ncurrently best data-dependent method by Andoni and Razenshteyn (STOC 2015).</p>\n", "tags": ["Hashing Methods", "ANN Search"], "tsne_embedding": [-13.73630142211914, -7.871171474456787], "cluster": 2}, {"key": "christiani2017fast", "year": "2017", "citations": "10", "title": "Fast Locality-sensitive Hashing Frameworks For Approximate Near Neighbor Search", "abstract": "<p>The Indyk-Motwani Locality-Sensitive Hashing (LSH) framework (STOC 1998) is a\ngeneral technique for constructing a data structure to answer approximate near\nneighbor queries by using a distribution \\(\\mathcal{H}\\) over locality-sensitive\nhash functions that partition space. For a collection of \\(n\\) points, after\npreprocessing, the query time is dominated by \\(O(n^{\\rho} log n)\\) evaluations\nof hash functions from \\(\\mathcal{H}\\) and \\(O(n^{\\rho})\\) hash table lookups and\ndistance computations where \\(\\rho \\in (0,1)\\) is determined by the\nlocality-sensitivity properties of \\(\\mathcal{H}\\). It follows from a recent\nresult by Dahlgaard et al. (FOCS 2017) that the number of locality-sensitive\nhash functions can be reduced to \\(O(log^2 n)\\), leaving the query time to be\ndominated by \\(O(n^{\\rho})\\) distance computations and \\(O(n^{\\rho} log n)\\)\nadditional word-RAM operations. We state this result as a general framework and\nprovide a simpler analysis showing that the number of lookups and distance\ncomputations closely match the Indyk-Motwani framework, making it a viable\nreplacement in practice. Using ideas from another locality-sensitive hashing\nframework by Andoni and Indyk (SODA 2006) we are able to reduce the number of\nadditional word-RAM operations to \\(O(n^\\rho)\\).</p>\n", "tags": ["Hashing Methods", "ANN Search"], "tsne_embedding": [-14.045589447021484, -6.140883445739746], "cluster": 2}, {"key": "christiani2018confirmation", "year": "2018", "citations": "1", "title": "Confirmation Sampling For Exact Nearest Neighbor Search", "abstract": "<p>Locality-sensitive hashing (LSH), introduced by Indyk and Motwani in STOC\n\u201898, has been an extremely influential framework for nearest neighbor search in\nhigh-dimensional data sets. While theoretical work has focused on the\napproximate nearest neighbor problems, in practice LSH data structures with\nsuitably chosen parameters are used to solve the exact nearest neighbor problem\n(with some error probability). Sublinear query time is often possible in\npractice even for exact nearest neighbor search, intuitively because the\nnearest neighbor tends to be significantly closer than other data points.\nHowever, theory offers little advice on how to choose LSH parameters outside of\npre-specified worst-case settings.\n  We introduce the technique of confirmation sampling for solving the exact\nnearest neighbor problem using LSH. First, we give a general reduction that\ntransforms a sequence of data structures that each find the nearest neighbor\nwith a small, unknown probability, into a data structure that returns the\nnearest neighbor with probability \\(1-\\delta\\), using as few queries as possible.\nSecond, we present a new query algorithm for the LSH Forest data structure with\n\\(L\\) trees that is able to return the exact nearest neighbor of a query point\nwithin the same time bound as an LSH Forest of \\(\u03a9(L)\\) trees with internal\nparameters specifically tuned to the query and data.</p>\n", "tags": ["ANN Search", "Hashing Methods"], "tsne_embedding": [-7.154738426208496, 1.53426194190979], "cluster": 4}, {"key": "christiani2019algorithms", "year": "2019", "citations": "0", "title": "Algorithms For Similarity Search And Pseudorandomness", "abstract": "<p>We study the problem of approximate near neighbor (ANN) search and show the\nfollowing results:</p>\n<ul>\n  <li>An improved framework for solving the ANN problem using locality-sensitive\nhashing, reducing the number of evaluations of locality-sensitive hash\nfunctions and the word-RAM complexity compared to the standard framework.</li>\n  <li>A framework for solving the ANN problem with space-time tradeoffs as well\nas tight upper and lower bounds for the space-time tradeoff of framework\nsolutions to the ANN problem under cosine similarity.</li>\n  <li>A novel approach to solving the ANN problem on sets along with a matching\nlower bound, improving the state of the art.</li>\n  <li>A self-tuning version of the algorithm is shown through experiments to\noutperform existing similarity join algorithms.</li>\n  <li>Tight lower bounds for asymmetric locality-sensitive hashing which has\napplications to the approximate furthest neighbor problem, orthogonal vector\nsearch, and annulus queries.</li>\n  <li>A proof of the optimality of a well-known Boolean locality-sensitive\nhashing scheme.\n  We study the problem of efficient algorithms for producing high-quality\npseudorandom numbers and obtain the following results:</li>\n  <li>A deterministic algorithm for generating pseudorandom numbers of\narbitrarily high quality in constant time using near-optimal space.</li>\n  <li>A randomized construction of a family of hash functions that outputs\npseudorandom numbers of arbitrarily high quality with space usage and running\ntime nearly matching known cell-probe lower bounds.</li>\n</ul>\n", "tags": ["ANN Search", "Hashing Methods", "Applications"], "tsne_embedding": [-11.006380081176758, -3.9461722373962402], "cluster": 2}, {"key": "christiani2020dartminhash", "year": "2020", "citations": "4", "title": "Dartminhash Fast Sketching For Weighted Sets", "abstract": "<p>Weighted minwise hashing is a standard dimensionality reduction technique\nwith applications to similarity search and large-scale kernel machines. We\nintroduce a simple algorithm that takes a weighted set \\(x \\in \\mathbb{R}<em>{\\geq\n0}^{d}\\) and computes \\(k\\) independent minhashes in expected time \\(O(k log k +\n\\Vert x \\Vert</em>{0}log( \\Vert x \\Vert_1 + 1/\\Vert x \\Vert_1))\\), improving upon\nthe state-of-the-art BagMinHash algorithm (KDD \u201818) and representing the\nfastest weighted minhash algorithm for sparse data. Our experiments show\nrunning times that scale better with \\(k\\) and \\(\\Vert x \\Vert_0\\) compared to ICWS\n(ICDM \u201810) and BagMinhash, obtaining \\(10\\)x speedups in common use cases. Our\napproach also gives rise to a technique for computing fully independent\nlocality-sensitive hash values for \\((L, K)\\)-parameterized approximate near\nneighbor search under weighted Jaccard similarity in optimal expected time\n\\(O(LK + \\Vert x \\Vert_0)\\), improving on prior work even in the case of\nunweighted sets.</p>\n", "tags": ["Hashing Methods", "ANN Search", "Efficient Learning"], "tsne_embedding": [-15.977234840393066, -1.8226310014724731], "cluster": 2}, {"key": "christiani2020fast", "year": "2020", "citations": "4", "title": "Dartminhash: Fast Sketching For Weighted Sets", "abstract": "<p>Weighted minwise hashing is a standard dimensionality reduction technique\nwith applications to similarity search and large-scale kernel machines. We\nintroduce a simple algorithm that takes a weighted set \\(x \\in \\mathbb{R}<em>{\\geq\n0}^{d}\\) and computes \\(k\\) independent minhashes in expected time \\(O(k log k +\n\\Vert x \\Vert</em>{0}log( \\Vert x \\Vert_1 + 1/\\Vert x \\Vert_1))\\), improving upon\nthe state-of-the-art BagMinHash algorithm (KDD \u201818) and representing the\nfastest weighted minhash algorithm for sparse data. Our experiments show\nrunning times that scale better with \\(k\\) and \\(\\Vert x \\Vert_0\\) compared to ICWS\n(ICDM \u201810) and BagMinhash, obtaining \\(10\\)x speedups in common use cases. Our\napproach also gives rise to a technique for computing fully independent\nlocality-sensitive hash values for \\((L, K)\\)-parameterized approximate near\nneighbor search under weighted Jaccard similarity in optimal expected time\n\\(O(LK + \\Vert x \\Vert_0)\\), improving on prior work even in the case of\nunweighted sets.</p>\n", "tags": ["Hashing Methods", "Efficient Learning", "Applications", "ANN Search", "KDD"], "tsne_embedding": [-15.977234840393066, -1.8226310014724731], "cluster": 2}, {"key": "chung2017learning", "year": "2017", "citations": "63", "title": "Learning Deep Representations Of Medical Images Using Siamese Cnns With Application To Content-based Image Retrieval", "abstract": "<p>Deep neural networks have been investigated in learning latent\nrepresentations of medical images, yet most of the studies limit their approach\nin a single supervised convolutional neural network (CNN), which usually rely\nheavily on a large scale annotated dataset for training. To learn image\nrepresentations with less supervision involved, we propose a deep Siamese CNN\n(SCNN) architecture that can be trained with only binary image pair\ninformation. We evaluated the learned image representations on a task of\ncontent-based medical image retrieval using a publicly available multiclass\ndiabetic retinopathy fundus image dataset. The experimental results show that\nour proposed deep SCNN is comparable to the state-of-the-art single supervised\nCNN, and requires much less supervision for training.</p>\n", "tags": ["Applications", "ANN Search", "Supervised"], "tsne_embedding": [-1.2421886920928955, 8.442235946655273], "cluster": 5}, {"key": "ciro2021lsh", "year": "2021", "citations": "0", "title": "LSH Methods For Data Deduplication In A Wikipedia Artificial Dataset", "abstract": "<p>This paper illustrates locality sensitive hasing (LSH) models for the\nidentification and removal of nearly redundant data in a text dataset. To\nevaluate the different models, we create an artificial dataset for data\ndeduplication using English Wikipedia articles. Area-Under-Curve (AUC) over 0.9\nwere observed for most models, with the best model reaching 0.96. Deduplication\nenables more effective model training by preventing the model from learning a\ndistribution that differs from the real one as a result of the repeated data.</p>\n", "tags": ["Hashing Methods", "ANN Search"], "tsne_embedding": [7.139118194580078, -5.192866802215576], "cluster": 8}, {"key": "cohen2005duality", "year": "2005", "citations": "2", "title": "Duality Between Packings And Coverings Of The Hamming Space", "abstract": "<p>We investigate the packing and covering densities of linear and nonlinear\nbinary codes, and establish a number of duality relationships between the\npacking and covering problems. Specifically, we prove that if almost all codes\n(in the class of linear or nonlinear codes) are good packings, then only a\nvanishing fraction of codes are good coverings, and vice versa: if almost all\ncodes are good coverings, then at most a vanishing fraction of codes are good\npackings. We also show that any specific maximal binary code is either a good\npacking or a good covering, in a certain well-defined sense.</p>\n", "tags": ["Hashing Methods", "Quantization"], "tsne_embedding": [-7.605100154876709, 5.265364646911621], "cluster": 4}, {"key": "coleman2019sub", "year": "2019", "citations": "3", "title": "Sub-linear Memory Sketches For Near Neighbor Search On Streaming Data", "abstract": "<p>We present the first sublinear memory sketch that can be queried to find the\nnearest neighbors in a dataset. Our online sketching algorithm compresses an N\nelement dataset to a sketch of size \\(O(N^b log^3 N)\\) in \\(O(N^{(b+1)} log^3\nN)\\) time, where \\(b &lt; 1\\). This sketch can correctly report the nearest neighbors\nof any query that satisfies a stability condition parameterized by \\(b\\). We\nachieve sublinear memory performance on stable queries by combining recent\nadvances in locality sensitive hash (LSH)-based estimators, online kernel\ndensity estimation, and compressed sensing. Our theoretical results shed new\nlight on the memory-accuracy tradeoff for nearest neighbor search, and our\nsketch, which consists entirely of short integer arrays, has a variety of\nattractive features in practice. We evaluate the memory-recall tradeoff of our\nmethod on a friend recommendation task in the Google Plus social media network.\nWe obtain orders of magnitude better compression than the random projection\nbased alternative while retaining the ability to report the nearest neighbors\nof practical queries.</p>\n", "tags": ["ANN Search", "Evaluation Metrics", "Tools and Libraries"], "tsne_embedding": [-9.321874618530273, 2.80299711227417], "cluster": 4}, {"key": "collyer2024know", "year": "2024", "citations": "0", "title": "Know Your Neighborhood General And Zero-shot Capable Binary Function Search Powered By Call Graphlets", "abstract": "<p>Binary code similarity detection is an important problem with applications in\nareas like malware analysis, vulnerability research and plagiarism detection.\nThis paper proposes a novel graph neural network architecture combined with a\nnovel graph data representation called call graphlets. A call graphlet encodes\nthe neighborhood around each function in a binary executable, capturing the\nlocal and global context through a series of statistical features. A\nspecialized graph neural network model is then designed to operate on this\ngraph representation, learning to map it to a feature vector that encodes\nsemantic code similarities using deep metric learning. The proposed approach is\nevaluated across four distinct datasets covering different architectures,\ncompiler toolchains, and optimization levels. Experimental results demonstrate\nthat the combination of call graphlets and the novel graph neural network\narchitecture achieves state-of-the-art performance compared to baseline\ntechniques across cross-architecture, mono-architecture and zero shot tasks. In\naddition, our proposed approach also performs well when evaluated against an\nout-of-domain function inlining task. Overall, the work provides a general and\neffective graph neural network-based solution for conducting binary code\nsimilarity detection.</p>\n", "tags": ["Graph", "Supervised", "Deep Hashing", "ANN Search"], "tsne_embedding": [6.192282199859619, -8.498160362243652], "cluster": 1}, {"key": "conjeti2016deep", "year": "2016", "citations": "3", "title": "Deep Residual Hashing", "abstract": "<p>Hashing aims at generating highly compact similarity preserving code words\nwhich are well suited for large-scale image retrieval tasks.\n  Most existing hashing methods first encode the images as a vector of\nhand-crafted features followed by a separate binarization step to generate hash\ncodes. This two-stage process may produce sub-optimal encoding. In this paper,\nfor the first time, we propose a deep architecture for supervised hashing\nthrough residual learning, termed Deep Residual Hashing (DRH), for an\nend-to-end simultaneous representation learning and hash coding. The DRH model\nconstitutes four key elements: (1) a sub-network with multiple stacked residual\nblocks; (2) hashing layer for binarization; (3) supervised retrieval loss\nfunction based on neighbourhood component analysis for similarity preserving\nembedding; and (4) hashing related losses and regularisation to control the\nquantization error and improve the quality of hash coding. We present results\nof extensive experiments on a large public chest x-ray image database with\nco-morbidities and discuss the outcome showing substantial improvements over\nthe latest state-of-the art methods.</p>\n", "tags": ["Applications", "Hashing Methods", "Quantization", "Supervised", "Loss Functions"], "tsne_embedding": [4.607931613922119, 3.9770398139953613], "cluster": 9}, {"key": "conjeti2017learning", "year": "2017", "citations": "1", "title": "Learning Robust Hash Codes For Multiple Instance Image Retrieval", "abstract": "<p>In this paper, for the first time, we introduce a multiple instance (MI) deep\nhashing technique for learning discriminative hash codes with weak bag-level\nsupervision suited for large-scale retrieval. We learn such hash codes by\naggregating deeply learnt hierarchical representations across bag members\nthrough a dedicated MI pool layer. For better trainability and retrieval\nquality, we propose a two-pronged approach that includes robust optimization\nand training with an auxiliary single instance hashing arm which is\ndown-regulated gradually. We pose retrieval for tumor assessment as an MI\nproblem because tumors often coexist with benign masses and could exhibit\ncomplementary signatures when scanned from different anatomical views.\nExperimental validations on benchmark mammography and histology datasets\ndemonstrate improved retrieval performance over the state-of-the-art methods.</p>\n", "tags": ["Applications", "ANN Search", "Evaluation Metrics", "Hashing Methods", "Supervision Type"], "tsne_embedding": [9.018797874450684, 7.399305820465088], "cluster": 3}, {"key": "connor2016hilbert", "year": "2016", "citations": "18", "title": "Hilbert Exclusion: Improved Metric Search Through Finite Isometric Embeddings", "abstract": "<p>Most research into similarity search in metric spaces relies upon the\ntriangle inequality property. This property allows the space to be arranged\naccording to relative distances to avoid searching some subspaces. We show that\nmany common metric spaces, notably including those using Euclidean and\nJensen-Shannon distances, also have a stronger property, sometimes called the\nfour-point property: in essence, these spaces allow an isometric embedding of\nany four points in three-dimensional Euclidean space, as well as any three\npoints in two-dimensional Euclidean space. In fact, we show that any space\nwhich is isometrically embeddable in Hilbert space has the stronger property.\nThis property gives stronger geometric guarantees, and one in particular, which\nwe name the Hilbert Exclusion property, allows any indexing mechanism which\nuses hyperplane partitioning to perform better. One outcome of this observation\nis that a number of state-of-the-art indexing mechanisms over high dimensional\nspaces can be easily extended to give a significant increase in performance;\nfurthermore, the improvement given is greater in higher dimensions. This\ntherefore leads to a significant improvement in the cost of metric search in\nthese spaces.</p>\n", "tags": ["Indexing", "ANN Search"], "tsne_embedding": [-15.432950973510742, 3.256193161010742], "cluster": 4}, {"key": "connor2017high", "year": "2017", "citations": "13", "title": "High-dimensional Simplexes For Supermetric Search", "abstract": "<p>In 1953, Blumenthal showed that every semi-metric space that is isometrically\nembeddable in a Hilbert space has the n-point property; we have previously\ncalled such spaces supermetric spaces. Although this is a strictly stronger\nproperty than triangle inequality, it is nonetheless closely related and many\nuseful metric spaces possess it. These include Euclidean, Cosine and\nJensen-Shannon spaces of any dimension. A simple corollary of the n-point\nproperty is that, for any (n+1) objects sampled from the space, there exists an\nn-dimensional simplex in Euclidean space whose edge lengths correspond to the\ndistances among the objects. We show how the construction of such simplexes in\nhigher dimensions can be used to give arbitrarily tight lower and upper bounds\non distances within the original space. This allows the construction of an\nn-dimensional Euclidean space, from which lower and upper bounds of the\noriginal space can be calculated, and which is itself an indexable space with\nthe n-point property. For similarity search, the engineering tradeoffs are\ngood: we show significant reductions in data size and metric cost with little\nloss of accuracy, leading to a significant overall improvement in search\nperformance.</p>\n", "tags": ["ANN Search", "Indexing", "Efficient Learning"], "tsne_embedding": [-15.421127319335938, 3.2144086360931396], "cluster": 4}, {"key": "corlay2011fast", "year": "2011", "citations": "4", "title": "A Fast Nearest Neighbor Search Algorithm Based On Vector Quantization", "abstract": "<p>In this article, we propose a new fast nearest neighbor search algorithm,\nbased on vector quantization. Like many other branch and bound search\nalgorithms [1,10], a preprocessing recursively partitions the data set into\ndisjointed subsets until the number of points in each part is small enough. In\ndoing so, a search-tree data structure is built. This preliminary recursive\ndata-set partition is based on the vector quantization of the empirical\ndistribution of the initial data-set. Unlike previously cited methods, this\nkind of partitions does not a priori allow to eliminate several brother nodes\nin the search tree with a single test. To overcome this difficulty, we propose\nan algorithm to reduce the number of tested brother nodes to a minimal list\nthat we call \u201cfriend Voronoi cells\u201d. The complete description of the method\nrequires a deeper insight into the properties of Delaunay triangulations and\nVoronoi diagrams</p>\n", "tags": ["ANN Search", "Quantization"], "tsne_embedding": [5.105544090270996, -12.925415992736816], "cluster": 1}, {"key": "cui2020exchnet", "year": "2020", "citations": "31", "title": "Exchnet A Unified Hashing Network For Large-scale Fine-grained Image Retrieval", "abstract": "<p>Retrieving content relevant images from a large-scale fine-grained dataset\ncould suffer from intolerably slow query speed and highly redundant storage\ncost, due to high-dimensional real-valued embeddings which aim to distinguish\nsubtle visual differences of fine-grained objects. In this paper, we study the\nnovel fine-grained hashing topic to generate compact binary codes for\nfine-grained images, leveraging the search and storage efficiency of hash\nlearning to alleviate the aforementioned problems. Specifically, we propose a\nunified end-to-end trainable network, termed as ExchNet. Based on attention\nmechanisms and proposed attention constraints, it can firstly obtain both local\nand global features to represent object parts and whole fine-grained objects,\nrespectively. Furthermore, to ensure the discriminative ability and semantic\nmeaning\u2019s consistency of these part-level features across images, we design a\nlocal feature alignment approach by performing a feature exchanging operation.\nLater, an alternative learning algorithm is employed to optimize the whole\nExchNet and then generate the final binary hash codes. Validated by extensive\nexperiments, our proposal consistently outperforms state-of-the-art generic\nhashing methods on five fine-grained datasets, which shows our effectiveness.\nMoreover, compared with other approximate nearest neighbor methods, ExchNet\nachieves the best speed-up and storage reduction, revealing its efficiency and\npracticality.</p>\n", "tags": ["Hashing Methods", "Deep Hashing", "ANN Search", "Applications", "Efficient Learning"], "tsne_embedding": [1.5562206506729126, 0.34056076407432556], "cluster": 9}, {"key": "cui2020unified", "year": "2020", "citations": "31", "title": "Exchnet: A Unified Hashing Network For Large-scale Fine-grained Image Retrieval", "abstract": "<p>Retrieving content relevant images from a large-scale fine-grained dataset\ncould suffer from intolerably slow query speed and highly redundant storage\ncost, due to high-dimensional real-valued embeddings which aim to distinguish\nsubtle visual differences of fine-grained objects. In this paper, we study the\nnovel fine-grained hashing topic to generate compact binary codes for\nfine-grained images, leveraging the search and storage efficiency of hash\nlearning to alleviate the aforementioned problems. Specifically, we propose a\nunified end-to-end trainable network, termed as ExchNet. Based on attention\nmechanisms and proposed attention constraints, it can firstly obtain both local\nand global features to represent object parts and whole fine-grained objects,\nrespectively. Furthermore, to ensure the discriminative ability and semantic\nmeaning\u2019s consistency of these part-level features across images, we design a\nlocal feature alignment approach by performing a feature exchanging operation.\nLater, an alternative learning algorithm is employed to optimize the whole\nExchNet and then generate the final binary hash codes. Validated by extensive\nexperiments, our proposal consistently outperforms state-of-the-art generic\nhashing methods on five fine-grained datasets, which shows our effectiveness.\nMoreover, compared with other approximate nearest neighbor methods, ExchNet\nachieves the best speed-up and storage reduction, revealing its efficiency and\npracticality.</p>\n", "tags": ["Applications", "Deep Hashing", "ANN Search", "Hashing Methods"], "tsne_embedding": [1.5562206506729126, 0.34056076407432556], "cluster": 9}, {"key": "curtin2016fast", "year": "2016", "citations": "0", "title": "Fast Approximate Furthest Neighbors With Data-dependent Hashing", "abstract": "<p>We present a novel hashing strategy for approximate furthest neighbor search\nthat selects projection bases using the data distribution. This strategy leads\nto an algorithm, which we call DrusillaHash, that is able to outperform\nexisting approximate furthest neighbor strategies. Our strategy is motivated by\nan empirical study of the behavior of the furthest neighbor search problem,\nwhich lends intuition for where our algorithm is most useful. We also present a\nvariant of the algorithm that gives an absolute approximation guarantee; to our\nknowledge, this is the first such approximate furthest neighbor hashing\napproach to give such a guarantee. Performance studies indicate that\nDrusillaHash can achieve comparable levels of approximation to other algorithms\nwhile giving up to an order of magnitude speedup. An implementation is\navailable in the mlpack machine learning library (found at\nhttp://www.mlpack.org).</p>\n", "tags": ["Efficient Learning", "Hashing Methods", "ANN Search"], "tsne_embedding": [-8.938013076782227, 2.12715482711792], "cluster": 4}, {"key": "dadaneh2020pairwise", "year": "2020", "citations": "11", "title": "Pairwise Supervised Hashing With Bernoulli Variational Auto-encoder And Self-control Gradient Estimator", "abstract": "<p>Semantic hashing has become a crucial component of fast similarity search in\nmany large-scale information retrieval systems, in particular, for text data.\nVariational auto-encoders (VAEs) with binary latent variables as hashing codes\nprovide state-of-the-art performance in terms of precision for document\nretrieval. We propose a pairwise loss function with discrete latent VAE to\nreward within-class similarity and between-class dissimilarity for supervised\nhashing. Instead of solving the optimization relying on existing biased\ngradient estimators, an unbiased low-variance gradient estimator is adopted to\noptimize the hashing function by evaluating the non-differentiable loss\nfunction over two correlated sets of binary hashing codes to control the\nvariance of gradient estimates. This new semantic hashing framework achieves\nsuperior performance compared to the state-of-the-arts, as demonstrated by our\ncomprehensive experiments.</p>\n", "tags": ["ANN Search", "Loss Functions", "Supervised", "Hashing Methods"], "tsne_embedding": [4.264454364776611, 2.5864524841308594], "cluster": 9}, {"key": "dahlgaard2014approximately", "year": "2014", "citations": "7", "title": "Approximately Minwise Independence With Twisted Tabulation", "abstract": "<p>A random hash function \\(h\\) is \\(\\epsilon\\)-minwise if for any set \\(S\\),\n\\(|S|=n\\), and element \\(x\\in S\\), \\(\\Pr[h(x)=\\min h(S)]=(1\\pm\\epsilon)/n\\).\nMinwise hash functions with low bias \\(\\epsilon\\) have widespread applications\nwithin similarity estimation.\n  Hashing from a universe \\([u]\\), the twisted tabulation hashing of\nP\\v{a}tra\\c{s}cu and Thorup [SODA\u201913] makes \\(c=O(1)\\) lookups in tables of size\n\\(u^{1/c}\\). Twisted tabulation was invented to get good concentration for\nhashing based sampling. Here we show that twisted tabulation yields \\(\\tilde\nO(1/u^{1/c})\\)-minwise hashing.\n  In the classic independence paradigm of Wegman and Carter [FOCS\u201979] \\(\\tilde\nO(1/u^{1/c})\\)-minwise hashing requires \\(\u03a9(log u)\\)-independence [Indyk\nSODA\u201999]. P\\v{a}tra\\c{s}cu and Thorup [STOC\u201911] had shown that simple\ntabulation, using same space and lookups yields \\(\\tilde O(1/n^{1/c})\\)-minwise\nindependence, which is good for large sets, but useless for small sets. Our\nanalysis uses some of the same methods, but is much cleaner bypassing a\ncomplicated induction argument.</p>\n", "tags": ["Hashing Methods", "Applications"], "tsne_embedding": [-15.806159973144531, -5.755678653717041], "cluster": 2}, {"key": "dahlgaard2017fast", "year": "2017", "citations": "16", "title": "Fast Similarity Sketching", "abstract": "<p>We consider the \\(\\textit{Similarity Sketching}\\) problem: Given a universe\n\\([u] = \\{0,\\ldots, u-1\\}\\) we want a random function \\(S\\) mapping subsets\n\\(A\\subseteq [u]\\) into vectors \\(S(A)\\) of size \\(t\\), such that the Jaccard\nsimilarity \\(J(A,B) = |A\\cap B|/|A\\cup B|\\) between sets \\(A\\) and \\(B\\) is\npreserved. More precisely, define \\(X_i = [S(A)[i] =\n  S(B)[i]]\\) and \\(X = \\sum_{i\\in [t]} X_i\\). We want \\(E[X_i]=J(A,B)\\), and we want\n\\(X\\) to be strongly concentrated around \\(E[X] = t \\cdot J(A,B)\\) (i.e.\nChernoff-style bounds). This is a fundamental problem which has found numerous\napplications in data mining, large-scale classification, computer vision,\nsimilarity search, etc. via the classic MinHash algorithm. The vectors \\(S(A)\\)\nare also called \\(\\textit{sketches}\\). Strong concentration is critical, for\noften we want to sketch many sets \\(B_1,\\ldots,B_n\\) so that we later, for a\nquery set \\(A\\), can find (one of) the most similar \\(B_i\\). It is then critical\nthat no \\(B_i\\) looks much more similar to \\(A\\) due to errors in the sketch.\n  The seminal \\(t\\times\\textit{MinHash}\\) algorithm uses \\(t\\) random hash\nfunctions \\(h_1,\\ldots, h_t\\), and stores \\(\\left ( \\min_{a\\in A} h_1(A),\\ldots,\n\\min_{a\\in A} h_t(A) \\right )\\) as the sketch of \\(A\\). The main drawback of\nMinHash is, however, its \\(O(t\\cdot |A|)\\) running time, and finding a sketch\nwith similar properties and faster running time has been the subject of several\npapers. (continued\u2026)</p>\n", "tags": ["Applications", "Evaluation Metrics"], "tsne_embedding": [-12.981821060180664, -8.852676391601562], "cluster": 2}, {"key": "dahlgaard2017practical", "year": "2017", "citations": "16", "title": "Practical Hash Functions For Similarity Estimation And Dimensionality Reduction", "abstract": "<p>Hashing is a basic tool for dimensionality reduction employed in several\naspects of machine learning. However, the perfomance analysis is often carried\nout under the abstract assumption that a truly random unit cost hash function\nis used, without concern for which concrete hash function is employed. The\nconcrete hash function may work fine on sufficiently random input. The question\nis if it can be trusted in the real world when faced with more structured\ninput.\n  In this paper we focus on two prominent applications of hashing, namely\nsimilarity estimation with the one permutation hashing (OPH) scheme of Li et\nal. [NIPS\u201912] and feature hashing (FH) of Weinberger et al. [ICML\u201909], both of\nwhich have found numerous applications, i.e. in approximate near-neighbour\nsearch with LSH and large-scale classification with SVM.\n  We consider mixed tabulation hashing of Dahlgaard et al.[FOCS\u201915] which was\nproved to perform like a truly random hash function in many applications,\nincluding OPH. Here we first show improved concentration bounds for FH with\ntruly random hashing and then argue that mixed tabulation performs similar for\nsparse input. Our main contribution, however, is an experimental comparison of\ndifferent hashing schemes when used inside FH, OPH, and LSH.\n  We find that mixed tabulation hashing is almost as fast as the\nmultiply-mod-prime scheme ax+b mod p. Mutiply-mod-prime is guaranteed to work\nwell on sufficiently random data, but we demonstrate that in the above\napplications, it can lead to bias and poor concentration on both real-world and\nsynthetic data. We also compare with the popular MurmurHash3, which has no\nproven guarantees. Mixed tabulation and MurmurHash3 both perform similar to\ntruly random hashing in our experiments. However, mixed tabulation is 40%\nfaster than MurmurHash3, and it has the proven guarantee of good performance on\nall possible input.</p>\n", "tags": ["Applications", "Hashing Methods", "ANN Search", "ICML"], "tsne_embedding": [-5.746038436889648, -4.308838367462158], "cluster": 7}, {"key": "dai2017stochastic", "year": "2017", "citations": "73", "title": "Stochastic Generative Hashing", "abstract": "<p>Learning-based binary hashing has become a powerful paradigm for fast search\nand retrieval in massive databases. However, due to the requirement of discrete\noutputs for the hash functions, learning such functions is known to be very\nchallenging. In addition, the objective functions adopted by existing hashing\ntechniques are mostly chosen heuristically. In this paper, we propose a novel\ngenerative approach to learn hash functions through Minimum Description Length\nprinciple such that the learned hash codes maximally compress the dataset and\ncan also be used to regenerate the inputs. We also develop an efficient\nlearning algorithm based on the stochastic distributional gradient, which\navoids the notorious difficulty caused by binary output constraints, to jointly\noptimize the parameters of the hash function and the associated generative\nmodel. Extensive experiments on a variety of large-scale datasets show that the\nproposed method achieves better retrieval results than the existing\nstate-of-the-art methods.</p>\n", "tags": ["Hashing Methods", "Efficient Learning"], "tsne_embedding": [3.2929255962371826, 7.095634460449219], "cluster": 5}, {"key": "datar2024locality", "year": "2024", "citations": "1931", "title": "Locality-sensitive Hashing Scheme Based On P-stable Distributions", "abstract": "<p>We present a novel Locality-Sensitive Hashing scheme for the Approximate Nearest Neighbor Problem under lp norm, based on p-stable distributions.Our scheme improves the running time of the earlier algorithm for the case of the lp norm. It also yields the first known provably efficient approximate NN algorithm for the case p&lt;1. We also show that the algorithm finds the exact near neigbhor in O(log n) time for data satisfying certain \u201cbounded growth\u201d condition.Unlike earlier schemes, our LSH scheme works directly on points in the Euclidean space without embeddings. Consequently, the resulting query time bound is free of large factors and is simple and easy to implement. Our experiments (on synthetic data sets) show that the our data structure is up to 40 times faster than kd-tree.</p>\n", "tags": ["Hashing Methods", "ANN Search", "Efficient Learning"], "tsne_embedding": [-15.00870418548584, -0.09125962108373642], "cluster": 4}, {"key": "davoodi2019forestdsh", "year": "2019", "citations": "0", "title": "Forestdsh A Universal Hash Design For Discrete Probability Distributions", "abstract": "<p>In this paper, we consider the problem of classification of \\(M\\) high\ndimensional queries \\(y^1,\\cdots,y^M\\in B^S\\) to \\(N\\) high dimensional classes\n\\(x^1,\\cdots,x^N\\in A^S\\) where \\(A\\) and \\(B\\) are discrete alphabets and the\nprobabilistic model that relates data to the classes \\(P(x,y)\\) is known. This\nproblem has applications in various fields including the database search\nproblem in mass spectrometry. The problem is analogous to the nearest neighbor\nsearch problem, where the goal is to find the data point in a database that is\nthe most similar to a query point. The state of the art method for solving an\napproximate version of the nearest neighbor search problem in high dimensions\nis locality sensitive hashing (LSH). LSH is based on designing hash functions\nthat map near points to the same buckets with a probability higher than random\n(far) points. To solve our high dimensional classification problem, we\nintroduce distribution sensitive hashes that map jointly generated pairs\n\\((x,y)\\sim P\\) to the same bucket with probability higher than random pairs\n\\(x\\sim P^A\\) and \\(y\\sim P^B\\), where \\(P^A\\) and \\(P^B\\) are the marginal probability\ndistributions of \\(P\\). We design distribution sensitive hashes using a forest of\ndecision trees and we show that the complexity of search grows with\n\\(O(N^{\\lambda^<em>(P)})\\) where \\(\\lambda^</em>(P)\\) is expressed in an analytical form.\nWe further show that the proposed hashes perform faster than state of the art\napproximate nearest neighbor search methods for a range of probability\ndistributions, in both theory and simulations. Finally, we apply our method to\nthe spectral library search problem in mass spectrometry, and show that it is\nan order of magnitude faster than the state of the art methods.</p>\n", "tags": ["LSH", "Supervised", "ANN Search"], "tsne_embedding": [-13.447617530822754, -5.661488056182861], "cluster": 2}, {"key": "davoodi2019universal", "year": "2019", "citations": "0", "title": "Forestdsh: A Universal Hash Design For Discrete Probability Distributions", "abstract": "<p>In this paper, we consider the problem of classification of \\(M\\) high\ndimensional queries \\(y^1,\\cdots,y^M\\in B^S\\) to \\(N\\) high dimensional classes\n\\(x^1,\\cdots,x^N\\in A^S\\) where \\(A\\) and \\(B\\) are discrete alphabets and the\nprobabilistic model that relates data to the classes \\(P(x,y)\\) is known. This\nproblem has applications in various fields including the database search\nproblem in mass spectrometry. The problem is analogous to the nearest neighbor\nsearch problem, where the goal is to find the data point in a database that is\nthe most similar to a query point. The state of the art method for solving an\napproximate version of the nearest neighbor search problem in high dimensions\nis locality sensitive hashing (LSH). LSH is based on designing hash functions\nthat map near points to the same buckets with a probability higher than random\n(far) points. To solve our high dimensional classification problem, we\nintroduce distribution sensitive hashes that map jointly generated pairs\n\\((x,y)\\sim P\\) to the same bucket with probability higher than random pairs\n\\(x\\sim P^A\\) and \\(y\\sim P^B\\), where \\(P^A\\) and \\(P^B\\) are the marginal probability\ndistributions of \\(P\\). We design distribution sensitive hashes using a forest of\ndecision trees and we show that the complexity of search grows with\n\\(O(N^{\\lambda^<em>(P)})\\) where \\(\\lambda^</em>(P)\\) is expressed in an analytical form.\nWe further show that the proposed hashes perform faster than state of the art\napproximate nearest neighbor search methods for a range of probability\ndistributions, in both theory and simulations. Finally, we apply our method to\nthe spectral library search problem in mass spectrometry, and show that it is\nan order of magnitude faster than the state of the art methods.</p>\n", "tags": ["Hashing Methods", "ANN Search", "Applications", "Evaluation Metrics"], "tsne_embedding": [-13.447521209716797, -5.661498546600342], "cluster": 2}, {"key": "defranca2014iterative", "year": "2014", "citations": "0", "title": "Iterative Universal Hash Function Generator For Minhashing", "abstract": "<p>Minhashing is a technique used to estimate the Jaccard Index between two sets\nby exploiting the probability of collision in a random permutation. In order to\nspeed up the computation, a random permutation can be approximated by using an\nuniversal hash function such as the \\(h_{a,b}\\) function proposed by Carter and\nWegman. A better estimate of the Jaccard Index can be achieved by using many of\nthese hash functions, created at random. In this paper a new iterative\nprocedure to generate a set of \\(h_{a,b}\\) functions is devised that eliminates\nthe need for a list of random values and avoid the multiplication operation\nduring the calculation. The properties of the generated hash functions remains\nthat of an universal hash function family. This is possible due to the random\nnature of features occurrence on sparse datasets. Results show that the\nuniformity of hashing the features is maintaned while obtaining a speed up of\nup to \\(1.38\\) compared to the traditional approach.</p>\n", "tags": ["Hashing Methods", "Tools and Libraries"], "tsne_embedding": [-5.051684379577637, 0.38326019048690796], "cluster": 7}, {"key": "defran\u00e7a2014hash", "year": "2014", "citations": "12", "title": "A Hash-based Co-clustering Algorithm For Categorical Data", "abstract": "<p>Many real-life data are described by categorical attributes without a\npre-classification. A common data mining method used to extract information\nfrom this type of data is clustering. This method group together the samples\nfrom the data that are more similar than all other samples. But, categorical\ndata pose a challenge when extracting information because: the calculation of\ntwo objects similarity is usually done by measuring the number of common\nfeatures, but ignore a possible importance weighting; if the data may be\ndivided differently according to different subsets of the features, the\nalgorithm may find clusters with different meanings from each other,\ndifficulting the post analysis. Data Co-Clustering of categorical data is the\ntechnique that tries to find subsets of samples that share a subset of features\nin common. By doing so, not only a sample may belong to more than one cluster\nbut, the feature selection of each cluster describe its own characteristics. In\nthis paper a novel Co-Clustering technique for categorical data is proposed by\nusing Locality Sensitive Hashing technique in order to preprocess a list of\nCo-Clusters seeds based on a previous research. Results indicate this technique\nis capable of finding high quality Co-Clusters in many different categorical\ndata sets and scales linearly with the data set size.</p>\n", "tags": ["Hashing Methods", "KDD"], "tsne_embedding": [5.15082311630249, 0.7616877555847168], "cluster": 9}, {"key": "deng2017learning", "year": "2017", "citations": "30", "title": "Learning Deep Similarity Models With Focus Ranking For Fabric Image Retrieval", "abstract": "<p>Fabric image retrieval is beneficial to many applications including clothing\nsearching, online shopping and cloth modeling. Learning pairwise image\nsimilarity is of great importance to an image retrieval task. With the\nresurgence of Convolutional Neural Networks (CNNs), recent works have achieved\nsignificant progresses via deep representation learning with metric embedding,\nwhich drives similar examples close to each other in a feature space, and\ndissimilar ones apart from each other. In this paper, we propose a novel\nembedding method termed focus ranking that can be easily unified into a CNN for\njointly learning image representations and metrics in the context of\nfine-grained fabric image retrieval. Focus ranking aims to rank similar\nexamples higher than all dissimilar ones by penalizing ranking disorders via\nthe minimization of the overall cost attributed to similar samples being ranked\nbelow dissimilar ones. At the training stage, training samples are organized\ninto focus ranking units for efficient optimization. We build a large-scale\nfabric image retrieval dataset (FIRD) with about 25,000 images of 4,300\nfabrics, and test the proposed model on the FIRD dataset. Experimental results\nshow the superiority of the proposed model over existing metric embedding\nmodels.</p>\n", "tags": ["Applications", "Benchmarks and Datasets", "Deep Hashing"], "tsne_embedding": [6.319767475128174, -0.7278599739074707], "cluster": 8}, {"key": "deng2019triplet", "year": "2019", "citations": "333", "title": "Triplet-based Deep Hashing Network For Cross-modal Retrieval", "abstract": "<p>Given the benefits of its low storage requirements and high retrieval\nefficiency, hashing has recently received increasing attention. In\nparticular,cross-modal hashing has been widely and successfully used in\nmultimedia similarity search applications. However, almost all existing methods\nemploying cross-modal hashing cannot obtain powerful hash codes due to their\nignoring the relative similarity between heterogeneous data that contains\nricher semantic information, leading to unsatisfactory retrieval performance.\nIn this paper, we propose a triplet-based deep hashing (TDH) network for\ncross-modal retrieval. First, we utilize the triplet labels, which describes\nthe relative relationships among three instances as supervision in order to\ncapture more general semantic correlations between cross-modal instances. We\nthen establish a loss function from the inter-modal view and the intra-modal\nview to boost the discriminative abilities of the hash codes. Finally, graph\nregularization is introduced into our proposed TDH method to preserve the\noriginal semantic similarity between hash codes in Hamming space. Experimental\nresults show that our proposed method outperforms several state-of-the-art\napproaches on two popular cross-modal datasets.</p>\n", "tags": ["Applications", "Deep Hashing", "ANN Search", "Multi-Modal Hashing", "Hashing Methods", "Loss Functions"], "tsne_embedding": [5.907427787780762, 4.269280910491943], "cluster": 3}, {"key": "deng2024two", "year": "2024", "citations": "63", "title": "Two-stream Deep Hashing With Class-specific Centers For Supervised Image Search", "abstract": "<p>Hashing has been widely used for large-scale approximate nearest neighbor search due to its storage and search efficiency. Recent supervised hashing research has shown that deep learning-based methods can significantly outperform nondeep methods. Most existing supervised deep hashing methods exploit supervisory signals to generate similar and dissimilar image pairs for training. However, natural images can have large intraclass and small interclass variations, which may degrade the accuracy of hash codes. To address this problem, we propose a novel two-stream ConvNet architecture, which learns hash codes with class-specific representation centers. Our basic idea is that if we can learn a unified binary representation for each class as a center and encourage hash codes of images to be close to the corresponding centers, the intraclass variation will be greatly reduced. Accordingly, we design a neural network that leverages label information and outputs a unified binary representation for each class. Moreover, we also design an image network to learn hash codes from images and force these hash codes to be close to the corresponding class-specific centers. These two neural networks are then seamlessly incorporated to create a unified, end-to-end trainable framework. Extensive experiments on three popular benchmarks corroborate that our proposed method outperforms current state-of-the-art methods.</p>\n", "tags": ["Deep Hashing", "Supervised", "ANN Search", "Evaluation Metrics"], "tsne_embedding": [1.445810079574585, 4.093669891357422], "cluster": 0}, {"key": "depalma2017distributed", "year": "2017", "citations": "1", "title": "Distributed Stratified Locality Sensitive Hashing For Critical Event Prediction In The Cloud", "abstract": "<p>The availability of massive healthcare data repositories calls for efficient\ntools for data-driven medicine. We introduce a distributed system for\nStratified Locality Sensitive Hashing to perform fast similarity-based\nprediction on large medical waveform datasets. Our implementation, for an ICU\nuse case, prioritizes latency over throughput and is targeted at a cloud\nenvironment. We demonstrate our system on Acute Hypotensive Episode prediction\nfrom Arterial Blood Pressure waveforms. On a dataset of \\(1.37\\) million points,\nwe show scaling up to \\(40\\) processors and a \\(21\\times\\) speedup in number of\ncomparisons to parallel exhaustive search at the price of a \\(10%\\) Matthews\ncorrelation coefficient (MCC) loss. Furthermore, if additional MCC loss can be\ntolerated, our system achieves speedups up to two orders of magnitude.</p>\n", "tags": ["Hashing Methods", "Efficient Learning", "Applications"], "tsne_embedding": [9.739974975585938, 8.597140312194824], "cluster": 3}, {"key": "desai2021semantically", "year": "2021", "citations": "2", "title": "Semantically Constrained Memory Allocation (SCMA) For Embedding In Efficient Recommendation Systems", "abstract": "<p>Deep learning-based models are utilized to achieve state-of-the-art\nperformance for recommendation systems. A key challenge for these models is to\nwork with millions of categorical classes or tokens. The standard approach is\nto learn end-to-end, dense latent representations or embeddings for each token.\nThe resulting embeddings require large amounts of memory that blow up with the\nnumber of tokens. Training and inference with these models create storage, and\nmemory bandwidth bottlenecks leading to significant computing and energy\nconsumption when deployed in practice. To this end, we present the problem of\n\\textit{Memory Allocation} under budget for embeddings and propose a novel\nformulation of memory shared embedding, where memory is shared in proportion to\nthe overlap in semantic information. Our formulation admits a practical and\nefficient randomized solution with Locality sensitive hashing based Memory\nAllocation (LMA). We demonstrate a significant reduction in the memory\nfootprint while maintaining performance. In particular, our LMA embeddings\nachieve the same performance compared to standard embeddings with a 16\\(\\times\\)\nreduction in memory footprint. Moreover, LMA achieves an average improvement of\nover 0.003 AUC across different memory regimes than standard DLRM models on\nCriteo and Avazu datasets</p>\n", "tags": ["Hashing Methods", "Applications", "Efficient Learning"], "tsne_embedding": [-0.5605145692825317, -7.5350847244262695], "cluster": 6}, {"key": "desai2024identity", "year": "2024", "citations": "0", "title": "Identity With Locality: An Ideal Hash For Gene Sequence Search", "abstract": "<p>Gene sequence search is a fundamental operation in computational genomics.\nDue to the petabyte scale of genome archives, most gene search systems now use\nhashing-based data structures such as Bloom Filters (BF). The state-of-the-art\nsystems such as Compact bit-slicing signature index (COBS) and Repeated And\nMerged Bloom filters (RAMBO) use BF with Random Hash (RH) functions for gene\nrepresentation and identification. The standard recipe is to cast the gene\nsearch problem as a sequence of membership problems testing if each subsequent\ngene substring (called kmer) of Q is present in the set of kmers of the entire\ngene database D. We observe that RH functions, which are crucial to the memory\nand the computational advantage of BF, are also detrimental to the system\nperformance of gene-search systems. While subsequent kmers being queried are\nlikely very similar, RH, oblivious to any similarity, uniformly distributes the\nkmers to different parts of potentially large BF, thus triggering excessive\ncache misses and causing system slowdown. We propose a novel hash function\ncalled the Identity with Locality (IDL) hash family, which co-locates the keys\nclose in input space without causing collisions. This approach ensures both\ncache locality and key preservation. IDL functions can be a drop-in replacement\nfor RH functions and help improve the performance of information retrieval\nsystems. We give a simple but practical construction of IDL function families\nand show that replacing the RH with IDL functions reduces cache misses by a\nfactor of 5x, thus improving query and indexing times of SOTA methods such as\nCOBS and RAMBO by factors up to 2x without compromising their quality. We also\nprovide a theoretical analysis of the false positive rate of BF with IDL\nfunctions. Our hash function is the first study that bridges Locality Sensitive\nHash (LSH) and RH to obtain cache efficiency.</p>\n", "tags": ["Hashing Methods", "Indexing", "ANN Search"], "tsne_embedding": [-5.9593377113342285, -6.766514301300049], "cluster": 7}, {"key": "deuser2025locality", "year": "2025", "citations": "0", "title": "Locality-sensitive Hashing For Efficient Hard Negative Sampling In Contrastive Learning", "abstract": "<p>Contrastive learning is a representational learning paradigm in which a neural network maps data elements to feature vectors. It improves the feature space by forming lots with an anchor and examples that are either positive or negative based on class similarity. Hard negative examples, which are close to the anchor in the feature space but from a different class, improve learning performance. Finding such examples of high quality efficiently in large, high-dimensional datasets is computationally challenging. In this paper, we propose a GPU-friendly Locality-Sensitive Hashing (LSH) scheme that quantizes real-valued feature vectors into binary representations for approximate nearest neighbor search. We investigate its theoretical properties and evaluate it on several datasets from textual and visual domain. Our approach achieves comparable or better performance while requiring significantly less computation than existing hard negative mining strategies.</p>\n", "tags": ["Hashing Methods", "ANN Search", "Evaluation Metrics", "Quantization"], "tsne_embedding": [2.9486825466156006, 0.5964339971542358], "cluster": 9}, {"key": "dey2018learning", "year": "2018", "citations": "17", "title": "Learning Cross-modal Deep Embeddings For Multi-object Image Retrieval Using Text And Sketch", "abstract": "<p>In this work we introduce a cross modal image retrieval system that allows\nboth text and sketch as input modalities for the query. A cross-modal deep\nnetwork architecture is formulated to jointly model the sketch and text input\nmodalities as well as the the image output modality, learning a common\nembedding between text and images and between sketches and images. In addition,\nan attention model is used to selectively focus the attention on the different\nobjects of the image, allowing for retrieval with multiple objects in the\nquery. Experiments show that the proposed method performs the best in both\nsingle and multiple object image retrieval in standard datasets.</p>\n", "tags": ["Multi-Modal Hashing", "Applications"], "tsne_embedding": [9.929966926574707, -3.7229485511779785], "cluster": 8}, {"key": "dhar2022linear", "year": "2022", "citations": "1", "title": "Linear Hashing With \\(\\ell_\\infty\\) Guarantees And Two-sided Kakeya Bounds", "abstract": "<p>We show that a randomly chosen linear map over a finite field gives a good\nhash function in the \\(\\ell_\\infty\\) sense. More concretely, consider a set \\(S\n\\subset \\mathbb{F}<em>q^n\\) and a randomly chosen linear map \\(L : \\mathbb{F}_q^n\n\\to \\mathbb{F}_q^t\\) with \\(q^t\\) taken to be sufficiently smaller than \\( |S|\\).\nLet \\(U_S\\) denote a random variable distributed uniformly on \\(S\\). Our main\ntheorem shows that, with high probability over the choice of \\(L\\), the random\nvariable \\(L(U_S)\\) is close to uniform in the \\(\\ell</em>\\infty\\) norm. In other\nwords, {\\em every} element in the range \\(\\mathbb{F}_q^t\\) has about the same\nnumber of elements in \\(S\\) mapped to it. This complements the widely-used\nLeftover Hash Lemma (LHL) which proves the analog statement under the\nstatistical, or \\(\\ell_1\\), distance (for a richer class of functions) as well as\nprior work on the expected largest \u2018bucket size\u2019 in linear hash functions\n[ADMPT99]. By known bounds from the load balancing literature [RS98], our\nresults are tight and show that linear functions hash as well as trully random\nfunction up to a constant factor in the entropy loss. Our proof leverages a\nconnection between linear hashing and the finite field Kakeya problem and\nextends some of the tools developed in this area, in particular the polynomial\nmethod.</p>\n", "tags": ["Hashing Methods", "Evaluation Metrics"], "tsne_embedding": [-15.114429473876953, -4.99190092086792], "cluster": 2}, {"key": "dhulipala2024multi", "year": "2024", "citations": "0", "title": "MUVERA: Multi-vector Retrieval Via Fixed Dimensional Encodings", "abstract": "<p>Neural embedding models have become a fundamental component of modern\ninformation retrieval (IR) pipelines. These models produce a single embedding\n\\(x \\in \\mathbb{R}^d\\) per data-point, allowing for fast retrieval via highly\noptimized maximum inner product search (MIPS) algorithms. Recently, beginning\nwith the landmark ColBERT paper, multi-vector models, which produce a set of\nembedding per data point, have achieved markedly superior performance for IR\ntasks. Unfortunately, using these models for IR is computationally expensive\ndue to the increased complexity of multi-vector retrieval and scoring.\n  In this paper, we introduce MUVERA (MUlti-VEctor Retrieval Algorithm), a\nretrieval mechanism which reduces multi-vector similarity search to\nsingle-vector similarity search. This enables the usage of off-the-shelf MIPS\nsolvers for multi-vector retrieval. MUVERA asymmetrically generates Fixed\nDimensional Encodings (FDEs) of queries and documents, which are vectors whose\ninner product approximates multi-vector similarity. We prove that FDEs give\nhigh-quality \\(\\epsilon\\)-approximations, thus providing the first single-vector\nproxy for multi-vector similarity with theoretical guarantees. Empirically, we\nfind that FDEs achieve the same recall as prior state-of-the-art heuristics\nwhile retrieving 2-5\\(\\times\\) fewer candidates. Compared to prior state of the\nart implementations, MUVERA achieves consistently good end-to-end recall and\nlatency across a diverse set of the BEIR retrieval datasets, achieving an\naverage of 10\\(%\\) improved recall with \\(90%\\) lower latency.</p>\n", "tags": ["Evaluation Metrics", "Benchmarks and Datasets", "Tools and Libraries"], "tsne_embedding": [-2.125948190689087, -6.475664138793945], "cluster": 6}, {"key": "dias2022pattern", "year": "2022", "citations": "0", "title": "Pattern Spotting And Image Retrieval In Historical Documents Using Deep Hashing", "abstract": "<p>This paper presents a deep learning approach for image retrieval and pattern\nspotting in digital collections of historical documents. First, a region\nproposal algorithm detects object candidates in the document page images. Next,\ndeep learning models are used for feature extraction, considering two distinct\nvariants, which provide either real-valued or binary code representations.\nFinally, candidate images are ranked by computing the feature similarity with a\ngiven input query. A robust experimental protocol evaluates the proposed\napproach considering each representation scheme (real-valued and binary code)\non the DocExplore image database. The experimental results show that the\nproposed deep models compare favorably to the state-of-the-art image retrieval\napproaches for images of historical documents, outperforming other deep models\nby 2.56 percentage points using the same techniques for pattern spotting.\nBesides, the proposed approach also reduces the search time by up to 200x and\nthe storage cost up to 6,000x when compared to related works based on\nreal-valued representations.</p>\n", "tags": ["Applications", "Hashing Methods", "Deep Hashing", "Efficient Learning"], "tsne_embedding": [8.710639953613281, -2.3248279094696045], "cluster": 8}, {"key": "ding2018mean", "year": "2018", "citations": "4", "title": "Mean Local Group Average Precision (mlgap): A New Performance Metric For Hashing-based Retrieval", "abstract": "<p>The research on hashing techniques for visual data is gaining increased\nattention in recent years due to the need for compact representations\nsupporting efficient search/retrieval in large-scale databases such as online\nimages. Among many possibilities, Mean Average Precision(mAP) has emerged as\nthe dominant performance metric for hashing-based retrieval. One glaring\nshortcoming of mAP is its inability in balancing retrieval accuracy and\nutilization of hash codes: pushing a system to attain higher mAP will\ninevitably lead to poorer utilization of the hash codes. Poor utilization of\nthe hash codes hinders good retrieval because of increased collision of samples\nin the hash space. This means that a model giving a higher mAP values does not\nnecessarily do a better job in retrieval. In this paper, we introduce a new\nmetric named Mean Local Group Average Precision (mLGAP) for better evaluation\nof the performance of hashing-based retrieval. The new metric provides a\nretrieval performance measure that also reconciles the utilization of hash\ncodes, leading to a more practically meaningful performance metric than\nconventional ones like mAP. To this end, we start by mathematical analysis of\nthe deficiencies of mAP for hashing-based retrieval. We then propose mLGAP and\nshow why it is more appropriate for hashing-based retrieval. Experiments on\nimage retrieval are used to demonstrate the effectiveness of the proposed\nmetric.</p>\n", "tags": ["Applications", "Evaluation Metrics", "Hashing Methods"], "tsne_embedding": [-4.783141613006592, 8.24105167388916], "cluster": 0}, {"key": "ding2019bilinear", "year": "2019", "citations": "13", "title": "Bilinear Supervised Hashing Based On 2D Image Features", "abstract": "<p>Hashing has been recognized as an efficient representation learning method to\neffectively handle big data due to its low computational complexity and memory\ncost. Most of the existing hashing methods focus on learning the\nlow-dimensional vectorized binary features based on the high-dimensional raw\nvectorized features. However, studies on how to obtain preferable binary codes\nfrom the original 2D image features for retrieval is very limited. This paper\nproposes a bilinear supervised discrete hashing (BSDH) method based on 2D image\nfeatures which utilizes bilinear projections to binarize the image matrix\nfeatures such that the intrinsic characteristics in the 2D image space are\npreserved in the learned binary codes. Meanwhile, the bilinear projection\napproximation and vectorization binary codes regression are seamlessly\nintegrated together to formulate the final robust learning framework.\nFurthermore, a discrete optimization strategy is developed to alternatively\nupdate each variable for obtaining the high-quality binary codes. In addition,\ntwo 2D image features, traditional SURF-based FVLAD feature and CNN-based\nAlexConv5 feature are designed for further improving the performance of the\nproposed BSDH method. Results of extensive experiments conducted on four\nbenchmark datasets show that the proposed BSDH method almost outperforms all\ncompeting hashing methods with different input features by different evaluation\nprotocols.</p>\n", "tags": ["Hashing Methods", "Supervised", "Evaluation Metrics", "Benchmarks and Datasets"], "tsne_embedding": [-0.34644049406051636, 4.180578231811523], "cluster": 0}, {"key": "ding2021dynamic", "year": "2021", "citations": "3", "title": "Dynamic Texture Recognition Using PDV Hashing And Dictionary Learning On Multi-scale Volume Local Binary Pattern", "abstract": "<p>Spatial-temporal local binary pattern (STLBP) has been widely used in dynamic\ntexture recognition. STLBP often encounters the high-dimension problem as its\ndimension increases exponentially, so that STLBP could only utilize a small\nneighborhood. To tackle this problem, we propose a method for dynamic texture\nrecognition using PDV hashing and dictionary learning on multi-scale volume\nlocal binary pattern (PHD-MVLBP). Instead of forming very high-dimensional LBP\nhistogram features, it first uses hash functions to map the pixel difference\nvectors (PDVs) to binary vectors, then forms a dictionary using the derived\nbinary vector, and encodes them using the derived dictionary. In such a way,\nthe PDVs are mapped to feature vectors of the size of dictionary, instead of\nLBP histograms of very high dimension. Such an encoding scheme could extract\nthe discriminant information from videos in a much larger neighborhood\neffectively. The experimental results on two widely-used dynamic textures\ndatasets, DynTex++ and UCLA, show the superiority performance of the proposed\napproach over the state-of-the-art methods.</p>\n", "tags": ["Hashing Methods", "Evaluation Metrics"], "tsne_embedding": [-0.49421948194503784, -1.4441423416137695], "cluster": 9}, {"key": "ding2024collective", "year": "2024", "citations": "495", "title": "Collective Matrix Factorization Hashing For Multimodal Data", "abstract": "<p>Nearest neighbor search methods based on hashing have\nattracted considerable attention for effective and efficient\nlarge-scale similarity search in computer vision and information\nretrieval community. In this paper, we study the\nproblems of learning hash functions in the context of multimodal\ndata for cross-view similarity search. We put forward\na novel hashing method, which is referred to Collective\nMatrix Factorization Hashing (CMFH). CMFH learns unified\nhash codes by collective matrix factorization with latent\nfactor model from different modalities of one instance,\nwhich can not only supports cross-view search but also increases\nthe search accuracy by merging multiple view information\nsources. We also prove that CMFH, a similaritypreserving\nhashing learning method, has upper and lower\nboundaries. Extensive experiments verify that CMFH significantly\noutperforms several state-of-the-art methods on\nthree different datasets.</p>\n", "tags": ["Multi-Modal Hashing", "Hashing Methods", "ANN Search", "Evaluation Metrics"], "tsne_embedding": [5.725856304168701, 1.9960271120071411], "cluster": 9}, {"key": "ding2024efficient", "year": "2024", "citations": "0", "title": "Efficient Retrieval With Learned Similarities", "abstract": "<p>Retrieval plays a fundamental role in recommendation systems, search, and\nnatural language processing by efficiently finding relevant items from a large\ncorpus given a query. Dot products have been widely used as the similarity\nfunction in such retrieval tasks, thanks to Maximum Inner Product Search (MIPS)\nthat enabled efficient retrieval based on dot products. However,\nstate-of-the-art retrieval algorithms have migrated to learned similarities.\nSuch algorithms vary in form; the queries can be represented with multiple\nembeddings, complex neural networks can be deployed, the item ids can be\ndecoded directly from queries using beam search, and multiple approaches can be\ncombined in hybrid solutions. Unfortunately, we lack efficient solutions for\nretrieval in these state-of-the-art setups. Our work investigates techniques\nfor approximate nearest neighbor search with learned similarity functions. We\nfirst prove that Mixture-of-Logits (MoL) is a universal approximator, and can\nexpress all learned similarity functions. We next propose techniques to\nretrieve the approximate top K results using MoL with a tight bound. We finally\ncompare our techniques with existing approaches, showing that MoL sets new\nstate-of-the-art results on recommendation retrieval tasks, and our approximate\ntop-k retrieval with learned similarities outperforms baselines by up to two\norders of magnitude in latency, while achieving &gt; .99 recall rate of exact\nalgorithms.</p>\n", "tags": ["Supervised", "ANN Search"], "tsne_embedding": [-7.445509910583496, -1.8094724416732788], "cluster": 7}, {"key": "ding2024knn", "year": "2024", "citations": "9", "title": "Knn Hashing With Factorized Neighborhood Representation", "abstract": "<p>Hashing is very effective for many tasks in reducing the\nprocessing time and in compressing massive databases. Although lots of approaches have been developed to learn\ndata-dependent hash functions in recent years, how to learn\nhash functions to yield good performance with acceptable\ncomputational and memory cost is still a challenging problem. Based on the observation that retrieval precision is\nhighly related to the kNN classification accuracy, this paper\nproposes a novel kNN-based supervised hashing method,\nwhich learns hash functions by directly maximizing the kNN\naccuracy of the Hamming-embedded training data. To make\nit scalable well to large problem, we propose a factorized\nneighborhood representation to parsimoniously model the\nneighborhood relationships inherent in training data. Considering that real-world data are often linearly inseparable,\nwe further kernelize this basic model to improve its performance. As a result, the proposed method is able to learn\naccurate hashing functions with tolerable computation and\nstorage cost. Experiments on four benchmarks demonstrate\nthat our method outperforms the state-of-the-arts.</p>\n", "tags": ["Supervised", "Hashing Methods", "ANN Search", "Evaluation Metrics", "Benchmarks and Datasets"], "tsne_embedding": [4.089784145355225, 7.43856954574585], "cluster": 5}, {"key": "dirksen2016fast", "year": "2016", "citations": "5", "title": "Fast Binary Embeddings With Gaussian Circulant Matrices Improved Bounds", "abstract": "<p>We consider the problem of encoding a finite set of vectors into a small\nnumber of bits while approximately retaining information on the angular\ndistances between the vectors. By deriving improved variance bounds related to\nbinary Gaussian circulant embeddings, we largely fix a gap in the proof of the\nbest known fast binary embedding method. Our bounds also show that\nwell-spreadness assumptions on the data vectors, which were needed in earlier\nwork on variance bounds, are unnecessary. In addition, we propose a new binary\nembedding with a faster running time on sparse data.</p>\n", "tags": ["Hashing Methods", "Efficient Learning"], "tsne_embedding": [-4.153484344482422, 1.6254280805587769], "cluster": 0}, {"key": "do2015discrete", "year": "2015", "citations": "10", "title": "Discrete Hashing With Deep Neural Network", "abstract": "<p>This paper addresses the problem of learning binary hash codes for large\nscale image search by proposing a novel hashing method based on deep neural\nnetwork. The advantage of our deep model over previous deep model used in\nhashing is that our model contains necessary criteria for producing good codes\nsuch as similarity preserving, balance and independence. Another advantage of\nour method is that instead of relaxing the binary constraint of codes during\nthe learning process as most previous works, in this paper, by introducing the\nauxiliary variable, we reformulate the optimization into two sub-optimization\nsteps allowing us to efficiently solve binary constraints without any\nrelaxation.\n  The proposed method is also extended to the supervised hashing by leveraging\nthe label information such that the learned binary codes preserve the pairwise\nlabel of inputs.\n  The experimental results on three benchmark datasets show the proposed\nmethods outperform state-of-the-art hashing methods.</p>\n", "tags": ["Hashing Methods", "Deep Hashing", "Supervised", "Benchmarks and Datasets", "Evaluation Metrics"], "tsne_embedding": [1.231931447982788, 5.514110088348389], "cluster": 5}, {"key": "do2016binary", "year": "2016", "citations": "12", "title": "Binary Hashing With Semidefinite Relaxation And Augmented Lagrangian", "abstract": "<p>This paper proposes two approaches for inferencing binary codes in two-step\n(supervised, unsupervised) hashing. We first introduce an unified formulation\nfor both supervised and unsupervised hashing. Then, we cast the learning of one\nbit as a Binary Quadratic Problem (BQP). We propose two approaches to solve\nBQP. In the first approach, we relax BQP as a semidefinite programming problem\nwhich its global optimum can be achieved. We theoretically prove that the\nobjective value of the binary solution achieved by this approach is well\nbounded. In the second approach, we propose an augmented Lagrangian based\napproach to solve BQP directly without relaxing the binary constraint.\nExperimental results on three benchmark datasets show that our proposed methods\ncompare favorably with the state of the art.</p>\n", "tags": ["Unsupervised", "Supervised", "Hashing Methods", "Benchmarks and Datasets", "Evaluation Metrics"], "tsne_embedding": [-2.6538028717041016, 4.862845420837402], "cluster": 0}, {"key": "do2016embedding", "year": "2016", "citations": "21", "title": "Embedding Based On Function Approximation For Large Scale Image Search", "abstract": "<p>The objective of this paper is to design an embedding method that maps local\nfeatures describing an image (e.g. SIFT) to a higher dimensional representation\nuseful for the image retrieval problem. First, motivated by the relationship\nbetween the linear approximation of a nonlinear function in high dimensional\nspace and the stateof-the-art feature representation used in image retrieval,\ni.e., VLAD, we propose a new approach for the approximation. The embedded\nvectors resulted by the function approximation process are then aggregated to\nform a single representation for image retrieval. Second, in order to make the\nproposed embedding method applicable to large scale problem, we further derive\nits fast version in which the embedded vectors can be efficiently computed,\ni.e., in the closed-form. We compare the proposed embedding methods with the\nstate of the art in the context of image search under various settings: when\nthe images are represented by medium length vectors, short vectors, or binary\nvectors. The experimental results show that the proposed embedding methods\noutperform existing the state of the art on the standard public image retrieval\nbenchmarks.</p>\n", "tags": ["Applications", "Evaluation Metrics"], "tsne_embedding": [4.314408302307129, -3.002009153366089], "cluster": 9}, {"key": "do2016learning", "year": "2016", "citations": "96", "title": "Learning To Hash With Binary Deep Neural Network", "abstract": "<p>This work proposes deep network models and learning algorithms for\nunsupervised and supervised binary hashing. Our novel network design constrains\none hidden layer to directly output the binary codes. This addresses a\nchallenging issue in some previous works: optimizing non-smooth objective\nfunctions due to binarization. Moreover, we incorporate independence and\nbalance properties in the direct and strict forms in the learning. Furthermore,\nwe include similarity preserving property in our objective function. Our\nresulting optimization with these binary, independence, and balance constraints\nis difficult to solve. We propose to attack it with alternating optimization\nand careful relaxation. Experimental results on three benchmark datasets show\nthat our proposed methods compare favorably with the state of the art.</p>\n", "tags": ["Unsupervised", "Supervised", "Hashing Methods", "Evaluation Metrics", "Benchmarks and Datasets"], "tsne_embedding": [1.8572633266448975, 5.859204292297363], "cluster": 5}, {"key": "do2017compact", "year": "2017", "citations": "14", "title": "Compact Hash Code Learning With Binary Deep Neural Network", "abstract": "<p>Learning compact binary codes for image retrieval problem using deep neural\nnetworks has recently attracted increasing attention. However, training deep\nhashing networks is challenging due to the binary constraints on the hash\ncodes. In this paper, we propose deep network models and learning algorithms\nfor learning binary hash codes given image representations under both\nunsupervised and supervised manners. The novelty of our network design is that\nwe constrain one hidden layer to directly output the binary codes. This design\nhas overcome a challenging problem in some previous works: optimizing\nnon-smooth objective functions because of binarization. In addition, we propose\nto incorporate independence and balance properties in the direct and strict\nforms into the learning schemes. We also include a similarity preserving\nproperty in our objective functions. The resulting optimizations involving\nthese binary, independence, and balance constraints are difficult to solve. To\ntackle this difficulty, we propose to learn the networks with alternating\noptimization and careful relaxation. Furthermore, by leveraging the powerful\ncapacity of convolutional neural networks, we propose an end-to-end\narchitecture that jointly learns to extract visual features and produce binary\nhash codes. Experimental results for the benchmark datasets show that the\nproposed methods compare favorably or outperform the state of the art.</p>\n", "tags": ["Deep Hashing", "Unsupervised", "Supervised", "Hashing Methods", "ANN Search", "Evaluation Metrics", "Benchmarks and Datasets"], "tsne_embedding": [1.6095153093338013, 5.16595458984375], "cluster": 5}, {"key": "do2017simultaneous", "year": "2017", "citations": "24", "title": "Simultaneous Feature Aggregating And Hashing For Large-scale Image Search", "abstract": "<p>In most state-of-the-art hashing-based visual search systems, local image\ndescriptors of an image are first aggregated as a single feature vector. This\nfeature vector is then subjected to a hashing function that produces a binary\nhash code. In previous work, the aggregating and the hashing processes are\ndesigned independently. In this paper, we propose a novel framework where\nfeature aggregating and hashing are designed simultaneously and optimized\njointly. Specifically, our joint optimization produces aggregated\nrepresentations that can be better reconstructed by some binary codes. This\nleads to more discriminative binary hash codes and improved retrieval accuracy.\nIn addition, we also propose a fast version of the recently-proposed Binary\nAutoencoder to be used in our proposed framework. We perform extensive\nretrieval experiments on several benchmark datasets with both SIFT and\nconvolutional features. Our results suggest that the proposed framework\nachieves significant improvements over the state of the art.</p>\n", "tags": ["Hashing Methods", "Evaluation Metrics", "Benchmarks and Datasets"], "tsne_embedding": [-1.0782570838928223, 1.4206035137176514], "cluster": 0}, {"key": "do2018binary", "year": "2018", "citations": "5", "title": "Binary Constrained Deep Hashing Network For Image Retrieval Without Manual Annotation", "abstract": "<p>Learning compact binary codes for image retrieval task using deep neural\nnetworks has attracted increasing attention recently. However, training deep\nhashing networks for the task is challenging due to the binary constraints on\nthe hash codes, the similarity preserving property, and the requirement for a\nvast amount of labelled images. To the best of our knowledge, none of the\nexisting methods has tackled all of these challenges completely in a unified\nframework. In this work, we propose a novel end-to-end deep learning approach\nfor the task, in which the network is trained to produce binary codes directly\nfrom image pixels without the need of manual annotation. In particular, to deal\nwith the non-smoothness of binary constraints, we propose a novel pairwise\nconstrained loss function, which simultaneously encodes the distances between\npairs of hash codes, and the binary quantization error. In order to train the\nnetwork with the proposed loss function, we propose an efficient parameter\nlearning algorithm. In addition, to provide similar / dissimilar training\nimages to train the network, we exploit 3D models reconstructed from unlabelled\nimages for automatic generation of enormous training image pairs. The extensive\nexperiments on image retrieval benchmark datasets demonstrate the improvements\nof the proposed method over the state-of-the-art compact representation methods\non the image retrieval problem.</p>\n", "tags": ["Applications", "Deep Hashing", "Quantization", "ANN Search", "Evaluation Metrics", "Benchmarks and Datasets", "Hashing Methods"], "tsne_embedding": [1.3617397546768188, 6.469388484954834], "cluster": 5}, {"key": "do2018from", "year": "2018", "citations": "21", "title": "From Selective Deep Convolutional Features To Compact Binary Representations For Image Retrieval", "abstract": "<p>In the large-scale image retrieval task, the two most important requirements\nare the discriminability of image representations and the efficiency in\ncomputation and storage of representations. Regarding the former requirement,\nConvolutional Neural Network (CNN) is proven to be a very powerful tool to\nextract highly discriminative local descriptors for effective image search.\nAdditionally, in order to further improve the discriminative power of the\ndescriptors, recent works adopt fine-tuned strategies. In this paper, taking a\ndifferent approach, we propose a novel, computationally efficient, and\ncompetitive framework. Specifically, we firstly propose various strategies to\ncompute masks, namely SIFT-mask, SUM-mask, and MAX-mask, to select a\nrepresentative subset of local convolutional features and eliminate redundant\nfeatures. Our in-depth analyses demonstrate that proposed masking schemes are\neffective to address the burstiness drawback and improve retrieval accuracy.\nSecondly, we propose to employ recent embedding and aggregating methods which\ncan significantly boost the feature discriminability. Regarding the computation\nand storage efficiency, we include a hashing module to produce very compact\nbinary image representations. Extensive experiments on six image retrieval\nbenchmarks demonstrate that our proposed framework achieves the\nstate-of-the-art retrieval performances.</p>\n", "tags": ["Applications", "Hashing Methods", "Evaluation Metrics"], "tsne_embedding": [3.2439258098602295, -2.146132469177246], "cluster": 9}, {"key": "do2019simultaneous", "year": "2019", "citations": "19", "title": "Simultaneous Feature Aggregating And Hashing For Compact Binary Code Learning", "abstract": "<p>Representing images by compact hash codes is an attractive approach for\nlarge-scale content-based image retrieval. In most state-of-the-art\nhashing-based image retrieval systems, for each image, local descriptors are\nfirst aggregated as a global representation vector. This global vector is then\nsubjected to a hashing function to generate a binary hash code. In previous\nworks, the aggregating and the hashing processes are designed independently.\nHence these frameworks may generate suboptimal hash codes. In this paper, we\nfirst propose a novel unsupervised hashing framework in which feature\naggregating and hashing are designed simultaneously and optimized jointly.\nSpecifically, our joint optimization generates aggregated representations that\ncan be better reconstructed by some binary codes. This leads to more\ndiscriminative binary hash codes and improved retrieval accuracy. In addition,\nthe proposed method is flexible. It can be extended for supervised hashing.\nWhen the data label is available, the framework can be adapted to learn binary\ncodes which minimize the reconstruction loss w.r.t. label vectors. Furthermore,\nwe also propose a fast version of the state-of-the-art hashing method Binary\nAutoencoder to be used in our proposed frameworks. Extensive experiments on\nbenchmark datasets under various settings show that the proposed methods\noutperform state-of-the-art unsupervised and supervised hashing methods.</p>\n", "tags": ["Applications", "Unsupervised", "Supervised", "Hashing Methods", "Benchmarks and Datasets", "Evaluation Metrics"], "tsne_embedding": [-0.46017003059387207, 1.6097259521484375], "cluster": 0}, {"key": "doan2020hidden", "year": "2020", "citations": "4", "title": "HM4: Hidden Markov Model With Memory Management For Visual Place Recognition", "abstract": "<p>Visual place recognition needs to be robust against appearance variability\ndue to natural and man-made causes. Training data collection should thus be an\nongoing process to allow continuous appearance changes to be recorded. However,\nthis creates an unboundedly-growing database that poses time and memory\nscalability challenges for place recognition methods. To tackle the scalability\nissue for visual place recognition in autonomous driving, we develop a Hidden\nMarkov Model approach with a two-tiered memory management. Our algorithm,\ndubbed HM\\(^4\\), exploits temporal look-ahead to transfer promising candidate\nimages between passive storage and active memory when needed. The inference\nprocess takes into account both promising images and a coarse representations\nof the full database. We show that this allows constant time and space\ninference for a fixed coverage area. The coarse representations can also be\nupdated incrementally to absorb new data. To further reduce the memory\nrequirements, we derive a compact image representation inspired by Locality\nSensitive Hashing (LSH). Through experiments on real world data, we demonstrate\nthe excellent scalability and accuracy of the approach under appearance changes\nand provide comparisons against state-of-the-art techniques.</p>\n", "tags": ["Hashing Methods", "ANN Search", "Efficient Learning"], "tsne_embedding": [9.684163093566895, 1.429585576057434], "cluster": 8}, {"key": "doan2020image", "year": "2020", "citations": "4", "title": "Image Hashing By Minimizing Discrete Component-wise Wasserstein Distance", "abstract": "<p>Image hashing is one of the fundamental problems that demand both efficient\nand effective solutions for various practical scenarios. Adversarial\nautoencoders are shown to be able to implicitly learn a robust,\nlocality-preserving hash function that generates balanced and high-quality hash\ncodes. However, the existing adversarial hashing methods are inefficient to be\nemployed for large-scale image retrieval applications. Specifically, they\nrequire an exponential number of samples to be able to generate optimal hash\ncodes and a significantly high computational cost to train. In this paper, we\nshow that the high sample-complexity requirement often results in sub-optimal\nretrieval performance of the adversarial hashing methods. To address this\nchallenge, we propose a new adversarial-autoencoder hashing approach that has a\nmuch lower sample requirement and computational cost. Specifically, by\nexploiting the desired properties of the hash function in the low-dimensional,\ndiscrete space, our method efficiently estimates a better variant of\nWasserstein distance by averaging a set of easy-to-compute one-dimensional\nWasserstein distances. The resulting hashing approach has an order-of-magnitude\nbetter sample complexity, thus better generalization property, compared to the\nother adversarial hashing methods. In addition, the computational cost is\nsignificantly reduced using our approach. We conduct experiments on several\nreal-world datasets and show that the proposed method outperforms the competing\nhashing methods, achieving up to 10% improvement over the current\nstate-of-the-art image hashing methods. The code accompanying this paper is\navailable on Github (https://github.com/khoadoan/adversarial-hashing).</p>\n", "tags": ["Applications", "Hashing Methods", "Deep Hashing", "Loss Functions", "Efficient Learning", "Tools and Libraries"], "tsne_embedding": [-2.051135301589966, -0.9073448181152344], "cluster": 0}, {"key": "doan2022asymmetric", "year": "2022", "citations": "1", "title": "Asymmetric Hashing For Fast Ranking Via Neural Network Measures", "abstract": "<p>Fast item ranking is an important task in recommender systems. In previous\nworks, graph-based Approximate Nearest Neighbor (ANN) approaches have\ndemonstrated good performance on item ranking tasks with generic\nsearching/matching measures (including complex measures such as neural network\nmeasures). However, since these ANN approaches must go through the neural\nmeasures several times during ranking, the computation is not practical if the\nneural measure is a large network. On the other hand, fast item ranking using\nexisting hashing-based approaches, such as Locality Sensitive Hashing (LSH),\nonly works with a limited set of measures. Previous learning-to-hash approaches\nare also not suitable to solve the fast item ranking problem since they can\ntake a significant amount of time and computation to train the hash functions.\nHashing approaches, however, are attractive because they provide a principle\nand efficient way to retrieve candidate items. In this paper, we propose a\nsimple and effective learning-to-hash approach for the fast item ranking\nproblem that can be used for any type of measure, including neural network\nmeasures. Specifically, we solve this problem with an asymmetric hashing\nframework based on discrete inner product fitting. We learn a pair of related\nhash functions that map heterogeneous objects (e.g., users and items) into a\ncommon discrete space where the inner product of their binary codes reveals\ntheir true similarity defined via the original searching measure. The fast\nranking problem is reduced to an ANN search via this asymmetric hashing scheme.\nThen, we propose a sampling strategy to efficiently select relevant and\ncontrastive samples to train the hashing model. We empirically validate the\nproposed method against the existing state-of-the-art fast item ranking methods\nin several combinations of non-linear searching functions and prominent\ndatasets.</p>\n", "tags": ["ANN Search", "Hashing Methods", "Evaluation Metrics", "Deep Hashing"], "tsne_embedding": [-3.3864917755126953, -3.5268020629882812], "cluster": 7}, {"key": "doan2022cooperative", "year": "2022", "citations": "2", "title": "Coophash: Cooperative Learning Of Multipurpose Descriptor And Contrastive Pair Generator Via Variational MCMC Teaching For Supervised Image Hashing", "abstract": "<p>Leveraging supervised information can lead to superior retrieval performance\nin the image hashing domain but the performance degrades significantly without\nenough labeled data. One effective solution to boost performance is to employ\ngenerative models, such as Generative Adversarial Networks (GANs), to generate\nsynthetic data in an image hashing model. However, GAN-based methods are\ndifficult to train, which prevents the hashing approaches from jointly training\nthe generative models and the hash functions. This limitation results in\nsub-optimal retrieval performance. To overcome this limitation, we propose a\nnovel framework, the generative cooperative hashing network, which is based on\nenergy-based cooperative learning. This framework jointly learns a powerful\ngenerative representation of the data and a robust hash function via two\ncomponents: a top-down contrastive pair generator that synthesizes contrastive\nimages and a bottom-up multipurpose descriptor that simultaneously represents\nthe images from multiple perspectives, including probability density, hash\ncode, latent code, and category. The two components are jointly learned via a\nnovel likelihood-based cooperative learning scheme. We conduct experiments on\nseveral real-world datasets and show that the proposed method outperforms the\ncompeting hashing supervised methods, achieving up to 10% relative improvement\nover the current state-of-the-art supervised hashing methods, and exhibits a\nsignificantly better performance in out-of-distribution retrieval.</p>\n", "tags": ["Hashing Methods", "Deep Hashing", "Supervised"], "tsne_embedding": [2.043842315673828, 4.133244514465332], "cluster": 9}, {"key": "doan2022one", "year": "2022", "citations": "39", "title": "One Loss For Quantization: Deep Hashing With Discrete Wasserstein Distributional Matching", "abstract": "<p>Image hashing is a principled approximate nearest neighbor approach to find\nsimilar items to a query in a large collection of images. Hashing aims to learn\na binary-output function that maps an image to a binary vector. For optimal\nretrieval performance, producing balanced hash codes with low-quantization\nerror to bridge the gap between the learning stage\u2019s continuous relaxation and\nthe inference stage\u2019s discrete quantization is important. However, in the\nexisting deep supervised hashing methods, coding balance and low-quantization\nerror are difficult to achieve and involve several losses. We argue that this\nis because the existing quantization approaches in these methods are\nheuristically constructed and not effective to achieve these objectives. This\npaper considers an alternative approach to learning the quantization\nconstraints. The task of learning balanced codes with low quantization error is\nre-formulated as matching the learned distribution of the continuous codes to a\npre-defined discrete, uniform distribution. This is equivalent to minimizing\nthe distance between two distributions. We then propose a computationally\nefficient distributional distance by leveraging the discrete property of the\nhash functions. This distributional distance is a valid distance and enjoys\nlower time and sample complexities. The proposed single-loss quantization\nobjective can be integrated into any existing supervised hashing method to\nimprove code balance and quantization error. Experiments confirm that the\nproposed approach substantially improves the performance of several\nrepresentative hashing~methods.</p>\n", "tags": ["Deep Hashing", "Quantization", "ANN Search", "Supervised", "Hashing Methods"], "tsne_embedding": [0.6409322619438171, 1.8361551761627197], "cluster": 9}, {"key": "dodds2018learning", "year": "2018", "citations": "4", "title": "Learning Embeddings For Product Visual Search With Triplet Loss And Online Sampling", "abstract": "<p>In this paper, we propose learning an embedding function for content-based\nimage retrieval within the e-commerce domain using the triplet loss and an\nonline sampling method that constructs triplets from within a minibatch. We\ncompare our method to several strong baselines as well as recent works on the\nDeepFashion and Stanford Online Product datasets. Our approach significantly\noutperforms the state-of-the-art on the DeepFashion dataset. With a\nmodification to favor sampling minibatches from a single product category, the\nsame approach demonstrates competitive results when compared to the\nstate-of-the-art for the Stanford Online Products dataset.</p>\n", "tags": ["Applications", "Loss Functions"], "tsne_embedding": [11.0604887008667, 4.456987380981445], "cluster": 3}, {"key": "dolhansky2020adversarial", "year": "2020", "citations": "9", "title": "Adversarial Collision Attacks On Image Hashing Functions", "abstract": "<p>Hashing images with a perceptual algorithm is a common approach to solving\nduplicate image detection problems. However, perceptual image hashing\nalgorithms are differentiable, and are thus vulnerable to gradient-based\nadversarial attacks. We demonstrate that not only is it possible to modify an\nimage to produce an unrelated hash, but an exact image hash collision between a\nsource and target image can be produced via minuscule adversarial\nperturbations. In a white box setting, these collisions can be replicated\nacross nearly every image pair and hash type (including both deep and\nnon-learned hashes). Furthermore, by attacking points other than the output of\na hashing function, an attacker can avoid having to know the details of a\nparticular algorithm, resulting in collisions that transfer across different\nhash sizes or model architectures. Using these techniques, an adversary can\npoison the image lookup table of a duplicate image detection service, resulting\nin undefined or unwanted behavior. Finally, we offer several potential\nmitigations to gradient-based image hash attacks.</p>\n", "tags": ["Hashing Methods", "Privacy and Security"], "tsne_embedding": [-4.734918117523193, 5.7076568603515625], "cluster": 0}, {"key": "dong2017video", "year": "2017", "citations": "14", "title": "Video Retrieval Based On Deep Convolutional Neural Network", "abstract": "<p>Recently, with the enormous growth of online videos, fast video retrieval\nresearch has received increasing attention. As an extension of image hashing\ntechniques, traditional video hashing methods mainly depend on hand-crafted\nfeatures and transform the real-valued features into binary hash codes. As\nvideos provide far more diverse and complex visual information than images,\nextracting features from videos is much more challenging than that from images.\nTherefore, high-level semantic features to represent videos are needed rather\nthan low-level hand-crafted methods. In this paper, a deep convolutional neural\nnetwork is proposed to extract high-level semantic features and a binary hash\nfunction is then integrated into this framework to achieve an end-to-end\noptimization. Particularly, our approach also combines triplet loss function\nwhich preserves the relative similarity and difference of videos and\nclassification loss function as the optimization objective. Experiments have\nbeen performed on two public datasets and the results demonstrate the\nsuperiority of our proposed method compared with other state-of-the-art video\nretrieval methods.</p>\n", "tags": ["Applications", "Hashing Methods", "Loss Functions", "Deep Hashing"], "tsne_embedding": [10.250763893127441, 5.189610481262207], "cluster": 3}, {"key": "dong2019document", "year": "2019", "citations": "8", "title": "Document Hashing With Mixture-prior Generative Models", "abstract": "<p>Hashing is promising for large-scale information retrieval tasks thanks to\nthe efficiency of distance evaluation between binary codes. Generative hashing\nis often used to generate hashing codes in an unsupervised way. However,\nexisting generative hashing methods only considered the use of simple priors,\nlike Gaussian and Bernoulli priors, which limits these methods to further\nimprove their performance. In this paper, two mixture-prior generative models\nare proposed, under the objective to produce high-quality hashing codes for\ndocuments. Specifically, a Gaussian mixture prior is first imposed onto the\nvariational auto-encoder (VAE), followed by a separate step to cast the\ncontinuous latent representation of VAE into binary code. To avoid the\nperformance loss caused by the separate casting, a model using a Bernoulli\nmixture prior is further developed, in which an end-to-end training is admitted\nby resorting to the straight-through (ST) discrete gradient estimator.\nExperimental results on several benchmark datasets demonstrate that the\nproposed methods, especially the one using Bernoulli mixture priors,\nconsistently outperform existing ones by a substantial margin.</p>\n", "tags": ["Unsupervised", "Hashing Methods", "Benchmarks and Datasets", "Evaluation Metrics"], "tsne_embedding": [-0.11156967282295227, 0.23175087571144104], "cluster": 9}, {"key": "dong2019learning", "year": "2019", "citations": "26", "title": "Learning Space Partitions For Nearest Neighbor Search", "abstract": "<p>Space partitions of \\(\\mathbb{R}^d\\) underlie a vast and important class of\nfast nearest neighbor search (NNS) algorithms. Inspired by recent theoretical\nwork on NNS for general metric spaces [Andoni, Naor, Nikolov, Razenshteyn,\nWaingarten STOC 2018, FOCS 2018], we develop a new framework for building space\npartitions reducing the problem to balanced graph partitioning followed by\nsupervised classification. We instantiate this general approach with the KaHIP\ngraph partitioner [Sanders, Schulz SEA 2013] and neural networks, respectively,\nto obtain a new partitioning procedure called Neural Locality-Sensitive Hashing\n(Neural LSH). On several standard benchmarks for NNS, our experiments show that\nthe partitions obtained by Neural LSH consistently outperform partitions found\nby quantization-based and tree-based methods as well as classic, data-oblivious\nLSH.</p>\n", "tags": ["ANN Search", "Supervised", "Hashing Methods", "Quantization", "Evaluation Metrics"], "tsne_embedding": [4.91066837310791, -11.59463119506836], "cluster": 1}, {"key": "dong2020using", "year": "2020", "citations": "4", "title": "Using Text To Teach Image Retrieval", "abstract": "<p>Image retrieval relies heavily on the quality of the data modeling and the\ndistance measurement in the feature space. Building on the concept of image\nmanifold, we first propose to represent the feature space of images, learned\nvia neural networks, as a graph. Neighborhoods in the feature space are now\ndefined by the geodesic distance between images, represented as graph vertices\nor manifold samples. When limited images are available, this manifold is\nsparsely sampled, making the geodesic computation and the corresponding\nretrieval harder. To address this, we augment the manifold samples with\ngeometrically aligned text, thereby using a plethora of sentences to teach us\nabout images. In addition to extensive results on standard datasets\nillustrating the power of text to help in image retrieval, a new public dataset\nbased on CLEVR is introduced to quantify the semantic similarity between visual\ndata and text data. The experimental results show that the joint embedding\nmanifold is a robust representation, allowing it to be a better basis to\nperform image retrieval given only an image and a textual instruction on the\ndesired modifications over the image</p>\n", "tags": ["Applications", "Multi-Modal Hashing"], "tsne_embedding": [6.899628162384033, -6.443288803100586], "cluster": 1}, {"key": "dong2022generalized", "year": "2022", "citations": "2", "title": "A Generalized Approach For Cancellable Template And Its Realization For Minutia Cylinder-code", "abstract": "<p>Hashing technology gains much attention in protecting the biometric template\nlately. For instance, Index-of-Max (IoM), a recent reported hashing technique,\nis a ranking-based locality sensitive hashing technique, which illustrates the\nfeasibility to protect the ordered and fixed-length biometric template.\nHowever, biometric templates are not always in the form of ordered and\nfixed-length, rather it may be an unordered and variable size point set e.g.\nfingerprint minutiae, which restricts the usage of the traditional hashing\ntechnology. In this paper, we proposed a generalized version of IoM hashing\nnamely gIoM, and therefore the unordered and variable size biometric template\ncan be used. We demonstrate a realization using a well-known variable size\nfeature vector, fingerprint Minutia Cylinder-Code (MCC). The gIoM transforms\nMCC into index domain to form indexing-based feature representation.\nConsequently, the inversion of MCC from the transformed representation is\ncomputational infeasible, thus to achieve non-invertibility while the\nperformance is preserved. Public fingerprint databases FVC2002 and FVC2004 are\nemployed for experiment as benchmark to demonstrate a fair comparison with\nother methods. Moreover, the security and privacy analysis suggest that gIoM\nmeets the criteria of template protection: non-invertibility, revocability, and\nnon-linkability.</p>\n", "tags": ["Hashing Methods", "Indexing", "Applications", "Benchmarks and Datasets", "Privacy and Security"], "tsne_embedding": [-4.7237348556518555, 10.82689380645752], "cluster": 0}, {"key": "dong2022learning", "year": "2022", "citations": "3", "title": "Learning-based Dimensionality Reduction For Computing Compact And Effective Local Feature Descriptors", "abstract": "<p>A distinctive representation of image patches in form of features is a key\ncomponent of many computer vision and robotics tasks, such as image matching,\nimage retrieval, and visual localization. State-of-the-art descriptors, from\nhand-crafted descriptors such as SIFT to learned ones such as HardNet, are\nusually high dimensional; 128 dimensions or even more. The higher the\ndimensionality, the larger the memory consumption and computational time for\napproaches using such descriptors. In this paper, we investigate multi-layer\nperceptrons (MLPs) to extract low-dimensional but high-quality descriptors. We\nthoroughly analyze our method in unsupervised, self-supervised, and supervised\nsettings, and evaluate the dimensionality reduction results on four\nrepresentative descriptors. We consider different applications, including\nvisual localization, patch verification, image matching and retrieval. The\nexperiments show that our lightweight MLPs achieve better dimensionality\nreduction than PCA. The lower-dimensional descriptors generated by our approach\noutperform the original higher-dimensional descriptors in downstream tasks,\nespecially for the hand-crafted ones. The code will be available at\nhttps://github.com/PRBonn/descriptor-dr.</p>\n", "tags": ["Applications", "Unsupervised", "Supervised"], "tsne_embedding": [-0.2765835225582123, 9.756596565246582], "cluster": 5}, {"key": "doshi2020lanns", "year": "2020", "citations": "0", "title": "LANNS A Web-scale Approximate Nearest Neighbor Lookup System", "abstract": "<p>Nearest neighbor search (NNS) has a wide range of applications in information\nretrieval, computer vision, machine learning, databases, and other areas.\nExisting state-of-the-art algorithm for nearest neighbor search, Hierarchical\nNavigable Small World Networks(HNSW), is unable to scale to large datasets of\n100M records in high dimensions. In this paper, we propose LANNS, an end-to-end\nplatform for Approximate Nearest Neighbor Search, which scales for web-scale\ndatasets. Library for Large Scale Approximate Nearest Neighbor Search (LANNS)\nis deployed in multiple production systems for identifying topK (\\(100 \\leq topK\n\\leq 200\\)) approximate nearest neighbors with a latency of a few milliseconds\nper query, high throughput of 2.5k Queries Per Second (QPS) on a single node,\non large (\\(\\sim\\)180M data points) high dimensional (50-2048 dimensional)\ndatasets.</p>\n", "tags": ["ANN Search", "Efficient Learning"], "tsne_embedding": [-10.969813346862793, 7.029354572296143], "cluster": 4}, {"key": "doshi2020web", "year": "2020", "citations": "0", "title": "LANNS: A Web-scale Approximate Nearest Neighbor Lookup System", "abstract": "<p>Nearest neighbor search (NNS) has a wide range of applications in information\nretrieval, computer vision, machine learning, databases, and other areas.\nExisting state-of-the-art algorithm for nearest neighbor search, Hierarchical\nNavigable Small World Networks(HNSW), is unable to scale to large datasets of\n100M records in high dimensions. In this paper, we propose LANNS, an end-to-end\nplatform for Approximate Nearest Neighbor Search, which scales for web-scale\ndatasets. Library for Large Scale Approximate Nearest Neighbor Search (LANNS)\nis deployed in multiple production systems for identifying topK (\\(100 \\leq topK\n\\leq 200\\)) approximate nearest neighbors with a latency of a few milliseconds\nper query, high throughput of 2.5k Queries Per Second (QPS) on a single node,\non large (\\(\\sim\\)180M data points) high dimensional (50-2048 dimensional)\ndatasets.</p>\n", "tags": ["ANN Search", "Applications", "Tools and Libraries"], "tsne_embedding": [-10.969813346862793, 7.029354572296143], "cluster": 4}, {"key": "dou2020learning", "year": "2020", "citations": "7", "title": "Learning Global And Local Consistent Representations For Unsupervised Image Retrieval Via Deep Graph Diffusion Networks", "abstract": "<p>Diffusion has shown great success in improving accuracy of unsupervised image\nretrieval systems by utilizing high-order structures of image manifold.\nHowever, existing diffusion methods suffer from three major limitations: 1)\nthey usually rely on local structures without considering global manifold\ninformation; 2) they focus on improving pair-wise similarities within existing\nimages input output transductively while lacking flexibility to learn\nrepresentations for novel unseen instances inductively; 3) they fail to scale\nto large datasets due to prohibitive memory consumption and computational\nburden due to intrinsic high-order operations on the whole graph. In this\npaper, to address these limitations, we propose a novel method, Graph Diffusion\nNetworks (GRAD-Net), that adopts graph neural networks (GNNs), a novel variant\nof deep learning algorithms on irregular graphs. GRAD-Net learns semantic\nrepresentations by exploiting both local and global structures of image\nmanifold in an unsupervised fashion. By utilizing sparse coding techniques,\nGRAD-Net not only preserves global information on the image manifold, but also\nenables scalable training and efficient querying. Experiments on several large\nbenchmark datasets demonstrate effectiveness of our method over\nstate-of-the-art diffusion algorithms for unsupervised image retrieval.</p>\n", "tags": ["Applications", "Unsupervised", "Evaluation Metrics", "Benchmarks and Datasets"], "tsne_embedding": [2.34073805809021, -8.171692848205566], "cluster": 1}, {"key": "douze2016polysemous", "year": "2016", "citations": "31", "title": "Polysemous Codes", "abstract": "<p>This paper considers the problem of approximate nearest neighbor search in\nthe compressed domain. We introduce polysemous codes, which offer both the\ndistance estimation quality of product quantization and the efficient\ncomparison of binary codes with Hamming distance. Their design is inspired by\nalgorithms introduced in the 90\u2019s to construct channel-optimized vector\nquantizers. At search time, this dual interpretation accelerates the search.\nMost of the indexed vectors are filtered out with Hamming distance, letting\nonly a fraction of the vectors to be ranked with an asymmetric distance\nestimator.\n  The method is complementary with a coarse partitioning of the feature space\nsuch as the inverted multi-index. This is shown by our experiments performed on\nseveral public benchmarks such as the BIGANN dataset comprising one billion\nvectors, for which we report state-of-the-art results for query times below\n0.3\\,millisecond per core. Last but not least, our approach allows the\napproximate computation of the k-NN graph associated with the Yahoo Flickr\nCreative Commons 100M, described by CNN image descriptors, in less than 8 hours\non a single machine.</p>\n", "tags": ["Quantization", "ANN Search", "Evaluation Metrics", "Benchmarks and Datasets"], "tsne_embedding": [-4.298792839050293, -9.168099403381348], "cluster": 6}, {"key": "douze2018link", "year": "2018", "citations": "21", "title": "Link And Code: Fast Indexing With Graphs And Compact Regression Codes", "abstract": "<p>Similarity search approaches based on graph walks have recently attained\noutstanding speed-accuracy trade-offs, taking aside the memory requirements. In\nthis paper, we revisit these approaches by considering, additionally, the\nmemory constraint required to index billions of images on a single server. This\nleads us to propose a method based both on graph traversal and compact\nrepresentations. We encode the indexed vectors using quantization and exploit\nthe graph structure to refine the similarity estimation.\n  In essence, our method takes the best of these two worlds: the search\nstrategy is based on nested graphs, thereby providing high precision with a\nrelatively small set of comparisons. At the same time it offers a significant\nmemory compression. As a result, our approach outperforms the state of the art\non operating points considering 64-128 bytes per vector, as demonstrated by our\nresults on two billion-scale public benchmarks.</p>\n", "tags": ["Indexing", "Quantization"], "tsne_embedding": [2.960338830947876, -9.92857837677002], "cluster": 1}, {"key": "douze2024faiss", "year": "2024", "citations": "15", "title": "The Faiss Library", "abstract": "<p>Vector databases typically manage large collections of embedding vectors.\nCurrently, AI applications are growing rapidly, and so is the number of\nembeddings that need to be stored and indexed. The Faiss library is dedicated\nto vector similarity search, a core functionality of vector databases. Faiss is\na toolkit of indexing methods and related primitives used to search, cluster,\ncompress and transform vectors. This paper describes the trade-off space of\nvector search and the design principles of Faiss in terms of structure,\napproach to optimization and interfacing. We benchmark key features of the\nlibrary and discuss a few selected applications to highlight its broad\napplicability.</p>\n", "tags": ["Applications", "Indexing", "Evaluation Metrics", "Tools and Libraries"], "tsne_embedding": [11.729172706604004, -3.985832691192627], "cluster": 8}, {"key": "driemel2017locality", "year": "2017", "citations": "31", "title": "Locality-sensitive Hashing Of Curves", "abstract": "<p>We study data structures for storing a set of polygonal curves in \\({\\rm R}^d\\)\nsuch that, given a query curve, we can efficiently retrieve similar curves from\nthe set, where similarity is measured using the discrete Fr'echet distance or\nthe dynamic time warping distance. To this end we devise the first\nlocality-sensitive hashing schemes for these distance measures. A major\nchallenge is posed by the fact that these distance measures internally optimize\nthe alignment between the curves. We give solutions for different types of\nalignments including constrained and unconstrained versions. For unconstrained\nalignments, we improve over a result by Indyk from 2002 for short curves. Let\n\\(n\\) be the number of input curves and let \\(m\\) be the maximum complexity of a\ncurve in the input. In the particular case where \\(m \\leq \\frac{\\alpha}{4d} log\nn\\), for some fixed \\(\\alpha&gt;0\\), our solutions imply an approximate near-neighbor\ndata structure for the discrete Fr'echet distance that uses space in\n\\(O(n^{1+\\alpha}log n)\\) and achieves query time in \\(O(n^{\\alpha}log^2 n)\\) and\nconstant approximation factor. Furthermore, our solutions provide a trade-off\nbetween approximation quality and computational performance: for any parameter\n\\(k \\in [m]\\), we can give a data structure that uses space in \\(O(2^{2k}m^{k-1} n\nlog n + nm)\\), answers queries in \\(O( 2^{2k} m^{k}log n)\\) time and achieves\napproximation factor in \\(O(m/k)\\).</p>\n", "tags": ["Hashing Methods", "ANN Search"], "tsne_embedding": [-14.628729820251465, -8.640941619873047], "cluster": 2}, {"key": "du2014inner", "year": "2014", "citations": "18", "title": "Inner Product Similarity Search Using Compositional Codes", "abstract": "<p>This paper addresses the nearest neighbor search problem under inner product\nsimilarity and introduces a compact code-based approach. The idea is to\napproximate a vector using the composition of several elements selected from a\nsource dictionary and to represent this vector by a short code composed of the\nindices of the selected elements. The inner product between a query vector and\na database vector is efficiently estimated from the query vector and the short\ncode of the database vector. We show the superior performance of the proposed\ngroup \\(M\\)-selection algorithm that selects \\(M\\) elements from \\(M\\) source\ndictionaries for vector approximation in terms of search accuracy and\nefficiency for compact codes of the same length via theoretical and empirical\nanalysis. Experimental results on large-scale datasets (\\(1M\\) and \\(1B\\) SIFT\nfeatures, \\(1M\\) linear models and Netflix) demonstrate the superiority of the\nproposed approach.</p>\n", "tags": ["ANN Search", "Efficient Learning"], "tsne_embedding": [-8.798551559448242, -0.40425148606300354], "cluster": 4}, {"key": "dubey2021vision", "year": "2021", "citations": "48", "title": "Vision Transformer Hashing For Image Retrieval", "abstract": "<p>Deep learning has shown a tremendous growth in hashing techniques for image\nretrieval. Recently, Transformer has emerged as a new architecture by utilizing\nself-attention without convolution. Transformer is also extended to Vision\nTransformer (ViT) for the visual recognition with a promising performance on\nImageNet. In this paper, we propose a Vision Transformer based Hashing (VTS)\nfor image retrieval. We utilize the pre-trained ViT on ImageNet as the backbone\nnetwork and add the hashing head. The proposed VTS model is fine tuned for\nhashing under six different image retrieval frameworks, including Deep\nSupervised Hashing (DSH), HashNet, GreedyHash, Improved Deep Hashing Network\n(IDHN), Deep Polarized Network (DPN) and Central Similarity Quantization (CSQ)\nwith their objective functions. We perform the extensive experiments on\nCIFAR10, ImageNet, NUS-Wide, and COCO datasets. The proposed VTS based image\nretrieval outperforms the recent state-of-the-art hashing techniques with a\ngreat margin. We also find the proposed VTS model as the backbone network is\nbetter than the existing networks, such as AlexNet and ResNet. The code is\nreleased at \\url{https://github.com/shivram1987/VisionTransformerHashing}.</p>\n", "tags": ["Applications", "Deep Hashing", "Quantization", "Graph and Transformer Models", "Supervised", "Benchmarks and Datasets", "Hashing Methods"], "tsne_embedding": [9.113327980041504, 3.577566146850586], "cluster": 3}, {"key": "dubey2024transformer", "year": "2024", "citations": "3", "title": "Transformer-based Clipped Contrastive Quantization Learning For Unsupervised Image Retrieval", "abstract": "<p>Unsupervised image retrieval aims to learn the important visual\ncharacteristics without any given level to retrieve the similar images for a\ngiven query image. The Convolutional Neural Network (CNN)-based approaches have\nbeen extensively exploited with self-supervised contrastive learning for image\nhashing. However, the existing approaches suffer due to lack of effective\nutilization of global features by CNNs and biased-ness created by false\nnegative pairs in the contrastive learning. In this paper, we propose a\nTransClippedCLR model by encoding the global context of an image using\nTransformer having local context through patch based processing, by generating\nthe hash codes through product quantization and by avoiding the potential false\nnegative pairs through clipped contrastive learning. The proposed model is\ntested with superior performance for unsupervised image retrieval on benchmark\ndatasets, including CIFAR10, NUS-Wide and Flickr25K, as compared to the recent\nstate-of-the-art deep models. The results using the proposed clipped\ncontrastive learning are greatly improved on all datasets as compared to same\nbackbone network with vanilla contrastive learning.</p>\n", "tags": ["Applications", "Unsupervised", "Quantization", "Evaluation Metrics", "Benchmarks and Datasets", "Hashing Methods"], "tsne_embedding": [-0.701297402381897, 7.916084289550781], "cluster": 5}, {"key": "duda2016distortion", "year": "2016", "citations": "0", "title": "Distortion-resistant Hashing For Rapid Search Of Similar DNA Subsequence", "abstract": "<p>One of the basic tasks in bioinformatics is localizing a short subsequence\n\\(S\\), read while sequencing, in a long reference sequence \\(R\\), like the human\ngeneome. A natural rapid approach would be finding a hash value for \\(S\\) and\ncompare it with a prepared database of hash values for each of length \\(|S|\\)\nsubsequences of \\(R\\). The problem with such approach is that it would only spot\na perfect match, while in reality there are lots of small changes:\nsubstitutions, deletions and insertions.\n  This issue could be repaired if having a hash function designed to tolerate\nsome small distortion accordingly to an alignment metric (like\nNeedleman-Wunch): designed to make that two similar sequences should most\nlikely give the same hash value. This paper discusses construction of\nDistortion-Resistant Hashing (DRH) to generate such fingerprints for rapid\nsearch of similar subsequences. The proposed approach is based on the rate\ndistortion theory: in a nearly uniform subset of length \\(|S|\\) sequences, the\nhash value represents the closest sequence to \\(S\\). This gives some control of\nthe distance of collisions: sequences having the same hash value.</p>\n", "tags": ["Hashing Methods", "ANN Search"], "tsne_embedding": [-6.129183769226074, -6.520334243774414], "cluster": 7}, {"key": "dutta2017stochastic", "year": "2017", "citations": "11", "title": "Stochastic Graphlet Embedding", "abstract": "<p>Graph-based methods are known to be successful in many machine learning and\npattern classification tasks. These methods consider semi-structured data as\ngraphs where nodes correspond to primitives (parts, interest points, segments,\netc.) and edges characterize the relationships between these primitives.\nHowever, these non-vectorial graph data cannot be straightforwardly plugged\ninto off-the-shelf machine learning algorithms without a preliminary step of \u2013\nexplicit/implicit \u2013 graph vectorization and embedding. This embedding process\nshould be resilient to intra-class graph variations while being highly\ndiscriminant. In this paper, we propose a novel high-order stochastic graphlet\nembedding (SGE) that maps graphs into vector spaces. Our main contribution\nincludes a new stochastic search procedure that efficiently parses a given\ngraph and extracts/samples unlimitedly high-order graphlets. We consider these\ngraphlets, with increasing orders, to model local primitives as well as their\nincreasingly complex interactions. In order to build our graph representation,\nwe measure the distribution of these graphlets into a given graph, using\nparticular hash functions that efficiently assign sampled graphlets into\nisomorphic sets with a very low probability of collision. When combined with\nmaximum margin classifiers, these graphlet-based representations have positive\nimpact on the performance of pattern comparison and recognition as corroborated\nthrough extensive experiments using standard benchmark databases.</p>\n", "tags": ["Hashing Methods", "ANN Search", "Evaluation Metrics"], "tsne_embedding": [5.901608467102051, -9.844070434570312], "cluster": 1}, {"key": "dutta2018graph", "year": "2018", "citations": "0", "title": "Graph Kernels Based On High Order Graphlet Parsing And Hashing", "abstract": "<p>Graph-based methods are known to be successful in many machine learning and\npattern classification tasks. These methods consider semi-structured data as\ngraphs where nodes correspond to primitives (parts, interest points, segments,\netc.) and edges characterize the relationships between these primitives.\nHowever, these non-vectorial graph data cannot be straightforwardly plugged\ninto off-the-shelf machine learning algorithms without a preliminary step of \u2013\nexplicit/implicit \u2013 graph vectorization and embedding. This embedding process\nshould be resilient to intra-class graph variations while being highly\ndiscriminant. In this paper, we propose a novel high-order stochastic graphlet\nembedding (SGE) that maps graphs into vector spaces. Our main contribution\nincludes a new stochastic search procedure that efficiently parses a given\ngraph and extracts/samples unlimitedly high-order graphlets. We consider these\ngraphlets, with increasing orders, to model local primitives as well as their\nincreasingly complex interactions. In order to build our graph representation,\nwe measure the distribution of these graphlets into a given graph, using\nparticular hash functions that efficiently assign sampled graphlets into\nisomorphic sets with a very low probability of collision. When combined with\nmaximum margin classifiers, these graphlet-based representations have positive\nimpact on the performance of pattern comparison and recognition as corroborated\nthrough extensive experiments using standard benchmark databases.</p>\n", "tags": ["Hashing Methods", "ANN Search", "Evaluation Metrics"], "tsne_embedding": [5.901608467102051, -9.844070434570312], "cluster": 1}, {"key": "dutta2018when", "year": "2018", "citations": "6", "title": "When Hashing Met Matching: Efficient Spatio-temporal Search For Ridesharing", "abstract": "<p>Carpooling, or sharing a ride with other passengers, holds immense potential\nfor urban transportation. Ridesharing platforms enable such sharing of rides\nusing real-time data. Finding ride matches in real-time at urban scale is a\ndifficult combinatorial optimization task and mostly heuristic approaches are\napplied. In this work, we mathematically model the problem as that of finding\nnear-neighbors and devise a novel efficient spatio-temporal search algorithm\nbased on the theory of locality sensitive hashing for Maximum Inner Product\nSearch (MIPS). The proposed algorithm can find \\(k\\) near-optimal potential\nmatches for every ride from a pool of \\(n\\) rides in time \\(O(n^{1 + \\rho} (k +\nlog n) log k)\\) and space \\(O(n^{1 + \\rho} log k)\\) for a small \\(\\rho &lt; 1\\). Our\nalgorithm can be extended in several useful and interesting ways increasing its\npractical appeal. Experiments with large NY yellow taxi trip datasets show that\nour algorithm consistently outperforms state-of-the-art heuristic methods\nthereby proving its practical applicability.</p>\n", "tags": ["Hashing Methods", "ANN Search", "Efficient Learning"], "tsne_embedding": [-17.281328201293945, -1.8151969909667969], "cluster": 2}, {"key": "efremenko2019fast", "year": "2019", "citations": "1", "title": "Fast And Bayes-consistent Nearest Neighbors", "abstract": "<p>Research on nearest-neighbor methods tends to focus somewhat dichotomously\neither on the statistical or the computational aspects \u2013 either on, say, Bayes\nconsistency and rates of convergence or on techniques for speeding up the\nproximity search. This paper aims at bridging these realms: to reap the\nadvantages of fast evaluation time while maintaining Bayes consistency, and\nfurther without sacrificing too much in the risk decay rate. We combine the\nlocality-sensitive hashing (LSH) technique with a novel missing-mass argument\nto obtain a fast and Bayes-consistent classifier. Our algorithm\u2019s prediction\nruntime compares favorably against state of the art approximate NN methods,\nwhile maintaining Bayes-consistency and attaining rates comparable to minimax.\nOn samples of size \\(n\\) in \\(\\R^d\\), our pre-processing phase has runtime \\(O(d n\nlog n)\\), while the evaluation phase has runtime \\(O(dlog n)\\) per query point.</p>\n", "tags": ["Hashing Methods", "ANN Search", "Efficient Learning"], "tsne_embedding": [-8.65194034576416, 1.0067501068115234], "cluster": 4}, {"key": "eghbali2016fast", "year": "2016", "citations": "10", "title": "Fast Cosine Similarity Search In Binary Space With Angular Multi-index Hashing", "abstract": "<p>Given a large dataset of binary codes and a binary query point, we address\nhow to efficiently find \\(K\\) codes in the dataset that yield the largest cosine\nsimilarities to the query. The straightforward answer to this problem is to\ncompare the query with all items in the dataset, but this is practical only for\nsmall datasets. One potential solution to enhance the search time and achieve\nsublinear cost is to use a hash table populated with binary codes of the\ndataset and then look up the nearby buckets to the query to retrieve the\nnearest neighbors. However, if codes are compared in terms of cosine similarity\nrather than the Hamming distance, then the main issue is that the order of\nbuckets to probe is not evident. To examine this issue, we first elaborate on\nthe connection between the Hamming distance and the cosine similarity. Doing\nthis allows us to systematically find the probing sequence in the hash table.\nHowever, solving the nearest neighbor search with a single table is only\npractical for short binary codes. To address this issue, we propose the angular\nmulti-index hashing search algorithm which relies on building multiple hash\ntables on binary code substrings. The proposed search algorithm solves the\nexact angular \\(K\\) nearest neighbor problem in a time that is often orders of\nmagnitude faster than the linear scan baseline and even approximation methods.</p>\n", "tags": ["ANN Search", "Hashing Methods", "Evaluation Metrics"], "tsne_embedding": [-6.851469039916992, -3.0339858531951904], "cluster": 7}, {"key": "eghbali2019deep", "year": "2019", "citations": "23", "title": "Deep Spherical Quantization For Image Search", "abstract": "<p>Hashing methods, which encode high-dimensional images with compact discrete\ncodes, have been widely applied to enhance large-scale image retrieval. In this\npaper, we put forward Deep Spherical Quantization (DSQ), a novel method to make\ndeep convolutional neural networks generate supervised and compact binary codes\nfor efficient image search. Our approach simultaneously learns a mapping that\ntransforms the input images into a low-dimensional discriminative space, and\nquantizes the transformed data points using multi-codebook quantization. To\neliminate the negative effect of norm variance on codebook learning, we force\nthe network to L_2 normalize the extracted features and then quantize the\nresulting vectors using a new supervised quantization technique specifically\ndesigned for points lying on a unit hypersphere. Furthermore, we introduce an\neasy-to-implement extension of our quantization technique that enforces\nsparsity on the codebooks. Extensive experiments demonstrate that DSQ and its\nsparse variant can generate semantically separable compact binary codes\noutperforming many state-of-the-art image retrieval methods on three\nbenchmarks.</p>\n", "tags": ["Applications", "Quantization", "Supervised", "Hashing Methods"], "tsne_embedding": [7.791261672973633, 0.0689982920885086], "cluster": 8}, {"key": "elnouby2021training", "year": "2021", "citations": "120", "title": "Training Vision Transformers For Image Retrieval", "abstract": "<p>Transformers have shown outstanding results for natural language\nunderstanding and, more recently, for image classification. We here extend this\nwork and propose a transformer-based approach for image retrieval: we adopt\nvision transformers for generating image descriptors and train the resulting\nmodel with a metric learning objective, which combines a contrastive loss with\na differential entropy regularizer. Our results show consistent and significant\nimprovements of transformers over convolution-based approaches. In particular,\nour method outperforms the state of the art on several public benchmarks for\ncategory-level retrieval, namely Stanford Online Product, In-Shop and CUB-200.\nFurthermore, our experiments on ROxford and RParis also show that, in\ncomparable settings, transformers are competitive for particular object\nretrieval, especially in the regime of short vector representations and\nlow-resolution images.</p>\n", "tags": ["Applications", "Evaluation Metrics", "Loss Functions"], "tsne_embedding": [-4.1654205322265625, 3.3126983642578125], "cluster": 0}]